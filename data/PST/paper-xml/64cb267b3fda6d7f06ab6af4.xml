<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Sparse to Soft Mixtures of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-08-02">2 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">From Sparse to Soft Mixtures of Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-02">2 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2308.00951v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5? lower inference cost (5.7? lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40? more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better. * Equal contribution. The order was decided by a coin toss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Larger Transformers improve performance at increased computational cost. Recent studies suggest that model size and training data must be scaled together to optimally use any given training compute budget <ref type="bibr" target="#b11">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b9">Hoffmann et al., 2022;</ref><ref type="bibr">Zhai et al., 2022a)</ref>. A promising alternative that allows to scale models in size without paying their full computational cost is sparse mixtures of experts <ref type="bibr">(MoEs)</ref>. Recently, a number of successful approaches have proposed ways to sparsely activate token paths across the network in language <ref type="bibr" target="#b13">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b6">Fedus et al., 2022)</ref>, vision <ref type="bibr" target="#b19">(Riquelme et al., 2021)</ref>, and multimodal models <ref type="bibr" target="#b17">(Mustafa et al., 2022)</ref>.</p><p>At the core of sparse MoE Transformers lies a discrete optimization problem: deciding which modules should be applied to each input token. These modules are commonly referred to as experts and are usually MLPs. Many techniques have been devised to find good token-to-expert matches: linear programs <ref type="bibr" target="#b14">(Lewis et al., 2021)</ref>, reinforcement learning <ref type="bibr" target="#b0">(Bengio et al., 2015)</ref>, deterministic fixed rules <ref type="bibr" target="#b20">(Roller et al., 2021)</ref>, optimal transport <ref type="bibr" target="#b15">(Liu et al., 2022)</ref>, greedy top-k experts per token <ref type="bibr" target="#b22">(Shazeer et al., 2017)</ref>, or greedy top-k tokens per expert <ref type="bibr" target="#b30">(Zhou et al., 2022)</ref>. In many cases, heuristic auxiliary losses are required to balance utilization of experts and minimize unassigned tokens. These challenges can be exacerbated in out-of-distribution scenarios: small inference batch sizes, novel inputs, or in transfer learning.</p><p>We introduce a new approach, Soft MoE, that overcomes many of these challenges. Rather than employing a sparse and discrete router that tries to find a good hard assignment between tokens and experts, Soft MoEs instead perform a soft assignment by mixing tokens. In particular, we compute several weighted averages of all tokens-with weights depending on both tokens and experts-and then we process each weighted average by its corresponding expert.</p><p>Soft MoE models avoid most of the challenges mentioned above which are caused by the discrete procedure at the core of sparse MoEs. Popular sparse MoE algorithms learn some router parameters, and the source of gradients is usually two-fold: post-multiplication of expert outputs with the selected routing scores, and auxiliary losses that enforce some desired behaviour and also depend on the routing scores. It has been observed that these mechanisms are often no better than random fixed routing <ref type="bibr" target="#b20">(Roller et al., 2021)</ref>. Soft MoE sidesteps this issue as every routing (or mixing) parameter is directly updated based on every single input token. Soft routing can provide stability while training a router; <ref type="bibr" target="#b17">(Mustafa et al., 2022)</ref> observed that during training large fractions of input tokens can simultaneously change discrete routes through the network, leading to training challenges. Further, hard routing can be challenging with many experts, with most works training with just a few dozen. We show that Soft MoE scales to thousands of experts, and it is balanced by construction. Finally, there are no batch-effects at inference, where one input can affect routing (due to limited expert capacity), and hence prediction, for other inputs.</p><p>Soft MoE L/16 beats ViT H/14 on upstream, fewshot and finetuning while requiring almost half the training time, and being 2? faster at inference. Moreover, Soft MoE B/16 matches ViT H/14 on fewshot and finetuning and outperforms it on upstream metrics after a comparable amount of training. Remarkably, Soft MoE B/16 is 5.7? faster at inference despite having 5.5? the number of parameters of ViT H/14. Section 4 demonstrates Soft MoE's potential to extend to other tasks: we train a contrastive model text tower against the frozen vision tower, showing that representations learned via soft routing preserve their benefits for image-text alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Soft Mixture of Experts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Algorithm description</head><p>The Soft MoE routing algorithm is depicted in Figure <ref type="figure" target="#fig_1">2</ref>. We denote the inputs tokens for one sequence by X ? R m?d , where m is the number of tokens and d is their dimension. Each MoE layer uses a set of n expert functions<ref type="foot" target="#foot_0">1</ref> applied on individual tokens, namely {f i : R d ? R d } 1:n . Each expert will process p slots, and each slot has a corresponding d-dimensional vector of parameters. We denote these parameters by ? ? R d? (n?p) .</p><p>In particular, the input slots X ? R (n?p)?d are the result of convex combinations of all the m input tokens, X:</p><formula xml:id="formula_0">D ij = exp((X?) ij ) m i ? =1 exp((X?) i ? j ) X = D ? X.</formula><p>(1)</p><p>Notice that D, which we call the dispatch weights, is simply the result of applying a softmax over the columns of X?. Then, as mentioned above, the corresponding expert function is applied on each slot (i.e. on rows of Soft MoE first computes scores or logits for every pair of input token and slot, based on some learnable per-slot parameters. These logits are then normalized per slot (columns) and every slot computes a linear combination of all the input tokens based on these weights (in green). Each expert (an MLP in this work) then processes its slots (e.g. 2 slots per expert, in this diagram).</p><p>Finally, the same original logits are normalized per token (i.e. by row) and used to combine all the slot outputs, for every input token (in blue). Dashed boxes represent learnable parameters.</p><p>X) to obtain the output slots: ?i = f ?i/p? ( Xi ).</p><p>(2)</p><p>Finally, the output tokens Y are computed as a convex combination of all (n ? p) output slots, ?, whose weights are computed similarly as before:</p><formula xml:id="formula_1">C ij = exp((X?) ij ) n?p j ? =1 exp((X?) ij ? ) Y = C ?.</formula><p>(3)</p><p>We refer to C as the combine weights, and it is the result of applying a softmax over the rows of X?.</p><p>Following the usual design for Sparse MoEs <ref type="bibr" target="#b13">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b6">Fedus et al., 2022;</ref><ref type="bibr" target="#b19">Riquelme et al., 2021;</ref><ref type="bibr" target="#b30">Zhou et al., 2022)</ref>, we replace a subset of the Transformer's MLP blocks with Soft MoE blocks. In particular, we typically replace the second half of MLP blocks. The total number of slots is a key hyperparameter of Soft MoE layers because the time complexity depends on the number of slots rather than on the number of experts. For example, one can set the number of slots equal to the input sequence length to match the FLOPs of the equivalent dense Transformer. Algorithm 1: Simple JAX <ref type="bibr" target="#b1">(Bradbury et al., 2018)</ref> implementation of a Soft MoE layer. Full code is available at https://github.com/google-research/vmoe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Properties of Soft MoEs and connections with Sparse MoEs</head><p>Fully differentiable At the core of all Sparse MoE algorithms there is an assignment problem between tokens and experts, which is usually subject to some specific capacity and balance constraints. Different algorithms relax the problem or approximate the solution in different ways: the Top-k or "Token Choice" router <ref type="bibr" target="#b22">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b13">Lepikhin et al., 2020;</ref><ref type="bibr" target="#b19">Riquelme et al., 2021)</ref>, for instance, selects the top-kscored experts for each token, while there are slots available in such expert (i.e. the expert has not filled its capacity). The "Expert Choice" router <ref type="bibr" target="#b30">(Zhou et al., 2022)</ref> selects the top-capacity-scored tokens for each expert. Other works suggest more advanced (and often costly) algorithms to compute the assignments, such as approaches based on Linear Programming algorithms <ref type="bibr" target="#b14">(Lewis et al., 2021</ref><ref type="bibr">), Optimal Transport (Liu et al., 2022;</ref><ref type="bibr" target="#b3">Clark et al., 2022)</ref> or Reinforcement Learning <ref type="bibr" target="#b3">(Clark et al., 2022)</ref>. Nevertheless virtually all of these approaches are discrete in nature, and thus non-differentiable. In contrast, all operations in Soft MoE layers are continuous and fully differentiable. Indeed, we can interpret the weighted averages with softmax scores as soft assignments -which motivates our algorithm's name-rather than the hard assignments that Sparse MoE methods typically use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No token dropping and expert unbalance</head><p>The classical routing mechanisms mentioned above tend to suffer from issues such as "token dropping" (i.e. some tokens are not assigned to any expert), or "expert unbalance" (i.e. some experts receive far more tokens than others). Unfortunately, performance can be severely impacted as a consequence. For instance, the popular Top-k or "Token Choice" router <ref type="bibr" target="#b22">(Shazeer et al., 2017)</ref> suffers from both, while the "Expert Choice" router <ref type="bibr" target="#b30">(Zhou et al., 2022)</ref> only suffers from the former (see Appendix B for some experiments regarding dropping in both cases). Soft MoEs are basically immune to token dropping and expert unbalance since every slot is filled with a weighted average of all tokens. All weights are (in theory) strictly positive thanks to the softmax (see Section 5 for detailed experiments).</p><p>Fast The total number of slots is the main hyperparameter that determines the cost of a Soft MoE layer.</p><p>Every input applies such number of MLPs. The total number of experts is irrelevant in this calculation: few experts with many slots per expert or many experts with few slots per expert will have matching costs if the total number of slots is identical. The only constraint we must meet is that the number of slots has to be greater or equal to the number of experts (as each expert must process at least one slot). The main advantage of Soft MoE is completely avoiding sort or top-k operations which are slow and typically not well suited for hardware accelerators. As a result, Soft MoE is significantly faster than most sparse MoEs (Figure <ref type="figure">6</ref>). See Section 2.3 for time complexity details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features of both sparse and dense</head><p>The sparsity in Sparse MoEs comes from the fact that expert parameters are only applied to a subset of the input tokens. However, Soft MoEs are not technically sparse, since every slot is a weighted average of all the input tokens. Every input token fractionally activates all the model parameters. Likewise, all output tokens are fractionally dependent on all slots (and experts). Finally, notice also that Soft MoEs are not Dense MoEs, where every expert processes all input tokens, since every expert only processes a subset of the slots.</p><p>Per-sequence determinism Under capacity constraints, all Sparse MoE approaches route tokens in groups of a fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different sequences or inputs, these tokens often compete against each other for available spots in expert buffers. As a consequence, the model is no longer deterministic at the sequence-level, but only at the batch-level, as some input sequences may affect the final prediction for other inputs. Models using larger groups tend to provide more freedom to the routing algorithm and usually perform better, while their computational cost is also higher. On the other hand, when groups contain tokens from a single sequence, the model is forced to use every expert on every input sequence. This may lead to more generalist experts. Moreover, changing the group size between training and inference can be problematic due to the potential distributional shift in token-to-expert assignments. We explore these aspects in Section 3.5.</p><p>Soft MoE gracefully sidesteps all these challenges. Since it combines all tokens in each input sequence, we just set the group size to be a single sequence. Every expert does handle tokens from every input, maybe somewhat limiting the amount of high-level specialization. Yet, this also implies that it is per-example deterministic and fast, while typical instances of Sparse MoEs are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation</head><p>Time complexity Assume the per-token cost of a single expert function is O(k). The time complexity of a Soft MoE layer is then O(mnpd + npk). By choosing p = O(m/n) slots per expert, i.e. the number of tokens over the number of experts, the cost reduces to O(m 2 d + mk). Given that each expert function has its own set of parameters, increasing the number of experts n and scaling p accordingly, allows us to increase the total number of parameters without any impact on the time complexity. Moreover, when the cost of applying an expert is large, the mk term dominates over m 2 d, and the overall cost of a Soft MoE layer becomes comparable to that of applying a single expert on all the input tokens. Finally, even when m 2 d is not dominated, this is the same as the (single-headed) self-attention cost, thus it does not become a bottleneck in Transformer models.</p><p>Normalization In Transformers, MoE layers are typically used to replace the feedforward layer in each encoder block. Thus, when using pre-normalization as most modern Transformer architectures <ref type="bibr" target="#b5">(Domhan, 2018;</ref><ref type="bibr" target="#b26">Xiong et al., 2020;</ref><ref type="bibr" target="#b19">Riquelme et al., 2021;</ref><ref type="bibr" target="#b6">Fedus et al., 2022)</ref>, the inputs to the MoE layer are "layer normalized". This causes stability issues when scaling the model dimension d, since the softmax approaches a one-hot vector as d ? ? (see Appendix E). Thus, in Line 3 of algorithm 1 we replace X and Phi with l2_normalize(X, axis=1) and scale * l2_normalize(Phi, axis=0), respectively; where scale is a trainable scalar, and l2_normalize normalizes the corresponding axis to have unit (L2) norm, as Algorithm 2 shows. Distributed model When the number of experts increases significantly, it is not possible to fit the entire model in memory on a single device, especially during training or when using MoEs on top of large model backbones. In these cases, we employ the standard techniques to distribute the model across many devices, as in <ref type="bibr" target="#b13">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b19">Riquelme et al., 2021;</ref><ref type="bibr" target="#b6">Fedus et al., 2022)</ref> and other works training large MoE models. Distributing the model typically adds an overhead in the cost of the model, which is not captured by the time complexity analysis based on FLOPs that we derived above. In order to account for this difference, in all of our experiments we measure not only the FLOPs, but also the wall-clock time in TPUv3-chip-hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Connections with other methods</head><p>Many existing works merge, mix or fuse input tokens to reduce the input sequence length <ref type="bibr" target="#b10">(Jaegle et al., 2021;</ref><ref type="bibr" target="#b21">Ryoo et al., 2021;</ref><ref type="bibr" target="#b18">Renggli et al., 2022;</ref><ref type="bibr" target="#b25">Wang et al., 2022)</ref>, typically using attention-like weighted averages with fixed keys, to try to alleviate the quadratic cost of self-attention with respect to the sequence length.</p><p>Although our dispatch and combine weights are computed in a similar fashion to these approaches, our goal is not to reduce the sequence length (while it is possible), and we actually recover the original sequence length after weighting the experts' outputs with the combine weights, at the end of each Soft MoE layer.</p><p>Multi-headed attention also shows some similarities with Soft MoE, beyond the use of softmax in weighted averages: the h different heads can be interpreted as different (linear) experts. The distinction is that, if m is the sequence length and each input token has dimensionality d, each of the h heads processes m vectors of size d/h. The m resulting vectors are combined using different weights for each of the m ? output tokens (i.e. the attention weights), on each head independently, and then the resulting (d/h)-dimensional vectors from each head are concatenated into one of dimension d. Our experts are non-linear and combine vectors of size d, at the input and output of such experts.</p><p>Finally, there are also connections with other MoE works that use a weighted combination of the experts parameters, rather than doing a sparse routing of the examples <ref type="bibr" target="#b27">(Yang et al., 2019;</ref><ref type="bibr" target="#b24">Tian et al., 2020;</ref><ref type="bibr" target="#b16">Muqeeth et al., 2023)</ref>. These approaches are also fully differentiable, although they can have a much higher cost, since 1) they must average the parameters of the experts, which can become a time and/or memory bottleneck when experts with many parameters are used; and 2) they cannot take advantage of vectorized operations as broadly as Soft (and Sparse) MoEs, since every input uses a different weighted combination of the parameters. We recommend the "computational cost" discussion in <ref type="bibr" target="#b16">(Muqeeth et al., 2023)</ref> that addresses these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Current limitations</head><p>Auto-regressive decoding One of the key aspects of Soft MoE consists in smartly merging all tokens in the input. This makes the use of Soft MoEs in auto-regressive decoders difficult, since causality between past and future tokens has to be preserved during training. Although causal masks used in attention layers could be used, one must be careful to not introduce any correlation between token and slot indices, since this would bias which token indices each expert is trained on. The use of Soft MoE in auto-regressive decoders is a promising research avenue that we leave for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lazy experts &amp; memory consumption</head><p>We extensively show in Section 3 that one slot per expert tends to be the optimal choice. In other words, rather than feeding one expert with two slots (or mixes of tokens), it is more effective from a performance standpoint to use two experts with one slot each. We hypothesize same-expert slots tend to somewhat align and provide small informational gains, and single experts may lack the flexibility to accommodate very different slot projections. We show this in Appendix H. Consequently, Soft MoE tends to leverage a large number of experts and -while its cost is still similar to the dense backbonethe memory requirements of the model can grow large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Classification Experiments</head><p>We Inference-time optimized models. Second, in Section 3.4, we present longer training runs ("overtraining").</p><p>Relative to ViT, Soft MoE brings large improvements in terms of inference speed (small models: S, B) and absolute performance (large models: L, H).</p><p>Model ablations. Third, in Section 3.5 we investigate some of the central aspects of Soft MoE routing (such as number of experts, slots per expert, etc), and compare their behavior with other routing algorithms. We present the optimal configurations for Soft MoE and the source of the improvement benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and evaluation data</head><p>We pretrain our models on JFT-4B <ref type="bibr" target="#b23">(Sun et al., 2017)</ref>, a proprietary dataset whose latest version contains more than 4B images, covering more than 29k classes. During pretraining, we typically evaluate the models on two metrics: upstream validation precision-at-1 on JFT-4B, and ImageNet 10-shot accuracy. The latter is computed by freezing the model weights and replacing the head with a new one that is only trained on a dataset containing 10 images per class from ImageNet-1k <ref type="bibr" target="#b4">(Deng et al., 2009)</ref>. Finally, we provide the accuracy on the validation set of ImageNet-1k after finetuning on the training set of ImageNet-1k (1.3 million images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse routing algorithms</head><p>We compare to the following popular MoE routing algorithms:</p><p>Tokens Choice. Every token selects the top-K experts with the highest routing score for the token <ref type="bibr" target="#b22">(Shazeer et al., 2017)</ref>. Increasing K typically leads to better performance at the expense of extra computational cost. Batch Priority Routing (BPR) <ref type="bibr" target="#b19">(Riquelme et al., 2021)</ref> significantly improves the model performance, especially in the case of K = 1 (see Appendix, Table <ref type="table" target="#tab_9">8</ref>). Accordingly we use Top-K routing with BPR and K ? {1, 2}. We also optimize the number of experts (Appendix, Figure <ref type="figure" target="#fig_0">15</ref>).</p><p>Experts Choice. Alternatively, experts can select the top-C tokens in terms of routing scores <ref type="bibr" target="#b30">(Zhou et al., 2022)</ref>. In this case, C is the buffer size, and we typically set E ? C = c ? T where E is the number of experts, T is the total number of tokens in the group, and c is the capacity multiplier. When c = 1, all tokens can be served via the union of experts. Note that in this type of routing, it is common that some tokens are simultaneously selected by several experts whereas some other tokens are not selected at all. Figure <ref type="figure" target="#fig_6">14</ref> illustrates this phenomenon. We experiment with c = 0.5, c = 1 and c = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Pareto-optimal models</head><p>We train VIT-S/8, VIT-S/16, VIT-S/32, VIT-B/16, VIT-B/32, VIT-L/16, VIT-L/32 and VIT-H/14 models, and their sparse counterparts. We consider three routing algorithms for the sparse models: Token Choice, Expert Choice, and Soft MoE. In each case, we train several model variants (different K, C and number of experts where it corresponds). In total, we train 106 models. The models are trained for 300k steps with batch size 4096 in all cases, and inverse square root learning rate decay.</p><p>Figure <ref type="figure" target="#fig_5">3a</ref> and Figure <ref type="figure" target="#fig_5">3b</ref> show the results for models in each class that lie on their respective training cost / performance Pareto frontiers. On both metrics, Soft MoE strongly outperforms dense and other sparse approaches for any given FLOPs or time budget. Table <ref type="table">9</ref> in Appendix I list all the models, with their parameters, performance and costs, and are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Long training runs</head><p>In addition to shorter runs and ablations, we trained a number of models for much longer (a few million steps) to test the Soft MoE capabilities at larger computational scales. We present two experiments.</p><p>First, in Section 3.4.1, we trained ViT and Soft MoE of different sizes ranging from Small to Huge for 4M steps. Figure <ref type="figure" target="#fig_6">4</ref> and Table <ref type="table" target="#tab_6">2</ref> show the results. Second, in Section 3.4.2, we kept training some of the previous Soft MoE models beyond the optimal point suggested by standard dense scaling laws. Sparse models can leverage the extra capacity to steadily improve their performance, leading to very strong Soft MoE models that are notably cheap at inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Comparison with large-scale Vision Transformers</head><p>We trained a number of Soft MoEs on JFT, following a comparable setting to that used by <ref type="bibr">Zhai et al. (2022a)</ref>. We replace the last half of the blocks in ViT S/16, B/16, L/16, and H/14 with Soft MoE layers with 128 experts, using one slot per expert. We train models ranging from 1B to 54B parameters. Large Soft MoE models incur in a small wall-clock time overhead compared to their dense counterparts due to the extra data transfers required by model parallelism. All variants were trained for 4M steps, except for H/14s which was trained for 2M steps for cost reasons.  <ref type="table" target="#tab_6">2</ref> contains all the results, and Figure <ref type="figure" target="#fig_8">19</ref> shows performance versus core-hours. Soft MoE models widely outperform Vision Transformer models for a given compute budget. For instance, the Soft MoE S/16 performs better than ViT B/16 on JFT and 10-shot ImageNet, and it also improves finetuning scores on the full ImageNet data, even though its training (and inference) cost is significantly smaller. Similarly, Soft MoE B/16 outperforms ViT L/16 upstream, and only lags 0.5 behind after finetuning while being 3x faster and requiring almost 4x fewer FLOPs. Finally, the Soft MoE L/16 model outperforms the dense H/14 one while again being around 3x faster to train and serve at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Soft MoEs optimized for inference</head><p>Encouraged by the fact that Soft MoEs with smaller backbones can match the quality of larger Vision Transformers, we continue training the small backbones to obtain models of higher quality at very low inference cost. Even after additional (over) training, the overall training time with respect to larger ViT models is comparable. For these long runs, we observe that longer cooldowns (period where the learning rate is decreased linearly to zero <ref type="bibr">(Zhai et al., 2022a)</ref>) work well for Soft MoE. Therefore, we increase the cooldown from 50k steps (used elsewhere) to up to 500k steps. Figure <ref type="figure">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Soft MoE Ablations</head><p>Here we establish the optimal configurations for Soft MoE models by exploring the following:</p><p>Optimal number of slots per expert. One or two slots per expert work best. We demonstrate this by fixing the total number of slots (which determines the compute cost of the model), and changing the number of experts, i.e. the slots per expert (Figure <ref type="figure">6</ref>).</p><p>Optimal number of experts. Roughly the same number of experts as input tokens work best when using one slot per expert. The model is then similarly expensive in terms of FLOPs as its dense equivalent. To show this, we increase the number of experts and train models for the same amount of time, and find the best performing model (Figure <ref type="figure">8</ref>).</p><p>Architectural/algorithmic ablations. To disentangle the source of the benefits, we compare Soft MoE to a number of ablated versions: route token i deterministically to expert i, fixed uniform dispatch/combine weights, and others (TablTable 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoE layers placement.</head><p>An additional ablation regarding where to place MoE layers is presented in Appendix D.  <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_6">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Number of Experts and Slots per Expert</head><p>When applying Soft MoE to a given architecture and input sequence length, one must decide how many experts and how many slots per expert to use. The total number of slots determines the amount of work (FLOPs) applied in the MoE layer (ignoring the small the routing cost). If the total number of slots is greater than the number of input tokens, the model will require more FLOPs than dense Transformers: more "tokens" will be processed in the MoE layer. Conversely, if the number of slots is lower than the original number of tokens, Soft MoE will save some compute relative to dense Transformers.</p><p>Unless stated otherwise, the following experiments use a ViT-S/16 backbone trained for 300k steps with batch size 4096. The MoEs have expert layers in their last six of twelve blocks.</p><p>Optimal number of slots per expert. In this experiment the total amount of compute is fixed, and we compare different configurations. Specifically, we fix the total number of slots to 4096, and we train models with different number of experts. MoE algorithms are often unable to scale well to a large number of experts (over 100). The model sizes range from just 38M (with 2 experts) to 9.7B parameters (when using 4096 experts). Figure <ref type="figure">6</ref> (and Figure <ref type="figure" target="#fig_21">26</ref>) shows the results in terms of pre-training precision (left) and the few-shot evaluation (middle). In the case of Experts Choice and Tokens Choice MoEs, the size of the union of all expert buffers is also 4096 per input image. We just vary the number of experts keeping the total number of tokens processed across the union of experts constant, as for Soft MoE. For the Sparse MoEs (Experts/Tokens Choice), there is an implementation detail: The "group size" is the subset of the batch that is routed together. All tokens in a group compete to be selected by each expert. items). In Figure <ref type="figure">6</ref>, we use group size eight. Figure <ref type="figure" target="#fig_1">20</ref>, Appendix, shows other options.</p><p>Figure <ref type="figure">6</ref> shows that Soft MoE scales with increased experts. The best configurations are 2048 and 4096 experts, at one/two slots per experts, respectively. In contrast, Experts Choice and Tokens Choice do not scale well with the number of experts, and performance degrades after 512 experts. In addition, Figure <ref type="figure">6</ref>, right, shows the step time for each model. Due to sorting leading to increased computational overhead, the Sparse MoE's step time increases substantially with more experts, which is not the case for Soft MoE.</p><p>Optimal number of experts. From the previous analysis, we set the number of slots per expert to one. The next question is how many experts to use. Here, the cost of models are not matched: more experts will increase cost (through more slots). Figure <ref type="figure" target="#fig_7">7</ref> shows that, both for Soft MoE and Experts Choice, more experts do better (up to 1024).</p><p>Next, we match the total training time for each model by adjusting the number of training steps (Figure <ref type="figure">8</ref>). At this scale (ViT-S), the optimal number of experts for a given training budget is around 128 or 256 experts.</p><p>The number of input tokens is 196, this corresponds to the minimum number of experts that does not lead to a strong token bottleneck (many fewer than 196 slots) in the MoE layer. For any number of experts, Soft MoE outperforms Experts Choice. Both models have the same capacity, but Experts Choice is significantly more expensive, especially with large group size.</p><p>More slots per expert. Appendix C explores how Soft MoE behaves when increasing the number of slots per expert. Appendix H looks at the (strong) correlation between the learned slot parameters in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Algorithmic Ablations: Identity &amp; Uniform Routing</head><p>Soft MoE relies on learning how to mix tokens for each expert. To understand the impact of finding useful linear combinations of input tokens, we ablate this aspect by testing some natural choices:</p><p>Identity routing. Tokens are not mixed: the first token goes to first expert, second token goes to second expert, etc.</p><p>Uniform Mixing. Every slot mixes all input tokens in the same way: by uniformly averaging them, both for dispatching and combining. In this case, we must independently and randomly initialize every expert as  The performance of all models improves as their capacity increases. However, the cost of Experts Choice grows faster than that of Soft MoE, especially when the group size is larger (gs= 32).</p><formula xml:id="formula_2">2</formula><p>otherwise the additional capacity coming from different experts will not be used (we end up with copies).</p><p>Soft / Uniform. We learn to mix tokens to create the slots (dispatch weights), but we uniformly average all expert outputs. This implies every input token is identically updated before the residual connection.</p><p>Uniform / Soft. All slots are filled with the uniform average of the input tokens. We learn to mix the expert output tokens depending on the input tokens.</p><p>Table <ref type="table" target="#tab_7">3</ref> summarizes our results, and Appendix A contains further details. Learning to mix tokens for dispatching and for combining tokens after expert processing seems essential to perform well, and dispatch mixing is slightly more important than the combine mixing. Dense underperform all variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Contrastive learning experiments</head><p>We test whether the learned representations are also significantly better when used for other tasks. In this section we explore a popular paradigm, image-language contrastive learning. We follow the approach in <ref type="bibr">Zhai et al. (2022b)</ref> where the image tower is pre-trained on an image classification task, and then frozen while training the text encoder on a dataset of image-text pairs.</p><p>We re-use the models trained on JFT in the previous section and compare their performance on a number of downstream applications. For contrastive learning we train on WebLI <ref type="bibr" target="#b2">(Chen et al., 2022)</ref>, a proprietary dataset consisting of 10B images and their ALT texts crawled from the internet. The image encoder is frozen, while the text encoder is trained from scratch.</p><p>Table <ref type="table" target="#tab_8">4</ref> shows our results. Overall, the gaps we observed on image classification are preserved in this setting.</p><p>For instance, Soft MoE-L/16 outperforms ViT-L/16 by more than 1% and 2% on Imagenet and Cifar-100 zero-shot, respectively. Retrieval numbers are generally modest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Inspection</head><p>In this section, we take a look at various aspects of the routing the model learns. Tokens contributions to slots. While there is no dropping in Soft MoE, it is still possible that some tokens contribute little to all slots if their logits are much lower than those of other tokens. We would like to see if some tokens contribute to slots in a disproportionate manner. Figure <ref type="figure" target="#fig_8">9</ref> (left) shows the distribution across tokens for the total weight each token provides to slots (i.e. summed over all slots). This was computed over a batch with 256 images with 196 tokens each on a Soft MoE S/16 finetuned on ImageNet. We see there is a heavy tail of tokens that provide a stronger total contribution to slots, and the shape is somewhat similar across layers. Around 2-5% of the tokens provide a summed weight above 2. Also, between 15% and 20% of the tokens only contribute up to 0.25 in total weight. The last layer is slightly different, where token contribution is softer tailed. Appendix G further explores this.</p><p>Experts contributions to outputs. Similarly, we would like to understand how much different slots end up contributing to the output tokens. We focus on the case of one slot per expert. We can approximate the total contribution of each expert (equivalently, slot) by averaging their corresponding coefficients in the linear combinations for all output tokens in a batch. Figure <ref type="figure" target="#fig_8">9</ref> (center) shows such (normalized) importance across experts for different MoE layers. We see that, depending on the layer, some experts can impact output tokens between 3x and 14x more than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of input tokens per slot.</head><p>For each slot, Figure <ref type="figure" target="#fig_8">9</ref> (right) shows how many input tokens are required to achieve a certain cumulative weight in its linear combination. The distribution varies significantly across slots. For a few slots the top 20-25 tokens account for 90% of the slot weight, while for other slots the distribution is more uniform and many tokens contribute to fill in the slot. In general, we see that slots tend to mix a large number of tokens unlike in standard Sparse MoEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual inspection.</head><p>In order to provide some intuition regarding how slots average input tokens, Figure <ref type="figure" target="#fig_9">10</ref> graphically shows the linear combinations for 8 different slots for the image shown in Figure <ref type="figure" target="#fig_0">1</ref>. We shade patches inversely proportionally to their weight in the slots; note that all tokens representations are eventually combined into a single one (with hidden dimension h) before being passed to the expert (unlike in our plot, where they are arranged in the usual way). These plots correspond to a Soft MoE S/16 with 128 experts and one slot per expert, and we handpicked 8 out of the 128 slots to highlight how different slots tend to focus on different elements of the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Sparse models can face infrastructural challenges which may have slowed down their broad adoption. Since these models were originally conceived to unlock massive model sizes, they tend to be distributed and most routing algorithms require additional communication costs: additional activations, gradients, or expert parameters are sent across devices. This is also true for Soft MoEs, where the experts may also be distributed. However, modern dense models are now sufficiently large that they are also distributed, thus closing the gap in this axis. In addition, the benefits of sparsity shine at small model scales, both in prior work <ref type="bibr" target="#b19">(Riquelme et al., 2021)</ref> and with Soft MoE, fitting with the current needs of the industry for faster inference.</p><p>We presented Soft MoE, a new sparse Transformer architecture that avoids the discrete token-to-expert assignment problem that is common in sparse mixture of experts models. By merging input tokens into linear combinations before dispatching them to experts, we are able to train a fast and fully-differentiable model. We perform extensive image-classification and image-language contrastive learning experiments comparing the performance of dense models and several sparse methods (Tokens Choice, Experts Choice, Soft MoE). These experiments suggest Soft MoE is surprisingly effective and strongly outperforms the other approaches while often being computationally cheaper. How to deal with causal masking for language decoders is an exciting and impactful research direction for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Soft vs. Uniform vs. Identity dispatch and combine weights</head><p>In this section, we compare Soft MoE (i.e. the algorithm that uses the dispatch and combine weights computed by Soft MoE in eq. ( <ref type="formula">1</ref>) and eq. ( <ref type="formula">3</ref>)) with different "fixed routing" alternatives, where neither the expert selected nor the weight of the convex combinations depend on the content of the tokens.</p><p>We consider the following simple modifications of Soft MoE:</p><p>Identity. The first token in the sequence is processed by the first expert, the second token by the second expert, and so on in a round robin fashion. When the sequence length is the same as the number of slots and experts, this is equivalent to replacing the matrix D in eq. ( <ref type="formula">1</ref>) (resp. C in eq. ( <ref type="formula">3</ref>)) with an identity matrix.</p><p>Uniform. Every input slot is filled with a uniform average of all input tokens, and every output token is a uniform average of all output slots. This is equivalent to replacing the matrix D from eq. ( <ref type="formula">1</ref>) with values 1 m in all elements, and a matrix C from eq. ( <ref type="formula">3</ref>) with values 1 np in all elements. We randomly and independently initialize every expert.</p><p>Uniform / Soft. Every input slot is filled with a uniform average of all input tokens, but we keep the definition of C from eq. ( <ref type="formula">3</ref>). Soft / Uniform. Every output token is a uniform average of all output slots, but we keep the definition of D in eq. ( <ref type="formula">1</ref>).</p><p>Figure <ref type="figure" target="#fig_0">11</ref> and Table <ref type="table" target="#tab_7">3</ref> shows the results from this experiment, training a S/14 backbone model with MoEs on the last 6 layers. Since the sequence length is 256, we choose 256 experts and slots (i.e. 1 slot per expert), so that the matrices D and C are squared. As shown in the figure, Soft MoE is far better than all the other alternatives. For context, we also add the dense ViT S/14 to the comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Token Dropping</head><p>In this appendix, we briefly explore token dropping for the Experts Choose and Tokens Choose algorithms. For Tokens Choose, each token selects K experts. When experts are full, some tokens assigned to that expert will not be processed. A token is "dropped" when none of its choices go through, and no expert at all processes the token. Expert Choose algorithms lead to an uneven amount of processing per token: some input tokens are selected by many experts, while some others are not selected by any. We usually define the number of tokens to be processed by each expert in a way that the combined capacity of all experts corresponds to the number of input tokens (or a multiple C of them). If we use a multiplier C higher than one (say, 2x or 3x), the amount of dropping will decrease but we will pay an increased computational cost. Thus, we mainly explore the K = 1 and C = 1 setup, where there is no slack in the buffers.</p><p>In all cases to follow we see a common trend: fixing everything constant, increasing the number of experts leads to more and more dropping both in Experts Choose and Tokens Choose.</p><p>Figure <ref type="figure" target="#fig_1">12</ref> compares Experts Choose and Tokens Choose with the same multiplier C = 1. This is the cheapest setup where every token could be assigned to an expert with balanced routing. We see that in both cases the amount of dropping quickly grows with the number of experts. Moreover, even though Experts Choose has higher levels of dropping (especially for large number of experts), it is still more performant than Tokens Choose. Note there is a fundamental difference: when Tokens Choose drops a token, the model wastes that amount of potential compute. On the other hand, for Experts Choose dropping just means some other token got that spot in the expert buffer, thus the model just transferred compute from one unlucky token to another lucky one.</p><p>In this setup, for a small number of experts (16-32) it is common to observe a ? 15% rate of dropping. On the other hand, we also experimented with a large number of experts (100-1000) where each expert selects very few tokens. In this case, the dropping rate for Experts Choose can grow above 40-50% in some layers: most experts select the very same tokens. Tokens Choose seems to completely drop up to ?25% of the tokens.</p><p>In Figures <ref type="figure" target="#fig_5">13</ref> and<ref type="figure" target="#fig_6">14</ref> we study how much a little bit of buffer slack (C = 1.125) can help in terms of performance and dropping to Experts Choose and Tokens Choose, respectively. Both plots are similar: the amount of dropping goes down around ?5% and performance slightly increases when the number of experts is large. Note that the step time also increases in these cases.</p><p>Finally, Figure <ref type="figure" target="#fig_0">15</ref> shows the effect of Batch Priority Routing <ref type="bibr" target="#b19">(Riquelme et al., 2021)</ref> for Tokens Choose. By smartly selecting which tokens to drop we do not only uniformly reduce the amount of dropping, but we significantly bump up performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Soft MoE Increasing Slots</head><p>In this section we explore the following question: for a fixed number of experts, how much does Soft MoE routing benefit from having additional slots per expert? Figure <ref type="figure" target="#fig_12">16</ref> shows results for Soft MoE S/16 with 32 experts. We also show Experts Choice with group sizes of one and eight images. When increasing the number of slots, the performance grows only modestly, while cost increases quickly. Experts Choice benefits much more from increased slots, catching up at a large group size, but at a very large cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E The collapse of softmax layers applied after layer normalization E.1 Theoretical analysis</head><p>A softmax layer with parameters ? ? R n?d transforms a vector x ? R d into the vector softmax(?x) ? R n , with elements:</p><formula xml:id="formula_3">softmax(?x) i = exp((?x) i ) n j=1 exp((?x) j ) = exp( d k=1 ? ik x k ) n j=1 exp( d k=1 ? jk x k )<label>(4)</label></formula><p>Layer normalization applies the following operation on x ? R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LN(x)</head><formula xml:id="formula_4">i = ? i x i -?(x) ?(x) + ? i ; where ?(x) = 1 d d i=1 x i and ?(x) = 1 d d i=1 (x i -?(x i )) 2<label>(5)</label></formula><p>Notice that LN(x) = LN(x -?(x)), thus we can rewrite LayerNorm with respect to the centered vector x = x -?(x), and the centered vector scaled to have unit norm xi = xi ?x? :</p><formula xml:id="formula_5">LN(x) i = ? i xi 1 d d j=1 x2 j + ? i = ? d? i xi ?x? + ? i = ? d? i xi + ? i (6)</formula><p>When a softmax layer is applied to the outputs of layer normalization, the outputs of the softmax are given by the equation: </p><formula xml:id="formula_6">softmax(?LN(x)) i = exp( d k=1 ? ik ( ? d? k xk + ? k )) n j=1 exp( d k=1 ? jk ( ? d? k xk + ? k ))<label>(7)</label></formula><formula xml:id="formula_7">? d? j + ? j ) (8) Define m = max i?[n] ? d? i -? i , M = {i ? [n] : ? d? i -? i = m}.</formula><p>Then, the following equality holds:</p><formula xml:id="formula_8">softmax(?LN(x)) i = exp( ? d? i + ? i -m) n j=1 exp( ? d? j + ? j -m)<label>(9)</label></formula><p>Given that lim d?? exp(</p><formula xml:id="formula_9">? d? i + ? i -m) = 1 : i ? M 0 : i / ? M</formula><p>the output of the softmax tends to:</p><formula xml:id="formula_10">lim d?? softmax(?LN(x)) i = 1 |M | i ? M 0 i / ? M<label>(10)</label></formula><p>In particular, when the maximum is only achieved by one of the components (i.e. |M | = 1), the softmax collapses to a one-hot vector (a vector with all elements equal to 0 except for one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Empirical analysis</head><p>The previous theoretical analysis assumes that the parameters of the softmax layer are constants, or more specifically that they do not depend on d. One might argue that using modern parameter initialization techniques, which take into account 1 ? d in the standard deviation of the initialization <ref type="bibr" target="#b7">Glorot and Bengio (2010)</ref>; <ref type="bibr" target="#b8">He et al. (2015)</ref>; <ref type="bibr" target="#b12">Klambauer et al. (2017)</ref>, might fix this issue. We found that they don't (in particular, we use the initialization from <ref type="bibr" target="#b7">Glorot and Bengio (2010)</ref>).  <ref type="formula">1664</ref>). The rest of the architecture parameters are fixed: 6 layers (3 dense layers followed by 3 MoE layers with 256 experts), 14x14 patches, and a MLP dimension of 1536. As the model dimension d increases, the figure shows that, if the inputs to the softmax in the SoftMoE layers are not normalized, the average maximum values of the dispatch and combine weights tend to grow (especially the former). When d is big enough, the ImageNet 10shot accuracy is significantly worse than that achieved by properly normalizing the inputs.</p><p>In the previous experiment, we trained our model with a linear decay schedule and a peak value of 10 -3 . In addition, we also found that applying the softmax layer directly on the output of layer normalization is also very sensible to the learning rate's configuration. Once again, our recipe suggested in Section 2.3 gives equal or better quality, and is generally more stable. Figure <ref type="figure" target="#fig_0">18</ref> shows different metric curves during the training of the same small SoftMoE model as before, with a model dimension of d = 1664, using an inverse square root learning rate schedule, with a fixed timescale of 10 5 , a linear warmup phase of 10 5 steps, and a linear cooldown of 5 ? 10 5 steps, varying the peak learning rate value. In this figure, similarly to the results from the previous experiment, the average maximum values of the dispatch and combine weights grows to values approaching 1.0 (indicating a collapse in the softmax layers to a one-hot vector), when the inputs to the softmax in the SoftMoE layers are not normalized, which eventually severely hurts the accuracy of the model. However, using the normalization in Section 2.3 gives better accuracy and makes the model less sensible to the choice of the peak value of the learning rate.        Step time when increasing the total number of experts while keeping the total amount of slots fixed (4096). Soft MoE achieves consistently better results with more experts, whereas cost is kept roughly constant (same FLOPs but communication costs vary due to higher topologies needed for larger models). The normalized train step time is computed with respect to Soft MoE with 32 experts. Model sizes range from 38M (2 experts) to 9.7B parameters (4096 experts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional analysis G.1 Cumulative sum of dispatch and combine weights</head><p>Figure <ref type="figure" target="#fig_22">27</ref> shows the distribution over slots of the cumulative sum (over tokens) of their corresponding dispatch weights. For each slot we compute the cumulative sum of the dispatch weights over tokens sorted in decreasing order. This indicates how many tokens are necessary to cover a given percentage of the total mass of the weighted average. We compute this cumulative sum for all slots over all the 50 000 ImageNet validation images, across all layers of the Soft MoE H/16 model after finetuning. In the plot, we represent with a solid line the average (over all slots and images) cumulative sum, and the different colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter colors) of cumulative sums. This tells us, for instance, how uniform is the weighted average over tokens used to compute each input slot. In particular, each slot in the last two layers is close to a uniform average of all the tokens (a completely uniform average would be represented by a straight line). This tells us that in these layers, every expert processes roughly the same inputs, at least after the model is trained. However, this weighted average is far from uniform in the rest of the layers, meaning that there are tokens that contribute far more than others. For example, in layer 28, a few tens of tokens already cover 80% of the weighted average mass. Finally, given the width of the colored areas, we can also see that there's a significant difference on the weighted averages depending on the slot, across all layers (except maybe the last two). This indicates that the dispatch weights vary across different slots and images.</p><p>Similarly, Figure <ref type="figure" target="#fig_23">28</ref> shows the corresponding plots for the cumulative sum of the combine weights. In this case, for each output token we compute the cumulative sum of the combine weights over slots sorted in decreasing order. Notice that, although the dispatch weights in the last two layers were almost uniform, the combine weights are not. This indicates that some slots (and thus, experts) are more important than others in computing the output tokens, and thus their corresponding expert parameters are not redundant. Of course, the identity of the "important" slots may vary depending on the input token.  For each input slot, we compute the cumulative sum of its corresponding dispatch weights (sorted by decreasing value). This indicates over how many input tokens a certain cumulative weight is distributed over. The line in each plot represents the average computed over all slots and ImageNet validation images of the given block in the SoftMoE H/14 model. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter, better seen in color).  For each output token, we compute the cumulative sum of its corresponding combine weights (sorted by decreasing value). This indicates over how many output slots a certain cumulative weight is distributed over. The line in each plot represents the average computed over all tokens and ImageNet validation images of the given block in the SoftMoE H/14 model. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter, better seen in color).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Slot Correlation</head><p>In this section we explore the correlation between the different slot parameters that Soft MoE learns, and its relationship with the number of slots per expert. Figures 29 to 31 show for each of 6 layers in a Soft MoE S/16 the inner product between each pair of (normalized) slot parameter vectors.</p><p>While Figure <ref type="figure" target="#fig_24">29</ref> shows no clear relationship between slots from different experts (as each expert only has one slot), we observe in Figures 30 and 31 how consecutive slots (corresponding to the same expert) are extremely aligned. This confirms our hypothesis that adding more slots to experts does not work very well as these slots end up aligning their value, and computing somewhat similar linear combinations. Therefore, these projections do not add too much useful information to the different tokens to be processed by the experts (in the extreme, these slots would be identical).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Main differences between Sparse and Soft MoE layers. While the router in Sparse MoE layers (left) learns to assign individual input tokens to each of the available slots, in Soft MoE layers (right) each slot is the result of a (different) weighted average of all the input tokens. Learning to make discrete assignments introduces several optimization and implementation issues that Soft MoE sidesteps.</figDesc><graphic url="image-10.png" coords="2,316.48,109.72,78.23,78.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The Soft MoE routing algorithm. Soft MoE first computes scores or logits for every pair of input token and slot, based on some learnable per-slot parameters. These logits are then normalized per slot (columns) and every slot computes a linear combination of all the input tokens based on these weights (in green). Each expert (an MLP in this work) then processes its slots (e.g. 2 slots per expert, in this diagram). Finally, the same original logits are normalized per token (i.e. by row) and used to combine all the slot outputs, for every input token (in blue). Dashed boxes represent learnable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>def soft_m oe_lay er (X , Phi , experts ) : # Compute the dispatch and combine weights . logits = jnp . einsum ( 'md , dnp -&gt; mnp ' , X , Phi ) D = jax . nn . softmax ( logits , axis =(0 ,) ) C = jax . nn . softmax ( logits , axis =(1 , 2) ) # The input slots are a weighted average of all the input tokens , # given by the dispatch weights . Xs = jnp . einsum ( 'md , mnp -&gt; npd ' , X , D ) # Apply the corresponding expert function to each input slot . Ys = jnp . stack ([ f_i ( Xs [i , : , :]) for i , f_i in enumerate ( experts ) ] , axis =0) # The output tokens are a weighted average of all the output slots , # given by the combine weights . Y = jnp . einsum ( 'npd , mnp -&gt; md ' , Ys , C ) return Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>present three types of experiments on image classification: Training Pareto frontiers. First, in Section 3.3 we systematically compare dense ViT models at the Small, Base, Large and Huge sizes with their sparse counterparts based on the most common routing techniques (Tokens Choice, Experts Choice) and Soft MoE routing. We study the training FLOPs versus performance and training time versus performance plots to conclude that Soft MoE dominates all other approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 22 .</head><label>22</label><figDesc>Figure3aand Figure3bshow the results for models in each class that lie on their respective training cost / performance Pareto frontiers. On both metrics, Soft MoE strongly outperforms dense and other sparse approaches for any given FLOPs or time budget. Table9in Appendix I list all the models, with their parameters, performance and costs, and are shown in Figure 22. Figures 23 to 25 in Appendix F compare Soft MoE individually to Dense, Token Choice and Expert Choice models respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pareto Models. Soft MoE dominates both ViTs (dense) and popular MoEs (Experts Choice,Tokens Choice) on the training cost / performance Pareto frontier. Each point is a model trained for 300k steps, and larger marker sizes indicate larger models: S/32, S/16, B/32, B/16, L/16 and H/14. Cost is shown both in terms of FLOPS and realized TPU-v3 training time. MoE runs include different configuration; for clarity, only models on their respective Pareto frontier are displayed. Figure 22 in Appendix F shows all models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>Figure4shows the JFT-4B precision, ImageNet 10-shot accuracy, and the ImageNet finetuning accuracy for Soft MoE and ViT versus training cost in ExaFLOPS. Table 2 contains all the results, and Figure19shows performance versus core-hours. Soft MoE models widely outperform Vision Transformer models for a given compute budget. For instance, the Soft MoE S/16 performs better than ViT B/16 on JFT and 10-shot ImageNet, and it also improves finetuning scores on the full ImageNet data, even though its training (and inference) cost is significantly smaller. Similarly, Soft MoE B/16 outperforms ViT L/16 upstream, and only lags 0.5 behind after finetuning while being 3x faster and requiring almost 4x fewer FLOPs. Finally, the Soft MoE L/16 model outperforms the dense H/14 one while again being around 3x faster to train and serve at inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance (left, center) and step time (right) for models trained with increased experts and one slot (or token) per expert for a fixed number of steps (300k).The performance of all models improves as their capacity increases. However, the cost of Experts Choice grows faster than that of Soft MoE, especially when the group size is larger (gs= 32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: (Left) Distribution of summed dispatch weights per token for different MoE layers. For instance, in layer 11, the dispatch weights for 90-95% of the input tokens summed over all the slots are at most 1. Only a tiny fraction of tokens contribute to slots by summing more than 3. (Middle) Distribution of combine weights per slot (or expert, as we use one slot per expert) summed across all input tokens. We normalize the sum by its minimum value across experts. (Right) Each curve corresponds to one slot. Dispatch weights from all tokens to each slot add up to 1. Distribution of how many inputs tokens are needed to achieve a certain fraction of the total weight for the slot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Linear combinations for 8 slots when using input image in Figure1. Model is Soft MoE S/16 with 128 experts and one slot per expert, and it was finetuned on ImageNet. We show results for the first MoE layer (seventh block). The selected slots (among 128) are cherry-picked to highlight differences across slots.</figDesc><graphic url="image-23.png" coords="16,74.99,382.74,106.02,106.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :Figure 12</head><label>1112</label><figDesc>Figure11: Soft MoE compared against different "fixed routing" strategies. Identity processes the i-th token with the i-th expert; Uniform replaces both the dispatch and combine matrices with uniform averages; Uniform / Soft replaces the dispatch weights with a uniform average, but the combine weights are computed as in Soft MoE; Soft / Uniform does the opposite replacement; and Soft uses the algorithm we present in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 Figure 14 Figure 15</head><label>131415</label><figDesc>Figure13: S/14. Performance and amount of token dropping for increasing experts for Tokens Choose with tight buffers (K = 1 and C = 1) and some amount of buffer slack (K = 1 and C = 1.125).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: Performance (left, center) and step time (right) of models with 32 experts, but increased slots, all trained for the same number of steps (300k). Increasing the number of slots per expert only increases performance of Soft MoE a small amount, while increasing cost substantially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>By setting ? i = d k=1 ? ik ? k xk , and ? i = d k=1 ? ik ? k , the previous equation can be rewritten as: softmax(?LN(x)) i = exp( ? d? i + ? i ) n j=1 exp(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17</head><label>17</label><figDesc>Figure 17 shows different metric curves during the training of a small SoftMoE model with different model dimensions. The model dimensions are those corresponding to different standard backbones: S (384), B (768), L (1024), H (1280) and G (1664). The rest of the architecture parameters are fixed: 6 layers (3 dense layers followed by 3 MoE layers with 256 experts), 14x14 patches, and a MLP dimension of 1536. As the model dimension d increases, the figure shows that, if the inputs to the softmax in the SoftMoE layers are not normalized, the average maximum values of the dispatch and combine weights tend to grow (especially the former). When d is big enough, the ImageNet 10shot accuracy is significantly worse than that achieved by properly normalizing the inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure 22: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k steps). The size of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different methods: Soft MoE (blue), Sparse MoEs with Experts Choice (orange) and Tokens Choice routing (green), and a Dense (red) model. MoE runs include different configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 23 :</head><label>23</label><figDesc>Figure23: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The size of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different methods: Soft MoE (blue) and Dense (red) models. MoE runs include different configurations. We only show the runs that are not dominated by another model using the same method (S/8 and L/32 were always dominated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Figure24: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The size of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different methods: Soft MoE (blue) and Sparse MoEs with Experts Choice (orange) models. MoE runs include different configurations. We only show the runs that are not dominated by another model using the same method (S/8 and L/32 were always dominated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 25 :</head><label>25</label><figDesc>Figure25: JFT-4B Precision-at-1 and ImageNet 10-shot accuracy on short runs (300k training steps). The size of the marker depends on the backbone size: S/32, S/16, B/32, B/16, L/16 and H/14. Colors represent different methods: Soft MoE (blue) and Sparse MoEs with Tokens Choice (green) models. MoE runs include different configurations. We only show the runs that are not dominated by another model using the same method (S/8 and L/32 were always dominated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: JFT Precision-at-1, ImageNet 10-shot Accuracy, and normalized TrainingStep time when increasing the total number of experts while keeping the total amount of slots fixed (4096). Soft MoE achieves consistently better results with more experts, whereas cost is kept roughly constant (same FLOPs but communication costs vary due to higher topologies needed for larger models). The normalized train step time is computed with respect to Soft MoE with 32 experts. Model sizes range from 38M (2 experts) to 9.7B parameters (4096 experts).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 27 :</head><label>27</label><figDesc>Figure27: Distribution of the cumulative sum of dispatch weights. For each input slot, we compute the cumulative sum of its corresponding dispatch weights (sorted by decreasing value). This indicates over how many input tokens a certain cumulative weight is distributed over. The line in each plot represents the average computed over all slots and ImageNet validation images of the given block in the SoftMoE H/14 model. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter, better seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 28 :</head><label>28</label><figDesc>Figure28: Distribution of the cumulative sum of combine weights. For each output token, we compute the cumulative sum of its corresponding combine weights (sorted by decreasing value). This indicates over how many output slots a certain cumulative weight is distributed over. The line in each plot represents the average computed over all tokens and ImageNet validation images of the given block in the SoftMoE H/14 model. The colored areas represent the central 60%, 80%, 90%, 95% and 99% of the distribution (from darker to lighter, better seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: Soft MoE S/16 with 1 slot per expert.</figDesc><graphic url="image-33.png" coords="37,88.53,408.08,109.66,109.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 30 :</head><label>30</label><figDesc>Figure 30: Soft MoE S/16 with 4 slots per expert.</figDesc><graphic url="image-45.png" coords="38,91.65,410.20,107.06,107.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>presents these models.We now summarize our main results. Soft MoE B/16 trained for 1k TPUv3 days outperforms ViT H/14 trained on a similar time budget (see Table 1, ViT H/14, 1M steps) while being 10? cheaper at inference in FLOPs and 5.7? in wall-clock time, and it almost matches the ViT H/14 model performance even if we double ViT-H/14's training budget (2M steps and 2039.8 train days for ViT H/14 versus 1011.4 days for Soft MoE B/16). Soft MoE L/16 beats all models substantially while being almost 2? faster at inference than ViT H/14. Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14 models trained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar training costs, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision at 1 (middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We report training wall-clock time in Figure 19. Training and finetuning results for Soft MoE and dense models. Finetuning performed on ImageNet at 384 resolution. Steps used for linear cooldown indicated in parentheses, these are included in the total train steps count. Results are plotted in Figure 5.</figDesc><table><row><cell>ImageNet 10-shot Accuracy</cell><cell>67% 73% 77% 81% 84%</cell><cell>S/16</cell><cell cols="2">10 3 Total Training ExaFLOPs B/16 L/16 H/14</cell><cell>JFT-4B Precision-at-1</cell><cell>51% 54% 57% 59%</cell><cell>S/16</cell><cell>10 3 Total Training ExaFLOPs B/16 L/16 H/14</cell><cell>ImageNet Finetune Accuracy</cell><cell>84% 85% 87% 88% 89%</cell><cell>S/16</cell><cell>Total Training ExaFLOPs 10 3 B/16 H/14 L/16 Soft MoE Dense</cell></row><row><cell cols="3">Figure 4: ViT S/16</cell><cell></cell><cell cols="2">33M 4M (50k)</cell><cell cols="2">153.5</cell><cell>227.1</cell><cell>0.5</cell><cell></cell><cell>9.2</cell><cell>51.3</cell><cell>67.6 84.0</cell></row><row><cell cols="3">ViT B/16</cell><cell></cell><cell cols="2">108M 4M (50k)</cell><cell cols="2">410.1</cell><cell>864.1</cell><cell>1.3</cell><cell></cell><cell>35.1</cell><cell>56.2</cell><cell>76.8 86.6</cell></row><row><cell cols="3">ViT L/16</cell><cell></cell><cell cols="2">333M 4M (50k)</cell><cell cols="2">1290.1</cell><cell>3025.4</cell><cell>4.9</cell><cell></cell><cell>122.9</cell><cell>59.8</cell><cell>81.5 88.5</cell></row><row><cell cols="3">ViT H/14</cell><cell></cell><cell cols="2">669M 2M (50k)</cell><cell cols="2">2039.8</cell><cell>4120.3</cell><cell>8.6</cell><cell></cell><cell>334.2</cell><cell>59.7</cell><cell>83.3 88.9</cell></row><row><cell cols="4">Soft MoE S/14 256E</cell><cell cols="2">1.8B 10M (50k)</cell><cell cols="2">494.7</cell><cell>814.2</cell><cell>0.9</cell><cell></cell><cell>13.2</cell><cell>60.1</cell><cell>80.6 87.5</cell></row><row><cell cols="4">Soft MoE B/16 128E</cell><cell cols="2">3.7B 9M (500k)</cell><cell cols="2">1011.4</cell><cell>1769.5</cell><cell>1.5</cell><cell></cell><cell>32.0</cell><cell>62.4</cell><cell>82.9 88.5</cell></row><row><cell cols="6">Soft MoE L/16 128E 13.1B 4M (500k)</cell><cell cols="2">1355.4</cell><cell>2734.1</cell><cell>4.8</cell><cell></cell><cell>111.1</cell><cell>63.0</cell><cell>84.3 89.2</cell></row></table><note><p><p>Model</p>Params Train steps Train days &amp; exaFLOP Eval Ms/img &amp; GFLOP/img JFT P@1 IN/10shot IN/ft</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>left, center), and training step time (right) as a function of number of experts, for models with a fixed number of slots (Soft MoE) or expert buffer capacity (Sparse MoEs) on</head><label></label><figDesc>a ViT-S/16 backbone with MoEs in the last two layers. Soft MoE achieves much better scaling with more experts, while cost is roughly constant. However, with Experts and Tokens Choice routers, having too many experts not only hurts performance but also significantly increases the cost (Tokens Choice reaches 3.9x with 4096 experts).</figDesc><table><row><cell></cell><cell>Soft MoE</cell><cell>Experts Choice</cell><cell cols="2">Tokens Choice</cell></row><row><cell>0.515 0.520 0.525 0.530 0.535 0.540 0.545 JFT-4B Precision-at-1</cell><cell>32 64 128 256 512 1024 2048 4096 Number of Experts ImageNet 10-shot Accuracy 0.70 0.71 0.72 0.73 0.74 0.75 128 64 32 16 8 4 2 1 Slots / Expert</cell><cell cols="2">32 64 128 256 512 1024 2048 4096 Number of Experts (normalized) Train Step Time 1.00 1.25 1.50 1.75 2.00 128 64 32 16 8 4 2 1 Slots / Expert</cell><cell>Number of Experts 32 64 128 256 512 1024 2048 4096 128 64 32 16 8 4 2 1 Slots / Expert</cell></row><row><cell cols="2">Figure 6: Performance (</cell><cell></cell><cell></cell></row></table><note><p>This can range from one image/group to the entire batch/group; the latter is more flexible, but increases computational overhead in routing (sorting the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Performance of models trained with increasing experts (one slot/token per expert), with matched training duration.</head><label></label><figDesc>The total number of steps in each case is computed to match the total training time of 300k steps for 1024-expert Experts Choice with 32 images per group. For context, the dashed line corresponds to Dense ViT-S/16. Here, Soft MoE outperforms Experts Choice at all capacities, and the optimum point is at around 512 experts.</figDesc><table><row><cell></cell><cell>Soft MoE</cell><cell>Experts Choice (gs=1 img)</cell><cell>Experts Choice (gs=8 img)</cell><cell>Dense</cell></row><row><cell>JFT-4B Precision-at-1</cell><cell>0.48 0.50 0.52 0.54 0.56</cell><cell>ImageNet 10-shot Accuracy</cell><cell>0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775</cell></row><row><cell></cell><cell cols="2">2 4 8 total number of experts 1 6 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4</cell><cell cols="2">2 4 8 1 6 3 2 6 4 1 2 8 2 5 6 5 1 2 1 0 2 4 total number of experts</cell></row><row><cell>Figure 8:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Training and finetuning results for Soft MoE and dense models. Finetuning results on ImageNet at 384 resolution. We use one slot per expert and did not increase this number during finetuning, thus Soft MoEs become cheaper than ViT, as the number of input tokens grows to 576 (patch size 16x16) and 752 (patch size 14x14) but the number slots is fixed to a much smaller number (either 128 or 256).</figDesc><table><row><cell>Model</cell><cell cols="7">Params Train steps Train days &amp; exaFLOP Eval Ms/img &amp; GFLOP/img JFT P@1 IN/10s IN/ft</cell></row><row><cell>ViT S/16</cell><cell>33M 4M (50k)</cell><cell>153.5</cell><cell>227.1</cell><cell>0.5</cell><cell>9.2</cell><cell>51.3</cell><cell>67.6 84.0</cell></row><row><cell>Soft MoE S/16 128E</cell><cell>933M 4M (50k)</cell><cell>175.1</cell><cell>211.9</cell><cell>0.7</cell><cell>8.6</cell><cell>58.1</cell><cell>78.8 86.8</cell></row><row><cell>Soft MoE S/16 128E</cell><cell>933M 10M (50k)</cell><cell>437.7</cell><cell>529.8</cell><cell>0.7</cell><cell>8.6</cell><cell>59.2</cell><cell>79.8 87.1</cell></row><row><cell>Soft MoE S/14 256E</cell><cell>1.8B 4M (50k)</cell><cell>197.9</cell><cell>325.7</cell><cell>0.9</cell><cell>13.2</cell><cell>58.9</cell><cell>80.0 87.2</cell></row><row><cell>Soft MoE S/14 256E</cell><cell>1.8B 10M (500k)</cell><cell>494.7</cell><cell>814.2</cell><cell>0.9</cell><cell>13.2</cell><cell>60.9</cell><cell>80.7 87.7</cell></row><row><cell>ViT B/16</cell><cell>108M 4M (50k)</cell><cell>410.1</cell><cell>864.1</cell><cell>1.3</cell><cell>35.1</cell><cell>56.2</cell><cell>76.8 86.6</cell></row><row><cell>Soft MoE B/16 128E</cell><cell>3.7B 4M (50k)</cell><cell>449.5</cell><cell>786.4</cell><cell>1.5</cell><cell>32.0</cell><cell>60.0</cell><cell>82.0 88.0</cell></row><row><cell>ViT L/16</cell><cell>333M 4M (50k)</cell><cell>1290.1</cell><cell>3025.4</cell><cell>4.9</cell><cell>122.9</cell><cell>59.8</cell><cell>81.5 88.5</cell></row><row><cell>Soft MoE L/16 128E</cell><cell>13.1B 1M (50k)</cell><cell>338.9</cell><cell>683.5</cell><cell>4.8</cell><cell>111.1</cell><cell>60.2</cell><cell>82.9 88.4</cell></row><row><cell>Soft MoE L/16 128E</cell><cell>13.1B 2M (50k)</cell><cell>677.7</cell><cell>1367.0</cell><cell>4.8</cell><cell>111.1</cell><cell>61.3</cell><cell>83.3 88.9</cell></row><row><cell>Soft MoE L/16 128E</cell><cell>13.1B 4M (50k)</cell><cell>1355.4</cell><cell>2734.1</cell><cell>4.8</cell><cell>111.1</cell><cell>61.3</cell><cell>83.7 88.9</cell></row><row><cell>ViT H/14</cell><cell>669M 1M (50k)</cell><cell>1019.9</cell><cell>2060.2</cell><cell>8.6</cell><cell>334.2</cell><cell>58.8</cell><cell>82.7 88.6</cell></row><row><cell>ViT H/14</cell><cell>669M 2M (50k)</cell><cell>2039.8</cell><cell>4120.3</cell><cell>8.6</cell><cell>334.2</cell><cell>59.7</cell><cell>83.3 88.9</cell></row><row><cell cols="2">Soft MoE H/14 128E 27.3B 1M (50k)</cell><cell>1112.7</cell><cell>1754.6</cell><cell>8.8</cell><cell>284.6</cell><cell>61.0</cell><cell>83.7 88.9</cell></row><row><cell cols="2">Soft MoE H/14 128E 27.3B 2M (50k)</cell><cell>2225.4</cell><cell>3509.2</cell><cell>8.8</cell><cell>284.6</cell><cell>61.7</cell><cell>84.2 89.1</cell></row><row><cell cols="2">Soft MoE H/14 256E 54.1B 1M (50k)</cell><cell>1276.9</cell><cell>2110.1</cell><cell>10.9</cell><cell>342.4</cell><cell>60.8</cell><cell>83.6 88.9</cell></row><row><cell cols="2">Soft MoE H/14 256E 54.1B 2M (50k)</cell><cell>2553.7</cell><cell>4220.3</cell><cell>10.9</cell><cell>342.4</cell><cell>62.1</cell><cell>84.3 89.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Algorithmic ablation on an S/14 backbone trained for 300k steps (with 256 experts).</figDesc><table><row><cell>Method</cell><cell cols="6">Experts Mixing Learned Dispatch Learned Combine JFT p@1 IN/10shot</cell></row><row><cell>Soft MoE</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>54.3%</cell><cell>74.8%</cell></row><row><cell>Soft / Uniform</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>53.6%</cell><cell>72.0%</cell></row><row><cell>Uniform / Soft</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>52.6%</cell><cell>71.8%</cell></row><row><cell>Uniform</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>51.8%</cell><cell>70.0%</cell></row><row><cell>Identity</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>51.5%</cell><cell>69.1%</cell></row><row><cell>Dense ViT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.3%</cell><cell>62.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>LIT-style evaluation with a ViT-g text tower trained for 18B input images (? 5 epochs).</figDesc><table><row><cell>Model</cell><cell cols="6">Experts IN/0shot Cifar100/0shot Pet/0shot Coco Img2Text Coco Text2Img</cell></row><row><cell>ViT-S/16</cell><cell>-</cell><cell>74.2%</cell><cell>56.6%</cell><cell>94.8%</cell><cell>53.6%</cell><cell>37.0%</cell></row><row><cell>Soft MoE-S/16</cell><cell>128</cell><cell>81.2%</cell><cell>67.2%</cell><cell>96.6%</cell><cell>56.0%</cell><cell>39.0%</cell></row><row><cell>Soft MoE-S/14</cell><cell>256</cell><cell>82.0%</cell><cell>75.1%</cell><cell>97.1%</cell><cell>56.5%</cell><cell>39.4%</cell></row><row><cell>ViT-B/16</cell><cell>-</cell><cell>79.6%</cell><cell>71.0%</cell><cell>96.4%</cell><cell>58.2%</cell><cell>41.5%</cell></row><row><cell>Soft MoE-B/16</cell><cell>128</cell><cell>82.5%</cell><cell>74.4%</cell><cell>97.6%</cell><cell>58.3%</cell><cell>41.6%</cell></row><row><cell>ViT-L/16</cell><cell>-</cell><cell>82.7%</cell><cell>77.5%</cell><cell>97.1%</cell><cell>60.7%</cell><cell>43.3%</cell></row><row><cell>Soft MoE-L/16</cell><cell>128</cell><cell>83.8%</cell><cell>79.9%</cell><cell>97.3%</cell><cell>60.9%</cell><cell>43.4%</cell></row><row><cell>Souped Soft MoE-L/16</cell><cell>128</cell><cell>84.3%</cell><cell>81.3%</cell><cell>97.2%</cell><cell>61.1%</cell><cell>44.5%</cell></row><row><cell>ViT-H/14</cell><cell>-</cell><cell>83.8%</cell><cell>84.7%</cell><cell>97.5%</cell><cell>62.7%</cell><cell>45.2%</cell></row><row><cell>Soft MoE-H/14</cell><cell>256</cell><cell>84.6%</cell><cell>86.3%</cell><cell>97.4%</cell><cell>61.0%</cell><cell>44.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison between Top-K with and without BPR.Figure20: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing the total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently better results with more experts, whereas cost is kept roughly constant. Adding too many experts to Experts Choice hurt performance and significantly increases the cost. Experts Choice can perform well with many experts if we increase the group size up to 32 images per group. The normalized train step time is computed with respect to Soft MoE with 32 experts. Experts Choice with 32 images per group and 4096 experts requires more than 2.5x its cost.Figure21: JFT precision-at-1, ImageNet 10-shot accuracy, and normalized training step time when increasing the total number of experts while keeping the total amount of slots fixed. Soft MoE achieves consistently better results with more experts, whereas cost is kept roughly constant. Adding too many experts to Tokens Choice hurt performance and significantly increases the cost. Even with a large group size (16 images), Tokens Choice struggles to perform well with a few thousand experts. The normalized train step time is computed with respect to Soft MoE with 32 experts. Tokens Choice with 8 or 16 images per group and 4096 experts requires almost 4x its cost.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell cols="7">Number of Experts K BPR JFT prec@1 IN/10shot</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>32</cell><cell>1</cell><cell>No</cell><cell>50.1%</cell><cell></cell><cell>64.5%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>32</cell><cell>1</cell><cell>Yes</cell><cell>51.2%</cell><cell></cell><cell>68.9%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>32</cell><cell>2</cell><cell>No</cell><cell>52.5%</cell><cell></cell><cell>71.0%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>32</cell><cell>2</cell><cell>Yes</cell><cell>52.8%</cell><cell></cell><cell>71.4%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>64</cell><cell>1</cell><cell>No</cell><cell>50.0%</cell><cell></cell><cell>64.4%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>64</cell><cell>1</cell><cell>Yes</cell><cell>51.5%</cell><cell></cell><cell>69.1%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>64</cell><cell>2</cell><cell>No</cell><cell>52.9%</cell><cell></cell><cell>70.9%</cell></row><row><cell></cell><cell></cell><cell>V-MoE S/16</cell><cell></cell><cell>64</cell><cell>2</cell><cell>Yes</cell><cell>52.9%</cell><cell></cell><cell>71.4%</cell></row><row><cell>ImageNet 10-shot Accuracy</cell><cell>67% 73% 77% 81% 84%</cell><cell>10 3 Total Training TPUv3-days S/16 B/16 L/16 H/14</cell><cell>JFT-4B Precision-at-1</cell><cell>51% 54% 57% 59%</cell><cell cols="2">10 3 Total Training TPUv3-days S/16 B/16 L/16 H/14</cell><cell>ImageNet Finetune Accuracy</cell><cell>84% 85% 87% 88% 89%</cell><cell>Total Training TPUv3-days 10 3 S/16 B/16 H/14 L/16 Soft MoE Dense</cell></row><row><cell cols="10">Figure 19: Long runs. Soft MoE and ViT models trained for 4 million steps with batch size 4096 (H/14</cell></row><row><cell cols="10">models trained for 2 million steps instead). Equivalent model classes (S/16, B/16, L/16, H/14) have similar</cell></row><row><cell cols="10">training costs, but Soft MoE outperforms ViT on all metrics. We show ImageNet 10-shot (left), JFT precision</cell></row><row><cell cols="10">at 1 (middle) and ImageNet accuracy after finetuning (right), versus total training FLOPs. See Table 2. We</cell></row><row><cell cols="4">report training FLOPs in Figure 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice, all experts apply the same function with different parameters, usually an MLP.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Rodolphe Jenatton</rs>, who provided extremely valuable feedback on an earlier version of this manuscript; <rs type="person">Ilya Tolstikhin</rs>, who suggested the "Identity router" used in Appendix A (or "Liquid router", as he dubbed it); and the rest of <rs type="institution">Google DeepMind</rs> folks for providing a supportive research environment, very especially to our colleagues in Europe.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Pareto Models</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+ NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Salz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06794</idno>
		<title level="m">A jointly-scaled multilingual language-image model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unified scaling laws for routed language models</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Las</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4057" to="4086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How much attention do you need? a granular analysis of neural machine translation architectures</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1799" to="1808" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5232" to="5270" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Base layers: Simplifying training of large, sparse models</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6265" to="6274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15466</idno>
		<title level="m">Sparsity-constrained optimal transport</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Soft merging of experts with adaptive routing</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multimodal contrastive learning with limoe: the language-image mixture of experts</title>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02770</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Renggli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12015</idno>
		<title level="m">Learning to merge tokens in vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8583" to="8595" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hash layers for large sparse models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17555" to="17566" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><surname>Tokenlearner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11297</idno>
		<title level="m">What can 8 learned tokens do for images and videos? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Zhi Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal token fusion for vision transformers</title>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lele</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="12186" to="12195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18123" to="18133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixture-of-experts with expert choice routing</title>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Laudon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7103" to="7114" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In this section, we provide the results of a simple experiment that can help better design the configuration of sparse models. We fix a total number of experts (E = 512) with one slot per expert, thus leading to matched number of parameters (note in this case FLOPs may vary greatly depending on the number of sparse layers). Then, for an S/16 backbone architecture, we distribute those experts in various ways (all in one layer, half of them in two layers, etc) and compare their performance after 300k training steps. Table 5 shows the results. Again, we observe that a number of experts close to the number of input tokens (there are 196 tokens, given the 16x16 patch size for 224x224 images) split over the last few layers works best. Moreover, note these models are indeed cheaper than those in the comparison with 512 or 256 experts per layer. Table 6 offers results for Tokens Choose routing with K = 1 and BPR Riquelme et al. (2021)</title>
		<author>
			<orgName type="collaboration">D Sparse Layers Placement Soft MoE</orgName>
		</author>
		<idno>IN/10shot JFT prec1 11 512 512 70.0% 51.5% 10 512 512 70.1% 52.0% 10, 11 256 512 71.7% 52.2% 5, 11 256 512 70.4% 52.1% 8</idno>
	</analytic>
	<monogr>
		<title level="m">the large number of degrees of freedom in these choices has usually made thorough ablations and optimization unfeasible</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Expert placing ablation with a V-MoE S/16 Experts Choose C = 1 with 12 layers. indexed as 0:11). Sparse Layers Experts per Layer Total Experts IN/10shot JFT prec1 11 512 512 65.3% 50.3% 10 512 512 66.5% 51.7% 10, 11 256 512 68.8% 51.8% 5, 11 256 512 65.9% 51.1% 8, 9, 10, 11 128 512 69.4% 52.2% 2, 5, 8, 11 128 512 68.0% 51.7% 4:11 64 512 69.0% 52.2% 1:4, 8:11 64 512 67.4% 51.1%</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
