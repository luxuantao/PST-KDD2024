<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Low-Rank Tensor Recovery with Rectification and Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">Robust Low-Rank Tensor Recovery with Rectification and Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">400DF1C3F93E4AF8C3F469F030B53424</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-rank tensor recovery</term>
					<term>rectification</term>
					<term>alignment</term>
					<term>ADMM</term>
					<term>proximal gradient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-rank tensor recovery in the presence of sparse but arbitrary errors is an important problem with many practical applications. In this work, we propose a general framework that recovers low-rank tensors, in which the data can be deformed by some unknown transformations and corrupted by arbitrary sparse errors. We give a unified presentation of the surrogate-based formulations that incorporate the features of rectification and alignment simultaneously, and establish worst-case error bounds of the recovered tensor. In this context, the state-of-the-art methods 'RASL' and 'TILT' can be viewed as two special cases of our work, and yet each only performs part of the function of our method. Subsequently, we study the optimization aspects of the problem in detail by deriving two algorithms, one based on the alternating direction method of multipliers (ADMM) and the other based on proximal gradient. We provide convergence guarantees for the latter algorithm, and demonstrate the performance of the former through in-depth simulations. Finally, we present extensive experimental results on public datasets to demonstrate the effectiveness and efficiency of the proposed framework and algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R Ecent years have witnessed tremendous advances in sensorial and information technology, where massive amounts of high-dimensional data, often un-labelled have become available. A key category therein is visual data, which are typically collected by various smart imaging devices (e.g. mobile phones, cameras, surveillance and medical imaging equipments). However, such data in their raw form cannot be directly used, as they often suffer from various degradation factors, such as noise pollution <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, missing observations <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, partial occlusion <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, misalignments <ref type="bibr" target="#b6">[7]</ref> and so on. As such, it has become an increasingly pressing challenge to develop efficient and effective computational tools that can automatically extract the hidden structures and useful information from such data for various computer vision tasks.</p><p>In the past decade, many revolutionary new tools <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> have been developed to recover low-dimensional structures in the form of sparse vectors or low-rank matrices from high dimensional data. Specifically, in the literature on matrix recovery problems, it has been shown that if the data are deformed or corrupted from an intrinsically low-rank matrix, one can recover the rectified low-rank structure despite different types of deformation (linear or nonlinear) and severe corruptions. Such concepts and methods have been successfully applied to rectify the so-called low-rank textures <ref type="bibr" target="#b18">[19]</ref> and to align • X. Zhang and D. Wang are with the Department of Computer Science, Wenzhou University, Zhejiang 325035, China (e-mail: zhangxiaoqinnan@gmail.com, wangdi@wzu.edu.cn). Z. Zhou is with the Department of Electrical Engineering, Stanford University, CA, USA (zyzhou@stanford.edu). Y. Ma is with the Department of Electrical Engineering and Computer Sciences, UC Berkeley, Berkeley, CA, USA (yima@eecs.berkeley.edu).</p><p>A preliminary version of this paper was accepted by Advances in Neural Information Processing Systems 2013 <ref type="bibr" target="#b46">[47]</ref>.</p><p>multiple correlated images (such as video frames or human faces) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Nevertheless, instead of matrices, much of the visual data in practical applications are given in their natural form as third-order (or even higher-order) tensors (e.g. color images, videos, hyper-spectral images, high-dynamical range images, 3-D range data etc.) <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Important structures and useful information are lost if the data are processed as an 1-D signal or a 2-D matrix. These data are often subject to many geometric deformations or corruptions, because of changes in view point, illuminations or occlusions. The true intrinsic structure of the data will not be correctly or fully revealed unless these nuisance factors are removed in the processing stage. Such applications naturally lead to tensor recovery problems, where matrix recovery techniques are not directly applicable. This is because standard matrix recovery tools, when applied to data of higher-order tensorial form, such as videos or 3-D range data, are only able to harness one type of low-dimensional structure at a time, and are not able to exploit the low-dimensional tensorial structures in the data. For instance, the previous work TILT rectifies a low-rank textural region in a single image <ref type="bibr" target="#b18">[19]</ref>. In contrast, RASL aligns multiple correlated images <ref type="bibr" target="#b6">[7]</ref>. They are highly complementary to each other: they exploit spatial and linear correlation in a given sequence of images, respectively. A natural question arises: can we simultaneously harness all such low-dimensional structures in an image sequence by viewing it as a third-order tensor?</p><p>A key challenge in successfully answering the above-raised question lies in an appropriate definition of tensor rank, which corresponds to the notion of intrinsic 'dimension' or 'degree of freedom' for the tensorial data. Traditionally, there are two definitions of tensor rank, which are based on the CP (CANDECOMP/PARAFAC) decomposition <ref type="bibr" target="#b26">[27]</ref> and Tucker decomposition <ref type="bibr" target="#b27">[28]</ref> respectively. Similar to the definition of matrix rank, the rank of a tensor based on CP decomposition is defined as the minimum number of rank-one decompositions of a given tensor. However, rank defined in this way is a nonconvex and nonsmooth function on the tensor space, and minimization of this function is NP-hard. An alternative definition of tensor rank is based on the so-called Tucker decomposition, and it is defined by ranks of the unfolding matrices of the tensor.</p><p>Due to the recent breakthroughs in the recovery of lowrank matrices <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, the latter definition has received increasing attention. Gandy et al. <ref type="bibr" target="#b28">[29]</ref> adopt the sum of the ranks of the different unfolding matrices as the rank of the tensor data, which is in turn approximated by the sum of nuclear norms (SNN). They then apply the augmented Lagrangian method to solve the SNN based tensor completion problem with Gaussian observation noise. Instead of directly adding up the ranks of the unfolding matrices, a weighted sum of the ranks of the unfolding matrices is introduced by Liu et al. <ref type="bibr" target="#b29">[30]</ref>. They also propose several optimization algorithms to estimate missing values for tensorial visual data, such as color images. Mu et al. <ref type="bibr" target="#b30">[31]</ref> define a new unfolding operator which can generate a balanced matrix, and then adopt the nuclear norm on that matrix as the norm of the original tensor. However, this tensor norm definition can not be used in case of misalignment. In <ref type="bibr" target="#b31">[32]</ref>, three different strategies have been developed to extend the matrix-trace-norm regularization to tensors: (1) treat tensor as a matrix; (2) traditional constrained optimization of low rank tensors as in <ref type="bibr" target="#b29">[30]</ref>; (3) mixture of low-rank tensors.</p><p>All of the above work address the tensor completion problem in which the locations of the missing entries are known, and moreover, observation noise is assumed to be simple Gaussian noise. However, in practice, a fraction of the tensorial entries can be arbitrarily corrupted by large errors, and the number and the location of the corrupted entries are unknown. Most closely related to our work is the robust tensor recovery problem by Li et al. <ref type="bibr" target="#b32">[33]</ref>, which has extended the robust principal component analysis (RPCA) <ref type="bibr" target="#b0">[1]</ref> from recovering a low-rank matrix to the tensor case, or more precisely, proposed a method to recover a low-rank tensor with sparse errors. Huang et al. <ref type="bibr" target="#b33">[34]</ref> prove that in case of the SNN convexification of the Tucker rank and 1 norm, the low-rank tensor can be exactly recovered under a set of tensor incoherence conditions. Goldfarb and Qin <ref type="bibr" target="#b34">[35]</ref> propose several different low-rank tensor recovery models from the tensor RPCA and tensor completion perspectives. They also propose some effective optimization algorithms with global convergence guarantees to solve their models. A key assumption therein is that the images that form the tensor must be well aligned. However, this is not the case in many computer vision applications: images of the same object or scene can appear drastically different even under moderate changes in the object's position or pose with respect to the camera. Furthermore, the above low-rank models break down even if the images are just slightly misaligned with respect to each other. Motivated by the above concern, we propose in this paper a general robust tensor recovery framework that addresses this issue, thereby greatly expanding the applicability of the tensor recovery framework in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>Our main contributions are three-fold.</p><p>First, we propose a robust low-rank tensor recovery framework, which deals with sparse noise corruption and simultaneously handles rectification and alignment. Specifically, the images that form the tensor do not need to be well-aligned or rectified, and can be corrupted with a small fraction of errors. In particular, the existing methods 'RASL' and 'TILT' can be viewed as two special cases of our method. We present two closely related formulations, one based on 1 minimization (Section 3), the other based on p minimization (Section 4). We note that in the matrix case, the p minimization (0 &lt; p &lt; 1) is known to be more effective than 1 minimization, because 1 minimization requires much stronger incoherence conditions than p minimization in order to achieve exact recovery under sparse noise <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. This conclusion makes intuitive sense because, in comparison to 1 norm, p based norms provide closer surrogates to the 0 norm, for 0 &lt; p &lt; 1. The downside is that p minimization is non-convex, and does not enjoy theoretical convergence guarantees. In tensor space, p minimization formulations have not been explored much. Here we provide such a formulation. Furthermore, in both formulations, we provide worst-case error bounds which measure the average errors of the recovered lowrank tensor in the worst case. As we see in the experiments, the recovered tensors typically exhibit much smaller errors than the worst-case bounds.</p><p>Second, we present two optimization algorithms that solve the tensor recovery problem efficiently. Specifically, we apply two algorithmic paradigms, one based on the alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, the other based on proximal gradient, and derive in detail the specific optimization algorithms. As explained in Section 5, each of the two algorithms has its own merits and drawbacks. ADMM converges faster in practice to near-optimal values, but the convergence cannot be guaranteed for minimizing objective function whose block variables are larger than two <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. The algorithm based on proximal gradient converges slightly more slowly in practice, but we establish the strong theoretical guarantee of convergence for the algorithm. Both algorithms are more efficient and effective than the related previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, which have no convergence guarantee in both papers.</p><p>Third, we present in Section 6 several in-depth simulations and experimental results to demonstrate the efficacy of the proposed robust low-rank tensor recovery framework. The experiments are divided into two parts. In the first part, we work with synthetic data, where the true low-rank tensor and sparse error tensor are generated artificially. Then the performance of recovery can be quantitatively measured. In the second part, we work with several publicly available datasets and demonstrate the superior performance of the proposed methods over others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MATHEMATICAL PRELIMINARIES</head><p>To avoid confusion, the symbols used in this paper are chosen as follows: 1) lowercase letters for scalars (a, b, c • ••); 2)  <ref type="figure">(A,</ref><ref type="figure">B,</ref><ref type="figure">C • ••</ref>). In the following subsections, the tensor algebra, as well as the tensor rank are briefly introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tensor Algebra</head><p>A tensor can be regarded as a multi-order 'array' lying in a vector space. We denote an N -order tensor as A ∈ R I1×I2×•••×I N , where I n (n = 1, 2, • • •, N ) is a positive integer. Each element in this tensor is represented as</p><formula xml:id="formula_0">a i1•••in•••i N ,</formula><p>where 1 ≤ i n ≤ I n . Each order of a tensor is associated with a 'mode'. By unfolding a tensor along a mode, the unfolding matrix corresponding to the mode is obtained. For example, the mode-n unfolding matrix A (n) ∈ R In×( i =n Ii) of A consists of I n -dimensional mode-n column vectors which are obtained by varying the nth-mode index i n and keeping the indices of the other modes fixed. This unfolding operator is represented by A (n) = unfold n (A). Fig. <ref type="figure" target="#fig_0">1</ref> shows the unfolding of a third-order tensor. The inverse operation of the mode-n unfolding is the mode-n folding which restores the original tensor A from the mode-n unfolding matrix A (n) . The folding is represented by A = fold n (A (n) ). The mode-n rank r n of A is defined as the rank of the mode-n unfolding matrix</p><formula xml:id="formula_1">A (n) : r n = rank(A (n) ).</formula><p>The operation of mode-n product of a tensor and a matrix forms a new tensor. The mode-n product of tensor A and matrix U is denoted as</p><formula xml:id="formula_2">A × n U . If the matrix U ∈ R In×Jn , then A × n U ∈ R I1×•••×In-1×Jn×In+1×•••×I N</formula><p>and its elements are calculated by:</p><formula xml:id="formula_3">(A × n U ) i1•••in-1jnin+1•••i N = in a i1•••in•••i N u jnin . (1)</formula><p>The scalar product of two tensors A and B with the same set of indices is defined as</p><formula xml:id="formula_4">A, B = i1 i2 • • • i N a i1•••i N b i1•••i N .</formula><p>(</p><formula xml:id="formula_5">)<label>2</label></formula><p>The Frobenius norm of</p><formula xml:id="formula_6">A ∈ R I1×I2×•••×I N is defined as ||A|| F = A, A = i1,...,i N a 2 i1...i N . The 0 norm ||A|| 0 is the number of non-zero entries in A and the 1 norm ||A|| 1 is given by ||A|| 1 = i1,•••,i N |a i1,•••,i N |. It can be shown that ||A|| F = ||A (k) || F , ||A|| 0 = ||A (k) || 0 and ||A|| 1 = ||A (k) || 1 for any 1 ≤ k ≤ N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor Rank</head><p>Traditionally, there are two definitions of tensor rank based on CP decomposition <ref type="bibr" target="#b26">[27]</ref> and Tucker decomposition <ref type="bibr" target="#b27">[28]</ref>, respectively.</p><p>As stated in <ref type="bibr" target="#b26">[27]</ref>, in analogy to singular value decomposition (SVD), the rank of a tensor A can be defined as the minimum number r of rank-one components in the sum as follows:</p><formula xml:id="formula_7">A = r j=1 λ j u (1) j •u (2) j •••••u (N ) j = D× 1 U (1) × 2 U (2) •••× N U (N ) ,</formula><p>(3) where • denotes outer product, D ∈ R r×r×•••×r is an Norder diagonal tensor whose (j, j, j)th element is λ j , and</p><formula xml:id="formula_8">U (n) = [u (n) 1 , . . . , u (n) r ].</formula><p>The above decomposition model is called PARAFAC. However, this rank definition is a highly nonconvex discontinuous function on the tensor space. In general, direct minimization of such a rank function is NPhard.</p><p>Another kind of rank definition considers the mode-n rank r n of tensors, which is inspired by the Tucker decomposition <ref type="bibr" target="#b27">[28]</ref>. The tensor A can be decomposed as follows:</p><formula xml:id="formula_9">A = G × 1 U (1) × 2 U (2) • • • × N U (N ) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_10">G = A × 1 U (1) T × 2 U (2) T • • • × N U (N ) T</formula><p>is the core tensor controlling the interaction among the N mode matrices U (1) , . . . , U (N ) . In the sense of Tucker decomposition, an appropriate definition of tensor rank should satisfy the follow condition: a low-rank tensor yields a low-rank matrix when unfolded appropriately. This means that the rank of tensor can be represented by the ranks of the unfolding matrices. As illustrated in <ref type="bibr" target="#b27">[28]</ref>, the orthonormal column vectors of U (n) span the column space of the mode-n unfolding matrix</p><formula xml:id="formula_11">A (n) (1 ≤ n ≤ N ), so that if U (n) ∈ R In×rn , n = 1, . . . , N ,</formula><p>then the rank of the mode-n unfolding matrix A (n) is r n . Accordingly, we call A a rank-(r 1 , . . . , r N ) tensor.</p><p>From Eqs. ( <ref type="formula" target="#formula_29">3</ref>) and (4), we can find that a rank-r tensor defined by the CP decomposition is always a rank-(r, • • • , r) tensor in the sense of Tucker decomposition. Although the reverse of the conclusion is not true, the rank definition defined by Tucker decomposition shares some consistences with the CP decomposition, and it is easy to be calculated, so we adopt this definition in the following work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ROBUST LOW-RANK TENSOR RECOVERY VIA 1 MINIMIZATION</head><p>In this section, we consider the problem of recovering lowrank tensors corrupted by sparse errors via 1 minimization. We first present the vanilla 1 -minimization problem, followed by an enhanced formulation that incorporates rectification and alignment. A worst case error bound on low-rank tensor recovery is also given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Low-Rank Tensor Recovery Formulation</head><p>Given a low-rank N -order tensor A ∈ R I1×I2×•••×I N , if the tensor data A is noise free, then we can easily extract its low rank structure. However, in real applications, the data are inevitably corrupted by noise or errors. Rather than modeling the noise as Gaussian, we model it as an additive sparse error term E which fulfills the following conditions: (1) only a small fraction of entries are corrupted; (2) the errors are large in magnitude; (3) the number and the location of the corrupted entries are unknown. Mathematically, we posit that the original tensor data A can be decomposed into a low-rank component L 0 and a sparse component E 0 :</p><formula xml:id="formula_12">A = L 0 + E 0 .<label>(5)</label></formula><p>The ultimate goal is to estimate the low-rank component from the erroneous observations A.</p><p>When the corruption is modeled, the low rank structure recovery problem for tensors can be formalized as follows.</p><formula xml:id="formula_13">min L,E rank(L) + γ||E|| 0 , s.t. A = L + E.<label>(6)</label></formula><p>The above optimization problem is not tractable because both rank and 0 -norm are nonconvex and discontinuous. To relax this limitation, we first adopt the rank definition based on the Tucker decomposition. In this way, the tensor rank is obtained from the ranks of a set of unfolding matrices, and then the rank function is relaxed as the nuclear norm since the nuclear norm is a good convex approximation of the rank function. The nuclear norm of a matrix A is defined as</p><formula xml:id="formula_14">||A|| * = m k=1 σ k (A)</formula><p>, where σ k (A) is the kth largest singular value of matrix A. Therefore, the nuclear norm of an N -order tensor is accordingly defined as follows:</p><formula xml:id="formula_15">||L|| * = N i=1 α i ||L (i) || * ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_16">N i=1 α i = 1.</formula><p>Moreover, it is well known that 1 -norm is a good convex surrogate of the 0 -norm. We hence replace ||E|| 0 with ||E|| 1 and the optimization problem in <ref type="bibr" target="#b5">(6)</ref> becomes</p><formula xml:id="formula_17">min L,E N i=1 α i ||L (i) || * + γ||E|| 1 , s.t. A = L + E. (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-Rank Tensor Recovery with Transformations: Simultaneous Rectification and Alignment</head><p>Without loss of generality, in this paper we focus on thirdorder tensors to study the low-rank recovery problem 1 . Most practical data and applications we experiment with belong to this class of tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Transformed 1 Minimization Formulation</head><p>An implicit assumption in Eq. ( <ref type="formula">8</ref>) is that the tensor formed by the image stack is well aligned. This ensures the tensor has a low rank. However, for most practical data, precise alignments are not always guaranteed and even small misalignments 1. The proposed low-rank structure recovery model can be easily extended to N -order tensor data will break the low-rank structure of the data. To compensate for possible misalignments, we need to search for a set of transformations, which act on the two-dimensional slices of the tensor data, such that the low-rank structure of the tensor formed by the transformed images is guaranteed. We denote the set of transformations as Γ = {τ 1 , . . . , τ I3 }, then Eq. ( <ref type="formula">8</ref>) becomes to</p><formula xml:id="formula_18">min L,E,Γ 3 i=1 α i ||L (i) || * + γ||E|| 1 , s.t. A • Γ = L + E , (9)</formula><p>where A • Γ means that the transformation τ i is applied to the slice A(:, :, i) for i = 1, . . . , I 3 .</p><p>However, the equality constraint A • Γ = L + E is highly nonlinear due to the domain transformations Γ, making the minimization ( <ref type="formula">9</ref>) not tractable. Linearization with respect to the transformation parameters is a popular way to approximate the above constraint when the changes in Γ are small or incremental. Accordingly, the first-order approximation to the above problem is as follows:</p><formula xml:id="formula_19">min L,E,∆Γ 3 i=1 α i ||L (i) || * + γ||E|| 1 s.t. A • Γ + fold 3 ( I3 i=1 J i ∆Γ i i ) = L + E,<label>(10)</label></formula><p>where J i represents the Jacobian of A(:, :, i) with respect to the transformation parameters τ i , and i denotes the standard basis for R I3 . As this linearization is only a local approximation to (9), we solve it iteratively in order to converge to a (local) minimum of (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Differences to Previous Work</head><p>As we see in Eq. ( <ref type="formula" target="#formula_19">10</ref>), the optimization problem is similar to the problems addressed in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, the proposed work differs from these earlier work in that:</p><p>1) RASL and TILT can be viewed as two special cases of our work. Consider the mode-3 unfolding matrix A (3) in the bottom row of Fig. <ref type="figure" target="#fig_0">1</ref>, we can find that each row of A (3) corresponds to the vectorized form of an image. If we set α 1 = 0, α 2 = 0 and α 3 = 1, our formulation reduces to RASL. While for the mode-1 and mode-2 unfolding matrices (see Fig. <ref type="figure" target="#fig_0">1</ref>), if we set α 1 = 0.5, α 2 = 0.5 and α 3 = 0, the proposed work acts as TILT. In this sense, our formulation is more general as it simultaneously performs rectification and alignment. 2) Our work vs. RASL: In the image alignment applications, RASL treats each image as a vector and does not make use of any spatial structure within each image. In contrast, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, in our work, the low-rank constraints on the mode-1 and mode-2 unfolding matrices effectively harness the spatial structures within images. 3) Our work vs. TILT: TILT deals with only one image at a time and harnesses spatial low-rank structures to rectify the image. However, it ignores the correlation among multiple images. While our work combines the merits of RASL and TILT, and thus can extract information from visual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Worst-Case Error Bound</head><p>We consider a generalized version of problem (9) as follows:</p><formula xml:id="formula_20">min L,E,Γ ||L|| * + γ||E|| 1 = N i=1 α i ||L (i) || * + γ||E|| 1 s.t. A • Γ = L + E .<label>(11)</label></formula><p>A standard way to measure the performance of recovery is to use the average recovery error on the low-rank component. Specifically, if the solution to the convex program ( <ref type="formula" target="#formula_20">11</ref>) is (L * , E * ) and the pair of true low-rank and sparse tensors is (L 0 , E 0 ), then the average recovery error is defined as</p><formula xml:id="formula_21">Err(L * ) = ||L * -L0|| F M</formula><p>, where M = N i=1 I i is the total number of entries in the tensor. Next, we give a worst-case average recovery error bound, which quantifies how well the solution to the transformed 1 minimization problem approximates the low-rank tensor. Note that this error bound is a worst-case bound. In practice, as we demonstrate in simulations and experiments in Section 6, real average recovery error can be significantly smaller than the worst-case bound.</p><p>Theorem 1. Let (L 0 , E 0 ) be the pair of true low-rank and sparse tensors and L * be an optimal solution to the optimization problem <ref type="bibr" target="#b10">(11)</ref>. Suppose the optimal Γ * recovers the underlying transformations. If the mean of the entries of the sparse component E 0 is bounded by T , and the cardinality of the sup-</p><formula xml:id="formula_22">port E 0 is bounded by m, then Err(L * ) ≤ 2mT M (1-1 γ N i=1 αi √ Ii) if γ &gt; ( N i=1 I 2 i )<label>1 4</label></formula><p>.</p><p>By specializing the parameters {α i } K i=1 and γ to different values, we are able to derive a family of bounds. Next we give a particularly simple bound.</p><p>Corollary 2. By properly choosing {α i } K i=1 and γ, we have Err(L * ) ≤ 4mT M .</p><p>Proof:</p><formula xml:id="formula_23">Take α i = 1 N ,γ = 2 max i { √ I i } N i=1 , we have ||L 0 -L * || F = 2mT 1 - 1 2 maxi { √ Ii} N i=1 N i=1 1 N √ I i ≤ 2mT 1 -1 2N N i=1 √ Ii √ Ii = 4mT.<label>(12)</label></formula><p>Note that under this choice of parameters, it holds that</p><formula xml:id="formula_24">1 &gt; 1 γ N i=1 α i √ I i . However, γ = 2 max i { √ I i } N i=1 is not necessarily larger than ( N i=1 I 2 i ) 1 4</formula><p>(a simple example is all I i are equal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3. m</head><p>M is the sparsity coefficient and T is the average value of all the non-zero components of the sparse error tenor. In a very sparse tensor ( m M &lt;&lt; 1), if T is bounded (the entries in visual data are typically bounded by a constant that is not too large, i.e. the biggest pixel value in an image is often 255), then the error bound is rather small, indicating good recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ROBUST LOW-RANK TENSOR RECOVERY VIA p MINIMIZATION</head><p>In this section, we extend the 1 minimization formulation to the p minimization formulation (0 &lt; p ≤ 1). For clarification, the development mostly parallelizes that of Section 3, albeit at a faster pace since several concepts used in this section have already been introduced in the 1 minimization case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transformed p Minimization Formulation</head><p>We start by recalling a few definitions related to p norms. First, similar to a vector, we can define the p norm 2 for a tensor A: A p,p = ( i1,...,i N a p i1,...,i N ) 1 p . Next, recall the Schatten-p norm of a matrix A is the p norm on the vector of singular values:</p><formula xml:id="formula_25">A p = ( r i=1 σ i (A) p ) 1 p</formula><p>, where r is the rank of A, and σ i (A) is the ith largest singular value of A. Following the Tucker decomposition, we can define a Schatten-p based norm on tensors as follows:</p><formula xml:id="formula_26">A p = N i=1 α i A (i) p p 1 p .</formula><p>With the above preliminaries, we explore an p minimization formulation for low-rank tensor recovery, in which the nuclear norm and 1 norm in problem ( <ref type="formula">9</ref>) are replaced by Schatten-p norm and p norm respectively, and obtain the following problem:</p><formula xml:id="formula_27">min L,E,Γ 3 i=1 α i ||L (i) || p p + γ E p p,p , s.t. L + E = A • Γ. (<label>13</label></formula><formula xml:id="formula_28">)</formula><p>Linearizing around the current estimate of transformations Γ when the changes in Γ are small, we obtain the first-order approximation to problem (13) as follows:</p><formula xml:id="formula_29">min L,E,∆Γ<label>3</label></formula><formula xml:id="formula_30">i=1 α i ||L (i) || p p + γ||E|| p p,p s.t. A • Γ + fold 3 I3 i=1 J i ∆Γ i i = L + E,<label>(14)</label></formula><p>As this linearization is only a local approximation to problem <ref type="bibr" target="#b12">(13)</ref>, we again solve it iteratively in order to converge to a (local) minimum of (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Worst-Case Error Bound</head><p>Here we establish an error bound under the transformed p minimization problem <ref type="bibr" target="#b12">(13)</ref>, which can be viewed as a generalization of the bound given in Theorem 1.</p><p>Theorem 4. Let (L 0 , E 0 ) be the pair of true low-rank and sparse tensors and L * be the solution to the optimization problem <ref type="bibr" target="#b12">(13)</ref>. Suppose the optimal Γ * recovers the underlying transformations. If the average of the entries of the sparse component E 0 is bounded by T , and the cardinality of the support E 0 is bounded by m, then Err(L * ) ≤ (2m)</p><formula xml:id="formula_31">1 p T M p 1-1 γ N i=1 αiI 1- p 2 i if γ &gt; ( N i=1 I 2 i ) 1 4 .</formula><p>Remark 5. Simple bounds and conditions on γ can be similarly derived as in Corollary 2, which will not be repeated here. Note that the error bound obtained in Theorem 4 is again a worst-case bound. It reduces to the bound in Theorem 1 when p = 1. In the 'Supplementary Material', we prove this 2. It is technically not a norm in our current setting where 0 &lt; p &lt; 1, because the triangle inequality does not hold. theorem for 0 &lt; p ≤ 1 and hence include Theorem 1. Finally, the proof of this theorem relies on bounding ||L * -L 0 || p p,p in terms of ||(L 0 -L * ) (i) || p F (and the sparsity constant) by using the optimality condition, the properties of the singular values and various algebraic inequalities. We then further provide a lower bound of ||L * -L 0 || p p,p in terms of ||(L 0 -L * ) (i) || p F . Combining the upper and lower bound yields the result. Remark 6. When Γ * = Γ 0 , then a similar line of argument yields the following bound:</p><formula xml:id="formula_32">||L 0 -L * || F ≤ 2mT p + ||A • (Γ 0 -Γ * )|| p p,p 1 p p 1 -1 γ N i=1 α i I 1-p 2 i .<label>(15)</label></formula><p>The additional term in the bound is a characterization of the extra penalty induced by the deviation between the optimal transformations Γ * and the true underlying transformations Γ 0 . The proof is given in the 'Supplementary Material'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OPTIMIZATION ALGORITHMS</head><p>In this section, we present two optimization algorithms for solving the robust tensor recovery problems with transformations in both the 1 formulation (10) and the p formulation (14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Optimization for 1 Minimization</head><p>We discuss the optimization aspects of the 1 minimization problem in detail. We will present two approaches for solving the 1 minimization problem <ref type="bibr" target="#b9">(10)</ref>, one based on ADMM and the other based on proximal gradient. Each of the two optimization algorithms has its own merits and drawbacks. At a high level, the ADMM based algorithm converges quite fast in practice, although the theoretical guarantee of convergence for minimizing objective function involved m (m ≥ 3) block variables has remained unclear <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Proximal gradient, on the other hand, converges somewhat slower than ADMM (see experiments). Nevertheless, we can establish the convergence guarantees for the proximal gradient algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Equivalent Reformulation</head><p>Although the problem in ( <ref type="formula" target="#formula_19">10</ref>) is convex, it is still difficult to solve due to the interdependent nuclear norm terms. To remove these interdependencies and optimize these terms independently, we introduce three auxiliary matrices {M i , i = 1, 2, 3} to replace {L (i) , i = 1, 2, 3}, and the optimization problem now becomes:</p><formula xml:id="formula_33">min L,E,Mi,∆ Γ 3 i=1 α i ||M i || * + γ||E|| 1 s.t. A • Γ + ∆ Γ = L + E L (i) = M i , i = 1, 2, 3 ,<label>(16)</label></formula><p>where we define ∆ Γ . = fold 3 (</p><formula xml:id="formula_34">I3 i=1 J i ∆Γ i T i</formula><p>) T for simplicity. Next, we form the augmented Lagrangian function <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_35">f µ (M i , E, L, ∆ Γ, Y, Q i ) = 3 i=1 α i ||M i || * + γ||E|| 1 -Y, T + 1 2µ ||T || 2 F + 3 i=1 -Q i , O i + 1 2µ ||O i || 2 F , (<label>17</label></formula><formula xml:id="formula_36">)</formula><p>where we define</p><formula xml:id="formula_37">T = L + E -A • Γ -∆ Γ, O i = L (i) -M i , i = 1, 2, 3.</formula><p>Y and Q i (i=1, 2, 3) are Lagrange multiplier tensor and matrices respectively. •, • denotes the inner product of matrices or tensors. µ is a positive scalar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Algorithm 1: ADMM</head><p>In the augmented Lagrangian function, there are several terms that need to be optimized. ADMM is effective for solving optimization problems with multiple terms. Per the framework of ADMM, the above optimization problem can be iteratively solved by the following steps.</p><formula xml:id="formula_38">                     M k+1 i : = arg min Mi f µ (M i , E k , L k , ∆ Γk , Y k , Q k i ); E k+1 : = arg min E f µ (M k+1 i , E, L k , ∆ Γk , Y k , Q k i ); L k+1 : = arg min L f µ (M k+1 i , E k+1 , L, ∆ Γk , Y k , Q k i ); ∆ Γk+1 : = arg min ∆ Γ f µ (M k+1 i , E k+1 , L k+1 , ∆ Γ, Y k , Q k i ); Y k+1 : = Y k -T k+1 /µ; Q k+1 i : = Q k i -O k+1 i /µ, i = 1, 2, 3.<label>(18)</label></formula><p>In detail, we compute the solutions in analytical forms as follows.</p><p>• For term M i (i = 1, 2, 3):</p><formula xml:id="formula_39">M k+1 i = arg min Mi α i µ||M i || * + 1 2 ||L k (i) -µQ k i -M i || 2 F = U i D αiµ (Λ i )V i ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_40">U i Λ i V i = L k (i) -µQ k i and D λ (•) is the shrinkage operator 3 D λ (x) = sgn(x) max(|x| -λ, 0) . (<label>20</label></formula><formula xml:id="formula_41">)</formula><p>• For term E:</p><formula xml:id="formula_42">E k+1 = arg min E γ||E|| 1 + 1 2µ ||A•Γ+∆ Γk +µY k -L k -E|| 2 F = D γµ A • Γ + ∆ Γk + µY k -L k .<label>(21)</label></formula><p>• For term L:</p><formula xml:id="formula_43">L k+1 = arg min L 3 i=1 1 2µ ||M k+1 i + µQ k i -L (i) || 2 F 1 2µ ||A • Γ + ∆ Γk + µY k -E k+1 -L|| 2 F = 1 4 A • Γ + ∆ Γk + µY k -E k+1 + 3 i=1 fold i (M k+1 i + µQ k i ) .<label>(22)</label></formula><p>• For term ∆ Γ:</p><formula xml:id="formula_44">∆ Γk+1 = arg min ∆ Γ 1 2µ ||A•Γ+∆ Γ-L k+1 +µY k -E k+1 || 2 F = L k+1 + E k+1 -A • Γ -µY k . (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>3. The extension of the shrinkage operator to vectors, matrices and tensors is applied in an element-wise way.</p><p>Here, ∆ Γk+1 is a tensor, and then ∆Γ k+1 can be formulated as</p><formula xml:id="formula_46">∆Γ k+1 = n i=1 J + i (∆ Γk+1 ) T (3) i T i ,<label>(24)</label></formula><p>where</p><formula xml:id="formula_47">J + i = (J T i J i ) -1 J T i is pseudo-inverse of J i and (∆ Γk+1 ) (3)</formula><p>is the mode-3 unfolding matrix of tensor ∆ Γk+1 . The above analytical solutions give a complete description of the ADMM-based optimization algorithm applied to the robust tensor recovery problem. Even though ADMM is effective in practice, as we shall see later in the experiments, it is known that the global convergence has remained unclear when there are more than two variable blocks (the current optimization problem described above has four variable blocks). In the next subsection, we present a proximal gradient based optimization algorithm for problem <ref type="bibr" target="#b9">(10)</ref> and establish a global convergence guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Algorithm 2: Alternating Direction Method with Proximal Gradient</head><p>In this section, an optimization algorithm termed as alternating direction method with proximal gradient (ADMPG) is proposed for solving <ref type="bibr" target="#b15">(16)</ref>. Specifically, we stack the variables M i (i = 1, 2, 3), L and E as one big block variable [M 1 ; M 2 ; M 3 ; L; E], and take ∆ Γ as the other block variable. Based on the ADMM framework, we alternatively take a proximal gradient step for the subproblems in ( <ref type="formula" target="#formula_33">16</ref>) with respect to these two blocks and obtain the following update scheme. Intermediate algebraic steps are omitted.</p><formula xml:id="formula_48">                                                     M k+1 i : = arg min Mi α i ||M i || * + 1 2µτ1 M i -M k i -τ 1 (M k i -L k (i) + µQ k i ) 2 F ; E k+1 : = arg min E γ E 1 + 1 2µτ1 E -E k -τ 1 (T k -µY k ) 2 F ; L k+1 : = arg min L 1 2µτ1 L -L k -τ 1 T k + 3L k -µY k - 3 i=1 fold i (M k i + µQ k i ) 2 F ; ∆ Γk+1 : = arg min ∆ Γ 1 2µτ2 ∆ Γ -∆ Γk -τ 2 (∆ Γk -L k+1 -E k+1 + A • Γ + µY k ) 2 F ; Y k+1 : = Y k -T k+1 /µ; Q k+1 i : = Q k i -O k+1 i /µ, i = 1, 2, 3.<label>(25)</label></formula><p>The analytical solutions are given as follows.</p><p>• For term M k+1 i (i = 1, 2, 3):</p><formula xml:id="formula_49">M k+1 i = U i D αiµτ1 (Λ)V T i ,</formula><p>where</p><formula xml:id="formula_50">U i ΛV T i = M k i -τ 1 (M k i -L k (i) + µQ k i ) and D λ (•)</formula><p>is the shrinkage operator as shown in <ref type="bibr" target="#b19">(20)</ref>.</p><p>• For term E k+1 :</p><formula xml:id="formula_51">E k+1 = D γµτ1 E k -τ 1 T k -µY k .</formula><p>• For term L k+1 :</p><formula xml:id="formula_52">L k+1 = L k -τ 1 T k +3L k -µY k - 3 i=1 fold i (M k i +µQ k i ) .</formula><p>• For term ∆ Γk+1 :</p><formula xml:id="formula_53">∆ Γk+1 = ∆ Γk -τ 2 (∆ Γk -L k+1 -E k+1 +A•Γ+µY k ) .</formula><p>We also transform ∆ Γk+1 to its original form by <ref type="bibr" target="#b23">(24)</ref>. Using proximal gradient, we establish the global convergence to the optimal solution of problem <ref type="bibr" target="#b9">(10)</ref>, as indicated by the following theorem. The proof is given in the 'Supplementary Material' 4 .</p><formula xml:id="formula_54">Theorem 7. If 0 &lt; τ 1 &lt; 1/5 and 0 &lt; τ 2 &lt; 1, then the sequence {M k i , L k , E k , ∆ Γk , Y k , Q k i , i = 1, 2,</formula><p>3} generated by the ADMPG algorithm converges to the optimal solution of problem <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optimization for p Minimization</head><p>We similarly give the equivalent reformulation of the p minimization problem <ref type="bibr" target="#b13">(14)</ref> as follows:</p><formula xml:id="formula_55">min L,E,Mi,∆ Γ 3 i=1 α i ||M i || p p + γ||E|| p p,p s.t. A • Γ + ∆ Γ = L + E L (i) = M i , i = 1, 2, 3 .<label>(26)</label></formula><p>As before, we can apply either ADMM or proximal gradient to solve this optimization problem. Indeed, in both algorithms, each step still admits analytical solution (albeit different from those for 1 minimization). However, one crucial difference here is that the p minimization problem (26) is not convex. Consequently, global convergence of the proximal gradient algorithm cannot be guaranteed (recall that the global convergence guarantee is the only advantage of proximal gradient over ADMM in 1 minimization). In light of this, we will only present the ADMM algorithm, as it converges faster in practice compared to proximal gradient.</p><p>The structure of ADMM is quite similar, so we provide a quick presentation here, mostly focused on the differences from 1 minimization. First, we write out the augmented Lagrangian function:</p><formula xml:id="formula_56">f µ (M i , E, L, ∆Γ, Y, Q i ) = 3 i=1 α i ||M i || p p +γ||E|| p p,p -Y, T + 1 2µ ||T || 2 F + 3 i=1 -Q i , O i + 1 2µ ||O i || 2 F .<label>(27)</label></formula><p>Then ADMM proceeds in the steps of <ref type="bibr" target="#b17">(18)</ref>. The analytical solutions for solving M i and E are different from the 1 minimization case, which we give the details below:</p><p>• For term M k+1 i (i = 1, 2, 3):</p><formula xml:id="formula_57">M k+1 i = arg min Mi α i µ||M i || p p + 1 2 ||L k (i) -µQ k i -M i || 2 F = U i T αiµ (Λ i )V i ,<label>(28)</label></formula><p>4. ADMPG algorithm is a special application of the more general algorithm PG-ADMM with convergence guaranteed in case of N = 1 and g(y) = 0 in the paper <ref type="bibr" target="#b47">[48]</ref>.  where</p><formula xml:id="formula_58">U i Λ i V i = L k (i) -µQ k i and T η (•) is the shrinkage operator: T η (z) =      0 if|z| &lt; κ {0, sgn(z)â} if|z| = κ sgn(z)â if|z| &gt; κ .<label>(29)</label></formula><p>In ( <ref type="formula" target="#formula_58">29</ref>)</p><formula xml:id="formula_59">, â = [2η(1 -p)] 1 2-p , κ = â + ηpâ p-1</formula><p>, and â * is the largest solution of a + ηpa p-1 = |z|, where a &gt; 0 .</p><p>(</p><formula xml:id="formula_60">)<label>30</label></formula><p>â * can be obtained from the iteration a (t+1) = |z|ηpa p-1 (t) with the initial value a (0) ∈ (â, |z|). • For term E k+1 :</p><formula xml:id="formula_61">E k+1 = arg min E γ||E|| p p,p + 1 2µ ||A•Γ+∆ Γk +µY k -L k -E|| 2 F = T γµ A • Γ + ∆ Γk + µY k -L k . (<label>31</label></formula><formula xml:id="formula_62">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>In this section, we present experiments on several synthetic and real-world datasets with the following five algorithms: 1) RASL <ref type="bibr" target="#b6">[7]</ref> implemented by algorithm IALM (Inexact Augmented Lagrange Multiplier) 5 . 2) Li's work <ref type="bibr" target="#b32">[33]</ref>.</p><p>3) ADMM with 1 -norm (denoted as 1 +ADMM). 4) ADMPG with 1 -norm (denoted as 1 +ADMPG). 5) ADMM with p -norm (denoted as p +ADMM). We choose RASL and Li's work for comparison because they are the state-of-the-art work that address similar problems to ours, and the effectiveness and efficiency of our work for recovery of low-rank tensors can be validated. In more detail, RASL is the robust matrix recovery model with unknown alignment, and Li's work is the robust tensor recovery model without alignment. The last three are the algorithms proposed 5. For more detail, please refer to http://perception.csl.illinois.edu/matrixrank/sample code.html in this paper. The results of the five algorithms are both qualitatively and quantitatively analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synthetic Results</head><p>This part tests the above five algorithms with synthetic data. To make a fair comparison, we start by clarifying some implementation details: (1) Since domain transformations are not considered in Li's work, we assume the synthetic data are well aligned. ( <ref type="formula" target="#formula_5">2</ref>) RASL is implemented without transformations. (3) As RASL is applied to one mode of the tensor, to make it more competitive, we apply RASL to each mode of the tensor and take the mode that has the minimal reconstruction error. (4) The iteration loop is terminated when the relative difference of variables in consecutive iterations is smaller than 10 -8 or the number of iterations achieves 500.</p><p>For synthetic data, we first randomly generate two tensors: (1) a low-rank tensor L 0 ∈ R 50×50×50 whose rank is <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b9">10)</ref>; (2) an error tensor E 0 ∈ R 50×50×50 in which c percent of entries are non-zero and the non-zero entries are i.i.d. uniformly in the interval [-500, 500]. To ensure the error to be sparse, the maximal value of c is set to 40. The tensor A . = L 0 + E 0 is the input to the algorithms. L r denotes the recovered low-rank structure of A. The reconstruction error is defined as error = ||L0-Lr|| F ||L0|| F . Following the common evaluation procedure, we repeat the experiments 50 times with random generated A to report the average errors.</p><p>In the proposed methods, α is set to [ 1 3 , 1 3 , 1 3 ] since the three modes of synthetic data have the same importance. We vary the values p, γ and c in the ranges {0.05, 0.10, 0.15, • • • , 1}, {0.2, 0.4, 0.6, • • • , 2} and {1, 2, . . . , 40} respectively, and obtain the average reconstruction errors for the optimization algorithms ADMM and ADMPG as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. From the results, we can conclude that p norm is superior than 1 norm in data recovery, and the optimal p and γ is 0.85 and 1 respectively.</p><p>The parameters of RASL and Li's work are also selected carefully with the same averaging procedure described in Fig. <ref type="figure" target="#fig_2">2</ref>. Fig. <ref type="figure" target="#fig_3">3(a)</ref> shows the average reconstruction errors, from  work can achieve at least 2 times speed-up. Moveover, the result shows that the average running time of our work is higher than RASL. This is because RASL only optimizes on a single mode while our work optimizes on all three modes. The relationship between average reconstruction errors and iterations is shown in Fig. <ref type="figure" target="#fig_3">3(c</ref>). We can see that the error curves of our work, especially p +ADMM, drops very quickly during the iteration process. This also greatly validates the advantage of the proposed optimization algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Sequence Recovery and Alignment</head><p>In this section, we apply the five algorithms to several realworld datasets. The first dataset contains 16 images of the side of a building, taken from various viewpoints by a camera, and with occlusions due to tree branches. The second dataset contains 100 images of the handwritten number '3', with a fair amount of diversity. For example, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a), the number '3' in the column 1 and row 6 is barely recognizable. The third dataset contains 140 frames of a video showing named as Al Gore talking. For real-world data, the optimization procedure includes iterative linearization of transformations Γ (outer loop) and the algorithms for the linearized inner loop of problems <ref type="bibr" target="#b9">(10)</ref> or <ref type="bibr" target="#b13">(14)</ref>. The stopping criterion of the outer loop is that the difference in the value of the cost function between two consecutive iterations is smaller  than 10 -2 . The stopping criterion of the inner loop is identical to that of the synthetic data, except the threshold which is set to 10 -6 .</p><p>To fully demonstrate the effectiveness of the proposed algorithms, we divide this subsection further into four parts: recovery on raw images, recovery on images with 'salt and pepper' noise, recovery on images with occlusions, and recovery on images with mixed noise. Since both RASL and our algorithms perform with transformations while Li's work doesn't, for fairness of comparison, we apply RASL's transformations on the image data before feeding them as inputs to Li's work.  The reason is that human face has a rich spatial low-rank structures due to symmetry, and our methods harness both temporal and spatial low-rank structures for rectification and alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Image with Salt and Pepper Noise</head><p>To verify the performance of our work on image sequence with sparse noise, we add different percentages of 'salt and pepper' noise to the original image sequence data. In   From those results, we can see that although RASL removes most of the noise (still quite noisy for 'Al Gore'), the images are not well-aligned. On the other hand, Li's work does not perform well in terms of removing the noise (the figure '3' and 'Al Gore' are particularly blurred). Compared with RASL and Li's work, our work are robust to the proportion of the 'salt and pepper' noise. Even if the percentage of noise is 50%, which makes recognition difficult even for humans, our algorithms can still effectively recover the desired low-rank structure and have the superior performance on image alignment. In our work, p +ADMM achieves the best results of all. The images recovered by p +ADMM are significantly sharper and clearer than those recovered by the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Image with Occlusions</head><p>In this subsection, we add the image 'baboon' to the image sequence data at random locations. This increases the difficulty of recovery because certain key features/information of the image may be lost as a result of occlusions.</p><p>In Fig. <ref type="figure">7</ref><ref type="figure" target="#fig_8">8</ref>, column (a) shows the original images with added occlusions, and the percents of occluded area are     that the proposed algorithms can well remove Gaussian noise, sparse noise and occlusions, and obtain better recover results than Li's work and RASL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Transformation Analysis</head><p>To clear show the influence of the transformations with varying levels of misalignment, 8 images from AR Face database are chosen and are resized to 80 × 60 as the groundtruth (see the top row of Fig. <ref type="figure" target="#fig_14">13</ref>). For each image, we generate 15 images by using Euclidean transformations whose angles of rotation are uniformly distributed in the range [-θ 0 /2, θ 0 /2] degrees, and whose x and y-translations are uniformly distributed in the range [-s 0 /2, s 0 /2] pixels. We then add 40% of 'salt and pepper' noise or 10% of occluded area on the 120 transformed images. Some of the generated face images are shown in the middle and bottom rows of Fig. <ref type="figure" target="#fig_14">13</ref>. For each group of generated images, we crop the images with 50 × 40 in the center to constructed the tensor A or matrix A (the rotation or translation will cause some pixel out of the image boundary), which is taken as the input to the five algorithms.</p><p>The recovered low-rank structure of A and A are respectively denoted by L r and L r . The reconstruction error is defined as L0-Lr F   We compare the following algorithms: (1) RASL, (2) Li's work with perfect alignment, (3) 1 +ADMPG, (4) 1 +ADMM, (5) p +ADMM. The recovery results in case of 'salt and pepper' noise and occlusion are respectively shown in the top row and bottom row of Fig. <ref type="figure" target="#fig_16">14</ref>, where x-axis and yaxis represent the different levels of translations and rotations respectively, and different colors represent the different values of recovery errors. In more detail, the reconstruction errors on some typical transformations are list in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>. The recover results with perfect alignment are shown in the first row in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>, from which we can see that the proposed algorithms including 1 +ADMPG, 1 +ADMM and p +ADMM perform better than RASL and Li's work. When the transformations are added to images, the reconstruction errors of the proposed algorithms gradually increase which demonstrates the challenges of transformations. To further analyze the results in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, we divide the results into groups based on the different translations, from which we can find that pixel translation has a greater impact on the recovery accuracy than rotation. The reason is that for face data, the pixels located near the image center contain important structural information (such as eyes, nose, mouth), while the pixels located at the image peripheral region contain less structural information. Image rotations will lose some pixels in image boundary and corner, while image translations will lose more pixels, and more importantly, it is more possible to lose the pixels near the center region and break the structural information.</p><p>To investigate the influence of the magnitude of realignment to the recover accuracy, we calculate the distance between the recovered transformations with the groudtruth transformations, which are shown in Fig. <ref type="figure" target="#fig_17">15</ref>. Fig. <ref type="figure" target="#fig_17">15 (a)-(b)</ref> and Fig. <ref type="figure" target="#fig_17">15 (c)-(d</ref>) are respectively the results in case of 'salt and pepper' noise and conclusion. First, let's focus on Fig. <ref type="figure" target="#fig_17">15</ref>(a) and Fig. <ref type="figure" target="#fig_17">15</ref>(c), we can see that the distance between the recovered rotation and the groudtruth rotation is approximately proportional to the value θ 0 of rotation level. The similar phenomenon can also be found for translation in Fig. <ref type="figure" target="#fig_17">15(b)</ref> and Fig. <ref type="figure" target="#fig_17">15(d)</ref>. Moreover, we can find that the transformations can be well recovered when the rotation level θ 0 is less than  20 and the translation level s 0 is less than 10. The reason is that the first-order approximation and the repeated iterations are used to find the local minimum of problems ( <ref type="formula">9</ref>) and (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Face Recognition</head><p>In this part, face recognition is conducted on the following three datasets: Each image is resized to 32 × 32. We randomly select 30 images as training samples for each person, and the rest are testing samples. The procedure of the experiment can be decomposed into the following three steps:</p><p>Step 1 We construct the corrupted images by adding different percentages of 'salt and pepper' noise to each image. The percentage is set to c = 4 : 4 : 40.</p><p>Step 2 For each percentage of noise, we respectively use the five algorithms to recover the low-rank structure from the corrupted images. Step 3 By using the 1-nearest neighbor (1NN) algorithm to the recovered images, the recognition accuracies of the testing samples are obtained. The optimal parameters for the five algorithms are selected via 10-fold cross validation. The recognition accuracies on the three datasets are shown in Fig. <ref type="figure" target="#fig_18">16</ref>, from which we can see that Li's work almost has the lowest accuracies, especially the noise proportion is small. The accuracies of RASL are comparable to those of our methods when the noise proportion is less than 16%. However, they have a sharp fall with the noise proportion increasing. Our work is not only superior to the other two methods in terms of accuracy, but also robust to the proportion of noise. Particularly for p +ADMM, it has the best performance under all noise proportion.</p><p>Partial face images of four persons are given in Fig. <ref type="figure" target="#fig_19">17</ref>. From top row to bottom row, they are respectively the original face images, the face images with 32 percent of 'salt and pepper' noise, the face images recovered by RASL, Li'work, 1 +ADMPG, 1 +ADMM and p +ADMM. It can be seen that, the images recovered by RASL and Li's work still have severe noise. 1 +ADMPG and 1 +ADMM are better than RASL and Li's work, however, the recovered images are blurred. The images recovered by p +ADMM have the best sharpness, and they are close to the original face images. The good performance on face recognition also validates the effectiveness of our work in image denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper has proposed a general low-rank tensor recovery framework, which can simultaneously realize rectification and alignment when it is applied to image sequences and video frames. In particular, the state-of-the-art methods 'RASL' and 'TILT' can be viewed as two special cases of the framework. Two algorithmic paradigms, respectively based on ADMM and proximal gradient, are presented to solve the low-rank tensor recovery formulations. From the theoretical point of view, we derive non-trivial error bounds which measure the average errors of the recovered low-rank tensor in the worst case, and provide the convergence guarantees for the proximal gradient algorithm. Extensive experimental results on synthetic and real-world datasets have validated the effectiveness and efficiency of the proposed framework and algorithms.</p><p>We should remark that the proposed model can not guarantee an exact recovery result like the RPCA model <ref type="bibr" target="#b0">[1]</ref> and the tensor RPCA model <ref type="bibr" target="#b33">[34]</ref>. The reason is that the optimization problem (either 1 -norm or p -norm) is highly nonlinear and nonconvex because of the transformations and there may not be any optimality guarantee whatsoever even for the simple matrix case. In particular, the RASL work, which studies matrix recovery under misalignment, only provides empirical performance validation, without any recovery guarantee. As such, in this context, our worst-case recovery result provides a novel addition to the literature.</p><p>The worst-case recovery result can be viewed as complementary with the existing exact recovery results. In particular, exact recovery results typically require specialized assumptions, both in terms of the incoherence structure of the underlying low-rank matrix, and in terms of the simple generative model of the sparse errors (typically the uniform and independent Bernoulli sampling error generation). On the other hand, the worst-case recovery result universally applies without these additional structural assumptions; but it comes at the cost of retaining some positive errors (even if exact recovery is being realized). Consequently, we think of our worst-case recovery result as providing some interesting light on tensor recovery, particularly when no exact recovery guarantee is available in the existing literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of unfolding a third-order tensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The reconstruction errors with different values p and λ. For each value p, λ and c, we perform 50 experiments and obtain the averaged error. We then average over c to obtain the mean reconstruction error for each p and λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results on synthetic data with different percent c of 'salt and pepper' noise.</figDesc><graphic coords="9,68.38,242.23,76.36,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Recovery results on images without adding noise.</figDesc><graphic coords="9,68.59,392.30,75.70,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Recovery results on images with 40% 'salt and pepper' noise.</figDesc><graphic coords="10,70.12,336.06,76.36,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Recovery results on images with 50% 'salt and pepper' noise.</figDesc><graphic coords="10,70.33,486.12,75.70,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6. 2 . 1 Fig. 7 .</head><label>217</label><figDesc>Fig. 7. Recovery results on images with 10% occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Recovery results on images with 15% occlusions.</figDesc><graphic coords="11,70.33,486.12,75.70,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 5-6, column (a) shows the original images with added 'salt and pepper' noise, and the percentages of added noise are respectively 40% and 50%. The recovery results of the five algorithms are shown in column (b)-(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Recovery results on images with standard deviation σ = 25 Gaussian noise and 40% 'salt and pepper' noise.</figDesc><graphic coords="12,70.12,336.06,76.36,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Recovery results on images with standard deviation σ = 50 Gaussian noise and 40% 'salt and pepper' noise.</figDesc><graphic coords="12,70.33,486.12,75.70,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 11. Recovery results on images with standard deviation σ = 25 Gaussian noise and 10% occlusions.</figDesc><graphic coords="13,70.12,336.06,76.36,74.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Recovery results on images with standard deviation σ = 50 Gaussian noise and 10% occlusions.</figDesc><graphic coords="13,70.33,486.12,75.70,84.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Original face images and partial misaligned and noisy images.</figDesc><graphic coords="14,66.49,480.81,215.41,110.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>L0 F or L0-Lr F L0 F . We vary levels of rotation θ 0 in the range {0, 4, 8, • • • , 40} and levels of translation s 0 in the range {0, 2, 4, • • • , 20}. Each pair of (θ 0 , s 0 ) is run 10 times for average. For each experiment, the outer eye corners of the first image are chosen and the same set of coordinates are used for all images to compute the initialized transformations Γ = {τ 1 , • • • , τ 120 }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Visualization of the reconstruction errors with varying levels of misalignment. The results in case of 'salt and pepper' noise and occlusion are respectively in the top row and bottom row.</figDesc><graphic coords="15,75.63,327.02,92.87,90.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Visualization of the distances of recovered transformations and ground-truth transformations with varying levels of misalignment. Figures (a) and (b) are results in case of 'salt and pepper' noise, and figures (c) and (d) are results in case of occlusion.</figDesc><graphic coords="15,198.71,327.02,94.39,90.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. The contrast results on the three face datasets.</figDesc><graphic coords="16,62.89,239.23,223.22,195.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Some recovered face images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Reconstruction errors with different misalignment levels on data with 'salt and pepper' noise.</figDesc><table><row><cell>Misalignment level</cell><cell>Li's work</cell><cell>RASL</cell><cell>1 +ADMPG</cell><cell>1 +ADMM</cell><cell>p+ADMM</cell></row><row><cell>θ 0 = 0, s 0 = 0</cell><cell cols="5">0.1418(±0.0017) 0.0612(±0.0007) 0.0544(±0.0007) 0.0594(±0.0006) 0.0340(±0.0005)</cell></row><row><cell>θ 0 = 4, s 0 = 0</cell><cell>---</cell><cell cols="4">0.0866(±0.0227) 0.0818(±0.0220) 0.0858(±0.0218) 0.0555(±0.0148)</cell></row><row><cell>θ 0 = 8, s 0 = 0</cell><cell>---</cell><cell cols="4">0.1306(±0.0141) 0.1264(±0.0132) 0.1277(±0.0135) 0.1059(±0.0209)</cell></row><row><cell>θ 0 = 16, s 0 = 0</cell><cell>---</cell><cell cols="4">0.1491(±0.0156) 0.1445(±0.0141) 0.1450(±0.0139) 0.1414(±0.0154)</cell></row><row><cell>θ 0 = 0, s 0 = 2</cell><cell>---</cell><cell cols="4">0.1328(±0.0161) 0.1309(±0.0161) 0.1315(±0.0160) 0.1163(±0.0235)</cell></row><row><cell>θ 0 = 4, s 0 = 2</cell><cell>---</cell><cell cols="4">0.1335(±0.0180) 0.1295(±0.0194) 0.1319(±0.0189) 0.1101(±0.0179)</cell></row><row><cell>θ 0 = 8, s 0 = 2</cell><cell>---</cell><cell cols="4">0.1447(±0.0105) 0.1417(±0.0106) 0.1423(±0.0105) 0.1333(±0.0140)</cell></row><row><cell>θ 0 = 16, s 0 = 2</cell><cell>---</cell><cell cols="4">0.1677(±0.0184) 0.1625(±0.0112) 0.1628(±0.0157) 0.1594(±0.1173)</cell></row><row><cell>θ 0 = 0, s 0 = 4</cell><cell>---</cell><cell cols="4">0.1738(±0.0215) 0.1713(±0.0221) 0.1873(±0.0224) 0.1715(±0.0214)</cell></row><row><cell>θ 0 = 4, s 0 = 4</cell><cell>---</cell><cell cols="4">0.1840(±0.0145) 0.1849(±0.0110) 0.1812(±0.0137) 0.1808(±0.0139)</cell></row><row><cell>θ 0 = 8, s 0 = 4</cell><cell>---</cell><cell cols="4">0.1739(±0.0158) 0.1705(±0.0155) 0.1717(±0.0142) 0.1704(±0.0143)</cell></row><row><cell>θ 0 = 16, s 0 = 4</cell><cell>---</cell><cell cols="4">0.1929(±0.0133) 0.1875(±0.0193) 0.1898(±0.0113) 0.1872(±0.0118)</cell></row><row><cell>θ 0 = 0, s 0 = 8</cell><cell>---</cell><cell cols="4">0.2119(±0.0206) 0.2077(±0.0183) 0.2095(±0.0180) 0.2082(±0.0175)</cell></row><row><cell>θ 0 = 4, s 0 = 8</cell><cell>---</cell><cell cols="4">0.2036(±0.0208) 0.2009(±0.0196) 0.2021(±0.0197) 0.2010(±0.0195)</cell></row><row><cell>θ 0 = 8, s 0 = 8</cell><cell>---</cell><cell cols="4">0.2126(±0.0221) 0.2096(±0.0199) 0.2085(±0.0212) 0.2073(±0.0215)</cell></row><row><cell>θ 0 = 16, s 0 = 8</cell><cell>---</cell><cell cols="4">0.2202(±0.0156) 0.2151(±0.0215) 0.2147(±0.0149) 0.2127(±0.0160)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Reconstruction errors with different misalignment levels on occluded data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• ORL database. The Cambridge ORL database consists 400 images for 40 different persons, and each person has 10 images. In our experiments, each image is resized to 32×32. For each person, we randomly select 5 images as training samples, and the rest are left for testing samples. • CMU PIE database. The CMU PIE database consists more than 40, 000 facial images of 68 persons. In the experiments, we choose the first 5 persons and 170 images per person from varying illuminations and facial expressions. Each image is resized to 32×32. The number of training samples and testing samples for each person are respectively 50 and 120. • Extended Yale B database. The Extended Yale B database contains 2, 414 frontal face images of 38 persons, with approximately 64 frontal face images per person.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>+ADMPG, 1 +ADMM and p +ADMM respectively. As shown in the top row of Fig.4, we see that our work, especially p +ADMM, achieve better performance than the other two methods. In particular, the digits '3' are clearer and more</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>+ADMPG and RASL all have slight shadows. In contrast, p +ADMM can successfully remove occlusions and have the best performances of all.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the National Key Research and Development Program of China [grant no. 2018YFB1004904], in part by the National Natural Science Foundation of China [grant nos. 61772374], in part by the Natural Science Foundation of Zhejiang Province [grant nos. LY17F030004, LR17F030001], in part by the Project of Science and Technology Plans of Wenzhou City [grant nos. C20170008, G20160002, ZG2017016].</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weighted Schatten p-norm Minimization for Image Denoising and Background Subtraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4842" to="4857" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Overview of Low-rank Matrix Recovery from Incomplete Observations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="622" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Outlier-robust Matrix Completion via Lpminimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1125" to="1140" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nuclear Norm based Matrix Regression with Applications to Face Recognition with Occlusion and Illumination Changes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="156" to="171" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust Online Matrix Factorization for Dynamic Background Subtraction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1726" to="1740" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RASL: Robust Alignment by Sparse and Low-rank Decomposition for Linearly Correlated Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2233" to="2246" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exact Matrix Completion via Convex Optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Singular Value Thresholding Algorithm for Matrix Completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix Completion from a Few Entries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2980" to="2998" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Power of Convex Relaxation: Near-optimal Matrix Completion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2053" to="2080" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix Completion with Noise</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="925" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guaranteed Minimum-rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Simpler Approach to Matrix Completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3413" to="3430" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recovering Low-rank Matrices from Few Coefficients in Any Basis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1548" to="1566" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ranksparsity Incoherence for Matrix Decomposition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="572" to="596" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and Accurate Matrix Completion via Truncated Nuclear Norm Regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2117" to="2130" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust Matrix Factorization by Majorization Minimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TILT: Transform-invariant Low-rank Textures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Joint Alignment of Complex Images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data Driven Image Models Through Continuous Joint Alignment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="250" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Least Squares Congealing for Unsupervised Alignment of Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint Alignment up to (Lossy) Transforamtions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensor LRR and Sparse Coding-based Subspace Clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2120" to="2133" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-rank Tensor Completion: A Pseudo-Bayesian Learning Approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Outlier-robust Tensor PCA</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Three-way Arrays: Rank and Uniqueness of Trilinear Decompositions, with Application to Arithmetic Complexity and Statistics</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="138" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensor Decompositions and Applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tensor Completion and Low-Nrank Tensor Recovery via Convex Optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inverse Problem</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">25010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tensor Completion for Estimating Missing Values in Visual Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Musialski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Estimation of Low-rank Tensors via Convex Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.0789</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimum Subspace Learning and Error Correction for Tensors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="790" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Provable Models for Robust Low-rank Tensor Completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="364" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust Low-rank Tensor Recovery: Models and Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="253" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iterative Reweighted Algorithms for Matrix Rank Minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3441" to="3473" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lower Bound Theory of Nonzero Entries in Solutions of l2-lp Minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2832" to="2852" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Comparison of Typical lp Minimization Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Low-Rank Matrix Recovery via Efficient Schatten p-norm Minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="655" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Schechter</surname></persName>
		</author>
		<title level="m">Handbook of Analysis and Its Foundations</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-rank Matrices</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno>UILU-ENG-09-2215</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>UIUC Technical Report</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Linearized Augmented Lagrangian and Alternating Direction Methods for Nuclear Norm Minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">281</biblScope>
			<biblScope unit="page" from="301" to="329" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Iteration Complexity Analysis of Multiblock ADMM for a Family of Convex Minimization without Strong Convexity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="81" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10803</idno>
		<title level="m">On the Equivalence of Inexact Proximal ALM and ADMM for a Class of Convex Composite Programming</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distributed Linearized Alternating Direction Method of Multipliers for Composite Convex Consensus Optimization</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Aybat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
