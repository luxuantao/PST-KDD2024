<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixup for Node and Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>wangyw_seu@foxmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
							<email>yuxliang@outlook.com</email>
							<affiliation key="aff2">
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
							<email>yujun001@e.ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
							<email>bhooi@comp.nus.edu.sg</email>
							<affiliation key="aff4">
								<orgName type="institution">National University</orgName>
								<address>
									<country>Singapore Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mixup for Node and Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449796</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Supervised learning by classification</term>
					<term>Neural networks</term>
					<term>Regularization data augmentation, node classification, graph classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup is an advanced data augmentation method for training neural network based image classifiers, which interpolates both features and labels of a pair of images to produce synthetic samples. However, devising the Mixup methods for graph learning is challenging due to the irregularity and connectivity of graph data. In this paper, we propose the Mixup methods for two fundamental tasks in graph learning: node and graph classification. To interpolate the irregular graph topology, we propose the two-branch graph convolution to mix the receptive field subgraphs for the paired nodes. Mixup on different node pairs can interfere with the mixed features for each other due to the connectivity between nodes. To block this interference, we propose the two-stage Mixup framework, which uses each node's neighbors' representations before Mixup for graph convolutions. For graph classification, we interpolate complex and diverse graphs in the semantic space. Qualitatively, our Mixup methods enable GNNs to learn more discriminative features and reduce over-fitting. Quantitative results show that our method yields consistent gains in terms of test accuracy and F1-micro scores on standard datasets, for both node and graph classification. Overall, our method effectively regularizes popular graph neural networks for better generalization without increasing their time complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have achieved state-of-the-art performance on graph learning tasks, including node classification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b64">[65]</ref>, and graph classification <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b59">[60]</ref>. GNNs are capable of making predictions based on complex graph structures, thanks to their advanced representational power. However, the increased representational capacity comes with higher model complexity, which can induce over-fitting and weaken the generalization ability of GNNs. In this case, a trained GNN may capture random error or noise instead of the underlying data distribution <ref type="bibr" target="#b65">[66]</ref>, which is not what we expect.</p><p>To combat the over-fitting of neural networks, data augmentation has been demonstrated to be effective <ref type="bibr" target="#b37">[38]</ref>. For node classification specifically, <ref type="bibr" target="#b39">[40]</ref> proposes a data augmentation method named DropEdge. DropEdge follows the Vicinal Risk Minimization (VRM) principle <ref type="bibr" target="#b6">[7]</ref> to define a vicinity around each node through randomly removing edges. Then, it draws additional virtual examples from the vicinity distribution to enlarge the support of the training distribution. In other words, it assumes that nodes have their class labels unchanged after the edge removals. However, whether this assumption holds is dataset-dependent and thus requires expert knowledge for usage. Furthermore, although DropEdge models the vicinity for the nodes sharing the same class, it does not describe the vicinity relation across samples of different classes.</p><p>Motivated by the above issues, we aim to design Mixup <ref type="bibr" target="#b66">[67]</ref> methods for graph learning. Mixup is a recently proposed data augmentation method for image classification. Through linearly interpolating pixels of random image pairs and their training targets, Mixup generates synthetic images for training (see Fig. <ref type="figure" target="#fig_0">1</ref>). Mixup does not need the ground-truth labels to be unchanged with the augmented features. In contrast, it incorporates the prior knowledge that interpolations of features should lead to interpolations of the associated targets <ref type="bibr" target="#b66">[67]</ref>. Thus, Mixup extends the training distribution by constructing virtual training samples across all classes. From this vantage, Mixup acts as an effective regularization strategy for training image classifiers, which smoothens decision boundaries and improves the arrangements of hidden representations <ref type="bibr" target="#b51">[52]</ref>.</p><p>Although Mixup is effective in augmenting the image data, designing Mixup methods for graph learning is challenging. The challenges are rooted in the irregularity and connectivity of graph data. GNNs learn nodes' representations via the 'message passing' mechanism, which aggregates the representations between each node and its neighbors at each layer <ref type="bibr" target="#b57">[58]</ref>. As a result, the representation of a node relies on the nodes and edges inside its receptive field <ref type="bibr" target="#b57">[58]</ref>, all of which act as its features. Thus, to mix a pair of nodes, we need to mix their receptive field subgraphs, which consist of nodes and topology. However, unlike image pixels, nodes are not placed on a regular grid but are instead unordered, which makes it difficult to pair the nodes in different (sub)graphs for Mixup. Besides, the interpolation is not well-defined for graph topology, which is necessary for Mixup. Furthermore, due to the connectivity between nodes, the use of Mixup on different node pairs can interfere with one another, which can cause conflicts and perturb the mixed features.</p><p>In our work, we propose Mixup methods for two fundamental tasks in graph learning: node and graph classification. For the former, we randomly pair nodes and aim to mix their receptive field subgraphs. We propose the two-branch Mixup graph convolution to interpolate the irregular graph topology. At each layer, we conduct the graph convolutions following the paired nodes' topology separately in two branches and then interpolate the aggregated representations from the two branches before the next layer. In this way, the receptive field subgraphs of the paired nodes contribute to the final prediction together. To resolve the conflicts between the results of Mixup on different node pairs, we propose the two-stage Mixup framework. In the first stage, we perform a feed-forward as in the original GNNs to obtain nodes' representations without Mixup. Then in the second stage, we conduct Mixup but use each node's neighbors' representations obtained from stage one to perform the graph convolutions. As a result, each node's representations after Mixup do not interfere with the 'message passing' for other nodes. For graph classification, we mix the paired graphs in semantic space.</p><p>Our Mixup methods can be incorporated into popular GNNs thanks to their succinct design. We evaluate our methods on node classification using the Citeseer, Cora, Pubmed <ref type="bibr" target="#b32">[33]</ref>, Flickr <ref type="bibr" target="#b34">[35]</ref>, Yelp, and Amazon <ref type="bibr" target="#b64">[65]</ref> datasets, and on graph classification using the standard chemical <ref type="bibr" target="#b14">[15]</ref> and social <ref type="bibr" target="#b61">[62]</ref> datasets. Qualitatively, our methods enable GNNs to learn more discriminative representations and effectively reduce over-fitting. We also observe quantitative improvements in terms of the test accuracy and F1-micro scores, which are higher than those achieved by the existing data augmentation strategies designed for specific domains <ref type="bibr" target="#b39">[40]</ref>. Overall, our Mixup methods effectively regularize GNN models for better generalization without increasing their time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Node Classification Graph neural networks are the state-of-theart solution for node classification <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b70">[71]</ref>. The first work that proposes the convolution operation on graph data is <ref type="bibr" target="#b4">[5]</ref>. More recently, <ref type="bibr" target="#b26">[27]</ref> made breakthrough advancements in the task of node classification. As a result, the model proposed in <ref type="bibr" target="#b26">[27]</ref> is generally denoted as the vanilla GCN or GCN (Graph Convolutional Network). After <ref type="bibr" target="#b26">[27]</ref>, numerous methods are proposed for better performance on the graph learning <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b38">[39]</ref>. There are two main lines of research in this field.</p><p>The first line is to propose new GNN architectures to improve the model capacity <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b67">[68]</ref>. For example, LGCN <ref type="bibr" target="#b17">[18]</ref> ranks a node's neighbors based on node features. It assembles a feature matrix that consists of its neighborhood and sorts this feature matrix along each column. <ref type="bibr" target="#b71">[72]</ref> utilizes the positive pointwise mutual information (PPMI) matrix to capture nodes co-occurrence information through random walks sampled from a graph. <ref type="bibr" target="#b27">[28]</ref> combines PageRank with GNNs to enable efficient information propagation. <ref type="bibr" target="#b50">[51]</ref> alternatively drives local network embeddings to capture global structural information by maximizing local mutual information. <ref type="bibr" target="#b5">[6]</ref> proposes a non-uniform graph convolutional strategy, which learns different convolutional kernel weights for different neighboring nodes according to their semantic meanings. <ref type="bibr" target="#b54">[55]</ref> proposes the low-pass 'message passing' for robust graph neural networks, inhibiting the adversarial signals propagated through edges.</p><p>Another line is to propose new mini-batch training techniques for GNNs to enhance their scalability without the loss of effectiveness <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b64">[65]</ref>. GraphSAGE <ref type="bibr" target="#b21">[22]</ref> performs uniform node sampling on the previous layer neighbors. It enforces a pre-defined budget on the sample size, so as to bound the mini-batch computation complexity. <ref type="bibr" target="#b7">[8]</ref> further restricts neighborhood size by requiring only two support nodes in the previous layer. Instead of sampling layers, ClusterGCN <ref type="bibr" target="#b9">[10]</ref> and GraphSAINT <ref type="bibr" target="#b64">[65]</ref> build mini-batches from subgraphs, so as to avoid the 'neighbor explosion' problem.</p><p>Our work is orthogonal to the above two lines in the sense that it does not alter the GNN architecture, or introduce a mini-batch technique. Instead, we propose a new method that can regularize GNN models to enhance their effectiveness by augmenting the graph data. DropEdge <ref type="bibr" target="#b39">[40]</ref> is a pioneering work for data augmentation on graphs. DropEdge assumes the class labels of nodes are unchanged after the edge removals and thus requires domain knowledge for usage. In contrast, our mixup does not need the ground-truth labels to be unchanged given the augmented features and extends the training distribution by incorporating the prior knowledge that interpolations of features should lead to that of the associated targets <ref type="bibr" target="#b66">[67]</ref>. We find that the favorable characteristics of model regularization provided by our Mixup methods lead to more accurate predictions. Graph Classification. Early solutions to graph classification include graph kernels. The pioneering work <ref type="bibr" target="#b23">[24]</ref> decomposes graphs into small subgraphs and computes kernel functions based on their pair-wise similarities. Subsequent work proposes various subgraphs, such as paths <ref type="bibr" target="#b2">[3]</ref>, and subtrees <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b35">[36]</ref>. More recently, many efforts have been made to design graph neural networks (GNNs) for graph classification <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Some work proposes the graph pooling methods to summarize the node representations <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The authors of <ref type="bibr" target="#b28">[29]</ref> provide a unified view of local pooling and node attention mechanisms, and study the ability of pooling methods to generalize to larger and noisy graphs. In <ref type="bibr" target="#b8">[9]</ref>, the authors report that linear convolutional filters followed by nonlinear set functions achieve competitive performances. These work focuses on developing GNN architectures of higher complexity to improve their fitting capacity. In contrast, our framework is orthogonal to them in the sense that we propose a new data augmentation method that enhances a GNN model by interpolating the graphs from all classes to enlarge the support for training distribution. Data Augmentation. Data Augmentation plays a central role in training neural networks. It operates on the input data and improves the performance significantly. For example, in image classification, DA strategies such as horizontal flips, random erasing <ref type="bibr" target="#b69">[70]</ref>, Hide-and-Seek <ref type="bibr" target="#b45">[46]</ref>, and Cutout <ref type="bibr" target="#b13">[14]</ref> have been shown to improve performance. On MNIST, elastic distortions across scale, position, and orientation have been applied to achieve impressive results <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Mixup <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b51">[52]</ref> is a particularly effective augmentation method for image classification, where the neural network is trained on convex combinations of images and their corresponding labels. We devise the Mixup methods for graph learning, for which we propose the two-branch graph convolution and the two-stage Mixup framework to handle the irregularity and connectivity of graph data. Different from existing data augmentation techniques designed for the graph data <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, which require the ground-truth labels to be unchanged after data augmentation, our method is dataset independent and do not require domain knowledge for usage. Our Mixup methods model the vicinity relations across nodes or graphs of different classes, which enables GNNs to learn better arrangements of representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We interpolate a pair of nodes/graphs as well as their ground-truth labels to produce a novel and synthetic sample for training. To mix the graph topology, which is highly irregular, we propose the two-branch Mixup graph convolution (see Fig. <ref type="figure" target="#fig_1">2(b)</ref>). Besides, to coordinate the Mixup of different nodes in the same mini-batch, we design a two-stage framework that utilizes the representations learned before Mixup (see Fig. <ref type="figure" target="#fig_3">4</ref>). Last but not least, we interpolate the diverse and complicated graphs in the semantic embedding space for graph classification. We discuss the details of our Mixup methods for node and graph classification next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background and Motivation</head><p>Mixup is first proposed in <ref type="bibr" target="#b66">[67]</ref> for image classification. Consider a pair of samples (x i , y i ) and (x j , y j ), where x denotes the input feature, and y the one-hot class label. Mixup produces the synthetic sample as (see Fig. <ref type="figure" target="#fig_0">1</ref>):</p><formula xml:id="formula_0">x = λx i + (1 − λ)x j ,<label>(1)</label></formula><formula xml:id="formula_1">ỹ = λy i + (1 − λ)y j ,<label>(2)</label></formula><p>where λ ∈ [0, 1]. In this way, Mixup extends the training distribution by incorporating the prior knowledge that interpolations of features should lead to interpolations of the associated labels <ref type="bibr" target="#b66">[67]</ref>. Implementation of Mixup randomly picks one image and then pairs it up with another image drawn from the same mini-batch.</p><p>In our work, we focus on two fundamental tasks in graph learning: node and graph classification, the former of which aims to learn a mapping function that maps every node to a predicted class label, while the latter maps every graph to a label. We define a graph as G = (V, E), where V denotes the set of nodes, and E is the set of edges. The input attribute vector of node i is x i , and the neighborhood of node i is N (i) = {j ∈ V |(i, j) ∈ E}. Graph neural networks (GNNs) are the state-of-the-art solution for both  We propose the two-stage Mixup method to resolve conflicts between the Mixup on different node pairs. At stage one, we perform the feed-forward as in existing GNNs without Mixup. Then at stage two, we randomly pair the nodes in the mini-batch graph and mix their input attributes. Next, we perform our two-branch Mixup graph convolutions (see Fig. <ref type="figure" target="#fig_1">2</ref>) for the paired nodes at each layer, where we use each node's neighbors' representations obtained from stage one. This ensures that each node's representations after Mixup do not interfere with the 'message passing' for other nodes.</p><p>node and graph classification <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Typically, GNNs obtain the nodes' representations h (l ) i at layer l through the 'message passing' mechanism:</p><formula xml:id="formula_2">h (l ) i = AGGREGATE h (l −1) i , h (l −1) j j ∈ N (i) , W (l ) , (3)</formula><p>where W (l ) denotes the trainable weights at layer l, and AGGREGATE is an aggregation function defined by the specific GNN model <ref type="bibr" target="#b59">[60]</ref>. h (0) i = x i holds at the input layer. For node classification, GNNs learn the high-level semantic representations by stacking L layers and minimizing the classification loss, e.g., cross-entropy <ref type="bibr" target="#b1">[2]</ref>, over the final-layer predictions, as presented in Fig. <ref type="figure" target="#fig_3">4(a)</ref>. For graph classification, GNNs summarize nodes' representations into a single graph embedding through a 'readout' function:</p><formula xml:id="formula_3">h G = READOUT h (L) i i ∈ V ,<label>(4)</label></formula><p>where READOUT can be a simple permutation invariant function such as summation or a more sophisticated graph pooling function <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Designing Mixup for graph learning is challenging due to the irregularity and connectivity of graph data. The classical Mixup in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is defined over the assumption that the input features x follow the format of plain vectors, which does not fit the graph data. This motivates us to design the Mixup methods that offer effective regularization for graph learning and easy to implement alongside existing GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mixup for Node Classification</head><p>We describe the 'message passing' of a GNN layer in Eq. (3) and Fig.</p><p>(2) <ref type="bibr" target="#b57">[58]</ref>. In principle, a GNN layer updates node i's representations by aggregating the representations of itself and its neighbors. By stacking L layers, GNNs make the final-layer prediction of node i based on its L-hop neighborhood, which is known as node i's</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Two-Stage Mixup for Node Classification</head><p>Input: Graph G = (V, E) of a mini-batch, with node attributes {x i |i ∈ V}, a GNN model with the aggregation function AGGREGATE(•), hyper-parameter α for the distribution of λ, the ground truth labels {y i |i ∈ V}.</p><p>Output: The trained parameters of GNN:</p><formula xml:id="formula_4">W (l ) l . 1: for i ← 1 to #V do 2: h (0) i ← x i 3: end for 4: for l ← 1 to L − 1 do 5: for i ← 1 to #V do 6: h (l ) i ← AGGREGATE h (l −1) i , h (l −1) j j ∈ N (i) , W (l ) 7:</formula><p>end for 8: end for 9: for i ← 1 to #V do 10:</p><formula xml:id="formula_5">Sample j from V 11: λ ← Beta(α, α) 12: xij ← λx i + (1 − λ)x j 13: ỹij ← λy i + (1 − λ)y j 14: h(0) i j ← xij 15:</formula><p>for l ← 1 to L do</p><formula xml:id="formula_6">16: h(l) i j,i ← AGGREGATE h(l−1) i j , h (l −1) k k ∈ N (i) , W (l ) 17: h(l) i j, j ← AGGREGATE h(l−1) i j , h (l −1) k k ∈ N (j) , W<label>(l ) 18:</label></formula><p>h(l)</p><formula xml:id="formula_7">i j ← λ h(l) i j,i + (1 − λ) h(l) i j, j 19:</formula><p>end for 20: end for 21: Calculate classification loss L on h(L) i j , ỹij i ∈ V .</p><p>22: Back-propagation on W (l ) l for minimizing L.</p><p>receptive field <ref type="bibr" target="#b57">[58]</ref>. In other words, to interpolate the paired nodes i and j, we need to mix their receptive field subgraphs. To achieve this, we propose the two-branch Mixup graph convolution as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where we mix the node attributes of nodes i and j before the input layer:</p><formula xml:id="formula_8">xij = λx i + (1 − λ)x j ,<label>(5)</label></formula><p>Next, we conduct the graph convolutions based on nodes i and j's topologies separately at each layer:</p><formula xml:id="formula_9">h(l) i j,i = AGGREGATE h(l−1) i j , h (l −1) k k ∈ N (i) , W (l ) , h(l) i j, j = AGGREGATE h(l−1) i j , h (l −1) k k ∈ N (j) , W (l ) ,<label>(6)</label></formula><p>and mix the aggregated features from the two topologies together before the next layer:</p><formula xml:id="formula_10">h(l) i j = λ h(l) i j,i + (1 − λ) h(l) i j, j ,<label>(7)</label></formula><p>where h(0) i j = xij holds. Here, how to compute the node i's neighbors' representations <ref type="formula" target="#formula_9">6</ref>) is an issue. If we follow the same implementation as the classical Mixup <ref type="bibr" target="#b66">[67]</ref>, i.e., we randomly pair the nodes in the mini-batch to conduct Mixup and conduct the feedforward for all nodes synchronously, we can only have h</p><formula xml:id="formula_11">h (l −1) k k ∈ N (i) in Eq. (</formula><formula xml:id="formula_12">(l −1) k = h(l−1)</formula><p>km , where m is the node paired with node i's neigbhor k. This causes conflicts, because node m interferes with the 'message passing' for node i (see Eq. ( <ref type="formula" target="#formula_9">6</ref>)) through the Mixup between m and k, but node m is likely to be outside the receptive field of node i. An example is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Specifically, when mixing nodes i and j, node i's neighbors can be mixed with a node outside node i and j's receptive fields, which adds unwanted external noise to perturb the input features: here, by 'external noise', we mean any perturbation to the input features that do not arise from the receptive fields of nodes i and j.</p><p>To address the above problem, i.e., we propose the two-stage Mixup framework as shown in Fig. <ref type="figure" target="#fig_3">4(b</ref>). In the first stage, we conduct the feed-forward to GNNs for the mini-batch graph to obtain the nodes' hidden representations without Mixup. Next, in the second stage, we randomly pair the nodes in the mini-batch to conduct the Mixup of node attributes. Then, we conduct our two-branch Mixup graph convolutions for the paired nodes as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>. Note that at each layer in the second stage, we use the neighbors' representations without Mixup, which are obtained from the first stage, to conduct the graph convolutions (see Eq. ( <ref type="formula" target="#formula_9">6</ref>)). In this way, each node's representations after Mixup do not interfere with the 'message passing' for other nodes. With our two-stage framework, we effectively prevent the input features from being perturbed by the nodes outside the receptive fields. We sample the Mixup weight λ from the distribution Beta(α, α) with a hyperparameter α <ref type="bibr" target="#b20">[21]</ref>. Our Mixup method for node classification is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixup for Graph Classification</head><p>Graph neural networks utilize a READOUT function to summarize the node-level embeddings into a graph embedding. GNNs embed the complex and irregular graph structures into the embedding vectors of fixed dimension. We conduct Mixup for graph classification in the embedding space (see Fig. <ref type="figure" target="#fig_4">5</ref>). In detail, given the graphs G 1 and G 2 with the embeddings h G 1 , h G 2 and the labels y G 1 , y G 2 respectively, we mix them as:</p><formula xml:id="formula_13">hG 1 G 2 = λh G 1 + (1 − λ)h G 2 ,<label>(8)</label></formula><formula xml:id="formula_14">ỹG 1 G 2 = λy G 1 + (1 − λ)y G 2 .<label>(9)</label></formula><p>Finally, the interpolated graph-level embedding hG 1 G 2 will be passed to a multi-layer perception followed by a softmax layer to produce the predicted distribution for the targeted classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Mixup has been successfully applied to the tasks on image and text data, e.g., the classification of images <ref type="bibr" target="#b66">[67]</ref> and sentences <ref type="bibr" target="#b19">[20]</ref>. However, the graph data significantly differs from the above two kinds of data. First, in a graph, the nodes are connected, while images or sentences are isolated. Second, both the images and sentences are well-structured, the former of which has a two-dimensional grid and the latter of which is a one-dimensional sequence. However, graphs hold complicated and irregular structures. These differences pose serious challenges for Mixup. When mixing the input features, we must consider not only the node attributes but also the graph topology, for which the interpolation is not well-defined. Therefore, we propose the two-branch Mixup graph convolutions to handle this problem. In this way, we do not mix the topology directly, but mix the aggregated messages from different topology across GNN layers. In addition to this, due to the connectivity between different nodes and the 'message passing' mechanism, we need to resolve the conflicts between the Mixup of different nodes, as visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. This motivates us to propose the two-stage Mixup framework for node classification, where each node's representation after Mixup does not interfere with the 'message passing' for other nodes. In this way, each node's feature is not perturbed by the mixup happening on its neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPLEXITY ANALYSIS</head><p>With Mixup, we train GNNs in the end-to-end style. First, since our Mixup method for graph classification does not induce extra computation, its complexity is the same as the original GNN model. Second, we analyze the time complexity of our two-stage Mixup framework for node classification. Given the dimension of node representations on layer l being d l , the time complexity of GCN <ref type="bibr" target="#b26">[27]</ref>. In our method, the time complexity of the first stage is</p><formula xml:id="formula_15">is O #E L l =1 d l + #V L l =1 d l −1 d l</formula><formula xml:id="formula_16">O #E L−1 l =1 d l + #V L−1 l =1 d l −1 d l . In the second stage, we have O #E L l =1 d l + #V L l =1 d l −1 d l .</formula><p>Taking all the computation into consideration, we have the com-</p><formula xml:id="formula_17">plexity of O #E L l =1 d l + #V L l =1 d l −1 d l ,</formula><p>which is as same as the original GCN. For other kinds of GNNs, the analysis is similar to the above. Indeed, our first stage is as same as the original GNN without the final layer computation, while each layer in the second stage contributes the same complexity as that of the original GNN. Thus, our Mixup method improves the effectiveness of GNNs without increasing their time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present the performance of various GNN models trained with our Mixup methods. For node classification, we report the experimental results under both the transductive and inductive settings. For graph classification, we report the test accuracy on both chemical and social graphs. After that, we adjust the volume of labeled data to evaluate the generalization of GNNs with and without our Mixup. In addition, we visualize the learned representations of GNNs trained with Mixup compared with the GNNs without Mixup. Last but not least, we conduct ablation studies to show the sensitivity of GNNs' performance with respect to the hyper-parameters of our Mixup methods.</p><p>For node classification, we use the standard benchmark datasets: Cora, Citeseer, Cora, Pubmed <ref type="bibr" target="#b32">[33]</ref>, Flickr <ref type="bibr" target="#b34">[35]</ref>, Yelp, and Amazon <ref type="bibr" target="#b64">[65]</ref> for evaluation. The first three are citation networks, where each node is a document and each edge is a citation link. In Flickr, each node represents one image. And an edge is built between two images if they share some common properties (e.g., same geographic location, same gallery, etc.). The Yelp dataset contains a social network, where an edge indicates that the connected users are friends. For the Amazon dataset, a node is a product on the Amazon website and an edge between two products is created if the products are bought by the same customer. Each of them contains an unweighted adjacency matrix and bag-of-words features. The statistics of these datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We use the standard benchmark datasets: D&amp;D <ref type="bibr" target="#b14">[15]</ref>, NCI1, PRO-TEINS <ref type="bibr" target="#b3">[4]</ref>, COLLAB, IMDB-M, REDDIT-5K <ref type="bibr" target="#b61">[62]</ref> for the evaluation of graph classification. The first three are chemical datasets, where the nodes have categorical input features. The last three are social datasets that do not have node attributes. We follow <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b68">[69]</ref> to use node degrees as attributes. The statistics of these datasets are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>For the hyper-parameters of the baseline methods, e.g., the number of hidden units, the optimizer, the learning rate, we set them as suggested by their authors. For the hyper-parameters of our Mixup methods, we set α = 1 for the distribution of Mixup weights by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Node Classification</head><p>We conduct the experiments under both transductive and inductive settings for a comprehensive evaluation. In the transductive setting, we have access to the attributes of all nodes but only the labels of nodes in the training set for training. In the inductive setting, both the attributes and labels of the nodes in the validation/testing set are unavailable during training.</p><p>In the transductive node classification, we take the popular GNN models of GCN <ref type="bibr" target="#b26">[27]</ref>, GAT <ref type="bibr" target="#b49">[50]</ref>, LGCN <ref type="bibr" target="#b17">[18]</ref>, JKNet <ref type="bibr" target="#b60">[61]</ref>, GMNN <ref type="bibr" target="#b38">[39]</ref>, ResGCN <ref type="bibr" target="#b30">[31]</ref>, and the regularization method DropEdge <ref type="bibr" target="#b39">[40]</ref> as the baseline methods for comparison. We split nodes in each graph into 60%, 20%, 20% for training, validation, and testing. We make 10 random splits and conduct the experiments for 100 trials with random weight initialization for each split. We vary the number of layers from 1 to 30 for each model and choose the best performing number with respect to the validation set. The results are reported in Table <ref type="table" target="#tab_2">3</ref>. We observe that our two-stage Mixup method improves the test accuracy of GCN by 2.1% on Citeseer, 1.9% on Cora, 1.7% on Pubmed, and improves JKNet by 2.6% on Citeseer, 1.5% on Cora, 1.6% on Pubmed respectively. As a result, our two-stage Mixup method enhances GCN and JKNet to outperform all the baseline methods.</p><p>In the inductive settings, we use the datasets Flickr, Yelp, Amazon with the fixed partition <ref type="bibr" target="#b64">[65]</ref> for evaluation. These datasets are too large to be handled well by the full-batch implementations of GCN architectures. Hence, we use more scalable GraphSAGE <ref type="bibr" target="#b21">[22]</ref> and GraphSAINT <ref type="bibr" target="#b64">[65]</ref> as the baselines for comparison. We vary the number of layers of each method from 1 to 30 for each model and choose the best performing model with respect to the validation set. We conduct the experiments for 100 trials with random weight initialization. The results are reported in Table <ref type="table" target="#tab_3">4</ref>. GraphSAGEmean/LSTM/pool denotes that GraphSAGE uses mean, LSTM, and max-pooling as the aggregator respectively. And GraphSAINT-GCN/GAT/JKNet means that GraphSAINT takes GCN, GAT, and JKNet as the backbone respectively. We implement our two-stage Mixup method with GraphSAGE-mean and GraphSAINT-GCN to study whether Mixup can improve the performance of GCNs under the inductive setting. We observe that our two-stage Mixup improves the test F1-micro scores of GraphSAGE-mean by 3.0% on Flickr, 1.9% on Yelp, 2.0% on Amazon, and GraphSAINT-GCN by 2.5% on Flickr, 1.5% on Yelp, and 0.6% on Amazon respectively. As a result, our two-stage Mixup method enhances them to outperform the baseline methods.</p><p>Given the same GCN architecture, our Mixup method consistently produces larger improvements than DropEdge. DropEdge assumes the class labels of nodes kept unchanged after the edge removals, which is dataset-dependent. DropEdge does not model the vicinity relation across examples belonging to different classes <ref type="bibr" target="#b6">[7]</ref>. In contrast, our mixup performs the data augmentation in a dataset independent manner and extends the training distribution by incorporating the prior knowledge that linear interpolations of features should lead to that of the associated targets, which has been demonstrated to induce better representation arrangements, and higher generalization ability <ref type="bibr" target="#b66">[67]</ref>. Overall, the results above validate that our approach is effective in improving the performance of the popular GCN models under both transductive and inductive settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Classification</head><p>For graph classification, we follow <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b59">[60]</ref> to use the 10-fold cross-validation scheme for a fair comparison and evaluation. For each training fold, as suggested by <ref type="bibr" target="#b15">[16]</ref>, we conduct an inner holdout technique with a 90%/10% training/validation split, i.e., we train fifty times on a training fold holding out a random fraction (10%) of the data to perform early stopping. These fifty separate trials are needed to smooth the effect of unfavorable random weight initialization on test performances. The final test fold score is obtained as the mean of these fifty runs. Figure <ref type="figure">6</ref>: The learned representations of the nodes in the Cora dataset (visualized by t-SNE <ref type="bibr" target="#b47">[48]</ref>). Colors denote the ground-truth class labels. The node representations of same classes given by GCN with our Mixup are concentrated more than those given by GCN.</p><p>First, we conduct the experiments on node classification. We randomly select r ∈ {30%, 40%, 50%} nodes from the whole set to form the training set, and randomly take half of the left nodes as the validation set, with the other half being the testing set. The results are reported in Table <ref type="table">6</ref>. Empirically, our Mixup method enhances the performance of GCN and JKNet for different sizes of the training set. In principle, with fewer labeled nodes, i.e., smaller r , our Mixup method gives larger accuracy improvements, because over-fitting is more serious when the training data is limited, where the regularization offered by our Mixup is essential to offer better generalization.</p><p>Next, we conduct the experiments on graph classification. We randomly select r ∈ {60%, 70%, 80%} graphs from the whole set to form the labeled data, with the left graphs being the test graphs, to form a split. For each labeled set, following <ref type="bibr" target="#b15">[16]</ref>, we conduct an inner holdout technique with a 90%/10% training/validation split. In other words, we train fifty times on a labeled set holding out a random fraction (10%) of the data to perform early stopping. We conduct the experiments on 20 random splits and report the mean and standard derivations over all the splits in Table <ref type="table">7</ref>. Empirically, Mixup enhances the performance of GCN and GIN for different sizes of the training set. In principle, with fewer labeled nodes, i.e., smaller r , our method gives larger accuracy improvements, which demonstrates the necessity of the regularization given by our Mixup especially when the labeled data is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization of Mixup</head><p>We study the effects of our Mixup method on GCN models during training. We depict the test loss at each training epoch in Fig. <ref type="figure" target="#fig_7">7</ref> on the Cora and Citeseer datasets. As we can see, for both GCNs with and without Mixup, their test loss decreases initially. However, our Mixup method significantly reduces the increase in test loss at later iterations and helps GCN models to converge to a lower test loss. This demonstrates that our Mixup method is able to effectively regularize GCNs to reduce over-fitting.</p><p>Fig. <ref type="figure">6</ref> presents the final-layer representations obtained by GCN and GCN with our Mixup on the Cora dataset. It is shown that the hidden layers supported by Mixup learn more discriminative representations, thanks to the regularization given by our Mixup.   These highly discriminative representations potentially help to produce better class predictions than less discriminative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>We conduct a number of ablations to analyze our Mixup methods. First, we investigate the effects of our two-stage framework. Using our two-stage Mixup method, we can optionally not use the hidden representations of neighbors from the first stage during the 'message passing'. This will enable each node's hidden representations after Mixup to contribute to the 'message passing' for other nodes, which is likely to be the unwanted inference. In that case, we only need to conduct the second stage of our Mixup method without the first stage. Thus, we call this simpler version as the Mixup method without our two stages. We compare the test accuracy of GCN trained with our Mixup with and without the two stages in Table <ref type="table" target="#tab_4">8</ref>. Mixup without our two stages does not provide improvements, and even causes a decrease in performance, while our two-stage Mixup method achieves consistent enhancements on the test accuracy. The reason is that, without our two stages, the Mixup happening on different nodes affects each other through the 'message passing' across GNN layers, which alters the learned representations of nodes and causes inconsistency between the mixed features and labels. As a result, the GNN models are not trained effectively to offer satisfactory performance. On the other hand, with our two-stage Mixup, we utilize each node's neighbors' representations without Mixup (given by the first stage) to process the 'message passing'. In this way, our method prevents the Mixup for different nodes from affecting affect each other, and the GNN models are thus trained to effectively model the vicinity relation across nodes of different classes <ref type="bibr" target="#b6">[7]</ref>.</p><p>Last but not least, we evaluate how sensitive our Mixup method is to the selection of hyper-parameter value: α, which controls the distribution from which we randomly select the Mixup weights. We present the experimental results on node and graph classification with different α in Table <ref type="table" target="#tab_5">9</ref> and 10 respectively. As we can see, the performance of both GCN and GIN with Mixup is relatively smooth when parameters are within certain ranges, while extremely large or small values of α result in low performances, which should be avoided in practice. Thus, empirically, we choose α = 1 as the default setting in our experiments and show that we achieve satisfactory performance with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Inspired by the success of Mixup, an advanced data augmentation method through sample interpolation for image classification, we explore to propose the Mixup methods for graph learning. In particular, we propose the two-branch Mixup graph convolution method and the two-stage Mixup framework to deal with the irregularity and connectivity of graph data, which is distinct to image data and poses serious challenges for Mixup. For the Mixup on graph classification, we interpolate the complex and diverse graphs in the semantic space. Empirical results show that our Mixup methods act as a dataset independent regularizer to offer better generalization for the popular GNN models on node and graph classification. Future work includes devising Mixup methods for other graph learning tasks beyond supervised learning, such as unsupervised, semi-supervised, and reinforcement learning. Extending our Mixup methods to feature-label extrapolation for more robust GNNs is worth exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) For image classification, the existing Mixup generates synthetic images by interpolating both image pixels and labels. (middle) For node classification, to mix a pair of nodes A (red) and B (blue), we need to mix their receptive field subgraphs. (right) For graph classification, we need to mix the nodes and graph topology of a pair of graphs.</figDesc><graphic url="image-1.png" coords="2,79.02,83.68,453.98,168.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (left) Typically, a GNN layer updates a node's (red) representation by aggregating the representations of its neighbors and itself. (right) We propose the two-branch graph convolution to mix both nodes' attributes and their topology. For a pair of nodes (red and blue) to be mixed, we mix their attributes first. Then at each layer, we conduct the graph convolutions in two branches corresponding to the graph topology of the paired nodes (red and blue) separately, and mix the aggregated representations from two branches before the next layer.</figDesc><graphic url="image-2.png" coords="3,317.96,83.69,240.23,89.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) A GNN model predicts the class of node A (red) by aggregating the nodes (orange) inside node A's receptive field. (b) To Mixup nodes A (red) and B (blue), we should mix the features inside A and B's receptive fields. However, if we conduct Mixup for Node C (orange) and Node D (grey) at the same time, the mixed input features from nodes A and B are perturbed by interference from Node D through Node C, which should be blocked.</figDesc><graphic url="image-4.png" coords="4,79.02,288.91,453.96,136.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Existing GNNs classify all nodes in a mini-batch graph at the same time. (b)We propose the two-stage Mixup method to resolve conflicts between the Mixup on different node pairs. At stage one, we perform the feed-forward as in existing GNNs without Mixup. Then at stage two, we randomly pair the nodes in the mini-batch graph and mix their input attributes. Next, we perform our two-branch Mixup graph convolutions (see Fig.2) for the paired nodes at each layer, where we use each node's neighbors' representations obtained from stage one. This ensures that each node's representations after Mixup do not interfere with the 'message passing' for other nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: We mix the graph-level representations for the Mixup on graph classification, which encodes both nodes' attributes and graph topology.</figDesc><graphic url="image-5.png" coords="6,89.84,83.69,168.18,123.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The training curves of GCN with and without our Mixup methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets for node classification. 'm' stands for multi-label classification, while 's' for singlelabel.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="2">#Classes #Attributes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7 (s)</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6 (s)</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>3 (s)</cell><cell>500</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>7 (s)</cell><cell>500</cell></row><row><cell>Yelp</cell><cell>716,847</cell><cell>6,977,410</cell><cell>100 (m)</cell><cell>300</cell></row><row><cell>Amazon</cell><cell cols="3">1,598,960 132,169,734 107 (m)</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets for graph classification. #Nodes and #Edges denotes the average number of nodes and edges per graph respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Graphs #Nodes #Edges #Classes</cell></row><row><cell>D&amp;D</cell><cell>1,178</cell><cell>284.32</cell><cell>715.66</cell><cell>2</cell></row><row><cell>NCI1</cell><cell>4,110</cell><cell>29.87</cell><cell>32.30</cell><cell>2</cell></row><row><cell>PROTEINS</cell><cell>1,113</cell><cell>39.06</cell><cell>72.82</cell><cell>2</cell></row><row><cell>COLLAB</cell><cell>5,000</cell><cell>74.49</cell><cell>2457.78</cell><cell>3</cell></row><row><cell>IMDB-M</cell><cell>1,500</cell><cell>13.00</cell><cell>65.94</cell><cell>3</cell></row><row><cell>REDDIT-5K</cell><cell>4,999</cell><cell>508.52</cell><cell>594.87</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test Accuracy (%) of transductive node classification. We conduct 100 trials with random weight initialization. The mean and standard deviations are reported.</figDesc><table><row><cell>Method</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>GCN [27]</cell><cell>77.1±1.4</cell><cell cols="2">88.3±0.8 86.4±1.1</cell></row><row><cell>GAT [50]</cell><cell>76.3±0.8</cell><cell cols="2">87.6±0.5 85.7±0.7</cell></row><row><cell>JKNet [61]</cell><cell>78.1±0.9</cell><cell cols="2">89.1±1.2 86.9±1.3</cell></row><row><cell>LGCN [18]</cell><cell>77.5±1.1</cell><cell cols="2">89.0±1.2 86.5±0.6</cell></row><row><cell>GMNN [39]</cell><cell>77.4±1.5</cell><cell cols="2">88.7±0.8 86.7±1.0</cell></row><row><cell>ResGCN [31]</cell><cell>77.9±0.8</cell><cell cols="2">88.1±0.6 87.1±1.2</cell></row><row><cell>DropEdge [40] + GCN</cell><cell>78.1±1.1</cell><cell cols="2">89.2±0.7 87.3±0.6</cell></row><row><cell cols="2">DropEdge [40] + JKNet 79.3±0.7</cell><cell cols="2">89.9±0.8 87.6±0.9</cell></row><row><cell>Mixup + GCN</cell><cell>78.7±0.9</cell><cell cols="2">90.0±0.7 87.9±0.8</cell></row><row><cell>Mixup + JKNet</cell><cell cols="3">80.1±0.8 90.4±0.9 88.3±0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test F1-micro score (%) of inductive node classification. We report mean and standard deviations of 100 trials with random weight initialization. We implement DropEdge and our Mixup method with GraphSAGE-mean and GraphSAINT-GCN.</figDesc><table><row><cell>Method</cell><cell>Flickr</cell><cell>Yelp</cell><cell>Amazon</cell></row><row><cell>GraphSAGE-mean [22]</cell><cell cols="3">50.1±1.1 63.4±0.6 75.8±0.2</cell></row><row><cell>GraphSAGE-LSTM [22]</cell><cell cols="3">50.3±1.3 63.2±0.8 75.7±0.1</cell></row><row><cell>GraphSAGE-pool [22]</cell><cell cols="3">50.0±0.8 63.1±0.5 75.5±0.2</cell></row><row><cell>DropEdge [40] + GraphSAGE</cell><cell cols="3">50.8±0.9 64.1±0.8 76.4±0.1</cell></row><row><cell>Mixup + GraphSAGE</cell><cell cols="3">51.6±0.8 64.6±0.6 77.3±0.1</cell></row><row><cell>GraphSAINT-GCN [65]</cell><cell cols="3">51.1±0.2 65.3±0.3 81.5±0.1</cell></row><row><cell>GraphSAINT-GAT [65]</cell><cell cols="3">50.5±0.1 65.1±0.2 81.5±0.1</cell></row><row><cell>GraphSAINT-JKNet [65]</cell><cell cols="3">51.3±0.5 65.3±0.4 81.6±0.1</cell></row><row><cell cols="4">DropEdge [40] + GraphSAINT 51.7±0.6 65.8±0.7 81.8±0.2</cell></row><row><cell>Mixup + GraphSAINT</cell><cell cols="3">52.4±0.4 66.3±0.4 82.0±0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 :</head><label>8</label><figDesc>Test Accuracy (%) of GCN on the Pubmed dataset and F1-micro score (%) of GraphSAINT-GCN on the Yelp dataset of node classification with and without our two-stage framework.</figDesc><table><row><cell>Method</cell><cell cols="2">two stages Pubmed</cell><cell>∆</cell><cell>Yelp</cell><cell>∆</cell></row><row><cell>GCN [27]</cell><cell>-</cell><cell>86.4±1.1</cell><cell>0</cell><cell>65.3±0.3</cell><cell>0</cell></row><row><cell>Mixup + GCN</cell><cell>w/o w/</cell><cell cols="4">85.8±1.3 -0.6 64.2±0.6 -1.1 87.9±0.8 +1.5 66.3±0.4 +1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 :</head><label>9</label><figDesc>Test Accuracy (%) of node classification given by GCN and GCN with our Mixup method of different α values.</figDesc><table><row><cell>Method</cell><cell cols="2">α Citeseer</cell><cell>Cora</cell><cell>Flickr</cell></row><row><cell>GCN [27]</cell><cell>-</cell><cell>77.1±1.4</cell><cell cols="2">88.3±0.8 51.1±0.2</cell></row><row><cell></cell><cell cols="2">0.2 78.1±0.9</cell><cell cols="2">89.2±0.8 52.0±0.3</cell></row><row><cell></cell><cell cols="2">0.5 78.4±0.8</cell><cell cols="2">89.5±0.7 52.1±0.3</cell></row><row><cell>Mixup + GCN</cell><cell>1</cell><cell cols="3">78.7±0.9 90.0±0.7 52.4±0.4</cell></row><row><cell></cell><cell>2</cell><cell>78.6±1.0</cell><cell cols="2">89.8±0.8 52.8±0.5</cell></row><row><cell></cell><cell>5</cell><cell>78.4±1.2</cell><cell cols="2">89.4±1.1 52.7±0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 :</head><label>10</label><figDesc>Test Accuracy (%) of graph classification given by GIN and GIN with our Mixup method of different α values.</figDesc><table><row><cell>Method</cell><cell>α</cell><cell>D&amp;D</cell><cell cols="2">PROTEINS IMDB-M</cell></row><row><cell>GIN [60]</cell><cell>-</cell><cell>75.4±2.6</cell><cell>73.5±3.8</cell><cell>48.5±3.3</cell></row><row><cell></cell><cell cols="2">0.2 76.1±2.7</cell><cell>74.1±3.6</cell><cell>49.0±3.3</cell></row><row><cell></cell><cell cols="4">0.5 76.5±2.8 74.4±3.4 49.6±3.1</cell></row><row><cell>Mixup + GIN</cell><cell cols="4">1 76.8±2.7 74.3±3.5 49.9±3.2</cell></row><row><cell></cell><cell>2</cell><cell>76.3±2.5</cell><cell>74.0±3.7</cell><cell>49.8±3.0</cell></row><row><cell></cell><cell>5</cell><cell>76.0±2.6</cell><cell>73.7±3.8</cell><cell>49.5±3.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This paper is supported by NUS ODPRT Grant R252-000-A81-133 and Singapore Ministry of Education Academic Research Fund Tier 3 under MOEs official grant number MOE2017-T3-1-007.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use popular graph classification models as the baselines: GRAPHLET <ref type="bibr" target="#b43">[44]</ref> and Weisfeiler-Lehman Kernel (WL) <ref type="bibr" target="#b42">[43]</ref> are classical graph kernel methods, while GCN <ref type="bibr" target="#b26">[27]</ref>, DGCNN <ref type="bibr" target="#b68">[69]</ref>, DiffPool <ref type="bibr" target="#b62">[63]</ref>, EigenPool <ref type="bibr" target="#b33">[34]</ref>, and GIN <ref type="bibr" target="#b59">[60]</ref> are the GNNs with state-of-theart performance in graph classification. We report the average and standard deviation of test accuracy across the 10 folds within the cross-validation on the chemical and social datasets in Table <ref type="table">5</ref> respectively. On the chemical datasets, we observe that our Mixup method improves the test accuracy of GCN by 1.6% on D&amp;D, 1.2% on NCI1, 1.1% on PROTEINS respectively, and enhances GIN by 1.9% on D&amp;D, 1.6% on NCI1, 1.1% on PROTEINS. On the social datasets, our Mixup method improves GCN by more than 1% and enhances GIN by at least 2% on the COLLAB, IMDB-M, and REDDIT-5K datasets in terms of test accuracy. Overall, Mixup achieves substantial improvements for GCN and GIN on both the chemical and social datasets. As a result, Mixup enhances GCN and GIN to outperform all the baseline methods.</p><p>Taking a closer look, we observe that the graph kernel methods, GRAPHLET and WL, generally present worse performance than the GNN methods. This demonstrates the stronger fitting capacity of the advanced neural network models. Mixup generally achieves higher improvements on GIN than that on GCN. The reason is that GIN is a more advanced GNN model proposed for graph classification than GCN. However, the increased learning power of GIN comes with higher risks of over-fitting. Our Mixup method effectively regularizes them by interpolating the graph representations to expand the training set, which reduces their over-fitting tendencies successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Set Sizing</head><p>Over-fitting tends to be more severe when training on smaller datasets. By conducting experiments using a restricted fraction of the available training data, we show that our Mixup method has more significant improvements for smaller training sets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Midas: microcluster-based detector of anomalies in edge streams</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3242" to="3249" />
		</imprint>
	</monogr>
	<note>Kijung Shin, and Christos Faloutsos</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE international conference on data mining (ICDM&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="416" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Are powerful graph neural nets necessary? a dissection on graph classification</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04579</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph Neural Network-Based Anomaly Detection in Multivariate Time Series</title>
		<author>
			<persName><forename type="first">Ailin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved Regularization of Convolutional Neural Networks with Cutout</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of beta distribution and its applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saralees</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Nadarajah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph Star Net for Generalized Multi-Task Learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Haonan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Xiuyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12330</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California . . .</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attpool: Towards hierarchical feature representation in graph convolutional networks via attention mechanism</title>
		<author>
			<persName><forename type="first">Jingjia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6480" to="6489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Amir Hosein Khasahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parsa</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quaid</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09518</idno>
		<title level="m">Memory-based graph networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02850</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning, ICML 2019. International Machine Learning Society (IMLS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6661" to="6670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collective Classification of Network Data</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Classification: Algorithms and Applications</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image labeling on a network: using social-network metadata for image classification</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="828" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Gmnn: Graph markov neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Ikuro</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensuke</forename><surname>Yokoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03229</idno>
		<title level="m">Apac: Augmented pattern classification with neural networks</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icdar</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond</title>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Sarmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02545</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13970</idno>
		<title level="m">Directed graph convolutional network</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph Attention Networks</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Provably Robust Node Classification via Low-Pass Message Passing</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemank</forename><surname>Lamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Detecting Implementation Bugs in Graph Convolutional Network based Node Classifiers</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Ca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beng</forename><forename type="middle">Chin</forename><surname>Ooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="313" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10564</idno>
		<title level="m">GraphCrop: Subgraph Cropping for Graph Classification</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">NodeAug: Semi-Supervised Node Classification with Data Augmentation</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A Comprehensive Survey on Graph Neural Networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Progressive Supervision for Node Classification</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
				<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
