<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Aware Transformer for Graph Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-07">7 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
							<email>dexiong.chen@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biosystems Science and Engineering</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>4058</postCode>
									<settlement>Basel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB Swiss Institute of Bioinformatics</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leslie</forename><surname>O'bray</surname></persName>
							<email>leslie.obray@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biosystems Science and Engineering</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>4058</postCode>
									<settlement>Basel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB Swiss Institute of Bioinformatics</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
							<email>karsten.borgwardt@bsse.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biosystems Science and Engineering</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<postCode>4058</postCode>
									<settlement>Basel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SIB Swiss Institute of Bioinformatics</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Aware Transformer for Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-07">7 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.03036v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and transformers. 1</p><p>1 Our code will be made available upon publication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph neural networks (GNNs) have been established as powerful and flexible tools for graph representation learning, with successful applications in drug discovery <ref type="bibr" target="#b11">(Gaudelet et al., 2021)</ref>, protein design <ref type="bibr" target="#b17">(Ingraham et al., 2019)</ref>, social network analysis <ref type="bibr" target="#b9">(Fan et al., 2019)</ref>, and so on. A large class of GNNs build multilayer models, where each layer operates on the previous layer to generate new representations using a message-passing mechanism <ref type="bibr" target="#b12">(Gilmer et al., 2017)</ref> to aggregate local neighborhood information. While many different message-passing strategies have been proposed, some critical limitations have been uncovered in this class of GNNs. These include the limited expressiveness of GNNs <ref type="bibr" target="#b39">(Xu et al., 2019;</ref><ref type="bibr" target="#b30">Morris et al., 2019)</ref>, as well as known problems such as over-smoothing <ref type="bibr" target="#b25">(Li et al., 2018</ref><ref type="bibr" target="#b23">(Li et al., , 2019;;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr" target="#b32">Oono &amp; Suzuki, 2020)</ref> and over-squashing <ref type="bibr">(Alon &amp; Yahav, 2021)</ref>. Oversmoothing manifests as all node representations converging to a constant after sufficiently many layers, while over-squashing occurs when messages from distant nodes are not effectively propagated through certain "bottlenecks" in a graph, since too many messages get compressed into a single fixed-length vector. Designing new architectures beyond neighborhood aggregation is thus essential to solve these problems.</p><p>Transformers <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, which have shown to be successful in natural language understanding <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>, computer vision <ref type="bibr" target="#b5">(Dosovitskiy et al., 2020)</ref>, and biological sequence modeling <ref type="bibr">(Rives et al., 2021)</ref>, offer the potential to address these issues. Rather than only aggregating local neighborhood information in the message-passing mechanism, the Transformer architecture is able to capture interaction information of any node pair via a single self-attention layer. Moreover, in contrast to GNNs, the Transformer avoids introducing any structural inductive bias at intermediate layers, addressing the expressivity limitation of GNNs. Instead, it encodes structural or positional information about nodes only into input node features, albeit limiting how much information it can learn from the graph structure. Integrating information about the graph structure into the transformer architecture has thus gained growing attention in the graph representation learning field. However, most existing approaches only encode positional relationships between nodes, rather than explicitly encoding the structural relationships. As a result, they may not identify structural similarities between nodes and could fail to model the structural interaction between nodes (see Figure <ref type="figure">1</ref>). This could explain why their performance was dominated by sparse GNNs in several tasks <ref type="bibr">(Dwivedi et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this work, we address the critical question of how to encode structural information into a transformer architecture. Our principal contribution is to introduce a flexible structure-aware self-attention mechanism that explicitly considers the graph structure and thus captures structural interaction between nodes. The resulting class of transformers, which we call the structure-aware transformer (SAT), can provide structure-aware representations of graphs, in contrast to most existing position-aware transformers for graph-structured data. Specifically:</p><p>• We reformulate the self-attention mechanism in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref> as a kernel smoother and extend the original exponential kernel on node features to also account for local structures, by extracting a subgraph representation centered around each node. • We propose several methods for automatically generating the subgraph representations, enabling the resulting kernel smoother to simultaneously capture structural and attributed similarities between nodes. The resulting representations are theoretically guaranteed to be at least as expressive as the subgraph representations. • We demonstrate the effectiveness of SAT models on five graph and node property prediction benchmarks by showing it achieves better performance than state-of-the-art GNNs and transformers. Furthermore, we show how SAT can easily leverage any GNN to compute the subgraph representations and outperform the base GNN, making it an effortless enhancer of any existing GNN. • Finally, we show that we can attribute the performance gains to the structure-aware aspect of our architecture, and showcase how SAT is more interpretable than the classic Transformer with an absolute encoding. We will present the related work and relevant background in Sections 2 and 3 before presenting our method in Section 4 and our experimental findings in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We present here the work most related to ours, namely the work stemming from message passing GNNs, positional representations on graphs, and graph transformers. u and v would receive identical encodings since their shortest paths to all other nodes are the same in both graphs. However, their structures are different, with v forming a triangle with its red neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message</head><p>GCN <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>, which was based on performing convolutions on the graph. <ref type="bibr" target="#b12">Gilmer et al. (2017)</ref> reformulated the early GNNs into a framework of message passing GNNs, which has since then become the predominant framework of GNNs in use today, with extensive examples <ref type="bibr" target="#b13">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b39">Xu et al., 2019;</ref><ref type="bibr" target="#b4">Corso et al., 2020;</ref><ref type="bibr" target="#b16">Hu et al., 2020b;</ref><ref type="bibr" target="#b38">Veličković et al., 2018)</ref>. However, as mentioned above, they suffer from problems of limited expressiveness, over-smoothing, and over-squashing.</p><p>Absolute encoding. Because of the limited expressiveness of GNNs, there has been some recent research into the use of absolute encoding <ref type="bibr" target="#b35">(Shaw et al., 2018)</ref>, which consists of adding or concatenating positional or structural representations to the input node features. While it is often called absolute positional encoding, we refer to it more generally as an absolute encoding to include both positional and structural encoding, which are both important in graph modeling. Absolute encoding primarily considers position or location relationships between nodes. Examples of position-based methods include Laplacian positional encoding <ref type="bibr">(Dwivedi &amp; Bresson, 2021;</ref><ref type="bibr">Kreuzer et al., 2021)</ref>, Weisfeiler-Lehman-based positional encoding <ref type="bibr" target="#b42">(Zhang et al., 2020)</ref>, and random walk positional encoding (RWPE) <ref type="bibr" target="#b24">(Li et al., 2020;</ref><ref type="bibr">Dwivedi et al., 2021)</ref>, while distance-based methods include distances to a predefined set of nodes <ref type="bibr" target="#b41">(You et al., 2019)</ref> and shortest path distances between pairs of nodes <ref type="bibr" target="#b42">(Zhang et al., 2020;</ref><ref type="bibr" target="#b24">Li et al., 2020)</ref>. <ref type="bibr">Dwivedi et al. (2021)</ref> extend these ideas by using a trainable absolute encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph transformers.</head><p>While the absolute encoding methods listed above can be used with message passing GNNs, they also play a crucial role in the (graph) transformer architecture. Graph Transformer <ref type="bibr">(Dwivedi &amp; Bresson, 2021)</ref> provided an early example of how to generalize the Transformer architecture to graphs, using Laplacian eigenvectors as an absolute encoding and computing attention on the immediate neighborhood of each node, rather than on the full graph. SAN <ref type="bibr">(Kreuzer et al., 2021</ref>) also used the Laplacian eigenvectors for computing an absolute encoding, but computed attention on the full graph, while distinguishing between true and created edges. Many graph transformer methods also use a relative encoding <ref type="bibr" target="#b35">(Shaw et al., 2018)</ref> in addition to absolute encoding. This strategy incorporates representations of the relative position or distances between nodes on the graph directly into the self-attention mechanism, as opposed to the absolute encoding which is only applied once to the input node features. <ref type="bibr" target="#b28">Mialon et al. (2021)</ref> propose a relative encoding by means of kernels on graphs to bias the self-attention calculation, which is then able to incorporate positional information into transformers via the choice of kernel function. Other recent work seeks to incorporate structural information into the graph transformer, for example by encoding some carefully selected graph theoretic properties such as centrality measures and shortest path distances as positional representations <ref type="bibr">(Ying et al., 2021)</ref> or by using GNNs to integrate the graph structure <ref type="bibr" target="#b34">(Rong et al., 2020;</ref><ref type="bibr">Jain et al., 2021;</ref><ref type="bibr" target="#b28">Mialon et al., 2021)</ref>.</p><p>In this work, we combine the best of both worlds from message passing GNNs and from the transformer architecture. We incorporate both an absolute as well as a novel relative encoding that explicitly incorporates the graph structure, thereby designing a transformer architecture that takes both local and global information into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In the following, we refer to a graph as G = (V, E, X), where the node attributes for node u ∈ V is denoted by x u ∈ X ⊂ R d and the node attributes for all nodes are stored in X ∈ R n×d for a graph with n nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformers on graphs</head><p>While GNNs use the graph structure explicitly, transformers remove that explicit structure, and instead infer relations between nodes by leveraging the node attributes. In this sense, the Transformer <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> ignores the graph structure and rather considers the graph as a (multi-) set of nodes, and uses the self-attention mechanism to infer the similarity between nodes. The Transformer itself is composed of two main blocks: a self-attention module followed by a feed-forward neural network. In the self-attention module, the input node features X are first projected to query (Q), key (K) and value (V) matrices through a linear projection such that Q = XW Q , K = XW K and V = XW V respectively. We can compute the self-attention via Attn(X) := softmax(</p><formula xml:id="formula_0">QK T √ d out )V ∈ R n×d out ,<label>(1)</label></formula><p>where d out refers to the dimension of Q, and W Q , W K , W V are trainable parameters. It is common to use multi-head attention, which concatenates multiple instances of Eq. ( <ref type="formula" target="#formula_0">1</ref>) and has shown to be effective in practice <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref>. Then, the output of the self-attention is followed by a skip-connection and a feed-forward network (FFN), which jointly compose a transformer layer, as shown below:</p><formula xml:id="formula_1">X = X + Attn(X), X = FFN(X ) := ReLU(X W 1 )W 2 .</formula><p>(2)</p><p>Multiple layers can be stacked to form a transformer model, which ultimately provides node-level representations of the graph. As the self-attention is equivariant to permutations of the input nodes, the Transformer will always generate the same representations for nodes with the same attributes regardless of their locations and surrounding structures in the graph. It is thus necessary to incorporate such information into the Transformer, generally via absolute encoding.</p><p>Absolute encoding. Absolute encoding refers to adding or concatenating the positional or structural representations of the graph to the input node features before the main transformer model, such as the Laplacian positional encoding <ref type="bibr">(Dwivedi &amp; Bresson, 2021)</ref> and RWPE <ref type="bibr">(Dwivedi et al., 2021)</ref>. The main shortcoming of these encoding methods is that they generally do not provide a measure of the structural similarity between nodes and their neighborhoods.</p><p>Self-attention as kernel smoothing. As noticed by <ref type="bibr" target="#b28">Mialon et al. (2021)</ref>, the self-attention in Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be rewritten as a kernel smoother</p><formula xml:id="formula_2">Attn(x v ) = ∑ u∈V κ exp (x v , x u ) ∑ w∈V κ exp (x v , x w ) f (x u ), ∀v ∈ V,<label>(3)</label></formula><p>where f (x) = W V x is the linear value function and κ exp is a (non-symmetric) exponential kernel on R d × R d parameterized by W Q and W K : where ., . is the dot product on R d . With this form, <ref type="bibr" target="#b28">Mialon et al. (2021)</ref> propose a relative positional encoding strategy via the product of this kernel and a diffusion kernel on the graph, which consequently captures the positional similarity between nodes. However, this method is only position-aware, in contrast to our structure-aware encoding that will be presented in Section 4.</p><formula xml:id="formula_3">κ exp (x, x ) := exp W Q x, W K x / d out ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structure-Aware Transformer</head><p>In this section, we will describe how to encode the graph structure into the self-attention mechanism and provide a class of transformer models based on this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Structure-aware self-attention</head><p>As presented above, self-attention in the Transformer can be rewritten as a kernel smoother where the kernel is a trainable exponential kernel defined on node features, which only captures attributed similarity between a pair of nodes. The problem with this kernel smoother is that it cannot filter out nodes that are structurally different from the node of interest when they have the same or similar node features. In order to also incorporate the structural similarity between nodes, we consider a more generalized kernel that additionally accounts for the local substructures around each node. By introducing a set of subgraphs centered at each node, we define our structure-aware attention as below:</p><formula xml:id="formula_4">SA-attn(v) := ∑ u∈V κ graph (S G (v), S G (u)) ∑ w∈V κ graph (S G (v), S G (w)) f (x u ),<label>(5)</label></formula><p>where S G (v) denotes a subgraph in G centered at a node v associated with node features X and κ graph can be any kernel that compares a pair of subgraphs. This new self-attention function not only takes the attributed similarity into account but also the structural similarity between subgraphs. It thus generates more expressive node representations than the original self-attention, as we will show in Section 4.4. Moreover, this self-attention is no longer equivariant to any permutation of nodes but only to nodes whose features and subgraphs coincide, which is a desirable property.</p><p>In the rest of the paper, we will consider the following form of κ graph that already includes a large class of expressive and computationally tractable models:</p><formula xml:id="formula_5">κ graph (S G (v), S G (u)) = κ exp (ϕ(v, G), ϕ(u, G)),<label>(6)</label></formula><p>where ϕ(u, G) is a structure extractor that extracts vector representations of some subgraph centered at u with node features X. We provide several alternatives of the structure extractor below. It is worth noting that our structure-aware self-attention is flexible enough to be combined with any model that generates representations of subgraphs, including GNNs and (differentiable) graph kernels. For notational simplicity, we assume there are no edge attributes, but our method can easily incorporate edge attributes as long as the structure extractor can accommodate them.</p><p>k-subtree GNN extractor. A straightforward way to extract local structural information at node u is to apply any existing GNN model to the input graph with node features X and take the output node representation at u as the subgraph representation at u. More formally, if we denote by GNN</p><formula xml:id="formula_6">(k)</formula><p>G an arbitrary GNN model with k layers applied to G with node features X, then</p><formula xml:id="formula_7">ϕ(u, G) = GNN (k) G (u). (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>This extractor is able to represent the k-subtree structure rooted at u <ref type="bibr" target="#b39">(Xu et al., 2019)</ref>. While this class of structure extractors is fast to compute and can flexibly leverage any existing GNN, they cannot be more expressive than the Weisfeiler-Lehman test due to the expressiveness limitation of message passing GNNs <ref type="bibr" target="#b39">(Xu et al., 2019)</ref>. In practice, a small value of k already leads to good performance, while not suffering from over-smoothing or over-squashing.</p><p>k-subgraph GNN extractor. A more expressive extractor is to use a GNN to directly compute the representation of the entire k-hop subgraph centered at u rather than just the node representation u.</p><p>The k-subgraph GNN extractor aggregates the updated node representations of all nodes within the k-hop neighborhood using a pooling function such as summation. Formally, if we denote by N k (u) the k-hop neighborhood of node u including itself, the representation of a node u is:</p><formula xml:id="formula_9">ϕ(u, G) = ∑ v∈N k (u) GNN (k) G (v).<label>(8)</label></formula><p>We observe that prior to the pooling function, the k-subgraph GNN extractor is equivalent to using the k-subtree GNN extractor within each k-hop subgraph. So as to capture the attributed similarity as well as structural similarity, we augment the node representation from k-subgraph GNN extractor with the original node features via concatenation. While this extractor provides more expressive subgraph representations than the k-subtree extractor, it requires enumerating all k-hop subgraphs, and consequently does not scale as well as the k-subtree extractor to large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other structure extractors.</head><p>Finally, we present a list of other potential structure extractors for different purposes. One possible choice is to directly learn a number of "hidden graphs" as the "anchor subgraphs" to represent subgraphs for better model interpretability, by using the concepts introduced in Nikolentzos &amp; Vazirgiannis (2020). While <ref type="bibr" target="#b31">Nikolentzos &amp; Vazirgiannis (2020)</ref> obtain a vector representation of the input graph by counting the number of matching walks between the whole graph and each of the hidden graphs, one could extend this to the node level by comparing the hidden graphs to the k-hop subgraph centered around each node. The adjacency matrix of the hidden graphs is a trainable parameter in the network, thereby enabling end-to-end training to identify which subgraph structures are predictive. Then, for a trained model, visualizing the learned hidden graphs provides useful insights about the structural motifs in the dataset.</p><p>Furthermore, more domain-specific GNNs could also be used to extract potentially more expressive subgraph representations. For instance, <ref type="bibr">Bodnar et al. (2021)</ref> recently proposed a new kind of message passing scheme operating on regular cell complexes which benefits from provably stronger expressivity for molecules. Our self-attention mechanism can fully benefit from the development of more domainspecific and expressive GNNs. Finally, another possible structure extractor is to use a non-parametric graph kernel (e.g. a Weisfeiler-Lehman graph kernel) on the k-hop subgraphs centered around each node. This provides a flexible way to combine graph kernels and deep learning, which might offer new theoretical insights into the link between the self-attention and kernel methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structure-aware transformer</head><p>Having defined our structure-aware self-attention function, the other components of the structureaware transformer follow the Transformer architecture as described in Section 3.1; see Figure <ref type="figure" target="#fig_1">2</ref> a visual overview. Specifically, the self-attention function is followed by a skip-connection, a FFN and two normalization layers before and after the FFN. In addition, we also include the degree factor in the skip-connection, which was found useful for reducing the overwhelming influence of highly connected graph components <ref type="bibr" target="#b28">(Mialon et al., 2021)</ref>, i.e.,</p><formula xml:id="formula_10">x v = x v + 1/ d v SA-attn(v),<label>(9)</label></formula><p>where d v denotes the degree of node v. After a transformer layer, we obtain a new graph with the same structure but different node features G = (V, E, X ), where X corresponds to the output of the transformer layer. Finally, for graph property prediction, there are various ways to aggregate node-level representations into a graph representation, such as by taking the average or sum. Alternatively, one can use the embedding of a virtual [CLS] node <ref type="bibr">Jain et al. (2021)</ref> that is attached to the input graph without any connectivity to other nodes. We compare these approaches in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Combination with absolute encoding</head><p>While the self-attention in ( <ref type="formula" target="#formula_4">5</ref>) is structure-aware, most absolute encoding techniques are only positionaware and could therefore provide complementary information. Indeed, we find that the combination leads to further performance improvements, which we show in Section 5. We choose to use the RWPE <ref type="bibr">(Dwivedi et al., 2021)</ref>, though any other absolute positional representations, including learnable ones, can also be used.</p><p>We further argue that only using absolute positional encoding with the Transformer would exhibit a too relaxed structural inductive bias which is not guaranteed to generate similar node representations even if two nodes have similar local structures. This is due to the fact that distance or Laplacian-based positional representations generally serve as structural or positional signatures but do not provide a measure of structural similarity between nodes, especially in the inductive case where two nodes are from different graphs. This is also empirically affirmed in Section 5 by their relatively worse performance without using our structural encoding. In contrast, the subgraph representations used in the structure-aware attention can be tailored to measure the structural similarity between nodes, and thus generate similar node-level representations if they possess similar attributes and surrounding structures. We can formally state this in the following theorem: Theorem 1. Assume that f is a Lipschitz mapping with the Lipschitz constant denoted by Lip( f ) and the structure extractor ϕ is bounded by a constant C ϕ on the space of subgraphs. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ) with the same number of nodes |V| = |V |, the distance between their representations after the structure-aware attention is bounded by:</p><formula xml:id="formula_11">SA-attn(v) − SA-attn(v ) ≤ C 1 [ h v − h v + D(H, H )] + C 2 D(X, X ), (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where C 1 , C 2 &gt; 0 are constants depending on |V|, Lip( f ), C ϕ and spectral norms of the parameters in SA-attn, whose expressions are given in the Appendix, and h w := ϕ(w, G) denotes the subgraph representation at node w for any w ∈ V and h w := ϕ(w , G ) similarly, and H = (h w ) w∈V and H = (h w ) w ∈V denote the multiset of subgraph representations in G and G respectively. Denoting by Π(V, V ) the set of permutations from V to V , D is an optimal matching metric between two multisets of representations with the same cardinality, defined as</p><formula xml:id="formula_13">D(X, X ) := inf π∈Π(V,V ) sup w∈V x w − x π(w) .</formula><p>The proof is provided in the Appendix. The metric D is an optimal matching metric between two multisets which measures how different they are. This theorem shows that two node representations from the SA-attn are similar if the graphs that they belong to have similar multisets of node features and subgraph representations overall, and at the same time, the subgraph representations at these two nodes are similar. In particular, if two nodes belong to the same graph, i.e. G = G , then the second and last terms on the right side of Eq. ( <ref type="formula" target="#formula_11">10</ref>) are equal to zero and the distance between their representations is thus constrained by the distance between their corresponding subgraph representations. However, for transformers with absolute positional encoding, the distance between two node representations is not constrained by their structural similarity, as the distance between two positional representations does not necessarily characterize how two nodes are structurally similar. Despite stronger inductive biases, we will show that our model is still sufficiently expressive in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Expressivity analysis</head><p>Expressive power of graph transformers compared to classic GNNs has hardly been studied since the soft structural inductive bias introduced in absolute encoding is generally hard to characterize. Thanks to the unique design of our SAT, which relies on a subgraph structure extractor, it becomes possible to study the expressiveness of the output representations. More specifically, we formally show that the node representation from a structure-aware attention layer is at least as expressive as its subgraph representation given by the structure extractor, following the injectivity of the attention function with respect to the query: Theorem 2. Assume that the space of node attributes X is countable. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ), assume that there exist a node u 1 in V such that x u 1 = x w for any w ∈ V and a node u 2 in V such that its subgraph representation ϕ(u 2 , G) = ϕ(w, G) for any w ∈ V. Then, there exists a set of parameters and a mapping f : X → R d out such that their representations after the </p><formula xml:id="formula_14">= ϕ(v , G ).</formula><p>Note that the assumptions made in the theorem are mild as one can always add some absolute encoding or random noise to make the attributes of one node different from all other nodes, and similarly for subgraph representations. The countable assumption on X is generally adopted for expressivity analysis of GNNs (e.g. <ref type="bibr" target="#b39">Xu et al. (2019)</ref>). We assume f to be any mapping rather than just a linear function as in the definition of the self-attention function since it can be practically approximated by a FFN in multi-layer transformers through the universal approximation theorem <ref type="bibr" target="#b14">(Hornik, 1991)</ref>. Theorem 2 suggests that if the structure extractor is sufficiently expressive, the resulting SAT model can also be at least equally expressive. Furthermore, more expressive extractors could lead to more expressively powerful SAT models and thus better prediction performance, which is also empirically confirmed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate SAT models versus several SOTA methods for graph representation learning, including GNNs and transformers, on five graph and node prediction tasks, as well as analyze the different components of our architecture to identify what drives the performance. In summary, we discovered the following aspects about SAT:</p><p>• The structure-aware framework achieves SOTA performance on graph and node classification tasks, outperforming SOTA graph transformers and sparse GNNs. • Both instances of the SAT, namely k-subtree and k-subgraph SAT, always improve upon the base GNN it is built upon, highlighting the improved expressiveness of our structure-aware approach. • We show that incorporating the structure via our structure-aware attention brings a notable improvement relative to the vanilla transformer with RWPE that just uses node attributes similarity instead of also incorporating structural similarity. We also show that a small value of k already leads to good performance, while not suffering from over-smoothing or over-squashing. • We show that choosing a proper absolute positional encoding and a readout method improves performance, but to a much lesser extent than incorporating the structure into the approach.</p><p>Table <ref type="table">3</ref>.: Since SAT uses a GNN to extract structures, we compare the performance of the original sparse GNN to SAT which uses that GNN ("base GNN"). Across different choices of GNNs, we observe that both k-subtree and k-subgraph SAT always outperform the original sparse GNN it uses. The evaluation metrics are the same as in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZINC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and experimental setup</head><p>We assess the performance of our method with five medium to large benchmark datasets for node and graph property prediction, including ZINC <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>, CLUSTER <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>, PATTERN <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref>. We compare our method to the following GNNs: GCN <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b13">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b38">(Veličković et al., 2018)</ref>, GIN <ref type="bibr" target="#b39">(Xu et al., 2019)</ref> and PNA <ref type="bibr" target="#b4">(Corso et al., 2020)</ref>. Our comparison partners also include several recently proposed transformers on graphs, including the original Transformer with RWPE <ref type="bibr">(Dwivedi et al., 2021)</ref>, Graph Transformer <ref type="bibr">(Dwivedi &amp; Bresson, 2021)</ref>, SAN <ref type="bibr">(Kreuzer et al., 2021)</ref>, Graphormer <ref type="bibr">(Ying et al., 2021)</ref> and GraphTrans <ref type="bibr">(Jain et al., 2021)</ref>, a model that uses the vanilla Transformer on top of a GNN. All results for the comparison methods are either taken from the original paper or from <ref type="bibr" target="#b7">Dwivedi et al. (2020)</ref> if not available. We consider k-subtree and k-subgraph SAT equipped with different GNN extractors, including GCN, GIN, GraphSAGE and PNA.</p><p>For OGBG-PPA and OGBG-CODE2, we do not run experiments for k-subgraph SAT models due to large memory requirements. Full details on the datasets and experimental setup are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to state-of-the-art methods</head><p>We show the performance of SATs compared to other GNNs and transformers in Table <ref type="table" target="#tab_1">1 and 2</ref>. SAT models consistently outperform SOTA methods on these datasets, showing its ability to combine the benefits of both GNNs and transformers. In particular, for large OGB datasets, our SAT models outperform SOTA methods by a large margin despite a relatively small number of parameters and minimal hyperparameter tuning, which will put it at the first place in the leaderboard for OGBG-CODE2. Figure <ref type="figure">3</ref>.: We provide an analysis of the different drivers of performance in SAT on the ZINC dataset (lower is better). In Figure <ref type="figure">3a</ref>, we show how changing the size of k affects performance (k=0 is equivalent to a vanilla transformer that is not structure-aware). Figure <ref type="figure">3b</ref> shows the effect of different absolute encoding methods, and Figure <ref type="figure">3c</ref> shows the effect of different readout methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">SAT models vs. sparse GNNs</head><p>large performance gains to its base GNN counterpart, making it a systematic enhancer of any GNN model. Furthermore, PNA, which is the most expressive GNN we considered, has consistently the best performance when used with SAT, empirically validating our theoretical finding in Section 4.4. k-subgraph SAT also outperforms or performs equally as k-subtree SAT in almost all the cases, showing its superior expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Hyperparameter studies</head><p>While Table <ref type="table">3</ref> showcases the added value of the SAT relative to sparse GNNs, we now dissect the components of SAT on the ZINC dataset to identify which aspects of the architecture bring the biggest performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of k in SAT.</head><p>The key contribution of SAT is its ability to explicitly incorporate structural information in the self-attention. Here, we seek to demonstrate that this information provides crucial predictive information, and study how the choice of k affects the results. Figure <ref type="figure">3a</ref> shows how the test MAE is impacted by varying k for k-subtree and k-subgraph extractors using PNA on the ZINC dataset. All models use the RWPE. k = 0 corresponds to the vanilla Transformer only using absolute positional encoding, i.e. not using structure. We find that incorporating structural information leads to substantial improvement in performance, with optimal performance around k = 3 for both k-subtree and k-subgraph extractors. As k increases beyond k = 4, the performance in k-subtree extractors deteriorated, which is consistent with the observed phenomenon that GNNs work best in shallower networks <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>. We observe that k-subgraph does not suffer as much from this issue, underscoring a new aspect of its usefulness. On the other hand, k-subtree extractors are more computationally efficient and scalable to larger OGB datasets.</p><p>Effect of absolute encoding. We assess here whether the absolute encoding brought complementary information to SAT. In Figure <ref type="figure">3b</ref>, we conduct an ablation study showing the results of SAT with and without absolute positional encoding, including RWPE and Laplacian PE <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>. Our SAT with a positional encoding outperforms its counterpart without it, confirming the complementary nature of the two encodings. However, we also note that the performance gain brought by the absolute encoding is far less than the gain obtained by using our structure-aware attention, as shown in Figure <ref type="figure">3a</ref>, emphasizing that our structure-aware attention is the more important aspect of the model.</p><p>Comparison of readout methods. Finally, we compare the performance of SAT models using different readout methods for aggregating node-level representations on the ZINC dataset in Figure <ref type="figure">3c</ref>, including the CLS pooling discussed in Section 4.2. Unlike the remarkable influence of the readout method in GNNs <ref type="bibr" target="#b39">(Xu et al., 2019)</ref>, we observe very little impact in SAT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Model interpretation</head><p>In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We introduced the SAT model, which successfully incorporates structural information into the Transformer architecture and overcomes the limitations of the absolute encoding. In addition to SOTA empirical performance, SAT also provides better interpretability than the Transformer. As mentioned above, k-subgraph SAT has higher memory requirements than k-subtree SAT, which can restrict its applicability if access to high memory GPUs is restricted. We see the main limitation of SAT is that it suffers from the same drawbacks as the Transformer, namely the quadratic complexity of the selfattention computation, but will benefit from the recent line of research on efficient transformers <ref type="bibr" target="#b36">(Tay et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides both theoretical and experimental materials and is organized as follows: Section A provides a more detailed background on graph neural networks. Section B presents proofs of Theorem 1 and 2. Section C provides experimental details and additional results. Section D provides details on the model interpretation and additional visualization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background on Graph Neural Networks</head><p>The overarching idea of a graph neural network is to iteratively update a node's embedding by incorporating information sent from its neighbors. <ref type="bibr" target="#b39">Xu et al. (2019)</ref> provide a general framework of the steps incorporated in this process by generalizing the different frameworks into AGGREGATE, COMBINE and READOUT steps. The various flavors of GNNs can be typically understood as variations within these three functions. For a given layer l, the AGGREGATE step aggregates (e.g. using the sum or mean) the representations of the neighbors of a given node, which is then combined with the given node's representation from the previous layer in the COMBINE step. This is followed by a non-linear function, such as ReLU, and the updated node representations are then passed to the next layer. These two steps are repeated for as many layers as there are in the network. It is worth noting that the output of these two steps provides representations of nodes which accounts for local sub-structures of size only increased by one, which would thus require a very deep network to capture interactions between the given node and all other nodes (the depth should not be smaller than the diameter of the graph). At the end of the network, the READOUT function provides a pooling function to convert the representations to the appropriate output-level granularity (e.g. node-level or graph-level). Both the AGGREGATE and READOUT steps must be invariant to node permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Controllability of the representations from the structure-aware attention</head><p>Theorem 1. Assume that f is a Lipschitz mapping with the Lipschitz constant denoted by Lip( f ) and the structure extractor ϕ is bounded by a constant C ϕ on the space of subgraphs. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ) with the same number of nodes |V| = |V | = n, the distance between their representations after the structure-aware attention is bounded by:</p><formula xml:id="formula_15">SA-attn(v) − SA-attn(v ) ≤ C 1 [ h v − h v + D(H, H )] + C 2 D(X, X ),<label>(11)</label></formula><p>where h w := ϕ(w, G) denotes the subgraph representation at node w for any w ∈ V and h w := ϕ(w , G ) similarly, and H = (h w ) w∈V and H = (h w ) w ∈V denote the multiset of subgraph representations in G and G respectively. Denoting by Π(V, V ) the set of permutations between V and V , D is a matching metric between two multisets of representations with the same cardinality, defined as</p><formula xml:id="formula_16">D(X, X ) := inf π∈Π(V,V ) sup w∈V x w − x π(w) .</formula><p>C 1 and C 2 are constants given by:</p><formula xml:id="formula_17">C 1 = 2 d out nLip( f )C ϕ W Q ∞ W K ∞ , C 2 = Lip( f ).</formula><p>Proof. Let us denote by</p><formula xml:id="formula_18">z v = ( W Q h v , W K h w ) w∈V ∈ R n , z v = ( W Q h v , W K h w ) w ∈V ∈ R n ,</formula><p>and by softmax(z) ∈ R n for any z ∈ R n with its i-th coefficient</p><formula xml:id="formula_19">softmax(z) i = exp(z i / √ d out ) ∑ n j=1 exp(z j / √ d out ) .</formula><p>Then, we have</p><formula xml:id="formula_20">SA-Attn(v) − SA-Attn(v ) = ∑ w∈V softmax(z v ) w f (x w ) − ∑ w ∈V softmax(z v ) w f (x w ) = ∑ w∈V (softmax(z v ) w − softmax(z v ) π(w) ) f (x w ) + ∑ w∈V softmax(z v ) π(w) f (x w ) − ∑ w ∈V softmax(z v ) w ( f (x w )) ≤ ∑ w∈V (softmax(z v ) w − softmax(z v ) π(w) ) f (x w ) + ∑ w ∈V softmax(z v ) w ( f (x π −1 (w ) ) − f (x w ))</formula><p>where π : V → V is an arbitrary permutation and we used the triangle inequality. Now we need to bound the two terms respectively. We first bound the second term:</p><formula xml:id="formula_21">∑ w ∈V softmax(z v ) w ( f (x π −1 (w ) ) − f (x w )) ≤ ∑ w ∈V softmax(z v ) w f (x π −1 (w ) ) − f (x w ) ≤ ∑ w ∈V softmax(z v ) w Lip( f ) x π −1 (w ) − x w = Lip( f ) ∑ w ∈V softmax(z v ) w x π −1 (w ) − x w ≤ Lip( f ) sup w ∈V x π −1 (w ) − x w = Lip( f ) sup w∈V x w − x π(w)</formula><p>where the first inequality is a triangle inequality, the second inequality uses the Lipschitzness of f . And for the first term, we can upper-bound it by</p><formula xml:id="formula_22">∑ w∈V (softmax(z v ) w − softmax(z v ) π(w) ) f (x w ) ≤ softmax(z v ) − softmax((z v ) π ) ∑ w∈V f (x w ) 2 ≤ 1 √ d out z v − (z v ) π √ nLip( f ),</formula><p>where by abuse of notation, (z) π ∈ R n denotes the vector whose w-th entry is z π(w) for any z ∈ R n . The first inequality comes from a simple matrix norm inequality, and the second inequality uses the fact that softmax function is 1/ √ d out -Lipschitz (see e.g. <ref type="bibr" target="#b10">Gao &amp; Pavel (2017)</ref>). Then, we have</p><formula xml:id="formula_23">z v − (z v ) π ) 2 = ∑ w∈V W Q h v , W K h w − W Q h v , W K h π(w) 2 = ∑ w∈V W Q h v , W K (h w − h π(w) ) + W Q (h v − h v ), W K h π(w) 2 ≤ 2 ∑ w∈V W Q h v , W K (h w − h π(w) ) 2 + W Q (h v − h v ), W K h π(w) 2 ≤ 2 ∑ w∈V W Q h v 2 W K (h w − h π(w) ) 2 + W Q (h v − h v ) 2 W K h π(w) 2 ≤ 2 ∑ w∈V C 2 ϕ W Q 2 ∞ W K 2 ∞ h w − h π(w) 2 + W Q 2 ∞ h v − h v 2 C 2 ϕ W K 2 ∞ ≤ 2nC 2 ϕ W Q 2 ∞ W K 2 ∞ h v − h v 2 + sup w∈V h w − h π(w) 2 ,</formula><p>where the first inequality comes from (a + b) 2 ≤ 2(a 2 + b 2 ), the second one uses the Cauchy-Schwarz inequality and the third one uses the definition of spectral norm and the bound of the structure extractor function. Then, we obtain the following inequality</p><formula xml:id="formula_24">∑ w∈V (softmax(z v ) w − softmax(z v ) π(w) ) f (x w ) ≤ 2 d out nLip( f )C ϕ W Q ∞ W K ∞ h v − h v + sup w∈V h w − h π(w)</formula><p>By combining the upper bounds of the first and the second term, we obtain an upper bound for the distance between the structure-aware attention representations:</p><formula xml:id="formula_25">SA-attn(v) − SA-attn(v ) ≤ C 1 h v − h v + sup w∈V h w − h π(w) + C 2 sup w∈V x w − x π(w) ,</formula><p>for any permutation π ∈ Π(V, V ), where</p><formula xml:id="formula_26">C 1 = 2 d out nLip( f )C ϕ W Q ∞ W K ∞ C 2 = Lip( f ).</formula><p>Finally, by taking the infimum over the set of permutations, we obtain the inequality in the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Expressivity Analysis</head><p>Here, we assume that f can be any continuous mapping and it is approximated by an MLP network through the universal approximation theorem <ref type="bibr" target="#b14">(Hornik, 1991)</ref> in practice.</p><p>Theorem 2. Assume that the space of node attributes X is countable. For any pair of nodes v and v in two graphs G = (V, E, X) and G = (V , E , X ), assume that there exists a node u 1 in V such that x u 1 = x w for any w ∈ V and a node u 2 in V such that its subgraph representation ϕ(u 2 , G) = ϕ(w, G) for any w ∈ V.</p><p>Then, there exists a set of parameters and a mapping f : X → R d out such that their representations after the structure-aware attention are different, i.e. SA-attn(v) = SA-attn(v ), if their subgraph representations are different, i.e. ϕ(v, G) = ϕ(v , G ).</p><p>Proof. This theorem amounts to showing the injectivity of the original dot-product attention with respect to the query, that is to show</p><formula xml:id="formula_27">Attn(h v , x v , G) = ∑ u∈V κ exp (h v , h u ) ∑ w∈V κ exp (h v , h w ) f (x u )</formula><p>is injective in h v , where</p><formula xml:id="formula_28">κ exp (h, h ) := exp W Q h + b Q , W K h + b K / d out . (<label>12</label></formula><formula xml:id="formula_29">)</formula><p>Here we consider the offset terms that were omitted in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Let us prove the contrapositive of the theorem. We assume that Attn(h v , x v , G) = Attn(h v , x v , G ) for any set of parameters and any mapping f and want to show that</p><formula xml:id="formula_30">h v = h v .</formula><p>Without loss of generality, we assume that G and G have the same number of nodes, that is |V| = |V | = n. Otherwise, one can easily add some virtual isolated nodes to the smaller graph. Now if we take W Q = W K = 0, all the softmax coefficients will be identical and we have</p><formula xml:id="formula_31">∑ w∈V f (x w ) = ∑ w ∈V f (x w ).</formula><p>Thus, by Lemma 5 of <ref type="bibr" target="#b39">Xu et al. (2019)</ref>, there exists a mapping f such that the multisets X and X are identical.</p><p>As a consequence, we can re-enumerate the nodes in two graphs by a sequence V (by abuse of notation, we keep using V here) such that x u = x u for any u ∈ V. Then, we can rewrite the equality Attn</p><formula xml:id="formula_32">(h v , x v , G) = Attn(h v , x v , G ) as ∑ u∈V κ exp (h v , h u ) ∑ w∈V κ exp (h v , h w ) − κ exp (h v , h u ) ∑ w∈V κ exp (h v , h w ) f (x u ) = 0.</formula><p>Now since there exists a node u 1 in V such that its attributes are different from all other nodes, i.e.</p><p>x u 1 = x w for any w ∈ V, we can find a mapping f such that f (x u 1 ) is not in the span of ( f (x w )) w∈V,w =u 1 . Then, by their independence we have</p><formula xml:id="formula_33">κ exp (h v , h u 1 ) ∑ w∈V κ exp (h v , h w ) = κ exp (h v , h u 1 ) ∑ w∈V κ exp (h v , h w ) , for any W Q , W K , b Q and b K .</formula><p>On the one hand, if we take W Q = 0, we have for any</p><formula xml:id="formula_34">W K , b Q and b K that exp ( b Q , W K h u 1 + b K / √ d out ) ∑ w∈V exp ( b Q , W K h w + b K / √ d out ) = exp ( b Q , W K h u 1 + b K / √ d out ) ∑ w∈V exp ( b Q , W K h w + b K / √ d out ) .</formula><p>On the other hand if we take b Q = 0 we have for any</p><formula xml:id="formula_35">W Q , W K and b K that exp ( W Q h v , W K h u 1 + b K / √ d out ) ∑ w∈V exp ( W Q h v , W K h w + b K / √ d out ) = exp ( W Q h v , W K h u 1 + b K / √ d out ) ∑ w∈V exp ( W Q h v , W K h w + b K / √ d out ) = exp ( W Q h v , W K h u 1 + b K / √ d out ) ∑ w∈V exp ( W Q h v , W K h w + b K / √ d out ) ,</formula><p>where the second equality is obtained by replacing b Q with W Q h v in the above equality. Then, we can rewrite the above equality as below:</p><formula xml:id="formula_36">∑ w∈V exp W Q h v , W K (h w − h u 1 ) √ d out = ∑ w∈V exp W Q h v , W K (h w − h u 1 ) √ d out .</formula><p>If we denote by φ : R d out → H the feature mapping associated with the dot product kernel κ exp (t, t ) = exp( t, t / √ d out ) and H the correspond reproducing kernel Hilbert space, we then have for any W Q and W K that</p><formula xml:id="formula_37">φ(W Q h v ) − φ(W Q h v ), ∑ w∈V φ(W K (h w − h u 1 )) H = 0.</formula><p>Since by assumption there exists a u 2 ∈ V such that h u 2 − h u 1 = 0 and κ exp is a universal kernel <ref type="bibr" target="#b29">(Micchelli et al., 2006)</ref></p><formula xml:id="formula_38">, W K → φ(W K (h u 2 − h u 1 )) is dense in H and we have φ(W Q h v ) = φ(W Q h v ). We can then conclude, by the injectivity of φ, that W Q h v = W Q h v ,</formula><p>for any W Q , and thus h v = h v . Now by taking h v = ϕ(v, G) and h v = ϕ(v , G ), we obtain the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Details and Additional Results</head><p>In this section, we provide implementation details and additional experimental results. Our code will be released upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Computation details</head><p>All experiments were performed on a shared GPU cluster equipped with GTX1080, GTX1080TI, GTX2080TI and TITAN RTX. About 20 of these GPUs were used simultaneously, and the total computational cost of this research project was about 1k GPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Datasets description</head><p>We provide details of the datasets used in our experiments, including ZINC <ref type="bibr" target="#b18">(Irwin et al., 2012)</ref>, CLUS-TER <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>, PATTERN <ref type="bibr" target="#b7">(Dwivedi et al., 2020)</ref>, OGBG-PPA <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref> and OGBG-CODE2 <ref type="bibr" target="#b15">(Hu et al., 2020a)</ref>. For each dataset, we follow their respective training protocols and use the standard train/validation/test splits and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZINC.</head><p>The ZINC dataset is a graph regression dataset comprised of molecules, where the task is to predict constrained solubility. Like <ref type="bibr" target="#b7">Dwivedi et al. (2020)</ref>, we use the subset of 12K molecules and follow their same splits.</p><p>PATTERN and CLUSTER. <ref type="bibr">PATTERN and CLUSTER Dwivedi et al. (2020)</ref> are synthetic datasets that were created using Stochastic Block Models <ref type="bibr" target="#b0">(Abbe, 2018)</ref>. The goal for both datasets is node classification, with PATTERN focused on detecting a given pattern in the dataset, and with CLUSTER focused on identifying communities within the graphs. For PATTERN, the binary class label corresponds to whether a node is part of the predefined pattern or not; for CLUSTER, the multi-class label indicates membership in a community. We use the splits as is used in <ref type="bibr" target="#b7">Dwivedi et al. (2020)</ref>.</p><p>OGBG-PPA. PPA <ref type="bibr" target="#b15">(Hu et al., 2020a</ref>) is comprised of protein-protein association networks where the goal is to correctly classify the network into one of 37 classes representing the category of species the network is from. Nodes represent proteins and edges represent associations between proteins. Edge attributes represent information relative to the association, such as co-expression. We use the standard splits provided by <ref type="bibr" target="#b15">Hu et al. (2020a)</ref>.  <ref type="formula">2020a</ref>) is a dataset containing source code from the Python programming language. It is made up of Abstract Syntax Trees where the task is to correctly classify the sub-tokens that comprise the method name. We use the standard splits provided by <ref type="bibr" target="#b15">Hu et al. (2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Hyperparameter choices and reproducibility</head><p>Hyperparameter choice. In general, we perform a very limited hyperparameter search to produce the results in Table <ref type="table" target="#tab_1">1 and Table 2</ref>. The hyperparameters for training SAT models on different datasets are summarized in Table <ref type="table" target="#tab_4">4</ref>, where only the dropout rate and the size of the subgraph k are tuned (k ∈ {1, 2, 3, 4}). We use fixed RWPE <ref type="bibr">(Dwivedi et al., 2021)</ref> with SAT on ZINC, PATTERN and CLUSTER.</p><p>In all experiments, we use the validation set to select the dropout rate and the size of the subtree or subgraph k ∈ {1, 2, 3, 4}. All other hyperparameters are fixed for simplicity, including setting the readout method to mean pooling. We did not use RWPE on OGBG-PPA and OGBG-CODE2 as we observed very little performance improvement. Note that we only use k = 1 for the k-subgraph SAT models on CLUSTER and PATTERN due to its large memory requirement, which already leads to performance boost compared to the k-subtree SAT using a larger k. Reported results are the average over 4 seeds on ZINC, PATTERN and CLUSTER, as is done in <ref type="bibr" target="#b7">Dwivedi et al. (2020)</ref>, and averaged over 10 seeds on OGBG-PPA and OGBG-CODE2.</p><p>Optimization. All our models are trained with the AdamW optimizer <ref type="bibr" target="#b27">(Loshchilov &amp; Hutter, 2018)</ref> with a standard warm-up strategy suggested for transformers in <ref type="bibr" target="#b37">Vaswani et al. (2017)</ref>. We use either the L1 loss or the cross-entropy loss depending on whether the task is regression or classification. The learning rate scheduler proposed in the Transformer is used on the ZINC, PATTERN and CLUSTER datasets and a cosine scheduler <ref type="bibr" target="#b26">(Loshchilov &amp; Hutter, 2016)</ref> is used on the larger OGBG-PPA and OGBG-CODE2 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameters and computation time.</head><p>In Table <ref type="table" target="#tab_5">5</ref>, we report the number of parameters and the training time per epoch for SAT with k-subtree GNN extractors using the hyperparameters selected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Additional Results</head><p>We provide additional experimental results on ZINC, OGBG-PPA and OGBG-CODE2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1. Additional results on ZINC</head><p>We report a more thorough comparison of SAT instances using different structure extractors and different readout methods in Table <ref type="table" target="#tab_6">6</ref>. We find that SAT models with PNA consistently outperform other GNNs. Additionally, the readout methods have very little impact on the prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.2. Additional results on OGBG-PPA</head><p>Table <ref type="table">7</ref> summarizes the results for k-subtree SAT with different GNNs compared to state-of-the-art methods on OGBG-PPA. All the results are computed from 10 runs using different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.3. Additional results on OGBG-CODE2</head><p>Table <ref type="table">8</ref> summarizes the results for k-subtree SAT with different GNNs compared to state-of-the-art methods on OGBG-CODE2. All the results are computed from 10 runs using different random seeds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1.: Position-aware vs. structure-aware: Using a positional encoding based on shortest paths, nodeu and v would receive identical encodings since their shortest paths to all other nodes are the same in both graphs. However, their structures are different, with v forming a triangle with its red neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.: Overview of a SAT layer: we first extract the k-hop subgraphs centered at each node (here, k = 1) and use a structure extractor to compute the subgraph representations; then, the subgraph representations are used to compute the query (Q) and key (K) matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>addition to performance improvement, we show that SAT offers better model interpretability compared to the classic Transformer with only absolute postional encoding. We respectively train a SAT model and a transformer with a CLS readout on the Mutagenicity dataset, and visualize the attention scores between the [CLS] node and other nodes learned by SAT and the Transformer in Figure4. While both models manage to identify some chemical motifs known for mutagenicity, such as NO 2 and NH 2 , the attention scores learned by SAT are sparser and more informative. The vanilla Transformer even fails to put attention to some important atoms such as the H atoms in the NH 2 group. The only H atoms highlighted by SAT are those in the NH 2 group, suggesting that our SAT indeed takes the structure into account. More results are provided in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.: Attention visualization of SAT and the Transformer. The middle column shows the attention weights of the [CLS] node learned by our SAT model and the right column shows the attention weights learned by the classic Transformer with RWPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>: Comparison of SAT to SOTA methods on graph regression and classification tasks. ZINC results use edge weights where applicable, otherwise without edge weights. indicates results obtained by adapting the code provided by the original paper.</figDesc><table><row><cell></cell><cell>ZINC</cell><cell>CLUSTER</cell><cell>PATTERN</cell></row><row><cell># GRAPHS</cell><cell>12,000</cell><cell>12,000</cell><cell>14,000</cell></row><row><cell>AVG. # NODES</cell><cell>23.2</cell><cell>117.2</cell><cell>118.9</cell></row><row><cell>AVG. # EDGES</cell><cell>49.8</cell><cell>4,303.9</cell><cell>6,098.9</cell></row><row><cell>METRIC</cell><cell>MAE</cell><cell>ACCURACY</cell><cell>ACCURACY</cell></row><row><cell>GIN</cell><cell cols="3">0.387±0.015 64.716±1.553 85.590±0.011</cell></row><row><cell>GAT</cell><cell cols="3">0.384±0.007 70.587±0.447 78.271±0.186</cell></row><row><cell>PNA</cell><cell cols="2">0.188±0.004 67.077±0.977</cell><cell>86.567±0.075</cell></row><row><cell cols="4">TRANSFORMER+RWPE 0.310±0.005 29.622±0.176 86.183±0.019</cell></row><row><cell cols="4">GRAPH TRANSFORMER 0.226±0.014 73.169±0.622 84.808±0.068</cell></row><row><cell>SAN</cell><cell cols="3">0.139±0.006 76.691±0.650 86.581±0.037</cell></row><row><cell>GRAPHORMER</cell><cell>0.122±0.006</cell><cell>-</cell><cell>-</cell></row><row><cell>K-SUBTREE SAT</cell><cell cols="3">0.102±0.005 77.751±0.121 86.865±0.043</cell></row><row><cell>K-SUBGRAPH SAT</cell><cell cols="3">0.094±0.008 77.856±0.104 86.848±0.037</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>: Comparison of SAT to SOTA methods on OGB datasets. -aware attention are different, i.e. SA-attn(v) = SA-attn(v ), if their subgraph representations aredifferent, i.e. ϕ(v, G)  </figDesc><table><row><cell></cell><cell>OGBG-PPA</cell><cell>OGBG-CODE2</cell></row><row><cell># GRAPHS</cell><cell>158,100</cell><cell>452,741</cell></row><row><cell>AVG. # NODES</cell><cell>243.4</cell><cell>125.2</cell></row><row><cell>AVG. # EDGES</cell><cell>2,266.1</cell><cell>124.2</cell></row><row><cell>METRIC</cell><cell>ACCURACY</cell><cell>F1 SCORE</cell></row><row><cell>GCN</cell><cell>0.6839±0.0084</cell><cell>0.1507±0.0018</cell></row><row><cell cols="2">GCN-VIRTUAL NODE 0.6857±0.0061</cell><cell>0.1595±0.0018</cell></row><row><cell>GIN</cell><cell>0.6892±0.0100</cell><cell>0.1495±0.0023</cell></row><row><cell cols="2">GIN-VIRTUAL NODE 0.7037±0.0107</cell><cell>0.1581±0.0026</cell></row><row><cell>TRANSFORMER</cell><cell>0.6454±0.0033</cell><cell>0.1670±0.0015</cell></row><row><cell>GRAPHTRANS</cell><cell>-</cell><cell>0.1830±0.0024</cell></row><row><cell>K-SUBTREE SAT</cell><cell>0.7522±0.0056</cell><cell>0.1937±0.0028</cell></row></table><note>structure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>: Hyperparameters for SAT models trained on different datasets. RWPE-p indicates using p steps in the random walk positional encoding, which results in a p-dimensional vector as the positional representation for each node.</figDesc><table><row><cell>Hyperparameter</cell><cell>ZINC</cell><cell cols="4">CLUSTER PATTERN OGBG-PPA OGBG-CODE2</cell></row><row><cell>#Layers</cell><cell>6</cell><cell>16</cell><cell>6</cell><cell>3</cell><cell>4</cell></row><row><cell>Hidden dimensions</cell><cell>64</cell><cell>48</cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>FFN hidden dimensions</cell><cell></cell><cell></cell><cell cols="2">2×Hidden dimensions</cell><cell></cell></row><row><cell>#Attention heads</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>{4, 8}</cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell cols="2">{0.0, 0.1, 0.2, 0.3, 0.4}</cell><cell></cell></row><row><cell>Size of subgraphs k</cell><cell></cell><cell></cell><cell>{1, 2, 3, 4}</cell><cell></cell><cell></cell></row><row><cell>Readout method</cell><cell>mean</cell><cell>None</cell><cell>None</cell><cell>mean</cell><cell>mean</cell></row><row><cell>Absolute PE</cell><cell>RWPE-20</cell><cell>RWPE-3</cell><cell>RWPE-7</cell><cell>None</cell><cell>None</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0003</cell><cell>0.0003</cell><cell>0.0001</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>#Epochs</cell><cell>2000</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>30</cell></row><row><cell>Warm-up steps</cell><cell>5000</cell><cell>5000</cell><cell>5000</cell><cell>10 epochs</cell><cell>2 epochs</cell></row><row><cell>Weight decay</cell><cell>1e-5</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-6</cell></row><row><cell cols="2">OGBG-CODE2. CODE2 Hu et al. (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>: Number of parameters and training time per epoch for k-subtree SAT models using the hyperparameters in Table4. Various GNNs are used as the base GNN in SAT.</figDesc><table><row><cell></cell><cell cols="5">ZINC CLUSTER PATTERN OGBG-PPA OGBG-CODE2</cell></row><row><cell>Base GNN</cell><cell></cell><cell></cell><cell cols="2">#Parameters</cell><cell></cell></row><row><cell>GCN</cell><cell>421k</cell><cell>571k</cell><cell>380k</cell><cell>766k</cell><cell>14,030k</cell></row><row><cell>GIN</cell><cell>495k</cell><cell>684k</cell><cell>455k</cell><cell>866k</cell><cell>14,554k</cell></row><row><cell>PNA</cell><cell>523k</cell><cell>741k</cell><cell>493k</cell><cell>1,088k</cell><cell>15,734k</cell></row><row><cell>Base GNN</cell><cell></cell><cell cols="3">GPU time on a single TITAN RTX/epoch</cell><cell></cell></row><row><cell>GCN</cell><cell>6s</cell><cell>142s</cell><cell>40s</cell><cell>308s</cell><cell>40min</cell></row><row><cell>GIN</cell><cell>6s</cell><cell>144s</cell><cell>62s</cell><cell>310s</cell><cell>40min</cell></row><row><cell>PNA</cell><cell>9s</cell><cell>178s</cell><cell>90s</cell><cell>660s</cell><cell>55min</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>: Test MAE for SAT models using different structure extractors and readout methods on the ZINC dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>W/O EDGE ATTRIBUTES</cell><cell></cell><cell></cell><cell>W/ EDGE ATTRIBUTES</cell></row><row><cell></cell><cell>BASE GNN</cell><cell>MEAN</cell><cell>SUM</cell><cell>CLS</cell><cell>MEAN</cell><cell>SUM</cell><cell>CLS</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">0.174±0.009 0.170±0.010 0.167±9.005 0.127±0.010 0.117±0.008 0.115±0.007</cell></row><row><cell>K-SUBTREE SAT</cell><cell cols="7">GIN GRAPHSAGE 0.164±0.004 0.165±0.008 0.156±0.005 0.166±0.007 0.162±0.010 0.157±0.002 0.115±0.005 0.112±0.008 0.104±0.003 ---</cell></row><row><cell></cell><cell>PNA</cell><cell cols="6">0.147±0.001 0.142±0.008 0.135±0.004 0.102±0.005 0.102±0.003 0.098±0.008</cell></row><row><cell></cell><cell>GCN</cell><cell cols="6">0.184±0.002 0.186±0.007 0.184±0.007 0.114±0.005 0.103±0.002 0.103±0.008</cell></row><row><cell>K-SUBGRAPH SAT</cell><cell cols="7">GIN GRAPHSAGE 0.168±0.005 0.165±0.005 0.169±0.005 0.162±0.013 0.158±0.007 0.162±0.005 0.095±0.002 0.097±0.002 0.098±0.010 ---</cell></row><row><cell></cell><cell>PNA</cell><cell cols="6">0.131±0.002 0.129±0.003 0.128±0.004 0.094±0.008 0.089±0.002 0.093±0.009</cell></row><row><cell>from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Note that the number of parameters used in our SAT on OGB datasets is smaller than most of the state-of-art methods.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung (K.B.). The authors would also like to thank Dr. Bastian Rieck and Dr. Carlos Oliver for their insightful feedback on the manuscript, which greatly improved it.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Interpretation</head><p>In this section, we provide implementation details about the model visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Dataset and training details</head><p>We use the Mutagenicity dataset <ref type="bibr" target="#b20">(Kersting et al., 2016)</ref>, consisting of 4337 molecular graphs labeled based on their mutagenic effect. We randomly split the dataset into train/val/test sets in a stratified way with a proportion of 80/10/10. We first train a two-layer vanilla transformer model using RWPE.</p><p>The hidden dimension and the number of heads are fixed to 64 and 8 respectively. The CLS pooling as described in Section 4.2 is chosen as the readout method for visualization purpose. We also train a k-subtree SAT using exactly the same hyperparameter setting except that it does not use any absolute positional encoding. k is fixed to 2. For both models, we use the AdamW optimizer and the optimization strategy described in Section C.3. We train enough epochs until both models converge. While the classic Transformer with RWPE achieves a test accuracy of 78%, the k-subtree SAT achieves a 82% test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Additional results</head><p>Visualization of attention scores. Here, we provide additional visualization examples of attention scores of the [CLS] node from the Mutagenicity dataset, learned by SAT and a vanilla transformer.</p><p>Figure <ref type="figure">5</ref> provides several examples of attention learned weights. SAT generally learns sparser and more informative weights even for very large graph as shown in the left panel of the middle row. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: Recent developments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">177</biblScope>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07875</idno>
		<title level="m">Graph neural networks with learnable structural and positional representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the properties of the softmax function with application in game theory and reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pavel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00805</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Utilizing graph machine learning within drug discovery and development</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zinc: A free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing long-range context for graph neural networks with global attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
				<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graphit: Encoding graph structure in transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Universal kernels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random walk graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter</title>
				<meeting>the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
