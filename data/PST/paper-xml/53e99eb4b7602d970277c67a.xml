<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A latent factor model for highly multi-relational data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
							<email>jenatton@cmap.polytechnique.fr</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Le Roux</surname></persName>
							<email>nicolas@le-roux.name</email>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>antoine.bordes@utc.fr</email>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
							<email>guillaume.obozinski@ens.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR CNRS 7641</orgName>
								<orgName type="institution">CMAP</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">INRIA -SIERRA Project Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">UMR CNRS 7253</orgName>
								<orgName type="institution">Université de Technologie de Compiègne</orgName>
								<address>
									<settlement>Heudiasyc</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">INRIA -SIERRA Project Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Ecole Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A latent factor model for highly multi-relational data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55AAA2418E1835EE5969FCDA0F713A90</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical Relational Learning (SRL) <ref type="bibr" target="#b6">[7]</ref> aims at modeling data consisting of relations between entities. Social networks, preference data from recommender systems, relational databases used for the semantic web or in bioinformatics, illustrate the diversity of applications in which such modeling has a potential impact.</p><p>Relational data typically involve different types of relations between entities or attributes. These entities can be users in the case of social networks or recommender systems, words in the case of lexical knowledge bases, or genes and proteins in the case of bioinformatics ontologies, to name a few. For binary relations, the data is naturally represented as a so called multi-relational graph consisting of nodes associated with entities and of different types of edges between nodes corresponding to the different types of relations. Equivalently the data consists of a collection of triplets of the form (subject, relation, object), listing the actual relationships where we will call subject and object respectively the first and second term of a binary relation. Relational data typically cumulates many difficulties. First, a large number of relation types, some being significantly more represented than others and possibly concerning only subsets of entities; second, the data is typically noisy and incomplete (missing or incorrect relationships, redundant entities); finally most datasets are large scale with up to millions of entities and billions of links for real-world knowledge bases.</p><p>Besides relational databases, SRL can also be used to model natural language semantics. A standard way of representing the meaning of language is to identify entities and relations in texts or speech utterances and to organize them. This can be conducted at various scales, from the word or sentence level (e.g. in parsing or semantic role labeling) to a collection of texts (e.g. in knowledge extraction). SRL systems are a useful tool there, as they can automatically extract high level information from the collected data by building summaries <ref type="bibr" target="#b21">[22]</ref>, sense categorization lexicons <ref type="bibr" target="#b10">[11]</ref>, ontologies <ref type="bibr" target="#b19">[20]</ref>, etc. Progress in SRL would be likely to lead to advances in natural language understanding.</p><p>In this paper, we introduce a model for relational data and apply it to multi-relational graphs and to natural language. In assigning high probabilities to valid relations and low probabilities to all the others, this model extracts meaningful representations of the various entities and relations in the data. Unlike other factorization methods (e.g. <ref type="bibr" target="#b14">[15]</ref>), our model is probabilistic which has the advantage of accounting explicitly for the uncertainties in the data. Besides, thanks to a sparse distributed representation of relation types, our model can handle data with a significantly larger number of relation types than was considered so far in the literature (a crucial aspect for natural language data). We empirically show that this approach ties or beats state-of-the-art algorithms on various benchmarks of link prediction, a standard test-bed for SRL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A branch of relational learning, motivated by applications such as collaborative filtering and link prediction in networks, models relations between entities as resulting from intrinsic latent attributes of these entities. <ref type="foot" target="#foot_0">1</ref> Work in what we will call relational learning from latent attributes (RLA) focused mostly on the problem of modeling a single relation type as opposed to trying to model simultaneously a collection of relations which can themselves be similar. As reflected by several formalisms proposed for relational learning <ref type="bibr" target="#b6">[7]</ref>, it is the latter multi-relational learning problem which is needed to model efficiently large scale relational databases. The fact that relations can be similar or related suggests that a superposition of independently learned models for each relation would be highly inefficient especially since the relationships observed for each relation are extremely sparse.</p><p>RLA translates often into learning an embedding of the entities, which corresponds algebraically to a matrix factorization problem (typically the matrix of observed relationships). A natural extension to learning multiple relations consists in stacking the matrices to be factorized and applying classical tensor factorization methods such as CANDECOMP/PARAFAC <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8]</ref>. This approach, which induces inherently some sharing of parameters between both different terms and different relations, has been applied successfully <ref type="bibr" target="#b7">[8]</ref> and has inspired some probabilistic formulations <ref type="bibr" target="#b3">[4]</ref>.</p><p>Another natural extension to learning several relations simultaneously can be to share the common embedding or the entities across relations via collective matrix factorization as proposed in RESCAL <ref type="bibr" target="#b14">[15]</ref> and other related work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>The simplest form of latent attribute that can be associated to an entity is a latent class: the resulting model is the classical stochastic blockmodel <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>. Several clustering-based approaches have been proposed for multi-relational learning: <ref type="bibr" target="#b8">[9]</ref> considered a non-parametric Bayesian extension of the stochastic blockmodel allowing to automatically infer the number of latent clusters; <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> refined this to allow entities to have a mixed clusters membership; <ref type="bibr" target="#b9">[10]</ref> introduced clustering in Markov-Logic networks; <ref type="bibr" target="#b23">[24]</ref> used a non-parametric Bayesian clustering of entities embedding in a collective matrix factorization formulation. To share parameters between relations, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> and <ref type="bibr" target="#b9">[10]</ref> build models that cluster not only entities but relations as well.</p><p>With the same aim of reducing the number of parameters, the Semantic Matching Energy model (SME) of <ref type="bibr" target="#b1">[2]</ref> embeds relations as a vector from the same space as the entities and models likely relationships by an energy combining together binary interactions between the relation vector and each of the vectors encoding the two terms.</p><p>In terms of scalability, RESCAL <ref type="bibr" target="#b14">[15]</ref>, which has been shown to achieve state of the art performance on several relation datasets, has recently been applied to the knowledge base YAGO <ref type="bibr" target="#b15">[16]</ref> thereby showing its ability to scale well on data with very large numbers of entities, although the number of relations modeled remained moderate (less than 100). As for SME <ref type="bibr" target="#b1">[2]</ref>, its modeling of relations by vectors allowed it to scale to several thousands of relations. Scalability can be also an issue for nonparametric Bayesian models (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>) because of the cost of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relational data modeling</head><p>We consider relational data consisting of triplets that encode the existence of relation between two entities that we will call the subject and the object. Specifically, we consider a set of n s subjects {S i } i∈ 1;ns along with n o objects {O k } k∈ 1;no which are related by some of n r relations {R j } j∈ 1;nr . A triplet encodes that the relation R j holds between the subject S i and the object O k , which we will write R j (S i , O k ) = 1. We will therefore refer to a triplet also as a relationship.</p><p>A typical example which we will discuss in greater detail is in natural language processing where a triplet (S i , R j , O k ) corresponds to the association of a subject and a direct object through a transitive verb. The goal is to learn a model of the relations to reliably predict unseen triplets. For instance, one might be interested in finding a likely relation R j based only on the subject and object (S i , O k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model description</head><p>In this work, we formulate the problem of learning a relation as a matrix factorization problem. Following a rationale underlying several previous approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>, we consider a model in which entities are embedded in R p and relations are encoded as bilinear operators on the entities. More precisely, we assume that the n s subjects (resp. n o objects) are represented by vectors of R p , stored as the columns of the matrix S [s 1 , . . . , s ns ] ∈ R p×ns (resp. as the columns of</p><formula xml:id="formula_0">O [o 1 , . . . , o no ] ∈ R p×no )</formula><p>. Each of the p-dimensional representations s i , o k will have to be learned. The relations are represented by a collection of matrices (R j ) 1≤j≤nr , with R j ∈ R p×p , which together form a three-dimensional tensor.</p><p>We consider a model of the probability of the event R j (S i , O k ) = 1 . Assuming first that s i and o k are fixed, our model is derived from a logistic model</p><formula xml:id="formula_1">P[R j (S i , O k ) = 1] σ η (j) ik , with σ(t) 1/(1 + e -t ). A natural form for η (j)</formula><p>ik is a linear function of the tensor product s i ⊗ o k which we can write η</p><formula xml:id="formula_2">(j) ik = s i , R j o k where •, • is the usual inner product in R p .</formula><p>If we think now of learning s i , R j and o k for all (i, j, k) simultaneously, this model learns together the matrices R j and optimal embeddings s i , o k of the entities so that the usual logistic regressions based on s i ⊗o k predict well the probability of the observed relationships. This is the initial model considered in <ref type="bibr" target="#b23">[24]</ref> and it matches the model considered in <ref type="bibr" target="#b15">[16]</ref> if the least-square loss is substituted to the logistic loss. We will refine this model in two ways: first by redefining the term η (j) ik as a function η (j) ik E(s i , R j , o k ) taking into account the different orders of interactions between s i , o k and R j , second, by parameterizing the relations R j by latent "relational" factors that reduce the overall number of parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A multiple order log-odds ratio model</head><p>One way of thinking about the probability of occurrence of a specific relationship corresponding to the triplet (S i , R j , O k ) is as resulting (a) from the marginal propensity of individual entities S i , O k to enter relations and the marginal propensity of relations R j to occur, (b) from 2-way interactions of (S i , R j ), (R j , O k ) corresponding to entities tending to occur marginally as left of right terms of a relation (c) from 2-way interactions of pairs of entities (S i , O k ) that overall tend to have more relations together, and (d) the 3-way dependencies between</p><formula xml:id="formula_3">(S i , R j , O k ).</formula><p>In NLP, we often refer to these as respectively unigram, bigram and trigram terms, a terminology which we will reuse in the rest of the paper. We therefore design E(s i , R j , o k ) to account for these interactions of various orders, retaining only<ref type="foot" target="#foot_1">2</ref> terms involving R j .</p><p>In particular, introducing new parameters y, y , z, z ∈ R p , we define η</p><formula xml:id="formula_4">(j) ik = E(s i , R j , o k ) as E(s i , R j , o k ) y, R j y + s i , R j z + z , R j o k + s i , R j o k ,<label>(1)</label></formula><p>where y, R j y , s i , R j z + z , R j o k and s i , R j o k are the uni-, bi-and trigram terms. This parametrization is redundant in general given that E(s i , R j , o k ) is of the form (s i + z), R j (o k + z ) + b j ; but it is however useful in the context of a regularized model (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sharing parameters across relations through latent factors</head><p>When learning a large number of relations, the number of observations for many relations can be quite small, leading to a risk of overfitting. Sutskever et al. <ref type="bibr" target="#b23">[24]</ref> addressed this issue with a nonparametric Bayesian model inducing clustering of both relations and entities. SME <ref type="bibr" target="#b1">[2]</ref> proposed to embed relations as vectors of R p , like entities, to tackle problems with hundreds of relation types.</p><p>With a similar motivation to decrease the overall number of parameters, instead of using a general parameterization of the matrices R j as in RESCAL <ref type="bibr" target="#b15">[16]</ref>, we require that all R j decompose over a common set of d rank one matrices {Θ r } 1≤r≤d representing some canonical relations:</p><formula xml:id="formula_5">R j = d r=1 α j r Θ r , for some sparse α j ∈ R d and Θ r = u r v r for u r , v r ∈ R p . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>The combined effect of (a) the sparsity of the decomposition and (b) the fact that d n r leads to sharing parameters across relations. Further, constraining Θ r to be the outer product u r v r also speeds up all computations relying on linear algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Regularized formulation and optimization</head><p>Denoting P (resp. N ) the set of indices of positively (resp. negatively) labeled relations, the likelihood we seek to maximize is</p><formula xml:id="formula_7">L (i,j,k)∈P P[R j (S i , O k ) = 1] • (i ,j ,k )∈N P[R j (S i , O k ) = 0] . The log-likelihood is thus log(L) = (i,j,k)∈P η (j) ik -(i,j,k)∈P∪N log(1 + exp(η (j) ik )), with η (j) ik = E(s i , R j , o k ).</formula><p>To properly normalize the terms appearing in ( <ref type="formula" target="#formula_4">1</ref>) and ( <ref type="formula" target="#formula_5">2</ref>), we carry out the minimization of the negative log-likelihood over a specific constraint set, namely min S,O,{α j }, {Θr}y,y ,z,z -log(L), with</p><formula xml:id="formula_8">   α j 1 ≤ λ, Θ r = u r • v r , z = z , O = S, s j , o k ,</formula><p>y, y , z, u r and v r in the ball w; w 2 ≤ 1 .</p><p>We chose to constrain α in 1 -norm based on preliminary experiments suggesting that it led to better results that the regularization in 2 -norm. The regularization parameter λ ≥ 0 controls the sparsity of the relation representations in <ref type="bibr" target="#b1">(2)</ref>. The equality constraints induce a shared representations between subject and objects which were shown to improve the model in preliminary experiments. Given the fact that the model is conditional on a pair (s i , o k ), only a single scale parameter, namely α j r , is necessary in the product α j r s i , Θ r o k , which motivates all the Euclidean unit ball constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithmic approach</head><p>Given the large scale of the problems we are interested in (e.g., |P| ≈ 10 6 ), and since we can project efficiently onto the constraint set (both the projections onto 1 -and 2 -norm balls can be performed in linear time <ref type="bibr" target="#b0">[1]</ref>), our optimization problem lends itself well to a stochastic projected gradient algorithm <ref type="bibr" target="#b2">[3]</ref>.</p><p>In order to speed up the optimization, we use several practical tricks. First, we consider a stochastic gradient descent scheme with mini-batches containing 100 triplets. Second, we use stepsizes of the form a/(1 + k) with k the iteration number and a a scalar (common to all parameters) optimized over a logarithmic grid on a validation set. <ref type="foot" target="#foot_2">3</ref>Additionally, we cannot treat the NLP application (see Sec. 8) as a standard tensor factorization problem. Indeed, in that case, we only have access to the positively labeled triplets P. Following <ref type="bibr" target="#b1">[2]</ref>, we generate elements in N by considering triplets of the form {(i, j , k)}, j = j for each (i, j, k) ∈ P. In practice, for each positive triplet, we sample a number of artificial negative triplets containing the same subject and object as our positive triplet but different verbs. This allowed us to change the problem into a multiclass one where the goal was to correctly classify the "positive" verb, in competition with the "negative" ones.</p><p>The standard approach for this problem is to use a multinomial logistic function. However, such a function is highly sensitive to the particular choice of negative verbs and using all the verbs as negative ones would be too costly. Another more robust approach consists in using the likelihood function defined above where we try to classify the positive verbs as a valid relationship and the negative ones as invalid relationships. Further, this approximation to the multinomial logistic function is asymptotically unbiased.</p><p>Finally, we observed that it was advantageous to down-weight the influence of the negative verbs to avoid swamping the influence of the positive ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relation to other models</head><p>Our model is closely related to several other models. First, if d is large, the parameters of the R j are decoupled and the RESCAL model is retrieved (up to a change of loss function).</p><p>Second, our model is also related to classical tensor factorization model such as PARAFAC which approximate the tensor [R k (S i , O j )] i,j,k in the least-square sense by a low rank tensor H of the form</p><formula xml:id="formula_9">d r=1 α r ⊗ β r ⊗ γ r for (α r , β r , γ r ) ∈ R nr×ns×no .</formula><p>The parameterization of all R j as linear combinations of d rank one matrices is in fact equivalent to constraining the tensor R = {R j } j∈ 1;nr to be the low rank tensor R = d r=1 α r ⊗ u r ⊗ v r . As a consequence, the tensor of all trigram terms<ref type="foot" target="#foot_3">4</ref> can be written also as d r=1 α r ⊗ β r ⊗ γ r with β r = S u r and γ r = O v r . This shows that our model is a particular form of tensor factorization which reduces to PARAFAC (up to a change of loss function) when p is sufficiently large.</p><p>Finally, the approach considered in <ref type="bibr" target="#b1">[2]</ref> seems a priori quite different from ours, in particular since relations are in that work embedded as vectors of R p like the entities as opposed to matrices of R p×p in our case. This choice can be detrimental to model complex relation patterns as we show in Section 7. In addition, no parameterization of the model <ref type="bibr" target="#b1">[2]</ref> is able of handling both bigram and trigram interactions as we propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Application to multi-relational benchmarks</head><p>We report in this section the performance of our model evaluated on standard tensor-factorization datasets, which we first briefly describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets</head><p>Kinships. Australian tribes are renowned among anthropologists for the complex relational structure of their kinship systems. This dataset, created by <ref type="bibr" target="#b5">[6]</ref>, focuses on the Alyawarra, a tribe from Central Australia. 104 tribe members were asked to provide the kinship terms they used for one another. This results in graph of 104 entities and 26 relation types, each of them depicting a different kinship term, such as Adiadya or Umbaidya. See <ref type="bibr" target="#b5">[6]</ref> or <ref type="bibr" target="#b8">[9]</ref> for more details.</p><p>UMLS. This dataset contains data from the Unified Medical Language System semantic work gathered by <ref type="bibr" target="#b11">[12]</ref>. This consists in a graph with 135 entities and 49 relation types. The entities are high-level concepts like 'Disease or Syndrome', 'Diagnostic Procedure', or 'Mammal'. The relations represent verbs depicting causal influence between concepts like 'affect' or 'cause'.</p><p>Nations. This dataset groups 14 countries (Brazil, China, Egypt, etc.) with 56 binary relation types representing interactions among them like 'economic aid', 'treaties' or 'rel diplomacy', and 111 features describing each country, which we treated as 111 additional entities interacting with the country through an additional 'has feature' relation <ref type="foot" target="#foot_4">5</ref> . See <ref type="bibr" target="#b20">[21]</ref> for details.  <ref type="bibr" target="#b15">[16]</ref>, MRC <ref type="bibr" target="#b9">[10]</ref> and SME <ref type="bibr" target="#b1">[2]</ref> over three standard datasets. The results are computed by 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>These three datasets are relatively small-scale and contain only a few relationships (in the order of tens). Since our model is primarily designed to handle a large number of relationships (see Sec. 4.2), this setting is the most favorable to evaluate the potential of our approach. As reported in Table <ref type="table" target="#tab_0">1</ref>, our method does nonetheless yield better or equally good performance as previous state-of-the-art techniques, both in terms of area under the precision-recall curve (AUC) and log-likelihood (LL).</p><p>The results displayed in Table <ref type="table" target="#tab_0">1</ref> are computed by 10-fold cross-validation <ref type="foot" target="#foot_5">6</ref> , averaged over 10 random splits of the datasets (90% for cross-validation and 10% for testing). We chose to compare our model with RESCAL <ref type="bibr" target="#b15">[16]</ref>, MRC <ref type="bibr" target="#b9">[10]</ref> and SME <ref type="bibr" target="#b1">[2]</ref> because they achieved the best published results on these benchmarks in terms of AUC and LL, to the best of our knowledge.</p><p>Interestingly, the trigram term from ( <ref type="formula" target="#formula_4">1</ref>) is essential to obtain good performance on Kinships (with the trigram term removed, we obtain 0.16 in AUC and -0.14 in LL), thus showing the need for modeling 3-way interactions in complex relational data. Moreover, and as expected due to the low number of relations, the value of λ selected by cross-validation is quite large (λ = n r × d), and as consequence does not lead to sparsity in <ref type="bibr" target="#b1">(2)</ref>. Results on this dataset also exhibits the benefit of modeling relations with matrices instead of vectors as does SME <ref type="bibr" target="#b1">[2]</ref>.</p><p>Zhu <ref type="bibr" target="#b27">[28]</ref> recently reported results on Nations and Kinships evaluated in terms of area under the receiver-operating-characteristic curve instead of area under the precision-recall curve as we display in Table <ref type="table" target="#tab_0">1</ref>. With this other metric, our model obtains 0.953 on Nations and 0.992 on Kinships and hence outperforms Zhu's approach, which achieves 0.926 and 0.962 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Learning semantic representations of verbs</head><p>By providing an approach to model the relational structure of language, SRL can be of great use for learning natural language semantics. Hence, this section proposes an application of our method on text data from Wikipedia for learning a representation of words, with a focus on verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experimental setting</head><p>Data. We collected this data in two stages. First, the SENNA software<ref type="foot" target="#foot_6">7</ref>  <ref type="bibr" target="#b4">[5]</ref> was used to perform part-of-speech tagging, chunking, lemmatization <ref type="foot" target="#foot_7">8</ref> and semantic role labeling on ≈2,000,000 Wikipedia articles. This data was then filtered to only select sentences for which the syntactic structure was (subject, verb, direct object) with each term of the triplet being a single word from the WordNet lexicon <ref type="bibr" target="#b12">[13]</ref>. Subjects and direct objects ended up being all single nouns, whose dictionary size is 30,605. The total number of relations in this dataset (i.e. the number of verbs) is 4,547: this is much larger than for previously published multi-relational benchmarks. We kept 1,000,000 such relationships to build a training set, 50,000 for a validation set and 250,000 for test. All triplets are unique and we made sure that all words appearing in the validation or test sets were occurring in the training set. <ref type="foot" target="#foot_8">9</ref>synonyms not considered best synonyms considered median/mean rank p@5 p@20 median/mean rank p@5 p@20 Our approach 50 / 195.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Results</head><p>Verb prediction. We first consider a direct evaluation of our approach based on the test set of 250,000 instances by measuring how well we predict a relevant and meaningful verb given a pair (subject, direct object). To this end, for each test relationship, we rank all verbs using our probability estimates given a pair (subject, direct object). Table <ref type="table" target="#tab_1">2</ref> displays our results with two kinds of metrics, namely, (1) the rank of the correct verb and (2) the fraction of test examples for which the correct verb is ranked in the top z% of the list. The latter criterion is referred to as p@z. In order to evaluate if some language semantics is captured by the representations, we also consider a less conservative approach where, instead of focusing on the correct verb only, we measure the minimum rank achieved over its set of synonyms obtained from WordNet. Our method is compared with that of SME <ref type="bibr" target="#b1">[2]</ref>, which was shown to scale well on data with large sets of relations, and with a bigram model, which estimates the probabilities of the pairs (subject, verb) and (verb, direct object).</p><p>The first observation is that the task of verb prediction can be quite well addressed by a simple model based on 2-way interactions, as shown by the good median rank obtained by the bigram model. This is confirmed by the mild influence of the trigram term on the performance of our model. On this data, we experienced that using bigram interactions in our energy function was essential to achieve good predictions. However, the drop in the mean rank between our approach and the bigram-only model still indicates that many examples do need a richer model to be correctly handled. By comparison, we tend to consistently match or improve upon the performance of SME. Remarquably, model selection led to the choice of λ = 0.  <ref type="bibr" target="#b1">[2]</ref>, Collobert et al. <ref type="bibr" target="#b4">[5]</ref> and the best (out of three) WordNet similarity measure <ref type="bibr" target="#b12">[13]</ref>. Details about the task can be found in the text.  <ref type="bibr" target="#b26">[27]</ref>, where we compare our approach, SME <ref type="bibr" target="#b1">[2]</ref>, Collobert et al.'s word embeddings <ref type="bibr" target="#b4">[5]</ref> and the best (out of 3) WordNet Similarity measure <ref type="bibr" target="#b18">[19]</ref> using area under the precision-recall curve. Details are given in the text.</p><p>Lexical similarity classification. Our method learns latent representations for verbs and imposes them some structure via shared parameters, as shown in Section 4.2. This should lead to similar representations for similar verbs. We consider the task of lexical similarity classification described in <ref type="bibr" target="#b26">[27]</ref> to evaluate this hypothesis. Their dataset consists of 130 pairs of verbs labeled by humans with a score in {0, 1, 2, 3, 4}. Higher scores means a stronger semantic similarity between the verbs composing the pair. For instance, (divide,split) is labeled 4, while (postpone,show) has a score of 0.</p><p>Based on the pairwise Euclidean distances<ref type="foot" target="#foot_9">10</ref> between our learned verb representations R j , we try to predict the class 4 (and also the "merged" classes {3, 4}) by using the assumption that the smallest the distance between R i and R j , the more likely the pair (i, j) should be labeled as 4. We compare to representations learnt by <ref type="bibr" target="#b1">[2]</ref> on the same training data, to word embeddings of <ref type="bibr" target="#b4">[5]</ref> (which are considered as efficient features in Natural Language Processing), and with three similarity measures provided by WordNet Similarity <ref type="bibr" target="#b18">[19]</ref>. For the latest, we only display the best one, named "path", which is built by counting the number of nodes along the shortest path between the senses in the "is-a" hierarchies of WordNet.</p><p>We report our results on precision-recall curves displayed in Figure <ref type="figure">1</ref> and the corresponding areas under the curve (AUC) in Table <ref type="table" target="#tab_3">3</ref>. Even though we tend to miss the first few pairs, we compare favorably to <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b4">[5]</ref> and our AUC is close to the reference established by WordNet Similarity.</p><p>Our method is capable of encoding meaningful semantic embeddings for verbs, even though it has been trained on noisy, automatically collected data and in spite of the fact that it was not our primary goal that distance in parameter space should satisfy any condition. Performance might be improved by training on cleaner triplets, such as those collected by <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Designing methods capable of handling large amounts of linked relations seems necessary to be able to model the wealth of relations underlying the semantics of any real-world problems. We tackle this problem by using a shared representation of relations naturally suited to multi-relational data, in which entities have a unique representation shared between relation types, and where we propose that relation themselves decompose over latent "relational" factors. This new approach ties or beats state-of-the art models on both standard relational learning problems and an NLP task. The decomposition of relations over latent factors allows a significant reduction of the number of parameters and is motivated both by computational and statistical reasons. In particular, our approach is quite scalable both with respect to the number of relations and to the data samples.</p><p>One might wonder about the relative importance of the various terms in our formulation. Interestingly, though the presence of the trigram term was crucial in the tensor factorization problems, it played a marginal role in the NLP experiment, where most of the information was contained in the bigram and unigram terms.</p><p>Finally, we believe that exploring the similarities of the relations through an analysis of the latent factors could provide some insight on the structures shared between different relation types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of the performance obtained by our approach, RESCAL</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Our approach</cell><cell>RESCAL [16]</cell><cell>MRC [10]</cell><cell>SME [2]</cell></row><row><cell>Kinships</cell><cell>Area under PR curve Log-likelihood</cell><cell>0.946 ± 0.005 -0.029 ± 0.001</cell><cell>0.95 N/A</cell><cell>0.84 -0.045 ± 0.002</cell><cell>0.907 ± 0.008 N/A</cell></row><row><cell>UMLS</cell><cell>Area under PR curve Log-likelihood</cell><cell>0.990 ± 0.003 -0.002 ± 0.0003</cell><cell>0.98 N/A</cell><cell>0.98 -0.004 ± 0.001</cell><cell>0.983 ± 0.003 N/A</cell></row><row><cell>Nations</cell><cell>Area under PR curve Log-likelihood</cell><cell>0.909 ± 0.009 -0.202 ± 0.008</cell><cell>0.84 N/A</cell><cell>0.75 -0.311 ± 0.022</cell><cell>0.883 ± 0.02 N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance obtained on the NLP dataset by our approach, SME<ref type="bibr" target="#b1">[2]</ref> and a bigram model. Details about the statistics of the table are given in the text.Practical training setup. During the training phase, we optimized over the validation set various parameters, namely, the size p ∈ {25, 50, 100} of the representations, the dimension d ∈ {50, 100, 200} of the latent decompositions (2), the value of the regularization parameter λ as a fraction {1, 0.5, 0.1, 0.05, 0.01} of n r × d, the stepsize in {0.1, 0.05, 0.01} and the weighting of the negative triplets. Moreover, to speed up the training, we gradually increased the number of sampled negative verbs (cf. Section 5.1), from 25 up to 50, which had the effect of refining the training.</figDesc><table><row><cell></cell><cell></cell><cell>0.78</cell><cell>0.95</cell><cell>19 / 96.7</cell><cell>0.89</cell><cell>0.98</cell></row><row><cell>SME [2]</cell><cell>56 / 199.6</cell><cell>0.77</cell><cell>0.95</cell><cell>19 / 99.2</cell><cell>0.89</cell><cell>0.98</cell></row><row><cell>Bigram</cell><cell>48 / 517.4</cell><cell>0.72</cell><cell>0.83</cell><cell>17 / 157.7</cell><cell>0.87</cell><cell>0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> • n r × d for which the coefficients α of the representations (2) are sparse in the sense they are dominated by few large values (e.g., the top 2% of the largest values of α account for about 25% of the total 1 -norm α 1 ). Precision-recall curves for the task of lexical similarity classification. The curves are computed based on different similarity measures between verbs, namely, our approach, SME</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Predicting class 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicting classes 3 and 4</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>Our approach</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>Our approach</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SME</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell>SME</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell>Collobert et al. Best WordNet</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell>Collobert et al. Best WordNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Precision</cell><cell>0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>0 0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance obtained on a task of lexical similarity classification</figDesc><table><row><cell></cell><cell cols="4">Our approach SME [2] Collobert et al. [5] Best WordNet [19]</cell></row><row><cell>AUC (class 4)</cell><cell>0.40</cell><cell>0.21</cell><cell>0.31</cell><cell>0.40</cell></row><row><cell>AUC (classes 3&amp;4)</cell><cell>0.54</cell><cell>0.36</cell><cell>0.48</cell><cell>0.59</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is called Statistical Predicate Invention by<ref type="bibr" target="#b9">[10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is motivated by the fact that we are primarily interested in modelling the relations terms, and that it is not necessary to introduce all terms to fully parameterize the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The code is available under an open-source license from http://goo.gl/TGYuh.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Other terms can be decomposed in a similar way.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The resulting new relationships were only used for training, and not considered at test time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The values of λ, d and p are searched in nr × d • {0.05, 0.1, 0.5, 1}, {100, 200, 500} and {10, 25, 50}.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Available from ronan.collobert.com/senna/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Lemmatization was carried out using NLTK (nltk.org) and transforms a word into its base form.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>The data set is available under an open-source license from http://goo.gl/TGYuh.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>Other distances could of course be considered, we choose the Euclidean metric for simplicity.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by the Pascal2 European Network of Excellence. NLR and RJ are supported by the European Research Council (resp., SIERRA-ERC-239993 &amp; SIPA-ERC-256919).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<title level="m">Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic models for incomplete multi-dimensional arrays</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="89" to="96" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The detection of patterns in Alyawarra nonverbal behavior</title>
		<author>
			<persName><forename type="first">W</forename><surname>Denham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parafac: parallel factor analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Lundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="72" />
			<date type="published" when="1994-08">Aug. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A large subcategorization lexicon for natural language processing applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Krymolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An upper level ontology for the biomedical domain</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comparative and Functional Genomics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="80" to="88" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">WordNet: a Lexical Database for English</title>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonparametric latent feature models for link prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1276" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Intl Conf. on Mach. Learn</title>
		<meeting>the 28th Intl Conf. on Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st intl conf. on WWW</title>
		<meeting>of the 21st intl conf. on WWW</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockstructures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nowicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A B</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">455</biblScope>
			<biblScope unit="page" from="1077" to="1087" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts using linear relational embedding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paccanaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet:: Similarity: measuring the relatedness of concepts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Demonstration Papers at HLT-NAACL 2004</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="38" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised ontology induction from text</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computl Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computl Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensionality of nations project: Attributes of nations and behavior of nation dyads</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Rummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPSR data file</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1950" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Document summarization using conditional random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Intl Joint Conf. on Artif. Intel</title>
		<meeting>of the 20th Intl Joint Conf. on Artif. Intel</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2862" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD&apos;08</title>
		<meeting>of SIGKDD&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. in Neur. Inf. Proc. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Some mathematical notes on three-mode factor analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels for directed graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">397</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Verb similarity on the taxonomy of wordnet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GWC-06</title>
		<meeting>GWC-06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max-margin nonparametric latent feature models for link prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Intl Conference on Machine Learning</title>
		<meeting>the 29th Intl Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
