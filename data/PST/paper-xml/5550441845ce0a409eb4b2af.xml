<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Win-Win Search: Dual-Agent Stochastic Game in Session Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
							<email>huiyang@cs.georgetown.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgetown University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">SIGIR&apos;14</orgName>
								<address>
									<addrLine>July 6-11, Gold Coast</addrLine>
									<postCode>2014</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Win-Win Search: Dual-Agent Stochastic Game in Session Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22F34A8F6B925A2351E2404B7C83F065</idno>
					<idno type="DOI">10.1145/2600428.2609629</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Systems ]: Information Storage and Retrieval-Information Search and Retrieval Dynamic Information Retrieval Modeling</term>
					<term>POMDP</term>
					<term>Stochastic Game</term>
					<term>Session Search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user's judgment of retrieved documents in the previous search iteration affects user's actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term "win-win search", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Users often need a multi-query session to accomplish a complex search task. A session usually starts with the user writing a query, sending it to the search engine, receiving a list of ranked documents ordered by decreasing relevance, then examining the snippets, clicking on the interesting ones, and spending more time reading them; we call one such sequence a "search iteration." In the next iteration, the user modifies the query or issues a new query to start the search again. As a result, a series of search iterations form, which include a series of queries q1, ..., qn, a series of returned documents D1, ..., Dn, and a series of clicks C1, ..., Cn, some of which are SAT clicks (satisfactory clicked documents <ref type="bibr">[9]</ref>). The session stops when the user's information need is satisfied or the user abandons the search <ref type="bibr" target="#b6">[6]</ref>. The information retrieval (IR) task in this setting is called session search <ref type="bibr">[7,</ref> Figure <ref type="figure">1</ref>: A Markov chain of decision states in session search. (S: decision states; q: queries; A: user actions such as query changes; D: documents).</p><p>11, <ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. Table <ref type="table" target="#tab_0">1</ref> lists example information needs and queries in session search.</p><p>We are often puzzled about what drives a user's search in a session and why they make certain moves. We observe that sometimes the same user behavior, such as a drift from one subtopic to another, can be explained by opposite reasons: either the user is satisfied with the search results and moves to another sub information need, or the user is not satisfied with the search results and leaves the previous search path. The complexity of users' decision making patterns makes session search quite challenging <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b29">29]</ref>.</p><p>Researchers have attempted to find out the causes of topic drifting in session search. The causes under study include personalization <ref type="bibr" target="#b29">[29]</ref>, task types <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b24">24]</ref>, and previous documents' relevance <ref type="bibr" target="#b11">[11]</ref>. A user study is usually needed to draw conclusions about user intent. However, the focus of this paper is not on identifying user intent. Instead, we simplify the complexity of users' decision states into a cross product of only two dimensions: whether previously retrieved documents are relevant and whether the user would like to explore the next sub information need. Our work differs from existing work in that we consider that a session goes through a series of hidden decision states, with which we design a statistical retrieval model for session search. Our emphasis is an effective retrieval model, not a user study to identify the query intent.</p><p>The hidden decision states form a Markov chain in session search. A Markov chain is a memoryless random process where the next state depends only on the current state <ref type="bibr" target="#b18">[18]</ref>. Figure <ref type="figure">1</ref> illustrates a Markov chain of hidden decision states for TREC 2013 Session 9. In a session, a user's judgment of the retrieved documents in the previous iteration affects or even decides the user's actions in the next iteration. A user's actions could include clicks, query changes, reading the documents, etc. The user's gain, which we call reward, is the amount of relevant information that he or she obtains in the retrieved documents. The reward motives the later user actions. If the decision states are known, we can use a Markov Decision Process (MDP) to model the process. However, in session search, users' decision states are hidden. We therefore model session search as a Partially Observable Markov Decision Process (POMDP) <ref type="bibr" target="#b18">[18]</ref>. Session 2: Information Need Session 2: Queries You want to buy a scooter. So you're inter-q 1 =scooter brands q 2 =scooter brands reliable q 3 =scooter ested in learning more facts about scooters q 4 =scooter cheap q 5 =scooter review q 6 =scooter price including:what brands of scooters are out q 7 =scooter price q 8 =scooter stores q 9 =where to buy scooters there? What brands of scooters are reliable? Which scooters are cheap? Which stores sell scooters? which stores sell the best scooters? Session 9: Information Need Session 9: Queries You want to know more about old US coins. q 1 =old us coins q 2 =collecting old us coins q 3 =selling old us coins Relevant information to you includes value of old US coins, types of old US coins, old US silver dollar, q 4 =selling old "usa coins" how to start collecting old US coins, how to sell old US coins and how to buy them, where to buy those coins.</p><p>Session 87: Information Need Session 87: Queries Suppose you're planning a trip to the q 1 =best us destinations q 2 =distance new york boston United States.You will be there for a q 3 =maps.bing.com q 4 =maps q 5 =bing maps month and able to travel within a 150-mile q 6 =hartford tourism q 7 =bing maps q 8 =hartford visitors radius of your destination.With that q 9 =hartford connecticut tourism q 10 =hartford boston travel constraint, what are the best cities to q 11 =boston tourism q 12 =nyc tourism q 13 =philadelphia nyc distance consider as possible destinations? q 14 =bing maps q 15 =philadelphia washington dc distance q 16 =bing maps q 17 =philadelphia tourism q 18 =washington dc tourism q 19 =philadelphia nyc travel q 20 =philadelphia nyc train q 21 =philadelphia nyc bus In fact, not only the user, but also the search engine, makes decisions in a Markov process. A search engine takes in a user's feedback and improves its retrieval algorithm iteration after iteration to achieve a better reward too. The search engine actions could include term weighting, turning on or turning off one or more of its search techniques, or adjusting parameters for the techniques. For instance, based on the reward, the search engine can select p in deciding the top p documents used in pseudo relevance feedback.</p><p>We propose to model session search as a dual-agent stochastic game. When there is more than one agent in a POMDP, the POMDP becomes a stochastic game (SG). The two agents in session search are the user agent and the search engine agent. In contrast to most two-player scenarios such as chess games in game theory, the two agents in session search are not opponents to each other; instead, they cooperate: they share the decision states and work together to jointly maximize their goals. We term the framework "win-win search" for its efforts in ensuring that both agents arrive at a winwin situation. One may argue that in reality a commercial search engine and a user may have different goals and that is why some commercial search engines put their sponsors high in the returned results. However, this paper focuses on the win-win setting and assume a common interest -fulfilling the information needs -for both agents.</p><p>The challenges of modeling session search as a stochastic game lie in how to design and determine the decision states and actions of each agent, how to observe their behaviors, and how to measure the rewards and set the optimization goals. We present the details in Sections 4 and 5. As a retrieval framework, we pay more attention to the search engine agent. When the search engine makes decisions, it picks a decision that jointly optimizes the common interest.</p><p>We evaluate the win-win search framework on TREC 2012 &amp; 2013 Session data. TREC (Text REtrieval Conference) 2010 -2013 Session Tracks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref> have spurred a great deal of research in session search <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b24">24]</ref>. The tracks provide interaction data within a session and aim to retrieve relevant documents for the last query qn in the session. The interaction data include queries, top returned documents, user clicks, and other relevant information such as dwell time. Document relevance is judged based on information need for the entire session, not just the last query. In this paper, all examples are from TREC 2013. Our experiments show that the proposed framework achieves statisti-cally significant improvements over state-of-the-art interactive search and session search algorithms.</p><p>The remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 provides preliminaries for POMDP. Section 4 details the win-win search framework, Section 5 elaborates the optimization, Section 6 evaluates the framework and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Session search</head><p>Session search has attracted a great amount of research from a variety of approaches <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b33">33]</ref>. They can be grouped into log-based methods and content-based methods.</p><p>There is a large body of work using query logs to study queries and sessions. Feild and Allan <ref type="bibr" target="#b8">[8]</ref> proposed a taskaware model for query recommendation using random walk over a term-query graph formed from logs. Song and He's work <ref type="bibr" target="#b27">[27]</ref> on optimal rare query suggestion also used random walk, with implicit feedback in logs. Wang et al. <ref type="bibr" target="#b30">[30]</ref> utilized the latent structural SVM to extract cross-session search tasks from logs. Recent log-based approaches also appear in the Web Search Click Data (WCSD) workshop series. 1  Content-based methods directly study the content of the query and the document. For instance, Raman et al. <ref type="bibr" target="#b25">[25]</ref> studied a particular case in session search where the search topics are intrinsically diversified. Content-based session search also include most research generated from the recent TREC Session Tracks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. Guan et al. <ref type="bibr" target="#b10">[10]</ref> organized phrase structure in queries within a session to improve retrieval effectiveness. Jiang et al. <ref type="bibr" target="#b14">[14]</ref> proposed an adaptive browsing model that handles novelty in session search. Jiang and He <ref type="bibr" target="#b13">[13]</ref> further analyzed the effects of past queries and click-through data on whole-session search effectiveness.</p><p>Others study even more complex search -search across multiple sessions <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30]</ref>. Kotov et al. <ref type="bibr" target="#b22">[22]</ref> proposed methods for modeling and analyzing users' search behaviors in multiple sessions. Wang et al. <ref type="bibr" target="#b30">[30]</ref> identified cross-session search by investigating inter-query dependencies learned from user behaviors.</p><p>Our approach is a content-based approach. However, it uniquely differs from other approaches by taking a Markov process point of view to study session search. 1 http://research.microsoft.com/en-us/um/people/ nickcr/wscd2014</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance feedback</head><p>Session search is closely related to relevance feedback, a traditional IR research field. Classic relevance feedback methods include Rocchio <ref type="bibr" target="#b16">[16]</ref>, pseudo relevance feedback <ref type="bibr" target="#b2">[2]</ref>, and implicit relevance feedback <ref type="bibr" target="#b27">[27]</ref> based on user behaviors such as clicks and dwell time. Recently, researchers have investigated new forms of relevance feedback. Jin et al. <ref type="bibr" target="#b15">[15]</ref> employed a special type of click -"go to the next page" -as relevance feedback to maximize retrieval effectiveness over multi-page results. Zhang et al. <ref type="bibr" target="#b33">[33]</ref> modeled query changes between adjacent queries as relevance feedback to improve retrieval accuracy in session search.</p><p>These relevance feedback approaches only considers oneway communication from the user to the search engine <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b33">33]</ref>. On the contrary, this paper explicitly sets up a two-way feedback channel where both parties transmit information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">MDP and POMDP in IR</head><p>Markov Decision Process (MDP) is an important topic in Artificial Intelligence (AI). An MDP can be solved by a family of reinforcement learning algorithms. Kaelbling et al. <ref type="bibr" target="#b18">[18]</ref> brought techniques from operational research to choose the optimal actions in partially observable problems, and designed algorithms for solving Partially Observable Markov Decision Processes (POMDPs). IR researchers have just begun showing interests in MDP and POMDP <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32]</ref> in finding solutions for IR problems.</p><p>Early work on interactive search modeling by Shen et al. <ref type="bibr" target="#b26">[26]</ref> used a Bayesian decision-theoretic framework, which is closely related to the MDP approaches. The QCM model proposed by Guan et al. <ref type="bibr" target="#b11">[11]</ref> models session search as an MDP and effectively improves the retrieval accuracy. However, <ref type="bibr" target="#b11">[11]</ref> used queries as states while we use a set of welldesigned hidden decision states. In addition, we explicitly model the stochastic game played between two agents, the user and the search engine, while <ref type="bibr" target="#b11">[11]</ref> focused on just the search engine. Another difference is that we model a wide range of actions including query changes, clicks, and document content while <ref type="bibr" target="#b11">[11]</ref> only used query changes.</p><p>Yuan and Wang <ref type="bibr" target="#b32">[32]</ref> applied POMDP for sequential selection of online advertisement recommendation. Their mathematical derivation shows that belief states of correlated ads can be updated using a formula similar to collaborative filtering. Jin et al. <ref type="bibr" target="#b15">[15]</ref> modeled Web search as a sequential search for re-ranking documents in multi-page results. Their hidden states are document relevance and the belief states are given by a multivariate Gaussian distribution. They consider "ranking" as actions and "clicking-on-the-next-page" as observations. In win-win search, we present a different set of actions, observations, and messages between two agents. The fundamental difference between our approach and theirs is that we model the retrieval task as a dual-agent cooperative game while <ref type="bibr" target="#b15">[15]</ref> uses a single agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRELIMINARIES: MDP AND POMDP</head><p>Markov Decision Process provides the basics for the winwin search framework. An MDP is composed by agents, states, actions, reward, policy, and transitions <ref type="bibr" target="#b19">[19]</ref>. An agent takes inputs from the environment and outputs actions. The actions in turn influences the states of the environment. An MDP can be represented by a tuple &lt; S, A, T, R &gt;:</p><p>States S is a discrete set of states. In session search, they can be queries <ref type="bibr" target="#b11">[11]</ref> or hidden decision states (Section 4.2).</p><p>Actions A is a discrete set of actions that an agent can take. For instance, user actions include query changes, clicks, and reading the returned documents or snippets.</p><p>Transition T is the state transition function T (si, a, sj) = P (sj|si, a). It is the probability of starting in state si, taking action a, and ending in state sj. The sum over all actions gives the total state transition probability between si and sj T (si, sj) = P (sj|si); which is similar to the state transition probability in the Hidden Markov Model (HMM) <ref type="bibr" target="#b1">[1]</ref>.</p><p>Reward r = R(s, a) is the immediate reward, also known as reinforcement. It gives the expected immediate reward of taking action a at state s.</p><p>Policy π describes the behaviors of an agent. A nonstationary policy is a sequence of mapping from states to actions. π is usually optimized to decide how to move around in the state space to optimize the long term reward ∞ t=1 r. Value function and Q-function Given a policy π at time t, a value function V calculates the expected long term reward starting from state s inductively:</p><formula xml:id="formula_0">V * π,t (s) = R(s, πt(s))+ γ s T (s, a = πt(s), s )V * π,t+1<label>(s )</label></formula><p>, where the initial value Vπ,t=1(s) = R(s, a = πt=1(s)), s is the current state, s is the next state, a = πt(s) is any valid action for s at t, γ is a future discount factor. Usually an auxiliary function, called the Q-function, is used for a pair of (s, a):</p><formula xml:id="formula_1">Q * (st, a) = R(st, a) + γ a P (st|st+1, a) maxa Q(st+1, a),</formula><p>where R(st, a) is the immediate reward at t, a is any valid action at t + 1. Note that V * (s) = maxa Q * (s, a).</p><p>Q-Learning Reinforcement learning (RL) algorithms provides solutions to MDPs <ref type="bibr" target="#b19">[19]</ref>. The most influential RL algorithm is Q-learning. Given a Q-function and a starting state s, the solution can be a greedy policy that at each step, it takes the action that maximizes the Q-function:</p><formula xml:id="formula_2">π * (s) = arg maxa R(s, a) + γ a T (s, a, s )Q(s , a)</formula><p>, where the base case maximizes R(s1, a).</p><p>Partially Observable MDP (POMDP) When states are unknown and can only be guessed through a probabilistic distribution, an MDP becomes a POMDP <ref type="bibr" target="#b18">[18]</ref>. POMDP is represented by a tuple &lt; S, A, T, Ω, O, B, R &gt;, where S, A, R are the same as in MDP. Since the states are unknown, the transition function T models transitions between beliefs, not transitions between states any more:</p><formula xml:id="formula_3">T : B × A × B → [0, 1]. Belief B</formula><p>is the set of beliefs defined over S, which indicates the probability that an agent is at a state s. It is also known as belief state. Observations Ω is a discrete set of observations that an agent makes about the states. O is the observation function which represents a probabilistic distribution for making observation ω given action a and landing in the next state s . A major difference between an HMM and a POMDP is that POMDP considers actions and rewards while HMM does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE WIN-WIN SEARCH FRAMEWORK</head><p>A session can be viewed as a Markov chain of evolving states (Figure <ref type="figure">1</ref>). Every time when a new query is issued, both the user and the search engine transition into a new state. In our setting, the two agents work together to achieve a win-win goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Illustration</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the proposed dual-agent SG, which is represented as a tuple &lt; S, Au, Ase, Σu, Σse, Ωu, Ωse, O, B, T, R &gt;.</p><p>S is the decision states that we will present in Section 4.2.  The reward function R is defined over B×A → R. It is the amount of document relevance that an agent obtains from the world. Rse is the nDCG score (normalized Discounted Cumulative Gain <ref type="bibr" target="#b20">[20]</ref>) that the search engine gains for the documents it returns. Ru is the relevance that the user gains from reading the documents. Our retrieval algorithm jointly optimize both Ru and Rse. (Section 5)</p><p>Table <ref type="table" target="#tab_1">2</ref> lists the symbols and their meanings in the dualagent SG. The two agents share the decision states and beliefs but differ in actions, messages, and policies. Although they also make different observations, both contribute to the belief updater; the difference is thus absorbed. As a retrieval model, we only pay attention to the search engine policy πse : B → A. The following describes their interactions in the stochastic game: Steps 3, 6, and 8 happen after making an observation from the world or from the other agent. They then all involve a belief update. In the remainder of this section, we present the details of states (Section 4.2), actions (Section 4.3), observation functions (Section 4.4), and belief updates (Section 4.5) for win-win search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">States</head><p>We often observe that the same user behavior in session search may be motivated by different reasons. For instance, in TREC 2013 Session 2 (Table <ref type="table" target="#tab_0">1</ref>), a user searches for "scooter brands" as q1 and finds that the 6 th returned document with title "Scooter Brands -The Scooter Review -The Scooter Review" is relevant. The user clicks this document and reads it for 48 seconds, which we identify as a SAT click since it lasts more than 30 seconds <ref type="bibr" target="#b12">[12]</ref>. Next, the user adds a new term 'reliable' into q1 to get q2 ="scooter brands reliable". We notice that 'reliability' does not appear in any previously retrieved documents D1. It suggests that the user is inspired to add 'reliability' from somewhere else, such as from personal background knowledge or the in-formation need. In this case, she finds relevant documents from the previously retrieved documents but still decides to explore other aspects about the search target.</p><p>On the contrary, in TREC 2013 Session 9 q1 (Table <ref type="table" target="#tab_0">1</ref>), another user searches for "old US coins" and also finds relevant documents, such as a document about "... We buy collectible U.S.A. coins for our existing coin collector clients...". He adds a new term 'collecting' to get the next query q2 "collecting old us coins". After reducing the query terms and document terms into their stemmed forms, the added term 'collecting' does appear in this document as we can see. It suggests that the user selects a term from the retrieval results and hones into the specifics. In this case, he finds relevant documents in the previously retrieved documents and decides to exploit the same sub information need and investigate it more.</p><p>We observe that even if both users show the same search behavior, e.g. adding terms, the reasons vary: one is adding the new search term because the original search results are not satisfactory, while the other is because the user wants to look into more specifics. This makes us realize that document relevance and users' desire to explore are two independent dimensions in deciding how to form the next query.</p><p>Inspired by earlier research on user intent and task types <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28]</ref> and our own observations, we propose four hidden decision making states for session search. They are identified based on two dimensions: 1) "relevant dimension" -whether the user thinks the returned documents are relevant, and 2) "exploration dimension" -whether the user would like to explore another subtopic. The two dimensions greatly simplify the complexity of user modeling in session search. The relatively small number of discrete states enables us to proceed with POMDP and its optimization at low cost.</p><p>The cross-product of the two dimensions result in four states: i) user finds a relevant document from the returned documents and decides to explore the next sub information need (relevant and exploration, e.g. scooter price → scooter stores), ii) user finds relevant information and decides to stay in the current sub information need to look into more relevant information (relevant and exploitation, e.g. hartford visitors → hartford connecticut tourism), iii) user finds out that the returned documents are not relevant and decides to stay and try out different expressions for the same sub information need (non-relevant and exploitation, e.g. philadelphia nyc travel → philadelphia nyc train), iv) user finds out that documents are not relevant and decides to give up and move on to another sub information need (nonrelevant and exploration, e.g. distance new york boston → maps.bing.com).</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the decision state diagram for win-win search. The subscriptions stand for {RT = Relevantexploi T ation, RR = RelevantexploRation, N RT = N onRelevant exploiT ation, N RR = N onRelevantexploRation}. We insert a dummy starting query q0 before any real query and it always goes to SNRR. The series of search iterations in a session move in the decision states from one to the next. A sequence of states can be time stamped and presented as st = Sm, where t = 1, 2, ..., n and m = {RT, RR, N RT, N RR}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Actions</head><p>There are two types of actions in our framework, domainlevel actions and communications-level actions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Domain-Level Actions</head><p>The domain-level actions Au and Ase represent the actions directly performed on the world (document collection) by the user agent and by the search engine agent, respectively.</p><p>The common user actions include writing a query, clicking a document, SAT clicking a document, reading a snippet, reading a document, changing a query, and eye-tracking the documents. In this paper, we only study query changes and clicks as user actions. However, the framework can be easily adopted for other types of user actions.</p><p>Query changes ∆q <ref type="bibr" target="#b11">[11]</ref> consist of added query terms +∆qt = qt\qt-1, removed query terms -∆qt = qt-1\qt, and theme terms q theme = LongestCommon Subsequence(qt, qt-1). For example, in Session 87 , given q19="philadelphia nyc travel" and q20="philadelphia nyc train", we obtain the following query changes: q theme = LCS(q19, q20) = "philadelphia", -∆q20 = "travel", and +∆q20 = "train". All stopwords and function words are removed.</p><p>The search engine domain-level actions Ase include increasing, decreasing, and maintaining the term weights, as well as adjusting parameters in one or more search techniques. We present the details in Sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Communications-Level Actions (Messages)</head><p>The second type of actions are communication-level actions (messages) Σu and Σse. They are actions that only performed between agents.</p><p>In our framework, the messages are essentially documents that an agent thinks are relevant. Σu is the set of documents that the user sends out; we define them as the clicked documents D clicked . In TREC 2013 Session, 31% search iterations contain SAT clicked documents. 23.9% sessions contain 1 to 4 SAT clicked documents, and a few sessions, for instance Sessions 45, 57 and 72, contain around 10 SAT clicked documents. 88.7% SAT clicked documents appear in the top 10 retrieved results.</p><p>Similarly, Σse is the set of documents that the search engine sends out. They are the top k returned documents (k ranges from 0 to 55 in the TREC setting). They demonstrate what documents the search engine thinks are the most relevant. In TREC 2013, 2.8% (10) search iterations return less than 10 documents, 90.7% (322) return exactly 10, 5.1% (18) return 10∼20, and 1.4% (5) return 20∼55 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Observations</head><p>Section 4.1 illustrates the win-win search framework and the interactions between agents. This section shows how we calculate the observation functions.</p><p>The observation function O(sj, at, ωt), defined as P (ωt|sj, at), is the probability of observing ωt ∈ Ω when agents take action at and land on state sj. The first type of observation is related to relevance. In Section 4.1 Step 8, after the user sends the message Σu (user clicks) out at Step 7, the search engine updates its after-message-belief-state bΣ•se based on its observation of user clicks. The observation function for 'Relevant' states is:</p><formula xml:id="formula_4">O(st=Rel, Σu, ωt=Rel) def = == = P (ωt = Rel|st = Rel, Σu)</formula><p>(1) It can be written as P (ω t =Rel,s t =Rel,Σu) P (s t =Rel,Σu)</p><p>. By taking P (st = Rel, Σu) as a constant, we can approximate it by P (ωt = Rel, st = Rel, Σu) = P (st = Rel|ωt = Rel, Σu)P (ωt = Rel, Σu). Given that user clicks Σu are highly correlated to ωt, we can approximate P (st = Rel|ωt = Rel, Σu) by P (st = Rel|ωt = Rel). Further, by taking P (Σ) as a constant, we have O(st=Rel, Σu, ωt=Rel) ∝ P (st = Rel|ωt = Rel)P (ωt = Rel, Σu) ∝ P (st = Rel|ωt = Rel)P (ωt = Rel|Σu)</p><p>(2) Similarly, we have O(st=Non-Rel, Σu, ωt=Non-Rel) ∝ P (st = Non-Rel|ωt = Non-Rel)P (ωt = Non-Rel|Σu)</p><p>(3) as well as O(st=Non-Rel, Σu, ωt=Rel) and O(st=Rel, Σu, ωt=Non-Rel).</p><p>Based on whether a SATClick exists or not, we calculate the probability of the SG landing at the "Relevant" states or the "Non-Relevant" states (the first dimension of hidden decision states). At search iteration t, if the set of previously returned documents leads to one or more SAT clicks, the current state is likely to be relevant, otherwise non-relevant. That is to say,</p><formula xml:id="formula_5">st is likely to be Relevant if ∃ d ∈ D t-1 and d is SATClicked Non-Relevant otherwise.</formula><p>Based on this intuition, we calculate P (ωt = Rel|Σu) and P (ωt = Non-Rel|Σu) as:</p><formula xml:id="formula_6">P (ωt = Rel|Σu) = P (∃ SATClicks ∈ D t-1 clicked )<label>(4)</label></formula><p>P (ωt = Non-Rel|Σu) = P ( SATClicks ∈ D t-1 clicked ) (5) The conditional probability of observations P (st = Rel|ωt = Rel) and P (st = Non-Rel|ωt = Non-Rel) can be calculated by maximum likelihood estimation (MLE). For instance, P (st = Rel|ω = Rel) = # of observed true relevant # of observed relevant , where "# of observed true relevant" is the number of times where the previously returned document set Dt-1 contain at least one SAT clicks and those SAT clicked documents are indeed relevant documents in the ground truth. "# of observed relevant" is the number of times where Dt-1 contains at least one SAT clicks. The ground truth of whether the SG lands on a "Relevant" state is generated by documents whose relevance grades ≥ 3 (relevant to highly relevant). The relevance are judged by NIST assessors <ref type="bibr" target="#b21">[21]</ref>.</p><p>The second type of observation is related to exploitation vs. exploration. This corresponds to a combined observation at Step 3 and the previous Step 6 (Section 4.1), where the SG update the before-message-belief-state b•Σse for a user action au (query change) and a search engine message Σse=Dt-1, the top returned documents at the previous iteration. The search engine agent makes observations about exploitation vs. exploration (the second dimension of hidden decision states) by: O(st=Exploitation, au=∆qt, Σse=D t-1 , ωt=Exploitation)</p><formula xml:id="formula_7">∝ P (st = Exploitation|ωt = Exploitation) ×P (ωt = Exploitation|∆qt, D t-1 ) (6) O(st=Exploration,au=∆qt, Σse=D t-1 , ωt=Exploration) ∝ P (st = Exploration|ωt = Exploration) ×P (ωt = Exploration|∆qt, D t-1 )<label>(7)</label></formula><p>The search engine can guess the hidden states based on the following intuition:</p><formula xml:id="formula_8">st is likely to be      Exploration if (+∆qt = ∅ and +∆qt / ∈ D t-1 ) or (+∆qt = ∅ and -∆qt = ∅ ) Exploitation if (+∆qt = ∅ and +∆qt ∈ D t-1 )</formula><p>or (+∆qt = ∅ and -∆qt = ∅ )</p><p>The idea is that given that Dt-1 is the message from search engine and au = ∆q is the message from user, if added query terms +∆q appear in Dt-1, it is likely that the user stays at the same sub information need from iteration t -1 to t for 'exploitation'. On the other hand, if the added terms +∆q do not appear in Dt-1, it is likely that the user moves to the next sub information need from iteration t -1 to t for 'exploration'. In addition, if there is no added terms (+∆qt is empty) but there are deleted terms ( -∆qt is not empty), it is likely that the user goes to a broader topic to explore. If +∆qt and -∆qt are both empty, it means there is no change to the query, it is likely to fall into exploitation. Hence, P (ωt|∆qt, Dt-1) can be calculated as:</p><formula xml:id="formula_9">P (ωt = Exploration|∆qt, Dt-1) = P (+∆qt = ∅ ∧ +∆qt / ∈ Dt-1) +P (+∆qt = ∅ ∧ -∆qt = ∅) (8) P (ωt = Exploitation|∆qt, Dt-1) = P (+∆qt = ∅ ∧ +∆qt ∈ Dt-1) +P (+∆qt = ∅ ∧ -∆qt = ∅) (9)</formula><p>where Dt-1 include all clicked documents and all snippets that are ranked higher than the last clicked document at iteration t -1. User actions au include the current query changes +∆qt and -∆qt. In fact, P (ωt|∆qt, Dt-1) needs to be calculated for each specific case. For instance, P (ωt = Exploration|a = 'delete term', ∆qt, Dt-1) = # of observed true explorations due to deleting terms # of observed explorations due to deleting terms . Here we only calculate for the actions with "deleted terms". "# of observed explorations" is the number of observed explorations suggesting that the user is likely to explore another subtopic based on Eq. 8, while "# of observed true explorations" is the number of observed explorations judged positive by human accessors in a ground truth. The annotations can be found online. <ref type="foot" target="#foot_0">2</ref>The conditional probability P (st = Exploitation|ωt = Exploitation) is calculated as # of observed true exploitations # of observed exploitations , where "# of observed exploitations" is the number of observed exploitations suggesting that the user is likely to exploit the same subtopic (based on Eq. 9), and "# of observed true exploitations" is the number of observed exploitations that are judged positive in the ground truth. P (st = Exploration|ωt = Exploration) is calculated in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Belief Updates</head><p>At every search iteration the belief state b is updated twice; once at Step 3, another at Step 8. It reflects the interaction and cooperative game between the two agents.</p><p>A belief bt(si) is defined as P (si|at, bt). The initial belief states can be calculated as: b0(si = Sz) = P (si = Sx)P (si = Sy), where x ∈ {R = Rel, N R = N on-Rel}, y ∈ {R = exploRation, T = exploiT ation}, z is the cross-product of x and y and z ∈ {RR, RT, N RR, N RT }. In addition, 0 ≤ b(si) ≤ 1 and s i b(si) = 1.</p><p>The belief update function is bt+1(sj) = P (sj|ωt, at, bt) by taking into account new observations ωt. It is updated from iteration t to iteration t + 1:</p><formula xml:id="formula_10">b t+1 (s j ) def = == = P (s j |ωt, at, bt) =</formula><p>O(s j , at, ωt) s i ∈S T (s i , at, s j )bt(s i )</p><formula xml:id="formula_11">P (ωt|at, bt)<label>(10)</label></formula><p>where si and sj are two states, i, j ∈ {RR, RT, N RR, N RT }. t indices the search iterations, and O(sj, at, ωt) = P (ωt|sj, at) is calculated based on Section 4.4. P (ωt|at, bt) is the normalization factor to keep</p><formula xml:id="formula_12">s i ∈S b(si) = 1.</formula><p>For notation simplicity, we will only use a to represent actions from now on. However, it is worthy noting that actions can be both domain-level actions a and messages Σ.</p><p>Transition probability T (si, at, sj) is defined as P (sj|si, at, bt). It is can be calculated as T (si, at, sj) = #T ransition(s i ,a t ,s j ) #T ransition(s i ,a t ,s * ) , where Transition (si, at, sj) is the sum of all transitions that starts at state si, takes action at, and lands at state sj. T ransition (si, at, s * ) is the sum of all transitions that starts at state si and lands at any state by action at.</p><p>Finally, taking O(sj, at, ωt) = P (ωt|sj, at), which also equals to P (ωt|sj, at, bt) when we consider beliefs, and T (si, at, sj) = P (sj|si, at, bt), the updated belief can be written as: b t+1 (s j ) = P (ωt|s j , at, bt) s i ∈S P (s j |s i , at, bt)bt(s i ) P (ωt|at, bt) = P (ωt|s j , at, bt) s i ∈S P (s j |s i , at, bt)bt(s i )</p><formula xml:id="formula_13">s k ∈S P (ωt|s k , at) s i ∈S P (s k |s i , at)bt(s i )<label>(11)</label></formula><p>where bt(si) is P (si|at, bt), whose initial value is b0(si).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">JOINT OPTIMIZATION AND RETRIEVAL</head><p>After every search iteration, we decide the actions for the search engine agent. We employ Q-learning <ref type="bibr" target="#b18">[18]</ref> to find out the optimal action. For all a ∈ Ase, we write the search engine's Q-function, which represents the search engine agent's long term reward, as:</p><formula xml:id="formula_14">Qse(b, a) = ρ(b, a)+γ ω∈Ω P (ω|b, au, Σse)P (ω|b, Σu) max a Qse(b , a)<label>(12)</label></formula><p>where the reward for a belief state b is ρ(b, a) = s∈S b(s)R(s, a). P (ω|b, au, Σse) corresponds to Eq. 8 and Eq. 9 and P (ω|b, Σu) corresponds to Eq. 4 and Eq. 5. b is the belief state updated by Eq. 11.</p><p>In win-win search, we take into account both the search engine reward and the user reward. As in <ref type="bibr" target="#b11">[11]</ref>, we have Qu calculated as the long term reward for the user agent:</p><formula xml:id="formula_15">Qu(b, au) = R(s, au) + γ au T (st|s t-1 , D t-1 ) maxs t-1 Qu(s t-1 , au) = P (qt|d) + γ a P (qt|q t-1 , D t-1 , a) max D t-1 P (q t-1 |D t-1 )<label>(13)</label></formula><p>which recursively calculates the reward starting from q1 and continues with the policy until qt. P (qt|d) is the current reward that the user gains through reading the documents. maxD t-1 P (qt-1|Dt-1) is the maximum of the past rewards. The formula matches well with common search scenarios where the user makes decisions about their next actions based on the most relevant document(s) they examined in the previous run of retrieval. Such a document we call it maximum rewarding document(s). We use document with the largest P (qt-1|dt-1) as the maximum rewarding document. P (qt-1|dt-1) is calculated as 1-t∈q t-1 {1 -P (t|dt-1)},</p><p>where</p><formula xml:id="formula_16">P (t|dt-1) = #(t,d t-1 ) |d t-1 | , #(t, dt-1)</formula><p>is the number of occurrences of term t in document dt-1, and |dt-1| is the document length.</p><p>By optimizing both long term rewards for the user and for the search engine, we learn the best policy π and use it to predict the next action for the search engine. The joint optimization for the dual-agent SG can be represented as:</p><formula xml:id="formula_17">ase = arg max a Qse(b, a) + Qu(b, au)<label>(14)</label></formula><p>where ase ∈ Ase at t = n and n is the number of search iterations in a session, i.e., the session length.</p><p>In win-win search, Ase can include many search engine actions. One type of actions is adjusting a query's term weight. Assuming the query is reformulated from the previous query by adding +∆q or deleting -∆q. That is to say, Ase = {increasing, decreasing, or keeping term weights}. The term weights are increased or decreased by multiplying a factor. We also use a range of search techniques/algorithms as action options for the search engine agent. They are reported in Section 6. Based on Eq. 14, the win-win search framework picks the optimal search engine action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>We evaluate the proposed framework on TREC 2012 and TREC 2013 Session Tracks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. The session logs are collected from users through a search system by the track organizers. The topics, i.e., information need (Table <ref type="table" target="#tab_0">1</ref>), are provided to users. The session logs record all URLs displayed to the user, snippets, clicks, and dwell time. Table <ref type="table" target="#tab_3">3</ref> shows the dataset statistics. The task is to retrieve a ranked list of 2,000 documents for the last query in a session. Document relevance is judged based on the whole-session relevance. We use the official TREC evaluation metrics in our experiments. They include nDCG@10, nERR@10, nDCG, and MAP <ref type="bibr" target="#b21">[21]</ref>. The ground truth relevant documents are provided by TREC.</p><p>The corpora used in our experiments are ClueWeb09 CatB (50 million English web pages crawled in 2009, used in TREC 2012), and ClueWeb12 CatB (50 million English web pages crawled in 2012, used in TREC 2013). Documents with the Waterloo spam scores <ref type="bibr" target="#b5">[5]</ref> less than 70 are filtered out. All duplicated documents are removed. We compare our system with the following systems: Lemur <ref type="bibr" target="#b23">[23]</ref> (language modeling + Dirichlet smoothing), PRF (Pseudo Relevance Feedback in Lemur assuming the top 20 documents are relevant), Rocchio (relevance feedback that as-   <ref type="bibr" target="#b11">[11]</ref>), and QCM SAT (a variation of QCM by <ref type="bibr" target="#b33">[33]</ref>). We choose Rocchio (a state-of-the-art interactive search algorithm) and QCM+DUP (a state-of-the-art session search algorithm) as two baseline systems and all other systems are compared against them. TREC median and TREC best scores are also included for reference. Note that TREC best are an aggregation from the best scores of each individual submitted TREC runs; it is not a single search system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Search Accuracy</head><p>Our run, win-win, implements six retrieval technologies. They are: (1) increasing weights of the added terms (+∆q) by a factor of x={1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2};</p><p>(2) decreasing weights of the added terms by a factor of y ={ 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95}; (3) the term weighting scheme proposed in <ref type="bibr" target="#b11">[11]</ref> with parameters α, β, δ, γ set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance Feedback) algorithm which assumes the top p retrieved documents are relevant while p ranges from 1 to 20; (5) an adhoc variation of win-win, which directly uses the last query in a session to perform retrieval; and (6) a brute-force variation of win-win, which combines all queries in a session, extracts all unique query terms from them, and weights them equally. Win-win examine 21 search engine action options in total to find out the optimal action that maximizes the joint long term reward Qse(b, a) + Qu(b, au) for both agents.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the search accuracy of all systems under comparison for TREC 2012 Session Track. We can see that win-win search is better than most systems except QCM SAT. It statistically significantly outperforms Rocchio by 20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10 (p-value&lt;.05, one-side t-test). It also outperforms Rocchio-CLK, Rocchio-SAT and QCM+DUP, but the results are not statistically significant. The trends for other evaluation metrics are similar to nDCG@10.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows the search accuracy of all systems for TREC 2013 Session Track. Since we only indexed ClueWeb12 CatB, after spam reduction, many relevant CatA documents are not included in the CatB collection. To evaluate the systems fairly, we created a filtered ground truth which only consists of relevant documents in CatB. The results are shown in Table <ref type="table" target="#tab_5">5</ref>. We can see that win-win is the best run among all systems. It shows statistically significant gain (p-value&lt;.01, one-sided t-test) over all other systems across all evaluation metrics. Particularly, the proposed approach achieves a significant 54% improvement of nDCG@10 comparing to QCM+DUP. The experimental results support that our approach is highly effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Immediate Search Accuracy</head><p>TREC Session tasks request for retrieval results for the last query in a session. Theoretically, however, win-win search can optimize at every search iteration throughout a session. We hence compare our approach (the Win-Win run) with the top returned documents provided by TREC (the Original run) in terms of immediate search accuracy. We define immediate search accuracy at i as an evaluation score that measures search accuracy at search iteration i. The evaluation scores used are nDCG@10 and nERR@10.</p><p>We report the averaged immediate search accuracy for all sessions. It is worthy noting that session lengths vary. To average across sessions with different lengths, we make all sessions equals to the maximum session length in a dataset. TREC 2012 and 2013 Session have different maximum session lengths; they are 11 and 21, respectively. When a session is shorter than the maximum session length, we use the retrieval results from its own last iteration as the retrieval results for iterations beyond its own last iteration. In addition, since TREC did not provide any retrieval results for the last query, the Original runs has no value at the last iteration.</p><p>Figures <ref type="figure" target="#fig_2">4</ref> and<ref type="figure" target="#fig_3">5</ref> plot the immediate search accuracy for TREC 2012 &amp; 2013 Session Tracks averaged over all sessions. We observe that win-win search's immediate search accuracy is statistically significantly better than the Original run at every iteration. In Figure <ref type="figure" target="#fig_2">4</ref>, win-win outperforms Original since iteration 2 in nDCG@10 and outperforms it since iteration 3 in nERR@10. At the last iteration, winwin outperforms Original by a statistically significant 27.1% in nDCG@10 (p-value&lt;.05, one-sided t-test). We observe similar trends in Figure <ref type="figure" target="#fig_3">5</ref>. Another interesting finding is that win-win search's immediate search accuracy increases while the number of search iterations increases. In Figure <ref type="figure" target="#fig_2">4</ref>, the nDCG@10 starts at 0.2145 at the first iteration and increases dramatically 37.1% to 0.2941 at the last iteration. It suggests that by involving more search iterations, i.e., learning from more interactions between the user and the search   engine, win-win is able to monotonically improve its search accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">State Transitions</head><p>This experiment investigates how legitimate the proposed states are in presenting the hidden mental states of users.</p><p>First, we use examples to demonstrate the state transitions in sessions. Session 87 (Table <ref type="table" target="#tab_0">1</ref>) is a long session with 21 queries. The chain of decision states identified for this session based on techniques presented in Sections 4.2 and 4.4 is: SNRR(q1=best us destination) → SRT (q2=distance new york boston) → SNRT → SNRR → SNRR → SRR → SRR → SNRR → SRT → SRT → SRR(q11=boston tourism) → SNRR(q12=nyc tourism) → SNRR(q13=philadelphia nyc distance) → SNRR → SRT → SNRR → SRR → SNRT → SNRT (q19=philadelphia nyc travel) → SNRT (q20=philadelphia nyc train) → SNRT (q21 =philadelphia nyc bus). Our states correctly suggests that the user is in the exploration states (RR, NRR, NRR) from q11 to q13, while he keeps changing queries to explore from city to city (boston, new york city, and philadelphia). The user eventually finds the cities, philadelphia and nyc, that fulfill the information need -"best US destinations within a 150-mile radius". During the last 3 queries, the user exploits the current subtopic (philadelphia and nyc) to find out more specifics on transportations (travel, train, bus) about them. Our system correctly recognizes the last three states as exploitation states (NRT, NRT, NRT). This example suggests that the proposed states are able to reflect the real user decision states quite accurately.</p><p>Second, we examine state transition patterns in long sessions since they contain enough transitions for us to study. Figure <ref type="figure" target="#fig_4">6</ref> plots state probabilities, state transition probabilities, and that under different user actions for long sessions (sessions with 4 or more queries). The data are combined from both TREC 2012 &amp; 2013 Session Tracks. We notice that NRR (non-relevant and exploration) is the most com-  mon state (42.4%). This reflects that a user spend may a long time to explore while receiving non-relevant documents. On the contrary, the RR state (relevant and exploration) is the least common state (11.3%).Moreover, we see that state transitions are not uniformly distributed. For instance, the transition from NRT to both relevant states (RT and RR) are very rare (in total 5.65%). In addition, we notice that actions are related to state transition probabilities. There are 90.8% transitions generated by adding terms and among all the transitions with removing terms, 84.8% of them lead to exploitation states (RT or NRT).</p><p>Third, we find that state probability distribution and state probability transitions differ among different session types. We plot the state probabilities and transition probabilities in Figures <ref type="figure" target="#fig_5">7</ref> to 10 for four different TREC session types, which were created along two aspects: search target (factual or intellectual ) and goal quality (specific or amorphous). Suggested by <ref type="bibr" target="#b24">[24]</ref>, the difficulty levels of the session types usually are FactualSpecific &lt; IntellectualSpecific &lt; FactualAmorphous &lt; IntellectualAmorphous. An interesting finding is that as the session difficult level increases, the transition probability from state NRT (non-relevant and exploitation) to state RT (relevant and exploitation) becomes lower: Fac-tualSpecific (0.25), IntellectualSpecific (0.2), FactualAmorphous (0.12), IntellectualAmorphous(0.1). It suggests that the harder the task is, the greater the necessity to explore rather than to exploit, when the user is not satisfied with the current retrieval results. In addition, we observe that Intellectual sessions (Figures <ref type="figure" target="#fig_7">9</ref> and<ref type="figure" target="#fig_8">10</ref>) have a larger probability, 0.1548, to be in the RR (relevant and exploration) state than the other session types (on average 0.1018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>This paper presents a novel session search framework, winwin search, that uses a dual-agent stochastic game to model the interactions between user and search engine. With a careful design of states, actions, and observations, the new framework is able to perform efficient optimization over a finite discrete set of options. The experiments on TREC Ses-   Session search is a complex IR task. The complexity comes from the involvement of many more factors other than just terms, queries and documents in most existing retrieval algorithms. The factors include query reformulations, clicks, time spent to examine the documents, personalization, query intent, feedback, etc. Most existing work on sessions and task-based search focuses on diving into one aspect. Through significantly simplifying the factors, we realize the integration of all the factors in a unified framework. For example, we simplify users' decision states into only four states, and discretize user actions and search engine actions into a finite number of options. Such simplification is necessary in creating practical search systems.</p><p>This paper views the search engine as an autonomous agent, that works together with user, another autonomous agent, to collaborate on a shared -fulfilling the information needs. This view assumes that the search engine is more like a "decision engine". Session search can be imagined as two agents exploring in a world full of information, searching for the goal in a trial-and-error manner. Here we assume a cooperative game between the two agents. However, as we mentioned in the introduction, the search engine agent can of course choose a different goal. It will be very interesting to see how to still satisfy the user to achieve winwin. We hope our work calls for future adventures in the fields of POMDP in IR and game theory in IR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENT</head><p>This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dual-agent stochastic game.</figDesc><graphic coords="4,70.20,192.23,203.24,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: States.</figDesc><graphic coords="5,345.04,53.80,179.58,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: TREC 2012 Immediate Search Accuracy.</figDesc><graphic coords="9,53.80,57.40,110.51,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: TREC 2013 Immediate Search Accuracy.</figDesc><graphic coords="9,76.67,180.13,190.30,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Long sessions (length &gt;= 4). Transition probabilities are listed with actions: Add (A), Remove (R), and Keep (K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Factual and Specific sessions.</figDesc><graphic coords="9,348.16,53.80,173.34,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Factual and Amorphous sessions.</figDesc><graphic coords="9,348.16,169.68,173.35,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Intellectual and Specific sessions.</figDesc><graphic coords="10,85.14,167.85,173.34,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Intellectual and Amorphous sessions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>sion 2012 and 2013 datasets show that the proposed framework is highly effective for session search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Information needs and queries (examples are from TREC 2013 Session Track).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Symbols in the dual-agent stochastic game.</figDesc><table /><note><p>Name Symbol Meanings State S the four hidden decision states in Figure 3 User action Au add query terms, remove query terms, keep query terms Search engine action Ase increase/decrease/keep term weights, adjust search techniques, etc Message from user to search engine Σu clicked and SAT clicked documents Message from search engine to user Σse top k returned documents User's observation Ωu observations that the user makes from the world Search engine's observation Ωse observations that the search engine makes from the world and from the user User reward Ru relevant information the user gains from reading the documents Search engine reward Rse nDCG that the search gains by returning documents Belief state B belief states generated from the belief updater and shared by both agents</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The user agent writes a query q t and takes the t th user agent action a t u , which is a query change from the previous query. 3. (*) The search engine agent makes observations Ωse from the world and updates its before-message-beliefstate b t •Σse based on O(S t , a t u , Ωse). 4. The search engine runs its optimization algorithm and picks the best policy πse, which maximizes the joint long term rewards for both agents. Following the policy, it takes actions a t se . This is where the model performs retrieval. 5. Search engine action a t se results in a set of documents D t , which are returned as message Σ t se sent from the search engine agent to the user agent through the world. 6. (*) The user agent receives message Σ t se and observes Ωu. If the user would like to stop the search, the process ends. Otherwise, the user updates the aftermessage-belief-state b t Σ•u based on O(S t , Σ t se , Ωu). 7. Based on the current beliefs, the user agent sends its feedback messages Σ t u to inform the search engine agent.</figDesc><table><row><cell cols="2">Σ t u are clicks, some of which are SAT clicks. It contains</cell></row><row><cell>a set of documents D clicked .</cell><cell></cell></row><row><cell cols="2">8. (*) The search engine agent observes Ωse from the</cell></row><row><cell cols="2">world and updates its after-message-belief-state b t Σ•se based on O(S t , Σ t u , Ωse).</cell></row><row><cell cols="2">9. The user agent picks a policy πu, which we don't study</cell></row><row><cell>here, and continues to send out actions a t+1 u</cell><cell>in the form</cell></row><row><cell cols="2">of query changes. The world moves into a new state</cell></row><row><cell cols="2">st+1. t = t + 1. The process repeats from step 3.</cell></row></table><note><p>1. At search iteration t = 0, both agents begin in the same initial state S 0 . 2. t increases by one: t = t + 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics.</figDesc><table><row><cell>TREC 2012 TREC 2013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Search accuracy on TREC 2012 Session ( * indicates a statistical significant improvement over Rocchio at p &lt; 0.05 (t-test, one-sided)); * indicates a statistical significant improvement over QCM+DUP at p &lt; 0.05 (t-test, one-sided)).</figDesc><table><row><cell>Approach</cell><cell cols="2">nDCG@10 nDCG</cell><cell>MAP</cell><cell>nERR@10</cell></row><row><cell>Lemur</cell><cell>0.2474</cell><cell>0.2627</cell><cell>0.1274</cell><cell>0.2857</cell></row><row><cell>TREC median</cell><cell>0.2608</cell><cell>0.2468</cell><cell>0.1440</cell><cell>0.2626</cell></row><row><cell>TREC best</cell><cell>0.3221</cell><cell>0.2865</cell><cell>0.1559</cell><cell>0.3595</cell></row><row><cell>PRF</cell><cell>0.2074</cell><cell>0.2335</cell><cell>0.1065</cell><cell>0.2415</cell></row><row><cell>Rocchio</cell><cell>0.2446</cell><cell>0.2714</cell><cell>0.1281</cell><cell>0.2950</cell></row><row><cell>Rocchio-CLK</cell><cell>0.2916 †</cell><cell>0.2866</cell><cell>0.1449</cell><cell>0.3366</cell></row><row><cell>Rocchio-SAT</cell><cell>0.2889</cell><cell>0.2836</cell><cell>0.1467</cell><cell>0.3254</cell></row><row><cell>QCM+DUP</cell><cell>0.2742</cell><cell cols="2">0.2560 0.1537 †</cell><cell>0.3221</cell></row><row><cell>QCM SAT</cell><cell>0.3350 *  †</cell><cell cols="2">0.3054 0.1534 †</cell><cell>0.1534</cell></row><row><cell>Win-Win</cell><cell>0.2941 †</cell><cell>0.2691</cell><cell>0.1346</cell><cell>0.3403</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">: Search accuracy on TREC 2013 Session ( †</cell></row><row><cell cols="5">indicates a statistical significant improvement over</cell></row><row><cell cols="5">Rocchio at p &lt; 0.01 (t-test, one-sided));  *  indicates a</cell></row><row><cell cols="5">statistical significant improvement over QCM+DUP</cell></row><row><cell cols="3">at p &lt; 0.01 (t-test, one-sided)).</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>nDCG@10</cell><cell>nDCG</cell><cell>MAP</cell><cell>nERR@10</cell></row><row><cell>Lemur</cell><cell>0.1147</cell><cell>0.1758</cell><cell>0.0926</cell><cell>0.1314</cell></row><row><cell>TREC median</cell><cell>0.1531</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TREC best</cell><cell>0.1952</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PRF</cell><cell>0.1061</cell><cell>0.1701</cell><cell>0.0787</cell><cell>0.1245</cell></row><row><cell>Rocchio</cell><cell>0.1320</cell><cell>0.1924</cell><cell>0.1060</cell><cell>0.1549</cell></row><row><cell>Rocchio-CLK</cell><cell>0.1315</cell><cell>0.1929</cell><cell>0.1060</cell><cell>0.1546</cell></row><row><cell>Rocchio-SAT</cell><cell>0.1147</cell><cell>0.1758</cell><cell>0.0926</cell><cell>0.1314</cell></row><row><cell>QCM+DUP</cell><cell>0.1316</cell><cell>0.1929</cell><cell>0.1060</cell><cell>0.1547</cell></row><row><cell>QCM SAT</cell><cell>0.1186</cell><cell>0.1754</cell><cell>0.0939</cell><cell>0.1425</cell></row><row><cell>Win-Win</cell><cell>0.2026 *  †</cell><cell cols="2">0.2609 *  † 0.1290 *  †</cell><cell>0.2328 *  †</cell></row><row><cell cols="5">sumes the top 10 previous retrieved documents are relevant),</cell></row><row><cell cols="5">Rocchio-CLK (implicit relevance feedback that assumes only</cell></row><row><cell cols="5">previous clicked documents are relevant), Rocchio-SAT (im-</cell></row><row><cell cols="5">plicit relevance feedback that assumes only previous SAT-</cell></row><row><cell cols="5">clicked documents are relevant), QCM+DUP (the QCM ap-</cell></row><row><cell cols="2">proach proposed by</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The manual annotations for "exploration" transitions can be found at www.cs.georgetown.edu/~huiyang/win-win.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simulating simple user behavior for system effectiveness evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intentions and attention in exploratory health search</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Cartright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leaving so soon?: Understanding and predicting web search abandonment rationales</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diriye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Personalizing atypical web search sessions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<idno>WSDM &apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task-aware query recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating implicit measures to improve web search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karnawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mydland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Effective structured query formulation for session search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
		<idno>TREC &apos;12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Utilizing query change for session search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ready to buy or just browsing?: detecting web searcher goals from interaction data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Different effects of click-through and past queries on whole-session search performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<idno>TREC &apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pitt at trec 2012 session track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In TREC &apos;12</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Interactive exploratory search for multi page search results</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW &apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A probabilistic analysis of the rocchio algorithm with tfidf for text categorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Klinkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning: a survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overview of the trec 2012 session track</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<idno>TREC&apos;12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Overview of the trec 2013 session track</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<idno>TREC&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling and analysis of cross-session search tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.lemurproject.org/" />
		<title level="m">Lemur Search Engine</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Personalizing information retrieval for multi-session tasks: The roles of task stage and task type</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward whole-session relevance: Exploring intrinsic diversity in web search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicit user modeling for personalized search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;05</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optimal rare query suggestion with implicit user feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>He</surname></persName>
		</author>
		<idno>WWW &apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relationships between categories of relevance criteria and stage in task completion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Amadio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">To personalize or not to personalize: Modeling queries with variation in user intent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Liebling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to extract cross-session search tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In WWW &apos;13</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating implicit feedback models using searcher simulations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ruthven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequential selection of correlated ads by pomdps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Query change as relevance feedback in session search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
