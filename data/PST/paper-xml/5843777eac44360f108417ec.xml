<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
							<email>zichaoy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
							<email>diyiy@cs.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling <ref type="bibr" target="#b28">(Wang and Manning, 2012)</ref>, sentiment classification <ref type="bibr" target="#b16">(Maas et al., 2011;</ref><ref type="bibr" target="#b19">Pang and Lee, 2008)</ref>, and spam detection <ref type="bibr" target="#b20">(Sahami et al., 1998)</ref>. Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation <ref type="bibr" target="#b28">(Wang and Manning, 2012;</ref><ref type="bibr" target="#b6">Joachims, 1998)</ref>. More recent approaches used deep learning, such as convolutional neural networks <ref type="bibr" target="#b1">(Blunsom et al., 2014)</ref> and recurrent neural networks based on long short-term memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref> to learn text representations.</p><p>pork belly = delicious . || scallops? || I don't even like scallops, and these were a-m-a-z-i-n-g . || fun and tasty cocktails. || next time I in Phoenix, I will go back here. || Highly recommend. Although neural-network-based approaches to text classification have been quite effective <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b31">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Johnson and Zhang, 2014;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>, in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation.</p><p>Our primary contribution is a new neural architecture ( §2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents have a hierarchical structure (words form sentences, sentences form a document), we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. Moreover, the impor-tance of words and sentences are highly context dependent, i.e. the same word or sentence may be differentially important in different context ( §3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b29">Xu et al., 2015)</ref> -one at the word level and one at the sentence level -that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. To illustrate, consider the example in Fig. <ref type="figure" target="#fig_0">1</ref>, which is a short Yelp review where the task is to predict the rating on a scale from 1-5. Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis <ref type="bibr" target="#b21">(Shen et al., 2014;</ref><ref type="bibr">Gao et al., 2014)</ref>.</p><p>The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets ( §3). Our model outperforms previous approaches by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hierarchical Attention Networks</head><p>The overall architecture of the Hierarchical Attention Network (HAN) is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It consists of several parts: a word sequence encoder, a word-level attention layer, a sentence encoder and a sentence-level attention layer. We describe the details of different components in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRU-based sequence encoder</head><p>The GRU <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> uses a gating mechanism to track the state of sequences without using separate memory cells. There are two types of gates: the reset gate r t and the update gate z t . They together control how information is updated to the</p><formula xml:id="formula_0">h 21 h 21 h 22 h 22 ! h 22 ! h 22 h 2T h 2T ! h 2T ! h 2T</formula><p>uw uw w21 w21 w22 w22 w2T w2T</p><p>word encoder word attention state. At time t, the GRU computes the new state as</p><formula xml:id="formula_1">h 1 h 1 ! h 1 ! h 1 h 2 h 2 ! h 2 ! h 2 h L h L ! h L ! h L us us s1 s1 s2 s2 sL sL ↵L ↵L sentence encoder sentence attention v v softmax ↵21 ↵21 ↵22 ↵22 ↵2T ↵2T ! h 21 ! h 21 ↵1 ↵1 ↵2 ↵2</formula><formula xml:id="formula_2">h t = (1 − z t ) ⊙ h t−1 + z t ⊙ ht .<label>(1)</label></formula><p>This is a linear interpolation between the previous state h t−1 and the current new state ht computed with new sequence information. The gate z t decides how much past information is kept and how much new information is added. z t is updated as:</p><formula xml:id="formula_3">z t = σ(W z x t + U z h t−1 + b z ),<label>(2)</label></formula><p>where x t is the sequence vector at time t. The candidate state ht is computed in a way similar to a traditional recurrent neural network (RNN):</p><formula xml:id="formula_4">ht = tanh(W h x t + r t ⊙ (U h h t−1 ) + b h ),<label>(3)</label></formula><p>Here r t is the reset gate which controls how much the past state contributes to the candidate state. If r t is zero, then it forgets the previous state. The reset gate is updated as follows:</p><formula xml:id="formula_5">r t = σ(W r x t + U r h t−1 + b r ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Attention</head><p>We focus on document-level classification in this work. Assume that a document has L sentences s i and each sentence contains T i words. w it with t ∈ [1, T ] represents the words in the ith sentence.</p><p>The proposed model projects the raw document into a vector representation, on which we build a classifier to perform document classification. In the following, we will present how we build the document level vector progressively from word vectors by using the hierarchical structure.</p><p>Word Encoder Given a sentence with words w it , t ∈ [0, T ], we first embed the words to vectors through an embedding matrix W e , x ij = W e w ij .</p><p>We use a bidirectional GRU <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> to get annotations of words by summarizing information from both directions for words, and therefore incorporate the contextual information in the annotation. The bidirectional GRU contains the forward GRU − → f which reads the sentence s i from w i1 to w iT and a backward GRU ← − f which reads from w iT to w i1 :</p><formula xml:id="formula_6">x it =W e w it , t ∈ [1, T ], − → h it = − −− → GRU(x it ), t ∈ [1, T ], ← − h it = ← −− − GRU(x it ), t ∈ [T, 1].</formula><p>We obtain an annotation for a given word w it by concatenating the forward hidden state − → h it and backward hidden state</p><formula xml:id="formula_7">← − h it , i.e., h it = [ − → h it , ← − h it ],</formula><p>which summarizes the information of the whole sentence centered around w it .</p><p>Note that we directly use word embeddings. For a more complete model we could use a GRU to get word vectors directly from characters, similarly to <ref type="bibr" target="#b15">(Ling et al., 2015)</ref>. We omitted this for simplicity.</p><p>Word Attention Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Specifically,</p><formula xml:id="formula_8">u it = tanh(W w h it + b w )</formula><p>(5)</p><formula xml:id="formula_9">α it = exp(u ⊤ it u w ) ∑ t exp(u ⊤ it u w )<label>(6)</label></formula><formula xml:id="formula_10">s i = ∑ t α it h it .<label>(7)</label></formula><p>That is, we first feed the word annotation h it through a one-layer MLP to get u it as a hidden representation of h it , then we measure the importance of the word as the similarity of u it with a word level context vector u w and get a normalized importance weight α it through a softmax function. After that, we compute the sentence vector s i (we abuse the notation here) as a weighted sum of the word annotations based on the weights. The context vector u w can be seen as a high level representation of a fixed query "what is the informative word" over the words like that used in memory networks <ref type="bibr" target="#b23">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Kumar et al., 2015)</ref>. The word context vector u w is randomly initialized and jointly learned during the training process.</p><p>Sentence Encoder Given the sentence vectors s i , we can get a document vector in a similar way. We use a bidirectional GRU to encode the sentences:</p><formula xml:id="formula_11">− → h i = − −− → GRU(s i ), i ∈ [1, L], ← − h i = ← −− − GRU(s i ), t ∈ [L, 1]. We concatenate − → h i and ← − h j to get an annotation of sentence i, i.e., h i = [ − → h i , ← − h i ]</formula><p>. h i summarizes the neighbor sentences around sentence i but still focus on sentence i.</p><p>Sentence Attention To reward sentences that are clues to correctly classify a document, we again use attention mechanism and introduce a sentence level context vector u s and use the vector to measure the importance of the sentences. This yields</p><formula xml:id="formula_12">u i = tanh(W s h i + b s ),<label>(8)</label></formula><formula xml:id="formula_13">α i = exp(u ⊤ i u s ) ∑ i exp(u ⊤ i u s ) , (<label>9</label></formula><formula xml:id="formula_14">) v = ∑ i α i h i , (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where v is the document vector that summarizes all the information of sentences in a document.</p><p>Similarly, the sentence level context vector can be randomly initialized and jointly learned during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Classification</head><p>The document vector v is a high level representation of the document and can be used as features for doc-ument classification:</p><formula xml:id="formula_16">p = softmax(W c v + b c ).<label>(11)</label></formula><p>We use the negative log likelihood of the correct labels as training loss:</p><formula xml:id="formula_17">L = − ∑ d log p dj , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where j is the label of document d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data sets</head><p>We evaluate the effectiveness of our model on six large scale document classification data sets. These data sets can be categorized into two types of document classification tasks: sentiment estimation and topic classification. The statistics of the data sets are summarized in Table <ref type="table" target="#tab_1">1</ref>. We use 80% of the data for training, 10% for validation, and the remaining 10% for test, unless stated otherwise.</p><p>Yelp reviews are obtained from the Yelp Dataset Challenge in 2013, 2014 and 2015 <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. There are five levels of ratings from 1 to 5 (higher is better). IMDB reviews are obtained from <ref type="bibr" target="#b2">(Diao et al., 2014)</ref>. The ratings range from 1 to 10. Yahoo answers are obtained from <ref type="bibr" target="#b31">(Zhang et al., 2015)</ref>. This is a topic We randomly select 10% of the training samples as validation. Amazon reviews are obtained from <ref type="bibr" target="#b31">(Zhang et al., 2015)</ref>. The ratings are from 1 to 5. 3,000,000 reviews are used for training and 650,000 reviews for testing. Similarly, we use 10% of the training samples as validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare HAN with several baseline methods, including traditional approaches such as linear methods, SVMs and paragraph embeddings using neural networks, LSTMs, word-based CNN, character-based CNN, and Conv-GRNN, LSTM-GRNN. These baseline methods and results are reported in <ref type="bibr" target="#b31">(Zhang et al., 2015;</ref><ref type="bibr" target="#b26">Tang et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Linear methods</head><p>Linear methods <ref type="bibr" target="#b31">(Zhang et al., 2015)</ref> use the constructed statistics as features. A linear classifier based on multinomial logistic regression is used to classify the documents using the features.</p><p>BOW and BOW+TFIDF The 50,000 most frequent words from the training set are selected and the count of each word is used features. Bow+TFIDF, as implied by the name, uses the TFIDF of counts as features. n-grams and n-grams+TFIDF used the most frequent 500,000 n-grams (up to 5-grams). Bag-of-means The average word2vec embedding <ref type="bibr" target="#b18">(Mikolov et al., 2013</ref>) is used as feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">SVMs</head><p>SVMs-based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>, including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE. In detail, Unigrams and Bigrams uses bag-of-unigrams and bagof-bigrams as features respectively.</p><p>Text Features are constructed according to <ref type="bibr" target="#b9">(Kiritchenko et al., 2014)</ref>, including word and character n-grams, sentiment lexicon features etc. AverageSG constructs 200-dimensional word vectors using word2vec and the average word embeddings of each document are used. SSWE uses sentiment specific word embeddings according to <ref type="bibr" target="#b25">(Tang et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Neural Network methods</head><p>The neural network based methods are reported in <ref type="bibr" target="#b26">(Tang et al., 2015)</ref> and <ref type="bibr" target="#b31">(Zhang et al., 2015)</ref>.</p><p>CNN-word Word based CNN models like that of <ref type="bibr" target="#b8">(Kim, 2014)</ref> are used. CNN-char Character level CNN models are reported in <ref type="bibr" target="#b31">(Zhang et al., 2015)</ref>. LSTM takes the whole document as a single sequence and the average of the hidden states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by <ref type="bibr" target="#b26">(Tang et al., 2015)</ref>. They also explore the hierarchical structure: a CNN or LSTM provides a sentence vector, and then a gated recurrent neural network (GRNN) combines the sentence vectors from a document level vector representation for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model configuration and training</head><p>We split documents into sentences and tokenize each sentence using Stanford's CoreNLP <ref type="bibr" target="#b17">(Manning et al., 2014)</ref>. We only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special UNK token.</p><p>We obtain the word embedding by training an unsupervised word2vec <ref type="bibr" target="#b18">(Mikolov et al., 2013)</ref> model on the training and validation splits and then use the word embedding to initialize W e . The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50. In this case a combination of forward and backward GRU gives us 100 dimensions for word/sentence annotation. The word/sentence context vectors also have a dimension of 100, initialized at random.</p><p>For training, we use a mini-batch size of 64 and documents of similar length (in terms of the number of sentences in the documents) are organized to be a batch. We find that length-adjustment can accelerate training by three times. We use stochastic gradient descent to train all models with momentum of 0.9. We pick the best learning rate using grid search on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results and analysis</head><p>The experimental results on all data sets are shown in Table <ref type="table" target="#tab_2">2</ref>. We refer to our models as HN-{AVE, MAX, ATT}. Here HN stands for Hierarchical Network, AVE indicates averaging, MAX indicates max-pooling, and ATT indicates our proposed hierarchical attention model. Results show that HN-ATT gives the best performance across all data sets.</p><p>The improvement is regardless of data sizes. For smaller data sets such as Yelp 2013 and IMDB, our model outperforms the previous best baseline methods by 3.1% and 4.1% respectively. This finding is consistent across other larger data sets. Our model outperforms previous best models by 3.2%, 3.4%, 4.6% and 6.0% on Yelp 2014, Yelp 2015, Yahoo Answers and Amazon Reviews. The improvement also occurs regardless of the type of task: sentiment classification, which includes Yelp 2013-2014, IMDB, Amazon Reviews and topic classification for Yahoo Answers.</p><p>From Table <ref type="table" target="#tab_2">2</ref> we can see that neural network based methods that do not explore hierarchical document structure, such as LSTM, CNN-word, CNNchar have little advantage over traditional methods for large scale (in terms of document size) text classification. E.g. SVM+TextFeatures gives performance <ref type="bibr">59.8, 61.8, 62.4, 40.5 for Yelp 2013</ref><ref type="bibr">, 2014</ref><ref type="bibr">, 2015 and IMDB respectively, while CNN-word has accuracy 59.7, 61.0, 61.5, 37.6</ref>  combined with hierarchical structure improves over previous models (LSTM-GRNN) by 3.1%, 3.4%, 3.5% and 4.1% respectively. More interestingly, in the experiments, HN-AVE is equivalent to using non-informative global word/sentence context vectors (e.g., if they are all-zero vectors, then the attention weights in Eq. 6 and 9 become uniform weights). Compared to HN-AVE, the HN-ATT model gives superior performance across the board. This clearly demonstrates the effectiveness of the proposed global word and sentence importance vectors for the HAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Context dependent attention weights</head><p>If words were inherently important or not important, models without attention mechanism might work well since the model could automatically assign low weights to irrelevant words and vice versa. However, the importance of words is highly context dependent. For example, the word good may appear in a review that has the lowest rating either because users are only happy with part of the product/service or because they use it in a negation, such as not good.</p><p>To verify that our model can capture context dependent word importance, we plot the distribution of the attention weights of the words good and bad from the test split of Yelp 2013 data set as shown in Figure <ref type="figure" target="#fig_2">3</ref>(a) and Figure <ref type="figure" target="#fig_4">4</ref>(a). We can see that the distribution has a attention weight assigned to a word from 0 to 1. This indicates that our model captures diverse context and assign context-dependent weight to the words.</p><p>For further illustration, we plot the distribution when conditioned on the ratings of the review. Subfigures 3(b)-(f) in Figure <ref type="figure" target="#fig_2">3</ref> and Figure <ref type="figure" target="#fig_4">4</ref> correspond to the rating 1-5 respectively. In particular, <ref type="bibr">Figure 3(b)</ref> shows that the weight of good concentrates on the low end in the reviews with rating 1. As the rating increases, so does the weight distribution. This means that the word good plays a more important role for reviews with higher ratings. We can observe the converse trend in Figure <ref type="figure" target="#fig_4">4</ref> for the word bad. This confirms that our model can capture the context-dependent word importance. with ratings 1-5 respectively. We can see that the weight distribution shifts to higher end as the rating goes higher.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Visualization of attention</head><p>In order to validate that our model is able to select informative sentences and words in a document, we visualize the hierarchical attention layers in Figures <ref type="figure" target="#fig_5">5  and 6</ref> for several documents from the Yelp 2013 and Yahoo Answers data sets.</p><p>Every line is a sentence (sometimes sentences spill over several lines due to their length). Red denotes the sentence weight and blue denotes the word weight. Due to the hierarchical structure, we normalize the word weight by the sentence weight to make sure that only important words in important sentences are emphasized. For visualization purposes we display √ p s p w . The √ p s term displays the important words in unimportant sentences to ensure that they are not totally invisible.</p><p>Figure <ref type="figure">5</ref> shows that our model can select the words carrying strong sentiment like delicious, amazing, terrible and their corresponding sentences.</p><p>Sentences containing many words like cocktails, pasta, entree are disregarded. Note that our model can not only select words carrying strong sentiment, it can also deal with complex across-sentence context. For example, there are sentences like i don't even like scallops in the first document of Fig. <ref type="figure">5</ref>, if looking purely at the single sentence, we may think this is negative comment. However, our model looks at the context of this sentence and figures out this is a positive review and chooses to ignore this sentence.</p><p>Our hierarchical attention mechanism also works well for topic classification in the Yahoo Answer data set. For example, for the left document in Figure <ref type="figure" target="#fig_5">6</ref> with label 1, which denotes Science and Mathematics, our model accurately localizes the words zebra, strips, camouflage, predator and their corresponding sentences. For the right document with label 4, which denotes Computers and Internet, our model focuses on web, searches, browsers and their corresponding sentences. Note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work <ref type="bibr" target="#b8">Kim (2014)</ref> use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision <ref type="bibr" target="#b12">(LeCun et al., 1998)</ref>, albeit with NLP interpretations. <ref type="bibr" target="#b7">Johnson and Zhang (2014)</ref> explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, <ref type="bibr" target="#b31">Zhang et al. (2015)</ref> apply a character-level CNN for text classification and achieve competitive results. <ref type="bibr" target="#b22">Socher et al. (2013)</ref>   explore the structure of a sentence and use a treestructured LSTMs for classification. There are also some works that combine LSTM and CNN structure to for sentence classification <ref type="bibr" target="#b11">(Lai et al., 2015;</ref><ref type="bibr" target="#b32">Zhou et al., 2015)</ref>. <ref type="bibr" target="#b26">Tang et al. (2015)</ref> use hierarchical structure in sentiment classification. They first use a CNN or LSTM to get a sentence vector and then a bi-directional gated recurrent neural network to compose the sentence vectors to get a document vectors. There are some other works that use hierarchical structure in sequence generation <ref type="bibr" target="#b13">(Li et al., 2015)</ref> and language modeling <ref type="bibr" target="#b14">(Lin et al., 2015)</ref>.</p><p>The attention mechanism was proposed by <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref> in machine translation. The encoder decoder framework is used and an attention mechanism is used to select the reference words in original language for words in foreign language before translation. <ref type="bibr" target="#b29">Xu et al. (2015)</ref> uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism include parsing <ref type="bibr" target="#b27">(Vinyals et al., 2014)</ref>, natural language question answering <ref type="bibr" target="#b23">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b10">Kumar et al., 2015;</ref><ref type="bibr" target="#b4">Hermann et al., 2015)</ref>, and image question answering <ref type="bibr" target="#b30">(Yang et al., 2015)</ref>. Unlike these works, we explore a hierarchical attention mechanism (to the best of our knowledge this is the first such instance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed hierarchical attention networks (HAN) for classifying documents. As a convenient side-effect we obtained better visualization using the highly informative components of a document. Our model progressively builds a document vector by aggregating important words into sentence vectors and then aggregating important sentences vectors to document vectors. Experimental results demonstrate that our model performs significantly better than previous methods. Visualization of these attention layers illustrates that our model is effective in picking out important words and sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical Attention Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention weight distribution of good. (a) -aggregate distribution on the test split; (b)-(f) stratified for reviews</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention weight distribution of the word bad. The setup is as above: (a) contains the aggregate distribution, while (b)-(f) contain stratifications to reviews with ratings 1-5 respectively. Contrary to before, the word bad is considered important for poor ratings and less so for good ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Documents from Yahoo Answers. Label 1 denotes Science and Mathematics and label 4 denotes Computers and Internet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Data statistics: #s denotes the number of sentences (average and maximum per document), #w denotes the number of words (average and maximum per document).</figDesc><table><row><cell>Data set</cell><cell cols="7">classes documents average #s max #s average #w max #w vocabulary</cell></row><row><cell>Yelp 2013</cell><cell>5</cell><cell>335,018</cell><cell>8.9</cell><cell>151</cell><cell>151.6</cell><cell>1184</cell><cell>211,245</cell></row><row><cell>Yelp 2014</cell><cell cols="2">5 1,125,457</cell><cell>9.2</cell><cell>151</cell><cell>156.9</cell><cell>1199</cell><cell>476,191</cell></row><row><cell>Yelp 2015</cell><cell cols="2">5 1,569,264</cell><cell>9.0</cell><cell>151</cell><cell>151.9</cell><cell>1199</cell><cell>612,636</cell></row><row><cell>IMDB review</cell><cell>10</cell><cell>348,415</cell><cell>14.0</cell><cell>148</cell><cell>325.6</cell><cell>2802</cell><cell>115,831</cell></row><row><cell>Yahoo Answer</cell><cell cols="2">10 1,450,000</cell><cell>6.4</cell><cell>515</cell><cell>108.4</cell><cell cols="2">4002 1,554,607</cell></row><row><cell>Amazon review</cell><cell cols="2">5 3,650,000</cell><cell>4.9</cell><cell>99</cell><cell>91.9</cell><cell cols="2">596 1,919,336</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Document Classification, in percentage</figDesc><table><row><cell>respectively.</cell></row><row><cell>Exploring the hierarchical structure only, as in</cell></row><row><cell>HN-AVE, HN-MAX, can significantly improve over</cell></row><row><cell>LSTM, CNN-word and CNN-char. For exam-</cell></row><row><cell>ple, our HN-AVE outperforms CNN-word by 7.3%,</cell></row><row><cell>8.8%, 8.5%, 10.2% than CNN-word on Yelp 2013,</cell></row><row><cell>2014, 2015 and IMDB respectively. Our model</cell></row><row><cell>HN-ATT that further utilizes attention mechanism</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>use recursive neural networks for text classification.<ref type="bibr" target="#b24">Tai et al. (2015)</ref> </figDesc><table><row><cell>GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n't . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i 'm in phoenix , i will go back here . highly recommend .</cell><cell>GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back .</cell></row><row><cell cols="2">Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1.</cell></row><row><cell>GT: 1 Prediction: 1 why does zebras have stripes ? what is the purpose or those stripes ? who do they serve the zebras in the wild life ? this provides camouflage -predator vision is such that it is usually difficult for them to see complex patterns</cell><cell>GT: 4 Prediction: 4 how do i get rid of all the old web searches i have on my web browser ? i want to clean up my web browser go to tools &gt; options . then click " delete history " and " clean up temporary internet files . "</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by Microsoft Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03340</idno>
		<title level="m">Teaching machines to read and comprehend</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiaodan Zhu, and Saif M Mohammad</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="723" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01057</idno>
		<title level="m">A hierarchical neural autoencoder for paragraphs and documents</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<title level="m">Finding function in form: Compositional character models for open vocabulary word representation</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bayesian approach to filtering junk e-mail</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning for Text Categorization: Papers from the 1998 workshop</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<title level="m">End-to-end memory networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01626</idno>
		<title level="m">Character-level convolutional networks for text classification</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<title level="m">A c-lstm neural network for text classification</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
