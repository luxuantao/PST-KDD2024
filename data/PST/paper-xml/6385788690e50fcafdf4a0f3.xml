<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets</title>
				<funder ref="#_5sFVrDB #_EvHejRP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-28">28 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianjin</forename><surname>Huang</surname></persName>
							<email>t.huang@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<email>tianlong.chen@utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Fang</surname></persName>
							<email>moefang@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vlado</forename><surname>Menkovski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxu</forename><surname>Zhao</surname></persName>
							<email>j.zhao@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
							<email>l.yin@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulong</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Decebal</forename><forename type="middle">Constantin</forename><surname>Mocanu</surname></persName>
							<email>d.c.mocanu@utwente.nl</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Twente</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
							<email>m.pechenizkiy@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Eindhoven University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>6 4 -2 1 2 8 -2 2 5 6 -2 6 4 -2 1 2 8 -2 2 5 6 -2 6 4 -2 1 2 8 -2 2 5 6 -2</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">GIN GCN GAT</orgName>
								<orgName type="institution">ACC Cora</orgName>
								<address>
									<addrLine>Sparsity=0.8) 6 4 -2 1 2 8 -2 2 5 6 -2 6 4 -2 1 2 8 -2 2 5 6 -2 6 4 -2 1 2 8 -2 2 5 6 -2 GIN GCN GAT 0. 52 0. 63 0. 74 Citeseer (Sparsity=0.8) 6 4 -2 2 5 6 -2 2 5 6 -5 6 4 -2 2 5 6 -2 2 5 6 -5 6 4 -2 2 5 6 -2 2 5 6 -5 GIN GCN GAT 0. 70 0. 76 0. 82 Pubmed (Sparsity=0.9) 6 4 -2 2 5 6 -2 2 5 6 -5 6 4 -2 2 5 6 -2 2 5 6 -5 6 4 -2 2 5 6 -2 2 5 6 -5</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-28">28 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.15335v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find untrained sparse subnetworks at the initialization, that can match the performance of fully trained dense GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks have appealing performance in out-of-distribution detection and robustness of input perturbations. We evaluate our method across widely-used GNN architectures on various popular datasets including the Open Graph Benchmark (OGB).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> have shown the power to learn representations from graphstructured data. Over the past decade, GNNs and their variants such as Graph Convolutional Networks (GCN) <ref type="bibr" target="#b2">[3]</ref>, Graph Isomorphism Networks (GIN) <ref type="bibr" target="#b3">[4]</ref>, Graph Attention Networks (GAT) <ref type="bibr" target="#b4">[5]</ref> have been successfully applied to a wide range of scenarios, e.g., social analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, protein feature learning <ref type="bibr" target="#b7">[8]</ref>, traffic prediction <ref type="bibr" target="#b8">[9]</ref>, and recommendation systems <ref type="bibr" target="#b9">[10]</ref>. In parallel, works on untrained networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> surprisingly discover the presence of untrained subnetworks in CNNs that can already match the accuracy of their fully trained dense CNNs with their initial weights, without any weight update. In this paper, we attempt to explore discovering untrained sparse networks in GNNs by asking the following question: Is it possible to find a well-performing graph neural (sub-) network without any training of the model weights?</p><p>Positive answers to this question will have significant impacts on the research field of GNNs. 1 If the answer is yes, it will shed light on a new direction of obtaining performant GNNs, e.g., traditional training might not be indispensable towards performant GNNs. <ref type="bibr" target="#b1">2</ref> The existence of such performant subnetworks will extend the recently proposed untrained subnetwork techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> in GNNs. Prior works <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> successfully find that randomly weighted full networks contain untrained subnetworks which perform well without ever modifying the weights, in convolutional   <ref type="bibr" target="#b11">[12]</ref>) and the corresponding trained dense GNNs. We demonstrate that as the model size increases, UGTs is able to find an untrained subnetwork with its random initializations, that can match the performance of the corresponding fully-trained dense GNNs. The x-axis denotes the corresponding model size for each point, e.g. "64-2" represents a model with 2 layers and width 64. neural networks (CNNs). However, the similar study has never been discussed for GNNs. While CNNs reasonably contain well-performing untrained subnetworks due to heavy over-parameterization, GNN models are usually much more compact, and it is unclear whether a performant subnetwork "should" still exist in GNNs.</p><p>Furthermore, we investigate the connection between untrained sparse networks and widely-known barriers in deep GNNs, such as over-smoothing. For instance, as analyzed in <ref type="bibr" target="#b13">[14]</ref>, by naively stacking many layers and adding non-linearity, the output features are prone to collapsing and becoming indistinguishable. Such undesirable properties significantly limit the power of deeper/wider GNNs, hindering the potential application of GNNs on large-scale graph datasets such as the latest Open Graph Benchmark (OGB) <ref type="bibr" target="#b14">[15]</ref>. It is interesting to see what would happen for untrained graph neural networks. Note that the goal of sparsity in our paper is not for efficiency, but to obtain nontrivial predictive performance without training (a.k.a., "masking is training" <ref type="bibr" target="#b10">[11]</ref>). We summarize our contributions as follows:</p><p>? We demonstrate for the first time that there exist untrained graph subnetworks with matching performance (referring to as good as the trained full networks), within randomly initialized dense networks and without any model weight training. Distinct from the popular lottery ticket hypothesis (LTH) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, neither the original dense networks nor the identified subnetworks need to be trained. ? We find that the gradual sparsification technique <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> can be a stronger performance booster.</p><p>Leveraging its global sparse variant <ref type="bibr" target="#b19">[20]</ref>, we propose our method -UGTs, which discovers matching untrained subnetworks within the dense GNNs at extremely high sparsities. For example, our method discovers untrained matching subnetworks with up to 99% sparsity. We validate it across various GNN architectures (GCN, GIN, GAT) on eight datasets, including the large-scale OGBN-ArXiv and OGBN-Products. ? We empirically show a surprising observation that our method significantly mitigates the oversmoothing problem without any additional tricks and can successfully scale GNNs up with negligible performance loss. Additionally, we show that UGTs also enjoys favorable performance on Out-of-Distribution (OOD) detection and robustness on different types of perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Neural Networks. Graph neural networks is a powerful deep learning approach for graphstructured data. Since proposed in <ref type="bibr" target="#b0">[1]</ref>, many variants of GNNs have been developed, e.g., GAT <ref type="bibr" target="#b4">[5]</ref>, GCN <ref type="bibr" target="#b2">[3]</ref>, GIN <ref type="bibr" target="#b3">[4]</ref>, GraphSage <ref type="bibr" target="#b20">[21]</ref>, SGC <ref type="bibr" target="#b21">[22]</ref>, and GAE <ref type="bibr" target="#b22">[23]</ref>. More and more recent works point out that deeper GNN architectures potentially provide benefits to practical graph structures, e.g., molecules <ref type="bibr" target="#b7">[8]</ref>, point clouds <ref type="bibr" target="#b23">[24]</ref>, and meshes <ref type="bibr" target="#b24">[25]</ref>, as well as large-scale graph dataset OGB. However, training deep GNNs usually is a well-known challenge due to various difficulties such as gradient vanishing and over-smoothing problems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. The existing approaches to address the above-mentioned problem can be categorized into three groups: (1) skip connection, e.g., Jumping connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, Residual connections <ref type="bibr" target="#b23">[24]</ref>, and Initial connections <ref type="bibr" target="#b28">[29]</ref>; (2) graph normalization, e.g., PairNorm <ref type="bibr" target="#b25">[26]</ref>, NodeNorm <ref type="bibr" target="#b29">[30]</ref>; (3) random dropping including DropNode <ref type="bibr" target="#b30">[31]</ref> and DropEdge <ref type="bibr" target="#b31">[32]</ref>.</p><p>Untrained Subnetworks. Untrained subnetworks refer to the hypothesis that there exists a subnetwork in a randomly intialized neural network that can achieve almost the same accuracy as a fully trained neural network without weight update. <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> first demonstrate that randomly initialized CNNs contain subnetworks that achieve impressive performance without updating weights at all. <ref type="bibr" target="#b12">[13]</ref> enhanced the performance of untrained subnetworks by iteratively reinitializing the weights that have been pruned. Besides the image classification task, some works also explore the power of untrained subnetworks in other domains, such as multi-tasks learning <ref type="bibr" target="#b32">[33]</ref> and adversarial robustness <ref type="bibr" target="#b33">[34]</ref>.</p><p>Instead of proposing well-versed techniques to enable deep GNNs training, we explore the possibility of finding well-performing deeper graph subnetworks at initialization in the hope of avoiding the difficulties of building deep GNNs without model weight training.</p><p>3 Untrained GNNs Tickets ? l m l . The sparsity level is the fraction of the weights that are zero-valued, calculated as s = 1l m l 0 l d l . Graph Neural Networks. GNNs denote a family of algorithms that extract structural information from graphs <ref type="bibr" target="#b34">[35]</ref> and it is consisted of Aggregate and Combine operations. Usually, Aggregate is a function that aggregates messages from its neighbor nodes, and Combine is an update function that updates the representation of the current node. Formally, given the graph G = (A, X) with node set V and edge set E, the l-th layer of a GNN is represented as follows:</p><formula xml:id="formula_0">a l v = Aggregate l ({h l-1 u : ?u ? N (v)}) (1) h l v = Combine l (h l-1 v , a l v )<label>(2)</label></formula><p>where a l v is the aggregated representation of the neighborhood for node v and N (v) denotes the neighbor nodes set of the node v, and h l v is the node representations at the l-th layer. After propagating through L layers, we achieve the final node representations h L v which can be applied to downstream node-level tasks, such as node classification, link prediction.</p><p>Untrained Subnetworks. Following the prior work <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> proposed Edge-Popup which enables finding untrained subnetworks hidden in the a randomly initialized full network f (?) by solving the following discrete optimization problem: min</p><formula xml:id="formula_1">m(S)?{0,1} |?| L(f (X; ? m(S)), y)<label>(3)</label></formula><p>where L is task-dependent loss function; represents an element-wise multiplication; y is the label for the input X and m is the binary mask that controls the sparsity level s. S is the latent score behind the binary mask m and it has the same dimension as m. To avoid confusion, here we use m(S) instead of m to indicate that m is generated by S. We will use m directly for brevity in the following content.</p><p>Different from the traditional training of deep neural networks, here the network weights are never updated, masks m are instead generated to search for the optimal untrained subnetwork. In practice, each mask m i has a latent score variable S i ? R that represents the importance score of the corresponding weight ? i . During training in the forward pass, the binary mask m is generated by setting top-s smallest elements of S to 0 otherwise 1. In the backward pass, all the values in S will be updated with straight-through estimation <ref type="bibr" target="#b35">[36]</ref>. At the end of the training, an untrained subnetwork can be found by the generated mask m according to the converged scores S. The performance of GNNs with increasing model depths. Experiments are conducted on various GNNs with Cora, Citeseer, Pubmed and OGBN-Arxiv. We observe that as the model goes deeper, fully-trained dense GNNs suffer from a sharp accuracy drop, while UGTs preserves the high accuracy. All the results reported are averaged from 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Untrained GNNs Tickets -UGTs</head><p>In this section, we adopt the untrained subnetwork techniques to GNNs and introduce our new approach -Untrained GNNs Tickets (UGTs). We share the pseudocode of UGTs in the Appendix C.</p><p>Formally, given a graph neural network g(A, X; ?), where A and X are adjacency matrix and nodal features respectively. The optimization problem of finding an untrained subnetwork in GNNs can be therefore described as follows:</p><formula xml:id="formula_2">min m?{0,1} |?| L(g(A, X; ? m), y)<label>(4)</label></formula><p>Although Edge-Popup <ref type="bibr" target="#b11">[12]</ref> can find untrained subnetworks with proper predictive accuracy, its performance is still away from satisfactory. For instance, Edge-Popup can only obtain matching subnetworks at a relatively low sparsity i.e., 50%.</p><p>We highlight two limitations of the existing prior research. First of all, prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> initially set the sparsity level of m i as s and maintain it throughout the optimization process. This is very appealing for the scenarios of sparse training <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref> that chases a better trade-off between performance and efficiency, since the fixed sparsity usually translates to fewer floating-point operations (FLOPs). This scheme, however, is not necessary and perhaps harmful to the finding of the smallest possible untrained subnetwork that still performs well. Particularly as shown in <ref type="bibr" target="#b19">[20]</ref>, larger searching space for sparse neural networks at the early optimization phase leads to better sparse solutions. The second limitation is that the existing methods sparsify networks layer-wise with a uniform sparsity ratio, which typically leads to inferior performance compared with the non-uniform layer-wise sparsity <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, especially for deep architectures <ref type="bibr" target="#b40">[41]</ref>.</p><p>Untrained GNNs Tickets (UGTs). Leveraging the above-mentioned insights, we propose a new approach UGTs here which can discover matching untrained subnetworks with extremely high sparsity levels, i.e., up to 99%. Instead of keeping the sparsity of m fixed throughout the sparsification process, we start from an untrained dense GNNs and gradually increase the sparsity to the target sparsity during the whole sparsification process. We adjust the original gradual sparsification schedule <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> to the linear decay schedule, since no big performance difference can be observed. The sparsity level s t of each adjusting step t is calculated as follows:</p><formula xml:id="formula_3">s t = s f + (s i -s f )(1 - t -t 0 n?t )<label>(5)</label></formula><p>t ? {t 0 , t 0 + ?t, ..., t 0 + n?t} where s f and s i refer to the final sparsity and initial sparsity, respectively; The initial sparsity is the sparsity at the start point of sparsification and it is set to 0 in this study. The final sparsity is the sparsity at the endpoint of sparsification. t 0 is the starting point of sparsification; ?t is the time between two adjusting steps; n is the total number of adjusting steps. We set ?t as one epoch of mask optimization in this paper.</p><p>To obtain a good non-uniform layer-wise sparsity ratio, we remove the weights with the smallest score values (S) across layers at each adjusting step. We do this because <ref type="bibr" target="#b19">[20]</ref> showed that the layer-wise sparsity obtained by this scheme outperforms the other well-studied sparsity ratios <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. More importantly, removing weights across layers theoretically has a larger search space than solely considering one layer. The former can be more appealing as the GNN architecture goes deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we conduct extensive experiments among multiple GNN architectures and datasets to evaluate UGTs. We summarize the experimental setups here. GNN Architectures. We use the three most widely used GNN architectures: GCN, GIN, and GAT<ref type="foot" target="#foot_0">1</ref> in our paper.</p><p>Datasets. We choose three popular small-scale graph datasets including Cora, Citeseer, PubMed <ref type="bibr" target="#b2">[3]</ref> and one latest large-scale graph dataset OGBN-Arxiv <ref type="bibr" target="#b14">[15]</ref> for our main experiments. To draw a solid conclusion, we also evaluate our method on other datasets including OGBN-Products <ref type="bibr" target="#b14">[15]</ref>, TEXAS <ref type="bibr" target="#b42">[43]</ref>, OGBG-molhiv <ref type="bibr" target="#b14">[15]</ref> and OGBG-molbace <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44]</ref>. More detailed information can be found in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Existence of Matching Subnetworks</head><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the effectiveness of UGTs with different GNNs, including GCN, GIN and GAT, on the four datasets. We can observe that as the model size increases, UGTs can find untrained subnetworks that match the fully-trained dense GNNs. This observation is perfectly in line with the previous findings <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which reveal that model size plays a crucial role to the existence of matching untrained subnetworks. Besides, it can be observed that the proposed UGTs consistently outperforms Edge-Popup across different settings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Over-smoothing Analysis</head><p>Deep architecture has been shown as a key factor that improves the model capability in computer vision <ref type="bibr" target="#b44">[45]</ref>. However, it becomes less appealing in GNNs mainly because the node interaction through the message-passing mechanism (i.e., aggregation operator) would make node representations less distinguishable <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref>, leading to a drastic drop of task performance. This phenomenon is well known as the over-smoothing problem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>. In this paper, we show a surprising result that UGTs can effectively mitigate over-smoothing in deep GNNs. We conduct extensive experiments to evaluate this claim in this section.</p><p>UGTs preserves the high accuracy as GNNs go deeper. In Figure <ref type="figure" target="#fig_2">2</ref>, we vary the model depth of various architectures and report the test accuracy. All the experiments are conducted with architectures containing width 448 except for GAT on OGBN-Arxiv, in which we choose width 256 for GAT with 2 ? 10 layers and width 128 for GAT with 11 ? 20 layers, due to the memory limitation.</p><p>As we can see, the performance of trained dense GNNs suffers from a sharp performance drop when the model goes deeper, whereas UGTs impressively preserves the high accuracy across models.</p><p>Especially at the mild sparsity, i.e., 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Distance (MAD).</head><p>To further evaluate whether or not the good performance of UGTs can be contributed to the mitigation of over-smoothing, we visualize the smoothness of the node representations learned by UGTs and trained dense GNNs respectively. Following <ref type="bibr" target="#b45">[46]</ref>, we calculate the MAD distance among node representations for each layer during the process of sparsification. Concretely, MAD <ref type="bibr" target="#b45">[46]</ref> is the quantitative metric for measuring the smoothness of the node representations. The smaller the MAD is, the smoother the node representations are. Results are reported in Figure <ref type="figure">4</ref>. It can be observed that the node representations learned by UGTs keeps having a large distance throughout the optimization process, indicating a relieving of over-smoothing. On the contrary, the densely trained GCN suffers from severely indistinguishable representations of nodes.</p><p>TSNE Visualizations. Additionally, we visualize the node representations learned by UGTs and the trained dense GNNs with 16 and 32 layers, respectively, on both GCN and GAT architectures. Due to the limited space, we show the results of GCN in Figure <ref type="figure" target="#fig_3">3</ref> and put the visualization of GAT in the Appendix B.1. We can see that the node representations learned by the trained dense GCN are over-mixing in all scenarios and, in the deeper models (i.e., 32 layers), seem to be more indistinguishable. Meanwhile, the projection of node representations learned by UGTs maintains clearly distinguishable, again providing the empirical evidence of UGTs in mitigating over-smoothing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effect of Sparsity on UGTs</head><p>To better understand the effect of sparsity on the performance of UGTs, we provide a comprehensive study in Figure <ref type="figure" target="#fig_4">5</ref> where the performance of UGTs with respect to different sparsity levels on different architectures. We summarize our observations below.</p><p>1 UGTs consistently finds matching untrained graph subnetworks at a large range of sparsities, including the extreme ones. A matching untrained graph subnetwork can be identified with sparsities from 0.1 even up to 0.99 on small-scale datasets such as Cora, Citeseer and Pubmed. For large-scale OGBN-Arxiv, it is more difficult to find matching untrained subnetworks. Matching subnetworks are mainly located within sparsities of 0.  In this section, we systematically study the performance of UGTs on out of distribution (OOD) detection, robustness against the input perturbations including feature and edge perturbations. Following <ref type="bibr" target="#b46">[47]</ref>, we create OOD samples by specifying all samples from 40% of classes and removing them from the training set. We create feature perturbations by replacing them with the noise sampled from Bernoulli distribution with p=0.5 and edge perturbations by moving edge's end point at random. The results of OOD experiments are reported in Figure <ref type="figure">6</ref> and Figure <ref type="figure" target="#fig_1">10</ref> (shown in Appendix B.2). The results of robustness experiments are reported in Figure <ref type="figure" target="#fig_6">8</ref> and Figure <ref type="figure">7</ref>. We summarize our observations as follows:</p><p>1 UGTs enjoys matching performance on OOD detection. Figure <ref type="figure">6</ref> and Figure <ref type="figure" target="#fig_1">10</ref> show that untrained graph subnetworks discovered by UGTs achieve matching performance on OOD detection compared with the trained dense GNNs in most cases. Besides, UGTs consistently outperforms Edge-Popup method at a large range of sparsities on OOD detection.</p><p>2 UGTs produces highly sparse yet robust subnetworks on input perturbations. Figure <ref type="figure">7</ref> and Figure <ref type="figure" target="#fig_6">8</ref> demonstrate that UGTs with high sparsity level (Sparsity=0.9) achieves more robust results than the trained dense GNNs on both feature and edge perturbations with perturbation percentage ranging from 0 to 40%. Again, UGTs consistently outperforms Edge-Popup with both perturbation types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on Graph-level Task and Other Datasets</head><p>To draw a solid conclusion, we further conduct extensive experiments of graph-level task on OGBGmolhiv and OGBG-molbace; node-level task on TEXAS and OGBN-Products. The experiments are based on GCN model with width=448 and depth=3. Table <ref type="table" target="#tab_6">3</ref> consistently verifies that a matching untrained subnetwork can be identified in GNNs across multiple tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we for the first time confirm the existence of matching untrained subnetworks at a large range of sparsity. UGTs consistently outperforms the previous untrained technique -Edge-Popup on multiple graph datasets across various GNN architectures. What's more, we show a surprising result that searching for an untrained subnetwork within a randomly weighted dense GNN instead of directly training the latter can significantly mitigate the over-smoothing problem of deep GNNs. Across popular datasets, e.g., Cora, Citeseer, Pubmed, and OGBN-Arxiv, our method UGTs can achieve comparable or better performance with the various well-studied techniques that are specifically designed for over-smoothing. Moreover, we empirically find that UGTs also achieves appealing performance on other desirable aspects, such as out-of-distribution detection and robustness. The strong results of our paper point out a surprising but perhaps worth-a-try direction to obtain highperforming GNNs, i.e., finding the Untrained Tickets located within a randomly weighted dense GNN instead of training it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>In this paper, all experiments on Cora/Citeseer/Pubmed datasets are conducted on 1 GeForce RTX 2080TI (11GB) and all experiments on OGBN-Arxiv are conducted on 1 DGX-A100 (40GB). All the results reported in this paper are conducted by 5 independent repeated runs.</p><p>Train-Val-Test splitting Datasets We use 140 (Cora), 120 (Citeseer) and 60 (PubMed) labeled data for training, 500 nodes for validation and 1000 nodes for testing. We follow the strategy in <ref type="bibr" target="#b14">[15]</ref> for splitting OGBN-Arxiv dataset.</p><p>Hyper-parameter Configuration We follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref> to configure the hyper-parameters for training dense GNN models. All hyper-parameters configurations for UGTs are summarized in Table <ref type="table" target="#tab_7">4</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Out of distribution detection</head><p>Figure <ref type="figure" target="#fig_1">10</ref> shows the OOD performance for UGTs and the trained dense GNNs based on GAT architecture. As we can observe, UGTs achieves very appealing results on OOD performance than the corresponding trained dense GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Robustness against input perturbations</head><p>In this section, we explore the robustness against input perturbations with varying the sparsity of untrained GNNs. Experiments are conducted on GAT and GCN architectures with width=256 and depth=2. Results are reported in Figure <ref type="figure" target="#fig_2">12</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 The accuracy performance w.r.t model width</head><p>Figure <ref type="figure" target="#fig_9">13</ref> shows the performance of UGTs on different architectures with varying model width from 16 to 1024 and fix depth=2. We summarize observations as follows:</p><p>1 Performance of UGTs improves with the width of the GNN models. With width increasing from 16 to 256, the performance of UGTs improves apparently and after width=256, the benefits from model width are saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Ablation studies</head><p>We conduct the ablation studies to show the effectiveness of UGTs.The results showed on To preliminary understand why UGTs can mitigate over-smoothing while the trained dense GNNs can not, we calculate the gradient norm of each layer for UGTs and dense GCN during training. In order to have a fair comparison, we calculate the gradient norm of ? (m l ? l ) L(g(A, X; ? m), y) for UGTs and the gradient norm of ? ? l L(g(A, X; ?), y) for dense GCN where l denotes the layer. Results are reported in Figure <ref type="figure" target="#fig_10">14</ref>.</p><p>As we can observe, the gradient vanishing problem may exist for training deep dense GCN since the gradient norm for dense GCN is extremely small while UGTs does not have this problem. This problem might also be indicated by the training loss where the training loss for dense GCN does not decrease while the training loss for UGTs decreases a lot. This might explain why UGTs performs well for deep GNNs.  The results are reported in Table <ref type="table" target="#tab_12">6</ref>. It can be observed again that UGTs consistently outperforms all the baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Pseudocode</head><p>Pseudocode is showed in Algothrim 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>,</head><label></label><figDesc>You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets. Proceedings of the First Learning on Graphs Conference (LoG 2022), PMLR 198, Virtual Event, December 9-12, 2022.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Performance of untrained graph subnetworks (UGTs (ours) and Edge-Popup<ref type="bibr" target="#b11">[12]</ref>) and the corresponding trained dense GNNs. We demonstrate that as the model size increases, UGTs is able to find an untrained subnetwork with its random initializations, that can match the performance of the corresponding fully-trained dense GNNs. The x-axis denotes the corresponding model size for each point, e.g. "64-2" represents a model with 2 layers and width 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:The performance of GNNs with increasing model depths. Experiments are conducted on various GNNs with Cora, Citeseer, Pubmed and OGBN-Arxiv. We observe that as the model goes deeper, fully-trained dense GNNs suffer from a sharp accuracy drop, while UGTs preserves the high accuracy. All the results reported are averaged from 5 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: TSNE visualization of node representations learned by densely trained GCN and UGTs. Ten classes are randomly sampled from OGBN-Arxiv for visualization. Model depth is set as 16 and 32 respectively; width is set as 448. See Appendix B.1 for GAT architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The accuracy of GNNs w.r.t varying sparsities. Experiments are conducted on various GNNs with 2 layers and width 256 for Cora, Citeseer and Pubmed, 4 layers and width 386 for OGBN-Arxiv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Out-of-distribution performance (ROC-AUC). Experiments are conducted with GCN (Width: 256, Depth: 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The robust performance on edge perturbations with the fraction of perturbed edges varying from 0% to 40%. Experiments are conducted with GCN and GAT (Width: 256, Depth: 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9</head><label>9</label><figDesc>Figure 9 provides the TSNE visualization for node representations learned by UGTs and dense GAT. It can be observed that the node representations learned by the trained dense GAT are mixed while the node representations learned by UGTs are disentangled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: TSNE visualization for node representations. Experiments are based on GAT with fixed width 448.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Model Width: The accuracy performance of subnetworks from untrained GNNs w.r.t varying Hidden-Size. The "S01,S05,S09" represents the sparsity of the untrained GNNs. The dashed line represents the results of the trained dense GNNs. Experiments are based on GNNs with 2 layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Gradient norm w.r.t each layer during training. Experiments are conducted on Cora with GCN architecture containing 32 layers and width 448.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We represent matrices by bold uppercase characters, e.g. X, vectors by bold lowercase characters, e.g. x, and scalars by normal lowercase characters, e.g. x. We denote the i th row of a matrix A by A[i, :], and the (i, j) th element of matrix A by A[i, j]. We consider a graph G = {V, E} where E is a set of edges and V is a set of nodes. Let g(A, X; ?) be a graph neural network where A ? {0, 1} |V |?|V | is adjacency matrix for describing the overall graph topology, and X denotes nodal features . A[i, j] = 1 denotes the edge between node v i and node v j . Let f (X; ?) be a neural network with the weights ?. ? 0 denotes the L 0 norm. Sparse Neural Networks. Given a dense network ? l ? R d l with a dimension of d l in each layer l ? {1, ..., L}, binary mask m l ? {0, 1} d l yielding a sparse Neural Networks with sparse weights</figDesc><table><row><cell>3.1 Preliminaries and Setups</cell></row><row><cell>Notations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) of different training techniques. The experiments are based on GCN models with 16, 32 layers, respectively. Width is set to 448. See Appendix B.7 for GAT architecture.The results of the other methods are obtained from<ref type="bibr" target="#b41">[42]</ref>.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell>N-Layers</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell>32</cell></row><row><cell>Trained Dense GCN</cell><cell>21.4</cell><cell>21.2</cell><cell>19.5</cell><cell>20.2</cell><cell>39.1</cell><cell>38.7</cell></row><row><cell>+Residual</cell><cell>20.1</cell><cell>19.6</cell><cell>20.8</cell><cell>20.90</cell><cell>38.8</cell><cell>38.7</cell></row><row><cell>+Jumping</cell><cell>76.0</cell><cell>75.5</cell><cell>58.3</cell><cell>55.0</cell><cell>75.6</cell><cell>75.3</cell></row><row><cell>+NodeNorm</cell><cell>21.5</cell><cell>21.4</cell><cell>18.8</cell><cell>19.1</cell><cell>18.9</cell><cell>18</cell></row><row><cell>+PairNorm</cell><cell>55.7</cell><cell>17.7</cell><cell>27.4</cell><cell>20.6</cell><cell>71.3</cell><cell>61.5</cell></row><row><cell>+DropNode</cell><cell>27.6</cell><cell>27.6</cell><cell>21.8</cell><cell>22.1</cell><cell>40.3</cell><cell>40.3</cell></row><row><cell>+DropEdge</cell><cell>28.0</cell><cell>27.8</cell><cell>22.9</cell><cell>22.9</cell><cell>40.6</cell><cell>40.5</cell></row><row><cell>UGTs-GCN</cell><cell cols="6">77.3 ? 0.9 77.5 ? 0.8 61.1?0.9 56.2?0.4 77.6?0.9 76.3?1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Graph datasets statistics.</figDesc><table><row><cell>DataSets</cell><cell>#Graphs</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="2">#Classes #Features</cell><cell>Metric</cell></row><row><cell>Cora</cell><cell>1</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell><cell>Accuracy</cell></row><row><cell>Citeseer</cell><cell>1</cell><cell>3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell><cell>Accuracy</cell></row><row><cell>Pubmed</cell><cell>1</cell><cell>19717</cell><cell>44338</cell><cell>3</cell><cell>3288</cell><cell>Accuracy</cell></row><row><cell>OGBN-Arxiv</cell><cell>1</cell><cell>169343</cell><cell>1166243</cell><cell>40</cell><cell>128</cell><cell>Accuracy</cell></row><row><cell>Texas</cell><cell>1</cell><cell>183</cell><cell>309</cell><cell>5</cell><cell>1703</cell><cell>Accuracy</cell></row><row><cell>OGBN-Products</cell><cell>1</cell><cell>24449029</cell><cell>61859140</cell><cell>47</cell><cell>100</cell><cell>Accuracy</cell></row><row><cell>OGBG-molhiv</cell><cell>41127</cell><cell cols="2">25.5(Average) 27.5(Average)</cell><cell>2</cell><cell>-</cell><cell>ROC-AUC</cell></row><row><cell>OGBG-molbace</cell><cell>1513</cell><cell cols="2">34.1(Average) 36.9(Average)</cell><cell>2</cell><cell>-</cell><cell>ROC-AUC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1, UGTs almost has no deterioration with the increased number of layers.UGTs achieves competitive performance with the well-versed training techniques. To further validate the effectiveness of UGTs in mitigating over-smoothing, we compare UGTs with six stateof-the-art techniques for the over-smoothing problem, including Residual connections, Jumping connections, NodeNorm, PairNorm, DropEdge, and DropNode. We follow the experimental setting in<ref type="bibr" target="#b41">[42]</ref> and conduct experiments on Cora/Citeseer/Pubmed with GAT containing 16 and 32 layers. Model width is set to 448 for GAT on Cora/Citeseer/Pubmed. The results of the other methods are obtained from<ref type="bibr" target="#b41">[42]</ref> 2 . These results again verify our hypothesis that training bottlenecks of deep GNNs (e.g., over-smoothing) can be avoided or mitigated by finding untrained subnetworks without training weights at all. Mean Average Distance among node representations of each GNN layer. Experiments are conducted on Cora with GCN containing 32 layers and width 448.</figDesc><table><row><cell>0.20 0.40 0.60 0.80 MAD 0.2 0.4 0.6 0.8 1.0</cell><cell>0 5</cell><cell>Epoch=10 1e 3</cell><cell cols="2">0.2 0.4 0.6 0.8</cell><cell></cell><cell>0.0 2.5</cell><cell cols="2">Epoch=150 1e 3</cell><cell></cell><cell cols="2">0.2 0.4 0.6 0.8</cell><cell>0.0 2.5</cell><cell>Epoch=270 1e 3</cell><cell>0.2 0.4 0.6 0.8</cell><cell>0 5 1e 3 Epoch=400</cell></row><row><cell cols="3">2 6 10 14 18 22 26 30 L-layers 0.00 0.0 0.2 0.0</cell><cell cols="2">2 0.0</cell><cell>6</cell><cell cols="2">10</cell><cell>14 L-layers 18 22 0.4 UGTs 26</cell><cell>30</cell><cell cols="2">2 6 10 14 18 22 26 30 L-layers 0.0 0.6 Trained Dense</cell><cell>2 6 10 14 18 22 26 0.0 0.8 30 L-layers</cell><cell>1.0</cell></row><row><cell cols="4">0.50 0.75 0.8 1.0 Figure 4: 0.25 GAT ACC 0.25 0.50 0.75 GCN ACC 0 0.01 0.03 0.05 0.08 0.1 0.2 Cora 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.25 0.50 0.75 GIN ACC 0.6 0.4 0.2 0.0 0.2 0.0</cell><cell cols="7">0.45 0.70 0.20 0.20 0.45 0.70 0 0.01 0.03 0.05 0.08 0.1 Citeseer 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.45 0.70 0.20 0.4 Edge-Popup UGTs</cell><cell>0.55 0.75 0.25 0.75 0.55 0.25 0.75 0.55 0 0.01 0.03 0.05 0.08 0.1 Pubmed 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.25 0.6 Trained Dense</cell><cell>0.25 0.55 0.75 0.75 0.55 0.25 0.75 0.55 0.25 0 0.01 0.03 0.05 0.08 OGBN-Arxiv 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.8 0.99 1.0 Sparsity</cell></row></table><note><p><p><p>Table</p>1</p>shows that UGTs consistently outperforms all these advanced techniques on Cora, Citeseer, and Pubmed. For instance, UGTs outperforms the best performing technique (+Jumping) by 2.0%, 1.2%, 1.0% on Cora, Citeseer and Pubmed respectively with 32 layers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>3 ? 0.6.2 What's more, UGTs consistently outperforms Edge-Popup. UGTs shows better performance than Edge-Popup at high sparsities across different architectures on Cora, Citeseer, Pubmed and OGBN-Arxiv. Surprisingly, increasing sparsity from 0.7 to 0.99, UGTs maintains very a high accuracy, whereas the accuracy of Edge-Popup shows a notable degradation. It is in accord with our expectation since UGTs finds important weights globally by searching for the well-performing sparse topology across layers.</figDesc><table><row><cell cols="4">4.4 Broader Evaluation of UGTs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cora ROC AUC Citeseer ROC AUC</cell><cell>0.20 0.40 0.60 0.80 0.20 0.40 0.60 0.80 0.80 0.4 0.6 0.8 1.0</cell><cell>256-2</cell><cell>0.20 0.40 0.60 0.80 0.20 0.80 0.40 0.60 0.80</cell><cell>256-4</cell><cell>0.80 0.20 0.40 0.60 0.80 0.20 0.40 0.60 0.80</cell><cell>256-6</cell></row><row><cell>Pubmed ROC AUC</cell><cell cols="2">0 0.01 0.03 0.05 0.08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.20 0.40 0.60 0.2 0.0 0.2 0.0</cell><cell cols="2">0.40 0.60 0 0.01 0.03 0.05 0.08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.20 0.4 0.6 Edge-Popup UGTs Trained Dense</cell><cell cols="2">0.40 0.60 0 0.01 0.03 0.05 0.08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.20 0.8 0.99 1.0 Sparsity</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Experments on graph-level tasks and other datasets. GCN Model with width:448, depth:3 are adopted for this experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">Node-level: OGBN-Products (Accuracy:%)</cell></row><row><cell cols="2">Sparsity= 0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9 0.95 0.99</cell></row><row><cell>Dense</cell><cell cols="9">79.5 79.5 79.5 79.5 79.5 79.5 79.5 79.5 79.5 79.5 79.5</cell></row><row><cell>UGTs</cell><cell cols="9">75.6 77.7 78.7 77.8 79.3 79.5 79.9 78.6 74.5 64.1 35.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Node-level: TEXAS (Accuracy:%)</cell><cell></cell></row><row><cell>Dense</cell><cell cols="9">62.2 62.2 62.2 62.2 62.2 62.2 62.2 62.2 62.2 62.2 62.2</cell></row><row><cell>UGTs</cell><cell cols="9">62.1 62.2 62.2 62.2 62.2 61.3 62.2 63.1 64.8 64.8 55.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Graph-level: OGBG-molhiv (ROCAUC:%)</cell></row><row><cell>Dense</cell><cell cols="9">77.4 77.4 77.4 77.4 77.4 77.4 77.4 77.4 77.4 77.4 77.4</cell></row><row><cell>UGTs</cell><cell cols="9">76.4 76.9 76.5 76.1 76.3 77.3 75.8 77.1 73.1 75.3 75.1</cell></row><row><cell></cell><cell></cell><cell cols="7">Graph-level: OGBG-molbace (ROCAUC:%)</cell></row><row><cell>Dense</cell><cell cols="9">78.3 78.3 78.3 78.3 78.3 78.3 78.3 78.3 78.3 78.3 78.3</cell></row><row><cell>UGTs</cell><cell cols="9">76.0 73.7 77.0 77.1 77.0 77.6 78.4 77.3 76.2 75.6 74.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Implementation details for UGTs.</figDesc><table><row><cell>DataSets</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell><cell>OGBN-Arxiv</cell></row><row><cell>Total Epoches</cell><cell>400</cell><cell>400</cell><cell>400</cell><cell>400</cell></row><row><cell>Learning Rate</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01 (GNNs with Layers&lt;10) 0.001(GNNs with Layers&gt;10)</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Weight Decay</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>n(total adjustion epoches)</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>t0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>?t</cell><cell cols="3">1 epoch 1 epoch 1 epoch</cell><cell>1 epoch</cell></row><row><cell cols="2">B More Experimental Results</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>B.1 TSNE visualization.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>and Figure 11.It can be observed that the robustness achieved by UGTs is increasing with the increase of sparsity for both edge and feature perturbation types. Besides, the robustness achieved by UGTs at large sparsity, e.g., sparsity =0.9, can outperform the counterpart trained dense GNNs.</figDesc><table><row><cell>Cora ROC AUC Citeseer ROC AUC Pubmed ROC AUC Cora ACC ACC Citeseer ACC Pubmed</cell><cell cols="6">0.20 0.40 0.60 0.80 0.20 0.40 0.60 0.80 0 0.01 0.03 0.05 0.08 0.1 0.2 256-2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.20 0.40 0.60 0.80 0.4 0.6 0.8 1.0 0.2 0.6 0.8 GAT 1.0 0.8 0.4 0.3 0.5 0.7 0.6 0.4 0.8 0.0 0.2 0.0 0.2 16 32 64 128 256 512 768 1024 Hidden-size 0.5 0.6 0.7 0.2 0.0 0.2 0.0 UGTs (S=0.5) UGTs (S=0.9) 0.20 0.40 0.60 0.80 0.20 0.40 0.60 0.80 0 0.01 0.03 0.05 0.08 0.1 0.2 256-4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.20 0.40 0.60 0.80 0.4 GCN 0.8 0.6 0.2 0.5 0.7 0.3 0.8 0.4 0.6 Edge-Popup UGTs 0.6 0.7 16 32 64 128 256 512 768 1024 Hidden-size 0.5 0.4 0.6 Trained Dense Edge-Popup (S=0.5) Edge-Popup (S=0.9) 0.20 0.40 0.60 0.80 0.20 0.40 0.60 0.80 0 0.01 0.03 0.05 0.08 0.1 0.2 256-6 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.95 0.99 Sparsity 0.20 0.40 0.60 0.8 GIN 0.6 0.4 0.2 0.5 0.7 0.3 0.8 0.80 0.8 1.0 0.6 0.7 16 32 64 128 256 512 768 0.5 0.8 1.0 1024 Hidden-size Trained Dense</cell></row><row><cell cols="7">Figure 10: Out-of-distribution performance (ROC-AUC). Experiments are based on GAT archi-</cell></row><row><cell cols="3">tecture (Width:256, Depth:2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8 1.0</cell><cell>Cora</cell><cell cols="2">0.7</cell><cell>Citeseer</cell><cell>0.8</cell><cell>Pubmed</cell></row><row><cell>GAT GCN ACC ACC</cell><cell>0.6 0.7 0.6 0.7 0.8 0.2 0.4 0.6 0.8</cell><cell></cell><cell cols="2">0.5 0.6 0.7 0.5 0.6</cell><cell></cell><cell>0.7 0.8 0.6 0.7</cell></row><row><cell></cell><cell cols="3">0.1 0.0 0.0 Trained Dense (R=0.1) 0.2 0.3 0.4 Sparsity 0.5 0.6 0.2 0.7 Trained Dense (R=0.3) 0.8 0.9</cell><cell>0.1</cell><cell>0.2 0.4 UGTs (R=0.1) 0.3 0.4 Sparsity 0.5 0.6 Edge-Popup (R=0.1) 0.7 0.8 0.9 0.6</cell><cell>0.1 0.6 UGTs (R=0.3) 0.2 0.3</cell><cell>0.4 0.8 Sparsity 0.5 0.6 Edge-Popup (R=0.3) 0.7 0.8</cell><cell>1.0 0.9</cell></row><row><cell cols="7">Figure 11: The robust performance on edge perturbations.R denotes the fraction of perturbed</cell></row><row><cell cols="3">edges.(width:256, Depth:2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.82 1.0</cell><cell>Cora</cell><cell></cell><cell>0.7</cell><cell>Citeseer</cell><cell>0.78</cell><cell>Pubmed</cell></row><row><cell>GAT ACC</cell><cell>0.75 0.8</cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.74</cell></row><row><cell></cell><cell>0.65 0.82 0.4 0.6</cell><cell></cell><cell></cell><cell>0.5 0.7</cell><cell></cell><cell>0.78 0.70</cell></row><row><cell>GCN ACC</cell><cell cols="3">0.1 0.67 0.75 0.2 0.0 0.0 Trained Dense (R=0.1) 0.2 0.3 0.4 Sparsity 0.5 0.6 0.2 0.7 Trained Dense (R=0.3) 0.8 0.9</cell><cell cols="2">0.6 0.1 0.5 UGTs (R=0.1) 0.2 0.3 0.4 Sparsity 0.5 0.6 0.4 Edge-Popup (R=0.1) 0.7 0.8 0.9 0.6</cell><cell>0.74 0.1 0.70 UGTs (R=0.3) 0.2 0.3 0.8 0.4 Sparsity 0.5 0.6 Edge-Popup (R=0.3) 0.7 0.8</cell><cell>1.0 0.9</cell></row></table><note><p>Figure 12: The robust performance on feature perturbations.R denotes the fraction of perturbed nodes. (width:256, Depth:2)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>The results are reported in the following table and it shows that global sparsification plays an important role for finding important weights and gradual sparsification is crucial for further boosting performance at high sparsity level.</figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies based on GAT (Depth:2, Width:256) and Cora.</figDesc><table><row><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.95</cell></row><row><cell>Edge-Popup</cell><cell cols="6">0.814 0.81 0.809 0.81 0.791 0.461</cell></row><row><cell>UGTs -global sparsification</cell><cell cols="6">0.807 0.816 0.817 0.804 0.799 0.731</cell></row><row><cell cols="7">UGTs -gradual sparsification 0.806 0.818 0.821 0.821 0.804 0.795</cell></row><row><cell>UGTs</cell><cell cols="6">0.797 0.811 0.81 0.815 0.817 0.822</cell></row><row><cell>B.6 Observations via gradient norm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) of different training techniques. The experiments are based on GAT models with 16, 32 layers, respectively. Width is set to 448.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell>N-Layers</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell>32</cell></row><row><cell>Trained Dense GAT</cell><cell>20.6</cell><cell>13.0</cell><cell>20.0</cell><cell>16.9</cell><cell>17.9</cell><cell>18.0</cell></row><row><cell>+Residual</cell><cell>19.9</cell><cell>20.7</cell><cell>17.7</cell><cell>19.2</cell><cell>41.6</cell><cell>40.8</cell></row><row><cell>+Jumping</cell><cell>39.7</cell><cell>27.8</cell><cell>29.1</cell><cell>25.5</cell><cell>57.3</cell><cell>57.1</cell></row><row><cell>+NodeNorm</cell><cell>70.9</cell><cell>11.0</cell><cell>17.1</cell><cell>18.4</cell><cell>72.2</cell><cell>59.7</cell></row><row><cell>+PairNorm</cell><cell>27.9</cell><cell>12.1</cell><cell>22.8</cell><cell>17.7</cell><cell>73.0</cell><cell>44.0</cell></row><row><cell>+DropNode</cell><cell>23.6</cell><cell>13.0</cell><cell>18.8</cell><cell>7.0</cell><cell>26.7</cell><cell>18.0</cell></row><row><cell>+DropEdge</cell><cell>24.8</cell><cell>13.0</cell><cell>19.4</cell><cell>7.0</cell><cell>19.3</cell><cell>18.0</cell></row><row><cell>UGTs-GAT</cell><cell cols="6">76.7 ? 1.1 74.9?0.2 62.7?0.7 56.5?1.1 77.9?0.5 75.5?1.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>All experiments based on GAT architecture are conducted with heads=1 in this study.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/VITA-Group/Deep_GCN_Benchmarking.git</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/VITA-Group/Deep_GCN_Benchmarking.git</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. <rs type="grantNumber">NWO-2021.060</rs> and <rs type="grantNumber">EINF-3214/L1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5sFVrDB">
					<idno type="grant-number">NWO-2021.060</idno>
				</org>
				<org type="funding" xml:id="_EvHejRP">
					<idno type="grant-number">EINF-3214/L1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Untrained GNNs Tickets (UGTs)</head><p>Input: a GNN g(A, X; ?), initial mask m = 1 ? R |?| with latent scores S, learning rate ?, hyperparameters for the gradual sparsification schedule s i , s f , t 0 , and ?t. Output: g(A, X; ? m), y Randomly initialize model weights ? and S. for t = 1 to T do #Calculate the current sparsity level s t by Eq. 5.</p><p>n?t ) #Get the global threshold value at top s t by sorting S in ascending order. S thres ?-T hresholding(S, s t ) #Generate the binary mask. m ?-0 if S &lt; S thres else 1 #Update S. S ?-S -?? S L(g(A, X; ? m), y) end for Return g(A, X; ? m), y</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017. 1, 2, 5, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujia</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2017">190-i198, 2017. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention based spatialtemporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01067</idno>
		<title level="m">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 3</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What&apos;s hidden in a randomly weighted neural network?</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 4, 6</date>
			<biblScope unit="page" from="11893" to="11902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Chijiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasutoshi</forename><surname>Shin'ya Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Ida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Umakoshi</surname></persName>
		</author>
		<author>
			<persName><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09269</idno>
		<title level="m">Pruning randomly initialized neural networks with iterative randomization</title>
		<imprint>
			<date type="published" when="2006">2021. 1, 3, 4, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified lottery ticket hypothesis for graph neural networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1695" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The state of sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574</idno>
		<imprint>
			<date type="published" when="2005">2019. 2, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse training via boosting pruning plasticity with neuroregeneration</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Atashgahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Decebal</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mocanu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2021. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometrically principled connections in graph neural networks</title>
		<author>
			<persName><forename type="first">Shunwang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11415" to="11424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding and resolving performance degradation in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07107</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tackling oversmoothing for general graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09864</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Dropedge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14769</idno>
		<title level="m">Supermasks in superposition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Drawing robust scratch tickets: Subnetworks with inborn robustness are found within randomly initialized networks</title>
		<author>
			<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Decebal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><forename type="middle">H</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware</title>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amarsagar</forename><surname>Reddy Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Matavalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><surname>Pechenizkiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Neural Computing and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Soft threshold weight reparameterization for learnable sparsity</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10521</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph posterior network: Bayesian predictive uncertainty for node classification</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234,2021.13</idno>
		<title level="m">Do transformers really perform bad for graph representation? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
