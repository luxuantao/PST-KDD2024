<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-19">19 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongqiang</forename><surname>Du</surname></persName>
							<email>hqdu@nwpu-aslp.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<email>lxie@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving robustness of one-shot voice conversion with deep discriminative speaker encoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-19">19 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.10406v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>voice conversion</term>
					<term>one-shot</term>
					<term>speaker embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot voice conversion has received significant attention since only one utterance from source speaker and target speaker respectively is required. Moreover, source speaker and target speaker do not need to be seen during training. However, available one-shot voice conversion approaches are not stable for unseen speakers as the speaker embedding extracted from one utterance of an unseen speaker is not reliable. In this paper, we propose a deep discriminative speaker encoder to extract speaker embedding from one utterance more effectively. Specifically, the speaker encoder first integrates residual network and squeeze-and-excitation network to extract discriminative speaker information in frame level by modeling framewise and channel-wise interdependence in features. Then attention mechanism is introduced to further emphasize speaker related information via assigning different weights to frame level speaker information. Finally a statistic pooling layer is used to aggregate weighted frame level speaker information to form utterance level speaker embedding. The experimental results demonstrate that our proposed speaker encoder can improve the robustness of one-shot voice conversion for unseen speakers and outperforms baseline systems in terms of speech quality and speaker similarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Voice conversion (VC) is a technique to modify the speech signal of a source speaker to make it sound like that of a target speaker without changing the linguistic content <ref type="bibr" target="#b0">[1]</ref>. This technique has many applications, including expressive speech synthesis, speech enhancement, movie dubbing as well as other entertainment applications.</p><p>Various approaches have been proposed to achieve voice conversion, such as Gaussian mixture model (GMM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, frequency warping approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, exemplar based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, and neural network based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. While these works require to know either source speaker or target speaker or both in training, which limits their use in the real application scenarios. Recently, one-shot voice conversion approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> are proposed. Compared with previous methods, source and target speakers at run-time are not required to be seen during training. Additionally, only one utterance from the source speaker and target speaker respectively is needed. The speaker identity of converted speech can be controlled independently by the speaker embedding extracted from target speech.</p><p>Despite recent progress, the available one-shot voice conversion approaches are not stable for unseen speakers <ref type="bibr" target="#b19">[20]</ref>. This *Lei Xie is the corresponding author, lxie@nwpu.edu.cn. is mainly because speaker embedding extracted from one utterance of an unseen speaker is not reliable <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>, which has a great influence on the stability of one-shot conversion <ref type="bibr" target="#b19">[20]</ref>. Speaker embedding extractor can be a speaker encoder which is jointly trained with a conversion model or a pre-trained model for speaker information extraction, such as i-vector <ref type="bibr" target="#b21">[22]</ref>, dvector <ref type="bibr" target="#b22">[23]</ref>, or x-vector <ref type="bibr" target="#b23">[24]</ref>. The speaker embedding extractor jointly trained with the conversion model is more suitable for voice conversion than the pre-trained models <ref type="bibr" target="#b19">[20]</ref>. When the network is jointly optimized, speaker embedding extractor is an inherent part of the model, which makes the generation of speech with correct speaker embedding consistently.</p><p>There are some studies on jointly training speaker encoder and voice conversion model. The speaker encoder generally consists of two parts: extracting frame level and utterance level features <ref type="bibr" target="#b24">[25]</ref>. The frame level extractor takes acoustic features as input and outputs frame level features. It can be done via recurrent neural networks <ref type="bibr" target="#b25">[26]</ref> and convolutional neural networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>. In particular, convolutional neural network based residual network (ResNet) is a powerful speaker embedding extractor <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>. The utterance level extractor further aggregates variable-length frame level features into utterance level speaker embedding. Average pooling <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref> is a popular method to obtain speaker embedding by averaging all frame level features. Another method <ref type="bibr" target="#b16">[17]</ref> first uses the last state of unidirectional gated recurrent unit (GRU) layer as the utterance level speaker representation and then multi-head attention is utilized as a post-processing module to obtain final speaker embedding.</p><p>In this paper, to further improve the effectiveness of speaker embedding extracted from only one utterance of an unseen speaker, we propose a deep discriminative speaker encoder. Inspired by <ref type="bibr" target="#b27">[28]</ref>, first residual network and squeezeand-excitation network <ref type="bibr" target="#b28">[29]</ref> are integrated to extract discriminative frame level speaker information by modeling frame-wise and channel-wise interdependence in features. Then attention mechanism is introduced to give different weights to frame level speaker information. Finally, a statistic pooling layer <ref type="bibr" target="#b29">[30]</ref> is used to aggregate weighted frame level speaker information to generate utterance level speaker embedding. Experimental results show that our proposed speaker encoder can improve the robustness of one-shot voice conversion and outperforms baseline systems in terms of speech quality and speaker similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Residual network based speaker embedding extractor</head><p>Residual network (ResNet) has been widely used in speaker verification, which achieves promising performance for both longduration and short-duration utterances <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The architecture of ResNet based speaker embedding extractor includes several ResNet blocks, followed by a statistics pooling layer and fully-connected (FC) layers. ResNet block operates on frame level features, which consists of convolutional layers, rectified linear units (ReLU) and batch normalization (BN) layers. Residual connection in ResNet block helps to build a deep neural network and avoids the vanishing gradient problem <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Increasing the depth of a neural network can significantly improve the quality of representations <ref type="bibr" target="#b28">[29]</ref>. Additionally, batch normalization helps to improve the stability of the training process of deep neural networks. Then a statistics pooling layer calculates the mean and standard deviation of each sample along the time-axis to form utterance level representation. Finally, two fully-connected (FC) layers project the utterance level representation into a fixed dimensional speaker embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Squeeze-and-excitation network</head><p>Squeeze-and-excitation (SE) network <ref type="bibr" target="#b28">[29]</ref> is first introduced to model channel interdependence in features for image classification. SE network can be used as a block and inserted in the convolutional neural network.</p><p>SE block consists of two operations: squeeze operation and excitation operation. Squeeze operation utilizes average pooling to generate channel-wise statistics. The statistics are mean vector z of frame level features ht across the time-axis.</p><formula xml:id="formula_0">z = 1 T T t ht<label>(1)</label></formula><p>Then z is used in the excitation operation to calculate weights for each channel. The excitation operation is formulated as follows:</p><formula xml:id="formula_1">s = σ(W2δ(W1z))<label>(2)</label></formula><p>where σ(•) and δ(•) are sigmoid and ReLU function respectively.</p><formula xml:id="formula_2">W1 ∈ R C× C r , W2 ∈ R C r ×C</formula><p>, C and r refer to the number of input channel and reduction ratio respectively. The channel-wise vector s contains the weight for each channel, which is between zero and one.</p><p>The final output ĥ of SE block is obtained by channel-wise multiplication between the original input h and corresponding weight in s. ĥ = sh</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Robust one-shot voice conversion 3.1. Deep discriminative speaker encoder</head><p>Speaker encoder is an important component in the framework of one-shot voice conversion, which is directly related to performance of the whole network for unseen speakers <ref type="bibr" target="#b19">[20]</ref>.</p><p>Inspired by the previous study on ResNet and SE block, we integrate ResNet with SE block to build a deep discriminative speaker encoder (DDSE) for robust one-shot voice conversion. Figure <ref type="figure" target="#fig_0">1</ref> (a) depicts the framework of speaker encoder. It consists of frame level feature processing and utterance level feature processing.</p><p>The frame level feature processing part consists of a Conv2D layer, a ReLU function and a batch normalization (BN) layer, followed by four ResNet-SE blocks. The framework of ResNet-SE block is shown in Figure <ref type="figure" target="#fig_0">1 (b)</ref>. A ResNet-SE block mainly consists of convolution layers. Filters in the convolution layer explicitly model local features and allow spatial translation invariance, which make convolution layer suitable to extract frame level features <ref type="bibr" target="#b27">[28]</ref>. SE block expands the temporal context of the frame level information by modeling channel interdependence in features, which has been verified to be helpful in speaker verification task <ref type="bibr" target="#b27">[28]</ref>. The framework of SE block is shown in Figure <ref type="figure" target="#fig_0">1 (c</ref>). An average pooling layer is utilized to generate channel-wise statistics. Then, two fully-connected layers capture the local channel dependencies. The first fullyconnected layer can be used to reduce the feature dimension for controlling the computational cost, while the second fullyconnected layer restores the number of feature to the original dimension. Finally, the channel-wise vector is obtained with a sigmoid layer to pay more attention to the discriminative channels for speaker representation.</p><p>The utterance level feature processing part consists of an attention block, followed by a statistic pooling layer and two fully-connected layers. Instead of directly using an average pooling layer where each frame level speaker information contributes equally to speaker embedding <ref type="bibr" target="#b17">[18]</ref>, we first introduce an attention block to further emphasize speaker related information. As shown in Figure <ref type="figure" target="#fig_0">1 (d)</ref>, the attention block takes the frame level speaker information as input and outputs the corresponding weights, which allows the speaker encoder to select the frames it deems relevant. Then a statistics pooling layer <ref type="bibr" target="#b24">[25]</ref> is used to calculate the weighted means and weighted standard deviations of the final extracted frame level features. The mean and deviation are combined to form an utterancelevel speaker representation. Finally, two fully-connected layers are introduced. The first one acts as a bottleneck layer to generate the low-dimensional speaker representation. The second one projects the speaker representation into a fixed dimensional speaker embedding.</p><p>In summary, convolution based ResNet is a powerful architecture to extract speaker representations by modeling relationships between frames. SE block contributes to the discriminative speaker representation learning by exploring the channel-wise information. Attention mechanism further makes speaker encoder emphasize speaker related information and overshadow other information. Therefore, the speaker embedding learned by this architecture concentrates on speaker characteristics more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust one-shot voice conversion with DDSE</head><p>The frameworks of available one-shot voice conversion approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> generally consist of a speaker encoder, a content encoder, and a decoder. AdaIN-VC <ref type="bibr" target="#b17">[18]</ref> is a successful end-to-end implementation, which is relatively more robust for unseen speakers <ref type="bibr" target="#b19">[20]</ref>. We use AdaIN-VC as a case study and replace the original speaker encoder with our proposed deep discriminative speaker encoder to make it more robust for unseen speakers. Note that the whole network is jointly optimized and the speaker encoder is optimized without explicit loss function.</p><p>The robust one-shot voice conversion method consists of two steps: conversion model training and run-time conversion. During the training stage, the speaker encoder and content encoder learn to extract speaker embedding and linguistic content representation from spectrum respectively. The decoder takes the two representations as inputs to reconstruct the spectrum. During run-time shown in Figure <ref type="figure" target="#fig_1">2</ref>, the speaker encoder extracts utterance level speaker embedding from target speech. The content encoder extracts frame level content representation from   database and VCC 2016 dataset <ref type="bibr" target="#b34">[35]</ref> respectively. For CMU-ARCTIC database, we select clb (female) and rms (male) as source speakers, and slt (female) and bdl (male) as target speakers. For VCC 2016 dataset, SF1 (female) and SM1 (male) are selected as source speakers, and TF2 (female) and TM3 (male) are selected as target speakers. For each target speaker, 20 utterances are used for evaluation. All audio files are downsampled to 16 kHz. Librosa is employed to extract 256 dimensional mel spectrogram with 50ms frame length and 12.5ms frame shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Systems and setup</head><p>• VAE-ORI: This is the original AdaIN-VC <ref type="bibr" target="#b17">[18]</ref> system.</p><p>The speaker encoder and content encoder take 256 dimensional mel spectrogram as input and the output is 128 dimensional speaker embedding and content representation respectively. To further improve the speech quality, the auto-regressive technique is applied to the decoder.</p><p>• VAE-GSE: This has the same setting as VAE-ORI except that the speaker encoder is replaced with global speaker embedding (GSE) utilized in <ref type="bibr" target="#b16">[17]</ref>.</p><p>• VAE-ResNet: This has the same setting as VAE-ORI except that the speaker encoder is replaced with ResNet.</p><p>• VAE-DDSE: This has the same setting as VAE-ORI except that the speaker encoder is replaced with our proposed deep discriminative speaker encoder (DDSE).</p><p>For the first ResNet-SE block, the kernel sizes and strides for the Conv2D layers are {3, 3, 1, 3, 3} and {{1, 1}, {1, 1}, {1, 1}, {1, 1}, {1, 1}} respectively. For the remaining ResNet-SE blocks, the kernel sizes and strides for the Conv2D layers are {3, 3, 1, 3, 3} and {{2, 2}, {1, 1}, {2, 2}, {1, 1}, {1, 1}} respectively. For the SE block, the reduction ratio r is set to 8.</p><p>Parallel WaveGAN <ref type="bibr" target="#b35">[36]</ref> is used to synthesize the converted speech. We follow the original configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Objective evaluation</head><p>Mel-cepstral distortion (MCD) <ref type="bibr" target="#b3">[4]</ref> is employed to measure the spectral distortion. MCD is the Euclidean distance of the mel spectrogram between the converted speech and the target speech. Given a speech frame, the MCD is defined as follows:</p><formula xml:id="formula_3">MCD[dB] = 10 ln 10 2 N n=1 X conv n − X ref n 2 ,<label>(4)</label></formula><p>where X conv Table <ref type="table" target="#tab_1">1</ref> shows the average MCD for different systems. Intra-gender conversion is better than inter-gender conversion for the four systems. We also observe that VAE-DDSE significantly outperforms the VAE-ORI and VAE-GSE in both intraand inter-gender conversions. VAE-DDSE performs better than VAE-ResNet and achieves the lowest average MCD of 10.75 dB. The objective evaluation results further confirm that the extracted speaker embedding has a great impact on the performance of one-shot voice conversion. In Figure <ref type="figure" target="#fig_3">3</ref>, we show an example that compares spectrum of the same utterance converted from clb (female) to bdl (male) by different systems: (a) VAE-ORI, (b) VAE-DDSE, and (c) Target. The harmonics of the spectrum are closely related to the speaker identity <ref type="bibr" target="#b36">[37]</ref>, which is controlled by the extracted speaker embedding. The harmonics in Figure <ref type="figure" target="#fig_3">3</ref> (a) are clearly higher than that in Figure <ref type="figure" target="#fig_3">3 (c)</ref>, which indicates that the converted speech is not stable and speaker similarity is degraded. The harmonics maintain roughly the same between Figure <ref type="figure" target="#fig_3">3</ref> (b) and Figure <ref type="figure" target="#fig_3">3</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Subjective evaluation</head><p>For subjective evaluation, we first conduct AB and XAB preference tests to assess speech quality and speaker similarity. Then the mean opinion score (MOS) is utilized to evaluate speech naturalness. Each listener is asked to give an opinion score on a five-point scale (5: excellent, 4: good, 3: fair, 2: poor, 1: bad). For each system, 20 samples are randomly selected from the 160 converted samples for listening tests. Ten listeners participated in all listening tests. Different listeners may listen to different samples. Listening tests cover all the 160 evaluation samples.</p><p>The subjective results of AB tests are presented in Figure <ref type="figure" target="#fig_4">4</ref>. It is observed that our proposed VAE-DDSE significantly outperforms VAE-GSE and VAE-ORI.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the similarity preference results of XAB tests. As shown in Figure <ref type="figure" target="#fig_5">5</ref> (a) and (b), we observe that VAE-DDSE outperforms VAE-GSE and VAE-ORI respectively in terms of speaker similarity.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the mean opinion scores for different systems. Benefiting from the discriminative speaker embedding extracted from DDSE, VAE-DDSE achieves the highest MOS   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this study, we propose a deep discriminative speaker encoder to improve the robustness of one-shot voice conversion for unseen speakers. The speaker encoder integrates residual netand squeeze-and-excitation network to extract frame level speaker information from time-axis and channel-axis. Then attention mechanism is used to further focus on the speaker related information. Finally a statistic pooling layer is used to aggregate weighted frame level speaker information to form utterance level speaker embedding. The experimental results demonstrate that our proposed speaker encoder can improve the robustness of one-shot voice conversion for unseen speakers in terms of speech quality and speaker similarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FrameFigure 1 :</head><label>1</label><figDesc>Figure 1: The framework of the proposed deep speaker encoder for robust one-shot voice conversion. The speaker encoder consists of frame level feature processing and utterance level feature processing.</figDesc><graphic url="image-34.png" coords="3,58.59,368.99,224.62,64.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The diagram of robust one-shot voice conversion at run-time conversion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>n</head><label></label><figDesc>and X ref n are the n th coefficient of the converted and target mel spectrogram, N is the dimension of mel spectrogram. The lower MCD indicates the smaller distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of spectrum of the same utterance converted from clb (female) to bdl (male) by different systems: (a) VAE-ORI, (b) VAE-DDSE, and (c) Target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Quality preference tests of converted speech samples with 95% confidence intervals for different systems.</figDesc><graphic url="image-41.png" coords="4,313.99,186.99,223.62,50.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Similarity preference tests of converted speech samwith 95% confidence intervals for different systems.</figDesc><graphic url="image-42.png" coords="4,313.49,283.09,224.82,62.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of mean opinion scores with 95% confidence intervals for different systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of average MCD (dB) for different systems.</figDesc><table><row><cell>System</cell><cell>Inter</cell><cell>Intra</cell><cell>Average</cell></row><row><cell>VAE-ORI</cell><cell>13.02</cell><cell>12.85</cell><cell>12.93</cell></row><row><cell>VAE-GSE</cell><cell>15.66</cell><cell>15.52</cell><cell>15.59</cell></row><row><cell>VAE-ResNet</cell><cell>10.95</cell><cell>10.81</cell><cell>10.88</cell></row><row><cell>VAE-DDSE</cell><cell>10.81</cell><cell>10.70</cell><cell>10.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://dhqadg.github.io/robust/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of voice conversion systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="65" to="82" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Voice conversion using GMM with enhanced global variance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Benisty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Continuous probabilistic transform for voice conversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voice conversion based on maximum-likelihood estimation of spectral parameter trajectory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2222" to="2235" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voice conversion based on weighted frequency warping</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="922" to="931" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voice conversion using dynamic frequency warping with amplitude scaling, for parallel or nonparallel corpora</title>
		<author>
			<persName><forename type="first">E</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rosec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chonavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1313" to="1323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse representation for frequency warping based voice conversion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4235" to="4239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exemplar-based voice conversion in noisy environment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exemplar-based sparse representation with residual compensation for voice conversion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1506" to="1521" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An exemplarbased approach to frequency warping for voice conversion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1863" to="1876" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Voice conversion using deep bidirectional long short-term memory based recurrent neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4869" to="4873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voice conversion from non-parallel corpora using variational auto-encoder</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Parallel-data-free voice conversion using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11293</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accent and speaker disentanglement in manyto-many voice conversion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Chinese Spoken Language Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The NUS &amp; NWPU system for voice conversion challenge 2020</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="170" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voice conversion across arbitrary speakers based on a single targetspeaker utterance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="496" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Oneshot voice conversion with global speaker embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="669" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">One-shot voice conversion by separating speaker and content representations with instance normalization</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="664" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autovc: Zero-shot voice style transfer with only autoencoder loss</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5210" to="5219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How far are we from robust voice conversion: A survey</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-H. Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="514" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep discriminative embeddings for duration robust speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tencent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2262" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker adaptation in DNN-based speech synthesis using d-vectors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Braunschweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3404" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attentive statistics pooling for deep speaker embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koshinaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2252" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Many-to-many crosslingual voice conversion with a jointly trained speaker embedding network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1282" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep speaker embeddings for far-field speaker recognition on short utterances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Volokhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Andzhukaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Novoselov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lavrentyeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gazizullina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shulipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorlanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avdeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pekhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matveev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Speaker and Language Recognition Workshop</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-metric learning for text-independent speaker verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page" from="394" to="400" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep neural network embeddings for text-independent speaker verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="999" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jhuhltcoe system for the voxsrc speaker recognition challenge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7559" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The CMU arctic speech databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kominek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA workshop on speech synthesis</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The voice conversion challenge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1632" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning latent representations for speech generation and transformation</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04222</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
