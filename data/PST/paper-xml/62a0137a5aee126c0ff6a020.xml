<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_erYkBux">
					<orgName type="full">German Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-08">June 8, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alice</forename><surname>Moallemy-Oureh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Inteligent Embedded Systems</orgName>
								<orgName type="institution">University of Kassel</orgName>
								<address>
									<postCode>34121</postCode>
									<settlement>Kassel</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvia</forename><surname>Beddar-Wiesing</surname></persName>
							<email>s.beddarwiesing@uni-kassel.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Inteligent Embedded Systems</orgName>
								<orgName type="institution">University of Kassel</orgName>
								<address>
									<postCode>34121</postCode>
									<settlement>Kassel</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R?diger</forename><surname>Nather</surname></persName>
							<email>rnather@uni-kassel.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Inteligent Embedded Systems</orgName>
								<orgName type="institution">University of Kassel</orgName>
								<address>
									<postCode>34121</postCode>
									<settlement>Kassel</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josephine</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
							<email>jthomas@uni-kassel.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Inteligent Embedded Systems</orgName>
								<orgName type="institution">University of Kassel</orgName>
								<address>
									<postCode>34121</postCode>
									<settlement>Kassel</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-08">June 8, 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.03469v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic Graph Neural Network</term>
					<term>Dynamic Graphs</term>
					<term>Temporal Point Process</term>
					<term>Graph Attention</term>
					<term>Dynamic Graph Embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic Graph Neural Networks recently became more and more important as graphs from many scientific fields, ranging from mathematics, biology, social sciences, and physics to computer science, are dynamic by nature. While temporal changes (dynamics) play an essential role in many real-world applications, most of the models in the literature on Graph Neural Networks (GNN) process static graphs. The few GNN models on dynamic graphs only consider exceptional cases of dynamics, e.g., node attribute-dynamic graphs or structure-dynamic graphs limited to additions or changes to the graph's edges, etc. Therefore, we present a novel Fully Dynamic Graph Neural Network (FDGNN) that can handle fully-dynamic graphs in continuous time. The proposed method provides a node and an edge embedding that includes their activity to address added and deleted nodes or edges, and possible attributes. Furthermore, the embeddings specify Temporal Point Processes for each event to encode the distributions of the structure-and attribute-related incoming graph events. In addition, our model can be updated efficiently by considering single events for local retraining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have become baseline models for learning on structured data in the past years. Many models have been developed for static graph data that can include structural information into the learning process for different problems. However, they cannot capture the evolution of a dynamic graph <ref type="bibr" target="#b0">[1]</ref>, although many real-world problems require the representations of dynamic changes in the graph data. Furthermore, the GNNs that can handle dynamics are restricted to specific graph types. More specifically, they can either handle node attribute changes <ref type="bibr" target="#b1">[2]</ref>, are restricted to additions of nodes or edges (growing graphs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, or can only handle edge-structure dynamics and node attribute changes <ref type="bibr" target="#b4">[5]</ref>. Further examples are provided in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>To address the missing possibility of handling deletions of nodes and edges and changing attributes, we introduce a Fully Dynamic Graph Neural Network (FDGNN) that can handle node and edge additions and deletions and node and edge attribute changes (dynamic events). The approach of FDGNN is based on the model DyREP <ref type="bibr" target="#b3">[4]</ref>, which only handles edge-growing graphs. Thus, FDGNN is an extension of DyREP, including the possibility to process deletions of nodes and edges and node/edge attribute changes. FDGNN uses a node and edge activation function that captures the existence of all the nodes/edges in each timestamp. Furthermore, the node/edge embeddings use two attention mechanisms (self-and neighborhood attention) to gather historical and structural information and application-specific node/edge attribute embedding functions. A Temporal Point Process (TPP) <ref type="bibr" target="#b7">[8]</ref> is used for every dynamic event type that encodes the graph's dynamics to enable a continuous-time representation. With every incoming event, the model is updateable by local retraining, which makes the training procedure efficient.</p><p>In this paper, the theoretical development of the FDGNN is proposed. Since the implementation is still in progress, the experimental results and further optimizations will be available in a second updated version of the paper. The paper is structured as follows: In ?2 the preliminaries are introduced that cover the main definitions used in the paper and specify which modules are used to incorporate the structure dynamics with activity functions ( ?2.2), the attribute dynamics ( ?2.3) and the continuous-time representation via Temporal Point Processes ( ?2.4). For better readability we have included our conventions of notation in ?2.5. In ?3 the architecture of the proposed FDGNN is introduced which includes the attention mechanisms, the intensity functions for each event type, and the final embedding functions. The learning procedure for the model is based on <ref type="bibr" target="#b3">[4]</ref> and adapted to the model in ?4. The future work and further discussions can be found in ?5. Section ?6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>The proposed FDGNN is constructed to process dynamic graphs in continuous-time representation. For this purpose, it is required to define graph streams, where dynamic events operate on static graph snapshots. The advantage of such continuous-time representation is the ability to update the GNN model, i.e., the model gets updated with every new incoming event, and, therefore, no retraining from scratch is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Stream and Graph Snapshot</head><p>Given that a static attributed graph is represented as a tuple (V, E, ?, ?) where V is a set of nodes and E is a set of edges with attribute functions ? : V ? A, ? : E ? B, the following terms are defined accordingly. where v ? V ?t := t?[t0,t] V t are nodes appearing in G up to timestamp t, and e ? E ?t := t?[t0,t] E t are edges appearing in G up to timestamp t. Furthermore, ? t : V t ? A, ? t : E t ? B are node and edge attribute functions where A, B are arbitrary attribute sets, and t ? T ? R ?0 are timestamps (abbreviated as ? v t := ? t (v) in the event tuples).</p><formula xml:id="formula_0">Let G = (G 0 := (V 0 , E 0 , ? 0 , ? 0 ), O) be a dynamic</formula><p>A graph snapshot or graph slice G t = (V t , E t , ? t , ? t ) is defined to be the graph that results from G 0 incorporating all the observations within [t 0 , t].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Node and Edge Activities</head><p>Given that FDGNN should deal with structure-dynamics, the main idea for the node and edge activity function is to capture which nodes and edges exist at any given time. To not lose the information of deleted nodes or edges, the output of the activity function for a freshly deleted object is 0. The deletion history is important for the model to learn the deletion behavior of the graph.</p><p>Let for any timestamp t ? T and any node or edge x ? V ? E the timestamp t refer to the timestamp of an event affecting x right before t. Furthermore, let ot(x) be an event at time t that affects the node or edge x ? Vt ? Et, and ? : (V ? E) ? T ? {0, 1} be the activity function of nodes and edges at time t &gt; t defined as</p><formula xml:id="formula_1">? x t = 1, if ot(x) ? {o 0 , o 2 , o 4</formula><p>, o 5 } addition and attribute change events 0, if ot(x) ? {o 1 , o 3 } deletion events (inactivity) for all x ? Vt ? Et.</p><p>The stati of the nodes or edges indicated by the activity are used in the proposed model to determine in which situations a specific observation can be made, e.g., just existing nodes can be deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attribute Embeddings</head><p>To incorporate attribute dynamics the idea is to first consider a "node/edge-independent" attribute embedding as formalized in Eq. (1). Among other modules introduced later in this paper, the attribute embeddings are used to compute the final node and edge embeddings separately (cf. Eq. ( <ref type="formula" target="#formula_10">7</ref>), Eq. ( <ref type="formula" target="#formula_12">8</ref>)).</p><p>The attribute embedding encodes the dynamic behaviour of the node or edge attribute respectively by considering the historical transformed embedding vector ut(?) ? R ? and the current embedding vector Z ? t ? R z . For attribute ? ? A ? B at timestamp t ? T , the embedding u t (?) ? R ? in a ?-dimensional space is obtained recursively w.r.t. time by</p><formula xml:id="formula_2">u t (?) = M 0 ? ut(?) Z ? t + b u .<label>(1)</label></formula><p>The parameters M 0 ? R ??(?+z) and b 0 ? R ? are the learnable weight matrix and bias in the FDGNN. The current embedding vector Z ? t is the output from application-specific chosen attribute embedding functions, e.g., CNN's for images, text2vec for text <ref type="bibr" target="#b8">[9]</ref> etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Temporal Point Process</head><p>A set of observations is given in form of a temporal point pattern. Given the set of observations we cannot tell at what time and in which amount an observation will occur. Especially when it comes to real-world applications, the occurrence of the different events can follow complex rules and are not necessarily fully random (e.g., an already deleted node cannot be deleted again, or a non-existing edge cannot change its attributes). Assuming that an observation is dependent on the historical point patterns, the underlying process can be modelled by a temporal point process (TPP) <ref type="bibr" target="#b7">[8]</ref>. The term point implies that events only occur at a certain timestamp and thus pose a data point in a time series. Furthermore, there may be additional information given for the events, as for example the event types k ? {0, . . . , 5} in the definition above, which are called marks in TPP literature. Therefore, only marked TPPs are considered in this work and simply denoted as TPPs. In addition, we assume here that at each timestamp only one event can occur, which makes the TPP simple. The consideration of simultaneously occurring events is left for future work.</p><p>To model a (marked) temporal point process and to specify the dependency of a present observation at time t on the history of events H tn for any t &gt; t n , a conditional intensity function can be used. Let f (t|H tn ) be the the conditional density function and F (t|H tn ) be its corresponding cumulative distribution function for any t &gt; t n .</p><p>For the marked case, let f (k|t) = f (k|t, Ht) be the conditional density function<ref type="foot" target="#foot_0">1</ref> of event type k, i.e., specifying the distribution of the event type k given t and the history Ht, which now includes information of both times and marks of past events. Then, the (marked) conditional intensity function is defined in <ref type="bibr" target="#b7">[8]</ref> by</p><formula xml:id="formula_3">?(k, t) = ?(t)f (k|t) = f (t, k|H tn ) 1 -F (t|H tn ) ,<label>(2)</label></formula><p>where f (t, k|H tn ) is the joint density of t and the k, and F (t|H tn ) is the conditional cumulative distribution function of t, both conditional on the past times and event types. ?(t) corresponds to the unmarked conditional intensity function.</p><p>Assuming that no events can occur simultaneously, the marked conditional intensity function can be heuristically interpreted as the expected number of events occurring at a timestamp given the historical information:</p><formula xml:id="formula_4">?(t, k)dt = E[N ([t, t + ?t] ? k)|H t ],<label>(3)</label></formula><p>with a function N ([t, t+?t] ? k) giving the number of events of type k occurring in the infinitisemal interval [t, t + ?t] with a time difference ?t &gt; 0. Here, the events are discrete and contain events such as addition/deletion of nodes/edges in 1.</p><p>Note that, for events where node or edge attributes are changing (k = 4, 5), two tasks are possible. On the one hand, considering Eq. (3) the events cover if an attribute is changing, or not. If the attribute sets A and B are metric spaces the following approach allows modelling the change of an attribute as mark m during an event o 4 , o 5 by</p><formula xml:id="formula_5">?(t, m)dt = E[N ([t, t+?t] ? B ?m (m))|H t ],</formula><p>where [t, t+?t] is as above and B ?m (m) is an open ball of radius ?m around m in the respective attribute spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Notation</head><p>The notation used throughout this work is listed in the following.</p><formula xml:id="formula_6">N 0 natural numbers starting at 0 R ?0 non-negative real numbers R k R vector space of dimension k |M | number of elements of a set M M ? matrix M transposed [i, j] closed interval from i to j ? subset ?</formula><p>factor set of two sets ? sigmoid activation function</p><p>In general, we use bold capital letters for matrices, such as A, bold small type letters for vectors such as a and small type normal letters for scalars or nodes or vectors such as a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section the individual parts of the model are introduced, i.e., the self attention and neighborhood attention modules ( ?3.1), the embedding function for attributes, nodes and edges ( ?3.2) and the intensity functions for each event-type (add/delete node/edge and change node/edge attribute, cf. ?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3).</head><p>In what follows, let for any timestamp t ? T and any node or edge x ? V ? E the timestamp t refer to the timestamp of an event affecting x right before t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Mechanisms</head><p>There are two attention mechanisms defined in the following: self-attention and neighborhood attention. These attentions are used for the embedding computation in Section 3.2 via weighting the historical information of a node/edge and the neighborhood information in the update, and therefore to improve the explainability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>The self-attention takes for each node/edge its historical behavior into account by using the activity status of the current timestamp t and the one right beforehand t. Furthermore, a similarity between the attributes of the two timestamps is considered. Moreover, the attention is normalized by the attentions of all known timestamps before.</p><p>The self-attention is defined for all x ? (V t ? Vt) ? (E t ? Et) as</p><formula xml:id="formula_7">p x t = exp ? x t ? ? x t ? ? x t ti?T exp(? x ti ? ? x ti ? ? x ti )<label>(4)</label></formula><p>using a similarity function between the two attribute embeddings u from Eq. (1) given by</p><formula xml:id="formula_8">? x t = sim(u t (? x t ), ut(? x t )), if x ? V t ? Vt sim(u t (? x t ), ut(? x t )), if x ? E t ? Et.<label>(5)</label></formula><p>for x being a node or an edge respectively. The similarity function sim can be an arbitrary similarity measure, as, e.g., the cosine similarity used in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Neighborhood Attention The neighborhood attention takes for each node/edge the activity of its neighborhood from the previous timestamp into account. It respects the active nodes and edges within its direct neighborhood and the corresponding edge attributes. With a similarity measure used on edge attributes, the focus is set on similar attributes.</p><p>Let u i ? N (v) := {u | ? e ? E : u, v ? e} for i = 0, . . . , |N (v)| be the neighbors of node v. The attention for a node v and a neighbor u i is determined by their activities and the activity of the connecting edge, together with a similarity measure between the two node attribute embeddings<ref type="foot" target="#foot_1">2</ref> , as in <ref type="bibr" target="#b4">(5)</ref>. Then, the attention is normalized by the attentions of all neighbors of v and is determined by</p><formula xml:id="formula_9">q {v,ui} t = exp ? v t ? ? ui t ? ? {ui,v} t ? ? {ui ,v} t m?N (v) exp(? m t ? ? v t ? ? {m,v} t ? ? {m,v} t ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embeddings</head><p>To incorporate attribute dynamics the idea is to first consider a "node/edge-independent" attribute embedding (cf. Eq. ( <ref type="formula" target="#formula_2">1</ref>)) and compute the node/edge embeddings (cf. Eq. ( <ref type="formula" target="#formula_10">7</ref>), Eq. ( <ref type="formula" target="#formula_12">8</ref>)) using the attribute embeddings, the local neighborhood information and attentions according to Eq. ( <ref type="formula" target="#formula_9">6</ref>) and Eq. ( <ref type="formula" target="#formula_7">4</ref>). In Section ?3.3 the intensity functions of the node and edge embeddings are processed to obtain models for the temporal behavior of the graph events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Embeddings</head><p>The node embedding consists of four additive components. The first is the local embedding propagation scheme that cumulates the neighborhood node embeddings and attributes embeddings. The second summand describes a self-propagation component that propagates the node's embedding from the timestamp before being multiplied by a self-attention factor. In the third component, the exogenous drive includes the time difference between the timestamps of the current event and the one beforehand that takes temporal dependencies into account. Moreover, the last component includes the attribute embedding of the node. Summarized, this gives the node embedding determined by</p><formula xml:id="formula_10">Z v (t) = ? ? ? ?M1hloc(v, t) loc. embd. prop. + M 2 Z v t ? p v t self-prop. + m 3 ? (t -t) exogenous drive + M 4 ? u t (? v t ) attribute prop. ? ? ? .<label>(7)</label></formula><p>The used local embedding propagation scheme is given by</p><formula xml:id="formula_11">h loc (v, t) = u?N (v) ? ? ? m u 5 Z u t neighborhood cum. + m u 6 u t (? u t ) neighborhood attr. cum. ? ? ? ? q {v,u} t</formula><p>, where m u 5 , m u 6 ? R are learnable weights and p v t , q {v,u} t are the attentions according to Eq. ( <ref type="formula" target="#formula_7">4</ref>) and <ref type="bibr" target="#b5">(6)</ref>.</p><p>Edge Embeddings The edge embedding is similar to the node embedding. The only substantial difference is the choice of the local embedding propagation scheme, which includes the embeddings of the incident nodes, their attributes, and the embedding of the edge attribute. In total, the edge embedding is defined by</p><formula xml:id="formula_12">Z u,v (t) = ? ? ? ?M7hloc(u, v, t) loc. embd. prop. + M 8 Z u,v t ? p {u,v} t self-prop. + m 9 ? (t -t) exogenous drive + M 10 ? u t (? u,v t ) attribute prop. ? ? ?<label>(8)</label></formula><p>with the local embedding propagation scheme</p><formula xml:id="formula_13">h loc (u, v, t) = m 11 Z u t p u t + m 12 Z v t p v t incident nodes cum. + m 13 u t (? u t )p u t + m 14 u t (? v t )p v t inc. node attr. cum.</formula><p>, where m 11 , m 12 , m 13 , m 14 ? R are learnable weights and p u t , p v t are the attentions according to Eq. ( <ref type="formula" target="#formula_7">4</ref>).</p><p>Remark In case that the activity function of a node or edge x is undefined up to a timestamp t, the initial embedding only includes the attribute propagation and the local embedding propagation at time t, i.e.,</p><formula xml:id="formula_14">Z x (t) = ? ? ? ?M ? h loc (x, t) loc. embd. prop. + M ?? ? u t (? x t ) attribute prop. ? ? ? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intensity Functions/Point Processes of the Events</head><p>This section defines the intensity functions that model the temporal behavior of the dynamic events occurring on the graph. For this purpose, consider the event types o k ? O defined in Tab. <ref type="bibr" target="#b0">(1)</ref>. Such events can be interpreted as Temporal Point Processes (TPP) that are formalized through an intensity function in the following. For each event type o k there is a separate intensity function ? x k (t) in use for x ? V t ? E t dependent on a timestamp t. The intensity functions are determined for active nodes and edges, while they are constant 0 if the event can not occur. Thus, each intensity function models the number of events of a certain type occurring at time t as a function of the graph status at the timestamp t of the event right before an event at time t.</p><p>Let f k be an activation function, Z x t an attribute embedding of a node or edge x at time t and the matrices W ? be learnable weights. Then, the intensity functions are defined as follows.</p><p>Add Node Additions of nodes at timestamp t are possible if and only if the nodes did not exist right before at timestamp t. Therefore, the activity of an existing node is 1 and thus the intensity function of node additions is 0, i.e.,</p><formula xml:id="formula_15">? v t = 1 ?? ? v 0 (t) = 0.</formula><p>Otherwise, the intensity function uses a function of the form W ? 0 Z v t that assigns a score to a node attribute considering the node attribute embedding. Hence, ? v 0 (t) is defined as</p><formula xml:id="formula_16">? v 0 (t) = ? ? ? f 0 ( W ? 0 Z v t scores attribute ), if (? v t = 0 ? ? v t = ?) 0, if (? v t = 1).</formula><p>Delete Node For the opposite event, delete node, it is important to switch the cases, i.e., a node can just be deleted at timestamp t if and only if it existed before in timestamp t. The corresponding intensity function is given as</p><formula xml:id="formula_17">? v 1 (t) = ? ? ? f 1 ( W ? 1 Z v t scores attribute ), if (? v t = 1) 0, if (? v t = 0).</formula><p>Add Edge Analogously to the addition of nodes, an edge e can only be added when it did not exist the timestamp before and, additionally, both nodes u, v ? e exist. The inner function of the intensity scores the node and edge attributes, respectively.</p><formula xml:id="formula_18">? e 2 (t) = ? ? ? ? ? ? ? ? ? ? ? ? ? f 2 ? ? W ? 2 ? ? Z u t Z v t Z e t ? ? ? ? scores node &amp; edge attributes , if (? e t = 0 ? ? u t = 1 ? ? v t = 1 ? ? e t = ?) 0, if (? e t = 1 ? ? u t = 0 ? ? v t = 0)</formula><p>Delete Edge The deletion of an edge e with nodes u, v ? e will definitely take place, when the edge existed at time t, and at least one of the incident nodes has been deleted. Thus, when the edge does not exist at time t, it cannot be deleted, i.e., ? e</p><formula xml:id="formula_19">3 (t) = 1 ?? ? (u,v) t = 1 ? (? u t = 0 ? ? v t = 0).</formula><p>Otherwise, the inner function of the intensity again scores the node and edge attributes. All in all, the intensity function to delete edges ? 3 is defined by</p><formula xml:id="formula_20">? e 3 (t) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? f 3 ? ? W ? 3 ? ? Z u t Z v t Z e t ? ? ? ? scores node &amp; edge attributes , if (? e t = 1 ? ? u t = 1 ? ? v t = 1) 1, if ? e t = 1 ? (? u t = 0 ? ? v t = 0) 0, if (? e t = 0).</formula><p>Change Node Attributes The attributes of nodes can only change if and only if the node whose attributes are in consideration exists. In this case a scoring</p><formula xml:id="formula_21">W ? 4 Z v t Z v t</formula><p>is needed that takes the node's attribute change into account.</p><formula xml:id="formula_22">? v 4 (t) = ? ? ? ? ? ? ? ? ? f 4 W ? 4 Z v t Z v t scores attribute change , if (? v t = 1) 0, if (? v t = 0).</formula><p>Change Edge Attributes For a node change event, the edge attribute change requires the existence of an edge at t is not sufficient. Given that an edge e ? E cannot exist without the incident nodes u, v ? e, the activity status of both the edge and its incident nodes is required to be 1. First, it is essential to score the events between nodes to ensure that something is happening on the edge between them. Second, if there is an event happening on the edge, the edge attributes change is scored. Altogether, the intensity function is defined by</p><formula xml:id="formula_23">? u,v 5 (t) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? f 5 ? ? ? ? ? ? ? W ? 5 ? ? Z u t Z v t Z e t ? ?</formula><p>scores events between u,v</p><formula xml:id="formula_24">+ ? ? 5 Z e t Z e t scores edge attribute change ? ? ? ? ? ? ? , if (? e t = 1 ? ? u t = 1 ? ? v t = 1) 0, if (? e t = 0 ? ? u t = 0 ? ? v t = 0).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Procedure</head><p>Depending on the application and the specific dynamic graph dataset, there are different suitable learning procedures, and only the necessary intensity functions have to be included in the learning. However, if every dynamic event is present in the observation set O, all the intensity functions have to be used for training and updating the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>The training procedure is done analogously to the training described in <ref type="bibr" target="#b3">[4]</ref>. Let ? k be the intensity function corresponding to the event o k ? O and x corresponds to the node or edge referred to in o k . Then the model parameters including the leaarnable weights M ? , m ? , and W ? from Sec. 3 are learnt by minimizing the negative log likelihood</p><formula xml:id="formula_25">L := - o k ?O log (? x k (t)) + T 0 ?(?) d?,<label>(9)</label></formula><p>where</p><formula xml:id="formula_26">?(?) = x?V ? ?E ? 5 k=0 ? x k (?)</formula><p>represents the total probability for all events including also those that did not occur in the current timestamp (which is referred to as survival probability).</p><p>To keep the training effort low and not to optimize L w.r.t. all events at once, mini-batch stochastic gradient descent can be utilized together with sampling techniques to approximate the integral <ref type="foot" target="#foot_2">3</ref> . Furthermore, the training is conducted in the Back Propagation Through Time (BPTT) manner. Assuming that several intensity functions share the weights independently from each other and can thus be computed in parallel, the training can be further accelerated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Update</head><p>The model can be updated with the aid of an event by applying the mini-batch stochastic gradient descent on the negative log likelihood (9) by only considering the event and the affected nodes or edges. This local retraining approach minimizes the update effort as only a small subset of the parameters from the intensity functions have to be touched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>The proposed FDGNN model provides opportunities for further development. In the following, the model's generativity, explainability, and generalization are briefly discussed.</p><p>Generative Attribute Embedding When it comes to the forecast of new nodes or edges and their attributes, the attribute embedding in Eq. ( <ref type="formula" target="#formula_2">1</ref>) has to be generative. Therefore, a generative model has to be selected concerning the application carefully.</p><p>Explainability GNNs are called explainable if the model outputs an explanation for the predicted result or reasoning that can be inferred based on the model architecture. In the future, the goal is to make the fully dynamic model more explainable to simplify the application of the model for real-world problems. For this purpose, it is necessary to include more components in the architecture that improve explanations for the model output considering the input graphs. For the beginning, the attention mechanisms (cf. ?3.1) allow for a better understanding of the model's behavior.</p><p>Definite Deletions vs. Inactivity Furthermore, the activity function from Section 2.2 will be extended to the consideration of definite deletions, in addition to activities and inactivities of nodes and edges. Thereby, nodes and edges lose the possibility to occur again in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More complex graph types</head><p>The structural graph properties of the dynamic graphs in this paper are rather elementary so that the FDGNN can be extended to more complex dynamical graph structures in the future. Given the results in <ref type="bibr" target="#b10">[11]</ref>, every graph type can be transformed into another graph type without any loss of information. The fact that the FDGNN model can handle undirected attributed dynamic graphs, together with the transformation costs of graph types <ref type="bibr" target="#b10">[11]</ref> advocates for the transformation of any more complex graph types into an undirected attributed graph, due to memory savings, with subsequent application of FDGNN. However, the question remains plausible if a proper extension, e.g., to dynamic hypergraphs, would be more efficient than applying FDGNN after a graph transformation.</p><p>A realization of such could be done by using the incidence matrix (instead of the adjacency matrix) and extending the intensity functions in ?3.3 to more than two involved nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a Fully Dynamic Graph Neural Network (FDGNN) that can handle structure-and attribute dynamics. I.e., node and edge attributes can change over time, and node and edge additions and deletions are supported. The approach of FDGNN is based on the model DyRep <ref type="bibr" target="#b3">[4]</ref>, which only handles edge-growing graphs. The proposed model can thus be seen as an extension of DyRep to other dynamic graph properties.</p><p>FDGNN models the temporal behavior of the structural dynamics of graphs with the aid of an activity function and temporal point processes formalized by intensity functions. To incorporate the attribute dynamics, different intensities are defined that catch the dynamics of the node and edge attributes. The constructed node and edge embeddings used for the intensities include the local neighborhood and historical information, the exogenous drive, and the corresponding attributes. Furthermore, the utilization of attention mechanisms for the neighborhood and the historical information provides the first step towards an explainable model.</p><p>Assuming that several intensities share their weights and others are computationally unrelated, the computation can be done in parallel, and the learning phase can be reduced. Furthermore, the update of the model is efficient due to using only one event and the corresponding nodes or edges for local retraining. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Contributions?</head><label></label><figDesc>Conceptualization: SBW, AMO ? Methodology: SBW, AMO, RN ? Resources: AMO ? Writing (Original Draft): SBW, AMO ? Writing (Review &amp; Editing): SBW, AMO, RN ? Supervision: RN, JT ? Project administration: SBW, AMO ? Funding acquisition: JT Alice Moallemy-Oureh is mainly responsible for the incorporation of attribute-dynamics, while Silvia Beddar-Wiesing is mainly responsible for modeling the structure-dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>graph in continuous-time representation (graph stream), with static start graph G 0 and o k ? O being events/observations of the following form: Events in a continuous-time representation of a graph.</figDesc><table><row><cell>Event Type</cell><cell>Node Event</cell><cell>Edge Event</cell></row><row><cell>Addition</cell><cell>o</cell><cell></cell></row></table><note><p><p>0 := (v, ? v t ,</p>t) add o 2 := (e, ? e t , t) add Deletion o 1 := (v, ? v t , t) del o 3 := (e, ? e t , t) del Attribute Change o 4 := (v, ? v t , t) atr o 5 := (e, ? e t , t) atr</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In general, if k is a continuous random variable, this is the usual (conditional) density function, but if it is a discrete random variable, this is its (conditional) probability function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In case of knowledge about the edge attributes an additional similarity factor can be added that respects the edge attribute similarity between the incident edges of v.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For example, Monte Carlo approximation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The project is funded by the <rs type="funder">German Ministry of Education and Research (BMBF)</rs> with the grant number <rs type="grantNumber">01IS20047A</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_erYkBux">
					<idno type="grant-number">01IS20047A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Representation Learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -25th International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Long</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chi-Sing</forename><surname>Leung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Seiichi</forename><surname>Ozawa</surname></persName>
		</editor>
		<meeting><address><addrLine>Siem Reap, Cambodia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-12-13">2018. December 13-16, 2018. 2018</date>
			<biblScope unit="volume">11301</biblScope>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dyrep: Learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjeet</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning attribute-structure co-evolutions in dynamic graphs</title>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR, abs/2007.13004</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Graph neural networks designed for different graph types: A survey</title>
		<author>
			<persName><forename type="first">Josephine</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Moallemy-Oureh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Beddar-Wiesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Holzh?ter</surname></persName>
		</author>
		<idno>CoRR, abs/2204.03080</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation Learning for Dynamic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Gulddahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmussen</forename></persName>
		</author>
		<idno>CoRR, abs/1806.00221</idno>
		<title level="m">Lecture Notes: Temporal Point Processes and the Conditional Intensity Function</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Package &apos;text2vec</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Selivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep linear graph attention model for attributed graph clustering</title>
		<author>
			<persName><forename type="first">Huifa</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengdong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page">108665</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph type expressivity and transformations</title>
		<author>
			<persName><forename type="first">Josephine</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Beddar-Wiesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Moallemy-Oureh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?diger</forename><surname>Nather</surname></persName>
		</author>
		<idno>CoRR, abs/2109.10708</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
