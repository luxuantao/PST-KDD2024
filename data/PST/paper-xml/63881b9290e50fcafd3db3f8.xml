<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PEX: MEMORY-EFFICIENT MICROCONTROLLER DEEP LEARNING THROUGH PARTIAL EXECUTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-30">30 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
						</author>
						<title level="a" type="main">PEX: MEMORY-EFFICIENT MICROCONTROLLER DEEP LEARNING THROUGH PARTIAL EXECUTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-30">30 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.17246v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedded and IoT devices, largely powered by microcontroller units (MCUs), could be made more intelligent by leveraging on-device deep learning. One of the main challenges of neural network inference on an MCU is the extremely limited amount of read-write on-chip memory (SRAM, &lt; 512 kB). SRAM is consumed by the neural network layer (operator) input and output buffers, which, traditionally, must be in memory (materialised) for an operator to execute. We discuss a novel execution paradigm for microcontroller deep learning, which modifies the execution of neural networks to avoid materialising full buffers in memory, drastically reducing SRAM usage with no computation overhead. This is achieved by exploiting the properties of operators, which can consume/produce a fraction of their input/output at a time. We describe a partial execution compiler, PEX, which produces memory-efficient execution schedules automatically by identifying subgraphs of operators whose execution can be split along the feature/"channel" dimension. Memory usage is reduced further by targeting memory bottlenecks with structured pruning, leading to the co-design of the network architecture and its execution schedule. Our evaluation of image and audio classification models: (a) establishes state-of-the-art performance in low SRAM usage regimes for considered tasks with up to +2.9% accuracy increase; (b) finds that a 4? memory reduction is possible by applying partial execution alone, or up to 10.5? when using the compiler-pruning co-design, while maintaining the classification accuracy compared to prior work; (c) uses the recovered SRAM to process higher resolution inputs instead, increasing accuracy by up to +3.9% on Visual Wake Words. Pre-print. Do not redistribute. MobileNet-v2 layers, in order of execution 0 500 1000 1500 Memory usage, KB 4.0? 5.5? Ordinary execution (1505 kB) Partial execution, P x (376 kB) P x-pruning co-design (276 kB)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The low cost and versatility of microcontroller platforms have made them an attractive choice for a wide range of applications: an estimated 29B units shipped in 2021 <ref type="bibr">(Grand View Research, 2022)</ref>. This includes numerous embedded, personal and IoT devices, many of which could be enhanced with computational intelligence brought by deep learning.</p><p>A microcontroller unit (MCU) is a single chip mainly consisting of a power-efficient CPU, read-only Flash memory and read-write static RAM (SRAM). Table <ref type="table" target="#tab_1">1</ref> compares MCUs along compute, memory and storage capacity axes with GPU server and mobile hardware capable of executing neural networks. The low price and power usage of MCUs come with a marked downgrade in the computational ability: memory capacity lies orders of magnitude behind the nextbest hardware, and there is little-to-no memory hierarchy and parallelism to exploit for optimisation. This presents a significant challenge to deep learning inference on the device itself, causing applications to rely on fully remote or Figure <ref type="figure">1</ref>. Per-layer memory usage of MobileNet-v2. PEX automatically creates partial execution schedules that reduce peak memory usage of inference on microcontroller platforms. Memory is reclaimed at no extra computational cost, and the architecture can be pruned to reduce the memory bottleneck identified by PEX. Here, memory usage is reduced by 4?, or 5.5? with pruning co-design. hybrid deployments <ref type="bibr" target="#b0">(Almeida et al., 2021)</ref>, which results in sacrifices in data privacy and autonomy.</p><p>Enabling on-device neural network inference has been shown to require specialised research into low-footprint deep learning. TinyML <ref type="bibr" target="#b30">(Warden &amp; Situnayake, 2020)</ref> is an active research direction, which encompasses hardware improvements <ref type="bibr" target="#b23">(Sadiq et al., 2022)</ref>; systems-level software research, such as runtimes <ref type="bibr" target="#b7">(David et al., 2020)</ref>, layer imple-  Execution transitions between partially-and fully-materialised tensors at loop boundaries; the schedule can contain multiple loops. mentations <ref type="bibr" target="#b16">(Lai et al., 2018)</ref> and model compilers <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>; model discovery, incl. compression and neural architecture search (NAS) <ref type="bibr">(Liberis &amp; Lane, 2021;</ref><ref type="bibr">Liberis et al., 2021;</ref><ref type="bibr" target="#b3">Banbury et al., 2020;</ref><ref type="bibr" target="#b20">Lin et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Memory usage of neural networks on MCUs</head><p>In this work, we focus on the issue of high memory (SRAM) usage of neural network inference, which prevents the deployment of models with large activation tensors.</p><p>A neural network is defined as a computational graph of layers, called operators, with data dependencies between them.</p><p>Most MCU deep learning runtimes and kernel implementations <ref type="bibr" target="#b5">(Chen et al., 2018;</ref><ref type="bibr" target="#b7">David et al., 2020;</ref><ref type="bibr" target="#b16">Lai et al., 2018)</ref>, have adopted a one-operator-at-a-time execution regime.</p><p>For each layer/operator in the network, the runtime:</p><p>1. Allocates the output buffer for the operator in SRAM;</p><p>2. Executes the operator, using inputs read from SRAM and parameters read from the separate Flash storage; 3. Marks the operator's inputs' buffers as available memory (i.e. deallocates from SRAM, may be implicit), if the buffers are not used later.</p><p>Under this execution regime, called ordinary execution, the memory bottleneck is the size of the largest operator's working set (input and output buffers) along with any other tensors that must be retained for subsequent operators. This bottleneck must lie within the SRAM capacity constraints.</p><p>Some systems-level model-agnostic methods have been developed to reduce SRAM usage of neural networks by optimising buffer layout in memory, such as buffer binpacking <ref type="bibr" target="#b7">(David et al., 2020)</ref> and operator reordering <ref type="bibr" target="#b17">(Liberis &amp; Lane, 2019)</ref>. SRAM usage is also an optimisation objective in MCU-compatible model discovery methods <ref type="bibr" target="#b11">(Fedorov et al., 2019;</ref><ref type="bibr">Liberis &amp; Lane, 2021;</ref><ref type="bibr" target="#b3">Banbury et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">PEX: Partial execution for memory efficiency</head><p>A core contribution of this paper is a novel model compiler, called PEX, which realises further gains in memory usage by moving beyond the paradigm of executing one operator at a time and instead executing operators partially to produce and/or consume one feature/channel of data at a time. The idea of partial execution can be applied to arbitrary neural networks. A particular instance of this was first described in MobileNet-v2 <ref type="bibr" target="#b25">(Sandler et al., 2018)</ref> as a memory-efficient trick to execute the inverted residual block (IRB). Figure <ref type="figure" target="#fig_0">2</ref> uses this example to illustrate the concept of partial execution, both diagrammatically and in pseudo-code.</p><p>Under ordinary execution, the IRB memory bottleneck resides at the operator C : the bottleneck consists of the input and output buffers of C , as well as output of A retained</p><p>for operator E , amounting to 52.7 KB (at 8-bit precision).</p><p>However, operators B , C , and D can be combined along the channel axis. In a partial execution loop, for each of the 144 channels, operator B (conv.) produces one output channel, operator C (depthwise conv.) transforms it, and operator D (conv.) consumes it and writes its contribution to its output buffer. The memory bottleneck is within the loop at operator C , amounting to 8.5 KB (at 8-bit precision).</p><p>It consists of the input to B , one channel output of B (input to C ), one channel output of C , and the full output buffer of D , preallocated before the start of the loop.</p><p>Such execution has largely become obsolete on mobile platforms due to the abundance of RAM (Table <ref type="table" target="#tab_1">1</ref>) and an inferior data access pattern. However, on MCUs data access pattern is of little concern due to the lack of data caches.</p><p>Neural networks inspired by the IRB continue to proliferate <ref type="bibr" target="#b13">(Howard et al., 2019;</ref><ref type="bibr" target="#b4">Cai et al., 2018;</ref><ref type="bibr" target="#b27">Tan &amp; Le, 2019)</ref>, so discovering analogous memory-saving execution schedules for new architectures would greatly benefit deep learning for MCUs by reducing the SRAM footprint of inference.</p><p>Automatic partial execution with PEX. We devise abstractions and execution rules required to apply partial execution to any network layer layout, and automate the scheduling decisions using a dynamic programming-based algorithm. These form the core of PEX, which discovers computationally-equivalent, yet more memory-efficient network execution schedules for arbitrary neural networks.</p><p>Figure <ref type="figure">1</ref> shows the per-layer memory usage profile of the MobileNet-v2 architecture, showing that partial execution enables a 4? peak memory usage reduction. However, partial execution alone cannot reduce peak memory usage to arbitrarily low values: a particular layer arrangement at the memory bottleneck may not lend itself to optimisation, or the sizes of buffers allocated at the start of the partial execution loop may cause the memory bottleneck.</p><p>Compiler+pruning network co-design. We propose using structured pruning to change the network architecture automatically to reduce peak memory usage under partial execution. Structured pruning entirely removes the least important feature maps (e.g. convolutional channels), and we instruct it to target memory bottlenecks identified by the compiler. PEX is repeatedly invoked during pruning, with new memory bottlenecks reported back to the pruning algorithm at each step. Conversely, removing feature maps changes loop sizes and execution instructions produced by the compiler. Therefore, the two work in tandem to codesign the network architecture and the execution schedule.</p><p>We discuss the place of our methodology within related microcontroller deep learning work in Section 2, give the algorithm details in Section 3, including integration with pruning and considerations around quantisation, and per-form quantitative comparisons in Section 4.</p><p>Overall, the contributions of this work are as follows:</p><p>? We devise abstractions and execution rules for arbitrary networks, which enable partial execution. ? We devise a model compiler, PEX, which automatically creates a partial execution schedule while minimising peak memory usage, unlocking significant memory usage reduction compared to ordinary evaluation. ? We leverage partial peak memory usage as an optimisation goal within structured pruning, reducing the model's memory footprint further automatically. ? We reduce memory usage by up to 10.5? and establish new state-of-the-art models for three classification tasks in the low SRAM usage regime, improving accuracy by up to 2.9%, compared to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning on microcontrollers</head><p>Prior work in microcontroller deep learning typically assumes an ordinary execution-based runtime, such as TFLM <ref type="bibr" target="#b7">(David et al., 2020)</ref>. MCU-friendly resource footprints are achieved by: manual architecture design <ref type="bibr" target="#b33">(Zhang et al., 2017;</ref><ref type="bibr" target="#b22">Mocerino &amp; Calimera, 2019)</ref>, quantisation <ref type="bibr" target="#b15">(Jacob et al., 2018)</ref>, network pruning <ref type="bibr">(Liberis &amp; Lane, 2021)</ref> or neural architecture search (NAS) <ref type="bibr" target="#b20">(Lin et al., 2020;</ref><ref type="bibr" target="#b3">Banbury et al., 2020)</ref>. Some methods target inference with sparse matrices <ref type="bibr" target="#b12">(Fedorov et al., 2022;</ref><ref type="bibr">2019)</ref>, or use pruning as a nested step within NAS <ref type="bibr">(Liberis et al., 2021)</ref>. Prior discovered models can operate within the partial execution regime.</p><p>Standardised runtimes are an important common ground for hardware, model compression and network design research. However, prior work shows that performance benefits can be reaped by moving beyond standard assumptions. TVM <ref type="bibr" target="#b5">(Chen et al., 2018</ref>) is a general model compiler that can perform both computation graph-and inference codelevel optimisations and produce binaries for various targets, including microcontroller platforms. MCUNet-v2 <ref type="bibr" target="#b21">(Lin et al., 2021)</ref> forgoes layer-by-layer execution in favour of executing memory-intense layers using one input tile ("patch") at a time; we discuss this execution style further on.</p><p>Alternatively, computational constraints can be relaxed by introducing memory hierarchy. <ref type="bibr" target="#b26">Svoboda et al. (2022)</ref> show that a microSD card can be used to offload weights and activations of a neural network, which also significantly increases latency and power usage due to data transfer and the presence of external storage. More promisingly, <ref type="bibr" target="#b23">Sadiq et al. (2022)</ref> show that a smaller (&lt; 8 MB) SDRAM and external Flash memory can be introduced to offload selected parameters and activation matrices transparently using DMA. This allows approaching the latency of on-chip memory-only inference, but at the cost of additional power usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computation split approaches</head><p>The computation of neural network layers' outputs can also be split along their spatial axes instead of (or in addition to) the channel axis, resulting in patch-based execution.</p><p>Computation split approaches for memory-constrained microcontroller platforms overlap, to some degree, with the techniques developed for model parallelism. Conceptually, the two optimise for different scales: model parallelism maximises utilisation, cache hits, or reduces the multi-gigabyte memory (vRAM) footprint or communication overhead on multi-core GPU/TPU hardware for neural network training; here, we split computation to save 100s KB of SRAM for inference on relatively simplistic single-core hardware, but in the presence of quantisation and greater sensitivity to computation overheads (strict compute budget).</p><p>The problems can be approached similarly: both develop frameworks to reason about neural networks, which identify independent computations and optimise their execution schedule. <ref type="bibr">Dryden et al. (2019a)</ref> parallelise execution over the sample and spatial dimensions (i.e. patches) and optimise workload assignment to processors under a benchmarkinformed cost model using a shortest-path graph algorithm; this methodology is also updated to consider channel and filter dimensions of a CNN <ref type="bibr">(Dryden et al., 2019b)</ref>; <ref type="bibr">Xu et al. (2022b)</ref> combine patch-based execution with gradient checkpointing to reduce vRAM usage for high-resolution inputs; <ref type="bibr" target="#b14">Ivanov et al. (2021)</ref> analyse data flow to minimise data movement in Transformer models by identifying data layout and operator fusion improvements. We are optimistic about any future methodology cross-over.</p><p>In deep learning for MCUs, patch-based execution has been explored by <ref type="bibr">MCUNet-v2 (Lin et al., 2021</ref>). An analogous idea has been previously explored by <ref type="bibr" target="#b1">Alwani et al. (2016)</ref> for avoiding off-chip memory access. Only a spatial segment (a "patch") of a layer's output is computed at a time (thus full output is never materialised in memory), before being consumed by subsequent layers. This can greatly reduce peak memory usage, but comes at the cost of computation overhead due to the re-computing of intermediate values that are required for more than one patch (edge overlap). The amount of extra computation varies per architecture and depends on convolutional kernel size and strides. Extended qualitative discussion of this alternative/orthogonal approach is in Appendix B. As it is the closest relevant prior work, we perform a quantitative comparison in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN OF PEX</head><p>PEX is a compiler for arbitrary neural networks which automatically generates execution schedules with partial execu-tion loops (e.g. Figure <ref type="figure">1</ref>). This drastically reduces SRAM usage compared to ordinary execution used in prior work.</p><p>Conceptually, PEX is built as a progression of the following three components: (1) the partial execution framework: rules, axioms and abstractions, which define how each operator may be executed;</p><p>(2) execution state and memory model, which allows deriving correct execution schedules for the entire network;</p><p>(3) a dynamic-programming-based algorithm which finds an execution schedule that minimises the peak memory usage.</p><p>In the following, we describe the three components of PEX and discuss the interaction between computational correctness and quantization, as well as using PEX within structured pruning to reduce SRAM usage further. For contextual clarity, we will use convolutional neural network (CNN) terminology and reference the MobileNet-v2 IRB example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partial execution framework</head><p>Definition 1. A neural network is a computation graph of tensors, produced by operators (layers). We consider arbitrary neural networks which process R-dimensional tensors (R ? 2) of shape N ? . . . ? C, where N is the batch size, and C is the number of channels/feature maps. A "channel" is a tensor slice in the C dimension. This includes 4-dimensional tensors of shape</p><formula xml:id="formula_0">N ? H ? W ? C seen in 2D</formula><p>CNNs, where H, W are the width and height of the image. The dimensions' order is unimportant: for CNNs, NCHW and NHWC are supported; NCHW may be preferred to avoid strided loads/writes.</p><p>Definition 2. A tensor t can be present in memory in two forms: fully materialised (denoted t F ) or partially materialised (only one channel, denoted t P ).</p><p>Definition 3. There are only two types of operators: channel-wise operators (borrowing CNN terminology; denoted with 'C') and aggregating operators (denoted with 'A'):</p><p>? Channel-wise operators (C-types) are depthwise convolutions, pooling layers (both local and global) and any element-wise operators such as addition or ReLU.</p><p>In principle, these operators consume one input channel to produce one output channel. Output channels are independent, and the implementation of the operator is often a simple loop repeating the computation that maps an input channel to an output channel. ? Aggregating operators (A-types) are dot products: convolutions and fully connected (dense) layers. To compute the output tensor, they map all input channels to one output channel (multiple times, for all output channels), or, equivalently, compute one input channel's contribution to all output channels (multiple times, for all input channels). That is, A-types can either gen-erate one channel of their output at a time, providing the full input buffer is present in memory, or consume one channel at a time and write its contribution to the fully allocated output buffer. Due to this generating/aggregating property, they will be found either at the start or the end of partial execution loops.</p><p>Remark 4. We describe partial execution loops as processing one channel of data at a time. However, in a loop with a total of C channels to be iterated over, the number of channels computed at once can be any K C. Increasing K increases the memory usage but can have a beneficial effect due to the use of SIMD kernel implementations (when K is a multiple of 4) and lowering overhead associated with operator context switching, if any (implementationdependent). The highest possible value of K that does not cause a memory constraint violation can be found for each loop independently using binary search. Definition 5. An operator is a mapping from input tensors i = {i 1 , . . .} to an output tensor o, denoted i op -? o. The operator's type (A or C) allows for multiple ways of executing it using different materialisation forms of inputs i and output o. For operators with multiple inputs (e.g. addition), all inputs always share the same materialisation form, so the set of inputs can be denoted with i F or i P .</p><p>In total, we establish six rules for executing an operator op (also see diagram in Figure <ref type="figure" target="#fig_0">2</ref>):</p><p>Full continue (for both A-and C-types): i F op -? o F . Input and output buffers are present in memory fully, and the operator is not in a partial execution loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial continue (C-type only): i P op</head><p>-? o P . A channelwise operator is executed for one input channel to produce one output channel. Example: a depthwise conv. operator computing one output channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate (A-type only). i F op</head><p>-? o P . Computes one output channel from fully-materialised input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accumulate (A-type only). i P op</head><p>-? o F Consumes one channel of input data and writes its contribution to a fully-materialised output buffer.</p><p>The following rules only change the materialisation form of a tensor t. For tensors consumed by multiple operators, the rule does not have to be applied to all branches.</p><p>Slice: t F -? t P . The runtime reads one channel at a time from tensor t to facilitate partial execution of any subsequent operator that requires partially-materialised input. In this case, t P does not need to be explicitly allocated: it is an offset/view into t F .</p><p>Post-concatenate: t P -? t F . Channels of t are written into a preallocated full buffer. In this case, t P does not need to be explicitly allocated: the producer of t P can write into a particular offset of t F . Definition 6. A loop context (C) is a set of tensors that are kept in memory (fully materialised) due to a current loop that will repeatedly read from or write to these tensors. Loop context tensors are allocated in SRAM prior to the first iteration of the loop and deallocated after the last iteration.</p><p>In MobileNet-v2 IRB (Figure <ref type="figure" target="#fig_0">2</ref>), the context consists of inputs to operator B (exec. 'Generate') and the output of operator D (exec. 'Accumulate'). At the i th iteration of the loop, B reads its entire input from the context to produce its i th channel, and D writes the contribution of its i th input channel to its output buffer in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deriving an execution schedule</head><p>A compiler produces a sequence of instructions, which determine which operator should be executed (and how), to obtain the network's outputs from its inputs. Conceptually, an instruction is an action that causes the execution state to transition from some current state to the next state. The instructions are applied until the final state is reached in which all neural network layers were executed.</p><p>We are interested in modelling memory usage, so the execution state will consist of information about tensors present in the memory, their materialisation form, and information about the current partial execution loop, if any.   must be satisfied for it to be applied. The transitions closely match the operator evaluation rules described earlier, but also modify the loop context as required.</p><formula xml:id="formula_1">End H Mem, ? ? done Mem, ? is S * (final state) Loop End H Mem, C ? H Mem, ? ?t ? Mem. t is t F Full Cont. H {i F op -? o F } ? R, C ? FullCont(op) :: H i F ? R, ? Eval. , No Loop Partial Cont. H {i P op -? o P } ? R, C ? PartCont(op) :: H i P ? R, C Eval. , In Loop , C Generate H {i F op -? o P } ? R, C ? Generate(op) :: H i F ? R, C ? i Eval. , Compat. Loop , A , o ? C Aggregate H {i P op -? o F } ? R, C ? Aggregate(op) :: H i P ? R, C ? {o} Eval. , Compat. Loop , A , o ? C Slice H {t F -? t P } ? R, C ? Slice(t) :: H t F ? R, C ? {t} Compat. Loop , t ? C Post-Concat. H {t P -? t F } ? R, C ? PostConcat(t) :: H t P ? R, C ? {t} Compat. Loop</formula><p>If derivation rules can be successfully applied to transition from the starting to the final state, we obtain a sequence of instructions (potentially, out of many) to execute a neural network. Figure <ref type="figure" target="#fig_1">3</ref> shows how the MobileNet-v2 IRB example can be viewed within the devised framework: first, by erasing particular layer information, leaving only A-and C-type annotations; then, by applying the derivation rules to produce a state transition graph. The figure also gives an example of a more complex execution schedule using more execution rules, containing two loops with multiple operators at the start and the end of a loop. The compiler may fall back to ordinary execution by only using 'Full Continue'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">An algorithm to minimise peak memory usage</head><p>Out of numerous valid derivations, we are interested in choosing instructions that minimise SRAM usage. If we knew which sequence of instructions minimises peak memory usage from a particular execution state, we could record and reuse this information to build up a full execution schedule-a hallmark of a dynamic programming algorithm.</p><p>Algorithm 1 describes a high-level pseudocode procedure 'OptimisePMU' which enumerates all state transitions to find a sequence of instructions that results in the smallest peak memory usage (PMU) for arbitrary neural network computation graphs. Note that calls are memoised: results for the same input are remembered and not recomputed. In practice, we are interested in not just accumulating execution history H but also keeping track of the best-seen peak memory usage and the complete loop context (denoted C * , returned from upstream computation) to calculate the PMU of operators within partial execution loops. The algorithm has exponential computational complexity in the number of operators but, due to small input size (&lt; 100 operators) and limited branching in the network architecture, the optimised schedules are obtained within seconds in practice.</p><p>The algorithm relies on four straightforward helper functions: (1) 'rule conditions are satisfied' checks if Mem and C satisfy set predicates set out in the definition of rule; (2) Rule(...) invocation updates Mem and C according to the rule definition; (3) LOCALPMU(t, Mem, C * ) computes the memory usage required (working set size) for tensor t to be computed using tensors in Mem; if loop context is present, it will be counted towards the memory usage; (4) 'configuration x is better than y' compares scheduling results: true if x has lower peak memory usage, if y is invalid, or if PMU is equal but x is simpler; the latter can express a preference to avoid partial execution when there is no PMU improvement.</p><p>Algorithm 1 OPTIMISEPMU computes a partial execution schedule which minimises peak memory usage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effects on quantisation</head><p>Neural network deployment on MCUs requires parameters and activations to be quantised. This both enables the execution and saves space: an MCU may not have floating-point processing units, or they may consume more power; Flash and SRAM usage is reduced when parameters and activations are stored at e.g. 8-bit precision, instead of 32 bits.</p><p>Popular runtime and layer implementations use affine quantisation <ref type="bibr" target="#b15">(Jacob et al., 2018)</ref>, where operators compute a single output element at a time (stored in a CPU register) by accumulating contributions from all input features (multiplied by the learned weight) at 32-bit precision. When all contributions have been added, an activation function is applied, and the output is written to SRAM at 8-bit precision.</p><p>This poses a memory usage vs. correctness trade-off for operators executed using the 'Accumulate' partial execution rule, which accumulate input contributions at each loop iteration: the accumulation/output buffers must be kept in SRAM at 32-bit precision for computational correctness. This would result in a 4? memory usage increase for these buffers (changing from 8-bit to 32-bit representation), potentially reducing memory usage gains under partial execution.</p><p>To address this issue, we experiment with reducing the precision of the accumulation thus removing the 4? memory inflation. The 'Accumulate' rule would only apply to a few layers in the network; we expect other layers to learn to compensate for any performance hit caused by the reduced precision of affected convolutions. We experiment with quantization-aware training using 32-bit, 16-bit and 8-bit precision for accumulation buffers and find no performance degradation in the network (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Structured pruning for partial execution</head><p>Most deep learning models are too resource-demanding for MCU hardware, not just due to their memory usage but also size or latency, which can be addressed with model compression. Typically, three dimensions of resource usage are optimised for MCU compatibility: (a) peak memory usage (bounded by SRAM size), (b) model size (bounded by Flash memory) and (c) the number of multiply-accumulate operations (MACs, <ref type="bibr">Liberis et al. (2021)</ref> show that MACs are a highly accurate proxy for latency on an MCU processor).</p><p>We integrate PEX with the state-of-the-art MCU-aware model compression, which operates on the basis of differentiable structured pruning <ref type="bibr">(Liberis &amp; Lane, 2021)</ref>, with the aim of (a) producing MCU-compatible models for our evaluation and (b) guiding pruning towards the memory bottleneck identified by our compiler. The method removes entire features/channels by repeatedly querying optimisation objectives during compression: we replace the default peak memory usage metric with an invocation of PEX, which reports the peak memory usage under partial execution and which operators' outputs constitute the bottleneck.</p><p>This leads to co-design between the model compression and the partial execution compiler. Both structured pruning and the compiler operate on the channel dimension of the neural network operators. Iteratively, the following takes place:</p><p>(1) pruning adjusts per-layer channel counts based on the current memory bottleneck, and other resource usage objectives;</p><p>(2) the compiler computes a new execution schedule using the updated operator channel dimensions: this new schedule is likely different due to new channel dimensions;</p><p>(3) the compiler reports a new peak memory usage result and new memory bottleneck back to the pruning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we quantitatively evaluate our contributions:</p><p>1. We compare partial execution with patch-based execution (MCUNet-v2), the closest relevant prior work, by considering peak memory usage reduction and overheads of either approach on MobileNet-v2 models. 2. We apply PEX to five classes of architectures, using partial execution alone or the compiler-pruning codesign, to establish new state-of-the-art solutions in the low SRAM usage regime, compared to ten prior low-footprint models on three classification tasks. 3. We test whether storing accumulation buffers at reduced precision affects models' accuracy (Section 3.4).   <ref type="table" target="#tab_5">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with patch-based execution</head><p>Patch-based execution is an alternative approach of splitting the computation to reduce memory usage (Section 2.2), which uses tensor slices along the image width and height axes ("patches"), instead of the channel axis. For MCUs, it is implemented by MCUNet-v2 <ref type="bibr" target="#b21">(Lin et al., 2021)</ref>: we compare it to PEX by examining the memory usage reduction and computational overheads for the MobileNet-v2 model, followed by several modifications to it that enable further improvement in SRAM usage.</p><p>Table <ref type="table" target="#tab_5">3</ref> (row 1) considers variations of MobileNet-v2 architecture, evaluated on ImageNet. ? The execution takes ?301 M MACs and has a peak memory usage of 1505 kB. Patchbased and partial execution reduce it to 321 kB (?4.7?) and 372 KB (?4?), respectively, at the cost of additional 9.3% increase in MAC operation count for patch-based execution ? The resource usage, overheads, and accuracy of MCUNet-v2 models have been adjusted from originally reported data due to differences in evaluation. We explain the rationale in Appendix C. yet zero computational overhead for our approach.</p><p>It is impossible to conclusively argue whether a patch-based or partial approach is generally superior at reducing peak memory usage: the outcome depends on the architecture. To illustrate this, we consider two modifications of  to reduce overheads of patch-based execution, MCUNet-v2 develop an alternative version with a redistributed receptive field (RD): a change to reduce the spatial resolution and kernel size earlier in the network; (b) we present a version that reduces the memory bottleneck via additional spatial downsampling within the first inverted residual block ("MBConv"), in its projection layer, instead of the following block. Table <ref type="table" target="#tab_5">3</ref> (rows 2, 3) shows that these minor modifications can drastically affect the models' resource usage: our variation of MobileNet-v2 significantly increases computation faced by patch-based execution compared to the original model (15.5% total MAC increase) but, in the partial execution paradigm, yields the lowest peak memory usage seen so far with no computation overhead.</p><p>The modifications above are specific to MobileNet-v2 and cannot be straightforwardly applied to other architectures. Therefore, we consider two other generic ways to reduce an architecture's peak memory usage: our compiler-pruning co-design and using lower input resolution. Table <ref type="table" target="#tab_5">3</ref>  <ref type="bibr">(rows 4, 5)</ref> show that (a) co-design can reduce PMU under partial execution to match that of patch-based execution while offering comparable accuracy/MACs to the two previously considered modifications; (b) by lowering the input resolution, we observe the lowest peak memory usage achieved so far (218-222 KB) at the cost of a &lt; 2% accuracy drop. Summary of 4.1. The network's architecture determines whether patch-based or partial execution will give greater memory reduction, and can be modified to benefit either approach. Partial execution has no computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models under partial execution regime</head><p>In this section, we show the peak memory usage improvements achievable by PEX alone and the compiler-pruning co-design. We limit our scope to CNNs, as the most popular type of low-footprint neural networks, and consider three classification tasks: ImageNet (1000-class image classification) <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>, VisualWakeWords (person detection) <ref type="bibr" target="#b6">(Chowdhery et al., 2019)</ref> and SpeechCommands (keyword recognition via spectrogram classification) <ref type="bibr" target="#b29">(Warden, 2018)</ref>, and five classes of architectures: MobileNet-v2 <ref type="bibr" target="#b25">(Sandler et al., 2018)</ref>, EfficientNet <ref type="bibr" target="#b27">(Tan &amp; Le, 2019)</ref>, RES-15 <ref type="bibr" target="#b28">(Tang &amp; Lin, 2018)</ref>, MCUNet-v1 networks <ref type="bibr" target="#b20">(Lin et al., 2020)</ref> and MicroNets <ref type="bibr" target="#b3">(Banbury et al., 2020)</ref>. Additional comparison points are provided by the differentiable pruning (DiffPru), DS-CNN <ref type="bibr" target="#b33">(Zhang et al., 2017)</ref>, <ref type="bibr">MCUNet-v2 (Lin et al., 2021)</ref>, RES-8 <ref type="bibr" target="#b28">(Tang &amp; Lin, 2018)</ref> and <ref type="bibr">ETinyNet et al., 2022a)</ref> architectures, resulting in ten low-footprint baselines. Models have been reevaluated under the same training pipeline and resource usage metrics (except for MCUNet-v2 ? ). We use the peak memory usage (PMU) versus classification accuracy plots for comparison; tabular data with all metrics are available in Appendix D.</p><p>ImageNet under 128 kB of SRAM. Figure <ref type="figure" target="#fig_3">5</ref> (top) shows the peak memory usage versus accuracy trade-off for Ima-geNet models. PEX alone improves PMU by 2.2? and 2.5? for MCUNet-v1 and EfficientNet-B0 architectures. The compiler-pruning co-design produces the only set of highperforming models under 128 kB SRAM usage amongst considered baselines in our evaluation environment. The models achieve 64.5%-65.4% accuracy, significantly outperforming a low-resolution MobileNet-v2 model (+13.9%).</p><p>VisualWakeWords under 32 kB of SRAM. Figure <ref type="figure" target="#fig_3">5</ref> (middle) shows the results for a variety of models for different input resolutions on the Visual Wake Words dataset. On MobileNet-v2, increasing input resolution increases PMU while producing diminishing returns in classification accuracy. At 80?80 input resolution, the compiler-pruning co-design decreases the PMU by 6.8? compared to the baseline, bringing it within the 32 kB range. This constitutes a +2.9% accuracy improvement compared to prior work (MicroNets VWW-2 and DiffPru) within this SRAM usage range. Alternatively, recovered SRAM allows using higher resolution inputs to increase accuracy by +3.9% while retaining comparable SRAM usage to the baseline.</p><p>Keyword spotting under 16 KiB SRAM. Figure <ref type="figure" target="#fig_3">5</ref> (bottom) shows results for the SpeechCommands dataset. We apply both the partial execution compiler alone, and the compiler-pruning co-design to the parent architecture of MicroNets-L (i.e. instantiated at full channel counts, abbr. MN-L in the figure), and the RES-15 architecture. The use of compiler alone yields 2.5? and 1.5? memory reduction, respectively, and the use of pruning brings the models under the desired SRAM limit, with only up to 1% accuracy loss compared to their original unpruned versions. This constitutes a +1.9% accuracy improvement compared to prior work (DS-CNN-S) under the 16 KiB SRAM limit. Applying co-design to the RES-15 architecture yields up to 10.5?  memory reduction while matching the classification accuracy compared to the MicroNets-KWS-L baseline (using ordinary execution). Summary of 4.2. PEX and PEX-pruning co-design reduce peak memory usage of high-performing models to establish state-of-the-art results in low SRAM usage regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accumulation buffer precision analysis</head><p>Previously, in Section 3.4, we discussed how the interaction of quantisation and partial execution requires intermediate results of operators executed using the 'Accumulate' rule to be stored at 32-bit precision to preserve computational To remedy the resulting memory usage increase, we suggest sacrificing computational correctness for affected operators by storing accumulation buffers at reduced precision and allowing other operators to learn to compensate for this imprecision.</p><p>We aim to see whether reduced precision results in accuracy loss. We extended TensorFlow's quantisation-aware training (QAT) implementation to learn quantisation parameters for accumulation buffers (pre-activation), which are used during execution to re-quantise the buffers to the desired precision when updated. We experiment with the 32-bit (correct), 16bit and 8-bit precision. The scheduler is parameterised with the memory cost of increased precision and, therefore, the 'Accumulate' rule will be avoided if there is an alternative with a lower PMU and fewer affected layers.</p><p>Partial execution is mathematically equivalent to ordinary execution: without quantisation, both approaches have tensor values within the noise introduced by a differing order of operations; with quantisation, this noise is amplified, resulting in accuracy differences. We repeat the QAT for different quantisation levels of accumulation buffers; this also introduces noise due to randomness in the training pipeline, and the reduced precision also acts as a regulariser during QAT.</p><p>Table <ref type="table" target="#tab_6">4</ref> shows the classification accuracy of MCUNet-v1-S, MobileNet-v2, and MicroNets-KWS-L models under ordinary and partial execution, which shows no accuracy loss is observed for all tasks for up to 16-bit accumulation precision, resulting in up to 2.6? PMU reduction. Lowering the precision to 8-bit results in up to 4? PMU reduction and -0.23% accuracy loss only in the MCUNet-v1-S experiment.</p><p>Summary of 4.3. Operators executed using the 'Accumulate' rule can use reduced accumulation precision for lower memory usage: this has no or negligible accuracy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations and future work</head><p>PEX's ability to reduce memory usage is limited by the network's architecture (also applies to patch-based execution, see Appendix B), which can be adjusted in an informed way to allow further memory improvement. Here, we leveraged pruning as a general way to do so; PEX could be integrated with a neural architecture search system (NAS) that considers different layer connectivity to avoid some unfavourable layer arrangements automatically.</p><p>Additionally, data from an on-device MCU deployment could be of interest as auxiliary evaluation; however, because partial evaluation yields deterministic memory usage gains, which we have also verified in simulation, we would expect any on-device deployment to only confirm this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>A limited amount of SRAM on microcontroller platforms significantly impedes the on-device execution of neural networks on embedded, personal and IoT devices. To address this, we created a partial execution framework for arbitrary neural networks within the PEX model compiler, which automatically finds execution schedules that minimise peak SRAM usage. This is achieved by identifying subgraphs of the network's layers whose execution can be split along the channel axis, resulting in a computationally-equivalent yet more memory-efficient schedule compared to ordinary execution, at no extra computational cost. We enhance PEX with a structured pruning method, resulting in a co-design between the network's architecture and its execution schedule, which reduces SRAM usage even further. We evaluate PEX on five architecture classes and establish state-of-theart performance in low SRAM usage regimes for ImageNet, VisualWakeWords and SpeechCommands datasets with up to +2.9% accuracy increase, compared to ten prior lowfootprint models. We find that a 4? memory usage reduction is possible by applying PEX alone, or up to 10.5? when using the compiler-pruning co-design, while maintaining the same classification accuracy. Alternatively, the recovered SRAM can be used to process higher resolution inputs, boosting accuracy by up to +3.9% on Visual Wake Words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DERIVATION RULE APPLICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONDITIONS</head><p>Each derivation rule listed in Table <ref type="table" target="#tab_3">2</ref> has a number of conditions that need to be satisfied before the rule can be applied. Some conditions arise naturally from the pattern matching in the definition of the rule, e.g. 'Partial Continue' can only be applied to partially-materialised tensors in Mem.</p><p>In Loop , No Loop check if there is (or, respectively, is not) a partial execution loop currently being built (C = ?). The presence of partially-materialised tensors in Mem implies In Loop .</p><p>A , C require the producing operator ('op') of the considered tensor to be of the aggregating or channel-wise type, respectively.</p><p>Eval. (can be evaluated) checks if an operator 'op' with output t can be executed within the current state. This is the case when t is not a predecessor of (that is, t is not required to compute) any other tensor in Mem (including itself in a different materialisation form). If 't' were required to compute another tensor t , op would have to be executed again in the computation path for t , resulting in redundant computation.</p><p>Compat. loop checks if the operator can be added to the current partial execution loop (trivially true for 'Aggregate' and 'Post-Concatenate', if there is no current loop) by checking if its inputs' or output's (as appropriate for the rule) channel dimensions match those of the current loop context tensors.</p><p>t ? C checks if the tensor is not already in the loop context; used to remove trivial computation sequences (e.g. 'Generate' followed by 'Post-Concatenate').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B QUALITATIVE COMPARISON AGAINST PATCH-BASED EXECUTION</head><p>Instead of the channel axis C, the computation can also be split along spatial axes H, W . In MCU deep learning, patch-based execution is exemplified by MCUNet-v2 <ref type="bibr" target="#b21">(Lin et al., 2021)</ref>, and in the following, we discuss the advantages, disadvantages and trade-offs of the two approaches.</p><p>Redundant computation in patch-based execution. As patch-based computation transitions between adjacent output "pixels" of a convolutional operator, it may use the same spatial regions of the input (overlapping receptive field) to obtain the output value. If the input patch is deallocated during this transition, this shared fraction of the input will have to be recomputed, leading to computational overheads (we refer to <ref type="bibr" target="#b21">Lin et al. (2021)</ref> for a more detailed diagrammatic explanation of this issue). This can be partly alleviated by minimising overlapping input regions for patch-executed operators, which authors do both manually and automatically by searching and adjusting, among other parameters, strides, and kernel and patch sizes within the MobileNet-v2-based backbone. In contrast, our methodology does not result in any redundant computation (Constraint 5).</p><p>Both techniques suffer under unfavourable layer arrangements. We showcase an unfavourable case for both approaches: squeeze-excitation (SE) layers in state-of-theart architectures, such as MobileNet-v3 (small, large) and EfficientNet-B0, which prevent both methods from achieving greater peak memory reduction.</p><p>Figure <ref type="figure">6</ref> shows the first six layers of the MobileNet-v3small architecture, which contain squeeze-excitation layers: a global pooling layer (reduction along all spatial dimensions), followed by compression and expansion of the channel dimension using two fully-connected layers ('squeeze' and 'excite'). Reduction across all spatial dimensions requires values from all input patches, which ends the patchbased execution span; similarly, the subsequent 'squeeze' layer requires values from all input channels, which terminates the current partial execution loop. Of course, a new partial execution loop can be started further in the network.</p><p>A straightforward workaround is modifying the architecture by removing the squeeze-excitation bottleneck from the high SRAM usage parts of the architecture. For example, a sibling architecture, MobileNet-v3-large, does not contain SE layers within the first three inverted residual blocks.</p><p>Greater memory reduction potential for very high spatial resolution low-channel layers with patch-based execution. When H, W C, patch-based execution offers a greater memory reduction potential: in the limit, 1/HW for a 1?1 patch (and, in that instance, a signifi-cant re-computation overhead) versus 1/C for one channel. Therefore, some high-resolution computer vision neural networks, such as those processing aerial or medical imaging data, may benefit from patch-based execution more, depending on their architecture.</p><p>We also note that these approaches are not mutually exclusive: channel-axis partial execution can be applied within patch-based execution to reduce SRAM usage and enable the use of larger tiles, thus lowering the computation overhead. Both approaches can be unified within a more complex framework that jointly considers inter-operator tiling in spatial and channel dimensions.</p><p>The discussion above would suggest that channel-axis partial execution offers lesser peak memory usage gains at the benefit of no redundant computation and thus faster execution. We quantitatively examine this claim in Section 4 by evaluating both execution approaches on a high-resolution (for MCU applications) MobileNet-v2 model trained for ImageNet classification. Specifically, we:</p><p>1. quantitatively evaluate differences in peak memory usage and computation overhead for both approaches; 2. examine how these quantities change with minor modifications to the architecture that would make it more favourable to either approach; 3. consider an automatically compressed model using structured pruning, instead; 4. check if the high-resolution input is actually necessary for the classification task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADJUSTMENTS FOR THE QUANTITATIVE COMPARISON AGAINST MCUNET</head><p>There are a number of adjustments that need to be made to the network performance and resource usage reported in the MCUNet-v1 and -v2 manuscripts for a fair evaluation.</p><p>Peak memory usage (MCUNet-v2 only). The numbers presented in Evaluation (Section 4 differ from the MCUNet-v2 manuscript, which departs from the evaluation settings of prior work, such as MCUNet-v1 <ref type="bibr" target="#b20">(Lin et al., 2020)</ref>, differentiable pruning <ref type="bibr">(Liberis &amp; Lane, 2021)</ref> or MicroNets <ref type="bibr" target="#b3">(Banbury et al., 2020)</ref>, by assuming that the full input tensor does not need to be stored in memory. Instead, compressed JPEG-encoded input ought to be available in memory for partial decoding. However, this assumption does not generally hold: while true for applications that have hardware with JPEG camera modules, it would not be the case for audio or sensor data processing applications. Compression of inputs and/or feature maps (and associated additional power usage and decoding overheads) is an interesting but, ultimately, tangential research direction to evaluating execution techniques, so we recompute patch-based execution memory usage within the same general environment for a fair comparison. For cases where the network architecture definition is not available, we assume that peak memory usage occurs in the patch-based execution stage and add the input size to the reported memory usage.</p><p>Classification accuracy on Visual Wake Words (VWW) <ref type="bibr">(-v1 and -v2)</ref>. The authors of VWW dataset provide training and validation data splits and suggest using the "minival" dataset (a small subset of the validation split) for final reporting. However, the "minival" happens to be a biased sample of the validation set, which results in noisier and inflated classification accuracy: models claiming to achieve &gt;94% accuracy only achieve ?90% on a larger validation set. Some prior work <ref type="bibr">(Liberis &amp; Lane, 2021;</ref><ref type="bibr" target="#b3">Banbury et al., 2020;</ref><ref type="bibr" target="#b24">Saha et al., 2020)</ref> uses the full validation set for evaluation, instead of "minival". For MCUNet-v1, architecture definitions are available, allowing us to re-evaluate the models (and observe the accuracy disparity between the two evaluation sets). For MCUNet-v2, definitions are unavailable; thus we linearly extrapolate from MCUNet-v1 results, assuming both use the same training pipeline.</p><p>Miscellaneous differences.</p><p>? We note different classification accuracies for the Ima-geNet dataset (MobileNet-v2 performing at 72.2% in the MCUNet-v2 manuscript vs ours' 71.5%) due to differences in the training pipeline. As these differences are under 1%, we consider them to be negligible. ? When the input resolution is not reported, we extrapolate from MobileNet-v2 results as to what input resolution would be appropriate to achieve reported resource usage or classification accuracy.</p><p>We acknowledge that differing evaluation assumptions and adjustments make the comparison challenging. Still, in the absence of other related work, we believe that the data reported here paint a fairer picture of the relative performance of partial and patch-based execution on common tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ARCHITECTURE RESOURCE USAGE</head><p>The following tables list detailed resource usage and classification accuracy data of models presented in Figure <ref type="figure" target="#fig_3">5</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (Left:) MobileNet-v2 IRB example within ordinary and partial execution. (Middle:) Sample code for partial execution of the IRB and memory bottlenecks before and after compilation. (Right:) Diagrammatic representation of all partial execution rules (Section 3.1).Execution transitions between partially-and fully-materialised tensors at loop boundaries; the schedule can contain multiple loops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (Left and middle:) MobileNet-v2 IRB example as a typeannotated and an execution state graph. (Right:) A more complex computation graph with multiple partial execution loops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Modified versions of MobileNet-v2 which, compared to the original architecture, allow greater SRAM usage reduction within patch-based (left) or partial (right) execution (Table3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Peak memory usage vs accuracy on the considered datasets. The datapoint label, if present, denotes the input resolution (e.g. 'r160' for 160x160 colour input) and, for MCUcompatible models, the size (storage requirement).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in Section 4. As with the Figure, icons next model name represent ordinary execution (?), partial execution (p), pruning-compiler co-design (?) or patch-based execution ( ). MCUNet-v2 results are adjusted and estimated ( ?): see Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Specifications of a high-end GPU, a smartphone, a micro-PC and Nucleo development boards using Cortex M7 and M4 chips. MCUs have the most favourable power efficiency and price but are significantly resource-constrained.</figDesc><table><row><cell>CPU</cell><cell>RAM</cell><cell cols="2">Storage Power</cell><cell>Price</cell></row><row><cell cols="2">GPU: NVIDIA A100 GPU</cell><cell></cell><cell></cell></row><row><cell>6912?@1.4 GHz</cell><cell>40 GB</cell><cell>-</cell><cell cols="2">400 W 12.5K$</cell></row><row><cell cols="3">Mobile: Galaxy S22 Smartphone</cell><cell></cell></row><row><cell>8? @ &lt;2.8 GHz</cell><cell cols="3">8 GB 128 GB ?3.6 W</cell><cell>750$</cell></row><row><cell cols="2">Micro-PC: Raspberry Pi 4B</cell><cell></cell><cell></cell></row><row><cell>4? @1.5 GHz</cell><cell cols="2">&lt;4 GB 512 GB</cell><cell>2.7 W</cell><cell>35$</cell></row><row><cell cols="4">MCU: NUCLEO-F767ZI (M7) (ARM mbed, 2022b)</cell></row><row><cell cols="2">216 MHz 512 KB</cell><cell>2 MB</cell><cell>0.3 W</cell><cell>9$</cell></row><row><cell cols="4">MCU: NUCLEO-F446RE (M4) (ARM mbed, 2022a)</cell></row><row><cell cols="3">180 MHz 128 KB 512 KB</cell><cell>0.1 W</cell><cell>3$</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Definition 9. The algorithm works backwards through the network computation graph G-tensors in 'Mem' are treated as tensors yet to be computed. The initial state S 0 is the set of fully-materialised graph output tensors: S 0 = {o F |o ? outputs(G)}, ? and, similarly, the final state S * is the set of fully-materialised inputs. This acts as a reachability analysis: only required operators will be executed.</figDesc><table /><note><p><p><p><p>Definition 7. The algorithm considers an execution state S = Mem, C and builds up an instruction list (history) H. 'Mem' is the state of memory: a set of tensors currently present in memory, each either fully or partially materialised; 'C' is the loop context (see Def. 6); H is a list of instructions required to arrive at state S.</p>Remark 8. The network's parameters (weights) do not reside in SRAM under any execution regime-individual values are loaded from the read-only on-chip Flash memory.</p>Constraint 10. No output is computed twice. We do not aim to increase the amount of computation required for a single inference pass of a neural network.</p>Definition 11. Table 2 lists allowed state transitions, called derivation rules. Each rule has a set of conditions which</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Execution state transition rules within PEX. The state is a tuple of tensors yet to be computed ('Mem') within a graph G and loop context tensors ('C'). Rules operate by decomposing 'Mem' via pattern matching: a rule can be matched to any tensor of 'Mem' via the left-hand side of the rule (providing conditions are satisfied). For example, '{i F op -? o F } ? R' matches some element of 'Mem' that is a fully-materialised tensor 'o F '; then, its producing operator is referred to as (bound to) 'op' whose fully-materialised inputs are 'i F ', and remaining tensors (Mem \ o F ) are 'R'. The execution instructions (named after the state transition rules) are prepended ('::') to the list 'H'</figDesc><table><row><cell>Rule</cell><cell>State transition</cell><cell>Conditions (Appendix A)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Input: a set of tensors to be computed Mem and loop context C (empty for the initial call). 2: Returns: the instruction list H, the computed PMU and the final loop context C * (empty iff C = ?).</figDesc><table><row><cell cols="2">3: function OPTIMISEPMU(Mem, C)</cell></row><row><cell cols="2">4: if End (Mem, C) conditions are satisfied then</cell></row><row><cell>5:</cell><cell>return H=[], PMU= t?Mem |t|, C  *  =?</cell></row><row><cell cols="2">6: if LoopEnd (Mem, C) conditions are satisfied then</cell></row><row><cell>7:</cell><cell>H, PMU, ? ? OPTIMISEPMU(Mem, ?)</cell></row><row><cell>8:</cell><cell>return H, PMU, C</cell></row><row><cell cols="2">9: bestConfig ? (unknown)</cell></row><row><cell>10:</cell><cell>for t ? Mem do</cell></row><row><cell>11:</cell><cell>for Rule ? {FullContinue, Slice, . . .} do</cell></row><row><cell>12:</cell><cell>if Rule (Mem, C) conditions are satisfied then</cell></row><row><cell>13:</cell><cell>Mem , C ? Rule(t, Mem, C)</cell></row><row><cell>14:</cell><cell>H , PMU , C  *  ? OPTIMISEPMU(Mem , C )</cell></row><row><cell>15:</cell><cell>PMU</cell></row></table><note><p>1: * ? max(PMU , LOCALPMU(t, Mem , C * )) 16: if H , PMU , C * better than bestConfig then 17: bestConfig ? H , PMU , C * 18: return bestConfig</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Resource usage of variations of MobileNet-v2 on ImageNet using ordinary, patch-based (4 H and W patches) or partial execution. PMU improvements brought by patch-based or partial execution vary, but partial execution always has zero computational overhead.</figDesc><table><row><cell cols="2">MobileNet-v2 variant</cell><cell cols="2">Loop overhead Patch  ? Partial</cell><cell cols="3">Total overhead Patch  ? Partial Ordinary Patch  ? Partial Peak memory usage (PMU)</cell><cell>Accuracy (8-bit q.)</cell><cell>MACs Ordinary</cell></row><row><cell>Original</cell><cell></cell><cell>+30.6%</cell><cell>0%</cell><cell>+9.3%</cell><cell>0%</cell><cell>1505 kB 321 kB 376 kB</cell><cell>71.52%</cell><cell>301 M</cell></row><row><cell cols="2">RD (MCUNet-v2)</cell><cell>+13.8%</cell><cell>0%</cell><cell>+2.9%</cell><cell>0%</cell><cell>1505 kB 283 kB 376 kB</cell><cell>71.16%</cell><cell>294 M</cell></row><row><cell cols="2">Our modification</cell><cell>+38.9%</cell><cell cols="2">0% +11.2%</cell><cell>0%</cell><cell>978 kB 278 kB 226 kB</cell><cell>71.20%</cell><cell>295 M</cell></row><row><cell cols="3">PEX-pruning co-design +36.1%</cell><cell>0%</cell><cell>+9.6%</cell><cell>0%</cell><cell>1505 kB 321 kB 276 kB</cell><cell>71.20%</cell><cell>288 M</cell></row><row><cell cols="3">Original, 172?172 res. +45.2%</cell><cell cols="2">0% +12.6%</cell><cell>0%</cell><cell>888 kB 218 kB 222 kB</cell><cell>69.58%</cell><cell>193 M</cell></row><row><cell>224?224?3</cell><cell>input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>112?112?32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The impact of reduced accumulator precision for three diverse models. The data shows no to negligible accuracy loss from sacrificing computational correctness for PMU reduction.</figDesc><table><row><cell>MCUNet-v1-S</cell><cell>MobileNet-v2</cell><cell>MicroNets-KWS-L</cell></row><row><cell>ImageNet 160x160</cell><cell>VWW 160x160</cell><cell>SpeechCommands</cell></row><row><cell cols="3">Mode: Ordinary execution in int8</cell></row><row><cell>PMU: 333 kB</cell><cell>PMU: 768 kB</cell><cell>PMU: 170 kB</cell></row><row><cell>Acc.: 59.89%</cell><cell>Acc: 90.14%</cell><cell>Acc: 96.77%</cell></row><row><cell>0 accum. layers</cell><cell>0 accum. layers</cell><cell>0 accum. layers</cell></row><row><cell cols="3">Mode: Partial execution w/ 32-bit accumulators (QAT)</cell></row><row><cell>186 kB (?1.8?)</cell><cell>307 kB (?2.5?)</cell><cell>69.0 kB (?2.5?)</cell></row><row><cell>Acc.: 59.91%</cell><cell>Acc.: 90.14%</cell><cell>Acc.: 96.77%</cell></row><row><cell>1 accum. layer</cell><cell>0 accum. layers</cell><cell>0 accum. layers</cell></row><row><cell cols="3">Mode: Partial execution w/ 16-bit (QAT)</cell></row><row><cell>186 kB (?1.8?)</cell><cell>294 KB (?2.6?)</cell><cell>69.0 kB (?2.5?)</cell></row><row><cell>Acc: 59.85</cell><cell>Acc: 90.12%</cell><cell>Acc: 96.71%</cell></row><row><cell>1 accum. layer</cell><cell>2 accum. layers</cell><cell>1 accum. layer</cell></row><row><cell cols="3">Mode: Partial execution w/ 8-bit accumulators (QAT)</cell></row><row><cell>141 kB (?2.4?)</cell><cell>192 kB (?4?)</cell><cell>65.8 kB (?2.6?)</cell></row><row><cell>59.66% (-0.23%)</cell><cell>Acc: 90.14%</cell><cell>Acc: 96.99%</cell></row><row><cell>4 accum. layers</cell><cell>3 accum. layers</cell><cell>2 accum. layers</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Department of Computer Science and Technology, University of Cambridge, UK</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Samsung AI Centre Cambridge, UK. Correspondence to: Edgar Liberis &lt;el398 at cam.ac.uk&gt;.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laskaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Venieris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><surname>Dyno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09949</idno>
		<title level="m">Dynamic onloading of deep neural networks from cloud to device</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fusedlayer cnn accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milder</surname></persName>
		</author>
		<ptr target="https://os.mbed.com/platforms/ST-Nucleo-F446RE/" />
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016-02">2016. February 2022</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>ARM mbed. NUCLEO-F446RE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NUCLEO-F767ZI</title>
		<ptr target="https://os.mbed.com/platforms/ST-Nucleo-F767ZI/" />
	</analytic>
	<monogr>
		<title level="m">ARM mbed</title>
		<imprint>
			<date type="published" when="2022-02">February 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Micronets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11267</idno>
		<title level="m">Neural network architectures for deploying tinyml applications on commodity microcontrollers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Proxylessnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tvm: end-to-end optimization stack for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">TensorFlow Lite Micro: Embedded machine learning on TinyML systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Regev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08678</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving strong-scaling of cnn training by exploiting finer-grained parallelism</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Channel and filter parallelism for large-scale cnn training</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SpArSe: Sparse architecture search for CNNs on resource-constrained microcontrollers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4977" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Udc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05842</idno>
		<ptr target="https://www.grandviewresearch.com/industry-analysis/microcontroller-market" />
		<title level="m">Grand View Research. Microcontroller market size, share &amp; trends report</title>
		<imprint>
			<date type="published" when="2022-02">2022. February 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Unified dnas for compressible tinyml models</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for MobileNet-v3</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="711" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integerarithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><surname>Cmsis-Nn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06601</idno>
		<title level="m">Efficient neural network kernels for ARM Cortex-M CPUs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05110</idno>
		<title level="m">Neural networks on microcontrollers: saving memory at inference via operator reordering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08350</idno>
		<title level="m">Differentiable network pruning for microcontrollers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constrained neural architecture search for microcontrollers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><surname>?nas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Machine Learning and Systems</title>
		<meeting>the 1st Workshop on Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Mcunet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<title level="m">Tiny deep learning on IoT devices</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memoryefficient patch-based inference for tiny deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2346" to="2358" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CoopNet: Cooperative convolutional neural network for low-power MCUs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mocerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calimera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="414" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet scale deep learning on microcontrollers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Craske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Merrett</surname></persName>
		</author>
		<author>
			<persName><surname>Tinyops</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2702" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rnnpool: efficient non-linear pooling for ram constrained inference</title>
		<author>
			<persName><forename type="first">O</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20473" to="20484" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MobileNet V2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning on microcontrollers: a study on deployment costs and challenges</title>
		<author>
			<persName><forename type="first">F</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd European Workshop on Machine Learning and Systems</title>
		<meeting>the 2nd European Workshop on Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for smallfootprint keyword spotting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">TinyML: Machine learning with TensorFlow Lite on Arduino and ultra-low-power microcontrollers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Situnayake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>O&apos;Reilly Media, Incorporated</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Etinynet: Extremely tiny network for tinyml</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training of deep learning pipelines on memory-constrained gpus via segmented fused-tiled execution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction</title>
		<meeting>the 31st ACM SIGPLAN International Conference on Compiler Construction</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="104" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Properties of ImageNet models. Model PMU Size MACs Acc</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07128</idno>
	</analytic>
	<monogr>
		<title level="m">Hello Edge: Keyword spotting on microcontrollers</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ETinyNet x1.00 ? 552 kB 979 kB 114 M 61.7% ETinyNet x0.75 ? 452 kB 660 kB 69.7 M 57.8%</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">MCUNet-v2 M4 , ? 273 kB 1010 kB 119 M 64</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">MB-v2 (mod.) r172 ? 128 kB 999 kB 110 M 64</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">MB-v2 (mod.) r256 ? 287 kB 1994 kB 255 M 69.8% MCUNet-v1-S ? 333 kB 748 kB 67</title>
		<imprint/>
	</monogr>
	<note>4 M 59.8%</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">MCUNet-v1-L ? 422 kB 1757 kB 126 M 67</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MCUNet-v1-L p 192 kB 1757 kB 126 M 67</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MCUNet-v1-L ? 128 kB 994 kB 88</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m">EfficientNet-B0 ? 768 kB 3987 kB 187 M 69</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">EfficientNet-B0 p 308 kB 3987 kB 187 M 69</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m">EfficientNet-B0 ? 128 kB 1999 kB 118 M 65</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m">EfficientNet-B0 ? 128 kB 1999 kB 118 M 65</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m">123 kB 1990 kB 18 M 50</title>
		<imprint>
			<date type="published" when="0590">2 x0.5 r90</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Properties of Visual Wake Words models. Model PMU Size MACs Acc. MicroNets VWW-1 ? 200 kB 616 kB 71</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m">MicroNets VWW-2 ? 27.9 kB 103 kB 3.38 M 83.5% MCUNet r144 ? 270 kB 657 kB 55</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m">MCUNet 5FPS ? 96.0 kB 455 kB 12.5 M 86</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">MCUNet-v2 , ? 268 kB unk</title>
		<imprint/>
	</monogr>
	<note>unk. ?90.4%</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">MCUNet-v2 , ? 89.6 kB unk</title>
		<imprint/>
	</monogr>
	<note>unk. ?86.4%</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">224 ? 223 kB 1 MB 209 M 91</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">MB-v2 (mod.) r160 ? 114 kB 1 MB 106 M 89</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">MB-v2 (mod.) r80 ? 27.6 kB 128 kB 11</title>
		<imprint/>
	</monogr>
	<note>8 M 86.7% MB-v2 r80 ? 192 kB 2.3 MB 42.7 M 87.4%</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0296">2 r96</date>
		</imprint>
	</monogr>
	<note>276 kB 2.3 MB 55.1 M 88.2%</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<idno>MB 81.0 M 88.6</idno>
		<imprint>
			<date type="published" when="2112">2 r112</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2128">2 r128</date>
		</imprint>
	</monogr>
	<note>492 kB 2.3 MB 97.9 M 89.3%</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2144">2 r144</date>
		</imprint>
	</monogr>
	<note>622 kB 2.3 MB 132 M 89.9%</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2160">2 r160</date>
		</imprint>
	</monogr>
	<note>768 kB 2.3 MB 153 M 90.1%</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mb-V</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2176">2 r176</date>
		</imprint>
	</monogr>
	<note>929 kB 2.3 MB 194 M 90.6%</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">MB-v2 ? 198 kB 606 kB 58</title>
		<author>
			<persName><surname>Diffpru</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Diffpru</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>9 kB 101 kB 3.34 M 83.8%</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m">Properties of Speech Commands models. Model PMU Size MACs Acc. MicroNets-KWS-L ? 170 kB 512</title>
		<imprint/>
	</monogr>
	<note>kB 65.7 M 96.5%</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">1 kB 117 kB 15.6 M 95</title>
		<author>
			<persName><forename type="first">Micronets-Kws-M</forename></persName>
		</author>
		<idno>MicroNets-KWS-S ? 51.6 kB 63.5 kB 8.4 M 95.4% RES-8 ? 23.7 kB 112 kB 4.16 M 93.6</idno>
		<imprint>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Micronets-L-Base</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>170 kB 582 kB 74.3 M 96</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m">MicroNets-L-Base p 69 kB 582 kB 74.3 M 96</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Ds-Cnn-M</forename></persName>
		</author>
		<title level="m">kB 140 kB 9.83 M 95</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ds-Cnn-S</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>0 kB 24.3 kB 2.66 M 94.6%</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
