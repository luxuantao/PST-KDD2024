<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Batch Execution and Leasing Using Virtual Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Borja</forename><surname>Sotomayor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kate</forename><surname>Keahey</surname></persName>
							<email>keahey@mcs.anl.gov</email>
							<affiliation key="aff1">
								<orgName type="department">Argonne National Laboratory</orgName>
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
							<email>foster@mcs.anl.gov</email>
							<affiliation key="aff2">
								<orgName type="department">Argonne National Laboratory</orgName>
								<orgName type="institution">University of Chicago Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Batch Execution and Leasing Using Virtual Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC0CA051E2D494AED52F18C605F74C7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.4.7 [Operating Systems]: Organization and Design-Distributed systems</term>
					<term>D.4.5 [Operating Systems]: Reliability-Checkpoint/restart</term>
					<term>C.2.4 [Computer-Communication Networks]: Distributed Systems Design, Management, Performance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As cluster computers are used for a wider range of applications, we encounter the need to deliver resources at particular times, to meet particular deadlines, and/or at the same time as other resources are provided elsewhere. To address such requirements, we describe a scheduling approach in which users request resource leases, where leases can request either as-soon-as-possible ("best-effort") or reservation start times. We present the design of a lease management architecture, Haizea, that implements leases as virtual machines (VMs), leveraging their ability to suspend, migrate, and resume computations and to provide leased resources with customized application environments. We discuss methods to minimize the overhead introduced by having to deploy VM images before the start of a lease. We also present the results of simulation studies that compare alternative approaches. Using workloads with various mixes of best-effort and advance reservation requests, we compare the performance of our VM-based approach with that of non-VMbased schedulers. We find that a VM-based approach can provide better performance (measured in terms of both total execution time and average delay incurred by best-effort requests) than a scheduler that does not support task pre-emption, and only slightly worse performance than a scheduler that does support task pre-emption. We also compare the impact of different VM image popularity distributions and VM image caching strategies on performance. These results emphasize the importance of VM image caching for the workloads studied and quantify the sensitivity of scheduling performance to VM image popularity distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many approaches have been developed to address the problem of providing computational resources to users. For example, an engineer wanting to run a simulation code may submit it as a batch job to a local or (via grid interfaces) remote cluster. A firm needing a web server for months or years may lease a dedicated server in a data center. A college instructor needing a small dedicated cluster for a few hours each week can obtain one from Amazon EC2 <ref type="bibr" target="#b38">[37]</ref>. Each of these solutions is specialized to a specific usage scenario and only partially supports other usage patterns (if at all).</p><p>In our research, we seek to develop a resource provisioning model and system that can support many such usage scenarios at the same time. Motivated by this general goal, we focus in this paper on the specific problem of supporting workloads that combine requests for resources during a specific time period ("advance reservation" requests) and requests for resources whenever available ("best-effort" requests). The need for advance reservations arises, for example, when applications require coscheduling of multiple resources <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b5">5]</ref>, in urgent computing applications <ref type="bibr" target="#b25">[25]</ref>, in applications in which resource availability must coincide with some other event, such as a class <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b39">38]</ref>, and in applications expressible as a workflow of independent tasks that can be executed more efficiently by multilevel scheduling methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b16">16]</ref>. The need for best-effort resource allocation arises in many contexts and is supported by batch schedulers.</p><p>Although batch schedulers can and do include support for advance reservations, this mechanism often leads to utilization problems <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22]</ref>, caused by the need to drain best-effort jobs from resources before a reservation can begin. Although checkpointing-based resource preemption can be used to minimize this effect, it requires either a checkpointing-capable OS (system-level checkpointing) or linking applications against checkpointing libraries (user-level checkpointing), both of which require specialized software that may not be available on all sites.</p><p>We propose here an approach that uses leasing, and not jobs, as the fundamental resource provisioning abstraction for both besteffort and advance reservation requests. The job abstraction used by batch schedulers ties together the provisioning of resources for the job and its execution, with resource provisioning typically happening as a side-effect of job submission. This forces resource consumers to use the provisioned resources through the job execution interfaces provided by a batch scheduler, instead of accessing the raw resources directly (e.g., in the simplest case, by logging into the resources). As a resource-provisioning mechanism, the job abstraction is insufficient for applications that are not easily expressible as jobs or where direct access to resources is required. A lease, on the other hand, is used only to provision resources, which can then be used at the user's discretion, including running jobs.</p><p>We develop lease terms for leases with best-effort availability as well as stricter availability constraints for advance reservation leases. Furthermore, using virtualization-based resource managers, we demonstrate that both can coexist without the utilization problems of advance reservations, without requiring applications to be modified (as required by user-level checkpointing) or needing to use a specific operating system (as required by system-level checkpointing). Hence, resource providers can satisfy the needs of batch computations, currently prevalent in scientific computing, while at the same time accommodating other usage scenarios. Our approach is based on using virtualization to suspend, migrate, and resume computations in virtual machines (VMs). In addition to leveraging these features of the VMs, we are in this way also able to provide leases with customized application environments.</p><p>Scheduling of best-effort and advance reservation jobs has been extensively studied <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>, but always within the context of batch job schedulers, and not resource leasing. Irwin et al. <ref type="bibr" target="#b17">[17]</ref> (Shirako), Adabala et al. <ref type="bibr" target="#b1">[1]</ref> (In-VIGO), Emeneker and Stanzione <ref type="bibr" target="#b7">[7]</ref> (Dynamic Virtual Clustering), Fallenbeck et al. <ref type="bibr" target="#b8">[8]</ref> (XGE), Kiyanclar et al. <ref type="bibr" target="#b19">[19]</ref> (Maestro-VC), Nishimura et al. <ref type="bibr" target="#b24">[24]</ref> and Yamasaki et al. <ref type="bibr" target="#b36">[35]</ref> (Virtual Clusters-on-the-Fly), and Ruth et al. <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27]</ref> (VIOLIN/VioCluster) have researched VMs as a mechanism for resource management but without exploring the implications of running workloads that combine best-effort and advance reservation requests. Our previous work defines the virtual workspace abstraction to represent execution environments that are dynamically and securely deployed on remote resources through interoperable interfaces <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b12">12]</ref> and an accurate resource model for VM-based resource management <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b33">32]</ref>.</p><p>In summary, our paper makes the following contributions:</p><p>• We describe a leasing-based architecture that integrates support for both best-effort and advance reservation leases.</p><p>• We describe an implementation of the lease-based architecture integrating those types of leases and explain how it will work within the current batch scheduling model.</p><p>• We show experimentally that our VM-based resource manager can provide resources more efficiently in certain cases, as measured by several resource utilization metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RESOURCE LEASES</head><p>We define a lease as a negotiated and renegotiable agreement between a resource provider and a resource consumer, where the former agrees to make a set of resources available to the latter, based on a set of lease terms presented by the resource consumer. In this work we use "agreement" and "agreement terms" as defined by the WS-Agreement specification <ref type="bibr" target="#b2">[2]</ref>.</p><p>The lease terms encompass the hardware resources required by the resource consumer, such as CPUs, memory, and network bandwidth; a software environment required on the leased resources; and an availability period during which a user requests that the hardware and software resources be available. In previous work <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b34">33]</ref>, we focused on lease terms for hardware resources and a software environment. Our focus here is on the availability dimension of a lease. We consider here the following terms.</p><p>• Start time may be unspecified (a best-effort lease) or specified (an advance reservation lease). In the latter case, the user may specify either a specific start time or a time period during which the lease start may occur.</p><p>• Maximum duration refers to the total maximum amount of time that the leased resources will be available.</p><p>• Leases can be preemptable. A preemptable lease can be safely paused without disrupting the computation that takes place inside the lease.</p><p>We have developed an XML Schema for the above terms, extending the schema for resource allocations presented by Freeman et al. <ref type="bibr" target="#b13">[13]</ref>, but do not include it here because of space constraints. The XML representation of these terms could be used in a web service such as the Virtual Workspace Service <ref type="bibr" target="#b18">[18]</ref> to negotiate a lease. In particular, a resource consumer would present the desired lease terms to the service, which would determine whether to accept or reject the request, and publish the accepted lease terms (which may be concretized by the service if the lease is accepted).</p><p>In this paper, we assume for simplicity that advance reservation leases are nonpreemptable and best-effort leases are preemptable. Furthermore, since we focus on availability, we make the simplifying assumptions that leases request a number of compute nodes with the same hardware requirements in each node and that the software environment is encapsulated inside a disk image (which could be used to reimage a hard drive or be used as a VM image). We further assume that, when determining whether to preempt a lease, a resource owner takes into consideration only the lease's preemptability (i.e., no other factors, such as priorities, would result in a decision not to preempt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESIGN AND IMPLEMENTATION</head><p>This section describes Haizea, a lease management architecture that enables resource consumers to negotiate the two kinds of leases described in the previous section. This architecture is composed of several components. Leases are requested through an interface component by using the XML language described in Section 2. These requests are then passed to a scheduler component. An enactment component is responsible for sending commands to nodes to start/end VMs, suspend/resume VMs, and initiate transfers. In this section we first describe the resource model for this architecture. Then we describe how leases are scheduled. Since a batch job scheduler could be a resource consumer (requesting leases to run best-effort jobs or advance reservation jobs), we discuss how our architecture can be used in conjunction with a job scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Resource Model</head><p>We assume that the lease management architecture manages W identical nodes each with a Virtual Machine Monitor (VMM) allowing the execution of virtual machines. Each node has P CPUs, M megabytes (MB) of memory, D MB of local disk storage, and a disk read/write transfer rate of Hr and Hw MB/s. We divide the disk space of each node into cache space Dp (used for caching disk images required by the VMs) and working space Dw (used for storing disk images of active and paused VMs), such that D = Dp + Dw. We assume that all required disk images are available in a repository from which they can be transferred to nodes as needed. For simplicity, we assume that the repository and nodes have the same characteristics and that all are connected at a bandwidth of B MB/s by a switched network with a nonblocking switch (i.e., the maximum transfer rate of the switch can support all the transfers).</p><p>A lease is implemented as a set of n VMs, each allocated resources described by a tuple (p, m, d, b), where p is number of CPUs, m is memory in MB, d is disk space in MB, and b is network bandwidth in MB/s. A disk image I with a size of sI MB must be transferred from the repository to a node's cache space before the VM can start. When transferring a disk image to multiple nodes, we use multicasting and model the transfer time as s I B . Once a VM disk image is transferred to a node's cache space, one or more disk image instances (tied to a specific virtual machine) of the same size can be created by locally copying the cached disk image to the node's working space. We assume that this latter copy operation is performed gradually, using copy-on-write, and incurs no additional cost. We also assume that once a VM is terminated its disk image instance can be discarded.</p><p>If a lease is preempted, it is suspended by suspending its VMs, which may then be either resumed on the same node or migrated to another node and resumed there. Suspension results in the creation of a memory state file (the contents of the VM's memory) on the node where the VM is running, and resumption requires reading that image back into memory and then discarding the file. The size of the memory state file is m, and the time to suspend and resume a VM is m Hw and m Hr seconds, respectively. When a suspended VM is migrated to a different node, its disk image instance and memory state file are transferred, requiring s I +m B seconds. Suspending multiple VMs communicating with each other can potentially disrupt communication between the nodes (e.g., messages that are lost "in flight" when the VMs are suspending). Emeneker et al. <ref type="bibr" target="#b6">[6]</ref> showed that this problem can be avoided by suspending and resuming all VMs simultaneously, using NTP to synchronize these events, resulting in "a coherent network state with no timeouts." Since there are no TCP timeouts to recover from, and all suspend/resume operations must happen simultaneously, we assume that suspending and resuming multiple VMs also requires m Hw and m Hr seconds, respectively.</p><p>We assume in this paper that P = 1, that only one VM can be run per node, and that there is no contention between network traffic generated by applications running within VMs and network traffic associated with image movement-either because the applications running inside the VMs do not generate substantial network traffic or because there is a separate network for application network traffic. In future work, we will investigate the implications of relaxing these assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lease Scheduling</head><p>We leverage our ability to suspend, migrate, and resume VMs to implement policies that seek to maximize scheduling performance by preempting certain (best-effort, preemptible) leases to make room for other (advance reservation, nonpreemptible) leases. In this way, we can use resources more efficiently than approaches that depend on node draining. In addition, we manage the transfer of disk images to ensure that required images are available on nodes when an advance reservation lease is scheduled; integrate disk image transfer into the scheduling process; and avoid image transfer operations, whenever possible, by managing caches of images on nodes. This model of explicitly scheduling VM overhead, instead of deducting it from the lease's execution time, was introduced in previous work <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b33">32]</ref>. In effect, our scheduler schedules each lease as a workflow of actions such as image transfer, deployment, suspend/resume, and migration.</p><p>A lease is represented internally in our architecture by a lease descriptor that includes all the lease's terms (using the XML representation described in Section 2), along with zero or more resource reservations, representing the workflow of actions required by that lease. The scheduler keeps track of the available resources by using a slot table: when an action gets scheduled the corresponding resource reservations are marked in the slot table. The table has three dimensions: resource usage (the resource tuple (p, m, d, b) described earlier), the number of nodes, and the duration. In addition to representations for all available nodes, the slot table includes the image repository, so that the bandwidth required for image transfers from the repository to nodes can be accounted for. In the case of a preemptible lease, the working space on the disk is additionally decreased by m to hold the VM's memory in case it is suspended.</p><p>As noted earlier, a disk image required by a VM that is to be executed on a node is first transferred to the node's image cache. Images in a node's cache are reference counted based on the leases that depend on them. An image's reference count is decremented each time a lease that depends on this image ends. If the image pool is full when an image is added, the scheduler removes the least recently used image with reference count equal to zero.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> summarizes how lease descriptors make their way through the lease manager, from submission to completion. Incoming advance reservation leases are always scheduled right away, whereas best-effort leases are put on a queue. Each scheduled lease has a pointer to the resources it is assigned in the slot table, so that those resources can be released at the end of the lease.</p><p>Each incoming best-effort lease is placed at the end of the queue. The scheduling function periodically evaluates the queue, using an aggressive backfilling algorithm <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">23]</ref>, to determine whether any best-effort leases can be scheduled. The scheduler first checks, for each node, the time when the required disk image can be made available on the node (by using a cached image, by piggybacking on a scheduled multicast, or by scheduling a separate transfer), and the time the node's resources will be available and takes the later of the two. These potential start times are sorted, and the n nodes with the earliest times are selected, giving preference first to nodes where the lease can run without having to be suspended; the latest time in this group is the time for which the lease will be scheduled. The scheduling of the lease also results in the scheduling of the dependency actions, that is, image transfers (if needed) or incrementing of reference counts of a cached image.</p><p>Note that a lease may be scheduled even if there is a "blocking" lease, such as an advance reservation lease scheduled in the future, that would prevent a best-effort lease to run for its entire requested duration before the blocking lease starts. In such a case, the VMs in the lease may be suspended before a blocking lease. The remainder of a suspended lease is put back in the queue, according to its submission time, and scheduled as if it were another best-effort request. If the scheduler maps the remaining duration to nodes different from those where the lease was suspended, the VM image and memory state file will be transferred to the new nodes before the lease can resume.</p><p>When an advance reservation lease is requested, the scheduling function first determines whether the request is viable, that is, whether sufficient resources are available at the requested time and whether necessary image transfers can be scheduled as needed. If the lease is found not to be viable, it is rejected. Other than this, we assume an "accept all" policy, such that advance reservation leases are always accepted as long as there are sufficient resources available for them. If the lease is accepted, the scheduler determines what nodes can support the lease. The scheduler first chooses nodes that will not require preempting another lease and then chooses nodes for which the required image can be found in the node's image cache (we found that reversing this policy had no significant effect in the context of our workloads). The exception to this policy is when a lease will be viable only if images are reused, in which case priority is given to nodes with a reusable copy of the required image.</p><p>For advance reservation leases, image transfers are scheduled using an earliest deadline first (EDF) algorithm <ref type="bibr" target="#b26">[26]</ref>, where the deadline for the image transfer is the start time of the lease. Since the start time of an advance reservation lease may occur long after the lease request, we modify the basic EDF algorithm so that transfers take place as close as possible to the deadline, preventing images from unnecessarily consuming disk space before the lease starts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Job and Lease Management</head><p>Our architecture is designed so that it can potentially act as a resource provisioning backend for existing job management architectures (e.g., as a scheduler for Torque or as a decision-making module for SGE's schedd), allowing users to continue to submit their computations in the form of jobs. When a job is submitted, the execution manager must request a best-effort lease for the job. Once the lease is accepted, the lease manager and the execution manager communicate through a series of events to coordinate their actions. The following events are sent from the lease manager to the execution manager:</p><p>Lease ready: Once a lease request is accepted, the lease manager must still provision resources on a best-effort basis, which may require queuing the lease request until it can be satisfied. When resources become available, the lease manager informs the execution manager of the resources that have been allocated to the job.</p><p>Lease suspended/resumed: If a best-effort lease must be preempted, the lease manager notifies the execution manager that the resources where the job is running will become unresponsive, so that it will not assume the job has failed or the nodes have crashed. Another event is sent when the lease is resumed.</p><p>The execution manager, on the other hand, will send a "Release lease" event when a job concludes its execution or is cancelled, and no longer requires the leased resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>We used a discrete event simulator to evaluate the performance of our lease management architecture for various workloads. We present here our simulation model, the workloads we used, and the results of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Model</head><p>We perform a discrete event simulation of the submission, scheduling, and execution of best-effort and advance reservation leases. The input to the simulator is (a) description of the simulated cluster and (b) a trace: a sequence of best-effort and advance reservation lease requests, each specified in the format described in Section 2 and annotated with the time at which the request was submitted. The submission of a lease results in an event that causes the scheduler adding entries to the slot table (if the lease is accepted). The start and the end of an allocation in the slot table is also treated as an event that causes the scheduler to reevaluate the schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workloads</head><p>We construct the workloads used in our experiments by adapting the SDSC Blue Horizon cluster job submission trace from the Parallel Workloads Archive <ref type="bibr" target="#b40">[39]</ref>. In general terms, we take a set of job submission requests from that trace and treat them as a set of best-effort lease requests and then insert an additional set of advance reservation requests. Keeping the best-effort requests fixed, we vary the advance reservation requests to obtain a set of 72 different workloads.</p><p>More specifically, we use as our best-effort requests the first 30 days of requests in the SDSC Blue Horizon trace, For each of these 5,545 requests, we extract from the trace its submission time, requested duration, and requested number of nodes. (In this 30-day extract, 66.85% of the requests have a requested duration of one hour or less, and 64.09% of the requests require four nodes or less.) We also set the per-node resource allocation to p = 1 and m = 1024. For simulation purposes, we take from the trace the actual run time, which is usually less than the requested job duration. Thus, in our simulations, best-effort leases frequently complete "prematurely."</p><p>To generate our workloads, we then interleave with this set of best-effort requests a set of advance reservation requests, generated according to three parameters:</p><p>• ρ, the aggregate duration of all advance reservation leases in a trace, computed as a percentage of the total CPU hours in the simulation's run, which is the number of nodes multiplied by the time when the last best-effort request is submitted. We use the values ρ =5%, 10%, 15%, 20%, 25%, and 30%. (We do not explore larger values because the trace's utilization is 76.2%, according to the Parallel Workloads Archive.)</p><p>• δ, the duration of each advance reservation lease, for which we use average values of 1, 2, 3, or 4 hours. (The duration is selected randomly from a range spanning δ ± 30m.)</p><p>• ν, the number of nodes requested by each lease, for which we use three ranges, from which the value is selected using a uniform distribution: small (between 1 and 24), medium (between 25 and 48), or large (between 49 and 72).</p><p>Our use of ρ allows us to compare results obtained with traces with different duration (δ) and size (ν) parameters, by ensuring that all traces with the same ρ value involve the same amount of extra work. Given values for ρ, δ, and ν, we then determine the arrival times of the advance reservation requests as follows. First, we determine the number of requests that will be generated, and we divide that number into 30 days to obtain an average interlease interval i. Then, we choose the intervals between requests at random in the range (i -1 hour, i + 1 hour). Thus, the smaller the average lease duration, the more frequent is the arrival of requests (since there will be more advance reservation lease requests). Similarly, the smaller the average number of nodes, the higher the frequency. We further constrain advance reservation lease requests to involve an advance notice of exactly H hours. In the experiments described below, we set H = 24, unless otherwise noted. As with the besteffort requests, the advance reservation lease requests have a pernode resource allocation of p = 1 and m = 1024.</p><p>In our experiments, we explore every combination of the parameters ρ, δ, and ν, for a total of 72 workloads. We refer to workloads using the notation [ρ%/δH/ν] (e.g., [10%/2H/medium]).</p><p>We perform experiments using three criteria for disk image selection:</p><p>single: assign the same disk image to each request.</p><p>uniform: assign one of 37 possible images to each requests at random using a uniform distribution.</p><p>skewed: assign one of 37 possible images using a distribution where 7 images each have a 10% probability of being selected, and the remaining 30 images each have a 1% probability of being selected.</p><p>All images have a disk size of 4GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Simulated Cluster</head><p>Our simulated cluster is modeled after the SDSC Blue Horizon cluster. It comprises 144 single-CPU nodes, each with 40 GB of disk (with Dp = 20 and Dw = 20) and 1 GB of memory, connected with a switched Ethernet network (100 Mb/s, we conservatively assume a 10 MB/s bandwidth). For the purposes of estimating the time required to suspend and resume a VM, we assume Hw and Hr to be 50 MB/s, based on results presented by Fallenbeck et al. <ref type="bibr" target="#b8">[8]</ref>. We conservatively assume that the sum of the boot-up and shutdown time of a VM does not exceed 20 seconds. To account for the slowdown produced by running inside a VM, we assume that any computation running inside a VM requires 5% more time to run. Further, we assume that the time required to send commands from the resource manager to the nodes is negligible and that the hardware will not behave erratically, and we inject no hardware failures into the simulated cluster. We plan to relax the no-failure assumption in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments</head><p>We ran our simulator using each of the workloads described above, with the following configurations: NOVM-NOSR -No VM, no suspend/resume: Leases do not run on VMs. We follow the resource model of Section 3, except there is no VM image to transfer and no 5% runtime slowdown. In addition, leases cannot be suspended or resumed; thus, a preempted lease is cancelled and requeued.</p><p>NOVM-SR -No VM, with suspend/resume and migration: Like NOVM-NOSR, but leases can be suspended and resumed. This configuration represents a job scheduler capable of checkpointing and migrating any job. We assume that, to be able to checkpoint any application, system-level checkpointing is used, and that suspending a lease requires saving the entire memory to disk, as would happen in the VM cases.</p><p>VM-PREDEPLOY -With VM, single predeployed image: The leases run on VMs, following the resource model described in Section 3, but all leases use the same VM image, which is assumed to be predeployed on the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VM-MULT -With VM, multiple images:</head><p>As VM-PREDEPLOY, but using the uniform list of requests and removing the assumption that images are predeployed (i.e., an image transfer has to be scheduled before the VM can start). Images are not reused on the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VM-REUSE-UNIFORM and VM-REUSE-SKEWED -With</head><p>VM, multiple images with image reuse: Same as VM-MULT, but reusing images on the nodes. We use the uniform and skewed lists of requests.</p><p>During the experiments, we observe, for each lease, the times ta (the arrival time, or time when the lease request is submitted), ts (the start time of the lease) and te (the time the lease ends). At the end of an experiment, we compute the following metrics: all-best-effort: We define all-best-effort as the time from the start of the trace to when the last best-effort request is completed. We normalize this value by presenting the relative difference between this time and the time required to run all the besteffort requests without advance reservation leases in configuration NOVM-NOSR (this time is 2,674,265 seconds, or roughly 30.95 days). Thus, a value x indicates that an experiment took 2, 674, 265 • x to run (with x = 1.0 meaning that the experiment took the same time as the baseline case).</p><p>Wait time of best-effort requests: We define wait time as ts -ta, the time a best-effort request must wait before it starts running.</p><p>Bounded slowdown of best-effort requests <ref type="bibr" target="#b10">[10]</ref>: If tu is the time the lease would take to run on a dedicated physical system (i.e., not in a VM), the lease's slowdown is te-ta tu . If tu is less than 10 seconds, the bounded slowdown is computed the same way, but assuming tu to be 10 seconds <ref type="bibr" target="#b10">[10]</ref>.</p><p>When computing the last two metrics, we discard the first 5% of measurements, to avoid ramp-up effects. We retain the rampdown period because this is where we observe the wait times and slowdowns of the requests that languish in the queue until there are no more advance reservation leases.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows all-best-effort results for all experiments. This metric provides a good measure of utilization, taking into account both VM runtime slowdown and the overhead of VM deployment. We observe that using suspend/resume and migration (with and without VMs) results in a shorter run time than NOVM-NOSR in every case. In fact, using suspend/resume and migration produces a slowdown of, at most, 10% relative to not injecting any advance reservation leases at all, whereas not using suspend/resume can produce a slowdown of up to 32.97%. Additionally, the duration of the advance reservation leases is more likely to affect this metric in the NOVM-NOSR configuration, with shorter-duration (and thus shorter-interval) leases producing the worst results.</p><p>As mentioned previously, all advance reservation leases were submitted to the scheduler with an advance notice of 24h. In preliminary tests with fewer workloads, based on a 15-day trace, we investigated the effect of using different advance notices (1h, 3h, 6h, 12h, and 24h). We observed that the advance notice can have an effect when suspend/resume is not used, with all-best-effort tending to increase for shorter advance notices (e.g., in the most extreme case, observed with workload [10%/2H/small] using a 1h advance notice, all-best-effort increases by 46.69% compared to using a 24h notice). We attribute this effect to the increasing likelihood that an advance reservation lease will preempt a best-effort lease that has already been scheduled, necessitating that it be cancelled and resubmitted. When using suspend/resume, on the other hand, the effect is negligible.</p><p>While all-best-effort gives a good measure of effective utilization (indicating how much faster an entire workload will be run from beginning to end), it does not say much about individual leases, which requires looking at the other two metrics. For example, an inspection of the run history reveals that the longer total run time when not using suspend/resume (in NOVM-NOSR) is due to besteffort leases that remain in the queue and are not run until the ramp down period, when no more best-effort leases are being submitted. These leases remain in the queue because the advance reservation leases prevent them from being scheduled, especially when the interval between advance reservation leases is short. Because of space limitations, we are unable to present the data for wait time and slowdown of best-effort requests from all the cases; instead, we constrain most of our discussion to the cases [10%/3H/medium], [20%/3H/medium], and [30%/3H/medium], which are representative of the trends that we observe across all cases.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the total run time, average wait time, and average slowdown for these three cases. Although the run time metric is always the worst (longest) in the NOVM-NOSR configuration, in the other two metrics the NOVM-NOSR configuration results in better performance (shorter average wait times and smaller average slowdowns) in most cases. We attribute this result to the fact that, without suspend/resume, the scheduler must rely heavily on backfilling to efficiently use the time and space before a blocking lease (such as an advance reservation lease). This behavior will favor short leases, which "skip the queue" when used as backfill. Since the majority of best-effort leases in our trace are shorter than one hour, they are ideal candidates for backfilling. Thus, many such leases end up with short wait times and small slowdowns. However, suspend/resume need not look ahead for shorter best-effort leases when backfilling: it can simply take the next best-effort lease, even if long, knowing that it can suspend this lease if it has not run to completion before a blocking lease is scheduled. Paradoxically, the aggregate effect of not preferentially selecting shorter leases for backfill can be to increase average wait time and slowdown.</p><p>We can support this observation by looking at how wait times and the slowdown vary with requested duration and number of nodes. Figure <ref type="figure" target="#fig_2">3</ref> shows the regression curves (using the Lowess smoother <ref type="bibr" target="#b4">[4]</ref>) for these metrics and variables for [20%/3H/medium]. The left two graphs show how wait time and slowdown vary with the requested lease duration. We see that short leases in the NOVM-NOSR configuration have shorter wait times and smaller slowdowns than all other configurations (which use suspend/resume). However, the tendency is for both the wait time and slowdown to increase as the requested duration increases since, as noted above, backfilling favors short requests. When using suspend/resume, on the other hand, we observe that the trend is for the wait times to not vary with the requested duration and for the slowdowns to exhibit a slight decreasing trend. Thus, all requests are treated more fairly, although the overall average does increase (the shorter requests, which make up the majority, have longer wait times because they no longer "skip the queue," thanks to backfilling). We note that this is a desirable effect in many scenarios as it provides a more "democratic" treatment for different application types. Fallenbeck et al. <ref type="bibr" target="#b8">[8]</ref> used similar strategies to equalize wait times for different application groups.</p><p>If we look only at the suspend/resume configurations, we can see that VM-MULT performs the worst, since no attempt is being made to reuse VM images on the nodes. Adding image reuse (VM-REUSE-UNIFORM and VM-REUSE-SKEWED) reduces wait times and slowdowns, although performance is still not as good The center two graphs show how the wait time and slowdown vary with the number of requested nodes. When looking just at the effect of node counts, we see that adding suspend/resume has little effect. In fact, we can observe the same trend across all configurations: wait time and slowdown do not vary when the node count is less than ten but tend to increase for larger node counts. If we observe how the wait time and slowdown vary with the requested CPU time (the product of the requested duration and the requested number of nodes) in the right two graphs, we see trends similar to those when looking just at the requested duration, except that the upward trend as the CPU time increases is more pronounced in the NOVM-NOSR configuration. This upward trend also is evident in the suspend/resume configurations, but only for large values of CPU time.</p><p>In general, we observe across all 72 workloads that, as ρ increases, wait times and slowdowns tend to increase more sharply in the NOVM-NOSR configurations, but tend not to vary in the configurations that use suspend/resume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>The problem of scheduling best-effort jobs has been extensively studied. The most commonly used optimization is backfill <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr">9]</ref>. We leverage the suspend/resume capability of virtual machines to extend backfill algorithms, allowing leases to be suspended before the start of a reservation, whether an advance reservation lease or a parallel best-effort lease reserved by a backfilling algorithm. Jobs running on nonvirtualized machines can also be preempted and have their state saved to disk (checkpointing), to achieve more fault-tolerant software or to optimize job scheduling. However, OS-level checkpointing requires applications to be compiled for the checkpointing-aware OS, and may require relinking an application with new libraries (e.g., the BLCR kernel <ref type="bibr" target="#b15">[15]</ref> can checkpoint MPI applications only if these are linked with BLCR-aware MPI libraries). The use of VMs allows any computation to be transparently checkpointed without modifying the application or porting it to a new architecture. Although nodes must use a specific VMM kernel, the guest VMs can potentially run any operating system.</p><p>Several modern batch schedulers, such as SGE and Maui, support advance reservations, which allow users to request strict availability periods, similar to the advance reservation leases described in this paper. Once a reservation is made, however, users must still interact with those resources using the job abstraction (typically through a queue that is created specifically to submit jobs to the reserved resources) and cannot request a custom software environment. Additionally, advance reservations can cause utilization problems in clusters <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22]</ref> as a result of the strict constraints they impose on the job schedule, which is only partially alleviated by backfilling. Nonetheless, there is a growing interest in supporting advance reservation capabilities in systems such as TeraGrid <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b22">22]</ref>, for a variety of usage cases, such as coscheduling of large parallel jobs across sites. Our work facilitates support of the leasing semantics required by advance reservations, while having a smaller impact on utilization and queue wait times than do existing approaches.</p><p>Fallenbeck et al. <ref type="bibr" target="#b8">[8]</ref> extended the SGE scheduler to use the save/restore functionality of Xen VMs, allowing large parallel jobs to start earlier by suspending VMs running serial jobs, and resuming them after the large parallel job finished. Emeneker et al. <ref type="bibr" target="#b7">[7]</ref> extended the Moab scheduler to support running jobs inside VMs, and explored different caching strategies for faster VM image deployment on a cluster. However, both studies use VMs only to support the execution of best-effort jobs and do not currently schedule image transfers separately; moreover, the Moab work does not integrate caching information into scheduler decisions.</p><p>The OAR batch scheduler, used in the Grid5000 <ref type="bibr" target="#b3">[3]</ref> architecture, is coupled to a node reconfiguration system that can deploy a software environment, using disk images, on a node before a job starts running. However, these disk images are used to reimage physical Several groups have investigated the use of multilevel scheduling at the cluster level by using existing job schedulers to provision compute resources, which are then managed by a separate scheduler, instead of being limited to using the job execution semantics provided by the job scheduler that provisioned the resources. The Condor scheduler's glidein mechanism <ref type="bibr" target="#b14">[14]</ref> was the first to apply this model on compute clusters, by starting (or "gliding in") Condor daemons on the provisioned resources. The MyCluster project <ref type="bibr" target="#b35">[34]</ref> similarly allows Condor or SGE clusters to be overlaid on top of TeraGrid resources to provide users with personal clusters. The Falkon task scheduler <ref type="bibr" target="#b16">[16]</ref> can also be deployed through a GRAM interface on compute resources and is optimized to manage the execution of lightweight tasks. In all these approaches, resource provisioning is not completely decoupled from the job scheduler, and there is no way to obtain a custom software environment.</p><p>Several datacenter-based solutions have emerged that completely decouple resource provisioning from job submission, including server hosting providers that provide long-term leases over virtualized resources.</p><p>More recently, Amazon's EC2 <ref type="bibr" target="#b38">[37]</ref> introduced the notion of cloud computing, where virtual machines are dynamically provisioned immediately with customized software environments and use is charged by the hour. These solutions excel at providing users with exactly the software environment required, and most provide a large number of hardware options; however, they support few types of availability periods. Server hosting providers are geared toward long availability periods, while EC2 requires resources to be provisioned immediately.</p><p>Several groups have explored the use of VMs to create "virtual clusters" on top of existing infrastructure. Nishimura et al.'s <ref type="bibr" target="#b24">[24]</ref> system for rapid deployment of virtual clusters can deploy a 190-node cluster in 40 seconds. Their system accomplishes this impressive performance by representing software environments as binary packages that are installed on the fly on generic VM images. They optimize installation by caching packages on the nodes, thus reducing the number of transfers from a package repository. This approach limits the possible software environments to those that are expressible as installable binary packages (which is not always possible) but does provide a faster alternative to VM image deployment if the installation time is short enough. Yamasaki et al. <ref type="bibr" target="#b36">[35]</ref> improved this system by developing a model for predicting the time to completely set up a new software environment on a node, allowing their scheduler to choose nodes that minimize the time to set up a new virtual cluster. Whereas our model assumes homogeneous nodes and uses disk images to encapsulate software environments (and thus can use a simple formula for estimating the time to set up a software environment), Yamasaki et al.'s model takes node heterogeneity into account and uses the parameters of each node (CPU frequency and disk read/write speeds) and empirical coefficients to predict the time to transfer and install all required packages, and then reboot the node. However, their model does not include an availability dimension and assumes that all resources are required immediately (a subset of advance reservation leases), while our model allows for deployment overhead to be scheduled in different ways depending on the lease's requested availability (best-effort or advance reservation).</p><p>The Shirako system <ref type="bibr" target="#b17">[17]</ref> uses VMs to partition a physical cluster into several virtual clusters. This work also relies on the lease abstraction, allowing users to obtain resource leases that can be redeemed in the future. Their model assumes that any overhead involved in deploying and managing the VM will be deducted from the lease's availability. In contrast, we seek to provide a guaranteed set of resources during an agreed-upon availability period, adequately managing overhead in such a way that this guarantee is not breached. Previous work <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b33">32]</ref> showed that not managing this overhead can prevent advance reservation leases from starting at their specified start time and thus would prevent a resource provider from entering into agreements that guarantee the lease's availability period.</p><p>Other groups have explored a variety of challenges involved in deploying and running a virtual cluster, including virtual networking and load balancing between multiple physical clusters (VIO-LIN/VioCluster <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27]</ref>), automatic configuration and creation of VMs (In-VIGO <ref type="bibr" target="#b1">[1]</ref> and VMPlants <ref type="bibr" target="#b20">[20]</ref>), and communication between a virtual cluster scheduler and a local scheduler running inside a virtual cluster (Maestro-VC's two-level scheduling <ref type="bibr" target="#b19">[19]</ref>). However, they do not explore workloads that combine best-effort and advance reservation requests, nor do they schedule deployment overhead of VMs separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FUTURE WORK</head><p>Our architecture makes certain simplifying assumptions on several policy points. Most notably, we (a) accept all lease requests, except advance reservation leases for which no resources can be provisioned (e.g., because an advance reservation lease has already been scheduled on those resources) and (b) assume that best-effort requests are always preemptible and advance reservation leases are not. In future work, we will explore policies that accept or reject leases based on criteria other than resource availability and the effect of different policies on performance. For example, we have only scratched the surface of the effect that advance notice (of advance reservation leases) can have on global performance. If a system administrator allows for short advance notices, users will presumably prefer to request advance reservation leases instead of having their requests wait in a queue, increasing the advance reservation lease workload, which this work has shown can result in worse performance. On the other hand, requiring long advance notices will reduce the value of advance reservations to users. Future work will investigate the effect that these policy points have on performance and ways to configure them in order to balance the requirements and needs of both resource providers and users. We will also look at preemptible advance reservation leases and nonpreemptible best-effort leases and will investigate policies that take into account additional factors (such as priorities) when deciding when a lease should, or should not, be preempted.</p><p>We also plan to develop a prototype implementation of our architecture that can manage real resources. An important issue will be handling hardware failures. Our experiments currently assume no hardware failures. Our scheduler could easily deal with failures by cancelling and resubmitting any best-effort leases affected by a failure. However, this strategy can be inefficient if, for example, a long lease is cancelled when most of its work is done. We can minimize the impact of failures by performing periodic checkpoints (using VM checkpointing mechanisms), at the cost of extra disk space. Another concern is that cancelling an advance reservation lease breaches the agreement made with the resource consumer. We will explore strategies that make advance reservation leases more resilient, such as overreserving resources in anticipation of failures.</p><p>We will also explore other types of leases, such as deadlinesensitive best-effort leases, and more complex lease terms, such as availability periods divided into multiple segments, each with different hardware resources (e.g., a lease may need network bandwidth during the first 10 minutes of an application's execution, to allow download information from a third site, but not require network connectivity for the remainder of the lease). Additionally, we will explore dynamic renegotiation of lease terms and lease metascheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We have described a lease management architecture that allows resource consumers to request resource leases with semantics that encompass hardware resource, software environments, and availability. We have focused on resource availability and, in particular, on two types of availability that are frequently required in practice: preemptible best-effort, roughly corresponding to the availability requirements of batch computations, and nonpreemptible advance reservation. Resource leases are a general-purpose abstraction for provisioning resources. They do not limit the resource consumer to access and use those resources through constrained interfaces, such as those of a job scheduler, which requires users to specify a single computation to be performed on the requested resources.</p><p>We have presented experimental results that show that, when using workloads that combine best-effort and advance reservation requests, a VM-based approach with suspend/resume can overcome the utilization problems typically associated with the use of advance reservations. Our results show that, even in the presence of the runtime overhead resulting from using VMs, a VM-based approach results in consistently better total execution time than a scheduler that does not support task preemption, and only slightly worse performance than a scheduler that does support task preemp-tion. Measurements of wait time and slowdown for the same experiments show that, although the average values of these metrics increase when using VMs, this effect is due to short leases not being preferentially selected as backfill before a blocking lease. In effect, a VM-based approach does not favor leases of a particular length over others, unlike systems that rely more heavily on backfilling. Moreover, we show that, although supporting the deployment of multiple software environments, in the form of multiple VM images, requires the transfer of potentially large files, this deployment overhead can be minimized through the use of image transfer scheduling and caching strategies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Submission and scheduling of a lease</figDesc><graphic coords="3,316.81,54.80,246.03,224.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Each data point in this graph represents the all-best-effort metric in each experiment. The x axis represents ρ and the y axis represents the value of all-best-effort (normalized as described in the text). The graphs are grouped by ν (the number of nodes requested by each lease) and the experiment configuration. The symbol of each point denotes the value of δ (see legend).</figDesc><graphic coords="6,78.91,53.80,451.91,237.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Lowess curves for [20%/3H/medium]</figDesc><graphic coords="7,53.80,53.80,502.12,242.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experiment running times, average waiting times, and average slowdowns for [</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>10%/3H/medium], [20%/3H/medium], and [30%/3H/medium]</head><label></label><figDesc></figDesc><table><row><cell cols="3">Time to complete best-effort leases,</cell><cell></cell></row><row><cell cols="4">relative to time without advance reservation leases</cell></row><row><cell>ρ →</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell></row><row><cell>NOVM-NOSR</cell><cell cols="3">3.91% 8.66% 23.93%</cell></row><row><cell>NOVM-SR</cell><cell cols="2">-0.76% 0.10%</cell><cell>4.14%</cell></row><row><cell>VM-PREDEPLOY</cell><cell cols="2">-0.27% 1.58%</cell><cell>7.19%</cell></row><row><cell>VM-MULT</cell><cell cols="3">0.60% 3.95% 12.21%</cell></row><row><cell>VM-REUSE-UNIFORM</cell><cell cols="3">0.03% 3.15% 10.74%</cell></row><row><cell>VM-REUSE-SKEWED</cell><cell cols="3">-0.36% 2.48% 10.43%</cell></row><row><cell cols="4">Average waiting time for best-effort leases,</cell></row><row><cell cols="3">in thousands of seconds</cell><cell></cell></row><row><cell>ρ →</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell></row><row><cell>NOVM-NOSR</cell><cell>16.91</cell><cell>23.91</cell><cell>68.95</cell></row><row><cell>NOVM-SR</cell><cell>6.18</cell><cell>8.14</cell><cell>14.84</cell></row><row><cell>VM-PREDEPLOY</cell><cell>8.05</cell><cell>12.17</cell><cell>22.89</cell></row><row><cell>VM-MULT</cell><cell>15.69</cell><cell>34.56</cell><cell>78.83</cell></row><row><cell>VM-REUSE-UNIFORM</cell><cell>12.31</cell><cell>25.68</cell><cell>62.52</cell></row><row><cell>VM-REUSE-SKEWED</cell><cell>10.69</cell><cell>22.85</cell><cell>58.44</cell></row><row><cell cols="3">Average bounded slowdown</cell><cell></cell></row><row><cell>ρ →</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell></row><row><cell>NOVM-NOSR</cell><cell>60.48</cell><cell>71.80</cell><cell>296.74</cell></row><row><cell>NOVM-SR</cell><cell>31.05</cell><cell>49.56</cell><cell>134.91</cell></row><row><cell>VM-PREDEPLOY</cell><cell>39.60</cell><cell>62.45</cell><cell>133.59</cell></row><row><cell>VM-MULT</cell><cell cols="2">91.94 203.39</cell><cell>441.10</cell></row><row><cell>VM-REUSE-UNIFORM</cell><cell cols="2">62.24 140.42</cell><cell>338.60</cell></row><row><cell>VM-REUSE-SKEWED</cell><cell cols="2">50.46 115.99</cell><cell>309.67</cell></row><row><cell cols="4">drives, instead of being used by VMs. Although this approach al-</cell></row><row><cell cols="4">lows practically any environment to be deployed, it increases the</cell></row><row><cell cols="4">deployment overhead which, in OAR, is not scheduled separately</cell></row><row><cell>from the job.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This work was supported by NSF CSR award #527448 and in part, by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Advanced Scientific Computing Research, SciDAC Program, Office of Science, U.S. Department of Energy, under Contract DE-AC02-06CH11357.</p><p>We thank Dan Nurmi for his suggestions on how to analyze our data, and Rich Wolski and our anonymous reviewers for their detailed and insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From virtualized resources to virtual computing grids: the In-VIGO system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adabala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krsul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsugawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="896" to="909" />
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Andrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pruyne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rofrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Web services agreement specification</title>
		<imprint>
			<publisher>WS-Agreement</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grid&apos;5000: a large scale and highly reconfigurable experimental grid testbed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bolze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daydé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Desprez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Melab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mornet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Namyst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quetier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-G</forename><surname>Talbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Irena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="494" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lowess: A program for smoothing scatterplots by robust locally weighted regression</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">54</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resource co-allocation in computational grids</title>
		<author>
			<persName><forename type="first">K</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPDC &apos;99: Proceedings of the 8th IEEE International Symposium on High Performance Distributed Computing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Increasing Reliability through Dynamic Virtual Clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Emeneker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanzione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Availability and Performance Computing Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Virtual Machine Caching in Dynamic Virtual Clusters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Emeneker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanzione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SRMPDS Workshop, ICAPDS 2007 Conference</title>
		<imprint>
			<date type="published" when="2007-12">December 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xen and the art of cluster scheduling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fallenbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Picht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VTDC &apos;06: Proceedings of the 1st International Workshop on Virtualization Technology in Distributed Computing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Feitelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schwiegelshohn</surname></persName>
		</author>
		<title level="m">Parallel job scheduling -a status report. 10th Workshop on Job Scheduling Strategies for Parallel Processing</title>
		<meeting><address><addrLine>New-York, NY.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Metrics and benchmarking for parallel job scheduling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1459</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A distributed resource management architecture that supports advance reservations and co-allocation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nahrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Quality of Service</title>
		<meeting>the International Workshop on Quality of Service</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Virtual clusters for grid communities</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scheftner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sotomayor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGRID</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Division of labor: Tools for growing and scaling grids</title>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sotomayor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wuerthwein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSOC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Condor-G: A computation management agent for multi-institutional grids</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="246" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Berkeley lab checkpoint/restart (blcr) for linux clusters</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hargrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="494" to="499" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Falkon: a fast and light-weight task execution framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sharing networked resources with brokered leases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yumerefendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Yocum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Technical Conference</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Virtual workspaces: Achieving quality of service and quality of life on the grid</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="276" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maestro-VC: A paravirtualized execution environment for secure on-demand cluster computing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyanclar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yurcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGRID &apos;06: Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID&apos;06)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vmplants: Providing and managing virtual machine execution environments for grid computing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Krsul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A B</forename><surname>Fortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;04: Proceedings of the 2004 ACM/IEEE conference on Supercomputing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ANL/IBM SP scheduling system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lifka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS &apos;95: Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Impact of reservations on production job scheduling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Margo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kovatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Workshop on Job Scheduling Strategies for Parallel Processing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Utilization, predictability, workloads, and user runtime estimates in scheduling the IBM SP2 with backfilling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="529" to="543" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Virtual clusters on the fly -fast, scalable, and flexible installation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGRID &apos;07: Proceedings of the Seventh IEEE International Symposium on Cluster Computing and the Grid</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SPRUCE: A system for supporting urgent high-performance computing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trebon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beschastnikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFIP International Federation for Information Processing, Grid-Based Problem Solving Environments</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="page" from="295" to="311" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Handbook of Scheduling: Algorithms, Models, and Performance Analysis, chapter Online Scheduling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pruhs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC Press, Inc</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VioCluster: Virtualization for dynamic computational domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcgachey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Cluster Computing (Cluster&apos;05)</title>
		<meeting>the IEEE International Conference on Cluster Computing (Cluster&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autonomic live adaptation of virtual computational environments in a multi-domain infrastructure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goasguen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Autonomic Computing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Performance impact of resource provisioning on workflows</title>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<idno>05-850</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of South California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scheduling with advanced reservations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS &apos;00: Proceedings of the 14th International Symposium on Parallel and Distributed Processing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">127</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The performance impact of advance reservation meta-scheduling</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gregory</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<idno>IPDPS &apos;00/JSSPP &apos;00</idno>
		<title level="m">Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing</title>
		<meeting>the Workshop on Job Scheduling Strategies for Parallel Processing<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A resource management model for VM-based virtual workspaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sotomayor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-02">February 2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overhead matters: A model for virtual resource management</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sotomayor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VTDC &apos;06: Proceedings of the 1st International Workshop on Virtualization Technology in Distributed Computing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Creating personal adaptive clusters for managing scientific tasks in a distributed computing environment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Litvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Challenges of Large Applications in Distributed Environments</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Model-based resource selection for efficient virtual cluster deployment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VTDC &apos;07: Proceedings of the 2nd International Workshop on Virtualization Technology in Distributed Computing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Advance reservation policies for workflows</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Workshop on Job Scheduling Strategies for Parallel Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="http://aws.amazon.com/ec2/" />
		<title level="m">Amazon EC2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="http://www.teragridforum.org/mediawiki/images/b/b4/MetaschedRatReport.pdf" />
		<title level="m">Final report. teragrid co-scheduling/metascheduling requirements analysis team</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="http://www.cs.huji.ac.il/labs/parallel/workload/" />
		<title level="m">Parallel workloads archive</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
