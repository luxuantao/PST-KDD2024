<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theory of Graph Neural Networks: Representation and Learning</title>
				<funder ref="#_d2nG3a5 #_SfZ8CHb #_VPZEzUr">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-16">16 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
							<email>stefje@mit.edu</email>
						</author>
						<title level="a" type="main">Theory of Graph Neural Networks: Representation and Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-16">16 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.07697v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs), neural network architectures targeted to learning representations of graphs, have become a popular learning model for prediction tasks on nodes, graphs and configurations of points, with wide success in practice. This article summarizes a selection of the emerging theoretical results on approximation and learning properties of widely used message passing GNNs and higher-order GNNs, focusing on representation, generalization and extrapolation. Along the way, it summarizes mathematical connections. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been growing interest in solving machine learning tasks when the input data is given in form of a graph G = (V, E, X, W) from a set of attributed graphs G, where X ? R d?|V| contains vectorial attributes for each node, and W ? R d w ?|E| contains attributes for each edge (X and W may be empty). Examples include predictions in social networks, recommender systems and link predicton (given two nodes, predict an edge), property prediction of molecules, prediction of drug interactions, traffic prediction, forecasting physics simulations, and learning combinatorial optimization algorithms for hard problems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b120">121]</ref> (and many other references). These examples use two types of tasks: <ref type="bibr" target="#b0">(1)</ref> given a graph G, predict a label F(G); <ref type="bibr" target="#b1">(2)</ref> given a graph G and node v ? V(G) (or pair of nodes (u, v)), predict a node label f (v).</p><p>Solving these tasks demands a sufficiently rich embedding of the graph or of each node that captures structural properties as well as the attribute information. While graph embeddings have been a widely studied topic, including spectral embeddings and graph kernels, recently, Graph Neural Networks (GNNs) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b95">96]</ref> have emerged as an empirically broadly successful model class that, as opposed to, e.g., spectral embeddings, allows to adapt the embedding to the task at hand, generalizes to other graphs of the same input type, and incorporates attributes. Due to space limits, this survey focuses on the popular message passing (spatial) GNNs, formally defined below, and a selection of their rich mathematical connections, with an excursion into higher-order GNNs.</p><p>When learning a GNN, we observe N i.i.d. samples D = {G i , y i } N i=1 ? (G ? Y ) N drawn from an underlying distribution P on G ? Y. The labels y i are often given by an unknwon target function g(G i ), and observed with or without i.i.d. noise. Given a (convex) loss function : G ? Y ? Y ? R that measures prediction error, i.e., mismatch of y and F(G), such as the squared loss or cross-entropy, we aim to estimate a model F from our GNN model class F to minimize the expected loss (population risk) R(F):</p><p>min When analyzing learning and risk, three main questions become important:</p><p>1. Representational power (Section 2). Which target functions g can be approximated well by a GNN model class F ? Answers to this question relate to graph isomorphism testing, approximation theory for neural networks, local algorithms and representing invariance/equivariance under permutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Graph Neural Networks (GNNs)</head><p>In this article, we focus on Message passing graph neural networks (MPNNs), which follow an iterative scheme <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b95">96]</ref>. Throughout, they maintain a representation (embedding) h (t) v ? R d t for each node v ? V. In each iteration t, they update each embedding h (t) v as a function of its neighbors' embeddings and possible edge attributes:</p><formula xml:id="formula_0">h (0) v = x v , ?v ? V (Initialization) (1.3) m (t) v = f (t) Agg h (t-1) v , {{h (t-1) u , w(u, v) | u ? N (v)}} , 1 ? t &lt; T (Aggregate) (1.4) h (t) v = f Up (h (t) v , m<label>(t)</label></formula><p>v ) (Update).</p><p>(1.5)</p><p>The final node representation f (v) = h (T) v , ?v ? V is the last iterate, possibly concatenated with a linear classifier. Throughout this paper, N (v) ? V denotes the neighborhood of v ? V, and {{?}} a multiset. Here, h (t) v encodes the t-hop neighborhood of node v, i.e., the subgraph of all nodes reachable from v within t steps. The number of iterations T is also termed the GNN depth, and one iteration may be viewed as a layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The aggregation function f (t)</head><p>Agg : R d t-1 ? R d t plays a major role and is shared by all nodes within an iteration. It is a nonlinear function of the form</p><formula xml:id="formula_1">f (t) Agg h (t-1) v , {{h (t-1) u , w(u, v) | u ? N (v)}} = ? (t) 1 ? u?N (v) ? (t) 2 h (t-1) u , h (t-1) v</formula><p>, w(u, v) .</p><p>(1.6)</p><p>The sum may also be replaced by an average, degree-normalized sum or coordinate-wise min or max.</p><p>In the most general form, the functions ? 1 , ? 2 are implemented as multi-layer perceptrons (MLPs), neural networks that alternate linear transformations and coordinate-wise nonlinear activations such as the ReLU (?(a) = max{a, 0}) or sigmoid function (?(a) = (1 + exp(-a)) -1 ): MLP(h; ?) = ?(W (M) . . . ?(W (2) ?(W (1) h + b (1) ) + b (2) ) . . . + b (M) ).</p><p>(1.7)</p><p>The learnable parameters ? of the MLP are the weight matrices W (j) and bias vectors b (j) . The update f Up in Equation (1.5) is typically a weighted combination with learnable weight matrices:</p><formula xml:id="formula_2">f Up h (t) v , m (t) v = ? W (t) 1 h (t) v + W (t) 2 m (t) v or f Up h (t) v , m (t) v = m (t) v . (1.8)</formula><p>Finally, if a graph-level prediction is desired, all node representations can be aggregated by a permutation invariant readout function</p><formula xml:id="formula_3">F(G) = f Read {{h (T) v | v ? V} } . (1.9)</formula><p>Here, we assume the readout has the form <ref type="bibr">(1.6)</ref> or is a simple sum or average. Typically, all parameters are learned jointly via stochastic gradient descent minimizing the empirical risk. Throughout this article, n = |V| denotes the number of nodes and N the number of training data points.</p><p>Permutation invariance. An important property of GNNs is permutation invariance of the graph, and equivariance of the node representations. Let A ? R n?n be the adjacency matrix of a graph G ? G, and X ? R n?d its node features. Permutation invariance/equivariance means that for all permutation matrices P ? R n?n and all G ? G:</p><formula xml:id="formula_4">F(PAP , PX) = F(A, X) (1.10) f (PAP , PX, v) = f (A, X, v). (1.11)</formula><p>Spectral GNNs. Besides message passing GNNs, other architectures have been devised. One important example are spectral graph neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref>, which learn a function of the graph Laplacian L, i.e., F(G) = p(L)X = V p(?)V X, where V and ? are the matrices of eigenvectors and eigenvalues (diagonal matrix), respectively, and p is a polynomial. In the sequel, we will focus on message passing GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Representational power of GNNs</head><p>For functions on graphs, representational power has mainly been studied in terms of graph isomorphism: which graphs a GNN can distinguish. Via variations of the Stone-Weierstrass theorem, these results yield universal approximation results. Other works bound the ability of GNNs to compute specific polynomials of the adjacency matrix and to distinguish graphons <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b71">72]</ref>. Observed limitations of MPNNs have inspired higher-order GNNs (Section 2.3). Moreover, if all node attributes are unique, then analogies to local algorithms yield algorithmic approximation results and lower bounds (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNNs and Graph isomorphism testing</head><p>A standard characterization of the discriminative power of GNNs is via the hierarchy of the Weisfeiler-Leman (WL) algorithm for graph isomorphism testing, also known as color refinement or vertex classification <ref type="bibr" target="#b87">[88]</ref>, which was inspired by the work of Weisfeiler and Leman <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b108">109]</ref>. The WL algorithm does not entirely solve the graph ismomorphism problem, but its power has been widely studied.</p><p>A labeled graph is a graph endowed with a node coloring l : V(G) ? ? for some sufficiently large alphabet ?. Given a labeled graph (G, l), the 1-dimensional WL algorithm (1-WL) iteratively computes a node coloring c</p><formula xml:id="formula_5">(t) l : V(G) ? ?. Starting with c (0) l in iteration t = 0, in iteration t &gt; 0 it sets for all v ? V c (t) l (v) = Hash c t-1 l (v), {{c t-1 l (u) | u ? N (v)}} ,<label>(2.1)</label></formula><p>where Hash is an injective map from the input pair to ?, i.e., it assigns a unique color to each neighborhood pattern. To compare two graphs G, G , the algorithm compares the multisets {{c</p><formula xml:id="formula_6">(t) l (v) | v ? V(G)} } and {{c (t) l (u) | u ? V(G )}} in each iteration.</formula><p>If the sets differ, then it determines that G = G . Otherwise, it terminates when the number of colors in iteration t and t -1 are the same, which occurs after at most max{|V(G)|, |V(G )|} iterations.</p><p>The computational analogy between the 1-WL algorithm and MPNNs is obvious. Since the WL algorithm uniquely colors each neighborhood, the coloring c</p><formula xml:id="formula_7">(t) l (v) always refines the coloring h (t)</formula><p>v from a GNN.</p><p>Theorem 1 ([79, 110]). If for two graphs G, G a message passing GNN outputs f G (G) = f G (G ), then the 1-WL algorithm will determine that G = G . For any t, there exists an MPGNN such that c (t) l ? h (t) . A sufficient condition is that the aggregate, update and readout operations are injective multiset functions.</p><p>GNNs that use the degree for normalization in the aggregation <ref type="bibr" target="#b58">[59]</ref> can be equivalent to the 1-WL agorithm too, but with one more iteration in the WL algorithm <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Representing multiset functions</head><p>Theorem 1 demands the neighbor aggregation f Agg to be an injective multiset function on sets S (|S| ? M). Theorem 2 shows how to universally approximate multiset functions.</p><p>Theorem 2 <ref type="bibr">([107, 110]</ref>). Any multiset function G : X ?M ? R on a countable domain X can be expressed as</p><formula xml:id="formula_8">G(S) = ? 1 ? s?S ? 2 (s) , (2.2)</formula><p>where ? 1 : R d 1 ? R d 2 and ? 2 : R d 2 ? R are nonlinear functions.</p><p>The proof idea is to show that there exists an injective function of the form ? s?S ?(s). The above result is an extension of a universal approximation result for set functions <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b117">118]</ref>, and suggests a neural network model for sets where ? 1 , ? 2 are approximated by MLPs. The Graph Isomorphism Network (GIN) <ref type="bibr" target="#b109">[110]</ref> implements this sum decomposition in the aggregation function to ensure the ability of injective operations.</p><p>Here, the latent dimension d 2 plays a role. Proofs for countable domains use a discontinuous mapping ? 1 into a fixed-dimensional space, whereas MLPs universally approximate continuous functions <ref type="bibr" target="#b26">[27]</ref>. Continuous set functions on R ?M (i.e., set cardinality |S| ? M) can be sum-decomposed as above with continuous ? 1 , ? 2 and latent dimension at least d 2 = M. This dimension is a necessary and sufficient condition for universal approximation <ref type="bibr" target="#b106">[107]</ref>. For GNNs, this means d 2 must be at least the maximum degree deg(G) of the input graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Implications for graph distinction</head><p>Theorem 1 allows to directly transfer any known result for the 1-WL test to MPNNs. For instance, the 1-WL test succeeds to distinguish graphs sampled uniformly from all graphs on n nodes with high probability, and failure probability going to zero as n ? ? <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. 1-WL can also distinguish any non-isomorphic pair of trees <ref type="bibr" target="#b51">[52]</ref>. It fails for regular graphs, as all node colors will be the same. The graphs that the 1-WL algorithm can distinguish from any non-isomorphic graph can be recognized in quasi-linear time <ref type="bibr" target="#b5">[6]</ref>. See also <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref> for more detailed results on the expressive power of variants of the WL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Computation trees and structural graph properties</head><p>To further illustrate the implications of GNNs' discriminative power, we look at some specific examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The maximum information contained in any embedding h (t)</head><p>v can be characterized by a computation tree T (h (t) v ), i.e., an "unrolling" of the message passing procedure. The 1-WL test essentially colors</p><formula xml:id="formula_9">A C B A C B B A C B C A B A C A C A B B C A B B C A C</formula><p>Figure <ref type="figure">1</ref>: Graphs G 1 (left, 2 connected components) and G 2 (middle) with node attributes indicated by letters. The computation tree rooted at the node with arrow (right) agrees in both graphs, and likewise for the other nodes. Hence, 1-WL and MPNNs cannot distinguish G 1 and G 2 . Figure adapted from <ref type="bibr" target="#b39">[40]</ref>.</p><p>computation trees. The tree T (h</p><formula xml:id="formula_10">(t) v ) is constructed recursively: let T (h (0) v ) = x v for all v ? V.</formula><p>For t &gt; 0, construct a root with label x v and, for any u ? N (v) construct a child subtree T (h (t-1) u</p><p>). Figure <ref type="figure">1</ref> illustrates an example.</p><formula xml:id="formula_11">Proposition 1. If for two nodes u = v, we have T (h (t) v ) = T (h (t) u ), then h (t) v = h (t) u .</formula><p>Comparing computation trees directly implies that MPNNs cannot distinguish regular graphs. It also shows further limitations with practical impact, as indicated in Figure <ref type="figure">1</ref>, in particular for learning combinatorial algorithms and for predicting properties of molecules, where functional groups are of key importance. We say a class of models F decides a graph property if there exists an F ? F such that for any two G, G that differ in the property, we obtain F(G) = F(G ).</p><p>Proposition 2. MPNNs cannot decide girth, circumference, diameter, radius, existence of a conjoint cycle, total number of cycles, and existence of a k-clique <ref type="bibr" target="#b39">[40]</ref>. MPNNs cannot count induced (attributed) subgraphs for any connected pattern of 3 or more nodes, except star-shaped patterns <ref type="bibr" target="#b24">[25]</ref>.</p><p>Motivated by these limitations, generalizations of GNNs were proposed that provably increase their representational power. Two main directions are to (1) introduce node IDs (Section 2.2), and (2) use higher-order functions that act on tuples of nodes (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Node IDs, local algorithms, combinatorial optimization and lower bounds</head><p>The major weaknesses of MPNNs arise from their inability to identify nodes as the origin of specific messages. Hence, MPNNs can be strengthened by making nodes more distinguishable. The gained representational power follows from connections with local algorithms, where the input graph defines both the computational problem and the network topology of a distributed system: there, each node v ? V is a local machine and generates a local output, and all nodes execute the same algorithm, without faults.</p><p>Approximation Algorithms. Sato et al. <ref type="bibr" target="#b93">[94]</ref> achieve a partial node distinction by transferring the idea of a port numbering from local algorithms. Edges incident to each node are numbered as outgoing ports. In each round, each node simultaneously sends a message to each port, but the messages can differ across ports:</p><formula xml:id="formula_12">m (t) v = f (t) Agg {{(port(u, v), port(v, u), h (t-1) u )}} . (2.3)</formula><p>Permutation invariance, though, is not immediate. This corresponds to the vector-vector consistent (VV C ) model for local algorithms <ref type="bibr" target="#b50">[51]</ref>. The VV C analogy allows to transfer results on representing approximation algorithms. CPNGNN is a specific VV C GNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 ([94]</head><p>). There exists a CPNGNN that can compute a (deg(G) + 1)-approximation for the minimum dominating set problem, a CPNGNN that can compute a 2-approximation for the minimum vertex cover problem, but no CPNGNN can do better. No CPNGNN can compute a constant-factor approximation for the maximum matching problem.</p><p>Adding a weak vertex 2-coloring leads to further results. Despite the increased power compared to MPNN, CPNGNNs retain most limitations of Proposition 2 <ref type="bibr" target="#b39">[40]</ref>.</p><p>A more powerful alternative is to endow nodes with fully unique identifiers <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b94">95]</ref>. E.g., augmenting the GIN model (the most expressive MPNN) <ref type="bibr" target="#b109">[110]</ref> with random node identifiers yields a model that can decide subgraphs that MPNN and CPNGNN cannot <ref type="bibr" target="#b94">[95]</ref>. This model can further achieve better approximation results for minimum dominating set (H(deg(G) + 1) + ), where H is the harmonic number) and maximum matching (1 + ).</p><p>Turing completeness. Analogies to local algorithms further imply that MPNNs with unique node IDs are Turing complete, i.e., they can compute any function that a Turing machine can compute, including graph isomorphism. In particular, the proof shows an equivalence to the Turing universal LOCAL model from distributed computing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b81">82]</ref>. Lower bounds. For bounded size, GNNs lose computational power. Via analogies to the CONGEST model <ref type="bibr" target="#b82">[83]</ref>, which bounds message sizes, one can transfer results on decision, optimization and estimation problems on graphs. These lead to lower bounds on the product of depth and width of the GNN if the nodes do not have access to a random generator. Here, as before, width of a GNN refers to the dimensionality of the embeddings h</p><formula xml:id="formula_13">(t) v .</formula><p>Theorem 5 <ref type="bibr">([70]</ref>). If a problem cannot be solved in less than T rounds in CONGEST using messages of at most b bits, then it cannot be solved by an MPNN of width w ? (blog 2 n)/p = O(b/ log n) and depth T, where p = ?(n).</p><p>Theorem 5 directly implies lower bounds for solving combinatorial problems, e.g., Tw = ?(n/ log n) for cycle detection and computing diameter, and T ? w = ?( ? n/ log n) for minimum spanning tree, minimum cut and shortest path <ref type="bibr" target="#b69">[70]</ref>.</p><p>Moreover, we can transfer ideas from communication complexity. The communication capacity c f of an MPNN f (with unique node IDs) is the maximumm number of symbols that the MPNN can transmit between any two disjoint sets V 1 , V 2 ? V of nodes when viewed as a communication network:</p><formula xml:id="formula_14">c f ? cut(V 1 , V 2 ) ? T t=1 min{m t , w t } + ? T t=1 ? t ,</formula><p>where T is the GNN depth, w t the width of layer t, m t the size of the messages, and ? t the size of a global state that is maintained. The communication capacity of the MPNN must be at least c f = ?(n) to distinguish all trees, and c f = ?(n 2 ) to distinguish all graphs <ref type="bibr" target="#b70">[71]</ref>. By relating discrimination and function approximation (Section 2.4), these results have implications for function approximation, too.</p><p>Random node IDs. While unique node IDs are powerful in theory, in many practical examples the input graphs do not have unique IDs. An alternative is to assign random node IDs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>. This can still yield GNNs that are essentially permutation invariant: while their outputs are random, the outputs for different graphs are still sufficiently separated <ref type="bibr" target="#b0">[1]</ref>. This leads to a probabilistic universal approximation result:</p><formula xml:id="formula_15">Theorem 6 ([1]</formula><p>). Let g : G ? R be a permutation invariant function on graphs of size n ? 1. Then for all , ? &gt; 0 there exists an MPNN F with access to a global readout and with random node IDs such that for every</p><formula xml:id="formula_16">G ? G it holds that Pr(|F(G) -g(G)| ? ) ? 1 -?.</formula><p>The proof builds on a result by <ref type="bibr" target="#b9">[10]</ref> that states that any logical sentence in FOC 2 can be expressed by the addressed GNN. The logic considered here is a fragment of first-order (FO) predicate logic that allows to incorporate counting quantifiers of the form ? ?k x?(x), i.e., there are at least k elements x satisfying ?, but is restricted to two variables. FOC 2 is tightly linked with the 1-WL test: for any nodes u, v ? V in any graph, 1-WL colors u and v the same if and only if they are classified the same by all FOC 2 classifiers <ref type="bibr" target="#b20">[21]</ref>.</p><p>Other approaches include <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b116">117]</ref>.</p><p>Augmentations. Another successful idea is to augment node attribute vectors with attributes that contain structural or topological information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b119">120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Higher-order GNNs</head><p>Instead of adding unique node IDs, one may increase the expressive power of GNNs by encoding subsets of V that are larger than the single nodes used in MPNNs. Three such directions are: (1) neural network versions of higher-dimensional WL algorithms, (2) (non)linear equivariant operations, and (3) recursion. Other strategies that could not be covered here use, e.g., simplicial and cell complexes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Most of these GNNs act on k-tuples s ? V k and may be written in a unified form via tensors H (t) ? R n k ?d t , where the first k coordinates index the tuple, and</p><formula xml:id="formula_17">H (t)</formula><p>s,: ? R d t is the representation of tuple s in layer t. For MPNNs, which use node and edge information, H (0) ? R n?n?(d+1) . The first d channels of H (0) encode the node attributes: </p><formula xml:id="formula_18">H (0) v,v,1:d = x v and H (0) u,v,1:d = 0 for u = v.</formula><formula xml:id="formula_19">k ?(d 0 ) ? R n k ?(d T ) : f (G) = m ? S E ? F (T) ? . . . ? F (1) ? shape(G),<label>(2.4)</label></formula><p>where m : R d T ? R d out is an MLP that is applied to each representation h T s separately, S E : R n k ?d T ? R n?d T is a reduction S E (H) v,: = ? s?V k :s 1 =v H s,: , and each layer F (t) : R n k ?d t-1 ? R n k ?d t is a message passing (aggregation and update) operation for MPNNs, and will be defined for higher-order networks. The first operation shapes the input into the correct tensor form, if needed. For a graph embedding, we switch to a reduction S I : R n k ?d T ? R d T , S I (H) = ? s?V k H s,: and apply the MLP m to the resulting vector: (1) ? shape(G). Different GNN models differ in their layers F (t) , which must be permutation equivariant.</p><formula xml:id="formula_20">F(G) = m ? S I ? F (T) ? . . . ? F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Higher-order WL networks</head><p>Extending analogies between MPNNs and the 1-WL algorithm <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b109">110]</ref>, the first class of higher-order GNNs imitates versions of the k-dimensional WL algorithm. The k-WL algorithms are defined on k-tuples of nodes, and different versions differ in their aggregation and definition of neighborhood. In iteration 0, the k-WL algorithm labels each k-tuple s ? V k by a unique ID for its isomorphism type. Then it aggregates over neighborhoods</p><formula xml:id="formula_21">N WL i (s) = {(s 1 , s 2 , . . . s i-1 , v, s i+1 , . . . s k ) | ?v ? V} for 1 ? i ? k: c (t) i (s) = {{c (t-1) (s ) | s ? N WL i (s)}} 1 ? i ? k, s ? V k (2.5) c (t) (s) = Hash c (t-1) (s), c (t) 1 (s), c (t) 2 (s) . . . , c (t) k (s) . (2.6)</formula><p>For two graphs G, G the k-WL algorithm then decides "not isomorphic" if {{c (t) (s)</p><formula xml:id="formula_22">| s ? V(G) k }} = {{c (t) (s ) | s ? V(G )</formula><p>k }} for some t, and returns "maybe isomorphic" otherwise. Like the 1-WL test, the k-WL test decides "not isomorphic" only if G G . The Folklore k-WL algorithm (k-FWL) differs in its update rule, which "swaps" the order of the aggregation steps <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_23">c (t) u (s) = c (t-1) (u,s 2 ,...,s k ) , c (t-1)</formula><p>(s 1 ,u,s 3 ,...,s k ) , . . . , c</p><formula xml:id="formula_24">(t-1) (s 1 ,...,s k-1 ,u) , ?u ? V, s ? V k (2.7) c (t) (s) = Hash c (t-1) (s), {{c (t) u (s) | u ? V} } , ?s ? V k . (2.8)</formula><p>The 1-WL and 2-WL tests are equivalent, and for k ? 2, the (k + 1)-WL test can distinguish strictly more graphs than the k-WL test <ref type="bibr" target="#b20">[21]</ref>. The k-FWL algorithm is as powerful as the (k + 1)-WL algorithm for k ? 2 <ref type="bibr" target="#b44">[45]</ref>.</p><p>Set-WL GNN. Since computations on k-tuples are expensive, Morris et al. <ref type="bibr" target="#b78">[79]</ref> consider a GNN that corresponds to a set version of a k-WL algorithm. For any set S ?</p><formula xml:id="formula_25">V with |S| = k, let N set (S) = {T ? V, |T| = k | |S ? T| = k -1}. The set-based WL (k-SWL) algorithm then updates as c (t) (S) = Hash c (t-1) (S), {{c (t-1) (T) | T ? N set (S)}} ;</formula><p>(2.9) its GNN analogue uses the aggregation and update (cf. Eqns. (1.6),(1.8))</p><formula xml:id="formula_26">h (t+1) S = ? W (t) 1 h (t) S + ? T?N set (S) W (t) 2 h (t) T ,<label>(2.10)</label></formula><p>where ? is a coordinatewise nonlinearity (e.g., sigmoid or ReLU). This family of GNNs is equivalent in power to the k-SWL test <ref type="bibr" target="#b78">[79]</ref> (Theorem 8). For computational efficiency, a local version restricts the neighborhood of S to sets T such that the nodes {u, v} = S?T in the symmetric difference are connected in the graph. This local version is weaker <ref type="bibr" target="#b0">[1]</ref>. Folklore WL GNN. In analogy to the k-FWL algorithm, Maron et al. <ref type="bibr" target="#b73">[74]</ref> define k-FGNNs with aggregations</p><formula xml:id="formula_27">h (t) s = f (t) Up h (t-1) s , ? v?V k ? i=1 f (t) i (h (t-1) (s 1 ,...,s i-1 ,v,s i+1 ,...s k ) ) .</formula><p>(2.11)</p><p>For k = 2, this model can be implemented via matrix multiplications. The input to the aggregation, for all pairs of nodes simultaneously, is a tensor H ? R n?n?d t , with H (u,v),: = h (u,v) . The initial</p><formula xml:id="formula_28">H (0) ? R n?n?(d+1</formula><p>) is defined as in the beginning of Section 2.3.</p><p>To compute the aggregation layer, first, we apply three MLPs m 1 , m 2 : R d 1 ? R d 2 and m 3 : R d 1 ? R d 3 to each embedding h (u,v) in H: m l (H) (u,v),: = m l (H (u,v),: ) for 1 ? l ? 3. Then one computes an intermediate representation H ? R n?n?d 2 by multiplying matching "slices" of the outputs of m 1 , m 2 : H :,:,i = m 1 (H) :,:,i ? m 2 (H) :,:,i . The final output of the aggregation is the concatenation (m 3 (H), H ) ? R n?n?(d 2 +d 3 ) . A variation of this model, a low-rank global attention model, was shown to relate attention and the 2-FWL algorithm via algorithmic alignment, which we discuss in Section 3.3 <ref type="bibr" target="#b83">[84]</ref>. Attention in neural networks introduces learned pair-wise weights in the aggregation function.</p><p>The family of k-FGNNs is a class of nonlinear equivariant networks, and is equivalent in power to the k-FWL test and the (k + 1)-WL test <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b73">74]</ref> (Theorem 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Linear equivariant layers.</head><p>While the models discussed so far rely on message passing, the GNN definition (2.4) only requires permutation equivariant or invariant operations in each layer. The k-linear (equivariant) GNNs (k-LEGNNs), introduced in <ref type="bibr" target="#b74">[75]</ref>, allow more general linear equivariant operations. In k-LEGNNs, each layer</p><formula xml:id="formula_29">F (t) = ? ? L (t) : R n k ?d t-1 ? R n k ?d t is</formula><p>a concatenation of a linear equivariant function L (t) and a coordinate-wise nonlinear activation function. The function ? may also be replaced with a nonlinear function f (t) 1 : R d t+1/2 ? R d t+1 (an MLP) applied separately to each tuple embedding L (t) (H (t-1) ) s,: . Characterizations of equivariant functions or networks were studied in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b86">87]</ref>. Maron et al. <ref type="bibr" target="#b74">[75]</ref> explicitly characterize all invariant and equivariant linear layers, and show that the vector space of linear invariant or equivariant functions f : R n k ? R n has dimension b(k) and b(k + ), respectively, where b(k) is the k-th Bell number. When including multiple channels and bias terms, one obtains the following bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 7 ([75]</head><p>). The space of invariant (equivariant) linear layers</p><formula xml:id="formula_30">R n k ?d ? R d (R n k ?d ? R n k ?d ) has dimension dd b(k) + d (for equivariant: dd b(2k) + d b(k)).</formula><p>The associated GNN model uses one parameter (coefficient) for each basis tensor. Importantly, the number of parameters is independent of the number of nodes. The proof for identifying the basis tensors sets up a fixed point equation with Kronecker products of any permutation matrix that any equivariant tensor must satisfy. The solutions to these equations are defined by equivalence classes of multi-indices in [n] k . Each equivalence class is represented by a partition ? of [k], e.g., ? = {{1}, {2, 3}} includes all multi-indices (i 1 , i 2 , i 3 ) where i 1 = i 2 , i 3 and i 2 = i 3 . The basis tensors B ? ? {0, 1} n k are then such that B ? s = 1 if and only if s ? ?. Linear equivariant GNNs of order k (k-LEGNNs) parameterized with the full basis are as discriminative as the k-WL algorithm <ref type="bibr" target="#b73">[74]</ref> (Theorem 8). To achieve this discriminative power, each entry H (0) s,: in the input tensor encodes an initial coloring of the isomorphism type of the subgraph indexed by the k-tuple s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Summary of Representational Power via WL</head><p>The following theorem summarizes equivalence results between the GNNs discussed so far and variants of the WL test. Following <ref type="bibr" target="#b6">[7]</ref>, we here use equivalence relations, as they suffice for universal approximation in Section 2.4. For a set F of functions defined on G, define an equivalence relation ? via the joint discriminative power of all functions F ? F , i.e., for any two graphs G, G ? G:</p><formula xml:id="formula_31">(G, G ) ? ?(F ) ? ?F ? F , F(G) = F(G ).</formula><p>(2.12)</p><p>Theorem 8. The above GNN families have the following equivalences:</p><formula xml:id="formula_32">?(MGNN) = ?(2-WL) [110] (2.13) ?(k-set-GNN) = ?(k-SWL) [79] (2.14) ?(k-LEGNN) = ?(k-WL) [41,<label>75] (2.15)</label></formula><p>?(k-FGNN) = ?((k+1)-WL) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b73">74]</ref>.</p><p>(2.16)</p><p>Analogous results hold for equivariant models (for node representations), with the exception of equality (2.15), which becomes an inclusion: ?(k-LGNN E ) ? ?(k-WL E ) <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Relational Pooling.</head><p>One option to obtain nonlinear permutation invariant functions is to average permutation-sensitive functions over the permutation group ? n . Murphy et al. <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref> propose such a model. Concretely, if A ? R n?n denotes the adjacency matrix of the input graph G and X ? R n?d the matrix of node attributes, then</p><formula xml:id="formula_33">F RP (G) = 1 n! ? ??? n g(A ?,? , X ? ) = g(? ? H (0) ),<label>(2.17)</label></formula><p>where X ? is X with permuted rows, and H (0) is the tensor combining adjacency matrix and node attributes. Here, g is any permutation-sensitive function, and may be modeled via various nonlinear function approximators, e.g. neural networks such as fully connected networks (MLPs), recurrent neural networks or a combination of a convolutional network applied to A and an MLP applied to X. In particular, this model allows to implement graph isomorphism testing via node IDs (cf. Section 2.2) if g is a universal approximator <ref type="bibr" target="#b79">[80]</ref>. For instance, node IDs may be permuted over nodes and concatenated with the node attributes:</p><formula xml:id="formula_34">F RP (G) = 1 n! ? ??? n (A ?,? , [X ? , I n ] ) = 1 n! ? ??? n g(A, [X , (I n ) ? ] ),<label>(2.18)</label></formula><p>where I n ? R n?n is the identity matrix. If g is an MPNN, the resulting model is strictly more powerful than the 1-WL test and hence g by itself.</p><p>The drawback of the Relational Pooling (2.17) is its computational intractability. Various approximations have been considered, e.g., defining canonical orders, stochastic approximations, and applying g to all possible k-subsets of V. In the latter case, increasing k strictly increases the expressive power. Local Relational Pooling is a variant that applies relational pooling to the k-hop subgraphs centered at each node, and then aggregates the results. This operation provably allows to identify and count subgraphs of size up to k <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Recursion</head><p>A general strategy for encoding a graph is to encode a collection of subgraphs and then aggregate these encodings. The question of what graphs G, G this process allows to distinguish (F(G) = F(G )), depends on the collection of subgraphs used, the subgraph encoding function and the aggregation function. As a special case, this process includes the reconstruction hypothesis <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b101">102]</ref>, i.e., the question whether any graph G can be reconstructed from the collection of its subgraphs G \ {v}, for all v in G. One challenge with the reconstruction hypothesis is that no alignment of the G \ {v} is available. Indeed, node correspondences across subgraphs provide important information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b99">100]</ref>.</p><p>Indeed, the expressive power of a model based on subgraph encodings depends on the set of subgraphs, the type of subgraph encodings and the aggregation. Tahmasebi et al. <ref type="bibr" target="#b99">[100]</ref> use recursion as a powerful tool: instead of iterative message passing or layering, a recursive application of the above subgraph embedding step, even with a simple set aggregation like (1.6), can enable a GNN that can count any bounded-size subgraphs, as opposed to MPNNs (Prop. 2).</p><p>Let N r (v) be the r-hop neighborhood of v in G. Recursive neighborhood pooling (RNP) encodes intersections of such neighborhoods of different radii. Given an input graph G with node attributes (h in u ) u?V(G) and a sequence (r 1 , . . . , r t ) of radii, RNP-GNN recursively encodes the node-deleted r 1neighborhoods G v = N r 1 (v) \ {v} of all nodes v ? V after marking the deletion in augmented representations h aug u , u ? V. It then combines the results, and returns node representations of all nodes. Concretely, for each v ? V, it computes G v and</p><formula xml:id="formula_35">h aug u = (h in u , 1[(u, v) ? E(G v )]); ?u ? G v (2.19) (h v,u ) u?G v ? RNP-GNN G v , (h aug u ) u?G v , (r 2 , r 3 , . . . , r t ) (recursion) (2.20) return h out v = f (t) Agg h in v , {{h v,u : u ? G v }} , ?v ? V (aggregation). (2.21)</formula><p>If the sequence of radii is empty (base case), then the algorithm returns the input attributes h in u . In contrast to iterative message passing, the encoded subgraphs here correspond to intersections of local neighborhoods. Together with the node deletions and markings that retain node correspondences, this maintains more structural information. Formally, if the sequence of radii dominates a covering sequence for a subgraph H of interest, then, with appropriate parameters, RNP can count the induced and non-induced subgraphs of G isomorphic to H <ref type="bibr" target="#b99">[100]</ref>. The computational cost is O(n k ) for recursion depth k, and better for very sparse graphs, in line with computational lower bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Universal approximation</head><p>Distinguishing given graphs is closely tied to approximating continuous functions on graphs. In early work, Scarselli et al. <ref type="bibr" target="#b96">[97]</ref> take a fixed point view and show a universal approximation result for infinite-depth MPNNs whose layers are contraction operators, for functions on equivalence classes defined by computation trees. Dehmamy et al. <ref type="bibr" target="#b31">[32]</ref> analyze the ability of GNNs to compute polynomials of the adjacency matrix.</p><p>Later works derive universal approximation results for graph and permutation-equivariant functions from graph discrimination results via extensions of the Stone-Weierstrass theorem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b75">76]</ref>. Maron et al. <ref type="bibr" target="#b75">[76]</ref> argue that H-invariant networks (for a permutation group H) can universally approximate H-invariant polynomials, which in turn can universally approximate any invariant function <ref type="bibr" target="#b112">[113]</ref>.</p><p>Keriven and Peyr? <ref type="bibr" target="#b56">[57]</ref> do not fix the size of the graph and show that shallow equivariant networks can, with a single set of parameters, well approximate a function on graphs of varying size. Both constructions involve very large tensors.</p><p>More generally, a Stone-Weierstrass theorem (for symmetries) allows to translate Theorem 8 into universal approximation results. Let C I (X , Y ) be the set of invariant continuous functions from X to Y. Then a class F of GNNs is universal if its closure F (in uniform norm) on a compact set K is the entire</p><formula xml:id="formula_36">C I (K, R p ). Theorem 9 ([7]). Let K disc ? G n ? R d 0 ?n , K ? R d 0 ?n</formula><p>be compact sets, where G n is the set of all unweighted graphs on n nodes.</p><formula xml:id="formula_37">MGNN = { f ? C I (K disc , R p ) : ?(2-WL) ? ?( f )} (2.22) k-LEGNN = { f ? C I (K, R p ) : ?(k-WL) ? ?( f )} (2.23) k-FGNN = { f ? C I (K, R p ) : ?((k+1)-WL) ? ?( f )}. (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24)</head><p>Analogous relations hold for equivariant functions, except for k-</p><formula xml:id="formula_38">LEGNN E = { f ? C E (K, R n?p ) : ?(k-LEGNN E ) ? ?( f )}, which is a superset of { f ? C E (K, R n?p ) : ?(k-WL E ? ?( f )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalization</head><p>Beyond approximation power, a second important question in machine learning is generalization. Generalization asks how well the estimated function F is performing according to the population risk, i.e., R( F), as a function of the number of data points N and model properties. Good generalization may demand explicit (e.g., via a penalty term) or implicit regularization (e.g., via the optimization algorithm). Hence, generalization analyses involve aspects of the complexity of the model class F , the target function we aim to learn, the data and the optimization procedure. This is particularly challenging for neural networks, due to the nested functional form and the non-convexity of the empirical risk.</p><p>A classic learning theoretic perspective bounds the generalization gap R( F) -R( F) via the complexity of the model class F (Section 3.1). These approaches do not take into account possible implicit regularization via the optimization procedure. One possibility to do so is via the Neural Tangent Kernel approximation (Section 3.2). Finally, for more complex, structured target functions, e.g., algorithms or physics simulations, one may want to also consider the structure of the target task. One such option is Algorithmic Alignment (Section 3.3). Another strategy for obtaining generalization bounds is via algorithmic stability, the condition that, if one data point is replaced, the outcome of the learning algorithm does not change much. This strategy led to some early bounds for spectral GNNs <ref type="bibr" target="#b104">[105]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalization bounds via complexity of the model class</head><p>Vapnik-Chervonenkis dimension. The first GNN generalization bound was based on bounding the Vapnik-Chervonenkis (VC) dimension <ref type="bibr" target="#b102">[103]</ref> of the GNN function class F . The VC dimension of F expresses the maximum cardinality of a set of data points such that for any binary labeling of the data, some GNN in F can perfectly fit, i.e., shatter, the set. The VC dimension directly leads to a bound on the generalization gap. Here, we only state the results for sigmoid activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 10 ([98]</head><p>). The VC dimension of GNNs with p parameters, H hidden neurons (in the MLP) and input graphs of size n is O(p 2 H 2 n 2 ).</p><p>Strictly speaking, Theorem 10 is for node classification with one hidden layer in the aggregation function MLPs. The VC dimension directly yields a bound on the generalization gap: for a class F with VC dimension D, with probability 1?, it holds that</p><formula xml:id="formula_39">R( F) -R( F) ? O D N log N D + 1 2N log 1 ? . (3.1)</formula><p>Interestingly, in these bounds, GNNs are a generalization of recurrent neural networks <ref type="bibr" target="#b97">[98]</ref>. The VC dimension bounds for GNNs are the same as for recurrent neural networks <ref type="bibr" target="#b59">[60]</ref>; the bounds for fully connected MLPs are missing the factor n 2 <ref type="bibr" target="#b54">[55]</ref>. Rademacher Complexity. Bounds that are in many cases tighter can be obtained via Rademacher complexity. The empirical Rademacher complexity R S (F ) of a function class F measures how well it can fit "noise" in the form of uniform random variables ? = (? 1 , . . . , ? N ) in {-1, +1}:</p><formula xml:id="formula_40">R S (F ) = E ? sup F?F 1 N N ? i=1 ? i F(x i ) , (3.2)</formula><p>for a fixed data sample S = {x 1 , . . . , x N }. Similarly to VC dimension, R S (F ) provides a bound on the probability of error under the full data distribution:</p><formula xml:id="formula_41">P[error(F)] ? R(F) + 2 R S (J ) + 3 log(2/?)</formula><p>2N , where J is the class of functions F ? F concatenated with the loss. Garg et al. <ref type="bibr" target="#b39">[40]</ref> analyze a GNN that applies a logistic linear binary classifier at each node, averages these predictions for a graph-level prediction, and uses a mean field update <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_42">h (t) v = ?(W 1 x v + W 2 ?(? u?N(v) g(h (t-1) u</formula><p>))), where ?, ?, g are nonlinear functions with bounded Lipschitz constant that are zero at zero (e.g., tanh), and</p><formula xml:id="formula_43">W 1 F , W 2 F ? B.</formula><p>The logistic classifier outputs a "probability" for the label 1, and is evaluated by a margin loss function that gives a (scaled) penalty if the "probability" of the correct label is below a threshold ( <ref type="formula" target="#formula_53">?+1</ref>2 ).</p><p>Theorem 11 <ref type="bibr">([40]</ref>). Let C be the product of the Lipschitz constants of ?, ?, g and B; T the number of GNN iterations; w the dimension of the embeddings h</p><formula xml:id="formula_44">(t)</formula><p>v , and d the maximum branching factor in the computation tree. Then the generalization gap of the GNN can be bounded as: O( wd</p><formula xml:id="formula_45">? N? ) for C &lt; 1/d, O( wdT ? N? ) for C = 1/d and O( wd ? wT ? N? ) for C &gt; 1/d.</formula><p>The factor d is equal to max v?G deg(v) -1. For recurrent neural networks, the same bounds hold, but with d = 1 <ref type="bibr" target="#b22">[23]</ref>: a sequence is a tree with branching factor 1. To compare these bounds with the bounds based on VC-dimension, we use that H = w, n &gt; d, and p the size of the matrices W, i.e., about w 2 , and obtain a VC-dimension based generalization bound of ?(w 3 n/ ? N), ignoring log factors. Later work tightens the bounds in Theorem 11 by using a PAC-Bayesian approach <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalization bounds via the Neural Tangent Kernel</head><p>Infinitely wide neural networks can be related to kernel learning techniques via the Neural Tangent Kernel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref>. Du et al. <ref type="bibr" target="#b33">[34]</ref> extend this analysis to a broad class of GNNs. The main idea underlying the Neural Tangent Kernel (NTK) is to approximate a neural network F(?, G) with a kernel derived from the training dynamics. Assume we fit F(?, G) with the squared loss</p><formula xml:id="formula_46">L(?) = ? N i=1 (F(?, G i ), y i ) = 1 2 ? N i=1 (F(?, G i ) -y i ) 2</formula><p>, where ? ? R m collects all parameters of the network. If we optimize with gradient descent with infinitesimally small step size, i.e.,</p><formula xml:id="formula_47">d?(t) dt = -?L(?(t)), then the network outputs u(t) = (F(?(t), G i )) N i=1 follow the dynamics du dt = -H(t)(u(t) -y), where H(t) ij = ?F(?(t), G i ) ?? , ?F(?(t), G j ) ?? . (<label>3.3)</label></formula><p>Here, y = (y i ) N i=1 . If ? is sufficiently large (i.e., the network sufficiently wide), then it was shown that the matrix H(t) ? R N?N remains approximately constant as a function of t. In this case, the neural network becomes approximately a kernel regression <ref type="bibr" target="#b98">[99]</ref>. If the parameters ?(0) are initialized as i.i.d. Gaussian, then the matrix H(0) converges to a deterministic kernel matrix H, the Neural Tangent Kernel, with closed form regression solution F H (G). Given this approximation, one may analyze generalization via kernel learning theory.</p><p>Theorem 12 <ref type="bibr">([11]</ref>). Given N i.i.d. training data points and any loss function : R ? R ? [0, 1] that is 1-Lipschitz in the first argument with (y, y) = 0, with probablity 1? the population risk of the Graph Neural Tangent predictor is bounded as</p><formula xml:id="formula_48">R(F H ) = O 1 N y H -1 y ? tr( H) + 1 N log(1/?) .</formula><p>In contrast to the results in Section 3.1, the complexity measure y H -1 y of the target function is data-dependent. If the target function to be learned follows a simple GNN structure with a polynomial, then this bound can be polynomial:</p><formula xml:id="formula_49">Theorem 13 ([34]). Let hv = c v ? u?N (v)?{v} h u . If the labels y i , 1 ? i ? N, satisfy y i = ? 1 ? v?V(G i ) ? 1 hv + ? ? l=1 ? 2l ? v?V ? 2l hv 2l for ? k ? R, ? k ? R d , then y H -1 y ? 2|? 1 | ? ? 1 2 + ? ? l=1 ? 2?(2l -1)|? 2l | ? ? 2l 2l 2 . With n = max i V(G i ), we have tr( H) = O(n 2 N).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalization via Algorithmic Alignment</head><p>The Graph NTK analysis shows a polynomial sample complexity if the function to be learned is close to the computational structure of the GNN, in a simple way. While this applies to mainly simpler learning tasks, the idea of an "alignment" of computational structure carries further. Recently, there has been growing interest in learning scientific tasks, e.g., given a set of particles or planets along with their location, mass and velocity, predict the next state of the system <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93]</ref>, and in "algorithmic reasoning", e.g., learning to solve combinatorial optimization problems in particular over graphs <ref type="bibr" target="#b21">[22]</ref>. In such cases, the target function corresponds to an algorithm, e.g., a dynamic program.</p><p>While many neural network architectures have the power to represent such tasks, empirically, they do not learn them equally well from data. In particular, GNNs perform well here, i.e., their architecture encodes suitable inductive biases <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b110">111]</ref>. As a concrete example, consider the Shortest Path problem. The computational structure of MPNNs matches that of the Bellman-Ford (BF) algorithm <ref type="bibr" target="#b13">[14]</ref> very well: both "algorithms" iterate, and in each iteration t, update the state as a function of the neighboring nodes and edge weights w(u, v):</p><formula xml:id="formula_50">BF: d[t][v] = min u?N (v) d[t -1][u] + w(u, v) GNN: h (t) v = ? u?N (v) MLP(h (t-1) u , h (t-1) v , w(u, v)).<label>(3.4)</label></formula><p>Hence, the GNN can simulate the BF algorithm if it uses sufficiently many iterations, and if the aggregation function approximates the BF state update (relaxation step). Intuitively, this update is a much simpler function to learn than the full algorithm as a black box, i.e., the GNN encodes much of the algorithmic structure, sparsity and invariances in the architecture. More generally, MPNNs match the structure of many dynamic programs in an analogous way <ref type="bibr" target="#b110">[111]</ref>, as long as the updates are permutation invariant or sufficient node identification is provided as input, in light of the results in Section 2. Dudzik and Veli?kovi? <ref type="bibr" target="#b36">[37]</ref> refine and generalize the relations between GNNs and dynamic programming by using category theory.</p><p>The NTK results formalize simplicity by a small function norm in the RKHS associated with the Graph NTK; this can become complicated with more complex tasks and multiple layers. To quantify structural match, Xu et al. <ref type="bibr" target="#b110">[111]</ref> define algorithmic alignment by viewing a neural network as a structured arrangement of learnable modules -in a GNN, the (MLPs in the) aggregation functions -and define complexity via sample complexity of those modules in a PAC-learning framework. Sample complexity in PAC learning is defined as follows: We are given a data sample {(x i , y i )} N i=1 drawn i.i.d. from a distribution P that satisfies y i = g(x i ) for an underlying target function g. Let f = A({x i , y i } N i=1 ) be the function output by a learning algorithm A. For a fixed error and failure probability 1?, the function g is (N, , ?)-PAC learnable with A if</p><formula xml:id="formula_51">P x?P | f (x) -g(x)| &lt; ? 1 -?. (3.5)</formula><p>The sample complexity C A (g, , ?) is the smallest N so that g is (N, , ?)-learnable with A.</p><p>Definition 1 (Algorithmic Alignment). Let g be a target function and N a neural network with M modules N i . The module functions f 1 , ..., f M generate g for N if, by replacing N i with f i , the network N simulates g. Then N (N, , ?)-algorithmically aligns with g if (1) f 1 , ..., f M generate g and ( <ref type="formula" target="#formula_5">2</ref>) there are learning algorithms A i for learning f i with N i , with sample complexity M</p><formula xml:id="formula_52">? max i C A i ( f i , , ?) ? N.</formula><p>Algorithmic alignment resembles Kolmogorov complexity <ref type="bibr" target="#b60">[61]</ref>. Thus, it can be hard to obtain the optimal alignment between a neural network and an algorithm. But, any algorithmic alignment yields a bound, and any with acceptable sample complexity may suffice. The complexity of the MLP modules in GNNs may be measured with a variety of techniques. One option is the NTK framework. The module-based bounds then resemble the polynomial bound in Theorem 13, since both are extensions of <ref type="bibr" target="#b4">[5]</ref>. However, here, the bounds are applied at a module level, and not for the entire GNN as a unit. Theorem 14 translates these bounds, in a simplified setting, into sample complexity bounds for the full network.</p><p>Theorem 14 <ref type="bibr">([111]</ref>). Fix and ?. Suppose {G i , y i } N i=1 ? P, where |V(G i )| ? n, and y i = g(G i ) for some g. Suppose N 1 , ..., N M are network N 's MLP modules in sequential order of processing. Suppose N and g (N, , ?)-algorithmically align via functions f 1 , ..., f M for a constant M. Under the following assumptions, g is (N, O( ), O(?))-learnable by N . a) Sequential learning. We train N i 's sequentially:</p><formula xml:id="formula_53">N 1 has input samples { x(1) i , f 1 ( x(1) i )} N i=1 , with x<label>(1)</label></formula><p>i obtained from G i . For j &gt; 1, the input x(j) i for N j are the outputs of the previous modules, but labels are generated by the correct functions f j-1 , ..., f 1 on</p><p>x(1) i . b) Algorithm stability. Let A be the learning algorithm for the N i 's. Suppose f = A({x i , y i } N i=1 ), and f = A({ xi , y i } N i=1 ). For any x, f (x) -f (x) ? L 0 ? max i x ixi , for some L 0 &lt; ?. c) Lipschitzness. The learned functions fj satisfy fj (x) -fj ( x) ? L 1 xx , for some L 1 &lt; ?.</p><p>The big O notation here hides factors including the Lipschitz constants, number of modules and graph size. When measuring module complexity via the NTK, Theorem 14 indeed yields a gap in upper bounds between fully connected networks and GNNs in simple cases <ref type="bibr" target="#b110">[111]</ref>, supporting empirical results. While some works use sequential training in experiments <ref type="bibr" target="#b103">[104]</ref>, empirically, better alignment improves learning and generalization even with more common "end-to-end" training, i.e., optimizing all parameters simultaneously <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b110">111]</ref>.</p><p>At a general level, these alignment results indicate how incorporating expert knowledge, e.g. in terms of algorithmic techniques or physics, into the design of the learning method can improve sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Extrapolation</head><p>Section 3 summarizes results for in-distribution generalization, i.e., how well a learned model performs on data from the same distribution P as the training data. Yet, in many practical scenarios, a model is applied to data from a different distribution. A strong case of such a distribution shift is extrapolation. It considers the expected loss E G?Q [ (G, g(G), F(G))] under a distribution Q with different support, e.g., supp(Q) ? supp(P ). For graphs, Q may entail graphs of different sizes, different degrees, or with node attributes in different ranges from the training graphs. As no data has been observed in supp(Q) \ supp(P ), extrapolation can be ill-defined without stronger assumptions on the task and model class. At least two types of such assumptions have been made. Theoretical results on extrapolation either assume that the graphs have sufficient structural similarity or that the model class is sufficiently restricted to extrapolate accurately. Empirically, while extrapolation has been difficult, several works achieve GNN extrapolation in tasks like predicting the time evolution of physical systems <ref type="bibr" target="#b11">[12]</ref>, learning graph algorithms <ref type="bibr" target="#b103">[104]</ref>, and solving equations <ref type="bibr" target="#b63">[64]</ref>.</p><p>Structural similarity of graphs. One possibility to guarantee successful extrapolation to larger graphs is to assume sufficient structural similarity between the graphs in P and Q, in particular, structural properties that matter for the GNN family under consideration. For spectral GNNs, this assumption has been formalized as the graphs arising from the same underlying topological space, manifold or graphon. Under such conditions, spectral GNNs -with conditions on the employed filterscan generalize to larger graphs <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90]</ref>. The underlying structure also ensures similar local structure of the graphs.</p><p>For spatial message passing GNNs, whose representations rely on computation trees as local structures (Section 2.1), an agreement in the distributions of the computation trees in the graphs sampled from P and Q is necessary <ref type="bibr" target="#b113">[114]</ref>. This is violated, for instance, if the degree distribution is a function of the graph size, as is the case for random graphs under the Erd?s-R?nyi or Preferential Attachment models. The computation tree of depth t rooted at a node v corresponds to the color c (t) (v) assigned by the 1-WL algorithm.</p><p>Theorem 15 <ref type="bibr">([114]</ref>). Let P and Q be finitely supported distributions of graphs. Let P t be the distribution of colors c (t) (v) over P and similarly Q t for Q. Assume that any graph in Q contains a node with a color in Q t \ P t . Then, for any graph regression task solvable by a GNN with depth t there exists a GNN with depth at most t + 3 that perfectly solves the task on P and predicts an answer with arbitrarily large error on all graphs from Q.</p><p>The proof exploits the fact that GNN predictions on nodes only depend on the associated computation tree and that a sufficiently flexible GNN can assign arbitrary target labels to any computation tree <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b113">114]</ref>. I.e., the available information allows for multiple local minima of the empirical risk. A similar result can be shown for node prediction tasks. (A "sufficiently large" GNN here means depth at least t + 2 layers and width max{(max deg(G) + 1) t ? |C|, 2 |P|}, where the max degree refers to any graph in the support, |C| is the finite number of possible input node attributes and P the set of colors encountered in graphs in the support.)</p><p>Conditions on the GNN. If sufficient structural similarity of the input graphs cannot be guaranteed, then further restrictions on the GNN can enable extrapolation to different graph sizes, structures and ranges of input node attributes. If there are no training observations in a certain range of attributes or set of local structures, then the predictions of the learned model depend on the inductive biases induced by the model architecture, loss function and training algorithm. Which prediction function, out of multiple fitting functions, a model will choose, depends on these biases.</p><p>Xu et al. <ref type="bibr" target="#b111">[112]</ref> analyze such biases to obtain conditions on the GNN for extrapolation. Taking the perspective of algorithmic alignment (Section 3.3), they first analyze how individual module functions, i.e., the MLPs in the aggregation function of a GNN, extrapolate, and then transfer this to the entire GNN. The aggregation functions enter the extrapolation regime, e.g., if the node attributes, node degrees or computation trees are different under Q compared to P, as they determine the inputs to the aggregations. The following theorem states that, sufficiently far away from supp(P ), MLPs implement directionally linear functions.</p><p>Theorem 16 <ref type="bibr">([112]</ref>). Suppose we train a two-layer MLP f : R d ? R with ReLU activation functions with squared loss in the Neural Tangent Kernel (NTK) regime. For any direction v ? R d , let x 0 = tv. As t ? ?, f (x 0 + hv)f (x 0 ) ? ? v ? h for any h &gt; 0, where ? v is a constant linear coefficient. Moreover, given &gt; 0, for t = O( 1 ), we have | f (x 0 +hv)-f (x 0 ) h</p><formula xml:id="formula_54">-? v | &lt; .</formula><p>The linear function and the constant terms in the convergence rate depend on the training data and the direction v. The proof of Theorem 16 relies on the fact that a neural network in the NTK regime learns a minimum-norm interpolation function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b52">53]</ref>. Although Theorem 16 uses a simplified setting of a wide 2-layer network, similar results hold empirically for more general MLPs <ref type="bibr" target="#b111">[112]</ref>.</p><p>To appreciate the implications of this result in the context of GNNs, consider the example of Shortest Path in Equation <ref type="bibr">(3.4)</ref>. For the aggregation function to mimic the Bellman-Ford algorithm, the MLP must approximate a nonlinear function. But, in the extrapolation regime, it implements a linear function and therefore is expected to not approximate Bellman Ford well any more. Indeed, empirical works that successfully extrapolate GNNs for Shortest Path use a different aggregation function of the form <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b103">104]</ref>   Here, the nonlinear parts do not need to be learned, allowing to extrapolate with a linear learned MLP. More generally, the directionally linear extrapolation suggests that (1) the architecture or (2) the input encoding should be set up such that the target function can be approximated when MLPs learn linear functions (linear algorithmic alignment). An example for (2) may be found in forecasting physical systems, e.g., predicting the evolution of n objects in a gravitational system, and the node (object) attributes are mass, location and velocity at time t. The position of an object at time t + 1 is a nonlinear function of the attributes of the other objects. When encoding the nonlinear function as transformed edge attributes, the function to be learned becomes linear. Many empirical works that successfully extrapolate implement the idea of linear algorithmic alignment <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b114">115]</ref>.</p><p>Finally, the geometry of the training data also plays an important role. <ref type="bibr" target="#b111">[112]</ref> show empirical results and initial theoretical results for learning max-degree, that, even with linear algorithmic alignment, sufficient diversity in the training data is needed to identify the correct linear functions. These data conditions are weaker than those implied by Theorem 15, due to the linear algorithmic alignment assumption.</p><p>For the case when the target test distribution Q is known, Yehudai et al. <ref type="bibr" target="#b113">[114]</ref> propose approaches for combining elements of P and Q to enhance the range of the data seen by the GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This survey summarized three main topics in theoretically understanding GNNs: representation and approximation, generalization, and extrapolation. As GNNs are an active research area, many results could not be included. E.g., we focused on MPNNs and main ideas for higher-order GNNs, but neglected spectral GNNs, which closely relate to ideas in graph signal processing. Other emergent topics include adversarial robustness, optimization behavior of the empirical risk and its improvements, and computational scalability and approximations. Overall, GNNs have a rich set of mathematical connections, a selection of which was covered here.</p><p>Many questions remain. Regarding approximation capabilities, the limitations of MPNNs have motivated powerful higher-order GNNs. However, these are still computationally expensive. What efficiency is theoretically possible? Moreover, most applications may not require full graph isomorphism power, or k-WL power for large k. What other measures of representational power make sense? Do they allow better and sharper complexity results? Initial works consider subgraph counting as a benchmark task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b99">100]</ref>.</p><p>The generalization results so far need to use simplifications in the analysis, similar to most theoretical analyses of deep learning. To what extent can they be relaxed? Do more specific tasks or graph classes allow sharper results? Which modifications of GNNs would allow them to generalize better, and how do higher-order GNNs generalize? Similar questions pertain to extrapolation and reliability under distribution shifts, a topic that has been studied even less than GNN generalization.</p><p>In general, revealing further mathematical connections may enable the design of richer models and enable a more thorough understanding of GNNs' learning abilities and limitations, and eventual improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F?FEF?F</head><label></label><figDesc>(G,y)?P [ (G, y, F(G))] ? min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 4 (</head><label>4</label><figDesc><ref type="bibr" target="#b69">[70]</ref>). If f Up and f Agg are Turing complete functions and the message passing GNN gets unique node IDs, then the classes GNN and LOCAL are equivalent. For any MPNN F there exists a local algorithm A of the same depth, such that F(G) = A(G), and vice versa.Corollary 1 ([70]). Under the conditions in Theorem 4, if the GNN depth (number of iterations) is at least diameter(G) and the width (dimensionality of the embeddings h (t) v ) is unbounded, then MPNNs can compute any Turing computable function over connected attributed graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The final channel captures the adjacency matrix A of the graph: H (0) :,:,(d+1) = A. Node embeddings are computed by a permutation equivariant network f : R n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>h</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This article is a modified version of an article originally written for the International Congress of Mathematicians</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2022" xml:id="foot_1"><p></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The author would like to thank <rs type="person">Keyulu Xu</rs>, <rs type="person">Derek Lim</rs>, <rs type="person">Behrooz Tahmasebi</rs>, <rs type="person">Vikas Garg</rs>, <rs type="person">Tommi Jaakkola</rs>, <rs type="person">Andreas Loukas</rs>, <rs type="person">Jingling Li</rs>, <rs type="person">Mozhi Zhang</rs>, <rs type="person">Simon Du</rs>, <rs type="person">Ken-ichi Kawarabayashi</rs>, <rs type="person">Weihua Hu</rs>, <rs type="person">Jure Leskovec</rs>, <rs type="person">Joan Bruna</rs> and <rs type="person">Yusu Wang</rs> for discussions on the theory of GNNs, collaborations and pointers.</p><p>This work was partially supported by <rs type="funder">NSF</rs> <rs type="grantName">CAREER award</rs> <rs type="grantNumber">1553284</rs>, <rs type="funder">NSF</rs> <rs type="grantName">SCALE MoDL award</rs> <rs type="grantNumber">2134108</rs>, and <rs type="funder">NSF</rs> <rs type="grantNumber">CCF-2112665</rs> (<rs type="projectName">TILOS AI Research Institute</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_d2nG3a5">
					<idno type="grant-number">1553284</idno>
					<orgName type="grant-name">CAREER award</orgName>
				</org>
				<org type="funding" xml:id="_SfZ8CHb">
					<idno type="grant-number">2134108</idno>
					<orgName type="grant-name">SCALE MoDL award</orgName>
				</org>
				<org type="funded-project" xml:id="_VPZEzUr">
					<idno type="grant-number">CCF-2112665</idno>
					<orgName type="project" subtype="full">TILOS AI Research Institute</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local and global properties in networks of processors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anglouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How graph neural networks go beyond weisfeiler-lehman?</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>A new perspective on. submitted to ICLR 2022</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the power of color refinement</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Johannes K ?bler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Fundamentals of Computation Theory (FCT)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lelarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Canonical labelling of graphs in linear average time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kucera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random graph isomorphism</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Erd?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Selkow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On a routing problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quart. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="87" to="90" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02910</idno>
		<title level="m">Equivariant Subgraph Aggregation Networks. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman go cellular: CW networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Ch?telat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09544</idno>
		<title level="m">Combinatorial optimization and reasoning with graph neural networks. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On generalization bounds of a family of recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>Int. Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering symbolic models from deep learning with inductive biases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals, and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coloring graph neural net-works for node disambiguation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ETA prediction with graph neural networks in Google Maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiltshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15544</idno>
		<title level="m">Graph Neural Networks are Dynamic Programmers. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafa</forename><forename type="middle">G</forename><surname>?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The expressive power of kth-order invariant graph networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12035</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Let&apos;s agree to degree: Comparing graph convolutional networks in the message-passing framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mazowiecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><forename type="middle">A</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pebble games and linear equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="844" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graph Representation Learning</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep models of interactions across sets</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weak models of distributed computing, with connections to modal logic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>J?rvisalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuusisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laurinharju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lempiainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luosto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Virtema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Distributed Computing (PODC)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Complexity Theory Retrospective, chapter Describing graphs: A first-order approach to graph canonization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="59" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Polynomial bounds for the VC dimension of sigmoidal and general Pfaffian networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Macintyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A congruence theorem for trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="961" to="968" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graphs identified by logics with counting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mathematical Foundations of Computer Science (MFCS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Vapnik-Chervonenkis dimension of recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="223" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On tables of random numbers</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="387" to="395" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) -Workshop Track</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Charton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Transferability of spectral graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A PAC-bayesian approach to generalization bounds for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Sign and Basis Invariant Networks for Spectral Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13013</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Locality in distributed graph algorithms</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">kdd</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">How hard is to distinguish graphs with graph neural networks?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The power of graph convolutional networks to distinguish random graph models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Magner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Automatic generation of complementary descriptors with molecular graph networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Merkwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Motifnet: a motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Science Workshop (DSW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Janossy pooling: Learning deep permutationinvariant functions for variable-size inputs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">What can be computed locally?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Distributed Computing: A Locality-Sensitive Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">From graph low-rank global attention to 2-FWL approximation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<title level="m">Deep Learning with Sets and Point Clouds. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Equivariance through parameter-sharing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The grgaph isomorphism disease</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Corneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="363" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Graphon neural networks and the transferability of graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F O</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Graph neural networks: architectures, stability and transferability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="660" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining (SDM)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Computational capabilities of graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The Vapnik-Chervonenkis dimension of graph and recursive neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Learning with kernels. Adaptive Computation and Machine Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Counting Substructures with Higher-Order Graph Neural Networks: Possibility and Impossibility Results</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Neural arithmetic logic units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">A Collection of Mathematical Problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ulam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
			<publisher>Interscience Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; its Applications</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="264" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural message-passing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">On the limitations of representing functions on sets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">On Construction and Identification of Graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">What can neural networks reason about?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Universal approximations of invariant maps by neural networks. Constructive Approximation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Neural-symbolic VQA: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Identity-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Persistence enhanced graph neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>Int. Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
