<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nonlinear Wavelet Image Processing: Variational Problems, Compression, and Noise Removal Through Wavelet Shrinkage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Antonin</forename><surname>Chambolle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nam-Yong</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Bradley</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
						</author>
						<title level="a" type="main">Nonlinear Wavelet Image Processing: Variational Problems, Compression, and Noise Removal Through Wavelet Shrinkage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35C4EEAF7338597437C71218CB326E4E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image compression</term>
					<term>noise removal</term>
					<term>variational problems</term>
					<term>wavelets</term>
					<term>wavelet shrinkage</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper examines the relationship between wavelet-based image processing algorithms and variational problems. Algorithms are derived as exact or approximate minimizers of variational problems; in particular, we show that wavelet shrinkage can be considered the exact minimizer of the following problem: Given an image F defined on a square I; minimize over all g in the Besov space B 1 1 (L 1 (I)) the functional jjF 0 gjj 2 L (I) + jjgjj B (L (I)) : We use the theory of nonlinear wavelet image compression in L2(I) to derive accurate error bounds for noise removal through wavelet shrinkage applied to images corrupted with i.i.d., mean zero, Gaussian noise. A new signal-to-noise ratio (SNR), which we claim more accurately reflects the visual perception of noise in images, arises in this derivation. We present extensive computations that support the hypothesis that near-optimal shrinkage parameters can be derived if one knows (or can estimate) only two parameters about an image F : the largest for which F 2 B q (L q (I)); 1=q = =2 + 1=2; and the norm jjF jj B (L (I)) : Both theoretical and experimental results indicate that our choice of shrinkage parameters yields uniformly better results than Donoho and Johnstone's VisuShrink procedure; an example suggests, however, that Donoho and Johnstone's SureShrink method, which uses a different shrinkage parameter for each dyadic level, achieves lower error than our procedure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HIS PAPER has several objectives. The first is to describe several families of variational problems that can be solved quickly using wavelets. These variational problems take the form: Given a positive parameter and an image, a signal, or noisy data defined for in some finite domain find A. Chambolle is with CEREMADE (CNRS URA 749), Universit√© de Paris-Dauphine, 75775 Paris Cedex 16, France (e-mail: antonin.chambolle@ceremade.dauphine.fr). R. A. DeVore is with the Department of Mathematics, University of South Carolina, Columbia, SC 29208 USA (devore@math.sc.edu).</p><p>N.-Y. Lee and B. J. Lucier are with the Department of Mathematics, Purdue University, West Lafayette, IN 47907-1395 USA (e-mail: nylee@math.purdue.edu; lucier@math.purdue.edu).</p><p>Publisher Item Identifier S 1057-7149(98)01784-9.</p><p>a function that minimizes over all possible functions the functional <ref type="bibr" target="#b0">(1)</ref> where is the root-mean-square error (or more generally, difference) between and and is the norm of the approximation in a smoothness space The original image could be noisy, or it could simply be "messy" (a medical image, for example), while would be a denoised, segmented, or compressed version of</p><p>The amount of noise removal, compression, or segmentation is determined by the parameter if is large, then necessarily must be smaller at the minimum, i.e., must be smoother, while when is small, can be rough, with large, and one achieves a small error at the minimum. These types of variational problems have become fairly common in image processing and statistics; see, e.g., <ref type="bibr" target="#b25">[27]</ref>. For example, Rudin-Osher-Fatemi <ref type="bibr" target="#b26">[28]</ref> set to the space of functions of bounded variation for images (see also <ref type="bibr" target="#b0">[1]</ref>), and nonparametric estimation sets to be the Sobolev space of functions all of whose th derivatives are square-integrable; see the monograph by Wahba <ref type="bibr" target="#b27">[29]</ref>. In fact, can be very general; one could, for example, let contain all piecewise constant functions, with equal to the number of different pieces or segments of this would result in segmentation of the original image Indeed, Morel and Solimini <ref type="bibr" target="#b24">[26]</ref> argue that almost any reasonable segmentation algorithm can be posed in this form. Techniques like this are also known as Tikhonov regularization; see <ref type="bibr" target="#b1">[2]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, we considered <ref type="bibr" target="#b0">(1)</ref> in the context of interpolation of function spaces; in that theory, the infimum of (1) over all is the -functional of between and A fast way of solving (1) is required for practical algorithms. In <ref type="bibr" target="#b10">[11]</ref>, we noted that the norms of in many function spaces can be expressed in terms of the wavelet coefficients of In other words, if we choose an (orthogonal or biorthogonal) wavelet basis for and we expand in terms of its wavelet coefficients, then the norm is equivalent to a norm of the wavelet coefficients of see, for example, <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or <ref type="bibr" target="#b18">[20]</ref>.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, we proposed that by choosing to be one of these norms and by calculating approximate minimizers rather than exact minimizers, one can find efficient computational algorithms in terms of the wavelet coefficients of the data In particular, we showed how choosing and approximately minimizing (1) leads to wavelet algorithms that are analogous to well-known linear algorithms for compression and noise removal. In particular, we find that the wavelet coefficients of are simply all wavelet coefficients of with frequency below a fixed value, determined by Additionally, we proposed choosing from the family of Besov spaces with and satisfying</p><p>(2)</p><p>These spaces, which contain, roughly speaking, functions with derivatives in arise naturally in two contexts. First, in image compression using wavelets, if can be approximated to in by wavelet sums with nonzero terms, then is necessarily in (Because of certain technicalities, this statement only approximates the truth; see <ref type="bibr" target="#b5">[6]</ref> for precise statements and definitions.) Conversely, if is in then scalar quantization of the wavelet coefficients with scale-dependent quantization levels yields compression algorithms with convergence rates of in (There is a complete theory for compression in for <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b11">[12]</ref>.) We emphasize that this is an equivalence-you achieve a given rate of approximation with wavelet image compression if and only if is in the corresponding Besov smoothness class. Second, one can ask which Besov spaces of minimal smoothness are embedded in One wishes to use function spaces of minimal smoothness to allow as many sample functions as possible. A variant of the Sobolev embedding theorem implies that Besov spaces of minimal smoothness necessarily satisfy <ref type="bibr" target="#b1">(2)</ref>. We note that restricting attention to Besov spaces with does not allow us to consider other important spaces, such as as proposed by Rudin-Osher-Fatemi <ref type="bibr" target="#b26">[28]</ref>, or the space both these spaces, while slightly larger than (which does satisfy (2)), are also contained in When approximate minimizers of (1) have wavelet expansions containing the wavelet coefficients of larger than a threshold determined by These nonlinear algorithms are related to threshold coding or certain types of progressive transmission in image compression. In <ref type="bibr" target="#b10">[11]</ref>, we provided simple analyzes of the performance of these algorithms. In all cases, the nonlinear algorithms are preferable to the linear for two reasons. First, the nonlinear algorithm achieves a given performance level for more images than the linear algorithm does. Second, for a fixed image, it is possible (even likely) that the nonlinear algorithm will achieve a higher level of performance than the linear algorithm; the converse never occurs in compression.</p><p>These nonlinear algorithms are related to the large body of work by Donoho and Johnstone on what they call wavelet shrinkage. Indeed, whereas the nonlinear algorithms derived in <ref type="bibr" target="#b10">[11]</ref> threshold the wavelet coefficients of to find the coefficients of wavelet shrinkage takes the coefficients of absolute value larger than the threshold and shrinks them by the threshold value toward zero. We show in Section III that wavelet shrinkage is the exact minimizer of (1) when and is given by a (waveletdependent) norm equivalent to the usual norm. (Examining the case in (2) was actually motivated by the practical success of setting in <ref type="bibr" target="#b26">[28]</ref>.) In a series of papers (see, e.g., <ref type="bibr">[17]</ref> and <ref type="bibr" target="#b17">[19]</ref>), Donoho and Johnstone show that wavelet shrinkage leads to near-optimal noise removal properties when the images are modeled stochastically as members of several Besov spaces</p><p>The second goal of this paper is to prove claims presented in <ref type="bibr" target="#b10">[11]</ref> about the rate of Gaussian noise removal from images. We continue the program of <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b10">[11]</ref> in advocating a deterministic smoothness model for images (see also <ref type="bibr" target="#b22">[24]</ref>). This model has proved highly successful in image compression <ref type="bibr" target="#b5">[6]</ref>, and we show here that it also leads to good results in analyzing noise removal. This model assigns two numbers, a smoothness class specified by and a norm to each image, and uses these numbers to choose the parameter We show that knowing (or estimating) leads to a better estimate of the smoothing parameter, and knowing allows one to make an even finer estimate of the optimal (Note: After this paper was submitted, we discovered that Donoho and Johnstone <ref type="bibr" target="#b16">[18]</ref> have previously calculated the same first-order dependence of the error on in some ways, our arguments parallel theirs.) Our results have several properties that make them useful for practical image processing. We consider images in Besov spaces of minimal smoothness with If images are assumed to have less smoothness, then weaker (or no) results hold. We explicitly consider models of data and observations in which measurements are not point values, but measurements of integrals of the image with local point spread functions. (One can think of this as point evaluation of a convolution of the image with a smoothing kernel.) The measurement functionals can be averages of an intensity field over pixels in the image, a model that closely fits the physics of CCD cameras. Not using point values is a mathematical necessity, because point values are not defined for the spaces of minimal smoothness we consider, and are not well defined for images. (One cannot define the point value of an image at an internal edge separating regions of light and dark, for example.) Restricting the data acquisition model to point evaluation implies that images are continuous (and that which may be natural in some contexts, but not for image processing, in which intensity fields are more naturally modeled as discontinuous functions.</p><p>The final goal of this paper is to provide rather sharp estimates of the best wavelet shrinkage parameter in removing Gaussian noise from images. We show through rather extensive computational examples that our analysis often leads to a that is within 10% of the optimal and which is generally 1/2-1/4 the shrinkage parameter suggested by Donoho and Johnstone in their VisuShrink method. In other words, the VisuShrink parameter leads to oversmoothing of the noisy image, resulting in the unnecessary loss of image details.</p><p>The rest of the paper is organized as follows. In Section II, we review some properties of wavelets and smoothness spaces that we need in the following sections. In Section III, we recall briefly from <ref type="bibr" target="#b10">[11]</ref> how our abstract framework leads quite naturally to common algorithms in image processing, and we expand further on this method by solving several more variational problems of interest. It is here that we show that wavelet shrinkage is equivalent to solving (1) with</p><p>In Section IV, we show how to compute accurate wavelet-based image representations given pixel measurements of very general form. Donoho discusses another approach to this problem in <ref type="bibr" target="#b13">[14]</ref>. In Section V, we review the simplified theory of wavelet compression in presented in <ref type="bibr" target="#b10">[11]</ref>; we use this theory in Section VI on noise removal. In Section VII, we formulate our noise-removal model and prove the main result of the paper on the rate of noise removal by wavelet shrinkage. It is here that we introduce a new signal-to-noise ratio (SNR) that is useful in estimating the size of the shrinkage parameter.</p><p>We note that all the preceding results apply not only to computations with orthogonal wavelets, but also to computations with biorthogonal wavelets (see, e.g., the analysis by Donoho in <ref type="bibr" target="#b14">[15]</ref>), and we use biorthogonal wavelets in Section III, where we present the results of rather extensive computational tests of wavelet shrinkage applied with the shrinkage parameter suggested in Section VI. Although our results are uniformly better than the VisuShrink procedure of Donoho and Johnstone [17], it appears from a single example that their later SureShrink procedure <ref type="bibr" target="#b15">[16]</ref>, which uses different shrinkage parameters for different dyadic levels, may give better results than our method in some cases.</p><p>A longer version of this paper, containing more discussion and the omitted proofs, can be found at http://www.math.purdue.edu/ lucier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Authors' Notes</head><p>We take the opportunity here to remark on issues arising from several previous papers.</p><p>Extra assumptions are needed in <ref type="bibr" target="#b10">[11]</ref> to prove the stated rate of convergence of the noise removal algorithm with oracle; one can assume, for example, that the image intensity is bounded, which is quite natural for problems in image processing.</p><p>In <ref type="bibr" target="#b8">[9]</ref>, we presented an interpretation of certain biorthogonal wavelets as derived in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b20">[22]</ref> that owed much to the connection between these wavelets and function reconstruction from cell averages as used in computational fluid dynamics. This connection was first recognized and developed by Harten (see, e.g., <ref type="bibr" target="#b19">[21]</ref>), whose work provided the inspiration for the approach taken in <ref type="bibr" target="#b8">[9]</ref>; we regret that Harten's work was not properly recognized in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WAVELETS AND SMOOTHNESS SPACES</head><p>In this paper, images (light intensity fields) are functions defined on the square and we consider variational problems of the form where is a space of test functions (generally embedded in is a positive parameter, and is an exponent that is chosen to make the computations (and analysis) easier.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, we suggested using spaces for which the norm of in is equivalent to a sequence norm of the wavelet coefficients of Let us first consider (real) orthogonal wavelets on as described by Cohen, Daubechies, and Vial <ref type="bibr" target="#b3">[4]</ref>. One begins with a one-dimensional (1-D) orthogonal wavelet such that if we set to be the scaled When one is concerned with a finite domain, e.g., the square then two changes must be made to this basis for all of to obtain an orthonormal basis for First, one does not consider all scales but only nonnegative scales and not all shifts but only those shifts for which intersects nontrivially. Second, one must adapt the wavelets that overlap the boundary of in order to preserve orthogonality on the domain. (Specifically, the modified for look more like the functions There are several ways to do this; the paper <ref type="bibr" target="#b3">[4]</ref> gives perhaps the best way and some historical comparisons. To ignore all further complications of this sort, we shall not precisely specify the domains of the indices of the sums and write for <ref type="bibr" target="#b2">(3)</ref> Not only can one determine whether is in by examining the coefficients but one can also determine whether is in many different function spaces</p><p>We shall consider the family of Besov spaces and These spaces have, roughly speaking, "derivatives" in the third parameter allows one to make finer distinctions in smoothness. Various settings of the parameters yield more familiar spaces. For example, when then is the Sobolev space and when and is the Lipschitz space When or then these spaces are not complete normed linear spaces, or Banach spaces, but rather complete quasinormed linear spaces; that is, the triangle inequality may not hold, but for each space there exists a constant such that for all and in With a certain abuse of terminology, we shall continue to call these quasinorms "norms".</p><p>The application of Besov spaces to image compression with wavelets can be found in <ref type="bibr" target="#b5">[6]</ref>, where the intrinsic definition of the Besov space norm can be found on p. 727. We need the following facts from <ref type="bibr" target="#b5">[6]</ref>. Assume that and satisfy so that is embedded in If there exists an integer such that for all and all pairs of nonnegative integers with and for and for some (the set of and for which this is true depends on then the norm is equivalent to a norm of the sequence of coefficients (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When</head><p>there is an obvious simplification <ref type="bibr" target="#b4">(5)</ref> In this paper means that there exist positive constants and such that for all The constants and depend on the parameters and and on the wavelet basis the expression on the right of ( <ref type="formula">5</ref>) is wavelet dependent.</p><p>We always use the equivalent sequence norm (4) in our calculations with</p><p>In the variational problem (1) the difference between and is always measured in Thus, there are two scales of Besov spaces of importance. The first is which measures smoothness of order in for which</p><p>The second is the scale of spaces with these are the spaces of the form with of minimal smoothness to be embedded in for which Another important fact that arises immediately from ( <ref type="formula">5</ref>) is that is embedded in if or see, e.g., <ref type="bibr" target="#b23">[25]</ref>. We need a bound on the smoothness in the Sobolev space of bounded functions in Our argument is typical of those used in the theory of interpolation of function spaces. For any bounded (not a practical restriction for images), we have It follows that is in since since Thus ( <ref type="formula">6</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SOLVING VARIATIONAL PROBLEMS WITH WAVELETS</head><p>Following the suggestion in <ref type="bibr" target="#b10">[11]</ref>, we now consider the problem: Find that minimizes over all the functional <ref type="bibr" target="#b6">(7)</ref> Using ( <ref type="formula">3</ref>) and ( <ref type="formula">5</ref>), we can expand and in their wavelet expansions and solve instead the equivalent problem of finding the minimizer of the functional <ref type="bibr" target="#b7">(8)</ref> One notes immediately that the infinite-dimensional nonlinear problem <ref type="bibr" target="#b6">(7)</ref> completely decouples in the wavelet representation to the separable problem <ref type="bibr" target="#b7">(8)</ref>. That is, one minimizes (8) by minimizing separately over for each and While ( <ref type="formula">8</ref>) can be minimized exactly in several interesting cases, an approximate minimizer can always be found. The problem reduces to finding the minimizer given of <ref type="bibr" target="#b8">(9)</ref> where and First, note that if is not between zero and then we can reduce by changing to be the closer of zero and thus, we can assume without loss of generality that is between 0 and Next, we remark that if then is no less than and if then is no less than Thus, if we set we have which is within a factor of of the minimum of <ref type="bibr" target="#b8">(9)</ref>. Using this formula for we can construct an approximate minimizer to <ref type="bibr" target="#b6">(7)</ref>. In the special case this reduces to setting when i.e., <ref type="bibr" target="#b9">(10)</ref> and otherwise setting to zero. This means that we keep in all coefficients such that is small enough (i.e., has low enough frequency) to satisfy <ref type="bibr" target="#b9">(10)</ref>, without regard to the relative sizes of the coefficients Since this is an approximate solution of the nonparametric estimation problem <ref type="bibr" target="#b27">[29]</ref>.</p><p>The other interesting special case is when so that has minimal smoothness to be embedded in In this case, and we set when i.e., and otherwise set to zero. Here, we keep in all coefficients above a certain threshold, without regard to the value of or equivalently, without regard to the frequency of Motivated by the practical success of Rudin-Osher-Fatemi <ref type="bibr" target="#b26">[28]</ref> in using in (1), we set and find that so we consider the space This space is very close to since Thus, it is interesting to consider separately the case where and In this case, calculus shows that the exact minimizer of is given by Thus we shrink the wavelet coefficients toward zero by an amount to obtain the exact minimizer This is precisely the wavelet shrinkage algorithm of Donoho and Johnstone <ref type="bibr">[17]</ref>. Thus, wavelet shrinkage can be interpreted as the solution of the minimization problem (1) using with its wavelet-dependent norm. In the spirit of searching for spaces of minimal smoothness for we note that when and not otherwise. (This can be easily derived from (4).) In fact, we can choose for any value of if then the spaces are not contained in but if is in then any minimizer of ( <ref type="formula">1</ref>) is necessarily in because is finite. Another issue of practical interest is whether the space contains images with edges, i.e., functions that are discontinuous across curves. does allow such images, but does not for When substituting the equivalent norm (4) in <ref type="bibr" target="#b0">(1)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(with yields</head><p>This problem no longer decouples as it does for On the other hand, calculus again shows that the minimizer satisfies where is the usual sign function is when is positive, when is negative, and zero when and is a scale-dependent shrinkage factor. If we denote by the dual exponent to that satisfies then one finds that This result obviously holds when for which we derived that for all so As a practical matter, we explain how to solve two related variational problems. First, we consider Using our usual substitution of wavelet coefficients, this reduces to setting where <ref type="bibr" target="#b10">(11)</ref> Thus, even though the problem does not decouple completely, as when we see that it decouples by scale. Given and the set of coefficients at a given scale ( <ref type="formula">11</ref>) is an implicit formula for that is easily solved. For example, one can sort in order of decreasing absolute value to obtain a sequence with this takes operations at scale Next one examines each coefficient in turn; when it first happens that for a particular value then one knows that is between and the next larger coefficient, and can be found by solving a trivial linear equation. This takes at most operations. The second variational problem we examine is to minimize <ref type="bibr" target="#b11">(12)</ref> Instead of solving this problem directly, we solve the following dual problem: Find the minimum of given <ref type="bibr" target="#b12">(13)</ref> for any fixed Any solution of ( <ref type="formula">12</ref>) is again a solution of <ref type="bibr" target="#b12">(13)</ref> with</p><p>After applying the wavelet transform, this dual problem is to minimize given Again the problem decouples by scale If, for a given then we obviously minimize at that scale by setting for all and Otherwise, a continuity argument can be used to show that the minimizer at level is also the minimizer of for some unknown we have already seen from our discussion of the minimization problem that the solution is</p><p>We choose such that</p><p>An algorithm similar to that given for the problem now suffices to find</p><p>To each we associate the value Using arguments from convex analysis, it can be shown that is finite and that the solution of our dual problem ( <ref type="formula">13</ref>) is the minimizer of ( <ref type="formula">12</ref>) with the associated value of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. WAVELET REPRESENTATION OF IMAGES</head><p>The purpose of this section is to relate more directly wavelet-based image processing to the observed pixel values. Our view of a digitized image is that the pixel values (observations) are samples, which depend on the measuring device, of an intensity field for on the square We start with the simplest model, that of a CCD camera, where the pixel samples are well modeled by averages of the intensity function over small squares. Furthermore, let's consider, in this special case, the Haar wavelets on the square.</p><p>We assume that pixel values are indexed by in the usual arrangement of rows and columns, and that each measurement is the average value of on the subsquare covered by that pixel. To fix notation, we note that the th pixel covers the square with sidelength and lower-left corner at the point We denote the characteristic function of by and the -normalized characteristic function of by</p><p>We can write each pixel value as</p><p>The standard practice in wavelet-based image processing is to use the observed pixel values to create the function which we call the observed image. It follows that if the wavelet expansion of the intensity field is then the wavelet expansion of is</p><p>The main point is that is the projection of onto Furthermore, if is in any function space whose norm is determined by a sequence norm of (Haar) wavelet coefficients, then so is and the sequence space norm of is no greater than the sequence space norm of One often uses wavelets that are smoother than the Haar wavelets. We consider this case now, without, however, changing our method of observation. In this more general setting, we assume for each scale the existence of orthonormal scaling functions associated with the orthonormal wavelet basis of Away from the boundary of for a single with and for near the boundary we assume the existence of special boundary scaling functions and wavelets and as posited in Section III. We also assume that span span contains all polynomials degree for some positive (For Haar wavelets, Thus,</p><p>For technical reasons, we assume that there exists a constant such that the support of each and is contained in a square with side-length that contains the point and that for all these conditions are satisfied for existing wavelet bases.</p><p>The projection of onto is now However, we have not measured but so our new "observed image" is which is not the projection of onto Thus, when we work with rather than we would like to bound the error that is added by not using the correct projection of the intensity field</p><p>The following principle was suggested by Cohen et al. <ref type="bibr" target="#b3">[4]</ref>: If is a polynomial of degree then should be a polynomial of degree</p><p>In <ref type="bibr" target="#b3">[4]</ref> they presented an algorithm to modify the to satisfy this principle; here we show that this is sufficient to guarantee that the error in using instead of is bounded in a reasonable way. To simplify our discussion, we consider the algorithm in one space dimension, i.e., Extensions to two dimensions are straightforward but notationally cumbersome.</p><p>We first note that if is a polynomial of any degree, then for a different polynomial of the same degree. Thus, if has degree we would like to again be a polynomial of degree for This will be true away from the boundary of where but not near the boundary of However, <ref type="bibr">Cohen et</ref>  We also have and However, is a polynomial of degree whenever is. This algorithm for changing the orthogonal wavelets to biorthogonal wavelets can be extended to multiple dimensions by tensor products.</p><p>After constructing these biorthogonal wavelets, we consider the case of completely general measurements given by where is the point spread function of the physical measuring device at location generally, this function has no relation whatsoever to</p><p>We assume that the point spread functions satisfy the following for constants independent of and . 1) The support of contains the point and is contained in an interval of width 2)</p><p>3) Except for values of near 0 and for a function normalized so that and . These conditions seem rather mild-they say that the point spread functions have bounded support, are not too big, are translation invariant away from the boundary, and are able to distinguish between polynomials of degree We note that for any polynomial of degree for another polynomial of the same degree, except for the leftmost and rightmost values of This is because for all and Because of our third assumption, the matrix . . . . . . . . . has full rank, and so does the similar matrix at the right side of the interval. Thus, there exist invertible matrices and such that if . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and otherwise, then for all</head><p>Applying these postprocessing matrices and is equivalent to modifying so that for any polynomial of degree Numerically, we compute the inner products with the modified point spread functions by applying the appropriate postprocessing matrices to the observed values near the boundary of Thus, with these suitably modified point spread functions and scaling functions, we have for any polynomial on of degree for a different polynomial on of the same degree. We take our observed image to be In two dimensions, if the point spread function is of the form then one can easily extend this construction using tensor products. In this case, one must apply pre-and postconditioning to the pixel values in the rows and columns immediately adjacent to the boundaries of the image.</p><p>With some work, one can prove two things about The first is that is close to If and are compatible to order in the sense that (</p><p>Since, by construction, for all ( <ref type="formula" target="#formula_0">14</ref>) is always true for so ( <ref type="formula">15</ref>) is true at least for Since the same result holds in two dimensions, and images with edges have this is sufficient for most purposes. If, in addition, away from the boundary of and are symmetric about the point then ( <ref type="formula" target="#formula_0">14</ref>) holds for and <ref type="bibr" target="#b14">(15)</ref> holds for This condition is satisfied for many biorthogonal scaling functions (but not for Daubechies' orthogonal wavelets), and, we presume, for many point spread functions.</p><p>The second property is that is just as smooth in Besov spaces as Specifically, for those values of and in dimensions such that is equivalent to a sequence norm of the wavelet coefficients of then With these two properties, all the theorems in this paper that are proved for also hold for the "observed image" once one takes into account the difference between and bounded by <ref type="bibr" target="#b14">(15)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LINEAR AND NONLINEAR COMPRESSION</head><p>In this section we briefly present some results from <ref type="bibr" target="#b10">[11]</ref> (which are all classical, by this time; see <ref type="bibr" target="#b10">[11]</ref> for references) that we need in the following sections.</p><p>The first result concerns what we call linear compression. We take for our approximation to the wavelet approximation i.e., we include in the approximation all coefficients with frequency less than This corresponds to progressive transmission, if one sends all coarse level wavelet coefficients before the finer level coefficients. We find that Thus, <ref type="bibr" target="#b15">(16)</ref> For our nonlinear compression algorithm, we take Thus, we take all large coefficients, no matter their frequency, with the extra provision that we must work from the data, i.e., If we assume that then the number of coefficients greater than satisfies so ( <ref type="formula">17</ref>) and <ref type="bibr" target="#b16">(18)</ref> since If is nonzero, then (17) implies <ref type="bibr" target="#b17">(19)</ref> and ( <ref type="formula">18</ref>) and ( <ref type="formula">19</ref>) yield <ref type="bibr" target="#b18">(20)</ref> Note that, since there are terms in and terms in we get the same rate of approximation in each estimate; the only difference is that we use different norms to measure the smoothness of The nonlinear estimate is better for two reasons: For a given there are more images in than there are in and if a fixed image is in , then it is in for some i.e., a given image will have greater smoothness in one of the nonlinear smoothness spaces than in the Sobolev spaces This analysis also applies to any compression scheme that satisfies with Thus, it applies to threshold coding, zero-tree coding (see <ref type="bibr" target="#b9">[10]</ref>), scalar quantization, and most types of vector quantization algorithms.</p><p>It is remarked in <ref type="bibr" target="#b5">[6]</ref> that ( <ref type="formula">20</ref>) is invertible, i.e., if we observe for some and then one can conclude that and one can define an equivalent norm on such that in this norm (This statement is incorrect, but is close enough to the truth to be useful in practice; see <ref type="bibr" target="#b5">[6]</ref> for the precise statement.) Thus, observing convergence rates for nonlinear wavelet approximations allows one to estimate the Besov smoothness of images.</p><p>Although the theory in <ref type="bibr" target="#b5">[6]</ref> relates the image error to the number of nonzero coefficients, and not the number of bytes in the compressed file, the latter two quantities are closely related in practice; see, e.g., Fig. <ref type="figure" target="#fig_5">15</ref> of <ref type="bibr" target="#b5">[6]</ref>. The discussion there shows that, on average, only 6 b were stored in the compressed image file for each nonzero coefficient. The computational results here provided a similar relationship between number of nonzero coefficients and file sizes of compressed images.</p><p>If is also in for some then</p><p>In other words, if then the (unobserved) rate of convergence of is the same as the observed rate of convergence of Since observing the rate of approximation for smaller (middle bit rates) gives a better estimate for the smoothness of than for the highest bit rates. By <ref type="bibr" target="#b5">(6)</ref>, if is bounded, then and we get better estimates of convergence rates if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LINEAR AND NONLINEAR NOISE REMOVAL</head><p>In the previous sections, we assumed that the measured pixel values were the exact values of (averages over squares in the Haar case). In this section, we assume that our measurements are corrupted by Gaussian noise, that is, that we measure not but where are i.i.d. normal random variables with mean zero and variance (denoted by From this we construct Since the wavelet transform that takes is an orthonormal transformation, we have that and where the are i.i.d. random variables. This model assumes that the expected value of the noise is independent of the number of pixels. We now examine how the linear and nonlinear algorithms can be applied to to achieve noise removal.</p><p>Starting with the linear algorithm calculates we choose to minimize Using the wavelet decompositions of and we calculate</p><p>The inequality follows from ( <ref type="formula">16</ref>), and the second equality holds because the random variables each have variance</p><p>We set and we minimize with respect to</p><p>Calculus shows that we overestimate the error at most by a factor of 2 if we set the two terms in our bound equal to each other, i.e., This yields and Using <ref type="bibr" target="#b15">(16)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>again with gives</head><p>This linear algorithm removes all terms with greater than or equal to a threshold these terms can be considered to have frequency at least Thus, the linear method considers any low-frequency structure to be signal, and any high-frequency structure to be noise, no matter how large the scaled amplitude of the signal, might be. This is not acceptable to people, such as astronomers, who deal with high-amplitude, small extent (and, hence, high-frequency) signals. Some astronomical researchers have proposed eliminating the low-order bit planes to achieve noise removal and compression if these bit planes have entropy close to one. This will remove all low amplitude features, no matter how great their extent, for example, variations in the level of background radiation. The nonlinear algorithm presented next, which employs Donoho and Johnstone's wavelet shrinkage, recognizes both high-amplitude, high-frequency structures and low-amplitude, low-frequency structures as signals. Similar algorithms have been used in astronomical calculations, e.g., by White <ref type="bibr" target="#b28">[30]</ref>.</p><p>We define the shrinkage operator Our noise-removed image is where is to be determined, and the error is Since for all one has Thus, if is normally distributed with mean zero and variance with probability distribution <ref type="bibr" target="#b19">(21)</ref> In estimating the error of noise removal by wavelet shrinkage, we now apply <ref type="bibr" target="#b19">(21)</ref> to the case where and where is to be determined.</p><p>Recall that if denotes the number of coefficients with <ref type="bibr" target="#b20">(22)</ref> while <ref type="bibr" target="#b21">(23)</ref> Combining ( <ref type="formula">21</ref>)-( <ref type="formula">23</ref>) yields <ref type="bibr" target="#b22">(24)</ref> where we have bounded the number of coefficients with simply by Inequality ( <ref type="formula">24</ref>) is our main estimate. We note, and emphasize, that given only two parameters characterizing the smoothness of an image from which we derive and and an estimate of the standard deviation of the noise in the image, one can numerically minimize <ref type="bibr" target="#b22">(24)</ref> with respect to and use as the value of that minimizes our bound on the error. We apply this technique in Section VII to various images.</p><p>Using the symbolic manipulation package Maple, we find that the last term in ( <ref type="formula">24</ref>) is bounded by <ref type="bibr" target="#b23">(25)</ref> for all in fact <ref type="bibr" target="#b23">(25)</ref> is the first term of the asymptotic expansion of and as</p><p>We can get a simple approximation to the critical and a bound on the error. One can determine so that or <ref type="bibr" target="#b24">(26)</ref> With this we have</p><p>If we assume that and is large enough that then since and If is bounded, then ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b15">(16)</ref> show that Thus, we achieve the same rate of approximation to the real intensity field as we do to the sampled image In <ref type="bibr" target="#b24">(26)</ref>, the quantity <ref type="bibr" target="#b25">(27)</ref> arises quite naturally. Insofar as can be interpreted as a measure of the structure in an image, can be interpreted as the amount of information in an image. Thus, we interpret <ref type="bibr" target="#b25">(27)</ref> as a pertinent (and important) new SNR that quantifies the visual effects of adding noise to an image more reliably than the usual SNR based on the norm of This quantity also arises naturally in an analysis of the waveletvaguelette transform together with wavelet shrinkage applied to homogeneous integral equations-see <ref type="bibr" target="#b21">[23]</ref>.</p><p>We remark that a similar analysis can be applied to the wavelet truncation method of noise removal proposed in <ref type="bibr" target="#b10">[11]</ref>. In this case and where is the truncation function</p><p>We have A rather crude inequality is and if is normally distributed with mean zero and variance By following the rest of our analysis for wavelet shrinkage, the reader can discover new choices of and new error bounds, which have the same rate of convergence as our bounds for wavelet shrinkage. Since we have observations, Donoho and Johnstone have suggested using as a "universal" choice for in their VisuShrink method. These two suggestions agree as i.e., as This would seem to be a good choice for the examples given in <ref type="bibr">[17]</ref> and <ref type="bibr" target="#b17">[19]</ref>, as their first three sample signals (Blocks, Bumps, and HeaviSine) are in the 1-D Besov spaces for all That is, in spite of the discontinuities and peaks in these sample functions, they are infinitely smooth in the scale of spaces However, images with edges have severe inherent limitations in smoothness, since <ref type="bibr" target="#b5">[6]</ref>, so and the smoothing parameter in [17] and <ref type="bibr" target="#b17">[19]</ref> results in oversmoothing. In fact, our estimates of the smoothness of images in <ref type="bibr" target="#b5">[6]</ref> and several examples here suggest that for many images so the smoothing parameter should be even smaller. At high SNR's, with the smoothing parameter should be reduced even more.</p><p>If we ignore the change in due to the SNR, then our error bound with is smaller than the bound achievable with only by a factor of where Since achieves its minimum of about 0.69 when the error bounds are not all that different. However, the greater error using is introduced by shrinking the real image coefficients more than necessary, so the effect is quite noticeable visually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. NOISE REMOVAL COMPUTATIONS</head><p>We conducted experiments using wavelet shrinkage to remove Gaussian noise from some images. Our main conclusion is that shrinkage parameters chosen by minimizing <ref type="bibr" target="#b22">(24)</ref> lead to less shrinkage (smoothing), smaller errors, and better images than using the parameter suggested by Donoho and Johnstone in VisuShrink or than given by <ref type="bibr" target="#b24">(26)</ref>.</p><p>Our main computations are applied to the 24 images on the Kodak Photo CD Photo Sampler, Final Version 2.0, widely distributed by Apple Computer Corporation with its Macintosh computers. We propose that researchers in image processing consider these images as test images for new algorithms. The images are rather large (2048 3072 pixels), are of relatively high quality, cover a range of subjects (although all are of natural scenes), and, as we shall see, are of varied smoothness. All images on the CD have been released for any image processing use. It is not clear how the lossy compression algorithms applied to images in sizes above 512 768 pixels has affected the images for test purposes; we still believe they are of significantly higher quality then, e.g., the Lenna image.</p><p>We used the program hpcdtoppm -ycc to extract the intensity component of each Photo CD image img0001-img0024 at the 2048 3072 size. The intensity images were not gamma-corrected before being used in our tests. Smaller images were calculated by averaging pixel values over 2 2, 4 4, squares of pixels. We use the fifth-order-accurate 2-10 biorthogonal wavelets illustrated on p. 272 of <ref type="bibr" target="#b4">[5]</ref> (cf. <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b20">[22]</ref>). (As we noted in the introduction, all our results apply to biorthogonal wavelets.) These wavelets have several theoretical and practical advantages for image processing. First, if we assume that measured pixel values are true averages of an underlying intensity field then the wavelet coefficients we calculate are exactly those of since the dual functionals of these wavelets are piecewise constant. Although not orthogonal, these wavelets are not too far from orthogonal; they have few oscillations; and they decay rapidly away from zero, looking like smoothed versions of the Haar wavelet. They lend themselves to fast fixed-point computations, since the wavelet coefficients are dyadic rationals with small numerators. We modified the wavelets at the boundary in a way equivalent to reflecting the image across each side, so that our scheme is formally first-order accurate at boundaries.</p><p>We first estimated the smoothness of each image as discussed in Section V. After instrumenting our compression program to report the number of nonzero coefficients in each compressed image we compressed each image at various compression levels (from about 150-1 000 000 nonzero coefficients) using scalar quantization of the wavelet coefficients. We then calculated the best least-squares line that expressed the relationship between error and the number of nonzero coefficients on a log-log scale, i.e., We estimated that (the true intensity field) is in the Besov space and that These values are reported in Table <ref type="table" target="#tab_4">I</ref> for each image; we report the correlation coefficient (rounded to the nearest hundredth) as a guide to how well the error versus numberof-nonzero-coefficients curve was approximated by a straight line.</p><p>Since our noise removal error estimates depend on the resolution at which we sample the image, we downsampled each 2048 3072 image by averaging to obtain 1024 1536, 512 768, 256 384, 128 192, and 64 96 images. Our rectangular images do not satisfy our previous assumption that images are square with pixels on a side; nonetheless, we applied our formulas after substituting the number of pixels in our images, for</p><p>We then added i.i.d. Gaussian noise with standard deviation 32 to the pixels of each image to obtain noisy images Wavelet shrinkage with various parameters was applied to remove noise from each image and obtain noise-removed images and the error was measured. Table <ref type="table" target="#tab_4">I</ref> contains the results of our tests, which are reported using slightly different conventions than elsewhere in this paper. The listing for each image contains six lines, corresponding to the six sizes of our downsampled images, with the largest (original) image listed first. We calculated three values of the shrinkage parameter. The first, which does not depend on image smoothness parameters, is the one proposed by <ref type="bibr">Donoho and</ref>   We also report the square of the error after wavelet shrinkage. We square the error before reporting it to make comparison with our bound on easier. The error is normalized so that the error before shrinkage, is almost exactly 1024.</p><p>In addition to we calculated where was calculated from <ref type="bibr" target="#b24">(26)</ref>. This "easy" estimate does not minimize <ref type="bibr" target="#b22">(24)</ref>, but does take into account the smoothness parameter and the SNR is the error after shrinkage with Finally, we found the that minimized the bound <ref type="bibr" target="#b22">(24)</ref>, and calculated the "critical" parameter Since this calculation is trivial in practice, there is really no reason to use except for illustrative purposes. We report the error for this critical as</p><p>We resist calling the "optimal" There is an optimal parameter that does minimize the error We estimated as follows. We calculated the error after shrinkage by the parameters and and fitted a quadratic polynomial through these three points. We found the value of at which was a minimum, and called this value For each image in the table, we reported whether was between and i.e., whether the optimal was within 10% of the critical calculated by minimizing <ref type="bibr" target="#b22">(24)</ref>. The results show that the optimal was within 10% of in 106 of 144 cases.</p><p>We did not use the VisuShrink procedure in Wavelab, developed by Donoho and others, because their program is designed to handle images only of dimension for some and our images are rectangular, not square. We did compare their program to ours for the next example, and obtained essentially the same results, both visually and in the measured error-only the location of the artifacts was different.</p><p>As a final example, we compare VisuShrink, our method, and SureShrink on an image with extreme smoothness properties-a 512 512 section of fp1.pgm, the first test image of the FBI wavelet compression algorithm, available at ftp://ftp.c3.lanl.gov/pub/WSQ. The original image is displayed in Fig. <ref type="figure">1</ref>. We note first that although the image is rather smooth (there is a great deal of structure, but no texture and no real edges), it contains a lot of information. Thus, we expect with rather high but also high</p><p>We compressed this image at several compression levels, with a quantization strategy that attempts to minimize the error in and obtained for  In Fig. <ref type="figure">2</ref>, we show the same image with i.i.d. Gaussian noise with mean zero and standard deviation 32 added to each pixel. Let us denote the original pixels by and their average over the entire image by Then, an SNR for this image can be defined by In other words, the standard deviation of the signal is only about 1.55 times the standard deviation of the noise we added-this SNR is quite small, and this number leads us to expect the signal to be almost obliterated by the added noise. Obviously, this is nonsense. The added noise obscures hardly any information in the image at all-under any reasonable measure, the signal-to-noise ratio of the noisy image is extremely high. Our new definition <ref type="bibr" target="#b25">(27)</ref> of SNR gives which predicts that the noise hardly affects the visual perception of the signal at all. Fig. <ref type="figure" target="#fig_3">3</ref> shows the result of setting the VisuShrink value of Many image features are smoothed away. The root-mean-square error between the original and smoothed images is 26.832 186 9 grey scales.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows the result of minimizing <ref type="bibr" target="#b22">(24)</ref>, which gives and leads to an RMS error between the original and smoothed images of 17.010 997 6 grey scales. The RMS error when shrinking by is 17.030 531 6, while the RMS error when shrinking by is 17.163 467 9, so we estimate that the optimal for this problem is indeed within 10% of Plugging this value of into <ref type="bibr" target="#b22">(24)</ref> gives an upper  bound for the expected RMS error of 18.493 854 2 grey scales, which is fairly close to the real error. Fig. <ref type="figure" target="#fig_5">5</ref> shows the results of applying M. Hilton's implementation of SureShrink, available at http://www.cs. scarolina.edu/ABOUT \_US/ Faculty/Hilton/shrink-demo.html. SureShrink uses a different shrinkage parameter for each dyadic level (indeed, for each type of wavelet at each dyadic level). SureShrink leaves more noise in the smoothed image, but removes fewer image details. It achieves an RMS error of 13.363 228 3 grey scales, significantly better than VisuShrink or our method with the critical (or even optimal) We leave it to the reader to compare the visual quality of the two denoised images.</p><p>We believe that these results show several things. Minimizing our bound on the error (24) leads to near-optimal shrinkage parameters for noise removal with wavelet shrinkage when using a single shrinkage parameter for all dyadic levels. Our technique for estimating the smoothness of images leads to accurate estimates of the true smoothness of images. The performance of wavelet image processing algorithms can be predicted accurately using only the two smoothness parameters and Our new definition <ref type="bibr" target="#b25">(27)</ref> of an SNR is a better measure than the one typically used. SureShrink achieved better results on our rather extreme example than any wavelet shrinkage method that uses a single shrinkage parameter over all dyadic levels. Finally, even though there is no a priori reason to assume that the smoothness of real images can be characterized by only these two parameters, and even though it is easy to come up with images that do not satisfy this assumption (a montage of unrelated images, for example), in practice real images often have rather uniform smoothness over the entire image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Manuscript received October 23</head><label>23</label><figDesc>, 1996; revised March 10, 1997. The work of A. Chambolle was supported by the CNRS. The work of R. A. DeVore was supported in part by the Office of Naval Research under Contract N00014-91-J-1076. The work of N.-Y. Lee was supported by the Purdue Research Foundation. The work of B. J. Lucier was supported in part by the Office of Naval Research under Contract N00014-91-J-1152. Part of this work was performed while B. J. Lucier was a Visiting Scholar at CEREMADE, Universit√© de Paris-Dauphine. The associate editor coordinating the review of this paper and approving it for publication was Dr. Guillermo Sapiro.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>basis for that is, for the coefficients Associated with is a scaling function from which one generates the functions The set is orthonormal for fixed For example, the Haar wavelets have the characteristic function of the interval [0, 1), and We can easily construct two-dimensional (2-D) wavelets from the 1-D and by setting for and If we let then the set of functions forms an orthonormal basis for i.e., for every there are coefficients such that and Instead of considering the sum over all dyadic levels one can sum over for a fixed in this case, we have and where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Noise removed with = 0 2 0m p 2 ln 2 2m (VisuShrink).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Noise removed by minimizing (24).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Noise removed using SureShrink.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Beginning with the largest image, we have 179.0550, 170.9440, 162.4270, 153.4390, 143.8900, and 133.6610, respectively. The actual shrinkage parameter is but we thought comparisons would be easier if we left out the factor in the table. In all cases, the shrinkage parameters are multiples of the standard deviation of the noise, which is 32.</figDesc><table><row><cell cols="2">Johnstone in VisuShrink</cell><cell></cell></row><row><cell>where the image has</cell><cell>pixels</cell><cell>for the</cell></row><row><cell>largest image, etc.).</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I (</head><label>I</label><figDesc>Continued.) SHRINKAGE PARAMETERS AND ERRORS very high. The correlation coefficient of this line (on a log-log graph) is</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Antonin Chambolle received the Ph.D. degree in applied mathematics from the Universit√© de Paris-Dauphine, Paris, France, in 1993.</p><p>From 1993 to 1995 and since 1996 he has been a Centre National de la Recherche Scientifique Researcher at the CEREMADE Mathematics Laboratory, Paris-Dauphine. From 1995 to 1996, he was with the Sector of Functional Analysis, Institute for Advanced Studies, Trieste, Italy. His research interests include mathematical image processing, partial differential equations, variational problems involving noncontinuous functions, and the numerical analysis of these problems.</p><p>Ronald A. DeVore received the Ph.D. degree from The Ohio State University, Columbus, in 1967.</p><p>He is the Robert L. Sumwalt Professor of Mathematics at the University of South Carolina, Columbia, where he has been since 1977. His recent research interests center on fast, wavelet-based algorithms for image processing and numerical methods for nonlinear partial differential equations. He has published over 100 research articles and three research monographs in the areas of approximation theory, harmonic analysis, nonlinear partial differential equations, numerical analysis, and image processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nam-Yong</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian estimation of transmission tomograms using segmentation based optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nucl. Sci</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1144" to="1152" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convergence of Tikhonov regularization for constrained ill-posed problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chavent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inv. Probl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="63" to="76" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biorthogonal bases of compactly supported wavelets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Feauveau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="485" to="560" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wavelets on the interval and fast wavelet transforms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="54" to="81" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Ten Lectures on Wavelets</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<date type="published" when="1992">1992</date>
			<publisher>SIAM</publisher>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image compression through wavelet transform coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jawerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, Special Issue Wavelet Transforms Multires. Anal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="719" to="746" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface compression</title>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Geom. Design</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="219" to="239" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compression of wavelet decompositions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jawerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Popov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amer. J. Math</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="737" to="785" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying the smoothness of images: Theory and applications to wavelet image processing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1994 IEEE Int. Conf. Image Processing</title>
		<meeting>1994 IEEE Int. Conf. Image essing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Error bounds for image compression by zero-tree coding of wavelet coefficients</title>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast wavelet techniques for near-optimal image processing</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Military Commun. Conf. Rec</title>
		<imprint>
			<biblScope unit="page" from="1129" to="1135" />
			<publisher>IEEE Press</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear wavelet approximations in the space C ( d ); progress in approximation theory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petrushev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. US/USSR Conf. Approx</title>
		<meeting>US/USSR Conf. Approx<address><addrLine>Tampa, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="261" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpolation of Besov spaces</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Popov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="397" to="414" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">De-noising by soft-thresholding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="613" to="627" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear solution of linear inverse problems by waveletvaguelette decomposition</title>
	</analytic>
	<monogr>
		<title level="j">Appl. Comput. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting to unknown smoothness via wavelet shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994">1995. 1994</date>
		</imprint>
	</monogr>
	<note>Biometrika</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neo-classical minimax problems, thresholding and adaptive function estimation</title>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="39" to="62" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wavelet shrinkage: Asymptopia?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kerkyacharian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="301" to="369" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A discrete transform and decompositions of distribution spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jawrth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Function. Anal</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="34" to="170" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discrete multi-resolution analysis and generalized wavelets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Numer. Math</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="153" to="192" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biorthogonal bases of symmetric compactly supported wavelets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets, Fractals, and Fourier Transforms</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="91" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inverting the Radon transform in the presence of noise</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wavelet-projection methods for inverse problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maass</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wavelets</forename><surname>Operators</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trans</forename><forename type="middle">D H</forename><surname>Salinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational methods in image segmentation</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solimini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Nonlinear Differential Equations and Their Applications</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Birkhauser</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Geometry-Driven Diffusion in Computer Vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Faemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spline models for observational data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CBMS-NSF Regional Conf. Series in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="1990">1990</date>
			<publisher>SIAM</publisher>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-performance compression of astronomical images</title>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conf</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting>Data Compression Conf<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc. Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
