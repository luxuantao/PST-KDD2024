<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimGNN: A Neural Network Approach to Fast Graph Similarity Computation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Bian</surname></persName>
							<email>biansonghz@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>tingchen@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>weiwang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Yunsheng Bai</orgName>
								<address>
									<addrLine>Song Bian</addrLine>
									<settlement>Hao Ding, Ting Chen, Yizhou Sun, Wei Wang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SimGNN: A Neural Network Approach to Fast Graph Similarity Computation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3289600.3290967</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>network embedding</term>
					<term>neural networks</term>
					<term>graph similarity computation</term>
					<term>graph edit distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph similarity search is among the most important graph-based applications, e.g. finding the chemical compounds that are most similar to a query compound. Graph similarity/distance computation, such as Graph Edit Distance (GED) and Maximum Common Subgraph (MCS), is the core operation of graph similarity search and many other applications, but very costly to compute in practice. Inspired by the recent success of neural network approaches to several graph applications, such as node or graph classification, we propose a novel neural network based approach to address this classic yet challenging graph problem, aiming to alleviate the computational burden while preserving a good performance.</p><p>The proposed approach, called SimGNN, combines two strategies. First, we design a learnable embedding function that maps every graph into an embedding vector, which provides a global summary of a graph. A novel attention mechanism is proposed to emphasize the important nodes with respect to a specific similarity metric. Second, we design a pairwise node comparison method to supplement the graph-level embeddings with fine-grained node-level information. Our model achieves better generalization on unseen graphs, and in the worst case runs in quadratic time with respect to the number of nodes in two graphs. Taking GED computation as an example, experimental results on three real graph datasets demonstrate the effectiveness and efficiency of our approach. Specifically, our model achieves smaller error rate and great time reduction compared against a series of baselines, including several approximation algorithms on GED computation, and many existing graph neural network based models. Our study suggests SimGNN provides a new direction for future research on graph similarity computation and graph similarity search 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are ubiquitous nowadays and have a wide range of applications in bioinformatics, chemistry, recommender systems, social network study, program static analysis, etc. Among these, one of the fundamental problems is to retrieve a set of similar graphs from a database given a user query. Different graph similarity/distance metrics are defined, such as Graph Edit Distance (GED) <ref type="bibr" target="#b2">[3]</ref>, Maximum Common Subgraph (MCS) <ref type="bibr" target="#b4">[5]</ref>, etc. However, the core operation, namely computing the GED or MCS between two graphs, is known to be NP-complete <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>. For GED, even the state-of-the-art algorithms cannot reliably compute the exact GED within reasonable time between graphs with more than 16 nodes <ref type="bibr" target="#b0">[1]</ref>.</p><p>Given the huge importance yet great difficulty of computing the exact graph distances, there have been two broad categories of methods to address the problem of graph similarity search. The first category of remedies is the pruning-verification framework <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>, under which the total amount of exact graph similarity computations for a query can be reduced to a tractable degree, via a series of database indexing techniques and pruning strategies. However, the fundamental problem of the exponential time complexity of exact graph similarity computation <ref type="bibr" target="#b27">[28]</ref> remains. The second category tries to reduce the cost of graph similarity computation directly. Instead of calculating the exact similarity metric, these methods find approximate values in a fast and heuristic way <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. However, these methods usually require rather complicated design and implementation based on discrete optimization or combinatorial search. The time complexity is usually still polynomial or even sub-exponential in the number of nodes in the graphs, such as A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b34">[35]</ref>, VJ <ref type="bibr" target="#b8">[9]</ref>, etc.</p><p>In this paper, we propose a novel approach to speed-up the graph similarity computation, with the same purpose as the second category of methods mentioned previously. However, instead of directly Each graph is mapped into an embedding vector (denoted as a dot in the plot), which preserves their similarity between each other in terms of a specific graph similarity metric. The green "+" sign denotes the embedding of an example query graph. Colors of dots indicate how similar a graph is to the query based on the ground truth (from red to blue, meaning from the most similar to the least similar).</p><p>computing the approximate similarities using combinatorial search, our solution turns it into a learning problem. More specifically, we design a neural network-based function that maps a pair of graphs into a similarity score. At the training stage, the parameters involved in this function will be learned by minimizing the difference between the predicted similarity scores and the ground truth, where each training data point is a pair of graphs together with their true similarity score. At the test stage, by feeding the learned function with any pair of graphs, we can obtain a predicted similarity score. We name such approach as SimGNN, i.e., Similarity Computation via Graph Neural Networks.</p><p>SimGNN enjoys the key advantage of efficiency due to the nature of neural network computation. As for effectiveness, however, we need to carefully design the neural network architecture to satisfy the following three properties:</p><p>(1) Representation-invariant. The same graph can be represented by different adjacency matrices by permuting the order of nodes. The computed similarity score should be invariant to such changes. (2) Inductive. The similarity computation should generalize to unseen graphs, i.e. compute the similarity score for graphs outside the training graph pairs. ( <ref type="formula" target="#formula_6">3</ref>) Learnable. The model should be adaptive to any similarity metric, by adjusting its parameters through training.</p><p>To achieve these goals, we propose the following two strategies. First, we design a learnable embedding function that maps every graph into an embedding vector, which provides a global summary of a graph through aggregating node-level embeddings. We propose a novel attention mechanism to select the important nodes out of an entire graph with respect to a specific similarity metric. This graph-level embedding can already largely preserve the similarity between graphs. For example, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, Graph A is very similar to Graph Q according to the ground truth similarity, which is reflected by the embedding as its embedding is close to Q in the embedding space. Also, such embedding-based similarity computation is very fast. Second, we design a pairwise node comparison method to supplement the graph-level embeddings with fine-grained node-level information. As one fixed-length embedding per graph may be too coarse, we further compute the pairwise similarity scores between nodes from the two graphs, from which the histogram features are extracted and combined with the graph-level information to boost the performance of our model. This results in the quadratic amount of operations in terms of graph size, which, however, is still among the most efficient methods for graph similarity computation. We conduct our experiments on GED computation, which is one of the most popular graph similarity/distance metrics. To demonstrate the effectiveness and efficiency of our approach, we conduct experiments on three real graph datasets. Compared with the baselines, which include several approximate GED computation algorithms, and many graph neural network based methods, our model achieves smaller error and great time reduction. It is worth mentioning that, our Strategy 1 already demonstrates superb performances compared with existing solutions. When running time is a major concern, we can drop the more time-consuming Strategy 2 for trade-off.</p><p>Our contributions can be summarized as follows:</p><p>• We address the challenging while classic problem of graph similarity computation by considering it as a learning problem, and propose a neural network based approach, called SimGNN, as the solution.</p><p>• Two novel strategies are proposed. First, we propose an efficient and effective attention mechanism to select the most relevant parts of a graph to generate a graph-level embedding, which preserves the similarity between graphs. Second, we propose a pairwise node comparison method to supplement the graph-level embeddings for more effective modeling of the similarity between two graphs. • We conduct extensive experiments on a very popular graph similarity/distance metric, GED, based on three real network datasets to demonstrate the effectiveness and efficiency of the proposed approach.</p><p>The rest of this paper is organized as follows. We introduce the preliminaries of our work in Section 2, describe our model in Section 3, present experimental results in Section 4, discuss related work in Section 5, and point out future directions in Section 6. A conclusion is provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Graph Edit Distance (GED)</head><p>In order to demonstrate the effectiveness and efficiency of SimGNN, we choose one of the most popular graph similarity/distance metric, Graph Edit Distance (GED), as a case study. GED has been widely used in many applications, such as graph similarity search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, graph classification <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, handwriting recognition <ref type="bibr" target="#b9">[10]</ref>, image indexing <ref type="bibr" target="#b44">[45]</ref>, etc.</p><p>Formally, the edit distance between G 1 and G 2 , denoted by GED(G 1 , G 2 ), is the number of edit operations in the optimal alignments that transform G 1 into G 2 , where an edit operation on a graph G is an insertion or deletion of a vertex/edge or relabelling of a vertex<ref type="foot" target="#foot_0">2</ref> . Intuitively, if two graphs are identical (isomorphic), their GED is 0. Fig. <ref type="figure" target="#fig_1">2</ref> shows an example of GED between two simple graphs.</p><p>Once the distance between two graphs is calculated, we transform it to a similarity score ranging between 0 and 1. More details about the transformation function can be found in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Networks (GCN)</head><p>Both strategies in SimGNN require node embedding computation. In Strategy 1, to compute graph-level embedding, it aggregates node-level embeddings using attention; and in Strategy 2, pairwise node comparison for two graphs is computed based on node-level embeddings as well.</p><p>Among many existing node embedding algorithms, we choose to use Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">[22]</ref>, as it is graph representation-invariant, as long as the initialization is carefully designed. It is also inductive, since for any unseen graph, we can always compute the node embedding following the GCN operation. GCN now is among the most popular models for node embeddings, and belong to the family of neighbor aggregation based methods. Its core operation, graph convolution, operates on the representation of a node, which is denoted as u n ∈ R D , and is defined as follows:</p><formula xml:id="formula_0">conv(u n ) = f 1 ( m ∈N(n) 1 √ d n d m u m W (l ) 1 + b (l ) 1 )<label>(1)</label></formula><p>where N (n) is the set of the first-order neighbors of node n plus n itself, d n is the degree of node n plus 1, W (l )</p><formula xml:id="formula_1">1 ∈ R D l ×D l +1</formula><p>is the weight matrix associated with the l-th GCN layer, b</p><formula xml:id="formula_2">(l ) 1 ∈ R D l +1</formula><p>is the bias, and f 1 (•) is an activation function such as ReLU(x) = max(0, x). Intuitively, the graph convolution operation aggregates the features from the first-order neighbors of the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED APPROACH: SIMGNN</head><p>Now we introduce our proposed approach SimGNN in detail, which is an end-to-end neural network based approach that attempts to learn a function to map a pair of graphs into a similarity score. An overview of SimGNN is illustrated in Fig. <ref type="figure">3</ref>. First, our model transforms the node of each graph into a vector, encoding the features and structural properties around each node. Then, two strategies are proposed to model the similarity between the two graphs, one based on the interaction between two graph-level embeddings, the other based on comparing two sets of node-level embeddings. Finally, two strategies are combined together to feed into a fully connected neural network to get the final similarity score. The rest of the section details these two strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Strategy One: Graph-Level Embedding Interaction</head><p>This strategy is based on the assumption that a good graph-level embedding can encode the structural and feature information of a graph, and by interacting the two graph-level embeddings, the similarity between two graphs can be predicted. It involves the following stages: (1) the node embedding stage, which transforms each node of a graph into a vector, encoding its features and structural properties;</p><p>(2) the graph embedding stage, which produces one embedding for each graph by attention-based aggregation of node embeddings generated in the previous stage;</p><p>(3) the graph-graph interaction stage, which receives two graph-level embeddings and returns the interaction scores representing the graph-graph similarity; and (4) the final graph similarity score computation stage, which further reduces the interaction scores into one final similarity score. It will be compared against the ground-truth similarity score to update parameters involved in the 4 stages.</p><p>3.1.1 Stage I: Node Embedding. Among the existing state-ofthe-art approaches, we adopt GCN, a neighbor aggregation based method, because it learns an aggregation function (Eq. 1) that are representation-invariant and can be applied to unseen nodes. In Fig. <ref type="figure">3</ref>, different colors represent different node types, and the original node representations are one-hot encoded. Notice that the one-hot encoding is based on node types (e.g., all the nodes with carbon type share the same one-hot encoding vector), so even if the node ids are permuted, the aggregation results would be the same. For graphs with unlabeled nodes, we treat every node to have the same label, resulting in the same constant number as the initialize representation. After multiple layers of GCNs (e.g., 3 layers in our experiment), the node embeddings are ready to be fed into the Attention module (Att), which is described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Stage II:</head><p>Graph Embedding: Global Context-Aware Attention. To generate one embedding per graph using a set of node embeddings, one could perform an unweighted average of node embeddings, or a weighted sum where a weight associated with a node is determined by its degree. However, which nodes are more important and should receive more weights is dependent on the specific similarity metric. Thus, we propose the following attention mechanism to let the model learn weights guided by the specific similarity metric.</p><p>Denote the input node embeddings as U ∈ R N ×D , where the n-th row, u n ∈ R D is the embedding of node n. First, a global graph context c ∈ R D is computed, which is a simple average of node embeddings followed by a nonlinear transformation: c = tanh(( 1</p><formula xml:id="formula_3">N N n=1 u n )W 2 )</formula><p>, where W 2 ∈ R D×D is a learnable weight matrix. The context c provides the global structural and feature information of the graph that is adaptive to the given similarity metric, via learning the weight matrix. Based on c, we can compute one attention weight for each node.</p><p>For node n, to make its attention a n aware of the global context, we take the inner product between c and its node embedding. The intuition is that, nodes similar to the global context should receive </p><formula xml:id="formula_4">(x) = 1 1+exp (−x )</formula><p>is applied to the result to ensure the attention weights is in the range (0, 1). We do not normalize the weights into length 1, since it is desirable to let the embedding norm reflect the graph size, which is essential for the task of graph similarity computation. Finally, the graph embedding h ∈ R D is the weighted sum of node embeddings, h = N n=1 a n u n . The following equation summarizes the proposed node attentive mechanism:</p><formula xml:id="formula_5">h = N n=1 f 2 (u T n c)u n = N n=1 f 2 (u T n tanh(( 1 N N m=1 u m )W 2 ))u n (2)</formula><p>where f 2 (•) is the sigmoid function σ (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Stage III:</head><p>Graph-Graph Interaction: Neural Tensor Network. Given the graph-level embeddings of two graphs produced by the previous stage, a simple way to model their relation is to take the inner product of the two, h i ∈ R D , h j ∈ R D . However, as discussed in <ref type="bibr" target="#b37">[38]</ref>, such simple usage of data representations often lead to insufficient or weak interaction between the two. Following <ref type="bibr" target="#b37">[38]</ref>, we use Neural Tensor Networks (NTN) to model the relation between two graph-level embeddings:</p><formula xml:id="formula_6">д(h i , h j ) = f 3 (h T i W [1:K ] 3 h j + V h i h j + b 3 )<label>(3)</label></formula><p>whereW</p><formula xml:id="formula_7">[1:K ] 3 ∈ R D×D×K is a weight tensor, [] denotes the concate- nation operation, V ∈ R K ×2D is a weight vector, b 3 ∈ R K is</formula><p>a bias vector, and f 3 (•) is an activation function. K is a hyperparameter controlling the number of interaction (similarity) scores produced by the model for each graph embedding pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Stage IV:</head><p>Graph Similarity Score Computation. After obtaining a list of similarity scores, we apply a standard multi-layer fully connected neural network to gradually reduce the dimension of the similarity score vector. In the end, one score, ŝ i j ∈ R, is predicted, and it is compared against the ground-truth similarity score using the following mean squared error loss function:</p><formula xml:id="formula_8">L = 1 |D| (i, j)∈ D ( ŝ i j − s(G i , G j )) 2 (4)</formula><p>where D is the set of training graph pairs, and s(G i , G j ) is the ground-truth similarity between G i and G j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Limitations of Strategy One.</head><p>As mentioned in Section 1, the node-level information such as the node feature distribution and graph size may be lost by the graph-level embedding. In many cases, the differences between two graphs lie in small substructures and are hard to be reflected by the graph level embedding. An analogy is that, in Natural Language Processing, the performance of sentence matching based on one embedding per sentence can be further enhanced through using fine-grained word-level information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. This leads to our second strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Strategy Two: Pairwise Node Comparison</head><p>To overcome the limitations mentioned previously, we consider bypassing the NTN module, and using the node-level embeddings directly. As illustrated in the bottom data flow of Fig. <ref type="figure">3</ref>, if G i has N i nodes and G j has N j nodes, there would be N i N j pairwise interaction scores, obtained by S = σ (U i U T j ), where U i ∈ R N i ×D and U j ∈ R N j ×D are the node embeddings of G i and G j , respectively. Since the node-level embeddings are not normalized, the sigmoid function is applied to ensure the similarities scores are in the range of (0, 1). For two graphs of different sizes, to emphasize their size difference, we pad fake nodes to the smaller graph. As shown in Fig. <ref type="figure">3</ref>, two fake nodes with zero embedding are padded to the bottom graph, resulting in two extra columns with zeros in S.</p><p>Denote N = max(N 1 , N 2 ). The pairwise node similarity matrix S ∈ R N ×N is a useful source of information, since it encodes finegrained pairwise node similarity scores. One simple way to utilize S is to vectorize it: vec(S) ∈ R N 2  , and feed it into the fully connected layers. However, there is usually no natural ordering between graph nodes. Different initial node ordering of the same graph would lead to different S and vec(S).  To ensure the model is invariant to the graph representations as mentioned in Section 1, one could preprocess the graph by applying some node ordering scheme <ref type="bibr" target="#b28">[29]</ref>, but we consider a much more efficient and natural way to utilize S. We extract its histogram features: hist(S) ∈ R B , where B is a hyperparameter that controls the number of bins in the histogram. In the case of Fig. <ref type="figure">3</ref>, seven bins are used for the histogram. The histogram feature vector is normalized and concatenated with the graph-level interaction scores д(h i , h j ), and fed to the fully connected layers to obtain a final similarity score for the graph pair.</p><p>It is important to note that the histogram features alone are not enough to train the model, since the histogram is not a continuous differential function and does not support backpropagation. In fact, we rely on Strategy 1 as the primary strategy to update the model weights, and use Strategy 2 to supplement the graph-level features, which brings extra performance gain to our model.</p><p>To sum up, we combine the coarse global comparison information captured by Strategy 1, and the fine-grained node-level comparison information captured by Strategy 2, to provide a thorough view of the graph comparison to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time Complexity Analysis</head><p>Once SimGNN has been trained, it can be used to compute a similarity score for any pair of graphs. The time complexity involves two parts: (1) the node-level and global-level embedding computation stages, which needs to be computed once for each graph; and (2) the similarity score computation stage, which needs to be computed for every pair of graphs. The node-level and global-level embedding computation stages. The time complexity associated with the generation of node-level and graph-level embeddings is O(E) <ref type="bibr" target="#b21">[22]</ref>, where E is the number of edges of the graph. Notice that the graph-level embeddings can be pre-computed and stored, and in the setting of graph similarity search, the unseen query graph only needs to be processed once to obtain its graph-level embedding. The similarity score computation stage. The time complexity for Strategy 1 is O(D 2 K), where D is the dimension of the graph level embedding, and K is the feature map dimension of the NTN. The time complexity for our Strategy 2 is O(DN 2 ), where N is the number of nodes in the larger graph. This can potentially be reduced by node sampling to construct the similarity matrix S. Moreover, the matrix multiplication S = σ (U 1 U T 2 ) can be greatly accelerated with GPUs. Our experimental results in Section 4.6.2 verify that there is no significant runtime increase when the second strategy is used.</p><p>In conclusion, among the two strategies we have proposed: Strategy 1 is the primary strategy, which is efficient but solely based on coarse graph level embeddings; and Strategy 2 is auxiliary, which includes fine-grained node-level information but is more time-consuming. In the worst case, the model runs in quadratic time with respect to the number of nodes, which is among the stateof-the-art algorithms for approximate graph distance computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>Three real-world graph datasets are used for the experiments. A concise summary and detailed visualizations can be found in Table <ref type="table" target="#tab_1">1</ref> and Fig. <ref type="figure" target="#fig_6">4</ref>, respectively.</p><p>AIDS. AIDS is a collection of antivirus screen chemical compounds from the Developmental Therapeutics Program at NCI/NIH 7 <ref type="foot" target="#foot_1">3</ref> , and has been used in several existing works on graph similarity search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>. It contains 42,687 chemical compound structures with Hydrogen atoms omitted. We select 700 graphs, each of which has 10 or less than 10 nodes. Each node is labeled with one of 29 types, as illustrated in Fig. <ref type="figure" target="#fig_6">4a</ref>.</p><p>LINUX. The LINUX dataset was originally introduced in <ref type="bibr" target="#b43">[44]</ref>. It consists of 48,747 Program Dependence Graphs (PDG) generated from the Linux kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements. We randomly select 1000 graphs of equal or less than 10 nodes each. The nodes are unlabeled.</p><p>IMDB. The IMDB dataset <ref type="bibr" target="#b45">[46]</ref> (named "IMDB-MULTI") consists of 1500 ego-networks of movie actors/actresses, where there is an edge if the two people appear in the same movie. To test the scalability and efficiency of our proposed approach, we use the full dataset without any selection. The nodes are unlabeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Preprocessing</head><p>For each dataset, we randomly split 60%, 20%, and 20% of all the graphs as training set, validation set, and testing set, respectively. The evaluation reflects the real-world scenario of graph query: For each graph in the testing set, we treat it as a query graph, and let the model compute the similarity between the query graph and every graph in the database. The database graphs are ranked according to the computed similarities to the query.</p><p>Since graphs from AIDS and LINUX are relatively small, an exponential-time exact GED computation algorithm named A* <ref type="bibr" target="#b35">[36]</ref> is used to compute the GEDs between all the graph pairs. For the IMDB dataset, however, A* can no longer be used, as a recent survey of exact GED computation <ref type="bibr" target="#b0">[1]</ref> concludes, "no currently available algorithm manages to reliably compute GED within reasonable time between graphs with more than 16 nodes. "</p><p>To properly handle the IMDB dataset, we take the smallest distance computed by three well-known approximate algorithms, Beam <ref type="bibr" target="#b27">[28]</ref>, Hungarian <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref>, and VJ <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. The minimum is taken instead of the average, because their returned GEDs are guaranteed to be greater than or equal to the true GEDs. Details on these algorithms can be found in Section 4.3. Incidentally, the ICPR 2016 Graph Distance Contest<ref type="foot" target="#foot_2">4</ref> also adopts this approach to obtaining ground-truth GEDs for large graphs.</p><p>To transform ground-truth GEDs into ground-truth similarity scores to train our model, we first normalize the GEDs according to <ref type="bibr" target="#b32">[33]</ref>    of nodes of G i . We then adopt the exponential function λ(x) = e −x to transform the normalized GED into a similarity score in the range of (0, 1]. Notice that there is a one-to-one mapping between the GED and the similarity score.</p><formula xml:id="formula_9">: nGED(G 1 , G 2 ) = GED(G 1 , G 2 ) ( | G 1 |+ | G 2 |)/2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Methods</head><p>Our baselines include two types of approaches, fast approximate GED computation algorithms and neural network based models.</p><p>The first category of baselines includes three classic algorithms for GED computation. (1) A*-Beamsearch (Beam) <ref type="bibr" target="#b27">[28]</ref>. It is a variant of the A* algorithm in sub-exponential time. ( <ref type="formula">2</ref>) Hungarian <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref> and (3) VJ <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> are two cubic-time algorithms based on the Hungarian Algorithm for bipartite graph matching, and the algorithm of Volgenant and Jonker, respectively.</p><p>The second category of baselines includes seven models of the following neural network architectures. (1) SimpleMean simply takes the unweighted average of all the node embeddings of a graph to generate its graph-level embedding. (2) HierarchicalMean and (3) HierarchicalMax <ref type="bibr" target="#b6">[7]</ref> are the original GCN architectures based on graph coarsening, which use the global mean or max pooling to generate a graph hierarchy. We use the implementation from the Github repository of the first author of GCN <ref type="foot" target="#foot_3">5</ref> . The next four models apply the attention mechanism on nodes. (4) AttDegree uses the natural log of the degree of a node as its attention weight, as described in Section 3.1.2. (5) AttGlobalContext and (6) AttLearn-ableGlobalContext (AttLearnableGC) both utilize the global graph context to compute the attention weights, but the former does not apply the nonlinear transformation with learnable weights on the context, while the latter does. <ref type="bibr" target="#b6">(7)</ref> SimGNN is our full model that combines the best of Strategy 1 (AttLearnableGC) and Strategy 2 as described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Settings</head><p>For the model architecture, we set the number of GCN layers to 3, and use ReLU as the activation function. For the initial node representations, we adopt the one-hot encoding scheme for AIDS reflecting the node type, and the constant encoding scheme for LINUX and IMDB, since their nodes are unlabeled, as mentioned in Section 3.1.1. The output dimensions for the 1st, 2nd, and 3rd layer of GCN are 64, 32, and 16, respectively. For the NTN layer, we set K to 16. For the pairwise node comparison strategy, we set the number of histogram bins to 16. We use 4 fully connected layers to reduce the dimension of the concatenated results from the NTN module, from 32 to 16, 16 to 8, 8 to 4, and 4 to 1.</p><p>We conduct all the experiments on a single machine with an Intel i7-6800K CPU and one Nvidia Titan GPU. As for training, we set the batch size to 128, use the Adam algorithm for optimization <ref type="bibr" target="#b20">[21]</ref>, and fix the initial learning rate to 0.001. We set the number of iterations to 10000, and select the best model based on the lowest validation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Metrics</head><p>The following metrics are used to evaluate all the models: Time. The wall time needed for each model to compute the similarity score for a pair of graphs is collected. Mean Squared Error (mse). The mean squared error measures the average squared difference between the computed similarities and the ground-truth similarities.</p><p>We also adopt the following metrics to evaluate the ranking results. Spearman's Rank Correlation Coefficient (ρ) <ref type="bibr" target="#b38">[39]</ref> and Kendall's Rank Correlation Coefficient (τ ) <ref type="bibr" target="#b19">[20]</ref> measure how well the predicted ranking results match the true ranking results. Precision at k (p@k). p@k is computed by taking the intersection of the predicted top k results and the ground-truth top k results divided by k. Compared with p@k, ρ and τ evaluates the global ranking results instead of focusing on the top k results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results</head><p>4.6.1 Effectiveness. The effectiveness results on the three datasets can be found in Table <ref type="table" target="#tab_2">2</ref>, 3, and 4. Our model, SimGNN, consistently achieves the best or the second best performance on all metrics across the three datasets. Within the neural network based methods, SimGNN consistently achieves the best results on all metrics. This suggests that our model can learn a good embedding function that generalizes to unseen test graphs.</p><p>Beam achieves the best precisions at 10 on AIDS and LINUX. We conjecture that it can be attributed to the imbalanced ground-truth GED distributions. As seen in Fig. <ref type="figure" target="#fig_6">4c</ref>, for AIDS, the training pairs have GEDs mostly around 10, causing our model to train the very similar pairs less frequently than the dissimilar ones. For LINUX, the situation for SimGNN is better, since most GEDs concentrate in the range of [0, 10], the gap between the precisions at 10 of Beam and SimGNN become smaller.   It is noteworthy that among the neural network based models, AttDegree achieves relatively good results on IMDB, but not on AIDS or LINUX. It could be due to the unique ego-network structures commonly present in IMDB. As seen in Fig. <ref type="figure" target="#fig_13">10</ref>, the high-degree central node denotes the particular actor/actress himself/herself, focusing on which could be a reasonable heuristic. In contrast, AttLearnableGC adapts to the GED metric via a learnable global context, and consistently performs better than AttDegree. Combined with Strategy 2, SimGNN achieves even better performances.</p><p>Visualizations of the node attentions can be seen in Fig. <ref type="figure" target="#fig_8">5</ref>. We observe that the following kinds of nodes receive relatively higher attention weights: hub nodes with large degrees, e.g. the "S" in (a) and (b), nodes with labels that rarely occur in the dataset, e.g. the "Ni" in (f), the "Pd" in (g), the "Br" in (h), nodes forming special substructures, e.g. the two middle "C"s in (e), etc. These patterns make intuitive sense, further confirming the effectiveness of the proposed approach. 4.6.2 Efficiency. The efficiency comparison on the three datasets is shown in Fig. <ref type="figure" target="#fig_2">6</ref>. The neural network based models consistently achieve the best results across all the three datasets. Specifically, compared with the exact algorithm, A*, SimGNN is 2174 times faster on AIDS, and 212 times faster on LINUX. The A* algorithm cannot even be applied on large graphs, and in the case of IMDB, its variant, Beam, is still 46 times slower than SimGNN. Moreover,  the time measured for SimGNN includes the time for graph embedding. As mentioned in Section 3.3, if graph embeddings are pre-computed and stored, SimGNN would spend even less time. All of these suggest that in practice, it is reasonable to use SimGNN as a fast approach to graph similarity computation, which is especially true for large graphs, as in IMDB, our computation time does not increase much compared with AIDS and LINUX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Parameter Sensitivity</head><p>We evaluate how the dimension of the graph-level embeddings and the number of the histogram bins can affect the results. We report the mean squared error on AIDS. As can be seen in Fig. <ref type="figure" target="#fig_10">7a</ref>, the performance becomes better if larger dimensions are used. This makes intuitive sense, since larger embedding dimensions give the model more capacity to represent graphs. In our Strategy 2, as shown in Fig. <ref type="figure" target="#fig_10">7b</ref>, the performance is relatively insensitive to the number of histogram bins. This suggests that in practice, as long as the histogram bins are not too few, relatively good performance can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Case Studies</head><p>We demonstrate three example queries, one from each dataset, in Fig. <ref type="figure" target="#fig_11">8</ref>, 9, and 10. In each demo, the top row depicts the query along with the ground-truth ranking results, labeled with their normalized GEDs to the query; The bottom row shows the graphs returned by our model, each with its rank shown at the top. SimGNN is able    to retrieve graphs similar to the query, e.g. in the case of LINUX (Fig. <ref type="figure" target="#fig_12">9</ref>), the top 6 results are the isomorphic graphs to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Network/Graph Embedding</head><p>Node-level embedding. Over the years, there are several categories of methods that have been proposed for learning node representations, including matrix factorization based methods (NetMF <ref type="bibr" target="#b31">[32]</ref>), skip-gram based methods (DeepWalk <ref type="bibr" target="#b30">[31]</ref>, Node2Vec <ref type="bibr" target="#b11">[12]</ref>, LINE <ref type="bibr" target="#b39">[40]</ref>), autoencoder based methods (SDNE <ref type="bibr" target="#b42">[43]</ref>), neighbor aggregation based methods (GCN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, GraphSAGE <ref type="bibr" target="#b12">[13]</ref>), etc.</p><p>Graph-level embedding. The most intuitive way to generate one embedding per graph is to aggregate the node-level embeddings, either by a simple average or some weighted average <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>, named the "sum-based" approaches <ref type="bibr" target="#b13">[14]</ref>. A more sophisticated way to represent graphs can be achieved by viewing a graph as a hierarchical data structure and applying graph coarsening <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>. Besides, <ref type="bibr" target="#b18">[19]</ref> aggregates sets of nodes via histograms, and <ref type="bibr" target="#b28">[29]</ref> applies node ordering on a graph to make it CNN suitable. Attention mechanism on graphs. <ref type="bibr" target="#b23">[24]</ref> uses an attention-guided walk to find the most relevant parts for graph classification. As a result, it only selects a fixed amount of nodes out of an entire graph, which is too coarse for our task, graph similarity computation. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> assign an attention weight to each neighbor of a node for node-level tasks. We are among the first to apply the attention mechanism for the task of graph similarity computation.</p><p>Graph neural network applications. A great amount of graphbased applications have been tackled by neural network based methods, most of which are framed as node-level prediction tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>. However, once moving to the graph-level tasks, most existing works deal with the classification of a single graph <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>. In this work, we consider graph similarity computation for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Similarity Computation</head><p>Graph distance/similarity metrics. The Graph Edit Distance (GED) <ref type="bibr" target="#b2">[3]</ref> can be considered as an extension of the String Edit Distance metric <ref type="bibr" target="#b24">[25]</ref>, which is defined as the minimum cost taken to transform one graph to the other via a sequence graph edit operations. Another metric is the Maximum Common Subgraph (MCS), which is equivalent to GED under a certain cost function <ref type="bibr" target="#b3">[4]</ref>. Graph kernels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> can be considered as a family of different graph similarity metrics, used primarily for graph classification.</p><p>Pairwise GED computation algorithms. A flurry of approximate algorithms has been proposed to reduce the time complexity with the sacrifice in accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. We are aware of some very recent work claiming their time complexity is O(n 2 ) [2], but their code is unstable at this stage for comparison. Graph Similarity search. Computing GED is a primitive operator in graph database analysis, and has been adopted in a series of works on graph similarity search <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>. These studies focus on database-level techniques to speed up the overall querying process involving exact GED computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSIONS AND FUTURE DIRECTIONS</head><p>There are several directions to go for the future work: (1) our model can handle graphs with node types but cannot process edge features.</p><p>In chemistry, bonds of a chemical compound are usually labeled, so it is useful to incorporate edge labels into our model; (2) it is promising to explore different techniques to further boost the precisions at the top k results, which is not preserved well mainly due to the skewed similarity distribution in the training dataset; and (3) given the constraint that the exact GEDs for large graphs cannot be computed, it would be interesting to see how the learned model generalize to large graphs, which is trained only on the exact GEDs between small graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We are at the intersection of graph deep learning and graph search problem, and taking the first step towards bridging the gap, by tackling the core operation of graph similarity computation , via a novel neural network based approach. The central idea is to learn a neural network based function that is representation-invariant, inductive, and adaptive to the specific similarity metric, which takes any two graphs as input and outputs their similarity score. Our model runs very fast compared to existing classic algorithms on approximate Graph Edit Distance computation, and achieves very competitive accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of similarity-preserving graph embedding. Each graph is mapped into an embedding vector (denoted as a dot in the plot), which preserves their similarity between each other in terms of a specific graph similarity metric. The green "+" sign denotes the embedding of an example query graph. Colors of dots indicate how similar a graph is to the query based on the ground truth (from red to blue, meaning from the most similar to the least similar).</figDesc><graphic url="image-4.png" coords="2,98.49,157.04,76.78,57.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The GED between the graph to the left and the graph to the right is 3, as the transformation needs 3 edit operations: (1) an edge deletion, (2) an edge insertion, and (3) a node relabeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Session 6 :</head><label>6</label><figDesc>Networks and Social Behavior WSDM '19, February 11-15, 2019, Melbourne, VIC, Australia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where |G i | denotes the number Node label distribution of AIDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Distribution of graph sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Distribution of GEDs of the training pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Some statistics of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Session 6 :</head><label>6</label><figDesc>Networks and Social Behavior WSDM '19, February 11-15, 2019, Melbourne, VIC, Australia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualizations of node attentions. The darker the color, the larger the attention weight.</figDesc><graphic url="image-8.png" coords="7,328.96,253.22,216.00,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Runtime comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Mean squared error w.r.t. the number of dimensions of graph-level embeddings, and the number of histogram bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A query case study on AIDS. Meanings of the colors can be found in Fig. 4a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: A query case study on LINUX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A query case study on IMDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell>Graph Meaning</cell><cell cols="2">#Graphs #Pairs</cell></row><row><cell>AIDS</cell><cell>Chemical Compounds</cell><cell>700</cell><cell>490K</cell></row><row><cell cols="2">LINUX Program Dependency Graphs</cell><cell>1000</cell><cell>1M</cell></row><row><cell>IMDB</cell><cell>Actor/Actress Ego-Networks</cell><cell>1500</cell><cell>2.25M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on AIDS.</figDesc><table><row><cell>Method</cell><cell>mse(10 −3 )</cell><cell>ρ</cell><cell>τ</cell><cell cols="2">p@10 p@20</cell></row><row><cell>Beam</cell><cell>12.090</cell><cell cols="2">0.609 0.463</cell><cell>0.481</cell><cell>0.493</cell></row><row><cell>Hungarian</cell><cell>25.296</cell><cell cols="3">0.510 0.378 0.360</cell><cell>0.392</cell></row><row><cell>VJ</cell><cell>29.157</cell><cell cols="3">0.517 0.383 0.310</cell><cell>0.345</cell></row><row><cell>SimpleMean</cell><cell>3.115</cell><cell cols="3">0.633 0.480 0.269</cell><cell>0.279</cell></row><row><cell>HierarchicalMean</cell><cell>3.046</cell><cell cols="3">0.681 0.629 0.246</cell><cell>0.340</cell></row><row><cell>HierarchicalMax</cell><cell>3.396</cell><cell cols="3">0.655 0.505 0.222</cell><cell>0.295</cell></row><row><cell>AttDegree</cell><cell>3.338</cell><cell cols="3">0.628 0.478 0.209</cell><cell>0.279</cell></row><row><cell>AttGlobalContext</cell><cell>1.472</cell><cell cols="3">0.813 0.653 0.376</cell><cell>0.473</cell></row><row><cell>AttLearnableGC</cell><cell>1.340</cell><cell cols="3">0.825 0.667 0.400</cell><cell>0.488</cell></row><row><cell>SimGNN</cell><cell>1.189</cell><cell cols="2">0.843 0.690</cell><cell>0.421</cell><cell>0.514</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on LINUX.</figDesc><table><row><cell>Method</cell><cell>mse(10 −3 )</cell><cell>ρ</cell><cell>τ</cell><cell cols="2">p@10 p@20</cell></row><row><cell>Beam</cell><cell>9.268</cell><cell cols="2">0.827 0.714</cell><cell>0.973</cell><cell>0.924</cell></row><row><cell>Hungarian</cell><cell>29.805</cell><cell cols="3">0.638 0.517 0.913</cell><cell>0.836</cell></row><row><cell>VJ</cell><cell>63.863</cell><cell cols="3">0.581 0.450 0.287</cell><cell>0.251</cell></row><row><cell>SimpleMean</cell><cell>16.950</cell><cell cols="3">0.020 0.016 0.432</cell><cell>0.465</cell></row><row><cell>HierarchicalMean</cell><cell>6.431</cell><cell cols="3">0.430 0.525 0.750</cell><cell>0.618</cell></row><row><cell>HierarchicalMax</cell><cell>6.575</cell><cell cols="3">0.879 0.740 0.551</cell><cell>0.575</cell></row><row><cell>AttDegree</cell><cell>8.064</cell><cell cols="3">0.742 0.609 0.427</cell><cell>0.460</cell></row><row><cell>AttGlobalContext</cell><cell>3.125</cell><cell cols="3">0.904 0.781 0.874</cell><cell>0.864</cell></row><row><cell>AttLearnableGC</cell><cell>2.055</cell><cell cols="3">0.916 0.804 0.903</cell><cell>0.887</cell></row><row><cell>SimGNN</cell><cell>1.509</cell><cell cols="2">0.939 0.830</cell><cell>0.942</cell><cell>0.933</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on IMDB. Beam, Hungarian, and VJ together are used to determine the ground-truth results.</figDesc><table><row><cell>Method</cell><cell>mse(10 −3 )</cell><cell>ρ</cell><cell>τ</cell><cell cols="2">p@10 p@20</cell></row><row><cell>SimpleMean</cell><cell>3.749</cell><cell cols="3">0.774 0.644 0.547</cell><cell>0.588</cell></row><row><cell>HierarchicalMean</cell><cell>5.019</cell><cell cols="3">0.456 0.378 0.567</cell><cell>0.553</cell></row><row><cell>HierarchicalMax</cell><cell>6.993</cell><cell cols="3">0.455 0.354 0.572</cell><cell>0.570</cell></row><row><cell>AttDegree</cell><cell>2.144</cell><cell cols="3">0.828 0.695 0.700</cell><cell>0.695</cell></row><row><cell>AttGlobalContext</cell><cell>3.555</cell><cell cols="3">0.684 0.553 0.657</cell><cell>0.656</cell></row><row><cell>AttLearnableGC</cell><cell>1.455</cell><cell cols="3">0.835 0.700 0.732</cell><cell>0.742</cell></row><row><cell>SimGNN</cell><cell>1.264</cell><cell cols="2">0.878 0.770</cell><cell>0.759</cell><cell>0.777</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Although other variants of GED exist<ref type="bibr" target="#b35">[36]</ref>, we adopt this basic version.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://gdc2016.greyc.fr/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://github.com/mdeff/cnn_graph</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported in part by NSF DBI 1565137, NSF DGE1829071, NSF III-1705169, NSF CAREER Award 1741634, NIH U01HG008488, NIH R01GM115833, Snapchat gift funds, and PPDai gift fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the exact computation of the graph edit distance</title>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johann</forename><surname>Blumenthal</surname></persName>
		</author>
		<author>
			<persName><surname>Gamper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph edit distance as a quadratic assignment problem</title>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Bougleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Carletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the distance between graphs</title>
		<author>
			<persName><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the EATCS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="35" to="39" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On a relation between graph edit distance and maximum common subgraph</title>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A graph distance metric based on the maximal common subgraph</title>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="255" to="259" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate graph edit distance by several local searches in parallel</title>
		<author>
			<persName><forename type="first">Évariste</forename><surname>Daller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bougleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speeding up graph edit distance computation through fast bipartite matching</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast matching algorithm for graph-based handwriting recognition</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkmar</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="937" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cyclic pattern kernels for predictive graph mining</title>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A shortest augmenting path algorithm for dense and sparse linear assignment problems</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Volgenant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="325" to="340" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Molecular convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName><forename type="first">Maurice</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938">1938. 1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955. 1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Classification using Structural Attention</title>
		<author>
			<persName><forename type="first">John</forename><surname>Boaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
				<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Similarity search in graph databases: A multi-layered indexing approach</title>
		<author>
			<persName><forename type="first">Yongjiang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixiang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE. IEEE</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="783" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Drug Similarity Integration Through Attentive Multi-view Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast suboptimal algorithms for the computation of graph edit distance</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching Node Embeddings for Graph Similarity</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph based shapes representation and recognition</title>
		<author>
			<persName><forename type="first">Jalal</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramel</surname></persName>
		</author>
		<author>
			<persName><surname>Cardot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">IAM graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximate graph edit distance computation by means of bipartite graph matching</title>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="950" to="959" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel software toolkit for graph edit distance computation</title>
		<author>
			<persName><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Emmenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="72" to="101" />
			<date type="published" when="1904">1904. 1904</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. International World Wide Web Conferences Steering Committee</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention-based Graph Neural Network for Semi-supervised Learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Kiran K Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An efficient graph indexing method</title>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Anthony Kh Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE. IEEE</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">HMM-based graph edit distance for image indexing</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Imaging Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="209" to="218" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Comparing stars: On approximating graph edit distance</title>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">Kh</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhu</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A partition-based approach to structure similarity search</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="169" to="180" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">Substructure Assembling Network for Graph Classification. AAAI</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph similarity search with edit distance constraint in large graph databases</title>
		<author>
			<persName><forename type="first">Weiguo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1595" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
