<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advances in Vision-Based Lane Detection: Algorithms, Integration, Assessment, and Perspectives on ACP-Based Parallel Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Xing</surname></persName>
							<email>y.xing@cranfield.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chen</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huaji</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
							<email>hong.wang@uwaterloo.ca</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Dongpu</forename><forename type="middle">P</forename><surname>Cao</surname></persName>
							<email>dongpu.cao@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Efstathios</forename><surname>Velenis</surname></persName>
							<email>e.velenis@cranfield.ac.uk</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cranfield University</orgName>
								<address>
									<postCode>MK43 0AL</postCode>
									<settlement>Bedford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">are with Mechanical and Mechatronics Engineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue West Waterloo</addrLine>
									<postCode>N2L 3G1</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Advances in Vision-Based Lane Detection: Algorithms, Integration, Assessment, and Perspectives on ACP-Based Parallel Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">86D3DE6F8318E857DC8181445DDBD055</idno>
					<idno type="DOI">10.1109/JAS.2018.7511063</idno>
					<note type="submission">received July 10, 2017; accepted December 6, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Advanced driver assistance systems (ADASs)</term>
					<term>ACP theory</term>
					<term>benchmark</term>
					<term>lane detection</term>
					<term>parallel vision</term>
					<term>performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is a fundamental aspect of most current advanced driver assistance systems (ADASs). A large number of existing results focus on the study of vision-based lane detection methods due to the extensive knowledge background and the low-cost of camera devices. In this paper, previous visionbased lane detection studies are reviewed in terms of three aspects, which are lane detection algorithms, integration, and evaluation methods. Next, considering the inevitable limitations that exist in the camera-based lane detection system, the system integration methodologies for constructing more robust detection systems are reviewed and analyzed. The integration methods are further divided into three levels, namely, algorithm, system, and sensor. Algorithm level combines different lane detection algorithms while system level integrates other object detection systems to comprehensively detect lane positions. Sensor level uses multi-modal sensors to build a robust lane recognition system. In view of the complexity of evaluating the detection system, and the lack of common evaluation procedure and uniform metrics in past studies, the existing evaluation methods and metrics are analyzed and classified to propose a better evaluation of the lane detection system. Next, a comparison of representative studies is performed. Finally, a discussion on the limitations of current lane detection systems and the future developing trends toward an Artificial Society, Computational experiment-based parallel lane detection framework is proposed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A. Background T RAFFIC accidents are mainly caused by human mistakes such as inattention, misbehavior, and distraction <ref type="bibr" target="#b0">[1]</ref>. A large number of companies and institutes have proposed methods and techniques for the improvement of driving safety and reduction of traffic accidents. Among these techniques, road perception and lane marking detection play a vital role in helping drivers avoid mistakes. The lane detection is the foundation of many advanced driver assistance systems (ADASs) such as the lane departure warning system (LDWS) and the lane keeping assistance system (LKAS) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Some successful ADAS or automotive enterprises, such as Mobileye, BMW, and Tesla, etc. have developed their own lane detection and lane keeping products and have obtained significant achievements in both research and real world applications. Either of the automotive enterprises or the personal customers have accepted the Mobileye Series ADAS products and Tesla Autopilot for self-driving. Almost all of the current mature lane assistance products use vision-based techniques since the lane markings are painted on the road for human visual perception. The utilization of vision-based techniques detects lanes from the camera devices and prevents the driver from making unintended lane changes. Therefore, the accuracy and robustness are two most important properties for lane detection systems. Lane detection systems should have the capability to be aware of unreasonable detections and adjust the detection and tracking algorithm accordingly <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. When a false alarm occurs, the ADAS should alert the driver to concentrate on the driving task. On the other hand, vehicles with high levels of automation continuously monitor their environments and should be able to deal with low-accuracy detection problems by themselves. Hence, evaluation of lane detection systems becomes even more critical with increasing automation of vehicles.</p><p>Most vision-based lane detection systems are commonly designed based on image processing techniques within similar frameworks. With the development of high-speed computing devices and advanced machine learning theories such as deep learning, lane detection problems can be solved in a more efficient fashion using an end-to-end detection procedure. However, the critical challenge faced by lane detection systems is the demand for high reliability and the diverse working conditions. One efficient way to construct robust and accurate advanced lane detection systems is to fuse multi-modal sensors and integrate lane detection systems with other object detection systems, such as detection by surrounding vehicles and road area recognition. It has been proved that lane detection performance can be improved with these multi-level integration techniques <ref type="bibr" target="#b3">[4]</ref>. However, the highly accurate sensors such as the light/laser detection and ranging (LIDAR/LADAR) are expensive and not available in public transport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contribution</head><p>In this study, the literature reviews on the lane detection algorithms, the integration methods, and evaluation methods are provided. The contribution of this paper can be summarized as follows.</p><p>A considerable number of existing studies do not provide enough information on the integration methodologies of lane detection systems and other systems or sensors. Therefore, in this study, the integration methodologies are analyzed in detail and the ways of integration are categorized into three levels: sensor level, system level, and algorithm level.</p><p>Due to the lack of ground truth data and uniform metrics, the evaluation of the lane detection system remains a challenge. Since various lane detection systems differ with respect to the hardware and software they use, it is difficult to undertake a comprehensive comparison and evaluation of these systems. In this study, previous evaluation methods are reviewed and classified into offline methods, which still use images and videos, and online methods, which are based on real time confidence calculation.</p><p>Finally, a novel lane detection system design framework based on the ACP parallel theory is introduced toward a more efficient way to deal with the training and evaluation of lane detection models. ACP is short for Artificial society, Computational experiments, and Parallel execution, which are three major components of the parallel systems. The ACPbased lane detection parallel system aims to construct virtual parallel scenarios for model training and benefit the corresponding real-world system. The construction method for the lane detection parallel vision system will be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Paper Organization</head><p>This paper is organized as follows: Section II provides a brief overview of existing lane detection algorithms. Section III summarizes the integration methods used in lane detection systems and three levels of integration methods are discussed. In Section IV, the online and offline evaluation methods for lane detection systems will be presented, followed by an analysis of evaluation metrics. In section V, the limitations of current approaches and discussion on developing advanced lane detection systems in the future developing trend will be proposed. The ACP-based parallel theory, as one of the powerful tool to assist the design of lane detection systems will also be introduced. Finally, we will conclude our work in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VISION-BASED LANE DETECTION ALGORITHM</head><p>REVIEW Literature reviews of lane detection algorithms and their corresponding general frameworks have been proposed in <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>. Hillel et al. concluded that road color, texture, boundaries, and lane markings are the main perception aspects for human drivers <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, McCall and Trivedi classified the lane detection objectives into three categories, which are lane departure warning, driver attention awareness, and automated vehicle control system design. However, they paid much attention to the design of lane detection algorithms and incompletely review the integration and evaluation methods. This study tries to comprehensively review the lane detection system from the perspective of algorithms, integration methods, and evaluation methods. Firstly, in this section, lane detection algorithms and techniques are reviewed from the scope of conventional image processing and novel machine learning methods. In the first part of this section, basic lane detection procedures and general frameworks will be analyzed. The second part will concentrate on the review of commonly used conventional image processing methods. In the last part, lane detection algorithms based on machine learning and deep learning methods, especially the utilization of convolutional neural network (CNN), will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General Lane Detection Procedure</head><p>Vision-based lane detection systems described in studies usually consist of three main procedures, which are image preprocessing, lane detection and lane tracking. Among these, the lane detection process, which comprises feature extraction and model fitting, is the most important aspect of the lane detection system, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The most common procedures in the pre-processing step include region of interest (ROI) selection, vanishing point detection, transferring color image into greyscale image or a different color format, noise removal and blur, inverse perspective mapping (IPM), also known as birds-eye view, segmentation, and edge statistics, etc. Among these tasks, determining the ROI is usually the first step performed in most of previous studies. The main reason for focusing on ROI is to increase computation efficiency and reduce false lane detections. ROI can be roughly selected as the lower portion of an input image or dynamically determined according to the detected lanes. It can also be more efficiently determined with prior road area detections <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Details of these methods are described in the next section. Generally speaking, a carefully-designed ROI will significantly improve lane detection accuracy as well as computation efficiency. Once the input images have been pre-processed, lane features such as the colors and edge features can be extracted and, therefore, can be detected based on these features. The Hough Transform algorithm, which uses the edge pixel images, is one of the most widely used algorithms for lane detection in previous studies. However, this method is designed to detect straight lines in the beginning and is not efficient in curve lane detection. Curve lanes can often be detected based on model fitting techniques such as random sample consensus (RANSAC). RANSAC fits lane models by recursively testing the model fitting score to find the optimal model parameters. Therefore, it has a strong ability to cope with outlier features. Finally, after lanes have been successfully detected, lane positions can be tracked with tracking algorithms such as Kalman filter or particle filters to refine the detection results and predict lane positions in a more efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conventional Image-Processing-Based Lane Detection Algorithms</head><p>Vision-based lane detection can be roughly classified into two categories: feature-based <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b18">[19]</ref> and model-based <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b28">[29]</ref>. Feature-based methods rely on the detection of lane marking features such as lane colors, textures, and edges. For example, in <ref type="bibr" target="#b8">[9]</ref>, noisy lane edge features were detected using the Sobel operator and the road images were divided into multiple subregions along the vertical direction. Suddamalla et al detected the curves and straight lanes using pixel intensity and edge information with lane markings being extracted with adaptive threshold techniques <ref type="bibr" target="#b9">[10]</ref>. To remove camera perspective distortions from the digital images and extract real lane features, lane markings can be efficiently detected with a perspective transform. Collado et al. created a bird-view of the road image and proposed an adaptive lane detection and classification method based on spatial lane features and the Hough transform algorithm <ref type="bibr" target="#b10">[11]</ref>. A combination of IPM and clustered particle filters method based on lane features was used to estimate multiple lanes in <ref type="bibr" target="#b11">[12]</ref>. The authors claimed that it is less robust if a strong lane model is used in the context and they only used a weak model for particle filter tracking. Instead of using color images, lanes can also be detected using other color format images. The general idea behind the color format transform is that the yellow and white lane markings can be more distinct in other color domain, so the contrast ratio is increased. In <ref type="bibr" target="#b12">[13]</ref>, lane edges were detected with an extended edge linking algorithm in the lane hypothesis stage. Lane pixels in the YUV format, edge orientation, and width of lane markings were used to select the candidate edge-link pairs in the lane verification step. In <ref type="bibr" target="#b13">[14]</ref>, lanes were recognized using an unsupervised and adaptive classifier. Color images were first converted to HSV format to increase the contrast. Then, the binary feature image was processed using the threshold method based on the brightness values. Although in some normal cases the color transform can benefit the lane detection, it is not robust and has limited ability to deal with shadows and illumination variation <ref type="bibr" target="#b3">[4]</ref>.</p><p>Borkar et al. proposed a layered approach to detect lanes at night <ref type="bibr" target="#b14">[15]</ref>. A temporal blur technique was used to reduce video noise and binary images were generated based on an adaptive local threshold method. The lane finding in another domain algorithm (LANA) represented lane features in the frequency domain <ref type="bibr" target="#b15">[16]</ref>. The algorithm captured the lane strength and orientation in the frequency domain and a deformable template was used to detect the lane markings. Results showed that LANA was robust under varying conditions. In <ref type="bibr" target="#b16">[17]</ref>, a spatiotemporal lane detection algorithm was introduced. A series of spatiotemporal images were generated by accumulating certain row pixels from the past frames and the lanes were detected using Hough transform applied on the synthesized images. In <ref type="bibr" target="#b17">[18]</ref>, a real-time lane detection system based on FPGA and DSP was designed based on lane gradient amplitude features and an improved Hough Transform. Ozgunalp and Dahnoun proposed an improved feature map for lane detection <ref type="bibr" target="#b18">[19]</ref>. The lane orientation histogram was first determined with edge orientations and then the feature map was improved and shifted based on the estimated lane orientation.</p><p>In general, feature-based methods have better computational efficiency and are able to accurately detect lanes when the lane markings are clear. However, due to too many constraints are assumed, such as the lane colors and shapes, the drawbacks of these methods include less robustness to deal with shadows and poor visibility conditions compared to model-based methods.</p><p>Model-based methods usually assume that lanes can be described with a specific model such as a linear model, a parabolic model, or various kinds of spline models. Besides, some assumptions about the road and lanes, such as a flat ground plane, are required. Among these models, spline models were popular in previous studies since these models are flexible enough to recover any shapes of the curve lanes. Wang et al. fitted lanes with different spline models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, a Catmull-Rom spline was used to model the lanes in the image. In <ref type="bibr" target="#b20">[21]</ref>, the lane model was improved to generate a B-snake model, which can model any arbitrary shape by changing the control points. In <ref type="bibr" target="#b21">[22]</ref>, a novel parallel-snake model was introduced. In <ref type="bibr" target="#b22">[23]</ref>, lane boundaries were detected based on a combination of Hough transform in near-field areas and a river-flow method in farfield areas. Finally, lanes were modelled with a B-spline model and tracked with a Kalman filter. Jung and Kelber described the lanes with a linearparabolic model and classified the lane types based on the estimated lane geometries <ref type="bibr" target="#b23">[24]</ref>. Aly proposed a multiple lane fitting method based on the integration of Hough transform, RANSAC, and B-spline model <ref type="bibr" target="#b24">[25]</ref>. Initial lane positions were first roughly detected with Hough transform and then improved with RANSAC and B-spline model. Moreover, a manually labelled lane dataset called the Caltech Lane dataset was introduced.</p><p>The RANSAC algorithm is the most popular way to iteratively estimate the lane model parameters. In <ref type="bibr" target="#b25">[26]</ref>, linear lane model and RANSAC were used to detect lanes, and a Kalman filter was used to refine the noisy output. Ridge features and adapted RANSAC for both straight and curve lane fitting were proposed in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The ridge features of lane pixels, which depend on the local structures rather than contrast, were defined as the center lines of a bright structure of a region in a greyscale image. In <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, hyperbolic model and RANSAC were used for lane fitting. In <ref type="bibr" target="#b29">[30]</ref>, input images were divided into two parts known as far-field area and near-field area. In near-field area, lanes were regarded as straight lines detected using the Hough transform algorithm. In far-field area, lanes were assumed to be curved lines and fitted using hyperbolic model and RANSAC.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, a conditional random field method was proposed to detect lane marks in urban areas. Bounini et al. introduced a lane boundary detection method for an autonomous vehicle working in a simulation environment <ref type="bibr" target="#b31">[32]</ref>. A least-square method was used to fit the line model and the computation cost was reduced by determining a dynamic ROI. In <ref type="bibr" target="#b32">[33]</ref>, an automated multi-segment lane-switch scheme and an RANSAC lane fitting method were proposed. The RANSAC algorithm was applied to fit the lines based on the edge image. A laneswitch scheme was used to determine lane curvatures and choose the correct lane models from straight and curve models to fit the lanes. In <ref type="bibr" target="#b33">[34]</ref>, a Gabor wavelet filter was applied to estimate the orientation of each pixel and match a second-order geometric lane model. Niu et al. proposed a novel curve fitting algorithm for lane detection with a two-stage feature extraction algorithm (LDTFE) <ref type="bibr" target="#b34">[35]</ref>. A density based spatial clustering of application with noise (DBSCAN) algorithm was applied to determine whether the candidate lane line segments belong to ego lanes or not. The identified small lane line segments can be fitted with curve model and this method is particularly efficient for small lane segment detection tasks.</p><p>Generally speaking, model-based methods are more robust than feature-based methods because of the use of modelfitting techniques. The noisy measurement and the outlier pixels of lane markings usually can be ignored with the model. However, model-based methods usually entail more computational cost since RANSAC has no upper limits on the number of iterations. Moreover, model-based methods are less easy to be implemented compared to the feature-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Machine Learning-Based Lane Detection Algorithms</head><p>Despite using conventional image processing-based methods to detect lane markings, some researchers focus on detecting lane marking using novel machine learning and deep learning methods. Deep learning techniques have been one of the hottest research areas in the past decade due to the development of deep network theories, parallel computing techniques, and large-scale data. Many deep learning algorithms show great advantages in computer vision tasks and the detection and recognition performance increases dramatically compared to conventional approaches. The convolution neural network (CNN) is one of the most popular approaches used for the object recognition research. CNN provides some impressive properties such as high detection accuracy, automatic feature learning, and end-to-end recognition. Recently, some researchers have successfully applied CNN and other deep learning techniques to lane detection. It was reported that by using CNN model, the lane detection accuracy increased dramatically from 80% to 90% compared with traditional image processing methods <ref type="bibr" target="#b35">[36]</ref>.</p><p>Li et al. proposed a lane detection system based on deep CNN and recurrent neural network (RNN) <ref type="bibr" target="#b36">[37]</ref>. A CNN was fed with a small ROI image that was used for multiple tasks. There are two types of CNN outputs. The first is a discrete classification result indicating if the visual cues are lane markers or not. If a lane is detected, then the other output will be the continuous estimation of lane orientation and location. To recognize the global lane structures in a video sequence instead of local lane positions in a single image, RNN was used to recognize the lane structures in sequence data with its internal memory scheme. Training was based on a merged scene with three cameras facing front, left side and rear area, respectively. It was shown that RNN can help recognize and connect lanes that are covered by vehicles or obstacles.</p><p>Gurghian et al. proposed another deep CNN method for lane marking detection using two side-facing cameras <ref type="bibr" target="#b37">[38]</ref>. The proposed CNN recognized the side lane positions with an end-to-end detection process. The CNN was trained with both real world images and synthesized images and achieved a 99% high detection accuracy. To solve the low accuracy and high computational cost problem, authors in <ref type="bibr" target="#b35">[36]</ref> proposed a novel lane marking detection method based on a point cloud map generated by a laser scanner. To improve the robustness and accuracy of the CNN result, a gradual up-sampling method was introduced. The output image was in the same format as the input images to get an accurate classification result. The reported computation cost of each algorithm is 28.8 s on average, which can be used for the offline high-precision road map construction.</p><p>In <ref type="bibr" target="#b38">[39]</ref>, a spiking neural network was used to extract edge images and lanes were detected based on Hough transform. This was inspired by the idea that a human neuron system produces a dense pulse response to edges while generating a sparse pulse signal to flat inputs. A similar approach can be found in <ref type="bibr" target="#b39">[40]</ref>. The study proposed a lane detection method based on RANSAC and CNN. One eight-layer CNN including three convolution layers was used to remove the noise in edge pixels if the input images were too complex. Otherwise, RANSAC was applied to the edge image directly to fit the lane model. He et al. proposed a dual-view CNN for lane detection <ref type="bibr" target="#b40">[41]</ref>. Two different views, which were the front view and top view of the road obtained from the same camera, were fed into the pre-trained CNN simultaneously. The CNN contained two sub-CNN networks to process two kinds of input images separately and concatenate the results eventually. Finally, an optimal global strategy taking into account lane length, width, and orientations was used to threshold the final lane markings.</p><p>Instead of using the general image processing and machine learning methods, some other researchers used evolution algorithms or heuristic algorithms to automatically search lane boundaries. For example, Revilloud et al. proposed a novel lane detection method using a confidence map and a multi-agent model inspired by human driver behaviors <ref type="bibr" target="#b41">[42]</ref>. Similarly, an ant colonies evolution algorithm for the optimal lane marking search was proposed in <ref type="bibr" target="#b42">[43]</ref>. A novel multiplelane detection method using directional random walking was introduced in <ref type="bibr" target="#b43">[44]</ref>. In that study, a morphology-based approach was used to extract lane mark features at the beginning.</p><p>Then, the directional random walking based on a Markov probability matrix was applied to link candidate lane features. The proposed algorithm required no assumption about the road curvatures or lane shapes.</p><p>In summary, it can be stated that machine learning algorithms or intelligent algorithms increase the lane detection accuracy significantly and provide many efficient detection architectures and techniques. Although these systems usually require more computational cost and need a large number of training data, these systems are more powerful than conventional methods. Therefore, many novel efficient and robust lane detection methods with lower training and computation requirements are expected to be developed in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INTEGRATION METHODOLOGIES FOR VISION-BASED LANE DETECTION SYSTEMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Integration Methods Introduction</head><p>Although many studies have been done to enable the accurate vision-based lane detection, the robustness of the detection systems still cannot meet the real-world requirements, especially in urban areas, due to the highly random properties of the traffic and the state of roads. Therefore, a reasonable way to enhance the lane detection system is to introduce redundancy algorithms, integrate with other object detection systems or use sensor fusion methods. It is a common agreement among automotive industries that a single sensor is not enough for vehicle perception tasks. Some companies such as Tesla, Mobileye, and Delphi developed their own intelligent on-vehicle perception systems using multiple sensors like cameras, and radar (especially the millimeter-wave radar). In this section, the integration methods will be classified into three levels, which are algorithm level, system level, and sensor level, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Specifically, the algorithm level integration combines different lane detection algorithms together to comprehensively determine reasonable lane positions and improve the robustness of the system. In the system level integration, different object detection systems work simultaneously with real-time communication with one another. Finally, in the sensor level integration, multi-modal sensors are integrated. The proposed sensor fusion methods in this level are believed to improve the robustness of the lane detection system most significantly. In the following sub-sections, the aforementioned multi-level integration techniques will be described in detail and the studies conducted within each scope will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm Level Integration</head><p>Integration of vision-based lane detection algorithms has been widely used in the past. Previous studies focused on two main integration architectures, which can be summarized as parallel and serial combination methods. Moreover, featurebased and model-based algorithms can also be combined together. Serial combination methods were commonly seen in the past. Studies in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b24">[25]</ref> demonstrate the examples of methods that serially combined the Hough transform, RANSAC, and spline model fitting methods. Another method followed in multiple studies involves applying a lane tracking system after the lane detection procedure to refine and improve the stability of the detected lanes <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>. For lane tracking, Kalman filter and particle filter were two most widely used tracking algorithms <ref type="bibr" target="#b3">[4]</ref>. Shin proposed a super-particle filter combining two separate particle filters for ego lane boundary tracking <ref type="bibr" target="#b47">[48]</ref>. In <ref type="bibr" target="#b48">[49]</ref>, a learning-based lane detection method was proposed and tracked with a particle filter. The learning-based algorithm requires no prior road model and vehicle velocity knowledge. Parallel combination methods can be found in <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. In <ref type="bibr" target="#b49">[50]</ref>, a monocular vision-based lane detection system was combined with two independent algorithms in parallel to make a comprehensive judgement. The first algorithm used a lane marking extractor and road shape estimation to find potential lanes. Meanwhile, a simple feature-based detection algorithm was applied to check the candidate lanes chosen by the first algorithm. If the final results from the two algorithms are comparable with each other, the detection result is accepted. Labayrade et al. proposed three parallel integrated algorithms to pursue a robust lane detection with higher confidence <ref type="bibr" target="#b50">[51]</ref>. Two lower level lane detection algorithms, namely, lateral and longitudinal consistent detection methods, were processed simultaneously. Then, the sampling points of the detected lanes given by these two lower level detection algorithms were tested. If the results were close to each other, the detection was viewed as a success and the average position from the two algorithms was selected as the lane position.</p><p>Some studies also combined different lane features to construct a more accurate feature vector for lane detections. In <ref type="bibr" target="#b51">[52]</ref>, the lane detection system was based on the fusion of color and edge features. Color features were used to separate road foreground and background regions using Otsus method, while edges were detected with a Canny detector. Finally, curve lanes in the image were fitted using the Lagrange interpolating polynomial. In <ref type="bibr" target="#b52">[53]</ref>, a three-features based automatic lane detection algorithm (TFALDA) was proposed. Lane boundary was represented as a three-feature vector, which includes intensity, position, and orientation values of the lane pixels. The continuity of lanes was used as the selection criteria to choose the best current lane vector that was at the minimum distance with the previous one.</p><p>Although parallel integration methods improve the robustness of the system by introducing redundancy algorithms, the computation burden will increase correspondingly. Therefore, a more efficient way is to combine algorithms in a dynamic manner and only initiate a redundancy system when it is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Level Integration</head><p>Lane detections in the real world can be affected by surrounding vehicles and other obstacles, which may have similar color or texture features to the lane markings in the digital images. For instance, the guardrail usually shows strong lane-like characteristics in color images and can easily cause false lane detections <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b55">[56]</ref>. Therefore, integrating the lane detection system with other onboard detection systems will enhance the accuracy of the lane detection system. Obstacle detections and road painting detections are two basic categories of vision-based detection techniques, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. By introducing an obstacle, noise measurement or outlier pixels can be filtered. Similarly, road recognition can narrow down the searching area for lane detections and lead to a reasonable result.</p><p>Lane detection algorithms usually require lane features for model fitting tasks. Nearby vehicles, especially passing vehicles are likely to cause a false detection result due to occlusion and similar factors. With the detection of surrounding vehicles, the color, shadow, appearance, and the noise generated by the vehicles ahead can be removed and a higher accuracy of lane boundaries can be achieved <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b56">[57]</ref>- <ref type="bibr" target="#b59">[60]</ref>, the lane detection result was reported to be more accurate with a front-vehicle detection system. This reduces the quantities of false-lane features, and improves the model fitting accuracy. Cheng et al. proposed an integrated lane and vehicle detection system. Lane markings were detected by analysing road and lane color features, and the system was designed so as not to be influenced by variations in illumination <ref type="bibr" target="#b56">[57]</ref>. Those vehicles that have similar colors to the lanes were distinguished on the basis of the size, shape, and motion information.</p><p>Sayanan and Trivedi <ref type="bibr" target="#b57">[58]</ref> proposed a driver assistance system based on an integration of lane and vehicle tracking systems. With the tracking of nearby vehicles, the position of surrounding vehicles within the detected lanes and their lane change behaviours can be recognized. Final evaluation results showed an impressive improvement compared to the results delivered by the single lane detection algorithm. In <ref type="bibr" target="#b60">[61]</ref>, a novel lane and vehicle detection integration method called an efficient lane and vehicle detection with integrated synergies (ELVIS) was proposed. The integration of vehicles and lane detection reduces the computation cost of finding the true lane positions by at least 35%. Similarly, an integrated lane detection and front vehicle recognition algorithm for a forward collision warning system was proposed in <ref type="bibr" target="#b61">[62]</ref>. Front vehicles were recognized with a Hough Forest method. The vehicle tracking system enhanced the accuracy of the lane detection result in high-density traffic scenarios.</p><p>In terms of road painting recognition, Qin et al. proposed a general framework of road marking detection and classification <ref type="bibr" target="#b62">[63]</ref>. Four common road markings (lanes, arrows, zebracrossing, and words) were detected and classified separately using a support vector machine. However, this system only identified the different kinds of road markings without further context explanation of each road marking. It is believed that road marking recognition results contribute to a better understanding of ego-lanes and help decide current lane types such as right/left turning lanes <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>. Finally, a large amount of research was dedicated to the integration of road detection and lane detection <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b65">[66]</ref>- <ref type="bibr" target="#b67">[68]</ref>. The Tesla and Mobileye are all reported to use a road segmentation to refine the lane detection algorithms <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. Road area is usually detected before lanes since an accurate recognition of road area increases the lane marking searching speed and provides an accurate ROI for lane detection. Besides, since the road boundaries and lanes are correlated and normally have the same direction, a road boundary orientation detection enhances the subsequent lane detection accuracy. Ma et al. proposed a Bayesian framework to integrate road boundary and lane edge detection <ref type="bibr" target="#b70">[71]</ref>. Lane and road boundaries were modelled with a second-order model and detected using a deformable template method.</p><p>Fritsch et al. proposed a road and ego-lane detection system particularly focusing on inner-city and rural roads <ref type="bibr" target="#b6">[7]</ref>. The proposed road and ego-lane detection algorithm was tested in three different road conditions. Another integrated road and ego-lane detection algorithm for urban areas was proposed in <ref type="bibr" target="#b71">[72]</ref>. Road segmentation based on an illumination invariant transform was the prior step for lane detection to reduce the detection time and increase the detection accuracy. The outputs of the system consisted of road region, ego-lane region and markings, local lane width, and the relative position and orientation of the vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sensor Level Integration</head><p>Sensor fusion dramatically improves the lane detection performance since more sensors are used and perception ability is boosted. Using multiple cameras including monocular, stereo cameras, and combining multiple cameras with different fields of view are the most common ways to enhance the lane detection system <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b72">[73]</ref>. In <ref type="bibr" target="#b72">[73]</ref>, a dense vanishing point detection method for lane detections using stereo cameras was proposed. The combination of global dense vanishing point detection and stereo camera makes the system very robust to various road conditions and multiple lane scenarios. Bertozzi and Broggi proposed a generic obstacle and lane detection (GOLD) system to detect obstacles and lanes based on stereo cameras and IPM images <ref type="bibr" target="#b54">[55]</ref>. The system was tested on the road for more than 3000 km and it showed robustness under exposure to shadow, illumination, and road variation. In <ref type="bibr" target="#b73">[74]</ref>, three wide-field cameras and one tele-lens camera were combined and sampled at the frequency of 14 Hz. Raw images were converted to the HSV format and IPM was performed. In <ref type="bibr" target="#b74">[75]</ref>, an around view monitoring (AVM) system with four fish eye cameras and one monocular front-looking camera were used for lane detection and vehicle localization. The benefit of using the AVM system is that a whole picture of the topview of the vehicle can be generated, which contains the front, surrounding, and rear views of the vehicle in one single image.</p><p>Instead of using only camera devices, the lane detection system can also be realised by combining cameras with global positioning system (GPS) and RADAR <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b81">[82]</ref>. An integration system based on vision and RADAR was proposed in <ref type="bibr" target="#b70">[71]</ref>. RADAR was particularly used for the road boundary detection in ill-illuminated conditions. Jung et al. proposed an adaptive ROI-based lane detection method aimed at designing an integrated adaptive cruise control (ACC) and lane keeping assistance (LKA) system <ref type="bibr" target="#b75">[76]</ref>. Range data from ACC was used to determine a dynamic ROI and improve the accuracy of the monocular vision-based lane detection system. The lane detection system was designed using a conventional method, which includes edge distribution function (ED), steerable filter, model fitting and tracking. If nearby vehicles were detected with the range sensor, all the edge pixels were eliminated to enhance the lane detection. Final results showed that recognition of nearby vehicles based on the range data improved the lane detection accuracy and simplified the detection algorithm.</p><p>Cui et al. proposed an autonomous vehicle positioning system based on GPS and vision system <ref type="bibr" target="#b76">[77]</ref>. Prior information like road shape was first extracted from GPS and then used to refine the lane detection system. The proposed method was extensively evaluated and found to be robust in varying road conditions. Jiang et al. proposed an integrated lane detection system in a structured highway scenario <ref type="bibr" target="#b77">[78]</ref>. Road curvatures were determined using GPS and digital maps in the beginning. Then, two lane detection modules designed for straight lanes and curved lanes, repectively, were selected accordingly. Schreiber et al. introduced a lane marking-based localisation system <ref type="bibr" target="#b82">[83]</ref>. Lane markings and curbs were detected with a stereo camera and vehicle localisation was performed with the integration of a global navigation satellite system (GNSS), a high accuracy map and a stereo vision system. The integrated localisation system achieved an accuracy up to a few centimetres in rural areas.</p><p>An integrated lane departure warning system using GPS, inertial sensor, high-accuracy map, and vision system was introduced in <ref type="bibr" target="#b83">[84]</ref>. The vsion-based LDWS is easily affected by various road conditions and weather. A sensor fusion scheme increases the stability of the lane detection system and makes the system more reliable. Moreover, a vision-based lane detection system and an accurate digital map help reduce the position errors from GPS, which leads to a more accurate vehicle localization and lane keeping.</p><p>Lidar was another widely used sensor and was the primary sensor used in most autonomous vehicles in the DARPA challenge <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, due to its high accuracy and robust sensing ability. Lane markings are on-road paintings that have higher reflective properties than the road surface in the 3D points cloud map given by Lidar. Lidar can detect lane markings according to those high reflectance points on the road. Lidar uses multiple channel laser lights to scan surrounding surfaces and build 3D images. Therefore, Lidar and vision integrated lane detection systems can be more accurate and robust to shadows and illumination change than vision-based systems <ref type="bibr" target="#b86">[87]</ref>. Shin et al. proposed a lane detection system using camera and Lidar <ref type="bibr" target="#b87">[88]</ref>. The algorithm consists of ground road extraction, lane detection with multi-modal data, and lane information combination. The proposed method shows a high detection accuracy performance (up to 90% accuracy) in real world experiments. Although camera and Lidar-based methods can cope with curved lanes, shadow, and illumination issues, they require a complex co-calibration of the multimodal sensors. Amaradi et al. proposed a lane-following and obstacle detection system using camera and Lidar <ref type="bibr" target="#b88">[89]</ref>. Lanes were first detected with Hough transform. Lidar was used to detect obstacles and measure the distance between the egovehicle and front obstacles to plan an obstacle free driving area. In <ref type="bibr" target="#b55">[56]</ref>, a fusion system of multiple cameras and Lidar was proposed to detect lane markings in urban areas. The test vehicle was reported as the only vehicle that used vision-based lane detection algorithm in the final stage of the DARPA urban challenge. The system detected multiple lanes followed by the estimation and tracking of the center lines. Lidar and cameras were first calibrated together to detect road paint and curbs.</p><p>Then, Lidar was used to reduce the false positive detection rate by detecting obstacles and drivable road area.</p><p>According to the implementation angle and surveying distances, the laser scanner device can efficiently identify the lane marking. Lane detection using this laser reflection method has also been widely applied <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b89">[90]</ref>- <ref type="bibr" target="#b93">[94]</ref>. Li et al. proposed a drivable region and lane detection system based on Lidar and vision fusion at the feature level <ref type="bibr" target="#b79">[80]</ref>. The test bed vehicle used two cameras mounted at different angles and three laser scanners. The algorithm detected the optimal drivable region using multi-modal sensors. The system was able to work under both structured and unstructured roads without any prior terrain knowledge. A laser-camera system for lane detection was introduced in <ref type="bibr" target="#b90">[91]</ref>. The two dimensional laser reflectivity map was generated on the roof of the vehicle. Instead of using constrained rule-based methods to detect lanes on the reflectivity map, a density-based spatial clustering of application with noise (DBSCAN) algorithm was applied to automatically determine the lane positions and the number of lanes in the field according to the 2D map. In <ref type="bibr" target="#b92">[93]</ref>, an integration system with laser scanner and stereo cameras was proposed. The system achieved an accurate driving area detection result even in the desert area. However, in some unstructured road or dirty road, the signals from laser scanner can carry more noise than the frame signals from the camera. Therefore, a signal filtering for the laser scanner and a sensor fusion are usually needed for the integrated systems. In this section, some sensors that are relevant to lane detection task are reviewed. Other sensors such as vehicle CAN-bus sensor and inertial measurement unit (IMU) are also commonly used in the construction of a complete vehicle perception system. Although Lidar-based lane detection system can be more precise than other systems, the cost is still too high for public transport. Therefore, recent studies like <ref type="bibr" target="#b78">[79]</ref> tend to fuse sensors such as GPS, digital map, and cameras, which are already available in commercial vehicles, to design a robust lane detection and driver assisting system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION METHODOLOGIES FOR VISION-BASED LANE DETECTION SYSTEMS</head><p>Most of the previous lane detection studies used visual verification to evaluate the system performance due to the lack of ground data, and only a few researchers proposed quantitative performance analysis and evaluation. In addition, lane detection evaluation is a complex task since the detection methods can vary across hardware and algorithms. There are not yet common metrics that can be used to comprehensively evaluate each aspect of lane detection algorithms. An accurate lane detection system in one place is not guaranteed to be accurate in another place since the road and lane situation in different countries or areas differ significantly. Some detection algorithms may even show significantly different detection results in days and nights. It is also not fair to say that a monocular vision-based system is not as good as a system with vision and Lidar fusion since the costis of the second systems are higher.</p><p>Therefore, the performance evaluation of lane detection systems is necessary and it should be noted that the best index for the lane detection performance is the driving safety issue and how robust the system is to the environment change. In this section, the evaluation methodologies used in studies are divided into offline evaluation and online evaluation categories, where the online evaluation can be viewed as a process of calculating the detection confidence in real time. The main evaluation architecture is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. As mentioned earlier, a common vision-based lane detection system can be roughly separated into three parts, which are the pre-processing, lane detection, and tracking. Accordingly, evaluation can be applied to all these three parts and the performance of these modules can be assessed separately. In the following section, influencing factors that affect the performance of a lane detection system will be summarized first. Then, the offline and online evaluation methods used in past studies and other literature are described. Finally, the evaluation metrics are discussed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influential Factors for Lane Detection Systems</head><p>The vision-based lane detection systems previous studies are different from the hardware, algorithms, and application aspects. Some focused on the highway implementation while some were used in urban areas. An accurate highway-oriented lane detection system is not guaranteed to be accurate in urban road areas since more disturbance and dense traffic will be observed in such areas. Therefore, it is impossible to use one single evaluation method or metric to assess all the existing systems. Some important factors that can affect the performance of lane detection system are listed in Table <ref type="table" target="#tab_0">I</ref>. A fair evaluation and comparison of lane detection systems should take these factors and the system working environment into consideration. Since different lane detection algorithms are designed and tested for different places, different road and lane factors in different places will affect the detection performance. Moreover, the data recording device, the camera or the other vision hardware is other aspects that can significantly influence lane detection systems. For example, the lane detection systems may have different resolutions and fields of view with different cameras, which will influence the detection accuracy. Finally, some traffic and weather factors can also lead to a different lane detection performances.</p><p>As shown in Table <ref type="table" target="#tab_0">I</ref>, many factors can cause a less accurate detection result and make the performance vary with other systems. For example, some lane detection systems were tested under a complex traffic context, which had more disturbances like crosswalks or poor quality lane markings, while some other systems were tested in standard highway environments with few influencing factors. Therefore, an ideal way is to use a common platform for algorithm evaluation, which is barely possible in real life. Hence, a mature evaluation system should take as many influential factors as possible into account and comprehensively assess the performance of the system. One potential solution for these problems is using parallel vision architecture, which will be discussed in the next section.</p><p>In the following part, the methodologies and metrics that can be used to propose a reasonable performance evaluation system are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Offline Evaluation</head><p>Offline evaluation is commonly used in previous literatures. After the framework of a lane detection system has been determined, the system performance is first evaluated offline using still images or video sequences. There are some public datasets such as KITTI Road and Caltech Road <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref> that are available on the internet. KITTI Road dataset consists of 289 training images and 290 testing images separated into three categories. The road and ego-lane area were labelled in the dataset. The evaluation is usually done by using receiver operating characteristic (ROC) curves to illustrate the pixellevel true and false detection rate. Caltech Road dataset contains 1224 labelled individual frames captured in four different road situations. Both datasets focus on evaluating road and lane detection performance in urban areas. The main drawbacks of image-based evaluation methods are that they are less reflective of real traffic environments and the datasets contain limited annotated test images.</p><p>On the other hand, video datasets depict much richer information and enable the reflection of real-life traffic situations. However, they normally require more human resources to label ground-truth lanes. To deal with this problem, Borkar et al. proposed a semi-automatic method to label lane pixels in video sequences <ref type="bibr" target="#b94">[95]</ref>. They used the time-sliced (TS) images and interpolation method to reduce the labelling workload. The time-sliced images were constructed by selecting the same rows from each video frame and re-arranging these row pixels according to the frame order. Two or more TS images were required and the accuracy of ground truth lanes was directly proportional to the number of images. The lane labelling tasks are converted to point labelling in the TS images. After the labelled ground truth points were selected from each TS image, the interpolated ground-truth lanes can be recovered into the video sequence accordingly. The authors significantly reduced the ground truth labelling workload by converting lane labelling into few points labelling tasks. This method was further improved in <ref type="bibr" target="#b48">[49]</ref> by using a so-called modified min-between-max thresholding algorithm (M2BMT) applied to both time-slices and spatial stripes of the video frames.</p><p>Despite the manual annotated ground truth, some researchers use the synthesis method to generate lane images with known position and curvature parameters in simulators <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Lopez et al. used MATLAB simulator to generate video sequences and ground truth lanes <ref type="bibr" target="#b27">[28]</ref>. Lane frames were created with known lane parameters and positions. This method was able to generate arbitrary road and lane models with an arbitrary number of video frames. Using a simulator to generate lane ground truth is an efficient way to assess the lane detection system under ideal road conditions. However, there are few driving simulators that can completely simulate real world traffic context at this moment. Therefore, the detection performance still has to be tested with real-world lane images or videos after evaluation using simulators. Another way is to test the system on real-world testing tracks to assess the lane detection system compared to the accurate lane position ground truth provided by GPS and high precision maps <ref type="bibr" target="#b78">[79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Online Evaluation</head><p>The online evaluation system combines road and lane geometry information and integrates with other sensors to generate a detection confidence. Lane geometry constraints are reliable metrics for online evaluation. Once the camera is calibrated and mounted on the vehicle, road and lane geometric characteristics such as the ego lane width can be determined. In <ref type="bibr" target="#b95">[96]</ref>, a real-time lane evaluation method was proposed based on width measurement of the detected lanes. The detected lanes were verified based on three criteria, which were the slopes and intercept of the straight lane model, the predetermined road width, and position of the vanishing point. The distribution of lane model parameters was analysed and a look-up table was created to determine the correctness of the detection. Once the detected lane width exceeds the threshold, re-estimation is proposed with respect to the lane width constraints.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, the authors used a world-coordinate measurement error instead of using errors in image coordinates to assess the detection accuracy. A road side down-facing camera was used to directly record lane information, generate ground truth, and estimate vehicle position within the lanes. In <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, real-time confidence was calculated based on the similarity measurement of the results given by different detection algorithms. The evaluation module usually assess whether the detected lane positions from different algorithms are within a certain distance. If similar results are obtained, then the detection results are averaged and a high detection confidence is reported. However, this method requires performing two algorithms simultaneously at each step, which increases the computation burden.</p><p>In <ref type="bibr" target="#b55">[56]</ref>, vision and Lidar-based algorithms were combined to build a confidence probability network. The travelling distance was adopted to determine the lane detection confidence. The system was said to have a high estimation confidence at certain meters in front of the vehicle if the vehicle can travel safely at that distance. Other online evaluation methods like estimating the offsets between the estimated center line and lane boundaries were also used in previous studies. Instead of using single sensor, vision-based lane detection results can be evaluated with other sensors such as GPS, Lidar, and highly accurate road models <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b76">[77]</ref>. A vanishing point lane detection algorithm was introduced in <ref type="bibr" target="#b96">[97]</ref>. Vanishing points of the lane segments were first detected according to a probabilistic voting method. Then, the vanishing points along with the line orientation threshold were used to determine correct lane segments. To further reduce the false detection rate, a real time inter-frame similarity model for evaluation of lane location consistency was adopted. This real time evaluation idea was also under the assumption that lane geometry properties do not change significantly within a short period of continuous frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>Existing studies mainly use visual evaluation or simple detection rates as evaluation metrics since there are still no common performance metrics to evaluate the lane detection performance. Li et al. designed a complete testing scheme for intelligent vehicles mainly focusing on the whole vehicle performance rather than just the lane detection system <ref type="bibr" target="#b97">[98]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, five major requirements for a lane detection system were given: shadow insensitivity, suitable for unpainted roads, handling of curved roads, meeting lane parallel constraints, and reliability measurement. Kluge introduced feature level metrics to measure the gradient orientation of the edge pixels and the angular deviation entropy <ref type="bibr" target="#b98">[99]</ref>. The proposed metrics are able to evaluate the edge points, the road curvatures, and the vanishing point information.</p><p>Veit et al. proposed another feature-level evaluation method based on a hand labelled dataset <ref type="bibr" target="#b99">[100]</ref>. Six different lane feature extraction algorithms were compared. The authors concluded that the feature extraction algorith, which combines the photometric and geometric features achieved the best result.</p><p>McCall and Trivedi examined the most important evaluation metrics to assess the lane detection system <ref type="bibr" target="#b100">[101]</ref>. They concluded that it is not appropriate to merely use detection rates as the metrics. Instead, three different metrics, which include standard deviation of error, mean absolute error, and standard deviation of error in rate of change were recommended.</p><p>Satzoda and Trivedi introduced five metrics to measure different properties of lane detection systems and to examine the trade-off between accuracy and computational efficiency <ref type="bibr" target="#b101">[102]</ref>. The five metrics consist of the measurement of lane feature accuracy, ego-vehicle localisation, lane position deviation, computation efficiency and accuracy, and the cumulative deviation in time. Among these metrics, the cumulative deviation in time helps determine the maximum amount of safety time and can be used to evaluate if the proposed system meets the critical response time of ADAS. However, all of these metrics pay much attention to the detection accuracy assessment and do not consider the robustness.</p><p>In summary, a lane detection system can be evaluated separately from the pre-processing, lane detection algorithms, and tracking aspects. Evaluation metrics are not limited to measuring the error between detected lanes and ground truth lanes but can also be extended to assess the lane prediction horizon, the shadow sensitivity, and the computational efficiency, etc. The specific evaluation metrics for a system should be determined based on the real-world application requirements. Generally speaking, there are three basic properties of a lane detection system, which are the accuracy, robustness, and efficiency. The primary objective of the lane detection algorithm is to meet the real-time safety requirement with acceptable accuracy and at low computational cost. Accuracy metrics measure if the algorithm can detect lanes with small errors for both straight and curved lanes. Lane detection accuracy issues have been widely studied in the past and many metrics can be found in literatures. However, the robustness issues of the detection system have not yet been sufficiently studied. Urban road images are usually used to assess the robustness of the system since more challenges are encountered in such situations. However, many other factors can also affect lane performances, such as weather, shadow and illumination, traffic, and road conditions. Some representative lane detection studies are illustrated in Table <ref type="table" target="#tab_0">II</ref>. Specifically, in Table <ref type="table" target="#tab_0">II</ref>, the pre-processing column records the image processing methods used in the literature. The integration column describes the integration methods used in the study, which may contain different levels of integration. Frame images and visual assessment in the evaluation column indicate that the proposed algorithm was only evaluated with still images and visual assessment method without any comparison with ground truth information. As shown in previous studies, a robust and accurate lane detection system usually combines detection and tracking algorithms. Besides, most advanced lane detection systems integrate with other objects detection systems or sensors to generate a more comprehensive detection network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this part, the current limitations of vision-based lane detection algorithm, integration, and evaluation are analyzed based on the context of above sections. Then, the framework of parallel vision-based lane detection system, which is regarded as a possible efficient way to solve the generalization and evaluation problems for lane algorithm design, is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Current Limitation and Challenges</head><p>Lane detection systems have been widely studied and successfully implemented in some commercial ADAS products in the past decade. A large volume of literatures can be found, which use vision-based algorithms due to the low cost of camera devices and extensive background knowledge of image processing. Although the vision-based lane detection system suffers from illumination variation, shadows, and bad weathers, it is still widely adopted and will continue dominating the future ADAS markets. The main task of a lane detection system is to design an accurate and robust detection algorithm. Accuracy issues were the main concerns of previous studies. Many advanced algorithms that are based on machine learning and deep learning methods are designed to construct a more precise system. However, the robustness issues are the key aspects that determine if a system can be applied in real life. The huge challenge to future vision-based systems is to maintain a stable and reliable lane measurement under heavy traffic and adverse weather conditions.</p><p>Considering this problem, one efficient method is to use the integration and fusion techniques. It has been proved that a single vision-based lane detection system has its limitation to deal with the varying road and traffic situation. Therefore, it is necessary to prepare a back-up system that can enrich the functionality of ADAS. Basically, a redundancy system can be constructed in three ways based on algorithm, system, and sensor level integration. Algorithm integration is a choice with the lowest cost and easy to implement. A system level integration combines lane detection system with other perception systems such as road and surrounding vehicle detection to improve the accuracy and robustness of the system. However, the two aforementioned integration methods still rely on camera vision systems and have their inevitable limitations. Sensor level integration, on the other hand, is the most reliable way to detect lanes under different situations. Another challenging task in lane detection systems is to design an evaluation system that can verify the system performance. Nowadays, a common problem is the lack of public benchmarks and data sets due to the difficulty of labelling lanes as the ground truth. Besides, there are no standard evaluation metrics that can be used to comprehensively assess the system performance with respect to both accuracy and robustness. Online confidence evaluation is another important task for lane detection systems. For ADAS and lower level automated vehicles, the driver should be alerted once a low detection confidence occurs. In terms of autonomous vehicles, it is also important to let the vehicle understand what it does in the lane detection task, which can be viewed as a self-aware and diagnostic process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Apply Parallel Theory Into Vision-Based Lane Detection Systems</head><p>Considering the aforementioned issues, a novel parallel framework for lane detection system design will be proposed in this part. The parallel lane detection framework is expected to be an efficient tool to assess the robustness as well as the lane detection system.</p><p>Parallel system is the product of advanced control systems and the computer simulation systems. It was introduced by Fei-Yue Wang and developed to control and manage complex systems <ref type="bibr" target="#b104">[105]</ref>- <ref type="bibr" target="#b106">[107]</ref>. The parallel theory is an efficient tool that can compensate the hard modelling and evaluating issue for the complex systems. The main objective of parallel system is to connect the real-world system with one or multiple artificial virtual systems that are in the cyberspace. The constructed virtual systems will have similar characteristics as the realworld complex system but not exactly the same. Here, parallel refers to a parallel interaction between the real-world system and its corresponding virtual counterparts. By connecting these systems together, analyzing and comparing their behaviors, the parallel system will be able to predict the future statuses of both the real-world system and the artificial one. According to the response and behaviors of the virtual system, the parallel system will automatically adjust the parameters of the realworld model to control and manage the real-world complex system such that an efficient solution is applied.</p><p>The construction of parallel system requires the ACP theory as the background knowledge. ACP is short for Artificial Society, Computational experiments, and Parallel execution, which are three major components of parallel systems. The complex system is firstly modeled using a holistic approach, whereas the real-world system is represented using an artificial system. After this step, the virtual system in the cyberspace becomes another solution domain of the complex system, which contributes to the potential complete solution along with the natural system in the physical space. It is hard to say that one solution will satisfy all the real world challenges. An effective solution for the control of complex systems should have the ability to deal with various situations occurring in the future. However, the limited testing scenarios in the real world cannot guarantee the potential solution being comprehensively tested. Therefore, the computation experiment module will execute a large number of virtual experiments according to the constructed artificial system in last step. Finally, since there are normally no unique solution for complex systems, the parallel execution provides an effective fashion to validate and evaluate various solutions. The parallel execution module will online update the local optimal solution to the real world system that is found in the cyberspace for better control and management <ref type="bibr" target="#b107">[108]</ref>.</p><p>Recently, a parallel vision architecture based on the ACP theory has been summarised and introduced to the computer vision society <ref type="bibr" target="#b108">[109]</ref>. The parallel vision theory offers an efficient way to deal with the detection and evaluation problems of the vision-based object detection systems. Similarly, the general idea of ACP theory within the parallel vision scope is to achieve perception and understanding of the complex realworld environment according to the combination of virtual realities and the real world information. In terms of lane detection tasks, the first artificial society module can be used to construct a virtual traffic environment and various road scenes using computer graphics and virtual reality techniques. Next, in the computation experiment module, the unlimited labelled traffic scene images and the limited real world driving images can be combined together to train a powerful lane detector using machine learning and deep learning methods. This process also contains two sub-procedures, namely, learning and training, testing and evaluating. The large-scale dataset will benefit the model training task. After that, a large amount of near-real data will sufficiently facilitate the model evaluation. Finally, in the parallel execution process, the lane detection model can be trained and evaluated with a parallel scheme in both real world and virtual environment. The lane detector can be online optimized according to its performance in the two parallel worlds.</p><p>In addition, the application of ACP parallel vision systems will efficiently solve the generalization and evaluation problems due to the utilization of the large-scale near-real synthesis images. To improve the generalization of the lane detection system, the detectors can be tested on virtual environments that have high similarity with the real world. The performance can also be sufficiently evaluated from the accuracy and robustness perspectives. Various computational experiments and model testing procedures can be continuously executed. In the computational experiments, the cutting-edge deep learning and reinforcement learning techniques can be applied to improve the accuracy and generalisation of the system without considering the lack of labelled data. Meanwhile, some deep learning models like the generative adversarial networks (GAN) can be used to generate near-real road scene images which can reflect the real world road characteristics such as illumination, occlusion, and poor visualization. In addition, in the virtual computational world, the GAN can be trained to discriminate whether the lane markings exist in the input image.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows a simplified architecture of the ACP-based lane detection system. The road and lane images are in parallel collected from the real world and the artificial cyberspace. The real world data is then used as a guide for generating nearreal artificial traffic scenes, which are automatically labeled. Both the real world data and synthesis data are fed into the data-driven computational level. Machine learning and deep learning methods are powerful tools in this level. For various driving scenarios occurred in both real world and the parallel virtual world, the model training process will try to come up with the most satisfying model. After that, the lane detection model will be exhaustively evaluated and validated in the cyberspace world according to large scale labeled data. Once a well-trained and evaluated lane detection model is constructed, the model can be applied in parallel to both the real world environment and the virtual world for real-time lane detection evaluation. Due to the safety, human resource limitation, and energy consumption, the number of experiments in real world is limited, which may not be able to deal with all the challenges from the road <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b110">[111]</ref>. In contrary, the experiments in the parallel virtual world are safer and economical to be applied. Moreover, the virtual world can simulate many more situations that are less possibly occur in the real world. Meanwhile, by using the online learning technique, the experience from the continuous learning and testing module in the virtual world will improve the real world performance. Some previous literatures have partially applied the parallel vision theory into the construction of lane detection systems <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b55">[56]</ref>. These studies try to simulate the lane detection model within the simulation environment, and process the lane detection model with the first two steps of the ACP architecture. However, to construct an actual parallel system, the ACP architecture should be treated as a whole. The final parallel execution step of the ACP theory is the core of a parallel system. This step will online update the real world model and adjust the corresponding model parameters according to the testing results in the parallel worlds. This step is also the core step, which guarantees that the learned lane detection model can be satisfied by various real-world driving scenarios. Beside applying parallel the theory to the design of intelligent transport and vehicles, one has widely used it in some other domains. For example, DeepMind use multiple processors to train their AlphaGo based on the deep reinforcement learning methods <ref type="bibr" target="#b111">[112]</ref>. The idea behind the reinforcement learning in this case is actually to construct a parallel virtual world for the virtual go player to do exercise. In summary, the parallel theory is drawing increasing attention from the researchers. The utilization of parallel vision techniques in the future is expected to become another efficient way to solve the generalization and evaluation problems for the lane detection algorithms. The ACP-based parallel lane detection system will not only assist to build an accurate model that is well tested and assessed, but also enable the intelligent vehicles to carefully adjust their detection strategies in real-time. Meanwhile, since there are too many different lane detection methodologies which can hardly be evaluated uniformly, a public virtual simulation platform can be used to compare these algorithms in the future. Those algorithms which achieve satisfactory performances in the parallel virtual worlds can then be implemented in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this study, vision-based lane detection systems are reviewed from three aspects, namely, algorithms, integration, and evaluation methods. Existing algorithms are summarized into two categories, which are conventional image processingbased, and novel machine learning (deep learning)-based methods. Next, previous integration methods of the lane detection system are divided into three levels, which are algorithm level, system level, and sensor level. In algorithm level, multiple lane detection and tracking algorithms are combined in a serial or parallel manner. System level integration combines vision-based lane detection with other road marking or obstacle detection systems. Sensor fusion enhances the vehicle perception system most significantly by fusion of multi-modal sensors. Finally, lane detection evaluation issues are analyzed from different aspects. Evaluation methods are divided into offline performance assessment and online realtime confidence evaluation.</p><p>As mentioned earlier, although the vision-based lane detection system has been widely studied in the past two decades, it is hard to say that research in this area has been mature. In fact, there are still many critical studies that need to be done, such as efficient low-cost system integration and the evaluation system design, especially the construction of parallel lane detection systems. Moreover, an increasing number of advanced object detection algorithms and architectures have been developed to optimize the lane detection systems. The continuous studies and the applications of these techniques will further benefit the ADAS and automated driving industry. The ACP-based parallel lane detection approach holds significant potentials for future implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General architecture of lane detection system. The feedback loop indicates that the tracked position of the lane markings can be used to narrow the searching and processing range of the pre-processing unit.</figDesc><graphic coords="2,324.01,628.46,226.80,78.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Diagram for lane detection integration level.</figDesc><graphic coords="5,321.51,168.27,232.44,299.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Lane detection evaluation architecture with two different evaluation methodologies.</figDesc><graphic coords="8,316.01,142.16,243.36,147.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Simple architecture of the ACP-based parallel lane detection and evaluation system.</figDesc><graphic coords="13,55.49,132.36,238.08,166.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FACTORS</head><label>I</label><figDesc>INFLUENCING LANE DETECTION SYSTEM</figDesc><table /><note><p>Lane and road factors Crosswalk, stop lane, lane color, lane style, road curvature, poor quality lane markings, complex road texture Hardware factors Camera types, camera calibration, camera mounting position, other sensors Traffic factors Road curbs, guardrail, surrounding vehicles, shadow, illumination issues, vibration Weather factors Cloudy, snowy, rainy, foggy</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chen Lv (S'14-M'16) is currently a Research Fellow at Advanced Vehicle Engineering Center, Cranfield University, UK. He received the Ph.D. degree at the Department of Automotive Engineering, Tsinghua University, China in 2016. From 2014 to 2015, he was a joint Ph.D. researcher at EECS Dept., University of California, Berkeley. His research focuses on cyber-physical system, hybrid system, advanced vehicle control and intelligence, where he has contributed over <ref type="bibr" target="#b39">40</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">National motor vehicle crash causation survey (NMVCCS), SAS analytical users manual, U.S. Department of Transportation, National Highway Traffic Safety Administration, Washington, DC, USA</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Page</surname></persName>
		</author>
		<idno>No. HS-811 053</idno>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lane departure and lane keeping</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gayko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Vehicles, A. Eskandarian</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="689" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Study on lane departure warning and lane change assistant systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Visvikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pitcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Research Laboratory</title>
		<imprint>
			<biblScope unit="page">374</biblScope>
			<date type="published" when="2008">2008</date>
			<pubPlace>Berks, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Transport Research Laboratory Project Rep. PPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video-based lane estimation and tracking for driver assistance: Survey, system, and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keeping the vehicle on the road: A survey on on-road lane detection systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yenikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yenikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new performance measure and evaluation benchmark for road detection algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhnl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. IEEE Conf. Intelligent Transportation Systems (ITSC 2013)</title>
		<meeting>16th Int. IEEE Conf. Intelligent Transportation Systems (ITSC 2013)<address><addrLine>The Hague, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision-based robust road lane detection in urban environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beyeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mirus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4920" to="4925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Road lane segmentation using dynamic programming for active safety vehicles</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3177" to="3185" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel algorithm of lane detection addressing varied scenarios of curved and dashed lanemarks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Suddamalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farkade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing Theory, Tools and Applications (IPTA)</title>
		<meeting>Int. Conf. Image essing Theory, Tools and Applications (IPTA)<address><addrLine>Orleans, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptative road lanes detection and classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Collado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hilario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Armingol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Advanced Concepts for Intelligent Vision Systems</title>
		<meeting>8th Int. Conf. Advanced Concepts for Intelligent Vision Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1151" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust lane detection in urban environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sehestedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alempijevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots and Systems<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="123" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time lane departure detection based on extended edge-linking algorithm</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. Computer Research and Development</title>
		<meeting>2nd Int. Conf. Computer Research and Development<address><addrLine>Kuala Lumpur, Malaysia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="725" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lanes detection based on unsupervised and adaptive classifier</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Cela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Computational Intelligence, Communication Systems and Networks (CICSyN)</title>
		<meeting>5th Int. Conf. Computational Intelligence, Communication Systems and Networks (CICSyN)<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A layered approach to robust lane detection at night</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Computational Intelligence in Vehicles and Vehicular Systems</title>
		<meeting>IEEE Workshop on Computational Intelligence in Vehicles and Vehicular Systems<address><addrLine>Nashville, TN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LANA: A lane extraction algorithm that uses frequency domain features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kreucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot. Autom</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="350" />
			<date type="published" when="1999-04">Apr. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient lane detection based on spatiotemporal images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="295" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A real-time system for lane detection based on FPGA and DSP</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sens. Imag</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lane detection based on improved feature map and efficient region of interest extraction</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ozgunalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dahnoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Conf. Signal and Information Processing (GlobalSIP)</title>
		<meeting>IEEE Global Conf. Signal and Information essing (GlobalSIP)<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="923" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lane detection using spline model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="677" to="689" />
			<date type="published" when="2000-07">Jul. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lane detection and tracking using B-Snake</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lane detection and tracking using a parallel-snake approach</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Robot. Syst</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="597" to="609" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">River flow lane detection and Kalman filtering-based B-spline lane tracking</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page">465819</biblScope>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved linear-parabolic model for lane following and curve detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Kelber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Brazilian Symp. Computer Graphics and Image Processing (SIBGRAPI&apos;05)</title>
		<meeting>18th Brazilian Symp. Computer Graphics and Image essing (SIBGRAPI&apos;05)<address><addrLine>Natal, Rio Grande do Norte, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp</title>
		<meeting>IEEE Intelligent Vehicles Symp<address><addrLine>Eindhoven, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking with ransac and Kalman filter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th IEEE Int. Conf. Image Processing (ICIP)</title>
		<meeting>16th IEEE Int. Conf. Image essing (ICIP)<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3261" to="3264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detection of lane markings based on ridgeness and RANSAC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saludes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lumbreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Transportation Systems</title>
		<meeting>IEEE Intelligent Transportation Systems<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="254" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust lane markings detection and road geometry computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lumbreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Automot. Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="407" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A real-time lane detection algorithm based on a hyperbola-pair model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp</title>
		<meeting>IEEE Intelligent Vehicles Symp<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="510" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved river flow and random sample consensus for curve lane detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-lane detection in urban driving environments using conditional random fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp. (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symp. (IV)<address><addrLine>Gold Coast, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1297" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autonomous vehicle and real time road lanes detection and tracking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bounini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gingras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lapointe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pollart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Vehicle Power and Propulsion Conf. (VPPC)</title>
		<meeting>IEEE Vehicle Power and Propulsion Conf. (VPPC)<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multi-segment lane-switch algorithm for efficient real-time lane detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Information and Automation (ICIA)</title>
		<meeting>IEEE Int. Conf. Information and Automation (ICIA)<address><addrLine>Hailar, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel lane detection based on geometrical model and Gabor filter</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symposium (IV)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust lane detection using two-stage feature extraction with curve fitting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="225" to="233" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lane marking detection based on Convolution Neural Network from point clouds</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Conf. Intelligent Transportation Systems (ITSC)</title>
		<meeting>19th Int. Conf. Intelligent Transportation Systems (ITSC)<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2475" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep neural network for structural prediction and lane detection in traffic scene</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="690" to="703" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepLanes: End-to-end lane position estimation using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gurghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Bailur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Murali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lane detection based on spiking neural network and Hough transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Congr. Image and Signal Processing (CISP)</title>
		<meeting>8th Int. Congr. Image and Signal essing (CISP)<address><addrLine>Shenyang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast learning method for convolutional neural networks using extreme learning machine and its application to lane detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="109" to="121" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accurate and robust lane detection based on Dual-View Convolutional Neutral Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symposium (IV)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1041" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new multi-agent approach for lane detection and tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Revilloud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Rahal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3147" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An evolutionary approach to lane markings detection in road environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fascioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atti del</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="627" to="636" />
			<date type="published" when="2002-08">Aug. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lane detection using directional random walks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp</title>
		<meeting>IEEE Intelligent Vehicles Symp<address><addrLine>Eindhoven, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="303" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Road tracking using particle filters with partition sampling and auxiliary variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1463" to="1471" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic lane tracking in difficult road scenarios using stereovision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Danescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="272" to="282" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust lane detection and tracking in challenging scenarios</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="26" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A superparticle filter for lane detection</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3333" to="3345" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Enhanced algorithm of automated ground truth generation and validation for lane detection system by M 2 BM T</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Suddamalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="996" to="1005" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A reliable road lane detector approach combining two vision-based algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. IEEE Conf. Intelligent Transportation Systems</title>
		<meeting>7th Int. IEEE Conf. Intelligent Transportation Systems<address><addrLine>Washington, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A reliable and robust lane detection system based on the parallel use of three algorithms for driving safety assistance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Douret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laneurit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chapuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inform. Syst</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2092" to="2100" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust lane marking detection based on multi-feature fusion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hernndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Human System Interactions (HSI)</title>
		<meeting>9th Int. Conf. Human System Interactions (HSI)<address><addrLine>Portsmouth, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Three-feature based automatic lane detection algorithm (TFALDA) for autonomous driving</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">U</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="225" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust monocular lane detection in urban environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Felisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symposium (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symposium (IV)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GOLD: A parallel real-time stereo vision system for generic obstacle and lane detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Finding multiple lanes in urban road networks with vision and lidar</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auton. Robots</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="103" to="122" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lane detection with moving vehicles in the traffic scenes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="582" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Integrated lane and vehicle detection, localization, and tracking: A synergistic approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Applying a functional neurofuzzy network to real-time lane detection and front-vehicle distance measurement</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part C (Appl. Rev.)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="589" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On-board vision system for lane recognition and front-vehicle detection to enhance driver&apos;s awareness</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation</title>
		<meeting>IEEE Int. Conf. Robotics and Automation<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">Jul. 2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2456" to="2461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient lane and vehicle detection with integrated synergies (ELVIS)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Integration of vehicle and lane detection for forward collision warning system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Consumer Electronics-Berlin</title>
		<meeting>6th Int. Conf. Consumer Electronics-Berlin<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
		<respStmt>
			<orgName>ICCE-Berlin)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A general framework for road marking detection and analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. IEEE Conf. Intelligent Transportation Systems-(ITSC)</title>
		<meeting>16th Int. IEEE Conf. Intelligent Transportation Systems-(ITSC)<address><addrLine>The Hague, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="619" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Automatic real-time road marking recognition using a feature driven approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kheyrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="133" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Detection and recognition of painted road surface markings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Pattern Recognition Applications and Methods</title>
		<meeting>4th Int. Conf. Pattern Recognition Applications and Methods<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient deep models for monocular road segmentation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)<address><addrLine>Daejeon, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4885" to="4891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vanishing point detection for road detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">StixelNet: A deep convolutional network for obstacle detection and road segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th British Machine Vision Conference</title>
		<meeting>26th British Machine Vision Conference<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Stereo-assist: Top-down stereo for driver assistance systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp. (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symp. (IV)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Development of a camera-based forward collision alert system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAE Int. J. Passeng. Cars-Mech. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="467" to="478" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Road and lane edge detection with multisensor fusion methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakahmanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing<address><addrLine>Kobe, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Vision-based robust road lane detection in urban environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beyeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mirus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4920" to="4925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multiple lane detection algorithm based on novel dense vanishing point estimation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ozgunalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dahnoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="632" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A fast and robust approach to lane marking detection and lane tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lipski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Linz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Southwest Symp. Image Analysis and Interpretation</title>
		<meeting>IEEE Southwest Symp. Image Analysis and Interpretation<address><addrLine>Santa Fe, NM, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Lane-level localization using an AVM camera for an automated driving vehicle in urban environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ASME Trans. Mechatron</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="290" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sensor fusion-based lane detection for LKS+ACC system</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Automot. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Real-time global localization of robotic cars in lane level via lane marking detection and shape registration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1039" to="1050" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Computer vision-based multiple-lane detection on straight road and in a curve</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Analysis and Signal Processing (IASP)</title>
		<meeting>Int. Conf. Image Analysis and Signal essing (IASP)<address><addrLine>Zhejiang, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An integrated vehicle navigation system utilizing lane-detection and lateral position estimation systems in difficult environments for GPS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Britt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bevly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2615" to="2629" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A sensor-fusion drivable-region and lane-detection system for autonomous vehicle navigation in challenging road scenarios</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nuchter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="555" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Lidar-based lane marker detection and mapping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp., Eindhoven, Netherlands</title>
		<meeting>IEEE Intelligent Vehicles Symp., Eindhoven, Netherlands</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1137" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Detection and tracking of road networks in rural terrain by fusing vision and LIDAR</title>
		<author>
			<persName><forename type="first">M</forename><surname>Manz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Himmelsbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wuensche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="4562" to="4568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Laneloc: Lane marking based localization using highly accurate maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Knoppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp. (IV)</title>
		<meeting>IEEE Intelligent Vehicles Symp. (IV)<address><addrLine>Gold Coast, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A low-cost solution for an integrated multisensor lane departure warning system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Clanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bevly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hodel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Junior: The stanford entry in the urban challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dahlkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dolgov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haehnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hilden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huhnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klumpp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paefgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petrovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pflueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="569" to="597" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">The DARPA Urban Challenge: Autonomous Vehicles in City Traffic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iagnemma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multichannel lidar processing for lane detection and estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wanielik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Isogai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. IEEE Conf. Intelligent Transportation Systems</title>
		<meeting>12th Int. IEEE Conf. Intelligent Transportation Systems<address><addrLine>St. Louis, MO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Combinatorial approach for lane detection using image and LIDAR reflectance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Conf. Ubiquitous Robots and Ambient Intelligence (URAI)</title>
		<meeting>12th Int. Conf. Ubiquitous Robots and Ambient Intelligence (URAI)<address><addrLine>Goyang, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="485" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Lane following and obstacle detection techniques in autonomous driving vehicles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Amaradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sriramoju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tewolde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Electro Information Technology (EIT)</title>
		<meeting>IEEE Int. Conf. Electro Information Technology (EIT)<address><addrLine>Grand Forks, ND, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Roadway detection and lane detection using multilayer laserscanner</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Microsystems for Automotive Applications 2005</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Valldorf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Gessner</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="197" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Lane surface identification based on reflectance using laser range finder</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hernndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/SICE Int. Symp. System Integration (SII)</title>
		<meeting>IEEE/SICE Int. Symp. System Integration (SII)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="621" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Lane detection and street type classification using laser range images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sparbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Streller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Transportation Systems</title>
		<meeting>IEEE Intelligent Transportation Systems<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A laserscanner-vision fusion system implemented on the terramax autonomous vehicle</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cattani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots and Systems<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A laser-scanner-based approach toward driving safety and traffic data collection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shibasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="534" to="546" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A robust lane detection and verification method for intelligent vehicles</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Symp. Intelligent Information Technology Application</title>
		<meeting>3rd Int. Symp. Intelligent Information Technology Application<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="521" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A robust lane detection method based on vanishing point estimation using the relevance of line segments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3254" to="3266" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Intelligence testing for autonomous vehicles: A new approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Veh</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="166" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Performance evaluation of vision-based lane sensing: Some preliminary tools, metrics, and results</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kluge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Intelligent Transportation Systems</title>
		<meeting>Conf. Intelligent Transportation Systems<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="723" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Evaluation of road marking feature extraction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. IEEE Conf. Intelligent Transportation Systems</title>
		<meeting>11th Int. IEEE Conf. Intelligent Transportation Systems<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Performance evaluation of a vision based lane tracker designed for driver assistance systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mccall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intelligent Vehicles Symp</title>
		<meeting>Intelligent Vehicles Symp<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">On performance evaluation metrics for lane estimation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Pattern Recognition</title>
		<meeting>22nd Int. Conf. Pattern Recognition<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2625" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A robust linear-parabolic model for lane following</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Kelber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Brazilian Symp. Computer Graphics and Image Processing</title>
		<meeting>17th Brazilian Symp. Computer Graphics and Image essing<address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A robust lane detection and departure warning system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Jayagopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intelligent Vehicles Symp., Seoul, South Korea</title>
		<meeting>IEEE Intelligent Vehicles Symp., Seoul, South Korea</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Parallel system methods for management and control of complex systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Dec</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="485" to="489" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Parallel control and management for intelligent transportation systems: Concepts, architectures, and applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="630" to="638" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Artificial societies, computational experiments, and parallel systems: A discussion on computational theory of complex socialeconomic systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst. Complexity Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Parallel learning-a new framework for machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Autom. Sinica</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Parallel vision for perception and understanding of complex scenes: Methods, framework, and perspectives</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="329" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Parallel driving in CPSS: A unified approach for transport automation and vehicle intelligence</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Autom. Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="587" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Simultaneous observation of hybrid states for cyber-physical systems: A case study of electric vehicle powertrain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2017.2738003</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">He then received the MSc. degree in control systems from the Department of Automatic Control and System Engineering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His work focuses on the understanding of driver behaviors and identification of driver intentions using machine-learning methods for intelligent and automated vehicles</title>
		<meeting><address><addrLine>China; UK</addrLine></address></meeting>
		<imprint>
			<publisher>The University of Sheffield</publisher>
			<date type="published" when="2012">Jan. 2016. 2012. 2014</date>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
		</imprint>
		<respStmt>
			<orgName>University of Science and Technology ; Cranfield University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include driver behavior modelling</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
