<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Deep Learning for Monocular Depth Map Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
							<email>yevhen.kuznietsov@rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jörg</forename><surname>Stückler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Deep Learning for Monocular Depth Map Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-theart methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating depth from single images is an ill-posed problem which cannot be solved directly from bottom-up geometric cues in general. Instead, a-priori knowledge about the typical appearance, layout and size of objects needs to be used, or further cues such as shape from shading or focus have to be employed which are difficult to model in realistic settings. In recent years, supervised deep learning approaches have demonstrated promising results for single image depth prediction. These learning approaches appear to capture the statistical relationship between appearance and distance to objects well.</p><p>Supervised deep learning, however, requires vast amounts of training data in order to achieve high accuracy and to generalize well to novel scenes. Supplementary depth sensors are typically used to capture ground truth. In the indoor setting, active RGB-D cameras can be used. Outdoors, 3D laser scanners are a popular choice to capture depth measurements. However, using such sensing devices bears several shortcomings. Firstly, the sensors have their Figure <ref type="figure">1</ref>. We concurrently train a CNN from unsupervised and supervised depth cues to achieve state-of-the-art performance in single image depth prediction. For supervised training we use (sparse) ground-truth depth readings from a supplementary sensing cue such as a 3D laser. Unsupervised direct image alignment complements the ground-truth measurements with a training signal that is purely based on the stereo images and the predicted depth map for an image. own error and noise characteristics, which will be learned by the network. In addition, when using 3D lasers, the measurements are typically much sparser than the images and do not capture high detail depth variations visible in the images well. Finally, accurate extrinsic and intrinsic calibration of the sensors is required. Ground truth data could alternatively be generated through synthetic rendering of depth maps. The rendered images, however, do not fully realistically display the scene and do not incorporate real image noise characteristics.</p><p>Very recently, unsupervised methods have been introduced <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> that learn to predict depth maps directly from the intensity images in a stereo setup-without the need for an additional supplementary modality for capturing the ground truth. One drawback of these approaches is the wellknown fact that stereo depth reconstruction based on image matching is an ill-posed problem on its own. To this end, common regularization schemes can be used which impose priors on the depth such as small depth gradient norms which may not be fully satisfied in the real environment.</p><p>In this paper, we propose a semi-supervised learning approach that makes use of supervised as well as unsupervised training cues to incorporate the best of both worlds. Our method benefits from ground-truth measurements as an unambiguous (but noisy and sparse) cue for the actual depth in the scene. Unsupervised image alignment complements the ground-truth by a huge amount of additional training data which is much simpler to obtain and counteracts the deficiencies of the ground-truth depth measurements. By the combination of both methods, we achieve significant improvements over the state-of-the-art in single image depth map prediction which we evaluate on the popular KITTI dataset <ref type="bibr" target="#b6">[7]</ref> in urban street scenes. We base our approach on a state-of-the-art deep residual network in an encoderdecoder architecture for this task <ref type="bibr" target="#b15">[16]</ref> and augment it with long skip connections between corresponding layers in encoder and decoder to predict high detail output depth maps. Our network converges quickly to a good model from little supervised training data, mainly due to the use of pretrained encoder weights (on ImageNet <ref type="bibr" target="#b21">[22]</ref> classification task) and unsupervised training. The use of supervised training also simplifies unsupervised learning significantly. For instance, a tedious coarse-to-fine image alignment loss as in previous unsupervised learning approaches <ref type="bibr" target="#b5">[6]</ref> is not required in our semi-supervised approach.</p><p>In summary, we make the following contributions: 1) We propose a novel semi-supervised deep learning approach to single image depth map prediction that uses supervised as well as unsupervised learning cues. 2) Our deep learning approach demonstrates state-of-the-art performance in challenging outdoor scenes on the KITTI benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the last years, several learning-based approaches to single image depth reconstruction have been proposed that are trained in a supervised way. Often, measured depth from RGB-D cameras or 3D laser scanners is used as groundtruth for training. Saxena et al. <ref type="bibr" target="#b23">[24]</ref> proposed one of the first supervised learning-based approaches to single image depth map prediction. They model depth prediction in a Markov random field and use multi-scale texture features that have been hand-crafted. The method also combines monocular cues with stereo correspondences within the MRF.</p><p>Many recent approaches learn image features using deep learning techniques. Eigen et al. <ref type="bibr" target="#b4">[5]</ref> propose a CNN architecture that integrates coarse-scale depth prediction with fine-scale prediction. The approach of Li et al. <ref type="bibr" target="#b16">[17]</ref> combines deep learning features on image patches with hierarchical CRFs defined on a superpixel segmentation of the image. They use pretrained AlexNet <ref type="bibr" target="#b13">[14]</ref> features of im-age patches to predict depth at the center of the superpixels. A hierarchical CRF refines the depth across individual pixels. <ref type="bibr">Liu et al. [20]</ref> also propose a deep structured learning approach that avoids hand-crafted features. Their deep convolutional neural fields allow for training CNN features of unary and pairwise potentials end-to-end, exploiting continuous depth and Gaussian assumptions on the pairwise potentials. Very recently, Laina et al. <ref type="bibr" target="#b15">[16]</ref> proposed to use a ResNet-based encoder-decoder architecture to produce dense depth maps. They demonstrate the approach to predict depth maps in indoor scenes using RGB-D images for training. Further lines of research in supervised training of depth map prediction use the idea of depth transfer from example images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>, or integrate depth map prediction with semantic segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Only few very recent methods attempt to learn depth map prediction in an unsupervised way. Garg et al. <ref type="bibr" target="#b5">[6]</ref> propose an encoder-decoder architecture similar to FlowNet <ref type="bibr" target="#b2">[3]</ref> which is trained to predict single image depth maps on an image alignment loss. The method only requires images of a corresponding camera in a stereo setup. The loss quantifies the photometric error of the input image warped into its corresponding stereo image using the predicted depth. The loss is linearized using first-order Taylor approximation and hence requires coarse-to-fine training. Xie et al. <ref type="bibr" target="#b26">[27]</ref> do not regress the depth maps directly, but produce probability maps for different disparity levels. A selection layer then reconstructs the right image using the left image and these probability maps. The network is trained to minimize pixel-wise reconstruction error. Godard et al. <ref type="bibr" target="#b8">[9]</ref> also use an image alignment loss in a convolutional encoder-decoder architecture but additionally enforce leftright consistency of the predicted disparities in the stereo pair. Our semi-supervised approach simplifies the use of unsupervised cues and does not require multi-scale depth map prediction in our network architecture. We also do not explicitly enforce left-right consistency, but use both images in the stereo pair equivalently to define our loss function. The semi-supervised method of Chen et al. <ref type="bibr" target="#b0">[1]</ref> incorporates the side-task of depth ranking of pairs of pixels for training a CNN on single image depth prediction. For the ranking task, ground-truth is much easier to obtain but only indirectly provides information on continuous depth values. Our approach uses image alignment as a geometric cue which does not require manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We base our approach on supervised as well as unsupervised principles for learning single image depth map prediction (see Fig. <ref type="figure">1</ref>). A straight-forward approach is to use a supplementary measuring device such as a 3D laser in order to capture ground-truth depth readings for supervised training. This process typically requires an accurate extrinsic calibration between the 3D laser sensor and the camera. Furthermore, the laser measurements have several shortcomings. Firstly, they are affected by erroneous readings and noise. They are also typically much sparser than the camera images when projected into the image. Finally, the center of projection of laser and camera do not coincide. This causes depth readings of objects that are occluded from the view point of the camera to project into the camera image. To counteract these drawbacks, we make use of twoview geometry principles to learn depth prediction directly from the stereo camera images in an unsupervised way. We achieve this by direct image alignment of one stereo image to the other. This process only requires a known camera calibration and the depth map predicted by the CNN. Our semi-supervised approach learns from supervised and unsupervised cues concurrently.</p><p>We train the CNN to predict the inverse depth ρ(x) at each pixel x ∈ Ω from the RGB image I. According to the ground truth, the predicted inverse depth should correspond to the LiDAR depth measurement Z(x) that projects to the same pixel, i.e. ρ(x)</p><formula xml:id="formula_0">−1 ! = Z(x).<label>(1)</label></formula><p>However, the laser measurements only project to a sparse subset Ω Z ⊆ Ω of the pixels in the image.</p><p>As the unsupervised training signal, we assume photoconsistency between the left and right stereo images, i.e.,</p><formula xml:id="formula_1">I 1 (x) ! = I 2 (ω(x, ρ(x))).<label>(2)</label></formula><p>In our calibrated stereo setup, the warping function can be defined as</p><formula xml:id="formula_2">ω(x, ρ(x)) := x − f b ρ(x)<label>(3)</label></formula><p>on the rectified images, where f is the focal length and b is the baseline. This image alignment constraint holds at every pixel in the image.</p><p>We additionally make use of the interchangeability of the stereo images. We quantify the supervised loss in both images by projecting the ground truth laser data into each of the stereo images. We also constrain the depth estimate between the left and right stereo images to be consistent implicitly by enforcing photoconsistency based on the inverse depth prediction for both images, i.e.,</p><formula xml:id="formula_3">I left (x) ! = I right (ω(x, ρ(x))) I right (x) ! = I left (ω(x, −ρ(x))).<label>(4)</label></formula><p>Finally, in textureless regions without ground truth depth readings, the depth map prediction problem is ill-posed and an adequate regularization needs to be imposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Loss function</head><p>We formulate a single loss function that incorporates both types of constraints that arise from supervised and unsupervised cues seamlessly,</p><formula xml:id="formula_4">L θ (I l , I r , Z l , Z r ) = λ t L S θ (I l , I r , Z l , Z r ) + γL U θ (I l , I r ) + L R θ (I l , I r ),<label>(5)</label></formula><p>where λ t and γ are trade-off parameters between supervised loss L S θ , unsupervised loss L U θ , and a regularization term L R θ . With θ we denote the CNN network parameters that generate the inverse depth maps ρ r/l,θ .</p><p>Supervised loss. The supervised loss term measures the deviation of the predicted depth map from the available ground truth at the pixels,</p><formula xml:id="formula_5">L S θ = x∈Ω Z,l ρ l,θ (x) −1 − Z l (x) δ + x∈Ω Z,r ρ r,θ (x) −1 − Z r (x) δ . (6)</formula><p>We use the berHu norm • δ as introduced in <ref type="bibr" target="#b15">[16]</ref> to focus training on larger depth residuals during CNN training,</p><formula xml:id="formula_6">d δ = |d|, d ≤ δ d 2 +δ 2 2δ , d &gt; δ .<label>(7)</label></formula><p>We adaptively set</p><formula xml:id="formula_7">δ = 0.2 max x∈Ω Z ρ(x) −1 − Z(x) .<label>(8)</label></formula><p>Note, that noise in the ground-truth measurements could be modelled as well, for instance, by weighting each residual with the inverse of the measurement variance.</p><p>Unsupervised loss. The unsupervised part of our loss quantifies the direct image alignment error in both directions</p><formula xml:id="formula_8">L U θ = x∈Ω U,l |(G σ * I l )(x) − (G σ * I r )(ω(x, ρ l,θ (x)))| + x∈Ω U,r |(G σ * I r )(x) − (G σ * I l )(ω(x, −ρ r,θ (x)))| ,<label>(9)</label></formula><p>with a Gaussian smoothing kernel G σ with a standard deviation of σ = 1 px. We found this small amount of Gaussian smoothing to be beneficial, presumably due to reducing image noise. We evaluate the direct image alignment loss at the sets of image pixels Ω U,l/r of the reconstructed images that warp to a valid location in the second image. We use linear interpolation for subpixel-level warping. Regularization loss. As suggested in <ref type="bibr" target="#b8">[9]</ref>, the smoothness term penalizes depth changes at pixels with low intensity variation. In order to allow for depth discontinuities at object contours, we downscale the regularization term anisotropically according to the intensity variation:</p><formula xml:id="formula_9">L R θ = i∈{l,r} x∈Ω φ (∇I i (x)) ⊤ ∇ρ i (x)<label>(10)</label></formula><p>with</p><formula xml:id="formula_10">φ(g) = (exp(−η |g x |), exp(−η |g y |))</formula><p>⊤ and η = 1 255 . Supervised, unsupervised, and regularization terms are seamlessly combined within our novel semi-supervised loss function formulation (see Fig. <ref type="figure" target="#fig_0">2</ref>). In contrast to previous methods, our approach treats both cameras in the stereo setup equivalently. All three loss components are formulated in a symmetric way for the cameras which implicitly enforces consistency in the predicted depth maps between the cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>We use a deep residual network architecture in an encoder-decoder scheme, similar to the supervised approach in <ref type="bibr" target="#b15">[16]</ref> (see Fig. <ref type="figure" target="#fig_1">3</ref>). Taking inspiration from nonresidual architectures such as FlowNet <ref type="bibr" target="#b2">[3]</ref> We input the final output layers at each resolution of the encoder at the respective decoder layers (long skip connections). This facilitates the prediction of fine detailed depth maps by the CNN.</p><p>includes long skip connections between the encoder and decoder to facilitate fine detail predictions at the output resolution. Table <ref type="table" target="#tab_0">1</ref> details the various layers in our network. Input to our network is the RGB camera image. The encoder resembles a ResNet-50 <ref type="bibr" target="#b10">[11]</ref> architecture (without the final fully connected layer) and successively extracts lowresolution high-dimensional features from the input image. The encoder subsamples the input image in 5 stages, the first stage convolving the image to half input resolution and each successive stage stacking multiple residual blocks. The decoder upprojects the output of the encoder using residual blocks. We found that adding long skipconnections between corresponding layers in encoder and decoder to this architecture slightly improves the performance on all metrics without affecting convergence. Moreover, the network is able to predict more detailed depth maps than without skip connections.   s with stride s. The residual is obtained from 3 successive convolutions, while the first convolution applies stride s. An additional convolution applies the same stride s and projects the input to the number of channels of the residual.</p><p>We denote a convolution of filter size k × k and stride s by conv k s . The same notation applies to pooling layers, e.g., max pool k s . Each convolution layer is followed by batch normalization with exception of the last layer in the network. Furthermore, we use ReLU activation functions on the output of the convolutions except at the inputs to the sum operation of the residual blocks where the ReLU comes after the sum operation. resblock i s denotes the residual block of type i with stride s at its first convolution layer, see Figs. <ref type="figure" target="#fig_3">4 and 5</ref> for details on each type of residual block. Smaller feature blocks consist of 16s maps, while larger blocks contain 4 times more feature maps, where s is the output scale of the residual block. Lastly, upproject is the upprojection layer proposed by Laina et al. <ref type="bibr" target="#b15">[16]</ref>. We use the fast implementation of upprojection layers, but for better illustration we visualize upprojection by its "naive" version (see Fig. <ref type="figure" target="#fig_4">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on the raw sequences of the KITTI benchmark <ref type="bibr" target="#b6">[7]</ref> which is a popular dataset for sin- gle image depth map prediction. The sequences contain stereo imagery taken from a driving car in an urban scenario. The dataset also provides 3D laser measurements from a Velodyne laser scanner that we use as ground-truth measurements (projected into the stereo images using the given intrinsics and extrinsics in KITTI). This dataset has been used to train and evaluate the state-of-the-art methods and allows for quantitative comparison.</p><p>We evaluate our approach on the KITTI Raw split into 28 testing scenes as proposed by Eigen et al. <ref type="bibr" target="#b4">[5]</ref>. We decided to use the remaining sequences of the KITTI Raw dataset for training and validation. We obtained a training set from 28 sequences in which we even the sequence distribution with 450 frames per sequence. This results in 7346 unique frames and 12600 frames in total for training. We also created a validation set by sampling every tenth frame from the remaining 5 sequences with little image motion. All these sequences are urban, so we additionally select those frames from the training sequences that are in the middle between 2 training images with distance of at least 20 frames. In total we obtain a validation set of 100 urban and 144 residential area images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We initialize the encoder part of our network with ResNet-50 <ref type="bibr" target="#b10">[11]</ref> weights pretrained for ImageNet classification task. The convolution filter weights in the decoder part are initialized randomly according to the approach of Glorot and Bengio <ref type="bibr" target="#b7">[8]</ref>. We also tried the initialization by He et al. <ref type="bibr" target="#b9">[10]</ref> but did not notice any performance difference.</p><p>We predict the inverse depth and initialize the network in such a way that the predicted values are close to 0 in the beginning of training. This way, the unsupervised direct image alignment loss is initialized with almost zero disparity between the images. However, this also results in large gradients from the supervised loss which would cause divergence of the model. To achieve a convergent optimization, we slowly fade-in the supervised loss with the number of iterations using λ t = βe −10 t . We also experimented with gradually fading in the unsupervised loss, but experienced degraded performance on the upper part of the image. In order to avoid overfitting we use L 2 regularization on all the model weights with weight decay w d = 0.00004. We also apply dropout to the output of the last upprojection layer with a dropout probability of 0.5.</p><p>To train the CNN on KITTI we use stochastic gradient descent with momentum with a learning rate of 0.01 and momentum of 0.9. We train the variants of our model for at least 15 epochs on a 6 GB NVIDIA GTX 980Ti with 6 GB memory which allows for a batch size of 5. We stop training when the validation loss starts to increase and select the best performing model on the validation set. The network is trained on a resolution of 621×187 pixels for both input images and ground truth depth maps. Hence, the resolution of the predicted inverse depth maps is 320×96. For evaluation we upsample the predicted depth maps to the resolution of the ground truth. For data augmentation, we use γ-augmentation and also randomly multiply the intensities of the input images by a value α ∈ [0.8; 1.2]. The inference from one image takes 0.048 s in average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We evaluate the accuracy of our method in depth prediction using the 3D laser ground truth on the test images. We use the following depth evaluation metrics used by Eigen et al. <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_11">RMSE: 1 T T i=1 ρ(x i ) −1 − Z(x i )) 2 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSE (log):</head><formula xml:id="formula_12">1 T T i=1 log(ρ(x i ) −1 ) − log(Z(x i ))) 2 2 , Accuracy: i∈{1,...,T } max ρ(x i ) −1 Z(x i ) , Z(x i ) ρ(x i ) −1 =δ&lt;thr T , ARD: 1 T T i=1 |ρ(xi) −1 −Z(xi)| Z(xi) , SRD: 1 T T i=1 |ρ(xi) −1 −Z(xi)| 2 Z(xi)</formula><p>where T is the number of pixels with ground-truth in the test set.</p><p>In order to compare our results with Eigen et al. <ref type="bibr" target="#b4">[5]</ref> and Godard et al. <ref type="bibr" target="#b8">[9]</ref>, we crop our image to the evaluation crop applied by Eigen et al. We also use the same resolution of the ground truth depth image and cap the predicted depth at 80 m <ref type="bibr" target="#b8">[9]</ref>. For comparison with Garg et al. <ref type="bibr" target="#b5">[6]</ref>, we apply their evaluation protocol and provide results when discarding ground-truth depth below 1 m and above 50 m while capping the predicted depths into this depth interval. This means, we set predicted depths to 1 m and 50 m if they are below 1 m or above 50 m, respectively. For an ablation study, we also give results for our method evaluated on the uncropped image without a cap on the predicted depths, but set the minimum ground-truth depth to 5 m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison with the State-of-the-Art</head><p>Table <ref type="table" target="#tab_1">2</ref> shows our results in relation to the state-of-the-art methods on the test images of the KITTI benchmark. For all metrics and setups, our system performs the best. We outperform the best setup of Godard et al. <ref type="bibr" target="#b8">[9]</ref> by 1.16 m (ca. 14%) in terms of RMSE and by 0.035 (ca. 16%) for its log scale at the cap of 80 m. When evaluating at a prediction cap of 50 m, our predictions are in average 1.586 m more accurate in RMSE than the results reported by Garg et al. <ref type="bibr" target="#b5">[6]</ref>. The benefit of adding the unsupervised loss is larger for the 0-80 m evaluation range where the ground truth is sparser for far distances.</p><p>We also qualitatively compare the output of our method with the state-of-the-art in Fig. <ref type="figure">7</ref>. In some parts, the predictions of Godard et al. <ref type="bibr" target="#b8">[9]</ref> may appear more detailed and our depth maps seem to be smoother. However, these details are not always consistent with the ground truth depth maps as also indicated by the quantitative results. For instance, our predictions for the thin traffic poles and lights of the top frame in Figure <ref type="figure">7</ref> appear more accurate. We provide additional qualitative results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Ablation Study</head><p>We also analyze the contributions of the various design choices in our approach (see Table <ref type="table" target="#tab_2">3</ref>). The use of the unsupervised loss term on all valid pixels improves the performance compared to the variant with unsupervised term evaluated only for valid pixels without available ground truth. When using the L 2 -norm on the supervised loss instead of the berHu norm, the RMSE evaluation metric on the ground-truth depth improves on the validation set, but is worse on the test set. The L 2 -norm also visually produces noisier depth maps. Thus, we prefer to use BerHu over L 2 , which reduces the noise (see Fig. <ref type="figure" target="#fig_5">8</ref>) and performs better on the test set. We also found that our system benefits from both long skip connections and Gaussian smoothing in the unsupervised loss. The latter also results in slightly faster convergence. Cumulatively, the performance drop without long skip connections and without Gaussian smoothing is 0.119 in RMSE towards our full approach. To show that our approach benefits from the semisupervised pipeline, we also give results for purely supervised and purely unsupervised training. For purely supervised learning, our network achieves less accurate depth map prediction (0.235 higher RMSE) than in the semisupervised setting. In the unsupervised case, the depth maps include larger amounts of outliers such that we provide results for capped depth predictions at a maximum of 50 m. Here, our network seems to perform less well than the unsupervised methods of Godard et al. <ref type="bibr" target="#b8">[9]</ref> and Garg et al. <ref type="bibr" target="#b5">[6]</ref>. Notably, our approach does not perform multi-scale image alignment, but uses the available ground truth to avoid local optima of the direct image alignment. We also demonstrate that our system does not suffer severely if the ground truth depth is reduced to 50% or 1% of the available measurements. To this end, we subsample the available laser data prior to projecting it into the camera image.</p><p>Our results clearly demonstrate the benefit of using a deep residual encoder-decoder architecture with long skip connection for the task of single image depth map prediction. Our semi-supervised approach gives additional training cues to the supervised loss through direct image alignment. This combination is even capable of improving depth prediction error for the laser ground-truth compared to purely supervised learning. Our semi-supervised learning method converges much faster (in about one third the number of iterations) than purely supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Generalization to Other Datasets</head><p>We also demonstrate the generalization ability of our model trained on KITTI to other datasets. Fig. <ref type="figure" target="#fig_6">9</ref> gives qualitative results of our model on test images of Make3D <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> and Cityscapes <ref type="bibr" target="#b1">[2]</ref>. We also evaluated our model quantitatively on Make3D where it results in 8.237 RMSE (m),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB GT [5] [20] [6]</head><p>[9] ours from <ref type="bibr" target="#b8">[9]</ref> from <ref type="bibr" target="#b8">[9]</ref> from <ref type="bibr" target="#b8">[9]</ref> from <ref type="bibr" target="#b8">[9]</ref> from <ref type="bibr" target="#b8">[9]</ref> Figure <ref type="figure">7</ref>. Qualitative results and comparison with state-of-the-art methods. Ground-truth (GT) has been interpolated for visualization. Note the crisper prediction of our method on objects such as cars, pedestrians and traffic signs. Also notice, how our method can learn appropriate depth predictions in the upper part of the image that is not covered by the ground-truth.  0.190 Log10 error (see <ref type="bibr" target="#b15">[16]</ref>) and 0.421 ARD. Qualitatively, our model can capture the general scene layout and objects such as cars, trees and pedestrians well in images that share similarities with the KITTI dataset. Further qualitative results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel semi-supervised deep learning approach to monocular depth map prediction. Purely supervised learning requires a vast amount of data. In outdoor environments, often supplementary sensors such as 3D lasers have to be used to acquire training data. These sensors come with their own shortcoming such as specific error and noise characteristics and sparsity of the measure-ments. We complement such supervised cues with unsupervised learning based on direct image alignment between the images in a stereo camera setup. We quantify the photoconsistency of pixels in both images that correspond to each others according to the depth predicted by the CNN.</p><p>We use a state-of-the-art deep residual network in an encoder-decoder architecture and enhance it with long skip connections. Our main contribution is a seamless combination of supervised, unsupervised, and regularization terms in our semi-supervised loss function. The loss terms are defined symmetrically for the available cameras in the stereo setup, which implicitly promotes consistency in the depth estimates. Our approach achieves state-of-the-art performance in single image depth map prediction on the popular KITTI dataset. It is able to predict detailed depth maps on thin and distant objects. It also estimates reasonable depth in image parts in which there is no ground-truth available for supervised learning.</p><p>In future work, we will investigate semi-supervised learning for further tasks such as semantic image segmentation. Our approach could also be extended to couple monocular and stereo depth cues in a unified deep learning framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Components and inputs of our novel semi-supervised loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of our deep residual encoder-decoder architecture (c1, c3, mp1 abbreviate conv1, conv3, and max pool1, respectively). Skip connections from corresponding encoder layers to the decoder facilitate fine detailed depth map prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Type 1 residual block resblock 1 s with stride s = 1. The residual is obtained from 3 successive convolutions. The residual has the same number of channels as the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Type 2 residual block resblock 2s with stride s. The residual is obtained from 3 successive convolutions, while the first convolution applies stride s. An additional convolution applies the same stride s and projects the input to the number of channels of the residual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Schematic illustration of the upprojection residual block. It unpools the input by a factor of 2 and applies a residual block which reduces the number of channels by a factor of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Qualitative results of variants of our semi-supervised learning approach on the KITTI raw test set. Shown variants are our full approach (full), our model trained supervised only (sup. only), our model with L2 norm on the supervised loss (L2) and using half the ground-truth laser measurements (half GT) for semi-supervised training.</figDesc><graphic url="image-53.png" coords="8,141.74,375.35,142.89,59.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative results on Make3D (left 2) and Cityscapes (right).</figDesc><graphic url="image-56.png" coords="8,141.79,438.01,142.79,59.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>, our architecture Layers in our deep residual encoder-decoder architecture.</figDesc><table><row><cell>Layer</cell><cell cols="2">Channels I/O Scaling</cell><cell>Inputs</cell></row><row><cell>conv1 7 2</cell><cell>3 / 64</cell><cell>2</cell><cell>RGB</cell></row><row><cell>max pool1 3 2</cell><cell>64 / 64</cell><cell>4</cell><cell>conv1</cell></row><row><cell>res block1 2 1</cell><cell>64 / 256</cell><cell>4</cell><cell>max pool1</cell></row><row><cell>res block2 1 1</cell><cell>256 / 256</cell><cell>4</cell><cell>res block1</cell></row><row><cell>res block3 1 1</cell><cell>256 / 256</cell><cell>4</cell><cell>res block2</cell></row><row><cell>res block4 2 2</cell><cell>256 / 512</cell><cell>8</cell><cell>res block3</cell></row><row><cell>res block5 1 1</cell><cell>512 / 512</cell><cell>8</cell><cell>res block4</cell></row><row><cell>res block6 1 1</cell><cell>512 / 512</cell><cell>8</cell><cell>res block5</cell></row><row><cell>res block7 1 1</cell><cell>512 / 512</cell><cell>8</cell><cell>res block6</cell></row><row><cell>res block8 2 2</cell><cell>512 / 1024</cell><cell>16</cell><cell>res block7</cell></row><row><cell>res block9 1 1</cell><cell>1024 / 1024</cell><cell>16</cell><cell>res block8</cell></row><row><cell>res block10 1 1</cell><cell>1024 / 1024</cell><cell>16</cell><cell>res block9</cell></row><row><cell>res block11 1 1</cell><cell>1024 / 1024</cell><cell>16</cell><cell>res block10</cell></row><row><cell>res block12 1 1</cell><cell>1024 / 1024</cell><cell>16</cell><cell>res block11</cell></row><row><cell>res block13 1 1</cell><cell>1024 / 1024</cell><cell>16</cell><cell>res block12</cell></row><row><cell>res block14 2 2</cell><cell>1024 / 2048</cell><cell>32</cell><cell>res block13</cell></row><row><cell>res block15 1 1</cell><cell>2048 / 2048</cell><cell>32</cell><cell>res block14</cell></row><row><cell>res block16 1 1</cell><cell>2048 / 2048</cell><cell>32</cell><cell>res block15</cell></row><row><cell>conv2 1 1</cell><cell>2048 / 1024</cell><cell>32</cell><cell>res block16</cell></row><row><cell>upproject1</cell><cell>1024 / 512</cell><cell>16</cell><cell>conv2</cell></row><row><cell>upproject2</cell><cell>512 / 256</cell><cell>8</cell><cell>upproject1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>res block13</cell></row><row><cell>upproject3</cell><cell>256 / 128</cell><cell>4</cell><cell>upproject2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>res block7</cell></row><row><cell>upproject4</cell><cell>128 / 64</cell><cell>2</cell><cell>upproject3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>res block3</cell></row><row><cell>conv3 3 1</cell><cell>64 / 1</cell><cell>2</cell><cell>upproject4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>RMSE RMSE (log) ARDSRD δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.253   Quantitative results of our method and approaches reported in the literature on the test set of the KITTI Raw dataset used by Eigen et al.<ref type="bibr" target="#b4">[5]</ref> for different caps on ground-truth and/or predicted depth. Best results shown in bold, second best in italic. RMSE RMSE (log) δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.253   </figDesc><table><row><cell>Approach</cell><cell>cap</cell><cell></cell><cell cols="2">lower is better</cell><cell></cell><cell cols="2">higher is better</cell><cell></cell></row><row><cell>Eigen et al. [5] coarse 28×144</cell><cell>0 -80 m</cell><cell>7.216</cell><cell>0.273</cell><cell>0.228</cell><cell>-</cell><cell>0.679</cell><cell>0.897</cell><cell>0.967</cell></row><row><cell>Eigen et al. [5] fine 27×142</cell><cell>0 -80 m</cell><cell>7.156</cell><cell>0.270</cell><cell>0.215</cell><cell>-</cell><cell>0.692</cell><cell>0.899</cell><cell>0.967</cell></row><row><cell>Liu et al. [20] DCNF-FCSP FT</cell><cell>0 -80 m</cell><cell>6.986</cell><cell>0.289</cell><cell cols="2">0.217 1.841</cell><cell>0.647</cell><cell>0.882</cell><cell>0.961</cell></row><row><cell>Godard et al. [9]</cell><cell>0 -80 m</cell><cell>5.849</cell><cell>0.242</cell><cell cols="2">0.141 1.369</cell><cell>0.818</cell><cell>0.929</cell><cell>0.966</cell></row><row><cell>Godard et al. [9] + CS</cell><cell>0 -80 m</cell><cell>5.763</cell><cell>0.236</cell><cell cols="2">0.136 1.512</cell><cell>0.836</cell><cell>0.935</cell><cell>0.968</cell></row><row><cell cols="2">Godard et al. [9] + CS + post-processing 0 -80 m</cell><cell>5.381</cell><cell>0.224</cell><cell cols="2">0.126 1.161</cell><cell>0.843</cell><cell>0.941</cell><cell>0.972</cell></row><row><cell>Ours, supervised only</cell><cell>0 -80 m</cell><cell>4.815</cell><cell>0.194</cell><cell cols="2">0.122 0.763</cell><cell>0.845</cell><cell>0.957</cell><cell>0.987</cell></row><row><cell>Ours, unsupervised only</cell><cell>0 -80 m</cell><cell>8.700</cell><cell>0.367</cell><cell cols="2">0.308 9.367</cell><cell>0.752</cell><cell>0.904</cell><cell>0.952</cell></row><row><cell>Ours</cell><cell>0 -80 m</cell><cell>4.621</cell><cell>0.189</cell><cell cols="2">0.113 0.741</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell></row><row><cell>Garg et al. [6] L12 Aug 8x</cell><cell>1 -50 m</cell><cell>5.104</cell><cell>0.273</cell><cell cols="2">0.169 1.080</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell></row><row><cell>Ours, supervised only</cell><cell>1 -50 m</cell><cell>3.531</cell><cell>0.183</cell><cell cols="2">0.117 0.597</cell><cell>0.861</cell><cell>0.964</cell><cell>0.989</cell></row><row><cell>Ours, unsupervised only</cell><cell>1 -50 m</cell><cell>6.182</cell><cell>0.338</cell><cell cols="2">0.262 4.537</cell><cell>0.768</cell><cell>0.912</cell><cell>0.955</cell></row><row><cell>Ours</cell><cell>1 -50 m</cell><cell>3.518</cell><cell>0.179</cell><cell cols="2">0.108 0.595</cell><cell>0.875</cell><cell>0.964</cell><cell>0.988</cell></row><row><cell>Approach</cell><cell></cell><cell></cell><cell cols="2">lower is better</cell><cell></cell><cell cols="2">higher is better</cell><cell></cell></row><row><cell>Supervised training only</cell><cell></cell><cell></cell><cell>4.862</cell><cell>0.197</cell><cell>0.839</cell><cell>0.956</cell><cell>0.986</cell><cell></cell></row><row><cell cols="2">Unsupervised training only (50 m cap)</cell><cell></cell><cell>6.930</cell><cell>0.330</cell><cell>0.745</cell><cell>0.903</cell><cell>0.952</cell><cell></cell></row><row><cell>Only 50 % of laser points used  *</cell><cell></cell><cell></cell><cell>4.808</cell><cell>0.192</cell><cell>0.852</cell><cell>0.958</cell><cell>0.986</cell><cell></cell></row><row><cell>Only 1 % of laser points used  *</cell><cell></cell><cell></cell><cell>4.892</cell><cell>0.202</cell><cell>0.843</cell><cell>0.952</cell><cell>0.983</cell><cell></cell></row><row><cell cols="3">No long skip connections and no Gaussian smoothing  *</cell><cell>4.798</cell><cell>0.195</cell><cell>0.853</cell><cell>0.957</cell><cell>0.984</cell><cell></cell></row><row><cell>No long skip connections  *</cell><cell></cell><cell></cell><cell>4.762</cell><cell>0.194</cell><cell>0.853</cell><cell>0.958</cell><cell>0.985</cell><cell></cell></row><row><cell cols="2">No Gaussian smoothing in unsupervised loss  *</cell><cell></cell><cell>4.752</cell><cell>0.193</cell><cell>0.854</cell><cell>0.958</cell><cell>0.986</cell><cell></cell></row><row><cell cols="3">L2-norm instead of BerHu-norm in supervised loss</cell><cell>4.659</cell><cell>0.195</cell><cell>0.841</cell><cell>0.958</cell><cell>0.986</cell><cell></cell></row><row><cell>Our full approach  *</cell><cell></cell><cell></cell><cell>4.679</cell><cell>0.192</cell><cell>0.854</cell><cell>0.959</cell><cell>0.985</cell><cell></cell></row><row><cell>Our full approach</cell><cell></cell><cell></cell><cell>4.627</cell><cell>0.189</cell><cell>0.856</cell><cell>0.960</cell><cell>0.986</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results of different variants of our approach on the KITTI Raw Eigen test split<ref type="bibr" target="#b4">[5]</ref> (without cropping and capping the predicted depth, ground truth minimum depth is 5 m). Approaches marked with * are trained with the unsupervised loss only for the pixels without available ground truth. Best results shown in bold.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by ERC Starting Grant CV-SUPER (ERC-2012-StG-307432).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
				<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Artifical Intelligence and Statistics Conference (AISTATS)</title>
				<meeting>of the Artifical Intelligence and Statistics Conference (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03677v2</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
				<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="775" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2D-to-3D image conversion by learning depth from examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on 3D Vision (3DV)</title>
				<meeting>of the Int. Conf. on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward holistic scene understanding: Feedback enabled cascaded classification models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Patterm Analysis and Machine Intelligence (PAMI)</title>
				<imprint>
			<date type="published" when="2012-07">July 2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1394" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D depth reconstruction from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Patterm Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of the IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
				<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
