<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Source Coding and Vector Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Geometric Source Coding and Vector Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0BC98FDDBF99FFABF0B09EEBE7E5BC25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstruct -A geometric formulation is presented for source coding and vector quantizer design. Motivated by the asymptotic equipartition principle, two broad classes of source codes and vector quantizers are considered elliptical codes and quantizers based on the Gaussian density function, and pyramid codes and quantizers based on the Laplacian density function. Elliptical and weighted pyamid vector quantizers are developed by selecting codewords as points in a lattice that lie on (or near) a specified ellipse or pyramid. The combination of geometric structure and lattice basis allows simple encoding and decoding algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>;-IE DATA compression (source coding or quantiza-T tion) of an analog source plays a fundamental role in the digital processing of data. Most basic of the approaches to source coding is the well-known uniform scalar quantizer, which is commonly used for analog-to-digital conversion. Unfortunately, even optimum scalar quantization of memoryless sources [l], [2] fails to achieve performance close to the rate-distortion bound, unless the quantizer outputs are entropy encoded [3]. During the last decade, many researchers have developed block or vector quantization approaches to the source coding problem (e.g., see <ref type="bibr">[4]</ref>, <ref type="bibr">[5]</ref> and the references therein), with performance bounds for vector quantization provided in <ref type="bibr">[6]</ref>. A major contribution to the design of vector quantizers (VQ's) was the clustering algorithm of Linde et al. <ref type="bibr">[7]</ref>. This algorithm uses a training sequence representative of the source to be encoded to iteratively design a locally optimum VQ. The algorithm has been applied successfully to a variety of sources [8]- <ref type="bibr">[ll]</ref>, demonstrating the clear superiority of vector over scalar quantization (without entropy coding).</p><p>Based on surveys of the VQ literature [4], <ref type="bibr">[5]</ref>, it is readily concluded that the clustering algorithm is currently the most popular VQ design technique. Significant improvement in low-rate source coding has been demonstrated [5]- <ref type="bibr">[ll]</ref>. However, the clustering algorithm approach is Manuscript received <ref type="bibr">April 13, 19x4;</ref><ref type="bibr">revised February 2, 1987</ref>. This work was supported in part by the Air Force Office of Scientific Research under Grant AFOSR 84-0003, and in part by the National Science Foundation under Grant MIP-8619888. Portions of this paper were presented at the 1985 and 1986 International Symposia on Information Theory.</p><p>The author was with the Department of Electrical Engineering. Texas A&amp;M University, College Station, TX. He is now with the Department of Electrical and Computer Engineering, Washington State University. Pullman, WA 99164.</p><p>IEEE Log Number X825715.</p><p>inherently limited in practical applications by its design and encoding complexity. More specifically, both the design complexity and the (full search) encoding complexity grow exponentially with the product of encoding rate and vector dimension. Thus it is obviously of interest to develop VQ design techniques that do not suffer an exponential growth in complexity as a function of the rate-dimension product and hence are instrumentable for moderate to large encoding rates or vector dimensions.</p><p>In a source coding context, the asymptotic equipartition principle (AEP) <ref type="bibr">[12]</ref> suggests that almost all codewords be selected to lie in a region of high probability <ref type="bibr">[13]</ref> specified by the entropy of the source. This region of high probability will have a geometry that is dependent on the source (e.g., the sphere for the memoryless Gaussian source [14], the hypercube for the uniform source, etc.). Since the probability density function is roughly constant in the region of high probability, the ,codewords, at least for large rate, should tend to be uniformly distributed in this region. Good codebooks (for large enough block length) can thus be constructed by a random coding argument (e.g., <ref type="bibr">[14]</ref>), but with the obvious disadvantage of large encoding complexity <ref type="bibr">[12, p. 1991. Alternatively, lattices [15]-[18]</ref> already fill space with uniformly distributed points; thus good codebooks can also be constructed as the intersection of the points in a lattice and the geometric region of high probability for the source. This geometric source coding approach has already yielded good vector quantizers for memoryless uniform [17]-[21], Gaussian <ref type="bibr">[22]</ref>, <ref type="bibr">[23]</ref>, and Laplacian [24], [25] sources, with simple encoding and decoding algorithms.</p><p>The present paper uses the AEP as motivation for the construction of two classes of vector quantizers. The vector quantizers are based on two simple geometries (the ellipse and the pyramid) that are induced by two common source models for the transform coding of speech <ref type="bibr">[26]</ref> and imagery <ref type="bibr">[27]</ref>. The codewords are selected as a subset of the points in a (rectangular) lattice, and simple VQ encoding algorithms are presented that have a complexity that grows polynomially with the vector dimension.</p><p>Aside from the AEP and our wish to take advantage of the potential of fast lattice encoding algorithms, another strong motivation for the development of these geometric VQ's is the desire for adaptive vector encoding. Adaptive scalar quantization, such as in the Jayant quantizer <ref type="bibr">[28]</ref>, has proven to be an important feature of scalar quantizer 0018-9448/89/0lOO-Ol37$01.00 01989 IEEE encoding systems for speech and imagery. Gain adaptive clustering VQ's have also been proposed <ref type="bibr">[33]</ref>. The geometric VQ's described in this paper are parametric in nature, with parameters that may readily be estimated for the source to be encoded. Hence these V Q s can be adapted on a vector-by-vector or frame-by-frame basis, as dictated by the source. We will defer to future papers, however, the application of geometric VQ's to adaptive encoding.</p><p>The remainder of the paper begins by describing the source models that motivate the geometric vector quantizers. The VQ's are then developed, encoding algorithms are outlined, and asymptotic mean-square error (mse) performance expressions are presented. The paper ends with two examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">SOURCE MODELS AND IMPLICIT GEOMETRIES</head><p>An implicit geometry useful for source coding is associated with each stationary ergodic source. This geometry may be simply characterized as follows. Let X, be a sequence of independent and identically distributed (i.i.d.) random variables with continuous probability density function (pdf) p ( x , ) , and let X = (XI, X,; . ., XL)T be an L-vector with components X,. The density function for X is given by L f ( x ) = n P ( <ref type="formula" target="#formula_3">4</ref>(1)</p><formula xml:id="formula_0">r = l</formula><p>and the differential entropy is defined as</p><formula xml:id="formula_1">CO h ( x) = -1 p ( x ) l o g p ( x ) dx.</formula><p>--M</p><p>For sufficiently large L, Shannon observed [13] that, for arbitrarily small c and A, for all vectors X , except those on a set of total probability less than A. That is, for sufficiently large L, X becomes localized to a particular region in L-dimensional space. In essence then, source coding of a continuous random variable amounts to nothing more than properly arranging representation points (codewords) in this region of high probability.</p><p>The geometry of (2) is obtained by setting</p><formula xml:id="formula_2">( l / L ) l o g f ( x ) = -h .</formula><p>We then have the following special case of the Shannon -McMillan -Brieman theorem.</p><p>Theorem: Let X be an L-vector of i.i.d. components with pdf as in (l), let A =supxp(x), and let S ( L , C ) denote an L-dimensional region of constant pdf given by Then X becomes localized to a region S(L,C*) in the sense that 1 L where C* is given by logC* = -h( X ) .</p><p>The proof involves a direct application of the strong law of large numbers and is omitted.</p><p>Although X becomes localized (as the dimension increases) to a particular contour of constant pdf in the logarithmic sense, X is not necessarily restricted to lie exactly on S( L , C*). However, for a large class of densities it does follow that ea" X becomes close to some corresponding vector, say X , on S( L , C*! in the per dimension norm distance sense. Letting IIX -XII, be defined as</p><formula xml:id="formula_3">l/u I l X -J k = C I x , -x , l " , v 2 1 ,<label>(4)</label></formula><p>i.l, 1 then for each X and for a &gt; 0, there exists an J? E S( L, C*) such that</p><p>Several observations can be made immediately from these results. First, for large L , it is sufficient to consider only the contour region specified by the entropy for optimum source coder design. For memoryless uniform, Gaussian, and Laplacian sources, this region is the hypercube, the sphere [14], and the pyramid [24], respectively. Second, an obvious source encoding strategy (the asymptotic equipartition principle) is to allocate representation regions about points (codewords) selected uniformly in S( L , C*). One way to select such points is by a random coding argument. Finally, a simple vector quantization strategy is suggested by (3) for the case that L is not large. The random variable r = l indexes contours of constant pdf. Each scalar quantized version of Y then specifies a particular contour region or "shell." A vector quantizer may be constructed to have codewords that lie exclusively on these concentric shells. Such a contour VQ has been developed for Gauss-Markov sources and speech <ref type="bibr">[29]</ref>, based on a clustering algorithm [7]  construction of the codebook. The shell construction has also been developed for the Laplacian source, with the codewords selected as the points in a lattice <ref type="bibr">[24]</ref>, <ref type="bibr">[25]</ref>. A product code VQ <ref type="bibr">[30]</ref> results if an equal number of codewords are assigned congruently to each shell.</p><p>The results for i.i.d. sources can be extended to the case of stationary sources with memory. There are few conceptual difficulties in doing this; however, several practical difficulties also arise. Let the L-dimensional vector X be a zero-mean stationary source with joint pdf f ( x). The source is assumed to satisfy the ergodic property h =--J..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>./f(x)logf(x)dxi:h as L+oo (6)</head><p>where h , is the entropy per degree of freedom </p><formula xml:id="formula_4">, L 1 L s( L , h L ) = { x: --log f( . ) = h 1.) (7)</formula><p>as a sufficient geometry for optimum source coding.</p><p>In the i.i.d. case, the region in (3) displays a certain symmetry in L-dimensional space and is aligned with the natural coordinate system for the source. In the present case, unfortunately, the region in (7) may be irregularly shaped or may not be aligned with the natural coordinate system. These latter features tend to make it difficult to use (7) to construct simple VQ s. For a general zero-mean stationary source Y with positive definite covariance A, it is well-known that a unitary transformation exists, say T (the Karhunen-Lokve (KL) transform) that diagonalizes A , so that if wluch would be the contour region of (7) for X in the special case that and provided the mean-absolute values of X , satisfy L 1 l ' L p 1 (Ghj + : &lt; a as L + a</p><formula xml:id="formula_5">X = T Y , (8) then CO. [ X ] = A, = diag [ U:],</formula><p>(so that the Shannon-McMillan-Breiman theorem holds).</p><p>A source code that assigns codewords to (12) is termed a (weighted) pyramid code. The ellipse S E ( L , A,) and the pyramid S,,(L, A ) are very simply represented in the transform coordinate system and are next combined with the points in a (rectangular) lattice to yield two easily implemented source codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">ELLIPTICAL AND WEIGHTED PYRAMID VECTOR QUANTIZERS</head><p>It is possible to use the geometric approach previously described, together with a random coding argument, to derive source codes that, asymptotic in rate and dimension, achieve a distortion arbitrarily close to the distortion-rate bound (e.g., Sakrison's treatment of the memoryless Gaussian source [14]). Since such codes lack structure and are termed noninstrumentable [ 121, we do not pursue them. Instead, attention is focused on geometries that are both motivated by important source models and have sufficient structure so that easily instrumented, though somewhat suboptimum in a rate-distortion sense, vector quantizers result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Elliptical VQ's</head><p>Following the development in the preceding section, we assume that (in the KL transform domain) the ellipse S,(L, A,) in (11) is a sufficient geometry for source coder design. The essence of our approach to VQ design is to scale X , and hence the important ellipse, so that there are approximately (but no more than) 2 R L lattice points near or on the ellipse surface. The lattice points are then selected as the codewords for the VQ. The practical difficulties with the approach involve the counting and indexing of the large number of codewords and the actual encoding procedure for quantization. The lattice structure greatly facilitates this latter task. Only the cubic lattice will be used in the VQ construction that follows, but the approach may be readily extended to other lattices.</p><p>Assume there exists a suitable scalar, a = = , such that, with 2 = ax, the ellipse (11) becomes</p><p>where w, =l/alu:, i =1; . ., L , and K = a 2 L are all integers. If some of the U : are irrational, this may not be possible but can be reasonably approximated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp; ( L , ~, K ) = ~. E S ~( L , W , K )</head><p>For a specified weighting vector w, NE(L, w, K ) can be computed, and an algorithm to do this is provided in the Appendix. In the special case that w, 7 1 for i = 1,. . , L , the ellipse simplifies to a sphere and N E ( L , w, K ) is given by the coefficients of the theta functjon <ref type="bibr">[31]</ref>.</p><p>If X has Gaussian components, SE( L , w, K ) is congruent to the contour region (7), and, at least for large rates and dimensions, the codewords should be distributed roughly unif_ormly on the surface of the ellipse. Unfortunately, the N E ( L , w, K ) cubic lattice points on the ellipse are generally not distributed uniformly. To overcome this difficulty partially, we may simply choose several concentric ellipsoids, say SE( L , w, K + j ) , for j = -J1, -J1 + 1; . -, J2, where integers J1, J2 2 0 and K -J1 2 0, and the corresponding</p><formula xml:id="formula_6">J2 N ~( L , ~, K , J ~, J , ) A C N E ( L , w , K + j )</formula><p>points are selected as codewords. Although many of these codewords fail to lie precisely on SE( L , w, K ) , for large L they are of negligible per dimension distance (for J1 and J2 of reasonable size) from the ellipse. Generally, J1 and J2 should be selected as small as possible, but so that the codewords are approximately uniform around gE( L , w, K ) .</p><p>To encode at rate R bits/dimension, a value of a should be selected so that NE( L , w, K , J1, J2) is as large as possible but does not exceed 2RL. The resulting NE( L , w, K , J1, J 2 ) cubic lattice points are the codewords of an elliptical VQ, with specific code parameters L, w, K , J1 and J2. An algorithm for elliptical VQ encoding is as follows.</p><p>J = -J 1 lattice point, however, may not be a codeword. Define L Iljll',+ c w,?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>r = l</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K -J l &lt; l l j 1 1 t , I K + J 2 , (14)</head><p>then j is a codeword and 3 = jj. Otherwise, j is formed by adjusting the components of jj in such a way that ( <ref type="formula">14</ref>) is satisfied and as little increase in mse as possible is realized. Such a search for an optimum 3 may be accomplished by dynamic programming, or a suboptimum search may be employed. A dynamic programming encoding algorithm is outlined in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weighted Pyramid V Q s</head><p>Analogous to the elliptical VQ development, we use (in the KL transform domain) the weighted pyramid Swp( L , A d ) as a geometry for vector quantizer _design. Assuming there exists a suitable scalar, a = ala2, X = ax, and the weighted pyramid becomes <ref type="figure">s',</ref><ref type="figure">,</ref><ref type="figure">(L,</ref><ref type="figure">W ,</ref><ref type="figure">K</ref> </p><formula xml:id="formula_7">) = 2 : wr12,1 = K j (15) i .I,</formula><p>where w, = AI/al, i =1; . ., L , and K = a 2 L are all positive integers. As for the ellipse, if some of the A, are irrational this may not be possible, but it cap be reasonably approximated. The weighted pyramid Swp( L , w, K ) has a number I ? , , , , ( L , w,, K ) of cubic lattice points that lie on its surface. Define NWp( L, w, K ) explicitly as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I .</head><p>number of vectors 2 such that I and 2, = integer for i = 1 , -. . , L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N,,( L , W , K ) = 2 E s, ( L , W , K )</head><p>For a specified weighting vector w, n ? , ( ; , w, K ) can be computed. An algorithm for computing Nwp( L , w, K ) is provided in the Appendix.</p><p>The distribution of lattice points on gWp( L , w, K ) may be somewhat nonuniform, so to design the VQ we may choose several concentric weighted pyramids, say SWp( L , w, K + j ) , for j = -J1, -J1 + 1; . -, J2, where the integers J,, J2 2 0 and K -J1 2 0, and the corresponding J, 1) Given (KL transformed) vector X , form J?E <ref type="figure">N ,</ref><ref type="figure">,</ref><ref type="figure">( L ,</ref><ref type="figure">w ,</ref><ref type="figure">K ,</ref><ref type="figure">J1,</ref><ref type="figure">J,</ref><ref type="figure">) A c N w p ( L ,</ref><ref type="figure">w ,</ref><ref type="figure">K +</ref>  <ref type="figure">S ,</ref><ref type="figure">( L ,</ref><ref type="figure">A d</ref> ) as a closest (in the mse sense) vector to x.</p><formula xml:id="formula_8">j ) j = -Jl</formula><p>2) Scale 2 by a to form 2 = a i E SE( L , w, K ) .</p><p>3 ) Quantize 2 as 3, where 3 is one of <ref type="figure">the NE( L ,</ref><ref type="figure">w,</ref><ref type="figure">K ,</ref><ref type="figure">J1,</ref><ref type="figure">J</ref> 2 ) lattice point codewords. 4) The VQ output is y = $/a. points are selected as codewords. To encode at rate R bits/dimension, a value of a should be chosen so that <ref type="figure">Nwp( L ,</ref><ref type="figure">w,</ref><ref type="figure">K ,</ref><ref type="figure">J1,</ref><ref type="figure">J</ref> 2 ) is as large as possible but does not exceed 2RL. The resulting Nwp( L , w, K , J1, J2) cubic lattice points are the codewords of a weighted pyramid VQ, with specific code parameters L , w, K , J1, and J2. An algorithm for weighted pyramid VQ encoding is as follows.</p><p>Step 4) in the algorithm is appropriate for large rate, but for small rates the mse may be reduced by scaling 3 by a -.</p><p>.</p><p>factor slightly smaller than-l/a (e.g., <ref type="bibr">[14], [24]</ref>). 1) Given (KL transformed) vector X , form J ? E S,( L , A ) as a closest (in the mse sense) vector to x.</p><p>Scale J? by a to form X = aJ? E gw,( L , w, K ) .</p><p>Step 3) in the algorithm may be-accomplished by first requires the rounding of each vector component. This finding the closest lattice point to X , say jj, which simply 2)</p><p>3) Quantize 2 as j , where j is one of the 4)</p><p>Step 1) in the algorithm requires straightforward application of the projection theorem. Steps 3) and 4) are analogous to the elliptical VQ encoding. For low encoding rates, it is useful to threshold the source vector prior to encoding, as discussed in <ref type="bibr">[24]</ref>. A dynamic programming encoding algorithm is outlined in the Appendix.</p><p>For either the elliptical or weighted pyramid VQ's to be of practical value, there must be a simple procedure for representing the lattice codeword ( j in step 3) of each respective algorithm as a binary sequence of RL bits. For small rate-dimension products, such an enumeration may be trivially accomplished by a look-up table. For large rate-dimension products, an enumeration encoding algorithm is easily derived by generalizing the method in <ref type="bibr">[24]</ref>.</p><p>N,, ( L , w, K , J1, J 2 ) lattice point codewords.</p><p>The VQ output is y = $/a, C. MSE Performance subject to ( <ref type="formula">19</ref>) has a similar solution:</p><p>Since the weighted pyramid and the ellipse are the contour regions for the Laplacian and Gaussian sources, respectively, selection of cubic lattice point codewords uniformly on these surfaces yields a distortion asymptotically equivalent in rate and dimension to that of the optimum allocation in ( <ref type="formula">21</ref>) or (20). That is, from ( <ref type="formula">20</ref>) and ( <ref type="formula">21</ref>) we conclude that the elliptical and weighted pyramid VQ (EVQ and WPVQ) mse distortions are and For a memoryless zero-mean source, the pyramid VQ (PVQ) (with equal weighting in each dimension) mse dis-( <ref type="formula">23</ref>) tortion was derived as <ref type="bibr">[24]</ref> for suitably large rate and dimension. With equal weighting in each dimension, the ellipse becomes a sphere, and the spherical VQ (SVQ) distortion, assuming rectangular lattice points are used as codewords, is</p><formula xml:id="formula_9">ae DsvQ( R ) = -E [ X2]2-2R, 6</formula><p>with X a zero-mean source and for suitably large rate and dimension. Suppose now that L zero-mean sources with variances U :</p><p>and mean absolute values l / A i are to be encoded subject to the distortions of ( <ref type="formula">16</ref>) or (17). The optimum selection of rates R,; . ., R to minimize respectively, for large rate and dimension, and provided that the codeword distribution is roughly uniform on S,(L, A d ) and S,,(L,X). Equations ( <ref type="formula">22</ref>) and ( <ref type="formula">23</ref>) are valid for general sources but for Gaussian and Laplacian sources reflect all of the "entropy gain" that is available and hence are equivalent (for large rate) to the performance of optimum entropy coded scalar quantization <ref type="bibr">[3]</ref>. Further improvement to the elliptical and pyramid VQ's is possible by using a better lattice, but this improvement is limited to about 1.5 dB. If the VQ codewords are selected as points in the general lattice A, then ( <ref type="formula">22</ref>) and ( <ref type="formula">23</ref> Consider the discrete cosine transform (DCT) encoding of speech or imagery. The transform coefficients are often modeled as Laplacian, except for the "dc" coefficient of imagery, which is generally modeled as Gaussian <ref type="bibr">[26]</ref>, <ref type="bibr">[27]</ref>. The expected performance of elliptical and weighted pyramid VQ encoding of speech or imagery can be evaluated by estimating the variance and mean absolute value of the respective transform coefficients and using the performance expressions of ( <ref type="formula">22</ref>) and ( <ref type="formula">23</ref>). Consider the transform coding structure of Fig. <ref type="figure">1</ref>, with I, <ref type="bibr">(E,lx,l)</ref>  --2-2Rj 14 -12 .</p><p>10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">.</head><p>6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">.</head><p>Fig (Such a source model is commonly used for the transform coefficients of speech or imagery.) For purposes of the example, the variances are set at U: = l/i2, i = 1; . ., 8. Both optimum scalar quantizer encoding and weighted pyramid encoding of the source are considered.</p><p>In scalar quantization, each component of X ( n ) is quantized with an optimum scalar Laplacian quantizer [32], with the quantizer bit rates adjusted to minimize the average mean-square error. If fractional rates were possible, the optimum rates would be given by the well-known formula</p><formula xml:id="formula_10">[26] Example 2</formula><p>where R is the average encoding rate (in bits per dimension) and assuming that all the resulting rates satisfy R , 2 0. Suitable rounding of these R , yields the optimum Consider the mse encoding of an eight-dimensional vector source X T ( n ) = ( X 1 ( n ) ; . . <ref type="figure">,</ref><ref type="figure">X ,</ref><ref type="figure">( T Z</ref>  Flg. 2. G,,, G, versus DCT block length for two sentences of speech integral rate allocation for the eight scalar quantizers.</p><p>For WPVQ encoding we consider three cases: direct vector quantization of X ( n ) ; vector quantization of a pair of source vectors ( X ( n ) , X ( n + 1)); and vector quantization of a four-tuple of source vectors. The advantage of encoding the pair of source vectors (in 16 dimensions) or the four-tuple (in 32 dimensions) is simply the advantage of encoding large-dimensional vectors; that is, the localization near a single weighted pyramid of typical source realizations.</p><p>The (asymptotic) expression for normalized WPVQ distortion is Fig. <ref type="figure" target="#fig_4">4</ref> compares the distortion in ( <ref type="formula">24</ref>), the distortion computed by monte carlo simulations of WPVQ encoding (of 10 000 vectors), and the distortion of the optimum scalar quantization. The WPVQ provides an obvious improvement over the optimum scalar quantization, with 16dimensional encoding providing most of the improvement promised by ( <ref type="formula">24</ref>). We also note that the DwPvQ(R) curve is 0.255 bits away from the Shannon lower bound, reflecting the cubic lattice basis of the WPVQ. Further reduction in mse using the weighted pyramid approach is possible by using a better lattice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R ) =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>A geometric formulation for the design of source codes and vector quantizers has been presented, based on the asymptotic equipartition principle of information theory. For large enough block size, all but a negligible number of source vectors lie close to a particular contour of constant probability density, so that source coder design entails assigning representation regions to this important contour region. For a moderate number of dimensions, a "shell" or product code vector quantizer formulation is useful.</p><p>Two specific classes of vector quantizers were introduced: the elliptical VQ (motivated by the correlated Gaussian source), and the weighted pyramid VQ (motivated by the Laplacian source model for the transform coefficients of speech and imagery). Explicit elliptical and weighted pyramid VQ designs were only presented for the cubic lattice, but the general method can be extended to use other lattices. Elliptical and weighted pyramid VQ's are easy to design and have structured codebooks and an encoding complexity that grows only polynomially with the rate-dimension product. The parametric nature of these VQ's makes the development of adaptive geometric VQ's a natural topic for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author wishes to thank Professor J. D. Gibson for useful discussions regarding the concepts of differential entropy and entropy power, Professor M. Rahe for discussions regarding multidimensional geometry, and M. Blain for computing the values of G, and G,, used in Fig. <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>A . Cubic Luttice Points on the Weighted Pyramid.</p><p>Let N W p ( L , w, K ) , where wl ; . ., w,, and K are positive integers, be the number of cubic lattice points on S,, ( L , w , K ) , and denote by N w p ( L -i, w, K ) the number of lattice points of the form </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cubic Luttice Points on the Ellipse</head><p>Let N,(L, w, K ) , where w I ; . . , w I -, and K are positive integers, be the number of cubic lattice points on S,( L , w, K ) , and denote by N ( L -i, w, K ) the number of lattice points of the form ( O ; . . , ~, X , + ~; . . , X ~~) on S , ( L , w , K ) . Let n , = int[JK/w,] be the largest integer such that nfw, 5 K. Then following the argument that led to (Al), NI:( L, w , K ) satisfies the recursive form NJ: ( L , w, K ) = N,( L -1, w, K ) + 2 "1 ( L -1 , w , Kj Z w l ) J -1 <ref type="bibr">('4.2)</ref> with boundary conditions <ref type="figure">N,</ref><ref type="figure">(i,</ref><ref type="figure">w,</ref><ref type="figure">O</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weighted Pyramid and Elliptical Encoding</head><p>In what follows, a dynamic programming weighted pyramid VQ encoding algorithm is outlined. The algorithm is readily extended to elliptical VQ encoding.</p><p>Assume x E S,, ( L , w, K ) ; we seek the closest (cubic lattice point) codeword on s,,,( L , W , K -K~)us,,( L , ~, K -K , + I ) U . . . U S , , ( L , w , K + K Z ) .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. ., L, are the eigenvalues of A. If Y is jointly Gaussian with pdf then the contour region for the source is S,(L, A ) = y : --yTA-'y =1} (10) ( Z in the original coordinate system, or in the transformed coordinate system and is the surface of an L-dimensional ellipse. For large dimension, the source coding problem for a correlated Gaussian source then revolves around assigning 2 R representation regions (for rate per dimension R ) to surface SE( L , A,) (or SE( L, A)). A source code that assigns codewords to SE( L , A d ) is termed an elliptical code. Returning to the transform expression (8), let E[ I X,l] = I / A , , f o r i = l ; . . , L , a n d l e t XT=(Al,...,AL).Thenthe weighted pyramid is defined as 1 = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) are appropriately modified by simply scaling each expression by 12a, where a is the mse per quantization cell[16]  for the lattice.IV. EXAMPLESThis section contains two examples that demonstrate the performance of the elliptical and weighted pyramid VQ's. -known problem[26], with solution (for R large enough and min, U : &gt; 0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>R</head><label></label><figDesc>or 1, (Euclidean) normalization after the block transform. Define the elliptical gain G, and weighted pyramid gain G w p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 1. Block transform structure for computing statistics used in Example 1.as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. MSE versus rate for (24), simulation of WPVQ encoding. and optimum scalar quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>( O , ~~~, O , X , + ~, X , + ~, ~~~, X ~~) on S,,(L,w,K), i.e., the number of points that have x, = x 2 = . . . = x , =O. Let n, = int[K/w,] be the largest integer such that n,w, I K. For x, = 0 there are N ( L -1, w, K ) vectors x on S( L , w, K ) . If x1 = i 1, there are 2 N ( L -1, w, Kwl) possible x. Extending the argument yields N , , ( L w , K ) = N , , ( L -1 , w , K )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>of all-integer coordinate vectors (i.e., points in the cubic lattice) that lie on the ellipse. Define</figDesc><table><row><cell>iE( L , w, K ) explicitly as</cell><cell></cell></row><row><cell>number of vectors 2 such that I and 2,</cell><cell>1.</cell></row></table><note><p><p><p>The ellipse</p>SE( L , w, K ) is just a scaled_version of S E ( L , A d ) , and</p>since the w, are all integers, S( L , w, K ) has a number, say gE( L , w, K ) , = integer for i = 1; . , L .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>) ) ~, where X , ( n ) are t G W P , GE dB -G E , unnormalizrd -GwP. el norm -G W P , e, norm -. Gg, el iiorni G E , Lz norm</head><label></label><figDesc></figDesc><table><row><cell>I . .</cell><cell>.</cell><cell></cell><cell>*</cell></row><row><cell>4 8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DCT Blork Length</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The encoding procedure is first to find the closest cubic lattice point, say j , to input x, and then adjust j (if necessary) to find the closest codeword. The lattice point closest to x is trivially found by rounding, denoted as</p><p>then j is also a codeword, and the encoding is complete. If (A3) is not satisfied, then j must be adjusted in a minimum distortion manner.</p><p>Lemma: Let the lattice point closest to x E S,( L, w, K ) be</p><p>Proof: Clearly, Ix, -jr I I 1/2 for i = 1; . . , L, so and (A4) follows directly.</p><p>As a consequence of the lemma, let</p><p>A dynamic programming search for the codeword closest to x need only consider the range Kmin I I I J J ~~~~~, I Kmm.</p><p>The encoding may be accomplished by a constrained dynamic programming search that adjusts j to the closest codeword. Let n 2 0 denote the depth of the search (i.e., the number of integral adjustments to the components of j ) ; let j(n), with j ( 0 ) = j , denote the adjusted vector with n changes to components of j , and j = 11 j ( n) Ill. ", be the weighted I, norm for j ( n). Finally, let C( n, j ) be the minimum mean-squared error distortion at depth n and weighted norm j . This is initialized as C(0, j ) II xj ( 0 ) I I : ? for j = II j ( 0 ) Ill,"?. i 0 0 , otherwise We can view the pairs (n, j ) as nodes in a trellis and seek the minimum distortion path through the trellis that begins with C(0,II jlll,H,) and ends at C(n, j )</p><p>If M( n, j ; n + 1, j') denotes the smallest change in mse in the adjustment from j(n) (with llj(n)lll,w,=j) to j(n +1) (with</p><p>then for each j e [ K -K,, K + K2] at depth</p><p>Two observations simplify the task in (A5). First, if j ' &lt; K -K , ( j' &gt; K + K 2 ) , then all j satisfying J' &lt; J I K + K, ( j' &gt; j 2 K -K,) can be excluded from the minimization. Second, if C(n', j ) = C(n'-1, j ) , n'21, then j can be excluded from the J minimization in (AS) for all n 2 n'. The first observation simply implies that the order of adjustments of components of j has no effect on computing the closest codeword, so that only adjustments that drive Ilj(n)ll,,M, toward [K -K , , K + K,] are used.</p><p>The second observation simply states that if the best path ending in (n, j ) is through ( n -1, j ) , then the optimum path does not pass through ( n , j ) .</p><p>It is straightforward to write a computer program to accomplish weighted pyramid VQ encoding by using the dynamic programming search outlined above. The approach is readily extended to elliptical VQ encoding.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note>unpublished memorandum, Bell Laboratories, 1957 (see IEEE Truns</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quantizing for minimum distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1960-03">Mar. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimum quantizer performance for a class of non-Gaussian memoryless sources</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Modestino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="485" to="497" />
			<date type="published" when="1984-05">May 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vector quantization: A patternmatching technique for speech coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mug</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="1983-12">Dec. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mug</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asymptotic performance of block quantizers with difference distortion measures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="14" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vector quantizers and predictive quantizers for Gauss-Markov sources</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="1982-02">Feb. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vector quantization of speech and speech-like waveforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rebolledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Acoust., Speech, Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="1982-06">June 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vector predictive coding of speech at 16 kbits/s</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="685" to="696" />
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector quantization of memoryless Gaussian, gamma, and Laplacian sources</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dicharry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comrnun</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1065" to="1069" />
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rate-Distortion Theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A geometric treatment of the source encoding of a Gaussian random variable</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="481" to="486" />
			<date type="published" when="1968-05">May 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymptotically optimal block quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voronoi regions of lattices, second moments of polytopes, and quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A lower bound on the average error of vector quantizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="106" to="109" />
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lattice quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>to appear in Aduunces Electron. Electron Ph.vs</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast quantizing and decoding algorithms for lattice quantizers and codes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="227" to="232" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for uniform vector quantizer design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Inform. Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast encoding method for lattice codes and quantizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Inform. Theory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An algorithm for spherical codes and quantizers from the Barnes-Wall lattice in 16-dimensions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Adoul</surname></persName>
		</author>
		<idno>IT-30</idno>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>NOV</publisher>
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>It-29</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>NOV</publisher>
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Symp. Inform. Theory, Univ. of Michigan</title>
		<imprint>
			<date type="published" when="1986-10">Oct. 1986</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoding algorithm for spherical codes from the Leech lattice</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Adoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. Inform. Theory</title>
		<meeting><address><addrLine>Brighton, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A pyramid vector quantizer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theoty</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="568" to="583" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structured vector quantizers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Swaszek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Information Sciences and Systems Conf., Johns Hopkins Univ</title>
		<meeting>Information Sciences and Systems Conf., Johns Hopkins Univ<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-03">Mar. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Digital Coding of Waueforms: Principles and Applications to Speech and Video</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributions of the twodimensional DCT coefficients for images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Reininger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="835" to="839" />
			<date type="published" when="1983-06">June 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive quantization with one-word memory</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Jayant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1119" to="1144" />
			<date type="published" when="1973-09">Sept. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contour gain vector quantization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="862" to="870" />
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Product code vector quantizers for waveform and voice coding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acomt., Speech, Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="474" to="488" />
			<date type="published" when="1984-06">June 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quantizing characteristics for signals having Laplacian amplitude probability density function</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Geisler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="1978-09">May 1981. 1978. Sept. 1987</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Commun.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
