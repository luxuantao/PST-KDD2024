<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CARE: A Concurrency-Aware Enhanced Lightweight Cache Management Framework</title>
				<funder ref="#_q9X8AhD #_CM3wFTu #_cUZp42p">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rujia</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute Science Illinois Institute of Technology Chicago</orgName>
								<address>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Compute Science Illinois Institute of Technology Chicago</orgName>
								<address>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Compute Science Illinois Institute of Technology Chicago</orgName>
								<address>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CARE: A Concurrency-Aware Enhanced Lightweight Cache Management Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Improving cache performance is a lasting research topic. While utilizing data locality to enhance cache performance becomes more and more difficult, data access concurrency provides a new opportunity for cache performance optimization. In this work, we propose a novel concurrency-aware cache management framework that outperforms state-of-the-art locality-only cache management schemes. First, we investigate the merit of data access concurrency and pinpoint that reducing the miss rate may not necessarily lead to better overall performance. Next, we introduce the pure miss contribution (PMC) metric, a lightweight and versatile concurrency-aware indicator, to accurately measure the cost of each outstanding miss access by considering data concurrency. Then, we present CARE, a dynamic adjustable, concurrency-aware, low-overhead cache management framework with the help of the PMC metric. We evaluate CARE with extensive experiments across different application domains and show significant performance gains with the consideration of data concurrency. In a 4-core system, CARE improves IPC by 10.3% over LRU replacement. In 8 and 16-core systems where more concurrent data accesses exist, CARE outperforms LRU by 13.0% and 17.1%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Intensive research has been conducted to address the memory wall problem <ref type="bibr" target="#b49">[50]</ref>, of which improving locality and concurrency are two fundamental approaches. Cache hierarchies utilize data locality to minimize the long delay of off-chip main memory accesses. Significant research focuses on taking advantage of data locality, resulting in many schemes that detect memory access patterns, so that cache eviction and insertion decisions can be determined by the reference predictions to reduce cache miss rate <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Although such locality-based cache management frameworks may reduce the number of misses, we find that considering both locality and concurrency can further improve the state-of-the-art locality-only optimizations.</p><p>Modern high-performance processors support data access concurrency <ref type="bibr" target="#b11">[12]</ref> with advanced caching techniques such as multi-port, multi-bank, pipelined, and non-blocking cache. As a result, multiple outstanding cache accesses can be generated by one processor and overlapped with each other. With data access concurrency in the memory hierarchy, the cost of a miss could vary. Some misses are isolated, some misses occur concurrently with other hits, and some misses overlap with other miss accesses <ref type="bibr" target="#b28">[29]</ref>. The performance loss resulting from a cache miss can be hidden by access overlapping. Thus, a more accurate cost metric for cache misses may help improve cache performance further when data concurrency and overlapping exist <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p><p>In this work, we first introduce and formally define the concept of Pure Miss Contribution (PMC). PMC is a new cost metric for cache misses, with a comprehensive analysis of both hit-miss and miss-miss overlapping in the memory system. PMC has high predictability and versatility. We observe that the PMC values of the misses caused by the same program counter (PC) are relatively stable; therefore, the past PMC value can be used to predict the future PMC value of the same load instruction. PMC is also lightweight to measure and versatile enough to be used to build concurrency-aware cache management frameworks. We then present CARE, a concurrency-aware cache management framework that takes both data locality and concurrency into account. CARE learns the re-reference behavior and PMC value of each miss access to guide future replacement decisions. CARE augments existing cache insertion and hit-promotion policies to reserve a small subset of performance-critical blocks with high locality and high PMC, and evict dead blocks or blocks with low PMC. CARE is also prefetch-aware, and it performs well under prefetchers. In CARE, we also implement a Dynamic Threshold Reconfiguration Mechanism (DTRM), which enables CARE to better adapt to different applications and execution phases. Our experimental results show that CARE outperforms state-of-theart cache management schemes. Furthermore, CARE has low overhead and can be practically implemented in hardware. To summarize, this paper makes the following contributions: 1) We introduce the pure miss contribution (PMC), a novel and accurate metric to quantify the cost and performance impact of outstanding cache misses. We describe how PMC can be measured in modern cache hierarchies. We find that PMC is predictable and can be used for cache optimization.</p><p>2) We present CARE, a comprehensive cache management framework that considers both locality and concurrency. CARE is general for all types of applications, practical with low hardware implementation overhead, and adaptive with a novel Dynamic Threshold Reconfiguration Mechanism (DTRM).</p><p>3) Our evaluations show that CARE substantially improves upon existing state-of-the-art cache management schemes over a wide variety of workloads in a wide range of system configurations and performs well with data prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Memory Level Parallelism</head><p>Multi-core and multi-threading designs, as well as advanced caching techniques <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, increase data access concurrency. As a result, a number of memory accesses can concurrently coexist in the memory hierarchy. In this case, some memory accesses may overlap with others, which reduces their performance impact on cores.</p><p>Memory Level Parallelism (MLP) can be used to measure miss concurrency. MLP captures the number of outstanding cache misses that can be generated and executed in an overlapped manner <ref type="bibr" target="#b15">[16]</ref>. Some misses are isolated, while some occur concurrently with other misses. The more cache misses occur concurrently, the smaller the impact of each cache miss on performance since all concurrent misses will amortize the total memory stall cycles. Therefore, based on the MLP concept, isolated misses are considered to hurt performance more than concurrent misses. MLP can be measured with MLP-based cost <ref type="bibr" target="#b33">[34]</ref>. The MLP-based cost of an isolated miss can be approximated by the number of miss-access cycles that the miss spends. For concurrent misses, the data access delay is divided equally among all concurrent outstanding misses, representing the MLP-based cost of each concurrent miss access.</p><p>MLP-based cost can identify costly misses by considering the miss-miss overlapping. However, we find that hit-miss overlapping impacts the cost of misses as well, and modern memory systems have a lot of such overlapping accesses (details in Section III-B). Therefore, we need a holistic metric that is able to catch all types of overlapping and provide a better memory performance optimization guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Concurrent Memory Access Model</head><p>To capture all types of concurrent memory accesses and quantify their impact on performance, a concurrent memory access model named C-AMAT was proposed <ref type="bibr" target="#b43">[44]</ref>. In the C-AMAT performance model, a cache access latency is composed of two parts: 1) base access cycles, which are the minimum time an access (hit or miss) needs to spend on a specific cache level; 2) miss access cycles, which are the additional time spent waiting for data in the next levels of the memory hierarchy. For a miss access, the tag lookup time is considered to be the base access cycles of the access. A miss access latency consists of both base access cycles and miss access cycles. Figure <ref type="figure" target="#fig_0">1</ref> shows several concurrent cache accesses. Each access spends two base access cycles, and each miss access consumes three additional miss access cycles.</p><p>Based on the C-AMAT model, the miss access cycles can be hidden when there is a hit-miss overlapping. 1 Therefore the 1 It refers to miss access cycles overlapped with the base access cycles of a hit/miss access. actual cost of the misses should be revisited. On the other side, for a miss access cycle that does not overlap with any base access cycle, we refer to this cycle as a pure miss cycle <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Based on the C-AMAT model, the memory active cycles on a memory layer are the cycles with memory activities <ref type="bibr" target="#b27">[28]</ref>. Active miss cycles are classified into two categories: active pure miss cycles and active non-pure miss cycles. Active pure miss cycles are the cycles that only contain the pure miss cycles (cycles 5 to 7 in Figure <ref type="figure" target="#fig_0">1</ref>), and these cycles cause more performance degradation. On the other hand, the active non-pure miss cycles do not introduce heavy degradation, as the miss access cycle is overlapped and hidden (cycle 4 in Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>The C-AMAT concurrent memory access model is general and can be applied to each level of the memory hierarchy. In multi-core systems, the model works by tracking the overlapping from each core. In other words, the pure miss in a multi-core system contains at least one miss access cycle without any overlapped base access cycles from the same core that overlaps with. We find that the concepts in C-AMAT can capture all types of memory access overlapping. If we can quantify the cost of memory misses with all types of overlapping, we can use the metric to enable cache optimization further. In this work, we present pure miss contribution metric (details in Section IV), which is inspired by C-AMAT model, and it shows the great potential to be incorporated with cache optimization frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Locality-based Cache Management</head><p>Locality-based cache management schemes are designed to increase performance by reducing the total number of misses. The ideal upper bound for such schemes is Belady's optimal replacement (OPT) <ref type="bibr" target="#b9">[10]</ref>, which always evicts the block with the largest future usage distance. Recent locality-based cache management studies have focused on exploring predictionbased schemes to reduce the number of cache misses <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Re-reference prediction. Several replacement studies are designed based on the re-reference prediction of cache blocks, determining the lifetime of the blocks in the cache. SRRIP <ref type="bibr" target="#b18">[19]</ref> statically predicts an "intermediate" re-reference interval at cache insertion time and updates the re-reference prediction on subsequent accesses. DRRIP <ref type="bibr" target="#b18">[19]</ref> is proposed to improve performance by selecting the inserting position among different policies. Recent studies exploit long-term information to increase prediction accuracy by analyzing the cache blocks that have been evicted. SHiP <ref type="bibr" target="#b47">[48]</ref> and SHiP++ <ref type="bibr" target="#b52">[53]</ref> provide a finer granularity re-reference prediction by correlating the rereference behavior of cache blocks to the PCs and learning the past behavior of SRRIP. SHiP uses a history table (SHCT) to learn the re-reference characteristic for each signature. SHCT updates on cache hits and block evictions. The re-reference characteristic of each incoming block is predicted by indexing it into the SHCT. SHiP++ enhances SHiP re-reference predictions and SHCT training, further improving the last-level cache hit rate on SHiP. Jain and Lin propose Hawkeye <ref type="bibr" target="#b16">[17]</ref>. Hawkeye simulates and learns Belady's optimal solution for a long history of memory accesses to predict the re-reference characteristic of future accesses. Hawkeye formulates the re-reference prediction as a binary prediction problem. If the incoming block is predicted to be "cache-friendly", it will be inserted with a high priority. Otherwise, "cache-averse" blocks will be marked as eviction candidates. Following in the footsteps of Hawkeye, Mockingjay <ref type="bibr" target="#b40">[41]</ref> mimics Belady's optimal solution effectively and introduces a cache replacement policy based on multi-class re-reference prediction. Machine learning for re-reference prediction. In recent years, machine learning is also widely used to increase the effectiveness of cache management. Teran et al. <ref type="bibr" target="#b44">[45]</ref> propose using perceptron learning for re-reference prediction. Perceptron learning can find independent correlations between multiple input features related to block re-reference, guaranteeing accurate re-reference prediction. Glider <ref type="bibr" target="#b41">[42]</ref> uses an offline attention-based long short-term memory (LSTM) model to improve prediction accuracy and gain insights. Then these insights are fed into an Integer Support Vector Machine (ISVM) that matches the LSTM's prediction accuracy. While machine learning models such as perceptron and ISVM can be trained online, this requires the involvement of a large number of prediction tables, which imposes a non-negligible overhead, especially in multi-core systems. Sethumurugan et al. <ref type="bibr" target="#b39">[40]</ref> use reinforcement learning to learn a cache replacement policy. Based on the insights derived from the neural network, a costeffective cache replacement policy RLR is proposed.</p><p>The overhead of machine learning-based techniques is difficult to justify, including the training overhead, computation cost, and model size. Our design focuses on efficiency and lightweight, which is more practical to be implemented on latency-critical cache hierarchy without any pre-processing overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cost-based Cache Management</head><p>Unlike locality-based cache management schemes that focus on reducing cache misses, several works improve cache performance by selectively eliminating expensive misses. LACS <ref type="bibr" target="#b21">[22]</ref> is proposed based on the observation that the more instructions the processor issues during the miss, the more likely it is to hide the penalty for that miss. Consequently, LACS utilizes the number of instructions issued during an LLC miss to estimate the miss cost. While simple, the cost estimation model of LACS is not cycle-accurate, it is impossible to estimate the penalty of the misses on performance accurately. The MLP-aware cache replacement policy SBAR <ref type="bibr" target="#b33">[34]</ref> takes into account the concurrency of the cache misses and observes that some misses occur alone while some occur concurrently with others. It improves performance by reducing the number of costly isolated misses. However, as we discussed in Section II-A, MLP does not consider hit-miss overlapping. Therefore, the MLP-aware replacement policy can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION A. The Limitations of Locality-based Cache Management</head><p>Modern mainstream processors contain many cores and run different applications concurrently. Therefore, the shared LLC can observe very mixed access patterns. The mixed access patterns can downgrade the effectiveness of locality-based schemes since most of them make predictions only dependent on one specific access pattern. In addition, at the LLC, the recency-friendly access patterns get filtered by upper-level caches, which makes it even harder to directly get benefits from locality-based schemes <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. On the other hand, locality-based schemes all have a simple optimization goal: reducing the number of misses. It works fine for sequential memory accesses but could be better for handling prevailing concurrent cache/memory activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Limitations of MLP-based Cache Management</head><p>In a scalable system with memory access concurrency, not all cache misses will have the same impact on performance <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Eliminating isolated misses (high-cost) helps performance more than eliminating concurrent misses (low-cost). According to the above assumption, an MLP-aware cache replacement algorithm <ref type="bibr" target="#b33">[34]</ref> was proposed to reduce the number of high-cost misses.</p><p>We introduce the definition of MLP-based cost in Section II-A. We show how MLP-based cost is calculated in a singlecore system using the study case in Figure <ref type="figure" target="#fig_1">2</ref>. The case in a multi-core system is similar since we only analyze the memory concurrency coming from each core independently. Here, B, F are hits; A, C, D, and E are misses. Each access consumes two base access cycles, and each miss access has six additional miss access cycles. All the accesses are at the same cache level in the memory hierarchy.</p><p>When considering the definition of MLP-based cost, access A is a miss with the highest MLP-based cost. Because there are no miss access cycles from other misses that overlap with A's miss access cycles from cycle 3 to cycle 6, and the miss access TABLE I: MLP-based cost analysis of the study case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss MLP-based cost</head><formula xml:id="formula_0">A 1 + 1 + 1 + 1 + 1/2 + 1/2 = 5 C 1/2 + 1/2 + 1/3 + 1/3 + 1/3 + 1/3 = 7/3 D 1/3 + 1/3 + 1/3 + 1/3 + 1/2 + 1/2 = 7/3 E 1/3 + 1/3 + 1/3 + 1/3 + 1/2 + 1/2 = 7/3</formula><p>cycles of access A only overlap with the miss access cycles of access C in cycle 7 and cycle 8. Therefore, the MLP-cost of access A is 5. From cycle 7 to cycle 8, the miss access cycles of access C overlap with the miss access cycles of access A, so the MLP-based cost of access C in these two cycles is 1 (1/2 ? 2). In addition, the miss access cycles of C still overlap with D's and E's miss access cycles from cycle 9 to cycle 12. Therefore, the MLP-based cost of access C in these four cycles is 4/3 (1/3 ? 4). To sum up, the MLP-based cost of access C is 7/3. D and E have a similar situation to C, and their MLP-based costs are also 7/3. The quantitative MLP-based cost analysis is summarized in Table <ref type="table">I</ref>.</p><p>If we re-evaluate the misses in this case study with the C-AMAT-based model, only access C, D, and E are pure misses. Access C has three pure miss cycles (cycles 10-12), and both access D and E have five pure miss cycles (cycles 10-14). Even though the MLP-based cost of access A is the highest, it does not hurt the performance the most. All of the miss access cycles of A are overlapped with base access cycles of B, C, D, E, and F. Furthermore, although the MLP-based costs of access C, D, and E are the same, they have different pure miss cycles and have various contributions to the total active pure miss cycles. This is because that MLP-based cost calculates the cost by analyzing the miss-miss overlapping but does not consider hit-miss overlapping.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates the percentage of LLC misses that have hit-miss overlapping from the same core for a set of 4-core multi-copy workloads with LRU policy (Section VI details the methodology). In all benchmarks, 30% to 80% misses have hit-miss overlapping. Therefore, in order to accurately quantify the cost of cache misses, hit-miss overlapping cannot be ignored. Although LLC pure misses do not directly cause CPU stalls, LLC pure misses can seriously increase the latency of providing data to the upper-level caches. In this work, we develop a concurrency-aware enhanced cache management framework for LLC to eliminate the costly pure misses and improve the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PURE MISS CONTRIBUTION A. Definition</head><p>In this section, we introduce Pure Miss Contribution (PMC), which is a new metric that considers the integrated influence of locality, concurrency, and overlapping of memory accesses. PMC recognizes that not all outstanding cache misses have the same cost, and identifies high-cost cache misses to better performance optimization.</p><p>PMC is defined as the contribution of each miss to the total active pure miss cycles from the same core. Consequently, PMC can be used to quantify the performance impact of each cache miss. A pure miss access has at least one pure miss cycle, which can significantly hurt the performance. Therefore, a pure miss contributes to active pure miss cycles with a positive PMC value. On the contrary, if a cache miss is not a pure miss and all its miss access cycles are overlapped with other base access cycles, its PMC value is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Measurement and Implementation</head><p>The algorithm to detect and measure PMC in a multi-core system is described in Algorithm 1. It can be applied at each level of a memory hierarchy. In particular, we use a parameter l to indicate the specific cache level under consideration. We declare the bit and field used for PMC calculation on top of Algorithm 1. The NoNewAccess x bit is used to identify the current cycle status of cache level l for core x. If there is a new cache access from core x at cache level l, NoNewAccess x is reset to 0 for base access cycles. Therefore, when NoNewAccess x is 1, it means that there are no base access cycles in any type of accesses that can be used to hide miss access cycles.</p><p>Modern memory systems manage data in cache blocks and utilize the Miss Status Holding Register (MSHR) <ref type="bibr" target="#b45">[46]</ref> to handle concurrent cache misses. An MSHR entry is allocated for a miss access when the tag search fails to match anything, which is during the base access cycles of a miss. MSHR can track the information of all outstanding cache misses. A new field PMC is added for each MSHR entry to calculate PMC.</p><p>When a miss access allocates an MSHR entry, the PMC counter associated with the miss will be initialized to zero. NoNewAccess x bit is used to determine whether this cycle is an active pure miss cycle for core x. If NoNewAccess x is set, all outstanding misses from core x in the MSHR are pure misses that contribute to core x's active pure miss cycles. Therefore, we only calculate and update the PMC of each cache block in active pure miss cycles. For concurrent pure misses, the active pure miss cycle can be evenly divided among all Algorithm 1 Measure PMC for cache misses at cache level l (called every active memory cycle) NoNewAccess x: single-bit cycle status identifier per core; set if no overlapping opportunites in this cycle; PMC: field in MSHR to calculate the pure miss contribution for a miss block; initialized to 0; update(N x ): check the number of outstanding misses from core x at cache level l in this cycle. concurrent pure misses. Let N x be the number of outstanding misses from core x at cache level l in the corresponding MSHR.</p><p>For every active pure miss cycle, the PMC counter of each outstanding miss in MSHR is incremented by 1/N x until the requested data is serviced. Please note that PMC measurement in a single-core system is a special case of multi-core (the number of cores is 1). Figure <ref type="figure">4</ref> illustrates the PMC measurement logic (PML), which follows Algorithm 1 to record PMC values of outstanding misses. The Access Detector (AD) can detect base access cycles and notify the Pure Miss Detector (PMD) whether the current cycle is an active pure miss cycle. The base access cycles are known and fixed for any given cache level. Therefore, the AD monitors for a fixed amount of cycles and sets the NoNewAccess bit accordingly.</p><p>Using the information collected from the AD and the miss information from the MSHR, PMD can identify the pure miss accesses. PMC Calculation Unit (PCU) updates the PMC value of each outstanding miss in each active pure miss cycle. The PCU implements the divider through a lookup table, which is fast in performance and cheap in hardware cost. Since the number of MSHR entries is limited (e.g., 64), N x is an integer ranging from 1 to 64. Therefore, we can store all possible values of 1/N x in the lookup table in advance for fast access. When a miss is served, the PMC value of the miss can be converted into a quantized integer value and stored in the tag-store entry of the corresponding cache block to guide the concurrency-aware cache management framework (Section V-F). PMC measurement in parallel multi-thread execution. In a multi-core execution, a private instance of PML is present on each core. If a core runs multiple threads, memory access from any thread contributes to the memory active cycles of that core. PMC evaluates the contribution of each cache miss to the total active pure miss cycles from the same core. Therefore, in a multi-threaded execution, the PMC value of each outstanding miss can be calculated on a per-core basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Revisit the Study Case with PMC Analysis</head><p>Recalling the case study in Section III-B, we re-evaluate the impact of each cache miss on performance according to the definition of PMC. When considering access concurrency and hit-miss overlapping, although access A has the highest MLPbased cost, it does not contribute to any active pure miss cycle. Therefore, the value of PMC for access A is 0. Access C has three pure miss cycles (cycle 10-12); they overlap with D's and E's pure miss cycles from cycle 10 to cycle 12. Therefore, the PMC of access C in these three cycles is 1 (1/3 ? 3). Access D and E have the same situation, and we take access D as an example for analysis. Access D has five pure miss cycles (cycle 10-14), which overlap with the pure miss cycles of C from cycle 10 to cycle 12 and overlap with E's pure miss cycles from cycle 10 to cycle 14. So access D has a PMC value of 2. Although access C, D, and E have the same MLP-based cost, access D and E contribute the most to active pure miss cycles and cause the most damage to performance.</p><p>In this case study, the sum of the PMC values of all cache misses is 5, which equals the number of active pure miss cycles. Table <ref type="table" target="#tab_1">II</ref> summarizes the value of PMC and MLP-based cost for each cache miss. Compared to MLP-based cost, PMC not only considers the miss-miss overlapping but also captures the hit-miss overlapping. Therefore, PMC can better reflect the impact of each cache miss on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Distribution of PMC</head><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the distribution of PMC for 16 workloads from SPEC CPU2006 <ref type="bibr" target="#b42">[43]</ref> and 2017 <ref type="bibr" target="#b5">[6]</ref> benchmark suite. The results are measured with a single-core configuration. Details about the simulation environment are described in Section VI. LRU replacement policy is used in the LLC by default. The y-axis represents the percentage of total LLC misses, and the x-axis represents several bins for different PMC values.  The distribution of PMC for different benchmarks clearly shows that each cache miss has a diverse impact on performance. We can utilize PMC to grade cache misses and optimize the performance by reducing the number of pure misses with high PMC values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Predictability of PMC</head><p>PC has been used successfully in predicting the reuse behavior of cache blocks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b52">[53]</ref>. We also find that the PMC value of a cache block is highly correlated with the PC of the instruction that caused the miss, which means that PMC has high predictability. Here, what predictability refers to is that the PMC value is relatively stable for all accesses for the same PC. We use PMC ? to represent the absolute difference in PMC values between two consecutive cache misses for the same PC. Table <ref type="table" target="#tab_2">III</ref> shows the distribution and median of PMC ? for different workloads in a single-core configuration. These statistics come from our offline profiling of all cache misses in each workload.</p><p>For all workloads, the majority of PMC ? values are less than 50 cycles. The median PMC ? of each workload is also relatively low, which indicates the PMC values of a PC are almost consistent and repetitive across consecutive misses. We also observed the same trend in the multi-core configurations, as the PMC value is calculated on each core individually. The predictability of PMC provides a basis for predicting the future PMC values of the same PC. Inspired by SHiP++ <ref type="bibr" target="#b52">[53]</ref>, we design CARE as a representative use case to utilize the PMC metric. CARE is a management framework that enhances SHiP++ by enabling comprehensive concurrent access pattern analysis (more details in Section V).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CARE: CONCURRENCY-AWARE CACHE MANAGEMENT</head><p>We introduce CARE, an LLC management framework that considers both data locality and data concurrency. The primary goal of CARE is to improve locality while utilizing concurrency to reduce the overall performance penalty of cache misses. CARE learns the re-reference characteristics and PMC values of the cache blocks by associating each cache reference with a PC-based signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Figure <ref type="figure" target="#fig_5">6</ref> shows a high-level overview of CARE. Signature History Table (SHT) is designed to keep track of the observed re-reference and PMC behaviors of LLC blocks by associating them with PC signature. The purpose of Signature-Based Predictor (SBP) is to make re-reference and PMC predictions on cache insertions and cache hits.</p><p>For every new cache access, CARE first extracts the signature from the PC of the cache request. The signature of the access is used to index into the SHT. Each SHT entry tracks the cache accesses associated with a specific PC-based signature. SHT contains two counters for each signature: Re-reference Confidence (RC) and PMC Degree (PD). RC indicates the re-reference behavior for a signature. PD reflects the cost degree of the cache miss associated with this signature. The past behaviors of the PC can be used to predict the likely re-reference and PMC characteristics of the incoming blocks. Based on SHT, SBP predicts the behaviors of each cache block. SBP then determines the insertion policy for cache misses and the promotion policy for hit accesses. CARE updates and stores the SHT on cache evictions and cache hits. To avoid the perblock overhead, Section V-G illustrates the use of set sampling <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b47">[48]</ref> to learn the caching behavior and update the SHT with limited overhead.</p><p>CARE uses PML to compute PMC values for all outstanding misses. The PMC values are then quantized into a 2-bit Pure Miss Contribution States (PMCS) by a Dynamic Threshold Reconfiguration Mechanism (DTRM) and stored in the metadata. In order to learn the re-reference and PMC patterns of a signature, the signature (14-bit hash of PC <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b52">[53]</ref>), a single re-reference bit (R), and 2-bit PMCS are needed to be stored as metadata for each cache block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Store and Update Access History in SHT</head><p>Metadata bits in cache blocks. The single R bit is used to track the re-reference behavior of each cache block. For an incoming block, the R bit is initially set to 0. When a miss is served, CARE quantizes the value of PMC into a 2-bit PMCS based on the comparison results of PMC and two thresholds PMC low and PMC high. If the PMC value of a miss is smaller than PMC low, the PMCS of the corresponding cache block is 0. If the PMC value of a miss is larger than PMC high, the PMCS of the corresponding cache block is set to 3. If the value of PMC is between the two thresholds, the corresponding PMCS is 1. Therefore, when an incoming block is inserted in the cache, its PMCS bits are set to indicate the PMC value. SHT entry structures. The SHT has 16K entries, each containing a 3-bit RC counter and a 3-bit PD counter. CARE uses the signature of the cache access to look up the SHT. A zero RC value indicates the future blocks associated with this signature are rarely reused. A positive RC counter implies that the future blocks associated with this signature have data locality, and they are likely to receive cache hits. Similar to RC, a larger PD value means that the cache misses associated with that signature have a high probability of having large PMC values in the future. Update SHT on hit accesses. If the cache block receives a hit, the R bit of the block is set to 1. If this is the first re-reference of the block, CARE increments the RC counter (in a saturated manner), which corresponds to the signature of the block in the SHT <ref type="bibr" target="#b52">[53]</ref>. Update SHT on eviction. When a block is evicted from the cache, if the block has never been reused since it was inserted into the cache (R is unset), then the RC counter associated with the signature is decremented (in a saturated manner) <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b52">[53]</ref>. If the PMCS of this evicted block is 0, it means that there is a high probability that future misses caused by the same signature will hardly damage performance. Therefore, the related PD counter in the SHT is decremented (in a saturated manner).</p><p>On the other hand, if the PMCS of this evicted block is 3, which implies that the future misses associated with the same signature are predicted to be costly, the related PD counter is incremented (in a saturated manner).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Predict Access Behavior with SBP</head><p>For every cache access, SHT is indexed by a signature of the block. SBP identifies the re-reference behavior of cache blocks as High-Reuse if the associated RC counter is saturated at its maximum value. A cache block is predicted as Low-Reuse if the related RC counter is 0. All other cache accesses are classified as Moderate-Reuse.</p><p>Similarly, SBP utilizes the PD counter to predict the impact of each cache block on performance. Suppose the PD associated with the signature of the block saturates to the highest value. In that case, the cache block is predicted as High-Cost by SBP. Suppose the signature of a block has a PD value of 0. In that case, the associated cache block is predicted to be Low-Cost because the misses associated with this signature are considered less detrimental to future performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. CARE Cache Management Policies</head><p>By keeping track of the reuse information, CARE can still leverage data locality to keep High-Reuse data blocks and evict Low-Reuse blocks. Additionally, CARE takes data concurrency into account for the blocks predicted to be Moderate-Reuse. CARE selectively reduces expensive misses by keeping High-Cost blocks in the cache for a longer period while giving the higher eviction priority to the Low-Cost blocks. To do so, CARE implements cache management by associating each cache block with a 2-bit Eviction Priority Value (EPV). <ref type="foot" target="#foot_0">2</ref> The EPV counter of a cache block reflects the eviction priorities of the cache blocks. An EPV of zero implies that a cache block has the lowest eviction priority. An EPV of saturation implies that a cache block has a high eviction priority and will be evicted sooner. CARE assigns/updates the EPV counter for each block based on the prediction information from SBP. Table <ref type="table" target="#tab_4">IV</ref> shows the updated cache policies for CARE. Insertion policy. When inserting new blocks in the cache, unlike LRU, which inserts all cache blocks to the MRU position of the "LRU chain" <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b48">[49]</ref>, CARE dynamically learns the re-reference and PMC information of each specific signature. CARE inserts blocks with different EPV values according to the predictions provided by SBP.</p><p>Blocks that are predicted to be High-Reuse are assigned an EPV value of 0 with the lowest eviction priority. For the blocks tagged as Low-Reuse, the corresponding EPV is set to 3, and the eviction priority is the highest. Unlike other cache management frameworks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b52">[53]</ref> that insert all predicted Moderate-Reuse blocks into a certain position in the cache, CARE determines the EPV value of each Moderate-Reuse block based on the PMC prediction. The EPV value of the Low-Cost blocks is assigned as 3, and the EPV value of the High-Cost blocks is assigned as 0. If a Moderate-Reuse block is neither predicted as High-Cost nor Low-Cost, its EPV value will be set to 2. Hit-promotion policy. The EPV of a cache block will have an opportunity to be updated when the block is hit. The primary purpose of the hit-promotion policy is to preserve blocks with good locality for a long time. For a cache block that is predicted to be High-Reuse or Moderate-Reuse, the EPV drops to 0 when the block is hit to protect it from thrashing. The likelihood of further reuse is quite limited for an LLC hit to a block classified as Low-Reuse. Therefore, CARE gradually decrements its EPV value on every hit. Victim selection. On a cache miss, CARE selects a victim block whose EPV is 3. If there are multiple candidates, CARE randomly selects one victim block from the candidates. Another solution is to consider the recency information and evict the least recent block. Through testing, there is no discernible performance difference between the two solutions. However, recording the recency information for each cache block requires a huge hardware cost. Therefore, CARE selects the candidate randomly. If there is no block with an EPV of 3, the EPV of all blocks in the cache set will be incremented and the search will be repeated until the victim block is found. By increasing the EPV value, the dead blocks can be evicted eventually. Writeback-aware. Writeback requests are treated as nondemand background cache requests. Such requests are rarely re-referenced <ref type="bibr" target="#b52">[53]</ref>. In order to not compete with valuable cache resources, all writebacks are inserted in the cache with an EPV of 3. The hit-promotion policy also does not apply to the writebacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Collaboration with Prefetching</head><p>PMC measurement with prefetching. We extend the definition of pure miss when there is a prefetcher in the system. No matter whether a miss is a demand miss or a prefetch miss, it is a pure miss if it has at least one miss-access cycle that does not overlap with any hit access from the same core. Then, we are able to calculate PMC as usual when a prefetcher exists. CARE with prefetching. In the presence of prefetching, the caching behavior of demand accesses and prefetch accesses are completely different <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Therefore, it would be beneficial to independently predict the caching behavior of demand accesses and prefetch accesses. Inspired by SHiP++ <ref type="bibr" target="#b52">[53]</ref>, we distinguish prefetch accesses from demand accesses by appending 1-bit prefetch into the signature. As a result, CARE independently learns the caching behavior of load instructions that result in both demand and prefetch accesses.</p><p>In addition, CARE applies different hit-promotion policies for prefetch and demand requests on cache hits. We find a prefetched block is often only accessed once by its demand request. So, CARE sets the EPV to 3 when a prefetched block is re-referenced by a demand request. CARE updates the EPV to 0 to keep it in the cache for longer if subsequent demands or prefetch requests further access the prefetched blocked. CARE does not update the EPV of a prefetched block if it is subsequently re-referenced only by prefetch requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Dynamic Threshold Reconfiguration Mechanism</head><p>To improve the robustness of CARE, we propose a dynamic threshold reconfiguration (DTRM) scheme that quantizes PMC value into PMCS to suit different workloads. With the help of DTRM, CARE dynamically adapts two thresholds PMC low and PMC high to achieve the purpose of increasing adaptability. The initial PMC high and PMC low are set to 350 cycles and 50 cycles, respectively.</p><p>PMC high is used to distinguish the cache block that hurts the performance the most. On a cache miss, if its PMC is larger than PMC high, we consider the corresponding block to be a costly miss and set its PMCS to 3. A 32-bit counter TCM counts the total number of costly misses during the application execution time.</p><p>At the end of each period (16K misses, half the number of blocks in the LLC for single-core configuration), PMC low and PMC high are updated based on the number of costly misses found during the period. If the number of costly misses is smaller than 0.5% ? 16K, PMC low and PMC high are decreased by 10 and 70 cycles, respectively. On the other hand, if the number of costly misses is larger than 5% ? 16K, PMC low and PMC high are increased by 10 and 70 cycles, respectively. We choose the values empirically based on the results of a large number of simulations.</p><p>At the end of each period, the update of PMC low and PMC high drives the update of the quantization scheme between PMC and PMCS. The updated thresholds and quantization scheme are then used throughout the next period. DTRM can perfectly adapt to the mechanism of CARE. By cooperating with DTRM, the robustness and resiliency of CARE are further enhanced without any pre-processing or training overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Hardware Cost and Complexity of CARE</head><p>We analyze the hardware cost of CARE with 2MB LLC and 64B block size as an example setting. CARE requires monitoring PMC for each cache miss during the execution time. We have introduced the measurement and implementation for tracking PMC in Section IV-B. The only needed bit per core is NoNewAccess. To measure the PMC value of each miss, a lookup table is used instead of a costly divider. For an LLC with 64-entry MSHR, the cost of the lookup table is 0.25KB.  In addition, a 32-bit wide register per MSHR entry is sufficient to store the PMC value. Each block in LLC is equipped with a 2-bit EPV for cache management and a 1-bit prefetch for detecting prefetch access. In order for SHT to learn the re-reference and PMC patterns of the signature, a 14-bit signature, 1-bit R, and 2-bit PMCS are needed to store for each block. CARE adopts an online set sampling method <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b47">[48]</ref> to reduce the storage overhead. In our study, CARE monitors the cache behaviors from 64 sampled sets, then updates the SHT. Therefore, only the blocks in the sampled set need to store these 17 bits of metadata. For a 16-way cache, the total cost of the sampled sets is 2.125KB.</p><p>The detailed hardware cost is shown in Table <ref type="table" target="#tab_5">V</ref>. In total, the hardware overhead of CARE is around 26.64KB, which is only 1.3% of the capacity of a 2MB LLC. This cost scales linearly with the LLC capacity. We marked the additional cost, which is due to the fact that the CARE takes concurrency into account. CARE only needs 6.76KB to support concurrency awareness. Table VI compares the hardware costs of different cache management frameworks. Compared with the machine learning-based framework Glider, CARE requires much less hardware cost.</p><p>For multi-core processors, if the size of LLC is constant, we need to have a NoNewAccess bit for each core to detect the overlapping and measure the PMC. In addition, each block in the sampled sets needs to add a core tag to track which core the access comes from. VI. METHODOLOGY Simulated system. We evaluate CARE against prior cache management schemes using the version of ChampSim <ref type="bibr" target="#b3">[4]</ref> used for the 1st instruction prefetching competition (IPC-1 <ref type="bibr" target="#b0">[1]</ref>).   ChampSim is a cycle-accurate simulator which was also used for 3rd data prefetching championships (DPC-3 <ref type="bibr" target="#b2">[3]</ref>) and the 2nd cache replacement championship (CRC-2 <ref type="bibr" target="#b1">[2]</ref>). The details of the configuration parameters are described in Table <ref type="table" target="#tab_7">VII</ref>.</p><p>For multi-core configurations, we scale the size of the LLC in proportion to the number of cores. To evaluate the performance of CARE with prefetching, we follow the methodology of CRC-2 by applying the next-line prefetching policy at L1 and IP-stride prefetching policy at L2. Benchmarks and workloads. We evaluate CARE using a diverse set of memory-intensive workloads spanning SPEC CPU2006 <ref type="bibr" target="#b42">[43]</ref>, SPEC CPU2017 <ref type="bibr" target="#b5">[6]</ref>, and GAP <ref type="bibr" target="#b8">[9]</ref> benchmark suites, which have at least 1 LLC miss per kilo instructions (MPKI) in the single-core baseline system with no prefetching. Fig. <ref type="figure">8</ref>: LLC pMR for all 4-core multi-copy SPEC workloads (collaboration with L1 and L2 prefetcher).</p><p>from Pin to only profile the core algorithm (avoid intercepting traces when loading graph dataset). For all simulations, we warm up each core using 50M instructions from each workload, and then run simulation over the next 200M instructions. We use both multi-copy and mixed workloads to simulate a multi-programmed system. An n-core multi-copy workload has n identical copies of a memory-intensive trace, for all the cores. Note that each trace does not start exactly at the same time in the simulator, so the runs are not synchronized. For an n-core mixed workload, we select n benchmarks randomly from the 30 memory-intensive SPEC benchmarks and run one trace in each core. We generate 100 mixed workloads in total. For each mixed workload, if a benchmark finishes early, it is replayed until each benchmark has finished running 200M instructions. Our multi-core simulation methodology is similar to the methodologies used by CRC-2 <ref type="bibr" target="#b1">[2]</ref> and DPC-3 <ref type="bibr" target="#b2">[3]</ref>. Compared schemes. We select LRU as the baseline for performance comparison. For multi-core configurations, we report the weighted speedup over LRU, which is commonly used to evaluate shared caches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b40">[41]</ref>. We compare CARE against three state-of-the-art LLC management schemes: SHiP++ <ref type="bibr" target="#b52">[53]</ref>, Hawkeye <ref type="bibr" target="#b16">[17]</ref>, Glider <ref type="bibr" target="#b41">[42]</ref>, and Mockingjay <ref type="bibr" target="#b40">[41]</ref>. We also extend the MLP-based cost <ref type="bibr" target="#b33">[34]</ref> and implement a cache management framework called M-CARE for performance comparison. The workflow of M-CARE is similar to CARE. The only difference from CARE is that M-CARE does not consider PMC but uses MLP-based cost to analyze data access concurrency and guide cache management. The impact of analyzing hit-miss overlapping can be seen by comparing the performance of CARE and M-CARE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CARE Performance Evaluation</head><p>We show the performance results of CARE over state-ofthe-art schemes in 4-core configuration with prefetching in this section.</p><p>1) Multi-copy SPEC workloads: For multi-copy SPEC workloads, Figure <ref type="figure">7</ref> shows the normalized IPC of schemes followed by the geometric mean across all the SPEC workloads. Fig. <ref type="figure">9</ref>: Normalized IPC for all 4-core multi-copy GAP workloads (collaboration with L1 and L2 prefetcher).</p><p>All results are normalized to the baseline of LRU. In the presence of prefetching, CARE achieves a geometric mean speedup of 10.3% over the LRU on the 30 memory-intensive benchmarks, while SHiP++, Hawkeye, Glider, and M-CARE improve performance over LRU by 7.6%, 6.2%, 7.2%, and 7.5%, respectively. Figure <ref type="figure">7</ref> demonstrates that since CARE considers both locality and concurrency in cache management, CARE outperforms state-of-the-art schemes. LRU, SHiP++, Hawkeye, and Glider are the locality-based schemes. Although Hawkeye and Glider each perform well for some specific workloads, their performance improvements are not consistent across different benchmarks. Hawkeye yields performance below LRU baseline for 7 workloads. Glider performs worse than the LRU baseline for 5 workloads. This observation exhibits that the performance of the locality-based schemes is limited by access patterns. M-CARE considers the miss concurrency but ignores the hitmiss overlapping. Therefore, M-CARE makes the replacement decisions at a coarse granularity. The accurate analysis of data concurrency by PMC gives the performance of CARE an advantage in comparison to M-CARE.</p><p>Figure <ref type="figure">8</ref> compares the LLC pure miss rate (pMR) with different schemes for the 30 memory-intensive SPEC workloads. workloads with increasing concurrency. In 16-core systems, Figure <ref type="figure" target="#fig_2">13</ref> illustrates that CARE improves performance on SPEC workloads by 19.4%, compared with a 11.9% improvement by the second-best scheme (Mockingjay in this case). Figure <ref type="figure" target="#fig_3">14</ref> shows that in 16-core systems, CARE outperforms LRU, SHiP++, Hawkeye, Glider, Mockingjay, and M-CARE across GAP workloads by 13.0%, 5.1%, 9.1%, 8.5%, 8.1%, and 5.4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>In this paper, we emphasize the importance of data concurrency to memory performance. We propose Pure Miss Contribution (PMC), a comprehensive metric used to weigh the performance cost of each cache miss. We first develop a detailed measurement mechanism for PMC. Then, we utilize PMC to build CARE, a locality and concurrency-aware, lightweight cache management framework. CARE considers locality, concurrency, and overlapping to guide cache replacement decisions. CARE is fully tested and analyzed. It outperforms state-ofthe-art cache management schemes. Experimental and analysis results show CARE has a true potential for data-intensive scalable computing systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration of C-AMAT model and Pure Miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Study case of concurrent memory accesses from a single application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Percentage of misses with hit-miss overlapping.</figDesc><graphic url="image-1.png" coords="4,314.13,50.54,246.33,71.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :Fig. 4 :</head><label>14</label><figDesc>Fig. 4: PMC Measurement Structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Distribution of PMC. (The x-axis represents the value of PMC in cycles. 1: 0-49 cycles; 2: 50-99 cycles; 3: 100-149 cycles; 4: 150-199 cycles; 5: 200-249 cycles; 6: 250-299 cycles; 7: 300-349 cycles; 8: 350+ cycles)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Block diagram of CARE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Speedup for 4, 8, 16 cores (multi-copy SPEC workloads without a prefetcher).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>If a miss access contains at least one pure miss cycle, this miss is categorized as pure miss. Pure miss has a higher performance impact because pure miss cycles have no overlapping base access cycles to hide the penalty. Similar to miss rate, the Pure Miss Rate (pMR) can measure the cache efficiency by considering data access concurrency. The formal definition of pMR is as below:</figDesc><table><row><cell>Pure Miss Rate (pMR) =</cell><cell>Num. of Pure Misses Num. of Total Accesses</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>PMC of the study case.</figDesc><table><row><cell>Miss</cell><cell>PMC</cell><cell>MLP-based cost</cell></row><row><cell>A</cell><cell>0</cell><cell>5</cell></row><row><cell>C</cell><cell>1/3 + 1/3 + 1/3 = 1</cell><cell>7/3</cell></row><row><cell>D</cell><cell>1/3 + 1/3 + 1/3 + 1/2 + 1/2 = 2</cell><cell>7/3</cell></row><row><cell>E</cell><cell>1/3 + 1/3 + 1/3 + 1/2 + 1/2 = 2</cell><cell>7/3</cell></row><row><cell></cell><cell cols="2">Active pure miss cycles : 5 (cycles 10-14)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Distribution and median of PMC ? .</figDesc><table><row><cell>PMC ?</cell><cell>403</cell><cell>429</cell><cell>433</cell><cell>436</cell><cell>437</cell><cell>450</cell><cell>459</cell><cell>462</cell><cell>470</cell><cell>473</cell><cell>482</cell><cell>603</cell><cell>621</cell><cell>623</cell><cell>649</cell><cell>654</cell></row><row><cell>[0,50)</cell><cell cols="16">89.40% 62.63% 64.02% 79.57% 68.19% 60.29% 57.74% 62.17% 59.99% 79.23% 57.72% 60.69% 64.77% 63.31% 50.66% 64.17%</cell></row><row><cell cols="17">[50,100) 3.89% 16.49% 14.52% 10.06% 18.70% 16.78% 14.95% 13.11% 16.23% 10.06% 18.18% 15.82% 15.24% 14.80% 19.85% 14.65%</cell></row><row><cell cols="17">[100,150) 5.56% 12.23% 12.27% 5.42% 9.33% 12.58% 11.00% 15.86% 7.22% 6.04% 12.93% 7.59% 9.09% 14.21% 16.87% 13.95%</cell></row><row><cell>? 150</cell><cell cols="16">1.15% 8.65% 9.18% 4.96% 3.79% 10.36% 16.30% 8.87% 16.56% 4.67% 11.18% 15.90% 10.90% 7.68% 12.62% 7.23%</cell></row><row><cell>Median</cell><cell>2.87</cell><cell>31.00</cell><cell>33.00</cell><cell>1.00</cell><cell>21.00</cell><cell>33.33</cell><cell>35.13</cell><cell>40.00</cell><cell>33.44</cell><cell>5.03</cell><cell>36.00</cell><cell>32.44</cell><cell>26.00</cell><cell>33.50</cell><cell>48.75</cell><cell>31.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Insertion and hit-promotion policy of CARE. Higher EPV value indicates higher eviction priority.</figDesc><table><row><cell>Reuse hint</cell><cell>Insertion policy</cell><cell>Hit policy</cell></row><row><cell>High-Reuse</cell><cell>EPV = 0</cell><cell>EPV = 0</cell></row><row><cell></cell><cell>if(Low-Cost) EPV = 3;</cell><cell></cell></row><row><cell>Moderate-Reuse</cell><cell>else if (High-Cost)</cell><cell>EPV = 0</cell></row><row><cell></cell><cell>EPV = 0; else EPV = 2;</cell><cell></cell></row><row><cell>Low-Reuse</cell><cell>EPV = 3</cell><cell>if(EPV &gt; 0) EPV --;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Hardware cost of CARE (16-way 2MB LLC).</figDesc><table><row><cell></cell><cell>Size</cell><cell>Used for</cell></row><row><cell>NoNewAccess(1-bit/core)</cell><cell>1bit</cell><cell>PMC</cell></row><row><cell>lookup table (32-bit/entry)</cell><cell>0.25KB</cell><cell>PMC</cell></row><row><cell>PMC(32-bit/MSHR entry)</cell><cell>0.25KB</cell><cell>PMC</cell></row><row><cell>PMC low</cell><cell>32bit</cell><cell>DTRM</cell></row><row><cell>PMC high</cell><cell>32bit</cell><cell>DTRM</cell></row><row><cell>TCM</cell><cell>32bit</cell><cell>DTRM</cell></row><row><cell>EPV(2-bit/block)</cell><cell>8KB</cell><cell>metadata</cell></row><row><cell>prefetch(1-bit/block)</cell><cell>4KB</cell><cell>metadata</cell></row><row><cell>signature(14-bit/sampled set)</cell><cell>1.75KB</cell><cell>metadata</cell></row><row><cell>R (1-bit/sampled set)</cell><cell cols="2">0.125KB metadata</cell></row><row><cell>PMCS(2-bit/sampled set)</cell><cell>0.25KB</cell><cell>metadata</cell></row><row><cell>RC(3-bit/SHT entry)</cell><cell>6KB</cell><cell>SHT</cell></row><row><cell>PD(3-bit/SHT entry)</cell><cell>6KB</cell><cell>SHT</cell></row><row><cell cols="3">Total 26.64KB (6.76KB for concurrency-aware)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Hardware costs for different replacement frameworks (16-way 2MB LLC).</figDesc><table><row><cell>Framework</cell><cell>Uses PC</cell><cell cols="2">Concurrency-aware Total cost</cell></row><row><cell>LRU</cell><cell>No</cell><cell>No</cell><cell>16KB</cell></row><row><cell>SBAR(MLP) [34]</cell><cell>No</cell><cell>Yes</cell><cell>28.09KB</cell></row><row><cell>SHiP++ [53]</cell><cell>Yes</cell><cell>No</cell><cell>16KB</cell></row><row><cell>Hawkeye [17]</cell><cell>Yes</cell><cell>No</cell><cell>30.94KB</cell></row><row><cell>Glider [42]</cell><cell>Yes</cell><cell>No</cell><cell>61.6KB</cell></row><row><cell>Mockingjay [41]</cell><cell>Yes</cell><cell>No</cell><cell>31.91KB</cell></row><row><cell>CARE</cell><cell>Yes</cell><cell>Yes</cell><cell>26.64KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Simulated system configurations.</figDesc><table><row><cell>Processor</cell><cell>1 to 16 cores, 4GHz, 8-issue width, 256-entry ROB</cell></row><row><cell>L1 Cache</cell><cell>private, split 32KB I/D-cache/core, 64B line, 8-way, 4-cycle latency, 8-entry MSHR, LRU</cell></row><row><cell>L2 Cache</cell><cell>private, 256KB/core, 64B line. 8-way, 10-cycle latency, 32-entry MSHR, LRU</cell></row><row><cell>L3 Cache</cell><cell>shared, 2MB/core, 64B line, 16-way, 20-cycle latency, 64-entry MSHR</cell></row><row><cell>Prefetcher</cell><cell>L1: next-line, L2: IP-stride</cell></row><row><cell></cell><cell>4GB 1 channel (single-core),</cell></row><row><cell>DRAM</cell><cell>8GB 2 channels (multi-core), 64-bit channel, 2400MT/s,</cell></row><row><cell></cell><cell>tRP=15ns, tRCD=15ns, tCAS=12.5ns</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>Evaluated SPEC workloads.</figDesc><table><row><cell>Suite</cell><cell>Workload</cell><cell>MPKI</cell><cell>Workload</cell><cell>MPKI</cell></row><row><cell></cell><cell>401.bzip2</cell><cell>1.34</cell><cell>403.gcc</cell><cell>25.55</cell></row><row><cell></cell><cell>410.bwaves</cell><cell>18.35</cell><cell>429.mcf</cell><cell>26.28</cell></row><row><cell></cell><cell>433.milc</cell><cell cols="3">19.00 436.cactusADM 4.99</cell></row><row><cell>SPEC</cell><cell>437.leslie3d</cell><cell>6.68</cell><cell>450.soplex</cell><cell>32.69</cell></row><row><cell>06</cell><cell>456.hmmer</cell><cell>2.72</cell><cell cols="2">459.GemsFDTD 24.44</cell></row><row><cell></cell><cell cols="2">462.libquantum 28.03</cell><cell>470.lbm</cell><cell>28.42</cell></row><row><cell></cell><cell>473.astar</cell><cell>35.88</cell><cell>481.wrf</cell><cell>5.66</cell></row><row><cell></cell><cell>482.sphinx3</cell><cell>12.96</cell><cell cols="2">483.xalancbmk 26.91</cell></row><row><cell></cell><cell>602.gcc s</cell><cell>17.77</cell><cell>603.bwaves s</cell><cell>19.00</cell></row><row><cell></cell><cell>605.mcf s</cell><cell cols="3">55.62 607.cactuBSSN s 3.51</cell></row><row><cell>SPEC 17</cell><cell>619.lbm s 621.wrf s 625.x264 s</cell><cell cols="3">40.64 19.22 623.xalancbmk s 24.26 620.omnetpp s 9.21 1.35 627.cam4 s 4.51</cell></row><row><cell></cell><cell>628.pop2 s</cell><cell>2.99</cell><cell cols="2">649.fotonik3d s 15.67</cell></row><row><cell></cell><cell>654.roms s</cell><cell>24.23</cell><cell>657.xz s</cell><cell>1.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX :</head><label>IX</label><figDesc>Graph datasets used in GAP workloads.</figDesc><table><row><cell>Dataset</cell><cell>Vertices</cell><cell>Edges</cell><cell>Description</cell></row><row><cell>orkut (or)</cell><cell>3.1M</cell><cell>117.2M</cell><cell>Social network</cell></row><row><cell>twitter (tw)</cell><cell>61.6M</cell><cell cols="2">1468.4M Social network</cell></row><row><cell>urand (ur)</cell><cell>134.2M</cell><cell>2147.4M</cell><cell>Synthetic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table VIII</head><label>VIII</label><figDesc>Normalized IPC for all 4-core multi-copy SPEC workloads (collaboration with L1 and L2 prefetcher).</figDesc><table><row><cell></cell><cell>1.6</cell><cell cols="2">SHiP++</cell><cell></cell><cell cols="2">Hawkeye</cell><cell></cell><cell>Glider</cell><cell></cell><cell cols="2">M-CARE</cell><cell></cell><cell>CARE</cell><cell></cell><cell>LRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized IPC</cell><cell>1.0 1.2 1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>401</cell><cell>403</cell><cell>410</cell><cell>429</cell><cell>433</cell><cell>436</cell><cell>437</cell><cell>450</cell><cell>456</cell><cell>459</cell><cell>462</cell><cell>470</cell><cell>473</cell><cell>481</cell><cell>482</cell><cell>483</cell><cell>602</cell><cell>603</cell><cell>605</cell><cell>607</cell><cell>619</cell><cell>620</cell><cell>621</cell><cell>623</cell><cell>625</cell><cell>627</cell><cell>628</cell><cell>649</cell><cell>654</cell><cell>657</cell><cell>GM</cell></row><row><cell>LLC pMR</cell><cell cols="2">0.4 0.7 Fig. 7: 0.1 1.0 LRU</cell><cell cols="2">SHiP++</cell><cell></cell><cell cols="2">Hawkeye</cell><cell></cell><cell>Glider</cell><cell></cell><cell cols="2">M-CARE</cell><cell></cell><cell>CARE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>401</cell><cell>403</cell><cell>410</cell><cell>429</cell><cell>433</cell><cell>436</cell><cell>437</cell><cell>450</cell><cell>456</cell><cell>459</cell><cell>462</cell><cell>470</cell><cell>473</cell><cell>481</cell><cell>482</cell><cell>483</cell><cell>602</cell><cell>603</cell><cell>605</cell><cell>607</cell><cell>619</cell><cell>620</cell><cell>621</cell><cell>623</cell><cell>625</cell><cell>627</cell><cell>628</cell><cell>649</cell><cell>654</cell><cell>657</cell><cell>GM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">shows LLC MPKI for the 30 evaluated SPEC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">workloads in our study. For SPEC workloads, the traces are</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">collected with SimPoint [32] and are provided by DPC-3 [3].</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">For GAP workloads, we select five primitive graph algorithms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="16">for evaluation, which are Betweenness Centrality (bc), Breadth</cell></row></table><note><p><p><p>First Search (bfs), Connected Components (cc), PageRank (pr), and Single Source Shortest Path (sssp). Table IX lists the graph datasets that are used in our experiments. We use the Intel Pin dynamic binary instrumentation tool</p><ref type="bibr" target="#b4">[5]</ref> </p>to collect the traces of GAP workloads. We use the region-of-interest (ROI) utility</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X :</head><label>X</label><figDesc>Average pMR and PMC for all 4-core multi-copy SPEC workloads (collaboration with L1 and L2 prefetcher).</figDesc><table><row><cell></cell><cell cols="14">LRU SHiP++ Hawkeye Glider M-CARE CARE</cell><cell></cell></row><row><cell cols="3">pMR 0.56</cell><cell></cell><cell>0.52</cell><cell></cell><cell>0.51</cell><cell></cell><cell cols="2">0.50</cell><cell></cell><cell>0.52</cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell></row><row><cell cols="5">PMC 114.46 97.98</cell><cell></cell><cell cols="4">99.44 101.43</cell><cell></cell><cell>97.80</cell><cell cols="2">95.11</cell><cell></cell><cell></cell></row><row><cell>1.3</cell><cell>SHiP++</cell><cell></cell><cell></cell><cell>Hawkeye</cell><cell></cell><cell cols="2">Glider</cell><cell></cell><cell cols="2">M-CARE</cell><cell></cell><cell>CARE</cell><cell></cell><cell>LRU</cell><cell></cell></row><row><cell>1.0 1.1 1.2 Normalized IPC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bc-or</cell><cell>bc-tw</cell><cell>bc-ur</cell><cell>bfs-or</cell><cell>bfs-tw</cell><cell>bfs-ur</cell><cell>cc-or</cell><cell>cc-tw</cell><cell>cc-ur</cell><cell>pr-or</cell><cell>pr-tw</cell><cell>pr-ur</cell><cell>sssp-or</cell><cell>sssp-tw</cell><cell>sssp-ur</cell><cell>GM</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>All advanced cache management schemes<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b18">[19]</ref>,<ref type="bibr" target="#b41">[42]</ref>,<ref type="bibr" target="#b47">[48]</ref>,<ref type="bibr" target="#b52">[53]</ref> have a similar counter for each cache block to indicate the eviction priority of the block. Cache management schemes implement specialized cache policies by assigning/updating the eviction priority of each block. The EPV in CARE does not introduce additional overhead.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank the anonymous reviewers for their helpful feedback. This research is supported in part by the <rs type="funder">National Science Foundation</rs> under Grants <rs type="grantNumber">CCF-2029014</rs>, <rs type="grantNumber">CCF-2008907</rs>, <rs type="grantNumber">CNS-2152497</rs>, and by the <rs type="funder">NSF</rs> supported Chameleon testbed facility.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_q9X8AhD">
					<idno type="grant-number">CCF-2029014</idno>
				</org>
				<org type="funding" xml:id="_CM3wFTu">
					<idno type="grant-number">CCF-2008907</idno>
				</org>
				<org type="funding" xml:id="_cUZp42p">
					<idno type="grant-number">CNS-2152497</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2) Multi-copy GAP workloads: Figure <ref type="figure">9</ref> shows the detailed results on the multi-copy GAP workloads. On a 4-core system with prefetching, CARE improves performance over LRU by 8.7%, while SHiP++, Hawkeye, Glider, and M-CARE improve performance by 5.4%, 1.8%, 3.0%, and 6.7%, respectively. The irregular access patterns of graph-analytic applications challenge the re-reference prediction. CARE performs better than state-of-the-art schemes, which can be attributed to the following two factors. First is its concurrency awareness, which minimizes the performance loss caused by misses. Second is the conservative nature of the hit policy. When a block incurs a hit, the eviction priority of that cache block is still reduced even if CARE predicts that the block is Low-Reuse.</p><p>3) Mixed Workloads: Figure <ref type="figure">10</ref> shows the normalized weighted IPC comparison between different schemes for 100 4-core mixed workloads. In the presence of prefetching, CARE offers a geometric mean speedup of 12.8% over LRU, compared with the 11.9%, 6.8%, 6.4%, and 11.4% improvements for SHiP++, Hawkeye, Glider, and M-CARE. CARE yields the best performance for 67 mixed workloads. CARE shows an advantage over other schemes in providing stable performance.  B. Scalability Evaluation 1) Speedup with increasing concurrency: In the presence of prefetching, Figure <ref type="figure">11</ref> summarizes CARE performance on multi-copy SPEC workloads as we increase the number of cores. We make three major observations from Figure <ref type="figure">11</ref>. First, the opportunity for cache management increases with multiple cores due to increased pressure on the LLC. Second, CARE outperforms every state-of-the-art cache management scheme in all configurations. Third, as the number of cores increases, the performance advantage of CARE becomes greater and greater. In the 4-core configuration, CARE achieves an improvement over LRU by 10.3% on average. As the number of cores increases, the gains are 13.0%, and 17.1%, respectively.</p><p>Similarly, Figure <ref type="figure">12</ref> shows the performance improvement of cache management schemes across GAP workloads in 4-core to 16-core systems. The performance improvement of CARE over other schemes increases with the number of cores. In 16-core systems, CARE outperforms LRU, SHiP++, Hawkeye, Glider, Mockingjay, and M-CARE by 16.1%, 7.8%, 12.7%, 11.6%, 11.4%, and 7.3%, respectively.</p><p>We measure Average Overlapping Cycles Per Access (AOCPA) on SPEC and GAP workloads to quantify the access concurrency on LLC. AOCPA is calculated on a per-core basis. A higher AOCPA means more data access overlapping in the LLC that CARE can exploit. Table <ref type="table">XI</ref> summarizes the value of AOCPA in the presence of prefetching for multicore configurations. Table <ref type="table">XI</ref> shows that as the number of cores increases, the AOCPA increases significantly. This is because when the LLC receives a heavier workload, the increasing miss rate and average miss access latency lead to more miss-miss and hit-miss overlapping cycles. Therefore, in a multi-core system where the AOCPA is increasing, CARE continues outperforming other locality-only based cache management schemes and shows a huge potential for improving the performance of data-intensive applications on large-scale computing systems.</p><p>2) Performance without prefetching: Figures 13 and 14 show the scalability results without prefetching for SPEC and GAP workloads, respectively. In the absence of prefetching, the performance of CARE still scales well on SPEC and GAP</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">1st instruction prefetching championship</title>
		<ptr target="https://research.ece.ncsu.edu/ipc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2nd cache replacement championship</title>
		<ptr target="https://crc2.ece.tamu.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3rd data prefetching championship</title>
		<ptr target="https://dpc3.compas.cs.stonybrook" />
		<imprint/>
	</monogr>
	<note>final programs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The champsim simulator</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pin-a dynamic binary instrumentation tool</title>
		<ptr target="https://www.intel.com/content/www/us/en/developer/articles/tool/pin-a-dynamic-binary-instrumentation-tool.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spec cpu2017 benchmark suite</title>
		<ptr target="http://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring high bandwidth pipelined cache architecture for scaled technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Design, Automation and Test in Europe</title>
		<meeting>the conference on Design, Automation and Test in Europe</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10778</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">P-opt: Practical optimal cache replacement for graph analytics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The gap benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03619</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtual-storage computer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pseudo-lifo: The foundation of a new family of replacement policies for last-level caches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 31st Annual International Symposium on Computer Architecture</title>
		<meeting>31st Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving cache management policies using dynamic reuse distances</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 45Th annual IEEE/ACM international symposium on microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain-specialized cache ment for graph analytics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="234" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dueling segmented lru replacement algorithm with adaptive bypassing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JWAC 2010-1st JILP Worshop on Computer Architecture Competitions: Cache Replacement Championship</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Glew</surname></persName>
		</author>
		<title level="m">Mlp yes! ilp no,&quot; ASPLOS Wild and Crazy Idea Session</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Back to the future: leveraging belady&apos;s algorithm for improved cache replacement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking belady&apos;s algorithm to accommodate prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="110" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (rrip)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiperspective reuse prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="436" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cache replacement based on reuse-distance prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 25th International Conference on Computer Design</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lacs: A locality-aware cost-sensitive cache replacement algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1975" to="1987" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counter-based cache replacement and bypassing algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="195" to="201" />
		</imprint>
	</monogr>
	<note>in 25 years of the international symposia on Computer architecture. selected papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; deadblock correlating prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
		<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lrfu: A spectrum of policies that subsumes the least recently used and least frequently used policies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cache bursts: A new approach for eliminating dead blocks and increasing cache efficiency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 41st IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study on modeling and optimization of memory systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Espina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lpm: A systematic methodology for concurrent data access pattern optimization from a matching perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2478" to="2493" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Apac: An accurate and adaptive prefetch framework with concurrent memory access analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 38th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Premier: A concurrency-aware pseudopartitioning framework for shared last-level cache</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 39th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="391" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="318" to="319" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for high performance caching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="391" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A case for mlp-aware cache replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Symposium on Computer Architecture (ISCA&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The v-way cache: demand-based associativity via global replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="544" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Emulating optimal replacement with a shepherd cache</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On high-bandwidth data cache design for multi-issue processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual ACM/IEEE international symposium on Microarchitecture</title>
		<meeting>the 30th annual ACM/IEEE international symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data cache management using frequency-based replacement</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Devarakonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems</title>
		<meeting>the 1990 ACM SIGMETRICS conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The evictedaddress filter: A unified mechanism to address both cache pollution and thrashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Designing a cost-effective cache replacement policy using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sartori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>the 27th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effective mimicry of belady&apos;s min policy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="558" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Applying deep learning to the cache replacement problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="413" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spec cpu2006 benchmark tools</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Spradling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="130" to="134" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Concurrent average memory access time</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Perceptron learning for reuse prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable cache miss handling for high memory-level parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 39th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="409" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modified lru policies for improving secondlevel cache behavior</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<idno>PR00550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings Sixth International Symposium on High-Performance Computer Architecture. HPCA-6</title>
		<meeting>Sixth International Symposium on High-Performance Computer Architecture. HPCA-6</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
	<note type="report_type">Cat. No.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ship: Signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="430" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pacman: prefetch-aware cache management for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="442" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pipp: Promotion/insertion pseudo-partitioning of multi-core shared caches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Copim: a concurrency-aware pim workload offloading architecture for graph applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Symposium on Low Power Electronics and Design</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ship++: Enhancing signature-based hit predictor for improved cache performance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cache Replacement Championship (CRC&apos;17) held in Conjunction with the International Symposium on Computer Architecture (ISCA&apos;17)</title>
		<meeting>the Cache Replacement Championship (CRC&apos;17) held in Conjunction with the International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
