<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clustered microarchitectures</term>
					<term>dynamic code partitioning</term>
					<term>steering logic</term>
					<term>dynamically scheduled processors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustered microarchitectures are an effective approach to reducing the penalties caused by wire delays inside a chip. Current superscalar processors have in fact a two-cluster microarchitecture with a naive code partitioning approach: integer instructions are allocated to one cluster and floating-point instructions to the other. This partitioning scheme is simple and results in no communications between the two clusters (just through memory) but it is in general far from optimal because the workload is not evenly distributed most of the time. In fact, when the processor is running integer programs, the workload is extremely unbalanced since the FP cluster is not used at all. In this work we investigate run-time mechanisms that dynamically distribute the instructions of a program among these two clusters. By optimizing the trade-off between inter-cluster communication penalty and workload balance, the proposed schemes can achieve an average speed-up of 36% for the SpecInt95 benchmark suite.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scaling-up current superscalar microarchitectures will face significant problems such as the growing impact of wire delays <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b12">[13]</ref> and increasing complexity of some parts such as the issue and rename logic <ref type="bibr" target="#b14">[15]</ref>. Clustering is an effective solution to these problems. A clustered microarchitecture partitions some of this critical logic into simpler parts and, at the same time, it reduces the impact of wire delays by keeping most of the communications local to single clusters and avoiding communications among different clusters whenever possible.</p><p>Current superscalar processors are in fact partitioned into two clusters, one for integer instructions and the other for FP operations. Each of these clusters has its own instruction queue 1 , issue logic, functional units and register file. However, these two data-paths can be underutilized due to a poor workload balance. This is especially true when the processor is running integer applications, which (almost) only use the integer data-path. Providing the two data-paths with the capability of executing any type of instructions would imply that every cluster should have any type of functional unit. However, since FP applications are rich in integer instructions, Palacharla and Smith <ref type="bibr" target="#b15">[16]</ref> addressed this drawback by proposing a more costeffective approach based on extending the FP data-path with functional units for simple integer and logical operations, which are usually the most frequent instructions. This requires minor modifications to existing microarchitectures and may result in significant speed-ups, especially for integer applications, since both data-paths can process instructions in parallel.</p><p>Deciding which instructions are executed in each cluster is a critical issue of clustered microarchitectures. We will refer to this problem as code partitioning. This work focuses on code partitioning mechanisms for clustered microarchitectures. Although the schemes presented are evaluated in one architecture, we believe that the same schemes can be used in more generic clustered architectures. We first show that dynamic mechanisms can be more effective than static ones. Then, we propose several dynamic mechanisms and evaluate their performance. We report average speed-ups of up to 36% for the SpecInt95 benchmark suite, when compared with a conventional microarchitecture with the naive integer-FP <ref type="bibr">1.</ref> In some processors the instruction queue may be shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Cluster Assignment Mechanisms</head><p>Ramon Canal, Joan Manuel Parcerisa and Antonio González Departament d'Arquitectura de Computadors Universitat Politècnica de Catalunya Jordi Girona, 1-3 Mòdul D6 08034 Barcelona, Spain {rcanal,jmanel,antonio}@ac.upc.es code partitioning. We also show that the proposed approaches outperform previously proposed dynamic schemes. The rest of this paper is organized as follows. Section 2 describes the assumed processor microarchitecture. Section 3 presents and evaluates several code partitioning mechanisms. It also includes comparisons with previously proposed mechanisms. Section 4 discusses some related work. Finally, Section 5 summarizes the main conclusions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Processor Microarchitecture</head><p>The target processor microarchitecture is based on the proposal made by Palacharla and Smith <ref type="bibr" target="#b15">[16]</ref> and also investigated by Sastry, Palacharla and Smith <ref type="bibr" target="#b17">[18]</ref>, which extends a conventional microarchitecture in order to allow simple integer and logic instructions to be executed in both clusters. Unlike those works, we propose a dynamic code partitioning scheme, which is performed by a hardware that is referred to as steering logic. Dynamic partitioning has the advantage of not requiring any modification to the ISA, unlike the static approaches. Furthermore, the complexity of this hardware is low, as will be later shown. Basically, it requires some modifications in the register renaming mechanism and some tables that hold information used by the partitioning heuristics.</p><p>Figure <ref type="figure">1</ref> shows a block diagram of the processor architecture. The main differences with a conventional architecture are the steering logic and the buses that allow values to be copied from one cluster to the other.</p><p>Instructions are fetched and decoded by a centralized hardware and then, they are dispatched to one of the two clusters. Each cluster has its own data-path and instruction issue logic. Both clusters have a set of simple integer and logic functional units. In addition, one cluster contains complex integer functional units (multiplier and divider) and the other has floating-point functional units. Local bypasses are responsible for forwarding result values produced in a cluster to the inputs of the functional units in the same cluster. A local bypass takes 0 cycles, so an output in cycle i can be an input of a FU the following cycle (i+1). Inter-cluster bypasses are responsible for forwarding values between different clusters. Therefore, they are slower than local ones, and we assume that they take one cycle.</p><p>Dynamic register renaming is performed by means of a physical register file in each cluster and a single register map table. Since integer instructions can be executed in both clusters, the entries of the map table for integer registers contain two fields that identify the mapping in each cluster. When an instruction is decoded, the steering logic decides in which cluster it is to be executed and a physical register from that cluster is allocated for the destination operand (if any). When a source operand of an instruction resides in the remote cluster, a physical register in the local cluster is allocated and the dispatch logic inserts a "copy" instruction that will move the data from the remote to the local cluster. This instruction will read the operand when it is available and will send the value through one of the inter-cluster bypasses. Copy instructions compete for issue slots and processor resources (e.g. register file ports) as any other instruction. This scheme implies some register replication but only for values that are used in the two clusters. We will show that the proposed partitioning schemes incur in a low degree of replication.</p><p>Load and store instructions are internally split into two operations, one for computing the effective address and another that performs the memory access. Address calculation can be performed in any of the two clusters since it involves a simple integer operation (addition). Then, the instruction is forwarded to a unique disambiguation logic that decides when the instruction can perform its memory access. A load reads from memory after being disambiguated with all previous stores, whereas stores write to memory at commit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Code Partitioning Schemes</head><p>This section presents several partitioning approaches and evaluates their performance. We first define some terminology and describe the experimental framework. Then, we compare the effectiveness of static partitioning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Terminology</head><p>The register dependence graph (RDG) represents all register dependences in a program. It is a directed graph that has a node associated to each static instruction and an edge for every data dependence (true dependence) through a register. Memory instructions are special cases since they are split into two disconnected nodes, one representing the address calculation and the other the memory access. Figure <ref type="figure">2</ref> shows an example of an RDG. Note that for the sake of clarity, in the assembly code memory instructions have already been split into two, one for address calculation (EA) and another for the memory access LD/ ST. The backward slice of an RDG with respect to a node v is defined as the set of nodes from which v can be reached, including v <ref type="bibr" target="#b17">[18]</ref>. Figure <ref type="figure">2</ref> shows the backward slice with respect to node 13 of the sample RDG.</p><p>The LdSt slice of a program is defined as the set of all instructions that belong to a backward slice of any address calculation instruction. Similarly, the Br slice of a program consists of all instructions that belong to the backward slice of any branch instruction. Figure <ref type="figure">2</ref> shows the LdSt slice and the Br slice of the sample program. Each backward slice is shaded in a different gray level.</p><p>We will refer to the cluster of the processor that can perform just integer operations as the integer cluster, and the cluster that can execute FP and simple integer instructions will be called the FP cluster. Backward slice with respect to node 13 (shaded nodes) Register dependency graph LdSt slice (shaded nodes) Br slice (shaded nodes)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Framework</head><p>Performance figures were obtained through a cyclebased timing simulator based on the SimpleScalar tool set v3.0 <ref type="bibr" target="#b2">[3]</ref>, which was extended to simulate the architecture described in section 2. Results are presented for the SpecInt95 benchmark suite. Table <ref type="table" target="#tab_4">1</ref> lists the benchmark programs and their inputs. Programs were compiled with the Compaq/Alpha C compiler with the -O5 optimization flag. For each benchmark, 100 million instructions were run after skipping the first 100 million. Table <ref type="table" target="#tab_3">2</ref> shows the architectural parameters of the assumed processor.</p><p>Performance will usually be reported as speed-up over a base architecture, which is a conventional microprocessor with the same architectural parameters listed in Table <ref type="table" target="#tab_3">2</ref> except that it has neither integer units in the FP cluster nor inter-cluster bypasses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Static versus Dynamic Partitioning</head><p>A static partitioning approach requires some extensions to the ISA in order to allow the compiler to specify to the hardware the target cluster for each instruction. Moreover, it is less flexible than a dynamic approach since all dynamic instances of the same static instruction are executed in the same cluster. On the other hand, its hardware complexity is negligible.</p><p>The static partitioning proposed by Sastry et al. <ref type="bibr" target="#b17">[18]</ref> is based on sending to the integer cluster all instructions that belong to the subgraph defined by the LdSt slice, probably extended with neighbor instructions. This extension is based on some heuristics that try to approximate its effect in terms of workload balance and communication overheads.</p><p>Figure <ref type="figure">3</ref> compares the speed-ups of that static partitioning with the speed-ups achieved by a simple dynamic partitioning that tries to dispatch all instructions in the LdSt slice to the integer cluster and the remaining instructions to the FP cluster (excepting complex integer instructions). We will refer to this dynamic partitioning scheme as LdSt slice steering. The numbers for the static partitioning have been obtained from the original paper <ref type="bibr" target="#b17">[18]</ref> and the dynamic approach has been simulated using the same compiler, the same compiler options, the same benchmarks and the same architecture. Note that the dynamic scheme significantly outperforms the static one for all the programs excepting m88ksim, for which both schemes achieve similar levels of performance. On average, the LdSt slice steering achieves an speed-up of 16% whereas the static partitioning speed-up is just 3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Configuration</head><p>Fetch  This dynamic partitioning can be implemented by including a table that is indexed by the PC of instructions. For each entry it has a one-bit flag that denotes whether the corresponding instruction belongs to the LdSt slice or not. Initially all the bits are cleared. For every instruction, if it is a memory instruction its flag is set. If an instruction finds its flag set, the flags of its parents in the RDG are also set. The parents are identified by means of an additional table that holds for each logical register the PC of the last decoded instruction that uses it as a destination register.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">LdSt Slice Steering versus Br Slice Steering</head><p>The performance of any partitioning scheme is quite sensitive to the number of inter-cluster communications that it generates. A communication has some latency that may delay the execution of the consumer instructions. Therefore, the criticality of consumer instructions is even more important than the absolute number of communications. An inter-cluster communication that is consumed by an instruction that is not critical may have no effect on the execution time. Some memory instructions, especially those that cause many cache misses, are critical in most programs, which suggests that the LdSt slice steering may be an appropriate partitioning because executing all the backward slice of a load in one cluster avoids adding communication delays to the computation of its address. However, branch instructions are also critical in non-numeric codes such as the SpecInt95. This suggest an alternative partitioning scheme that steers instructions in the Br slice to the integer cluster and the remaining instructions to the FP cluster (excepting complex integer instructions). We will refer to this scheme as Br slice steering. The hardware to implement this scheme is basically the same as that described in section 3.3 for the LdSt slice steering.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> compares the performance of the LdSt slice steering with that of the Br slice steering. Note that the performance of the Br slice steering is somewhat lower, which is explained by the larger number of communications that it generates, as shown in Figure <ref type="figure">5</ref>. This figure shows the average number of communications per dynamic instruction, split into critical and non-critical. We consider that a communication is critical when there is any instruction in the destination cluster that has been delayed due to the communication.</p><p>Another critical factor for the performance of a clustered architecture is the workload balance. The workload of a cluster can be measured as the number of ready instructions it has. Figure <ref type="figure">6</ref> shows the distribution function of the workload balance (that is, the difference between the number of ready instructions in each cluster for each cycle). It can be seen that both dynamic partitioning schemes result in a similar workload balance. In both cases, there is a significant percentage of time in which the two clusters have different workload: either the integer or the FP cluster is overloaded. Note that the overload of the FP cluster could be reduced if some of the instructions that are not part of the LdSt slice (resp. Br slice) were dispatched to the integer cluster. This motivates the next partitioning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Non-Slice Balance Steering</head><p>As motivated in the previous section, a better workload balance could be achieved if instructions that are not in the slice are used to balance the workload. However, sending   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% of cycles</head><p>Ld/St Slice Br. Slice every non-slice instruction to the least loaded cluster would result in too many communications. A more effective approach would be to send non-slice instructions to the least loaded cluster only when there is a strong workload imbalance (see next paragraph). Otherwise, these instructions are sent to the cluster where their operands reside in order to reduce communications. We refer to this approach as non-slice balance steering.</p><p>The workload imbalance may be estimated by counting the difference in the number of instructions steered to each cluster (we refer to this metric as I1). However, this metric does not consider the amount of parallelism present in each instruction window at a given time.</p><p>On the other hand, the workload of a cluster may be computed as the number of ready instructions it has. The workload is considered imbalanced when one cluster has more ready instruction than its issue width, and the other has less than its issue width. Just in this scenario, the instant workload imbalance is quantified as the difference in number of ready instructions (metric I2). In any other scenario, the processor can execute the instructions at the maximum possible rate, so the workload is then considered balanced.</p><p>The load balancing mechanism presented in this work considers the two metrics (I1 and I2) by maintaining a single integer imbalance counter that combines the two informations. Each cycle, the counter is updated according to the average of I2, computed along N cycles. It is also updated with I1, by incrementing or decrementing it for each instruction steered, so every instruction decoded in the same cycle sees a different value of the workload balance and thus, massive steerings to one cluster are avoided.</p><p>To determine whether there is a strong imbalance, the absolute value of this counter is compared with a given threshold. We have empirically determined that 16 and 8 are adequate values for N and the threshold respectively. We have empirically observed that the metric I1 is more effective than the I2 to balance the workload when both are considered isolated. In fact, metric I1 alone gives performance figures quite close to those produced by the combination of I1 and I2.</p><p>Figure <ref type="figure">7</ref> compares the performance of the non-slice balance steering with that of the slice steering. It can be seen that the non-slice balance steering is beneficial for the Br slice but detrimental for the LdSt slice, in spite of the fact that this scheme improves the workload balance. This is explained by the amount of communications that these schemes generate, which are depicted in Figure <ref type="figure">8</ref>. This figure shows that the non-slice balance steering significantly increases the number of communications for the LdSt slice whereas it has about the same number of communications as the slice steering for the Br slice.</p><p>Figure <ref type="figure">9</ref> shows the distribution function of the workload balance for the non-slice balance steering. Note that the workload balance has improved (see the shape of the curve) in comparison with the slice steering scheme (figure <ref type="figure">6</ref>), but there is still a large percentage of cycles where the imbalance is significant. It is especially remarkable the overload of the integer cluster, which motivates the next partitioning scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% of cycles</head><p>Ld/St non-slice balance Br non-slice balance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Slice Balance Steering</head><p>The Br slice (or LdSt slice) of a program consists of several backward slices of branches (resp. loads/stores). A better balance could be achieved if the instructions in a given backward slice were sent to the same cluster but different backwards slices could be sent to different clusters. We refer to this scheme as slice balance steering. In this scheme, instructions are classified into backward slices (or slices for short) at run-time by means of the tables shown in Figure <ref type="figure" target="#fig_6">10</ref>. The slice table identifies for each instruction the slice to which it belongs. The backward slice of instruction v is identified by the PC of v. Initially no instruction belongs to any slice, which is denoted by a special value in the slice table. When a branch is executed (resp. a load/store), the slice table is modified to indicate that this instruction belongs to its own slice. Every time that an instruction in a slice is executed, it propagates the slice ID to its parents, which are identified by means of the parent table. For each logical register, this table holds the PC of the last decoded instruction that uses this register as its destination operand. The cluster where each slice is currently mapped is identified by means of the cluster table.</p><p>Instructions that belong to a slice are dispatched to the cluster where the slice is assigned (according to the cluster table). However, if this cluster is strongly overloaded (using the same workload measures as in the previous steering scheme), the whole slice is re-assigned to the other cluster. Instructions that do not belong to any slice are handled as in the non-slice balance steering approach.</p><p>Figure <ref type="figure">11</ref> shows the speed-up of the slice balance steering scheme over the base architecture. It can be seen that the performance for both types of slices (LdSt and Br) are very similar, and overall, the effectiveness of this is much higher than previous schemes. The average speed-up is 27% for the LdSt slice and 26.5% for the Br slice.</p><p>This good performance is due to a significant improvement in workload balance and a reduction in number of communications alike. Figure <ref type="figure" target="#fig_8">12</ref> shows the distribution of the workload balance for the slice balance steering (LdSt and Br) and compares it with that of a naive steering scheme that alternatively sends instructions to each cluster, if they can be executed in both. Note that this scheme has a low performance (as we will later show) due to its high number of communications, but it distributes the workload quite evenly. We refer to this scheme as modulo steering. We can see that the workload balance of the slice balance steering is almost the same as that of the modulo steering. Regarding communications, the slice balance steering generates 0.07 (LdSt) and 0.08 (Br) communications per dynamic instruction on average, which is quite less than previous schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Priority Slice Balance Steering</head><p>The objective of dispatching a whole slice of a load/store or branch instruction to the same cluster is to avoid communications in critical parts of the code. However, not all slices are equally critical. In particular, one may expect that slices corresponding to loads that miss very often in cache, or branches that are wrongly-predicted very often are more critical than the others since they cause significant penalties. Thus, slices could be classified according to their criticality. Computing the criticality of each instruction is by itself a complex problem that is beyond the scope of this work. Instead, we approximate the criticality of a slice by the number of cache misses or branch mispredictions of the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slice Table</head><p>Cluster   instruction that defines the slice, depending on the type of slice.</p><p>The priority slice balance steering tries to dispatch the instructions of any slice corresponding to a critical instruction to the same cluster, whereas the remaining instructions are dispatched following the same approach as the non-slice balance steering scheme. The threshold for deciding whether an instruction is critical or not will be dynamically adjusted so that around 50% of the instructions belong to critical slices. In particular, every 8192 (2 13 ) cycles the processor computes the number of instructions that have been considered as belonging to a critical slice. If this number is higher than half of the number of executed instructions, the threshold is increased; otherwise, it is decreased.</p><p>The main advantage of this scheme is that now, only the critical slices will be treated as slices. This scheme improves the flexibility for balancing the workload since there are more instructions that are individually treated than in the previous schemes. Having more flexibility to balance the workload with individual instructions reduces the number of slice re-mappings caused by strong imbalances (see Section 3.5 for a definition). Such remappings can arise in the middle of the execution of a given slice, and therefore, they may cause undesired intraslice communications. Thus, we expect this scheme to reduce the number of critical communications, although it might increase the total number of communications when trying to improve the workload balance. Overall, this scheme tries to minimize the communications in the critical slices while it tries to maximize the workload balance by means of the rest.</p><p>As far as the hardware implementation is concerned, we need a cycle counter (13 bit counter), a threshold register with an increment and decrement hardware, a critical instruction counter -16 bits are enough (2 13 cycles x 2 3 issue-width)-and a non-critical instruction counter. In addition, the cluster table (see figure <ref type="figure" target="#fig_6">10</ref>) should be augmented with a new field that counts for each slice the number of cache misses or branch mispredictions of the instruction that defines the slice, and a flag that indicates whether the slice is critical.</p><p>Figure <ref type="figure">13</ref> shows the performance of the priority slice balance steering. It achieves an average speedup of 27.7% (LdSt slice) and 28.8% (Br slice) over the base architecture, which is slightly better than that of the slice balance steering (see figure <ref type="figure">11</ref>). This improvement is due to the reduction in number of critical communications per dynamic instruction, which on average decreases from 0.050 to 0.045 for the LdSt slice and from 0.055 to 0.043 for the Br slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">General Balance Steering</head><p>The last presented scheme is a particular case of the previous one, in which the criticality threshold is set so high that there are no critical instructions and all instructions are steered as if they were non-slice instructions. That is, instructions are sent to the least loaded cluster when there is a strong workload imbalance or they have an equal number of operands in both clusters. Otherwise, they are sent to the cluster where most of their operands reside. The immediate consequence is that the required hardware to identify programs slices (see figure <ref type="figure" target="#fig_6">10</ref>) is not needed and no extra hardware is required to detect the criticality of instructions.</p><p>Figure <ref type="figure" target="#fig_3">14</ref> shows the performance of this scheme. It also includes the performance of the modulo steering and that of a 16-way issue processor (8 integer and 8 FP). The performance of this latter architecture can be considered as an upper-bound for any instruction partitioning approach since it has the same integer instruction throughput as the assumed architecture but it does not incur in any communication penalty. The general balance steering achieves an average speed-up of 36%, which is higher than previous schemes and just 8% smaller than the upperbound. On the other hand, the modulo steering produces a rather low improvement (2.8% on average).</p><p>As outlined in section 2, the cluster microarchitecture requires some degree of register replication. We have evaluated the average number of logical register that have a physical register allocated in both clusters. Results show (see figure <ref type="figure">15</ref>) that the required register replication is very low. Instead of replicating the whole physical register file as the Alpha 21264 processor does <ref type="bibr" target="#b9">[10]</ref>, on average this architecture requires only 3.1 registers to be replicated. This saving in register storage may have a significant impact on the register file access time, which in turn is one of the critical delays of superscalar processors <ref type="bibr" target="#b6">[7]</ref>. This scheme performs at the same level when there is just one bus each way to connect the clusters. Similar schemes to the General Balance one can be found in a work of the same authors <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Comparison with Other Dynamic Partitioning Approaches</head><p>Palacharla, Jouppi and Smith <ref type="bibr" target="#b14">[15]</ref> recently proposed a dynamic partitioning approach for a different clustered architecture that could be also applied to our assumed architecture. The basic idea is to model each instruction queue as if it was a collection of FIFO queues with instructions capable of issuing from any slot within each individual FIFO. Instructions are steered to FIFOs following some heuristics that ensures that a FIFO only contains dependent instructions, each instruction being dependent on the previous instruction in the same FIFO (for more details refer to the original paper <ref type="bibr" target="#b14">[15]</ref>). In this case, the FIFO approach has been implemented (8 FIFOs in each cluster and each 8-deep); and thus, it has been simulated with the same architecture and benchmarks used for the schemes presented in this work.</p><p>Figure <ref type="figure">16</ref> shows that the performance of the general balance steering described in the previous section significantly outperforms the steering scheme based on FIFOs for all the programs. On average, the FIFO-based steering increases the IPC of the conventional microarchitecture by 13% whereas the general balance steering achieves a 36% improvement. This difference in performance is explained by the fact that both schemes result in quite similar workload balance but the FIFO-based approach generates a significantly higher number of communications. On average, the general balance steering produces 0.042 intercluster communications per dynamic instruction whereas the FIFO-based approach results in 0.162 communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>The proposal of Sastry, Palacharla and Smith <ref type="bibr" target="#b17">[18]</ref> and that of Palacharla, Jouppi and Smith <ref type="bibr" target="#b14">[15]</ref> are two partitioning schemes that can be applied to the same type of architecture assumed in this work. The former is a static approach whereas the latter is based on run-time mechanisms. We have briefly outlined these techniques in sections 3.3 and 3.9 respectively and we have shown that the mechanisms proposed in this work, in particular the general balance steering, significantly outperform both of them. Another clustered architecture with a mostly-static code partitioning with some run-time support to improve workload balance is the Multicluster architecture <ref type="bibr" target="#b5">[6]</ref>. In this case, the processor consists of several identical clusters whereas our proposal focuses on an architecture that requires minor modifications to a conventional processor microarchitecture.</p><p>Kemp and Franklin proposed a clustered architecture <ref type="bibr" target="#b10">[11]</ref> where instructions are assigned to clusters based on inter-instruction register dependencies. However, since they assume a centralized register file, the steering scheme only needs to group two dependent instructions in the same cluster when the value from the producer is not still available at the time the consumer is decoded. This simple steering scheme is not suitable for our "distributed" register file, and in addition, it does not address the load balancing problem.</p><p>Clustering can also be applied to VLIW architectures <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b13">[14]</ref>. In this case the partitioning is done at compile time. Examples of such architectures are the Multiscalar <ref type="bibr" target="#b8">[9]</ref> [19], SPSM <ref type="bibr" target="#b4">[5]</ref>, Superthreaded <ref type="bibr" target="#b19">[20]</ref>, Trace Processors <ref type="bibr" target="#b16">[17]</ref> [21], Speculative Multithreaded <ref type="bibr" target="#b11">[12]</ref> and Dynamic Multithreaded <ref type="bibr" target="#b0">[1]</ref>. In such architectures, each cluster executes a different thread of control, all except one being speculative. Partitioning to reduce branch penalties and data dependence penalties are orthogonal paradigms that attack different problems and the two would combine nicely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed a number of run-time mechanisms that dynamically partition a sequential program into the different clusters of a clustered microarchitecture. We have focused on a two-cluster processor that is based on a conventional superscalar microarchitecture with the capability of executing simple integer operations in both the integer and the FP datapaths. Nevertheless, the schemes can be used in a generic clustered architecture with symmetric clusters.</p><p>The different proposed schemes have different levels of performance that are explained by their effectiveness to both reduce/hide inter-cluster communications and balance the workload. We have shown that all the schemes provide a significant speed-up over a conventional microarchitecture. For instance, the general balance steering scheme achieves an average speed-up of 36% for the SpecInt95 and its IPC is just 8% below that of a conventional processor with twice its issue width. We have also shown that the proposed schemes significantly outperform previous dynamic and static proposals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: LdSt slice versus Br slice steering go gcc compress li ijpeg vortex perl m88ksim H-mean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Average number of communications per dynamic instruction for slice steering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Figure 7: Non-slice balance steering versus slice steering go gcc compress li ijpeg vortex perl m88ksim H-mean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Hardware support for the slice balance steering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12:Distribution of the difference in the number of ready instructions between each cluster (SpecInt95 average)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Priority slice balance steering performance go gcc compress li ijpeg vortex perl m88ksim H-mean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: Register replication for the general balance steering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Machine parameters (split into cluster 1 and</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">cluster 2 if not common)</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Perf. improvement (%)</cell><cell>10 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Adv. Sch Sastry et al. Ld/St slice</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>perl</cell><cell>go</cell><cell>gcc</cell><cell>li</cell><cell>compress</cell><cell>ijpeg</cell><cell>m88ksim</cell><cell>G-mean</cell></row><row><cell cols="9">Figure 3: Static versus dynamic partitioning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Benchmarks and their inputs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table Parent Table Figure 11:</head><label>ParentFigure</label><figDesc>Slice balance steering performance</figDesc><table><row><cell>go</cell><cell>gcc</cell><cell>compress</cell><cell>li</cell><cell>ijpeg</cell><cell>vortex</cell><cell>perl</cell><cell>m88ksim</cell><cell>H-mean</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Ministry of Education of Spain under contract CYCIT TIC98-0511-C02-01 and by the European Union through the ESPRIT program under the MHAOTEU (EP24942) project. The research conducted in this paper has been developed using the resources of the CEPBA. Ramon Canal would like to thank his fellow PBC's, Sandra and Dídac for their patience and precious help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Dynamic Multithreading Processor</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Driscoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st. Int. Symp. on Microarchitecture</title>
				<meeting>of the 31st. Int. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="226" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interconnect Scaling -The Real Limiter to High Performance VLSI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1995 IEEE Int. Electron Devices Meeting</title>
				<meeting>of the 1995 IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<idno>CS-TR-96-1308</idno>
		<title level="m">Evaluating Future Microprocessors: The SimpleScalar Tool Set</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Cost-Effective Clusterd Architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Canal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Parcerisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting>Int. Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="160" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-Program Speculative Multithreading (SPSM) Architecture: Compiler-Assisted Fine-Grained Multithreading</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Parallel Architectures and Compilation Techniques</title>
				<meeting>Int. Conf. on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Multicluster Architecture: Reducing Cycle Time Through Partitioning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Vranesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 30th</title>
				<meeting>of the 30th</meeting>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Register File Considerations in Dynamically Scheduled Processors</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Symp. on High-Performance Computer Architecture</title>
				<meeting>of Int. Symp. on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed Modulo Scheduling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 5th. Int. Symp. on High Performance Comp. Arch</title>
				<meeting>of the 5th. Int. Symp. on High Performance Comp. Arch<address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-01">Jan 1999</date>
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Multiscalar Architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<idno>TR 1196</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences Department, Univ. of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<title level="m">Digital 21264 Sets New Standard</title>
				<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PEWs: A Decentralized Dynamic Scheduler for ILP Processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Parallel Processing</title>
				<meeting>of the Int. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speculative Multithreaded Processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marcuello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tubella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 12th ACM Int. Conf. on Supercomputing</title>
				<meeting>of the 12th ACM Int. Conf. on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Will Physical Scalability Sabotage Performance Gains</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="37" to="39" />
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective Cluster Assignment for Modulo Scheduling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eichenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 31st. Ann. Symp. on Microarchitecture</title>
				<meeting>of the 31st. Ann. Symp. on Microarchitecture<address><addrLine>Dallas, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complexity-Effective Superscalar Processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 24th. Int. Symp. on Comp. Architecture, 1997</title>
				<meeting>of the 24th. Int. Symp. on Comp. Architecture, 1997</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupling Integer Execution in Superscalar Processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th. Ann. Symp. on Microarchitecture</title>
				<meeting>of the 28th. Ann. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trace Processors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the 30th. Ann. Symp. on Microarchitecture</title>
				<meeting>of the 30th. Ann. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting Idle Floating-Point Resources For Integer Execution</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Programming Lang. Design and Implementation</title>
				<meeting>of the Int. Conf. on Programming Lang. Design and Implementation</meeting>
		<imprint>
			<publisher>Montreal</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiscalar Processors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Int. Symp. on Computer Architecture</title>
				<meeting>of the 22nd Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="414" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Superthreaded Architecture: Thread Pipelining with Run-Time Data Dependence Checking and Control Speculation</title>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-C</forename><surname>Yew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Parallel Architectures and Compilation Techniques</title>
				<meeting>Int. Conf. on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Superscalar Instruction Dispatch and Issue by Exploiting Dynamic Code Sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vajapeyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Symp. on Computer Architecture</title>
				<meeting>of the Int. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
