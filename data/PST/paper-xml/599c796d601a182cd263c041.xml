<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Segment Networks for Action Recognition in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<title level="a" type="main">Temporal Segment Networks for Action Recognition in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ADB6A31F504A410C29464CF11AFDEFF2</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2868668</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2868668, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Temporal Segment Networks</term>
					<term>Temporal Modeling</term>
					<term>Good Practices</term>
					<term>ConvNets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the implementation of temporal segment network framework given limited training samples. Our approach obtains the state-the-of-art performance on five challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%), THUMOS14 (80.1%), ActivityNet v1.2 (89.6%), and Kinetics400 (75.7%). In addition, using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the proposed TSN framework, we won the video classification track at the ActivityNet challenge 2016 among 24 teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video-based action recognition has drawn considerable attention from the academic community <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, owing to its applications in many areas like security and behavior analysis. For action recognition in videos, there are two crucial and complementary cues: appearances and temporal dynamics. The performance of a recognition system depends, to a large extent, on whether it is able to extract and utilize relevant information therefrom. However, extracting such information is non-trivial due to a number of difficulties, such as scale variations, view point changes, and camera motions. Thus it becomes crucial to design effective representations to tackle these challenges while learning categorical information of action classes.</p><p>Recently, Convolutional Neural Networks (ConvNets) <ref type="bibr" target="#b5">[6]</ref> have achieved great success in classifying images of objects <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, scenes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, and complex events <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. ConvNets have also been introduced to solve the problem of video-based action recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Deep ConvNets come with excellent modeling capacity and are capable of learning discriminative representations from raw visual data in large-scale supervised datasets (e.g., ImageNet <ref type="bibr" target="#b17">[18]</ref>, Places <ref type="bibr" target="#b9">[10]</ref>). However, unlike image classification, improvement brought by end-to-end deep ConvNets remains limited compared with traditional hand-crafted features for video-based action recognition.</p><p>We argue that the application of ConvNets to action recognition in unconstrained videos is impeded by three major obstacles. First, although long-range temporal structure has been proven crucial for understanding the dynamics in traditional methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, it has not been considered as a critical factor in deep ConvNet frameworks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. These methods usually focus on appearances and short-term motions (i.e., up to 16 frames), thus lacking the capacity to incorporate long-range temporal structure. Recently there are a few attempts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> to deal with this problem. These methods mostly rely on dense temporal sampling with a pre-defined sampling interval, which would incur excessive computational cost when applied to long videos. More importantly, the limited memory space available severely limits the duration of video to be modeled. This poses a risk of missing important information for videos longer than the affordable sampling duration. Second, existing action recognition methods were mostly devised for trimmed videos. However, to deploy the learned action models in a realistic setting, we often need to deal with untrimmed videos (e.g., THUMOS <ref type="bibr" target="#b25">[26]</ref>, Activ-ityNet <ref type="bibr" target="#b26">[27]</ref>), where each action instance may only occupy a small portion of the whole video. The dominating background portions may interfere with the prediction of action recognition models. To mitigate this issue, we need to take account of focusing on action instances and avoiding the influence of background video at the same time. Therefore, it is a non-trivial task to apply the learned action models to action recognition in untrimmed videos.</p><p>Third, training action recognition models often meets a number of practical difficulties: 1) training deep ConvNets usually requires a large volume of training samples to achieve optimal performance. However, publicly available action recognition datasets (e.g., UCF101 <ref type="bibr" target="#b27">[28]</ref>, HMDB51 <ref type="bibr" target="#b28">[29]</ref>) remain limited in both size and diversity, making the model training prone to over-fitting. 2) optical flow extraction to capture short-term motion information becomes a computational bottleneck for deploying the learned models to largescale action recognition datasets.</p><p>These challenges motivate us to study the action recognition problem in this paper from the following three aspects : 1) how to effectively learn video representation that captures longrange temporal structure; 2) how to exploit these learned ConvNet models for the more realistic setting of untrimmed videos; 3) how to efficiently learn the ConvNet models given limited training samples and apply them on large scale data.</p><p>To capture long-range temporal structure, we develop a modular video-level architecture, called temporal segment network (TSN), which provides a conceptually simple, flexible, and general framework for learning action models in videos. It is based on our observation that consecutive frames are highly redundant, where a sparse and global temporal sampling strategy would be more favorable and efficient in this case. The TSN framework first extracts short snippets over a long video sequence with a sparse sampling scheme, where the video is first divided into a fixed number of segments and one snippet is randomly sampled from each segment. Then, a segmental consensus function is employed to aggregate information from the sampled snippets. By this means, temporal segment networks can model long-range temporal structures over the whole video, in a way that its computational cost is independent of the video duration. In practice, we comprehensively study the effect of different segment numbers and propose five aggregation functions to summarize the prediction scores from these sampled snippets, including three basic forms: average pooling, max pooling, and weighted average, as well as two advanced schemes: top-K pooling and adaptive attention weighting. The latter two are designed to automatically highlight discriminative snippets while reducing the impact of less relevant ones during training, thus contribute to a better learned action model.</p><p>To apply the action models learned by TSN to untrimmed videos, we design a hierarchical aggregating strategy, called Multi-scale Temporal Window Integration (M-TWI), to yield the final prediction results for untrimmed videos. Most of previous action recognition methods are constrained to classify manually trimmed video clips. However, this setting may be impractical and unrealistic, as videos on the web are untrimmed by nature and manually trimming these videos is labor demanding. Following the idea of temporal segment network framework, we first divide the untrimmed video into a sequence of short windows of fixed duration. We then perform action recognition for each window independently by max pooling over these snippet-level recognition scores inside this window. Finally, following the aggregation function of temporal segment network framework, we employ the top-K pooling or attention weighting to aggregate the predictions from these windows to produce the video-level recognition results. Due to its capability of implicitly selecting intervals with discriminative action instances while suppressing the influence of noisy background, this newly designed aggregation module is effective for untrimmed video recognition To tackle the practical difficulties in learning and applying action recognition models, we discover a number of good practices to resolve the issues caused by the limited training samples, and perform a systematical study over the input modalities to unleash the full potential of Con-vNets for action recognition. Specifically, we first propose a cross-modality initialization strategy to transfer the learned representations from RGB modality to other modalities like optical flow and RGB difference. Second, we develop a principled method to perform Batch Normalization (BN) in a fine-tuning scenario, denoted as partial BN, where only the mean and variance of first BN layer are updated adaptively to handle domain shift. Moreover, to fully utilize visual content from videos, we empirically study four types of input modalities with our temporal segment network framework, namely a single RGB image, stacked RGB difference, stacked optical flow field, and stacked warped optical flow field. Combining RGB and RGB difference, we build the best-ever real-time action recognition system, which has numerous potential applications in real-world problems.</p><p>We perform experiments on five challenging action recognition datasets, namely HMDB51 <ref type="bibr" target="#b28">[29]</ref>, UCF101 <ref type="bibr" target="#b27">[28]</ref>, THUMOS <ref type="bibr" target="#b25">[26]</ref>, ActivityNet <ref type="bibr" target="#b26">[27]</ref>, and Kinetics <ref type="bibr" target="#b29">[30]</ref>, to verify the effectiveness of our method for action recognition in both trimmed and untrimmed videos. In experiments, models learned using the temporal segment network significantly outperform the state of the art on these four challenging action recognition benchmark datasets. Additionally, following the basic temporal segment network framework, we further improve our action recognition method by introducing the latest deep model architectures (e.g., ResNet <ref type="bibr" target="#b30">[31]</ref> and Inception V3 <ref type="bibr" target="#b31">[32]</ref>), and incorporating the audio as a complementary channel. Our final action recognition method secures the 1st place in untrimmed video classification at the ActivityNet Large Scale Activity Recognition Challenge 2016. We also visualize our learned two-stream models trying to provide insights into how they work. These visualized models also justify the effectiveness of our temporal segment network framework qualitatively.</p><p>Overall, we analyze different aspects of the problems in efficiently and effectively learning and applying action recognition models and make three major contributions: 1) we propose an end-to-end framework, dubbed temporal segment network (TSN), for learning video representation that captures long-term temporal information; 2) we design a hierarchical aggregation scheme to apply action recognition models to untrimmed videos; 3) we investigate a series of good practices for learning and applying deep action recognition models.</p><p>This journal paper extends our previous work <ref type="bibr" target="#b32">[33]</ref> in a number of aspects. First, we introduce new aggregation functions into the temporal segment network framework, which turn out to be effective to highlight important snippets while suppress background noise. Second, we extend the original action recognition pipeline to untrimmed video classification, by designing a hierarchical aggregating strategy. Third, we add more exploration studies on the different aspects of temporal segment network framework and more experimental investigation on three new datasets (i.e., THUMOS, ActivityNet, and Kinetics). Finally, based on our temporal segment network framework, we present an effective and efficient action recognition solution for Ac-tivtyNet Large Scale Activity Challenge 2016, which ranks #1 in untrimmed video classification among 24 teams, and give a detailed analysis on different components of our method to highlight the important ingredients. The code of our method and learned models are publicly available to facilitate future research 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Action recognition has been studied extensively in recent years and readers can refer to <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> for good surveys. Here, we only cover the work related to our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Representation</head><p>For action recognition in videos, the visual representation plays a crucial role. We roughly categorize the related action recognition approaches into two types: methods based on hand-crafted features and those using deeply-learned features.</p><p>Hand-crafted features. In recent years, researchers have developed many different spatio-temporal feature detectors for video, such as 3D-Harris <ref type="bibr" target="#b36">[37]</ref>, 3D-Hessian <ref type="bibr" target="#b37">[38]</ref>, Cuboids <ref type="bibr" target="#b38">[39]</ref>, Dense Trajectories <ref type="bibr" target="#b39">[40]</ref>, Improved Trajectories <ref type="bibr" target="#b1">[2]</ref>. Usually, a local 3D-region is extracted around the interest points or trajectories, and a histogram descriptor is computed to capture the appearance and motion information, such as Histogram of Gradient and Histogram of Flow (HOG/HOF) <ref type="bibr" target="#b40">[41]</ref>, Histogram of Motion Boundary (MBH) <ref type="bibr" target="#b39">[40]</ref>, 3D Histogram of Gradient (HOG3D) <ref type="bibr" target="#b41">[42]</ref>, Extended SURF (ESURF) <ref type="bibr" target="#b37">[38]</ref>, and so on. Then encoding methods are employed to aggregate these local descriptors into a global representation, and typical encoding methods include Bag of Visual Words (BoVW) <ref type="bibr" target="#b42">[43]</ref>, Fisher vector (FV) <ref type="bibr" target="#b43">[44]</ref>, Vector of Locally Aggregated Descriptors (VLAD) <ref type="bibr" target="#b44">[45]</ref>, and Multi-View Super Vector (MVSV) <ref type="bibr" target="#b45">[46]</ref>. These local features share the merits of locality and simplicity, but may lack semantic and discriminative capacity.</p><p>To overcome the limitation of local descriptors, several mid-level representations have been proposed for action recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Raptis et al. <ref type="bibr" target="#b46">[47]</ref> grouped similar trajectories into clusters, each of which was regarded as an action part. Jain et al. <ref type="bibr" target="#b47">[48]</ref> extended the idea of discriminative patches into videos and proposed discriminative spatio-temporal patches for representing videos. Zhang et al. <ref type="bibr" target="#b48">[49]</ref> proposed to discover a set of mid-level patches in a strongly-supervised manner. Similar to 2-D poselet <ref type="bibr" target="#b51">[52]</ref>, they tightly clustered action parts using human joint labeling, dubbed acteme. Wang et al. <ref type="bibr" target="#b2">[3]</ref> proposed a data-driven approach to discover those effective parts with high motion salience, known as motionlet. Zhu et al. <ref type="bibr" target="#b49">[50]</ref> proposed a two-layer acton representation for action recognition. The weakly-supervised actons were learned via a max-margin multi-channel multiple instance learning framework. Wang et al. <ref type="bibr" target="#b22">[23]</ref> proposed a multiple level representation called as MoFAP by concatenating motion features, atoms, and phrases. Sadanand et al. <ref type="bibr" target="#b50">[51]</ref> presented a high-level video representation called as Action Bank by using a set action templates to describe the video content. In summary, these mid-level representations have the merits of representative and discriminative power, but still depends on the low-level hand-crafted features.</p><p>Deeply-learned features. Several works have been trying to learn deep features and design effective ConvNet architectures for action recognition in videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Karpathy et al. <ref type="bibr" target="#b14">[15]</ref> first tested ConvNets with deep structures on a large dataset (Sports-1M). Simonyan et al. <ref type="bibr" target="#b0">[1]</ref> designed two-stream Con-vNets containing spatial and temporal nets by exploiting ImageNet dataset for pre-training and calculating optical flow to explicitly capture motion information. Tran et al. <ref type="bibr" target="#b15">[16]</ref> explored 3D ConvNets <ref type="bibr" target="#b52">[53]</ref> on the realistic and large-scale video datasets, where they tried to learn spatio-temporal features with the operations of 3D convolution and pooling. Carreira et al. <ref type="bibr" target="#b55">[56]</ref> proposed a new Two-Stream Inflated 3D CNNs (I3D) based on 2D CNN inflation, which allows for pre-training with ImageNet models. Sun et al. <ref type="bibr" target="#b53">[54]</ref> proposed a factorized spatio-temporal ConvNets and exploited different ways to decompose 3D convolutional kernels, and Qiu et al. <ref type="bibr" target="#b56">[57]</ref> designed a new Pseudo-3D Residual Networks by implementing spatio-temporal factorization with a residual learning module. Wang et al. <ref type="bibr" target="#b4">[5]</ref> proposed a hybrid representation by using trajectory-pooled deepconvolutional descriptors (TDD), which share the merits of improved trajectories <ref type="bibr" target="#b1">[2]</ref> and two-stream ConvNets <ref type="bibr" target="#b0">[1]</ref>. Feichtenhofer et al. <ref type="bibr" target="#b57">[58]</ref> further extended the two-stream ConvNets with convolutional fusion of two streams. Several works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref> tried to use recurrent neural networks (RNN), in particular LSTM, to model the temporal evolution of frame features for action recognition in videos.</p><p>Our work is related to those deep learning methods. In fact, any existing ConvNet architecture can work with TSN framework, and thus be combined with the proposed sparse sampling strategy and aggregation functions to enhance the modeling capacity with long-range information. Meanwhile, our temporal segment network is an end-to-end architecture, where the model parameters could be jointly optimized with the standard back propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal Structure Modeling</head><p>Many research works have been devoted to modeling the temporal structure of video for action recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. Gaidon et al. <ref type="bibr" target="#b19">[20]</ref> annotated each atomic action for each video and proposed Actom Sequence Model (ASM) for action detection. Niebles et al. <ref type="bibr" target="#b18">[19]</ref> proposed to use latent variables to model the temporal decomposition of complex actions, and resorted to the Latent SVM <ref type="bibr" target="#b60">[61]</ref> to learn the model parameters in an iterative approach. Wang et al. <ref type="bibr" target="#b20">[21]</ref> and Pirsiavash et al. <ref type="bibr" target="#b58">[59]</ref> extended the temporal decomposition of complex action into a hierarchical manner using Latent Hierarchical Model (LHM) and Segmental Grammar Model (SGM), respectively. Wang et al. <ref type="bibr" target="#b59">[60]</ref> designed a sequential skeleton model (SSM) to capture the relations among dynamic-poselets, and performed spatiotemporal action detection. Fernando et al. <ref type="bibr" target="#b21">[22]</ref> modeled the temporal evolution of BoVW representations for action recognition.</p><p>Several recent works focused on modeling long-range temporal structure with ConvNets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>. In general, these methods directly operated on a continuous video frame sequence with recurrent neural networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref> or 3D ConvNets <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Although these methods aim to deal with longer video duration, they usually process sequences of fixed lengths ranging from 5 to 120 frames due to the limit of computational cost and GPU memory. It is still non-trivial for these methods to learn from the entire video due to their limited temporal coverage. Our method differs from these end-to-end deep ConvNets by its novel adoption of a sparse temporal sampling strategy, which enables efficient learning using the entire videos without the limitation of sequence length. Therefore, our temporal segment network is a video-level and end-to-end framework for temporal structure modeling on the entire video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEMPORAL SEGMENT NETWORKS</head><p>In this section, we give a detailed description of our temporal segment network framework. Specifically, we first discuss the motivation of segment based sampling. Then, we introduce the architecture of temporal segment network framework. After this, we present several aggregating functions of temporal segment network and provide analysis on these functions. Finally, we investigate several practical issues for the instantiation of temporal segment network framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segment Based Sampling</head><p>As discussed in Sec. 1, long-range temporal modeling is important for action understanding in videos. The existing deep architectures such as two-stream ConvNets <ref type="bibr" target="#b0">[1]</ref> and 3D convolutional networks <ref type="bibr" target="#b15">[16]</ref> are designed to operate on a single frame or a stack of frames (e.g., 16 frames) with limited temporal durations. Therefore, these structures lack capacity of incorporating long-range temporal information of videos into the learning of action models.</p><p>In order to model long-range temporal structures, several approaches have been proposed to stack more consecutive frames at a fixed sampling rate <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Although this dense and local sampling could help to relieve the problem of the original short-term CovNets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, it still suffers in both computational and modeling aspects. From the computational perspective, it would greatly increase the cost of ConvNet training, as this dense sampling usually requires a large number of frames to capture long-range structures. For example, it totally samples 100 frames in the work of <ref type="bibr" target="#b23">[24]</ref> and 120 frames for the method of <ref type="bibr" target="#b3">[4]</ref>. From the modeling perspective, its temporal coverage is still local and limited by its fixed sampling interval, failing to capture the visual content over the entire video. For instance, the sampled 100 frames <ref type="bibr" target="#b23">[24]</ref> only occupy a small portion of a 10-second video (around 300 frames).</p><p>We observe that although the frames are densely recorded in the videos, the content changes relatively slowly. This motivates us to explore a new paradigm for temporal structure modeling, called segment based sampling. This strategy is essentially a kind of sparse and global sampling method. Concerning the property of spareness, only a small number of sparsely sampled snippets would be used to model the temporal structures in a human action. Normally, the number of sampled frames for one training iteration is fixed to a predefined value independent of the durations of the videos. This guarantees that the computational cost will be constant, regardless of the temporal range we are dealing with. On the global property, our segment based sampling ensures these sampled snippets would distribute uniformly along the temporal dimension. Therefore, no matter how long the action videos will last for, our sampled snippets would always roughly cover the visual content of whole video, enabling us to model the long-range temporal structure throughout the entire video. Based on this paradigm for temporal structure modeling, we propose temporal segment network, a video-level training framework as shown in Figure <ref type="figure" target="#fig_0">1</ref>, which would be explained in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework and Formulation</head><p>We aim to design an effective and efficient video-level framework, coined Temporal Segment Network (TSN), by using a new strategy of segment based sampling. Instead of working on a single frame or a short frame stack, temporal segment networks operate on a sequence of short snippets sampled from the entire video. To make these sampled snippets represent the contents of the whole video while still keeping reasonable computational cost, our segment based sampling first divides the video into several segments of equal duration, and then one snippet is randomly sampled from its corresponding segment. Each snippet in this sequence produces its own snippet-level prediction of the action classes, and a consensus function is designed to aggregate these snippet-level predictions into the videolevel scores. This video-level score is more reliable and informative than the original snippet-level prediction, since it captures the long-range information over the entire video. During the training process, the optimization objectives are defined on the video-level predictions and optimized by iteratively updating the model parameters.</p><p>Formally, given a video V , we divide it into K segments {S 1 , S 2 , • • • , S K } of equal durations. One snippet T k is randomly sampled from its corresponding segment S k . Then, the temporal segment network models a sequence of snippets (T 1 , T 2 , • • • , T K ) as follows:</p><formula xml:id="formula_0">TSN(T 1 , T 2 , • • • , T K ) = H(G(F(T 1 ; W), F(T 2 ; W), • • • , F(T K ; W))).<label>(1)</label></formula><p>Here, the temporal duration of each snippet T k depends on the input modalities and it could be 1 frame for RGB or  and the differentiability allows our temporal segment network framework to be easily optimized using backpropagation. We will provide the details on these consensus functions in the next subsection.</p><note type="other">Segment Based Sampling Segment Aggregation ConvNet ConvNet</note><p>Combining standard categorical cross-entropy loss, the final loss function regarding the segmental consensus</p><formula xml:id="formula_1">G = G(F(T 1 ; W), F(T 2 ; W), • • • , F(T K ; W)) is formed as L(y, G) = - C i=1 y i   g i -log C j=1 exp g j   ,<label>(2)</label></formula><p>where C is the number of action classes, y i the groundtruth label concerning class i, and g j the j th dimension of G.</p><p>During the training phase of our temporal segment network framework, the gradients of the loss value L with respect to model parameters W can be derived as</p><formula xml:id="formula_2">∂L(y, G) ∂W = ∂L ∂G K k=1 ∂G ∂F(T k ) ∂F(T k ) ∂W ,<label>(3)</label></formula><p>where K is number of segments in temporal segment network. When we use a gradient-based optimization method, such as stochastic gradient descent (SGD), to learn the model parameters, Eq. 3 shows that the parameter updates are utilizing the segmental consensus G derived from all snippet-level predictions. In this sense, temporal segment network can learn model parameters from the entire video rather than a short snippet. Furthermore, by fixing K for all videos, we assemble a sparse temporal sampling to select a small number of snippets. It drastically reduces the computational cost for evaluating ConvNets on the frames, compared with previous works using densely sampled frames <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregation Function and Analysis</head><p>As analyzed above, the consensus (aggregation) function is an important component in our temporal segment network framework. In this subsection, we give a detailed description about the design of aggregation functions and derive their gradients with respect to snippet-level prediction scores. We also analyze the properties of different kinds of aggregation functions and provide some modeling insight. Specifically, we propose five types of aggregation functions: max pooling, average pooling, top-K pooling, weighted average, and attention weighting. Max pooling. In this aggregation function, we apply max pooling to the prediction score of each category among the sampled snippets, i.e.,</p><formula xml:id="formula_3">g i = max k∈{1,2,••• ,K} f k i , where f k i is the i th element of F k = F(T k ; W).</formula><p>The gradient of g i with respect to f k i can be easily computed as:</p><formula xml:id="formula_4">∂g i ∂f k i = 1, if k = arg max l f l i , 0, otherwise.<label>(4)</label></formula><p>The basic idea of max pooling is to seek a single and most discriminative snippet for each action class and utilize this strongest activation as the video-level response of this category. Intuitively, it devotes its emphasis to a single snippet, while completely ignoring the responses of other snippets. Thus, this aggregating function encourages temporal segment network to learn from a most discriminative snippet for each action class, but lacks the capacity of jointly modeling multiple snippets for a video-level action understanding.</p><p>Average pooling. One alternative to max pooling aggregation function is the average pooling, where we perform average operation over these snippet-level prediction scores for each class, i.e.,</p><formula xml:id="formula_5">g i = 1 K K k=1 f k i .</formula><p>The gradient of average aggregation function g i withe respect to f k i is derived as follows:</p><formula xml:id="formula_6">∂g i ∂f k i = 1 K .<label>(5)</label></formula><p>The basic intuition behind average pooling is to leverage the responses of all snippets for action recognition, and use their mean activation as the video-level prediction. In this sense, average pooling is able to jointly model multiple snippets and capture the visual information from the whole video. On the other hand, in particular for noisy videos with complex background, some snippets may be actionirrelevant and averaging over these background snippets may hurt the final recognition performance.</p><p>Top-K pooling. To strike a balance between max pooling and average pooling, we propose a new aggregation function, named Top-K pooling. In this aggregation function, we first select K most discriminative snippets for each action category and then perform average pooling over these selected snippets, i.e.,</p><formula xml:id="formula_7">g i = 1 K K k=1 α k f k i</formula><p>, where α k is the indicator of selection, and is set as 1 if selected and otherwise 0. Max pooling and average pooling can be considered as special cases of top-K pooling, where K is set to 1 or K, respectively. Similarly, the gradient of g i with respect to f k i can be computed as follows:</p><formula xml:id="formula_8">∂g i ∂f k i = 1 K , if α k = 1, 0, otherwise.<label>(6)</label></formula><p>Intuitively, this aggregation function is able to determine a subset of discriminative snippets adaptively for different videos. As a result, it shares merits of both max pooling and average pooling, having capacity of jointly modeling multiple relevant snippets while avoiding the influence of background snippets. Linear weighting. In this aggregation function, we aim to perform an element-wise weighted linear combination on the prediction score for each action category. Specifically, we define the aggregation function as</p><formula xml:id="formula_9">g i = K k=1 ω k f k i</formula><p>, where w k is the weight for the k th snippet. In this aggregation function, we introduce a model parameter ω and compute the gradients of g i with respect to f k i and w k as follows:</p><formula xml:id="formula_10">∂g i ∂f k i = ω k , ∂g i ∂ω k = f k i .<label>(7)</label></formula><p>In practice, we use this equation to update the network weights W and the combination weights ω alternatively.</p><p>The basic assumption underlying this aggregation function is that action can be decomposed into several phases and these different phases may play different roles in recognizing action classes. This aggregation function is expected to learn importance weights of different phases of an action class. Compared with previous pooling based aggregation functions, this linear weighting acts as a soft version of snippet selection. Attention weighting. It is obvious that the above linear weighting scheme is data independent, thus lacking the capacity of considering the difference between videos. To overcome this limitation, we propose an adaptive weighting method, called attention weighting. In this aggregation function, we aim to learn a function to automatically assign an importance weight to each snippet according to the video content. Formally, the aggregation function is defined as</p><formula xml:id="formula_11">g i = K k=1 A(T k )f k i ,</formula><p>where A(T k ) is the attention weight for snippet T k and calculated according to video content adaptively. Within this formulation, we could calculate the gradient of g i with respect to f k i and A(T k ) as follows:</p><formula xml:id="formula_12">∂g i ∂f k i = A(T k ), ∂g i ∂A(T k ) = f k i .<label>(8)</label></formula><p>In this attention weighting scheme, the design of attention weighting function A(T k ) is crucial for final performance. In the current implementation, we first extract </p><formula xml:id="formula_13">e k = ω att R(T k ), A(T k ) = exp(e k ) K l=1 exp(e l ) ,<label>(9)</label></formula><p>where ω att is the parameter of attention weighting function and will be learned jointly with network weights W. Here R(T k ) is the visual feature for the k th snippet. Currently it is the activation of last hidden layer. Within this formation, we can calculate the gradient of A(T k ) with respect to attention model parameter ω att as:</p><formula xml:id="formula_14">∂A(T k ) ∂ω att = K l=1 ∂A(T k ) ∂e l R(T l ),<label>(10)</label></formula><p>where the gradient of</p><formula xml:id="formula_15">∂A(T k ) ∂e l</formula><p>is computed as:</p><formula xml:id="formula_16">∂A(T k ) ∂e l = A(T k )(1 -A(T l )), if l = k, -A(T k )A(T l ), otherwise.<label>(11)</label></formula><p>Having this gradient formula, we can learn the attention model parameters ω att using back-propagation together with the ConvNet parameters W. In addition, due to the introduction of attention model A(T k ), the basic backpropagation formula in Eq. 3 should be rectified as follows: </p><formula xml:id="formula_17">∂L(y, G) ∂W = ∂L ∂G K k=1 ∂G ∂F(T k ) ∂F(T k ) ∂W + ∂G ∂A(T k ) ∂A(T k ) ∂W .<label>(12</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TSN in Practice</head><p>Temporal segment network (TSN) provides a general framework to perform video-level learning. In order to train TSN models to achieve optimal performance, a few practical issues have to be taken into account. To this end, we study a series of practical matters from the aspects of TSN architectures, TSN inputs, and TSN training.</p><p>TSN Architectures. Our TSN is a general and flexible framework for video-level learning. To demonstrate the generality of our approach, we instantiate TSN with multiple network architectures. Specifically, for ablation studies on standard action recognition benchmarks, we choose the Inception architecture with Batch Normalization (BN-Inception) <ref type="bibr" target="#b61">[62]</ref> due to its good balance between accuracy and efficiency. Compared with other ConvNet architectures deployed in videos <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, this architecture is equipped with better modeling capacity, allowing to demonstrate the improvement of TSN against a strong baseline. In the ActivtyNet Challenge 2016, we investigate more powerful architectures including the Inception V3 <ref type="bibr" target="#b31">[32]</ref> and ResNet-152 <ref type="bibr" target="#b30">[31]</ref>, to fully unleash the potential of TSN framework in video classification. In addition, we also instantiate the TSN with the architecture of I3D <ref type="bibr" target="#b55">[56]</ref> on the Kinetics dataset, which unifies the short-term modeling (3D convolution and pooling) and long-term training.</p><p>TSN Inputs. Unlike static images, the additional temporal dimension of videos delivers another important cue for action understanding, namely motion. In <ref type="bibr" target="#b0">[1]</ref>, using dense optical flow fields as the source of motion representation is proven to be effective. In this work, we extend this approach in two aspects, namely accuracy and speed. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, in addition to the original input modalities of RGB and optical flow <ref type="bibr" target="#b0">[1]</ref>, we also investigate two other modalities: warped optical flow and RGB differences.</p><p>1) Warped Optical Flow. Inspired by the work of improved dense trajectories <ref type="bibr" target="#b1">[2]</ref>, we investigate using warped optical flow fields as the source for motion modeling. Warped optical flow fields are known to be robust to camera motion and help concentrate on human motion. We expect this to help to improve the accuracy in motion perception and thus boost the action recognition performance.</p><p>2) RGB Differences. Despite the superior recognition accuracy, one issue that impedes the application of two-stream based approaches is the tremendous time cost of optical flow extraction. To address this problem, we build a motion representation without optical flow. Inspired by the success of frame volumes <ref type="bibr" target="#b15">[16]</ref> and motion vector <ref type="bibr" target="#b16">[17]</ref> in motion modeling, we revisit the simplest cues for apparent motion perception: the stacked differences of RGB pixel intensities between consecutive frames. Recalling the seminal work on dense optical flow in <ref type="bibr" target="#b62">[63]</ref>, the partial derivatives of pixel intensities with respect to time play critical roles in computing optical flow. It is reasonable to hypothesize that the power of optical flow in representing motion could be learned from the simple cues of RGB differences. This motivates us to investigate using RGB differences as the input of the temporal stream, which greatly saves the time of optical flow extraction.</p><p>TSN Training. As discussed before, existing human annotated datasets for action recognition are limited in terms of sizes. In practice, training deep ConvNets on these datasets are prone to over-fitting. To mitigate this issue, we design several strategies to improve the training in the temporal segment network framework.</p><p>1) Cross Modality Initialization. Pre-training the network parameters on large-scale image recognition datasets, such as ImageNet <ref type="bibr" target="#b17">[18]</ref>, has turned out to be an effective remedy when the target dataset does not have enough training samples <ref type="bibr" target="#b0">[1]</ref>. As spatial networks take RGB images as inputs, it is natural to exploit models trained on the Im-ageNet as initialization. For other input modalities such as optical flow and RGB difference, we come up with a cross modality initialization strategy. Specifically, we first discretize optical flow fields into the interval of 0 to 255 by linear transformation. Then, we average the weights of pretrained RGB models across the RGB channels in the first layer and replicate the mean by the channel number of temporal network input. Finally, the weights of remaining layers of the temporal network are directly copied from the pretrained RGB networks.</p><p>2) Regularization. Batch Normalization <ref type="bibr" target="#b61">[62]</ref> is able to deal with the problem of covariate shift by estimating the activation mean and variance within each batch to normalize these activation values. This operation speeds up the convergence of training, but also increases the risk of over-fitting in the transfer learning process, due to the biased estimation of mean and variance from a limited number of training samples in target dataset. Therefore, after initialization with pretrained models, we choose to freeze the mean and variance parameters of all Batch Normalization layers except the first one. As the distribution of optical flow is different from the RGB images, the activation value of first convolution layer will have a distinct distribution and we need to re-estimate the mean and variance accordingly. We call this strategy partial BN. Meanwhile, we add an extra dropout layer with high dropout ratio (set as 0.8 in experiment) after the global pooling layer to further reduce the effect of over-fitting.</p><p>3) Data Augmentation. In the original two-stream Con-vNets <ref type="bibr" target="#b0">[1]</ref>, random cropping and horizontal flipping are employed to augment training samples. We exploit two new data augmentation techniques: corner cropping and scalejittering. In the corner cropping technique, the extracted regions are only selected from the corners or the center of an image to avoid implicitly focusing more on the center area. In the multi-scale cropping technique, we adapt the scale jittering technique <ref type="bibr" target="#b7">[8]</ref> used in ImageNet classification to action recognition. We present an efficient implementation of scale jittering. We fix the input size as 256 × 340, and the width and height of cropped regions are randomly selected from {256, 224, 192, 168}. Finally, these cropped regions will be resized to 224 × 224 for network training. In fact, this implementation not only contains scale jittering, but also involves aspect ratio jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACTION RECOGNITION WITH TSN MODELS</head><p>With the principled framework of temporal segment networks, there still remains the question of how to use the models learned with this framework to recognize actions in realistic videos. In this section, we describe how to apply action models under two different conditions: trimmed videos and untrimmed videos, and devise a series of techniques in order to improve the robustness of action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Action Recognition in Trimmed Video</head><p>In trimmed videos, action instances are manually cropped from the long video sequences and thereby action recognition could be simply cast as a classification problem. Due to the fact that all snippet-level ConvNets share the model parameters in temporal segment networks, the learned models can perform frame-wise evaluation as normal ConvNets <ref type="bibr" target="#b0">[1]</ref>. This also allows us to carry out fair comparison with models learned without the temporal segment network framework. Specifically, we follow the testing scheme of the original two-stream ConvNets <ref type="bibr" target="#b0">[1]</ref>, where we sample 25 snippets of different modalities. Meanwhile, we crop 4 corners and 1 center, and their horizontal flipping from the sampled snippets to evaluate the ConvNets. We use average pooling to aggregate the predictions of different crops and snippets. For the fusion of predictions from multiple modalities, we take a weighted average of them, where the fusion weights are determined empirically. It is described in Sec. 3.2 that the segmental consensus function is applied before the Softmax normalization. To test the models in compliance with their training, we fuse the prediction scores of 25 frames and different streams before Softmax normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Action Recognition in Untrimmed Videos</head><p>The major obstacle for action recognition in untrimmed videos is the large portion of irrelevant content in the input videos. Since our action models are trained on trimmed action clips, reusing the technique used for trimmed video, i.e., simply averaging scores from every location in a video, has a high risk of factoring in the unpredictable responses of the models on background contents. This makes it necessary to design a specialized method for applying the trained action recognition models to untrimmed videos. For this purpose, we start by summarizing the following challenges posed by untrimmed videos.</p><p>• Location issue: an action clip can appear at any temporal location of the video.</p><p>• Duration issue: the action clip can be either longlasting or ephemeral.</p><p>• Background issue: the irrelevant content in a video can have high variations and can possibly occupy a large portion of the whole duration of a video.</p><p>To deal with these challenges, we develop a detection based method to apply action models to untrimmed videos. First, to cover any location that the action instance can reside, we sample snippets from the input videos in a fixed sampling rate (e.g., 1FPS). A trained TSN model is then evaluated on these sampled snippets. Then, in order to cover the highly varying durations of action clips, a series of temporal sliding windows with different sizes are then applied on the frame scores. The maximum scores of the classes within a window are used to represent it. To alleviate the interference of background contents, windows with the same length are then aggregated with a top-K pooling scheme. The aggregation results from different window sizes then vote for the final prediction of the whole video.</p><p>Formally, for a video in length of M seconds, we will obtain M snippets {T 1 , . . . , T M }. Applying the TSN model, we will obtain class scores F(T m ) for the snippet T m . We then build temporal sliding windows with the size of l ∈ {1, 2, 4, 8, 16}. The windows will slide through the whole duration of videos, with a stride of 0.8 × l. For a window position starting at the s th second, a series of snippets will be covered as {T s+1 , . . . , T s+l }, with their class scores {F(T s+1 ), . . . , F(T s+l )}. The class scores for this window F s,l can be calculated by:</p><formula xml:id="formula_18">F s,l i = max p∈{1,2...,l} {f s+p i }.<label>(13)</label></formula><p>In this way, for size l we will obtain N l windows, where N l = M 0.8l . Then we apply the aforementioned top-K pooling scheme to obtain the consensus G l of from these N l windows of size l. Here the parameter K is determined as K = max(15, N l /4 ). This gives us 5 sets of class scores for window size l ∈ {1, 2, 4, 8, 16}. The final score is then calculated as P = 1 5 l∈{1,2,4,8,16} G l , which is the average of the five window sizes. We term this video classification technique as Multi-scale Temporal Window Integration, abbreviated as M-TWI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first introduce the evaluation datasets and implementation details of our approach. Then we discuss the practical issues for action recognition with deep learning and our proposed good practices to mitigate them. After dealing with these issues, we provide detailed analysis of the proposed temporal segment network framework, to demonstrate the importance of modeling long-term temporal structures. Finally, we compare the performance of our method with the state of the art on the four action recognition benchmarks. We also present the results of our approach in the ActivityNet challenge 2016 and describe the winner solution to this challenge. Additionally, we visualize our learned ConvNet models to help qualitatively justify the performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The datasets adopted to evaluate the performance of temporal segment network framework are from two types of videos, i.e., trimmed videos and untrimmed videos. Now we describe the details of these datasets.</p><p>Trimmed Video Datasets. We conduct experiments on three standard action recognition datasets of trimmed videos, namely HMDB51 <ref type="bibr" target="#b28">[29]</ref>, UCF101 <ref type="bibr" target="#b27">[28]</ref>, and Kinetic-s400 <ref type="bibr" target="#b29">[30]</ref>. The UCF101 dataset contains 101 action classes and 13, 320 video clips. We follow the evaluation scheme of the THUMOS13 challenge <ref type="bibr" target="#b63">[64]</ref> and adopt the three training/testing splits for evaluation. The HMDB51 dataset is a large collection of realistic videos from various sources, such as movies and web videos. The dataset is composed of 6, 766 video clips from 51 action categories. Our experiments follow the original evaluation scheme using three training/testing splits and report average accuracy over these splits. The Kinetics400 dataset is the largest welllabeled action recognition dataset. Its current version contains 400 action classes and each category has at least 400 videos. In total, there are around 240, 000 training videos, 20, 000 validation videos, and 40, 000 testing videos. The evaluation metric on the Kinetics dataset is the top-1 and top-5 accuracy.</p><p>Untrimmed Video Datasets. We conduct experiments of untrimmed video action recognition on two publicly available large-scale datasets. The first is the THUMOS14 <ref type="bibr" target="#b64">[65]</ref>. It has 101 classes of human actions. This dataset is composed of training set, validation set, testing set, and background set. We use the training set (UCF101) and validation set (1, 010 videos) for TSN training and evaluate the learned models on its testing set, which has 1, 575 videos. The second dataset for untrimmed videos is the ActivityNet <ref type="bibr" target="#b26">[27]</ref> dataset. We use its release version 1.2, termed as ActivityNet v1.2. The ActivityNet v1.2 dataset has 100 classes of human activities. It consists of 4, 819 videos for training, 2, 383 videos for validation, and 2, 480 videos for testing. We follow the standard splits to train and evaluate the our TSN framework. On both datasets for untrimmed videos, the evaluation metric is mean average precision (mAP) for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We use the mini-batch stochastic gradient descent algorithm to learn the network parameters, where the batch size is set to 128 and momentum set to 0.9. We initialize network weights with pre-trained models from ImageNet <ref type="bibr" target="#b17">[18]</ref>. We set a smaller learning rate in our experiments. On the dataset of UCF101, for spatial networks, the learning rate is initialized as 0.001 and decreases to its 1  10 every 1, 500 iterations. The whole training procedure stops at 3, 500 iterations. For temporal networks, we initialize the learning rate as 0.005, which reduces to its 1  10 after 12, 000 and 18, 000 iterations. The maximum iteration is set as 20, 000. To speed up training, we employ a data-parallel strategy with multiple GPUs, implemented with our modified version of Caffe <ref type="bibr" target="#b65">[66]</ref> and OpenMPI 2 . The whole training time on UCF101 is around 0.6 hours for spatial TSNs and 8 hours for temporal TSNs with 8 TITANX GPUs. For other datasets such as HMDB51, THUMOS14, ActivityNet, the learning process is the same with that of UCF101, except that the iteration numbers are adjusted according to the dataset sizes. Concerning data augmentation, we use the techniques of location jittering, horizontal flipping, corner cropping, and scale jittering, as specified in Section 3.4. For the extraction of optical flow and warped optical flow, we choose the TVL1 optical flow algorithm <ref type="bibr" target="#b66">[67]</ref> implemented by OpenCV with CUDA. If not specifically noted, the experiments in the section are conducted with BN-Inception <ref type="bibr" target="#b61">[62]</ref> as the underlying CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effectiveness of the Proposed Practices</head><p>In this section, we focus on investigating the effect of the good practices described in Sec. 3.4, including the training strategies and the input modalities. In this exploration study, we use the two-stream ConvNets with very deep architecture adapted from <ref type="bibr" target="#b61">[62]</ref>.</p><p>Different learning strategy. Compared with the original two-stream ConvNets <ref type="bibr" target="#b0">[1]</ref>, we propose two new training strategies in Section 3.4, namely cross modality pre-training and partial BN with dropout. Specifically, we compare four settings on the split1 of UCF101: (1) training from scratch;</p><p>(2) only pre-train spatial stream as in <ref type="bibr" target="#b0">[1]</ref>; <ref type="bibr" target="#b2">(3)</ref>  TABLE 2 Exploration of different combination of modalities with TSN on the UCF101 dataset (over three splits). In this table, "RGB" refers to the RGB video frame stream. "Flow" refers to modality of optical flow input. "Warp" refers to the modality of warped optical flow. "RGB Diff." refers to the modality using differences of RGB video frames. The speed for testing is evaluated on a TitanX GPU. In the lower half of the training from scratch is much worse than that of the original two-stream ConvNets (baseline), which implies carefully designed learning strategy is necessary to reduce the risk of over-fitting, especially for spatial networks. Then, we resort to the pre-training of the spatial stream and cross modality pre-training of the temporal stream to help initialize two-stream ConvNets and it achieves better performance than the baseline. We further utilize the partial BN with dropout to regularize the training procedure, which boosts the recognition performance to 92.0%. We also perform a comparative experiment to verify the effectiveness of corner cropping by cropping regions in a 3 × 3 grid. We report the performance in the last row of Table <ref type="table" target="#tab_2">1</ref> and its result is slightly worse than corner cropping. Therefore, in the remaining experiments, we employ all these good practices for model training. Different input modalities. We propose two new types of modalities in Section 3.4: RGB difference and warped optical flow fields. We try combining different modalities and report the results in Table <ref type="table">2</ref>. These experiments are carried out with all the good practices verified in Table <ref type="table" target="#tab_2">1</ref>. We perform multiple experiments with or without TSN (7 segments) to investigate the performance of different input modalities. We first observe that RGB and optical flow, which is the basic combination in the two-stream ConvNets also works well with TSN, yielding recognition accuracy of 94.9%. Then we observe that the warped optical flow </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Study on Temporal Segment Networks</head><p>In this subsection, we focus on studying the effectiveness of temporal network framework. As described in Sec 3, the TSN framework is based on segment based sampling, which provides an efficient and effective scheme for video-level learning. We first compare segment based sampling with the regular sampling method <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref> on the dataset of UCF101, to demonstrate the effectiveness of segment based sampling. In addition, the TSN framework has two other critical components: the sparse snippet sampling scheme and the segment consensus (aggregation) functions. To analyze the TSN framework in-depth, we first explore the effect of segment number and then analyze different consensus (aggregation) functions. These experiments are performed on the datasets of UCF101 and ActivityNet, to reflect the scenarios of both trimmed and untrimmed video action recognition. Finally, to further demonstrate the importance of TSN in long-range modeling, we also compare the performance of TSN with other very deep network architectures on the UCF101 dataset. Study on segment based sampling. Our proposed segment based sampling is a global and sparse sampling method. Compared with those regular sampling methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref>, it aims to model long-term temporal structure in a more efficient and effective way. Here we perform an experimental comparison with the regular sampling method. Specifically, for regular sampling, we select T snippets (each snippet has a single frame for RGB and 5 stacked frames for Flow) at a fixed sampling stride τ . For fair comparison, the remaining implementation details are kept the same, such as sampled snippet number, training strategy, and snippet-level score fusion.</p><p>The experimental results are summarized in Table <ref type="table" target="#tab_4">3</ref>. In this experiment, we keep the number of sampled snippets as 3 and vary the sampling stride τ from 1 to 20. We see that increasing sampling stride would improve the performance of regular sampling (from 92.1% to 93.7%), as a larger sampling stride contributes to longer-term modeling. However, the performance of regular sampling is still lower than that of segment based sampling (93.7% vs. 94.2%). We analyze that our proposed segment based sampling is adaptive to the video duration and capable of describing the visual content of entire video.</p><p>Evaluation on segment number. The most crucial parameter governing the sparse snippet sampling scheme in TSN is the number of segments K. When K equals to 1, TSN degenerates to the plain two-stream ConvNets. Increasing K is expected to improve the recognition performance of the learned models. In experiments, we vary the number of K from 1 to 9 and evaluate the recognition performance using the same test approaches.</p><p>The results are summarized in Table <ref type="table" target="#tab_5">4</ref>. We observe that increasing the number of segments will generally lead to better performance. For example, the performance of TSN with 7 segments is better than that of TSN with 3 segments (94.9% vs. 94.2%). This improvement implies that using more temporal segments will help to capture richer information to better model temporal structure of the whole video . However, when the segment number K increases from 5 to 9, it brings a very small improvement. Thus, to strike a balance between recognition performance and computational burden, we set K = 7 in the following experiments.</p><p>Evaluation on aggregation function. In Eq. ( <ref type="formula" target="#formula_0">1</ref>), a segmental consensus function is defined by its aggregation function G, which could be crucial to the final recognition performance. Here we evaluate five candidates, including the relatively basic: (1) max pooling, (2) average pooling, (3) weighted average, and the more complex: (4) top-K pooling and (5) attention weighting, for the form of G.</p><p>The experimental results are summarized in Table <ref type="table" target="#tab_6">5</ref>. On UCF101, which consists of trimmed human action videos, the average aggregation function achieves the best performance, and the weight average and attention weighting obtain quite similar performance. On ActivityNet, the top-K and attention weighting aggregation functions achieve comparable performance, which slightly outperforms (0.4%) the basic ones such as average pooling. This fact suggests that on datasets with more complex and diverse temporal structure, the advanced aggregation functions will lead to better recognition accuracies. In this sense, we default to average pooling for short videos (HMDB51 and UCF101) and top-K pooling for complex videos (ActivityNet) in later experiments.</p><p>Comparison of CNN architectures. We have conducted previous experiments mostly with the BN-Inception architecture. Here we compare the performance of different network architectures on the UCF101 dataset and the results are summarized in Table <ref type="table" target="#tab_7">6</ref>. We use K = 1 in these experiments, which is equivalent to the original two-stream ConvNets. Specifically, we compare the performance of four very deep architectures: BN-Inception <ref type="bibr" target="#b61">[62]</ref>, GoogLeNet <ref type="bibr" target="#b8">[9]</ref>, VGGNet-16 <ref type="bibr" target="#b7">[8]</ref>, and ResNet-152 <ref type="bibr" target="#b30">[31]</ref>. The results of different architectures are directly cited from the corresponding references Among the compared architectures, the very deep twostream ConvNets adapted from BN-Inception <ref type="bibr" target="#b61">[62]</ref> achieves    the best accuracy of 92.4%, which is still better than the ResNet-152 by 0.6%. This performance improvement may be ascribed to the good practices proposed by our approach. Furthermore, when trained with TSN (K = 7), the accuracy is boosted to 94.9%. This clearly justifies the effectiveness of modeling long-range temporal structures with TSN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with the State of the Art</head><p>After analyzing the effect of the components in temporal segment networks and coming to a reasonable setting, we now compare our action recognition approach against the state-of-the-art methods on both trimmed videos and untrimmed videos. We conduct experiments on four action recognition datasets. The first two, HMDB51 and UCF101, are composed of trimmed videos. The last two, THUMOS14 and ActivityNet v1.2, consist of untrimmed videos. We expect the experimental results on these datasets would provide a thorough comparison with the existing state-ofthe-art methods. In experiments, we use the RGB and optical flow modalities to make fair comparison with previous methods. Trimmed Video Datasets. We experiment on two challenging trimmed video datasets: HMDB51 and UCF101. The results are summarized in the left columns of Table <ref type="table" target="#tab_10">9</ref>, where we compare our method with both traditional approaches such as improved dense trajectories (iDTs) <ref type="bibr" target="#b1">[2]</ref>, MoFAP representations <ref type="bibr" target="#b22">[23]</ref>, and deep learning representations, such as 3D convolutional networks (C3D) <ref type="bibr" target="#b15">[16]</ref>, trajectory-pooled deep-convolutional descriptors (TDD) <ref type="bibr" target="#b4">[5]</ref>, factorized spatiotemporal convolutional networks (F ST CN) <ref type="bibr" target="#b53">[54]</ref>, long term convolution networks (LTC) <ref type="bibr" target="#b23">[24]</ref>, and key volume mining framework (KVMF) <ref type="bibr" target="#b76">[77]</ref>. We present the results of TSN with 3 and 7 segments with the average aggregation function. We fuse the prediction scores of RGB and optical flow modalities with equal weights (1:1). Our best results outperform other methods by 5.5% on the HMDB51 dataset, and 1.8% on the UCF101 dataset. The superior performance of our method demonstrates the effectiveness of temporal segment network on trimmed videos and the importance of effective long-term temporal modeling. We also report the performance on the recent Kinetics dataset and build the TSN with I3D architecture, which combines the merits of short-term and long-term modeling. We see that against the very competitive I3D method, our TSN training is still able to improve the performance from 74.9% to 75.7%.</p><p>Untrimmed Video Datasets. We also compare our approach with other methods on two untrimmed video datasets: THUMOS14 and ActivityNet v1.2. The results are summarized in the right columns of Table <ref type="table" target="#tab_10">9</ref>. We 59.4% Two Stream <ref type="bibr" target="#b0">[1]</ref> 88.0% Two Stream <ref type="bibr" target="#b0">[1]</ref> 66.1% Two Stream <ref type="bibr" target="#b0">[1]</ref> 71.9% Two Stream <ref type="bibr" target="#b0">[1]</ref> 61.0% / VideoDarwin <ref type="bibr" target="#b21">[22]</ref> 63.7% C3D (3 nets) <ref type="bibr" target="#b15">[16]</ref> 85.2% EMV+RGB <ref type="bibr" target="#b16">[17]</ref> 61.5% C3D <ref type="bibr" target="#b15">[16]</ref> 74.1% Two Stream I3D <ref type="bibr" target="#b55">[56]</ref> 74.9% / 91.8% MPR <ref type="bibr" target="#b75">[76]</ref> 65.5% Two stream +LSTM <ref type="bibr" target="#b3">[4]</ref> 88.6% FSTCN <ref type="bibr" target="#b53">[54]</ref> 59.1% FSTCN <ref type="bibr" target="#b53">[54]</ref> 88.1% TDD+FV <ref type="bibr" target="#b4">[5]</ref> 63.2% TDD+FV <ref type="bibr" target="#b4">[5]</ref> 90.3% LTC <ref type="bibr" target="#b23">[24]</ref> 64.8% LTC <ref type="bibr" target="#b23">[24]</ref> 91.7% KVMF <ref type="bibr" target="#b76">[77]</ref> 63 compare TSN with the existing methods for untrimmed video action recognition, including improved dense trajectories (iDTs) <ref type="bibr" target="#b1">[2]</ref>, two-stream ConvNet <ref type="bibr" target="#b0">[1]</ref>, enhanced motion vectors <ref type="bibr" target="#b16">[17]</ref>, 3D convolutional networks <ref type="bibr" target="#b15">[16]</ref>, objec-t+motion <ref type="bibr" target="#b72">[73]</ref>, and Depth2Action <ref type="bibr" target="#b73">[74]</ref>. We also present the results of TSN with segment numbers of 3 and 7 and the aggregation function in TSN is top-K pooling. Our approach clearly outperforms these compared methods. For example, our TSN (7 seg) is better than the previous best performance by 8.5% on the THUMOS14 dataset and 11.5% on the Activ-ityNet dataset. This confirms that models learned with TSN also perform well in untrimmed videos, given a reasonable testing scheme, as described in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">ActivityNet Challenge 2016</head><p>The power of the temporal segment network framework is further verified in the ActivityNet large scale activity recognition challenge 2016. In this challenge we use the videos from the ActivityNet <ref type="bibr" target="#b26">[27]</ref> version 1.3 for training and testing. In Particular, we train TSN models using the trimmed activity instances from the ActivityNet v1.3. To test the models, we follow the approach described in Sec. 4.2.</p><p>Understanding that the underlying CNN architecture plays an important role in boosting the performance, we also instantiate TSN with the ultra-deep Inception V3 <ref type="bibr" target="#b31">[32]</ref> and ResNet <ref type="bibr" target="#b30">[31]</ref> architectures.</p><p>To evaluate the performance of TSN, we experiment with two settings. First we train the models on the "training" subset of ActivityNet v1.3 and test the recognition accuracy in terms of mean average precision (mAP) on the "validation" subset. In the second setting, we train the models with both "training" and "validation" subsets and test the recognition accuracy on the "testing" subset. The mAP values on the "testing" subset are reported by the publicly available test server of the challenge 4 . The results on validation set are summarized in Table <ref type="table" target="#tab_8">7</ref>. We observe that TSN significantly boosts the performance over plain two-stream ConvNets (from 78.9% to 84.7%). The performance gain is further amplified by using deep CNN architectures such as Inception V3. Also, the advanced aggregation function such as Top-K pooling leads to even better performance. After all, we find that models trained with different aggregation functions (i.e., average pooling, Top-K pooling, attention weighting) and CNN architectures (i.e., Inception V3, ResNet-152) are 4. http://activity-net.org/challenges/2016/evaluation.html complementary when combining into an ensemble, leading to an mAP value of 89.7%.</p><p>Challenge solution and result. The results on the testing set are summarized in Table <ref type="table" target="#tab_9">8</ref>. Out entry "CES" ranks first among all 24 challenge participants with an mAP of 93.23% on the testing set. The submission is an ensemble of TSN models trained on training and validation data with Inception V3 and ResNet-152 architectures, and the audio models <ref type="bibr" target="#b78">[79]</ref> trained on audio signals of the videos. For references we also list the results from other participants of this challenge in Table <ref type="table" target="#tab_9">8</ref>. It is worth noting that thanks to the high efficiency of TSN, our models in the challenge can be trained within 10 hours on a single node with 8 TitanX GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Model Visualization</head><p>Besides recognition accuracies, we would like to attain further insight into the learned ConvNet models. In this sense, we adopt the DeepDraw <ref type="bibr" target="#b77">[78]</ref> toolbox. This tool conducts iterative gradient ascent on input images with only white noises. Thus the output after a number of iterations can be considered as class visualization based solely on class knowledge inside the ConvNet model. The original version of the tool only deals with RGB data. To conduct visualization on optical flow based models, we adapt the tool to work with our temporal ConvNets. As a result, we for the first time visualize interesting class information in two-stream ConvNet models. We randomly pick five classes from the UCF101 dataset, Taichi, Punch, Diving, Long Jump, and Biking, and five classes from the ActivityNet dataset, Poole Vault, Zumba, Skate Board, Rock Climb, and Cheer Leading. The results are shown in Fig. <ref type="figure">3</ref>. For both RGB and optical flow, we visualize the ConvNet models learned with following two settings: (1) training without temporal segment networkand <ref type="bibr" target="#b1">(2)</ref> training with temporal segment network.</p><p>It is easy to notice that the models, trained with only short-term information such as a single frame, tend to focus more on the scenery patterns and objects in the videos. For example, in the class "Diving", the single-frame spatial stream ConvNet pays much attention on diving platforms while the person performing diving is less clear than these platforms. With the training of temporal segment network, the spatial ConvNet is able to generate an image that human is the major visual information and different poses can be identified. Its temporal stream counterpart, working on optical flow without temporal segment network, tends to focus on the noisy motion pattern. With long-term temporal</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Temporal segment network: One input video is divided into K segments (here we show the K = 3 case) and a short snippet is randomly selected from each segment. The snippets are represented by modalities such as RGB frames, optical flow (upper grayscale images), and RGB differences (lower grayscale images). For presentation clarity, we The class scores of different snippets are fused by an the segmental consensus function to yield segmental consensus, which is a video-level prediction. Predictions from all modalities are fused to produce the final prediction. ConvNets on all snippets share parameters.</figDesc><graphic coords="5,240.72,66.45,247.94,170.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of four types of input modality: RGB images, RGB difference, optical flow fields (x,y directions), and warped optical flow fields (x,y directions) visual feature R = R(T k ) from each snippet with the same ConvNet and then produce the attention weights as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) Overall, the advantages of introducing attention model A(T k ) come from two aspects: (1) The attention model enhances the modeling capacity of our temporal segment network framework by automatically estimating the importance weight of each snippet based on the video content. (2) Due to the fact that the attention model is based on ConvNet representations R, it leverages extra backpropagation information to guide the learning process of ConvNet parameter W and may accelerate the convergence of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>with cross modality pre-training; (4) combination of cross modality pre-training and partial BN with dropout. The results are summarized in Table 1. First, we see that the performance of Exploration of different training strategies for two-stream ConvNets on the UCF101 dataset (split 1). Here, "from scratch" refers to the case we initialize the CNN parameters with Gaussian distribution. "pre-train spatial" means only pre-training spatial stream CNN while training temporal stream CNN from scratch. Experiments here are conducted without TSN.</figDesc><table><row><cell>Training setting</cell><cell cols="3">Spatial Temporal Two-Stream</cell></row><row><cell>Baseline [1]</cell><cell>72.7%</cell><cell>81.0%</cell><cell>87.0%</cell></row><row><cell>From Scratch</cell><cell>48.7%</cell><cell>81.7%</cell><cell>82.9%</cell></row><row><cell>Pre-train Spatial</cell><cell>84.1%</cell><cell>81.7%</cell><cell>90.0%</cell></row><row><cell>+ Cross modality pre-training</cell><cell>84.1%</cell><cell>86.6%</cell><cell>91.5%</cell></row><row><cell>+ Partial BN with dropout</cell><cell>84.5%</cell><cell>87.2%</cell><cell>92.0%</cell></row><row><cell>without corner cropping</cell><cell>84.2%</cell><cell>86.8%</cell><cell>91.8%</cell></row></table><note><p>2. https://github.com/yjxiong/caffe</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>table, we compare "RGB+RGB Diff." with other real-time action recognition methods (FPS &gt; 30).</figDesc><table><row><cell>Modalities</cell><cell cols="3">TSN Accuracy Speed (FPS)</cell></row><row><cell>RGB+Flow</cell><cell>No</cell><cell>92.4%</cell><cell>14</cell></row><row><cell>RGB+Flow</cell><cell>Yes</cell><cell>94.9%</cell><cell>14</cell></row><row><cell>RGB+Flow+Warp</cell><cell>Yes</cell><cell>95.0%</cell><cell>5</cell></row><row><cell>EMV [17]</cell><cell>-</cell><cell>81.6% 3</cell><cell>480</cell></row><row><cell>RGB Diff.</cell><cell>No</cell><cell>84.2%</cell><cell>660</cell></row><row><cell>RGB Diff.</cell><cell>Yes</cell><cell>87.7%</cell><cell>660</cell></row><row><cell cols="2">Two-Stream 3DCNN [68] -</cell><cell>90.2%</cell><cell>246</cell></row><row><cell>RGB+EMV [17]</cell><cell>-</cell><cell>86.4%</cell><cell>390</cell></row><row><cell>RGB+RGB Diff.</cell><cell>No</cell><cell>86.8%</cell><cell>340</cell></row><row><cell>RGB+RGB Diff.</cell><cell>Yes</cell><cell>91.0%</cell><cell>340</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Comparison of segment based sampling of TSN with the regular sampling on the UCF101 dataset (over three splits). For fair comparison, in addition to sampling strategy, the remaining implementation details are kept the same. The number of sampled snippets is set as 3.</figDesc><table><row><cell>Sampling strategy</cell><cell cols="3">Spatial Temporal Two-Stream</cell></row><row><cell>Regular sampling (stride 1)</cell><cell>84.3%</cell><cell>85.6%</cell><cell>92.1%</cell></row><row><cell>Regular sampling (stride 5)</cell><cell>84.9%</cell><cell>86.7%</cell><cell>93.2%</cell></row><row><cell>Regular sampling (stride 10)</cell><cell>85.3%</cell><cell>86.6%</cell><cell>93.5%</cell></row><row><cell>Regular sampling (stride 15)</cell><cell>85.3%</cell><cell>86.5%</cell><cell>93.6%</cell></row><row><cell>Regular sampling (stride 20)</cell><cell>85.3%</cell><cell>86.6%</cell><cell>93.7%</cell></row><row><cell>Segment based sampling (TSN)</cell><cell>86.5%</cell><cell>89.8%</cell><cell>94.2%</cell></row><row><cell cols="4">slightly increases the performance (94.9% to 95.0%), but</cell></row><row><cell cols="4">severely slows down the testing speed to only 5 FPS. So</cell></row><row><cell cols="4">we only use RGB and optical flow to report the final perfor-</cell></row><row><cell cols="4">mance. Another interesting finding is that the simple motion</cell></row><row><cell cols="4">representation of RGB differences, when used together with</cell></row><row><cell cols="4">RGB data under TSN framework, can provide competitive</cell></row><row><cell cols="4">recognition performance (91.0%) while running at a very</cell></row><row><cell cols="4">fast speed of 340FPS. It also outperforms other state-of-the-</cell></row><row><cell cols="4">art real-time action recognition methods as shown in Table 2.</cell></row><row><cell cols="4">This suggests that "RGB + RGB Diff." can serve well for</cell></row><row><cell cols="4">building real-time action recognition systems with moderate</cell></row><row><cell>accuracy requirement.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Exploration of different segment numbers K in temporal segment networks on the UCF101 dataset (over three splits) and the ActivityNet dataset (train on the training set and test on the validation set). We use the average consensus function in these experiments.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>UCF101</cell><cell></cell><cell></cell><cell cols="2">ActivityNet v1.2 Val.</cell></row><row><cell>K</cell><cell cols="6">Spatial Temporal Two-Stream (1:1) Spatial Temporal Two-Stream (1:0.5)</cell></row><row><cell>1</cell><cell>85.0%</cell><cell>88.3%</cell><cell>92.4%</cell><cell>82.0%</cell><cell>61.4%</cell><cell>84.7%</cell></row><row><cell>3</cell><cell>86.5%</cell><cell>89.8%</cell><cell>94.2%</cell><cell>83.6%</cell><cell>70.6%</cell><cell>86.9%</cell></row><row><cell>5</cell><cell>86.7%</cell><cell>90.1%</cell><cell>94.7%</cell><cell>84.6%</cell><cell>72.9%</cell><cell>87.6%</cell></row><row><cell>7</cell><cell>86.4%</cell><cell>90.1%</cell><cell>94.9%</cell><cell>84.0%</cell><cell>72.8%</cell><cell>87.8%</cell></row><row><cell>9</cell><cell>86.2%</cell><cell>89.7%</cell><cell>94.9%</cell><cell>83.7%</cell><cell>72.6%</cell><cell>87.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Exploration of different segmental consensus functions for temporal segment networks on the UCF101 dataset (over three splits) and the ActivtyNet dataset (train on the training set and test on the validation set). We set segment number K as 7 in these experiments.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>UCF101</cell><cell></cell><cell></cell><cell cols="2">ActivityNet v1.2 Val.</cell></row><row><cell>Consensus Function</cell><cell cols="6">Spatial Temporal Two-Stream (1:1) Spatial Temporal Two-Stream (1:0.5)</cell></row><row><cell>Max Pooling</cell><cell>84.9%</cell><cell>83.5%</cell><cell>92.4%</cell><cell>81.8%</cell><cell>62.0%</cell><cell>85.4%</cell></row><row><cell>Average Pooling</cell><cell>86.4%</cell><cell>90.1%</cell><cell>94.9%</cell><cell>84.0%</cell><cell>72.8%</cell><cell>87.8%</cell></row><row><cell>Weighted Average</cell><cell>86.4%</cell><cell>89.7%</cell><cell>94.8%</cell><cell>83.1%</cell><cell>70.5%</cell><cell>86.4%</cell></row><row><cell>Top-K Pooling</cell><cell>85.5%</cell><cell>88.8%</cell><cell>94.2%</cell><cell>84.7%</cell><cell>73.6%</cell><cell>88.1%</cell></row><row><cell>Attention Weighting</cell><cell>86.1%</cell><cell>89.1%</cell><cell>94.6%</cell><cell>84.1%</cell><cell>71.8%</cell><cell>88.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Comparison of different ConvNet architectures on the UCF101 dataset (over three splits). "BN-Inception+TSN" refers to the setting where the temporal segment networkframework is applied on top of the best performing BN-Inception [62] architecture. It is worth noting that our reported BN-Inception result is not directly comparable to those previous works as its training is based on our proposed good training practices.</figDesc><table><row><cell>Training setting</cell><cell cols="3">Spatial Temporal Two-Stream</cell></row><row><cell>VGG-M [69] (from [1])</cell><cell>73.0%</cell><cell>83.7%</cell><cell>86.9%</cell></row><row><cell>GoogLeNet [9] (from [70])</cell><cell>75.3%</cell><cell>85.8%</cell><cell>89.3%</cell></row><row><cell>VGGNet-16 [8] (from [70])</cell><cell>78.4%</cell><cell>87.0%</cell><cell>91.4%</cell></row><row><cell>ResNet-152 [31] (from [71])</cell><cell>83.4%</cell><cell>87.2%</cell><cell>91.8%</cell></row><row><cell>BN-Inception [62]</cell><cell>85.0%</cell><cell>88.3%</cell><cell>92.4%</cell></row><row><cell>BN-Inception+TSN</cell><cell>86.4%</cell><cell>90.1%</cell><cell>94.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>Evaluation on the validation set of ActivityNet challenge 2016 data (ActivityNet v1.3 Val.). "BN-Inception w/o TSN" indicates that we train the models without TSN. "TSN+X", indicates that we train TSN with underlying the CNN architecture "X". "TSN-Top3" refers to the case where the top-K aggregation function is used with K set to 3. Here we use the fusion weights of 1:0.5 for RGB and optical flow, respectively.</figDesc><table><row><cell>Settings</cell><cell cols="3">mAP on ActivityNet v1.3 Val. Spatial Temporal Two Stream</cell></row><row><cell>BN-Inception w/o TSN</cell><cell>76.6%</cell><cell>52.7%</cell><cell>78.9%</cell></row><row><cell>TSN + BN-Inception</cell><cell>79.7%</cell><cell>63.6%</cell><cell>84.7%</cell></row><row><cell>TSN + Inception V3</cell><cell>83.3%</cell><cell>64.4%</cell><cell>87.7%</cell></row><row><cell>TSN-Top3 + Inception V3</cell><cell>84.5%</cell><cell>64.0%</cell><cell>88.0%</cell></row><row><cell>TSN-Ensemble</cell><cell>85.9%</cell><cell>68.3%</cell><cell>89.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Winning entries in the untrimmed video classification task of ActivityNet challenge 2016 (ActivityNet v1.3 Test). We present the recognition accuracies in the form of mAP values and top-1 accuracies. The entries were ranked by mAP values in the challenge.</figDesc><table><row><cell>Team</cell><cell>mAP</cell><cell>Top1 Accuracy</cell></row><row><cell>CES (ours)</cell><cell>93.23%</cell><cell>88.14%</cell></row><row><cell>QCIS</cell><cell>92.41%</cell><cell>87.79%</cell></row><row><cell>MSRA</cell><cell>91.94%</cell><cell>86.69%</cell></row><row><cell>UTS</cell><cell>87.16%</cell><cell>84.90%</cell></row><row><cell>Tokyo Univ.</cell><cell>86.46%</cell><cell>80.43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc>Comparison of our method based on temporal segment network (TSN) with other state-of-the-art approaches on the datasets of HMDB51, UCF101, THUMOS14, ActivityNet v1.2, and Kinetics400. First, we instantiate TSN with the two-stream 2D CovnNets (BN-Inception) on the five datasets. In addition, we also use two-stream I3D as the backbone architecture for TSN training on the Kinetics400 dataset.</figDesc><table><row><cell>HMDB51</cell><cell>UCF101</cell><cell>THUMOS14</cell><cell cols="2">ActivityNet v1.2 Test</cell><cell cols="2">Kinetics400 Val. (top1 / top5)</cell></row><row><cell>iDT+FV [2]</cell><cell>57.2% iDT+FV [72]</cell><cell>85.9% iDT+FV [72]</cell><cell>63.1% iDT+FV [72]</cell><cell cols="2">66.5% RGB-I3D [56]</cell><cell>72.9% / 90.8%</cell></row><row><cell>DT+MVSV [46]</cell><cell>55.9% DT+MVSV [46]</cell><cell>83.5% object+motion [73]</cell><cell>71.6% Depth2Action [74]</cell><cell cols="2">78.1% RGB-I3D + TSN</cell><cell>73.5% / 91.6%</cell></row><row><cell>iDT+HSV [75]</cell><cell>61.1% iDT+HSV [75]</cell><cell>87.9%</cell><cell></cell><cell></cell><cell>FLOW-I3D [56]</cell><cell>65.3% / 86.7%</cell></row><row><cell>MoFAP [23]</cell><cell>61.7% MoFAP [23]</cell><cell>88.3%</cell><cell></cell><cell></cell><cell>FLOW-I3D+TSN</cell><cell>65.4% / 86.7%</cell></row><row><cell>Two Stream [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>This result is got by personal communication with the first author of<ref type="bibr" target="#b16">[17]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we presented the Temporal Segment Network (TSN), a video-level framework that aims to model longrange temporal structure. As demonstrated on four action recognition benchmarks and ActivtyNet challenge 2016, this work has brought the state of the art to a new level, while maintaining a reasonable computational cost. This is largely ascribed to the segmental architecture with sparse sampling, as well as a series of good practices that we explored in this work. The former provides an effective and efficient way to capture long-range temporal structure, while the latter makes it possible to train very deep networks on a limited training set without severe over-fitting.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>modeling of temporal segment network, the learned models focus more on humans in the videos, and seem to model the long-range structure of the action class. Similar observation would be identified in other action classes such as "Long Jump" and "Cheer Leading". This suggests that models learned with the proposed method may perform better, which is well reflected in our quantitative experiments. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognize complex events from static images by fusing deep channels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1600" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transferring deep object and scene representations for event recognition in still images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="390" to="409" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="810" to="822" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="271" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1604.04494</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The THUMOS challenge on action recognition for videos in the wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The kinetics human action video dataset</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Computational studies of human motion: Part 1, tracking and motion synthesis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Machine recognition of human activities: A survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1473" to="1488" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on PETS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3D-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on statistical learning in computer vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="596" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1242" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representing videos using mid-level discriminative patches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2571" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with actons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3559" to="3566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-L 1 optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th DAGM Symposium on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Efficient two-stream motion and appearance 3d cnns for video classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08851</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1507.02159</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">LEAR-INRIA submission for the thumos workshop</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on THUMOS Challenge</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Depth2action: Exploring embedded depth for large-scale action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="668" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Motion part regularization: Improving action recognition via trajectory group selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3698" to="3706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Deep draw</title>
		<ptr target="https://github.com/auduno/deepdraw" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning multiscale features directly fromwaveforms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<idno>abs/1603.09509</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
