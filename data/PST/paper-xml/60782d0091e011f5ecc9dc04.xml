<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Negative and Positive Learning for Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-14">14 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Youngdong</forename><forename type="middle">Kim</forename><surname>Juseung</surname></persName>
							<email>juseungyun@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Hyounguk</surname></persName>
							<email>hyounguk.shon@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shon</forename><forename type="middle">Junmo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Negative and Positive Learning for Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-14">14 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.06574v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training of Convolutional Neural Networks (CNNs) with data with noisy labels is known to be a challenge. Based on the fact that directly providing the label to the data (Positive Learning; PL) has a risk of allowing CNNs to memorize the contaminated labels for the case of noisy data, the indirect learning approach that uses complementary labels (Negative Learning for Noisy Labels; NLNL) has proven to be highly effective in preventing overfitting to noisy data as it reduces the risk of providing faulty target. NLNL further employs a three-stage pipeline to improve convergence. As a result, filtering noisy data through the NLNL pipeline is cumbersome, increasing the training cost. In this study, we propose a novel improvement of NLNL, named Joint Negative and Positive Learning (JNPL), that unifies the filtering pipeline into a single stage. JNPL trains CNN via two losses, NL+ and PL+, which are improved upon NL and PL loss functions, respectively. We analyze the fundamental issue of NL loss function and develop new NL+ loss function producing gradient that enhances the convergence of noisy data. Furthermore, PL+ loss function is designed to enable faster convergence to expected-to-be-clean data. We show that the NL+ and PL+ train CNN simultaneously, significantly simplifying the pipeline, allowing greater ease of practical use compared to NLNL. With a simple semisupervised training technique, our method achieves stateof-the-art accuracy for noisy data classification based on the superior filtering ability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have led to great improvements in many supervised tasks. However, CNNs' performance relies heavily on the quality of labels, and accurately labeling a huge amount of data is expensive and time-consuming. Furthermore, accurate labeling is done by hand, which can eventually lead to mismatched labeling. Therefore, the robust training of CNNs with noisy data is of great practical importance. There are many approaches regarding this issue. For example, there are meth-ods that design noise-robust loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>, use two neural networks to select clean labels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>, and utilize label correction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>. These existing approaches commonly use the given labels in a direct manner, i.e., "input image belongs to this label" (Positive Learning; PL). This behavior carries the risk of providing faulty information to the CNNs when noisy labels are involved.</p><p>Motivated by this reason, Negative Learning for Noisy Labels; NLNL <ref type="bibr" target="#b11">[12]</ref>, which is an indirect learning method for training CNNs, has been proposed recently. Negative Learning (NL) uses randomly chosen complementary labels and trains the CNN that "input image does not belong to this complementary label," reducing the risk of providing the wrong information because of the high chance of not selecting a true label as a complementary label. Additionally, NLNL proposed three-stage pipeline for filtering noisy data from training data (Figure <ref type="figure" target="#fig_0">1 (a)</ref>). Each stage is composed of NL → NL while discarding data of low confidence (Selective NL; SelNL) → PL while only retaining data of high confidence (Selective PL; SelPL), enabling more convergence after NL. However, the fundamental problem that NL loss function causes underfitting to the overall training data still remains. This is the reason that NL requires an additional sequential step, SelNL. Furthermore, the threestage pipeline for filtering noisy data is quite inefficient, extending the time for training CNNs.</p><p>In this study, we propose a novel version of NLNL: Joint Negative Learning and Positive Learning; JNPL which has a unified single-stage pipeline for filtering noisy data (Figure <ref type="figure" target="#fig_0">1 (b)</ref>). JNPL is composed of two losses to train CNN, NL+ and PL+ losses, dedicated to filtering noisy data from training data. Each is developed from NL and PL loss functions, respectively. Firstly, our paper focuses on analyzing the NL loss function to understand the cause for underfitting. Then we develop a new loss function NL+ that resolves the issue, which produces a gradient appropriate for convergence on a noisy training dataset. Our study demonstrates the effectiveness of NL+, showing improved convergence across various label noise types and noise rates. Secondly, while we utilize PL to aid in training with noisy data, PL+ loss function is also newly designed to enable faster training with expected-to-be-clean data. Our paper shows the effectiveness of the PL+ loss function compared to the previous PL loss function. Finally, as both loss functions of our method (NL+ and PL+) jointly train the model through a single stage, it is simple and easier to use than NLNL. Our experiments show that JNPL successfully filters noisy data in a single stage, thereby providing significantly faster training of CNN as well as better filtering compared to NLNL. After filtering noisy data from the training data we perform pseudo-labeling for noisy data classification. We achieve state-of-the-art accuracy across various settings in CIFAR10, CIFAR100 <ref type="bibr" target="#b12">[13]</ref>, and Clothing1M <ref type="bibr" target="#b30">[31]</ref> datasets, proving the superior filtering ability of JNPL.</p><p>The main contributions of this paper are as follows:</p><p>• We propose an improved version of NLNL, named "Joint Negative and Positive Learning (JNPL)," featuring a single-stage pipeline for filtering noisy data, therefore enabling easier usage compared to NLNL. • Two novel loss functions are newly designed, each named NL+ loss and PL+ loss. NL+ solves the underfitting problem of the NL loss, and provides better convergence on various types and ratios of label noises in the training data. Moreover, PL+ enables faster training compared to the previous PL loss function. • Our method filters noisy data, more robust across different types and ratios of noise than NLNL. Our method also achieves state-of-the-art noisy data classification results when used along with pseudo-labeling. • Prior knowledge of the type or number of noisy data is not required for our method. It does not require any hyper-parameter tuning that depend on prior knowledge, allowing our method to be applicable in practice.</p><p>The remainder of this paper is organized as follows. Section 3 describes NLNL method in depth, which is targeted throughout the whole paper, and discusses the cause of the underfitting problem of the method. Section 4 describes our proposed method, JNPL, and explains in detail on NL+ loss and PL+ loss terms. Section 5 demonstrates the overall comparison between JNPL and NLNL, showing the distinct advantages of JNPL over NLNL. Section 6 discusses the evaluations of our method in comparison to baseline methods. Finally, we summarize and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Several methods that aim to mitigate label noise have been proposed. Here, we summarize some of the recent approaches to noise-robust learning. Designing noise-robust loss The commonly used crossentropy (CE) loss is known to be prone to overfitting when there is noise in the labels. Therefore, a family of studies aims to design novel loss functions that are tolerant of label noise. Ghosh et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> showed that the mean absolute error (MAE) loss is theoretically robust against label noise. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> proposed Generalized Cross Entropy loss, which is a generalized function that can interpolate between the forms of CE and MAE, which enables it to adjust tradeoffs between robust loss and non-robust loss.</p><p>However, in many cases, such noise-robust losses carry the problem of underfitting, which motivates the combination of a robust loss with a non-robust loss to improve convergence. Wang et al. <ref type="bibr" target="#b28">[29]</ref> proposed Symmetric Cross Entropy loss, which combines CE loss with Reverse Cross Entropy loss. Recently, Ma et al. <ref type="bibr" target="#b17">[18]</ref> proposed a loss normalization technique that transforms a non-robust loss function into a robust loss function. They also showed that such normalized loss used in combination with another robust loss function improves convergence and coined the term Active Passive Loss (APL). Weighting samples In some researches, each sample in the training set is weighted by the reliability of the label <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15]</ref>. Moreover, other methods proposed meta-learning algorithms that predicts the weights for each sample <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. However, these methods require a clean validation set, which is often difficult to guarantee in practice. Correction methods Some other researches used correction methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref>. They assume that prior knowledge like noise rate or noisy transition matrix is known or that some clean data is accessible. However, in a practical case, prior knowledge and clean data is usually hard to obtain. Some other works used CNN with additional layer <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5]</ref>, and noise transition matrix is approximated to correct loss. Many efforts gradually change the data label to the prediction value of the network <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>. Arazo et al. <ref type="bibr" target="#b0">[1]</ref>, fits a mixture of beta distributions that models the loss of clean and noisy samples during training. Selecting clean labels Some attempted to identify clean labels from a noisy dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref>. Ding et al. <ref type="bibr" target="#b1">[2]</ref> proposed a selection of clean examples based on predicted likelihoods. The labels of the remaining samples are discarded, and the network is trained by semi-supervision. Some of the successful approaches train two deep neural networks simultaneously and let them teach each other <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>. Each network selects possibly clean data and trains the other network with this data. Use of complementary labels Kim et al. <ref type="bibr" target="#b11">[12]</ref> proposed a noise-robust learning method where instead of maximizing the log-likelihood on the target position, it minimizes the log-likelihood on the complementary positions, termed Negative Learning (NL). They employ a three-stage pipeline based on NL that separates the clean data from the noisy data. Finally, the network is trained using standard CE loss with semi-supervision by treating the noisy set as unlabeled.</p><p>Other approaches Li et al. <ref type="bibr" target="#b15">[16]</ref> uses meta-learning to obtain weights that can be easily fine-tuned to a given noisy dataset. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> proposed to learn confidence scores of each samples from the relationship between noisy samples in the feature space, then use the confidence scores to generate cleaner representations. Harutyunyan et al. <ref type="bibr" target="#b6">[7]</ref> proposed training algorithm based on mutual information between weights and labels to regularize the memorization of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Negative Learning for Noisy Labels (NLNL)</head><p>Throughout this paper, we consider the problem of cclass classification. Let x ∈ X be an input, y, y ∈ Y = {1, ..., c} be its label and complementary label, respectively, and y, y ∈ {0, 1} c be their one-hot vector. Suppose the CNN f (x; θ) maps the input space to the c-dimensional score space f : X → R c , where θ is the set of network parameters. If f passes through the softmax function, the output can be interpreted as a probability vector p ∈ ∆ c−1 , where ∆ c−1 denotes the c-dimensional simplex.</p><p>NL <ref type="bibr" target="#b11">[12]</ref> is an indirect learning method for training CNNs with noisy data. Instead of using given labels, it chooses random complementary label y and train CNNs as in "input image does not belong to this complementary label."</p><p>The loss function following this definition is as below, along with the classic PL loss function for comparison:</p><formula xml:id="formula_0">L P L (f, y) = − c k=1 y k log p k<label>(1)</label></formula><formula xml:id="formula_1">L N L (f, y) = − c k=1 y k log(1 − p k ).<label>(2)</label></formula><p>To improve convergence after NL, SelNL is performed as a subsequent step. SelNL trains the CNNs only with the data having confidence over 1 c (p y &gt; 1 c ). Since data involved in training tend to be less noisy than before, CNNs converge better after SelNL. Furthermore, PL is considered a faster and more accurate method than NL, only if training data is assumed to be clean. After training with NL and SelNL, SelPL train CNNs only with data that has confidence above γ (= 0.5), assuming that such data are clean. After filtering noisy data with these three steps (NL→SelNL→SelPL), semi-supervised learning (pseudolabeling <ref type="bibr" target="#b13">[14]</ref>) is performed utilizing labeled expected-to-beclean data and unlabeled noisy data.</p><p>As mentioned in Section 1, the fundamental problem of underfitting of NL still remains. To analyze the root of this phenomenon, we observe the gradient resulting from the NL loss function (Eq 2) as follows:</p><formula xml:id="formula_2">∇L N L = ∂L N L (f, y) ∂f i = p i if i = y − p y 1−p y p i if i = y.<label>(3)</label></formula><p>Eq 3 states that at classes except for y receives gradient of −  <ref type="figure" target="#fig_4">2 (a)</ref>). However, considering noisy data, ground-truth labels may be chosen as y. In this case, all classes, except for ground truth label, receive high ∇L N L(i =y) because of high p y , resulting in underfitting of that data as the confidence of classes other than the ground-truth label increases. In Section 4.1, we describe the developed loss function of NL (NL+) that resolves this underfitting issue.</p><formula xml:id="formula_3">p y 1−p y p i (∇L N L(i =y) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Joint Negative and Positive Learning (JNPL)</head><p>The loss function of the proposed method, JNPL, is composed of two loss functions:  Each of which is dedicated to filtering noisy data from training data. L N L+ is the advanced version of NL, which resolves underfitting issue. L P L+ is other newly designed loss for PL that trains on expected-to-be-clean data, empowering training on data of higher confidence. λ is added to scale the overall magnitude of PL+ so that it does not overwhelm the magnitude of NL+. We set λ = 0.01 throughout the whole paper. These two losses enable successful filtering of noisy data. Finally, noisy data classification is done in semi-supervised manner, utilizing these filtered noisy data confidence as pseudo-label. In the following sections, we further introduce each of the loss functions and describe the concept and implementation respectively.</p><formula xml:id="formula_4">L JN P L = L N L+ + λL P L+ .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">NL+</head><p>As discussed in Section 3, we argue that the cause of the underfitting problem with NL is due to the nature of its gradient ∇L N L(i =y) (Figure <ref type="figure" target="#fig_4">2 (a)</ref>). This is more pronounced as the noise rate increases, as shown in Figure <ref type="figure" target="#fig_4">2 (b)-(d)</ref>. This problem occurs when noisy data receives high gradient to classes except for y when the confidence of y is high, y being most likely to be ground truth label. To solve this issue, we propose a modification to the NL loss function, named NL+ loss, as follows:</p><formula xml:id="formula_5">L N L+ (f, y) = −(1 − p y ) c k=1 y k log(1 − p k ). (5)</formula><p>It should be noted that 1 − p y acts as a constant weighting factor. Intuitively, this factor has the effect of decreas-ing the loss for noisy data when corresponding p y is high, y being most likely to be ground truth label. That way, it reduces the risk of pressing down on the confidence of ground truth label for noisy data, reducing the risk of underfitting. This is further analyzed by observing the gradient of NL+ (∇L N L+(i =y) ), given by Eq 5:</p><formula xml:id="formula_6">∇L N L+(i =y) = (1 − p y )∇L N L(i =y) = −p y p i .<label>(6)</label></formula><p>The gradient map of ∇L N L+(i =y) is shown in Figure <ref type="figure" target="#fig_3">2</ref> (e). Compared to Figure <ref type="figure" target="#fig_3">2</ref> (a), it shows gradient at upper-left region is reduced. This implies that as the training progresses with NL+, noisy data is gathered at the upper-left region. With NL+, gradient received for noisy data of high p y is reduced, allowing noisy data to maintain high p y value, where y is most likely to be ground truth label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PL+</head><p>In this section, we introduce the second loss function L P L+ in JNPL. As mentioned in Section 1, when training data is verified to have clean labels, PL is a faster and more accurate method than NL. Following this fact, we apply PL+ to our method for faster convergence. But compared to NLNL, this is not applied in a sequential step but rather as a unified step. First of all, the criteria for selecting the training data for PL+ is required. Previously, NLNL applied PL to data over the threshold (p y &gt; 0.5). However, the criteria for selecting data for PL should be stricter. Even if a data satisfies p y &gt; 0.5, a probability of other class may reach as much as 0.5, resulting in the risk of selecting noisy data as clean data. Hence, PL+ considers the probabilities of classes other than the given label. When probabilities other classes except for given label are under distribution 1 c , this is a candidate for PL+ (Figure <ref type="figure" target="#fig_5">3 (a)</ref>). Additionally, among the candidates for PL+, it is selected through Bernoulli sampling with respect to p y . The higher the p y , the more frequently the data would be trained with PL+. Furthermore, PL+ selects data not only from expected-to-be-clean data but also from noisy data. Meaning that, when the probabilities of other classes except for the label of maximum probability is under the uniform distribution, the data is also a candidate for PL+ using the maximum probability class label (= ŷ) (Figure <ref type="figure" target="#fig_5">3 (b)</ref>). In this way, PL+ selects data for training more strictly, but also, the candidate area is increased. The pseudocode for PL+ process is shown in Algorithm 1.</p><p>PL is usually done using cross-entropy (CE) loss (Eq 1). However, while it may be tolerable when training clean data, it may not be as tolerable as when training noisy data. The reason for PL in our method is to train faster on more confident data. However, when observing the gradient of CE in Figure <ref type="figure" target="#fig_5">3</ref> (c), it states that a smaller gradient is pro-</p><formula xml:id="formula_7">Algorithm 1: PL+ Input: mini-batch D Result: L P L+ over mini-batch DP L+ for (x, y) ∈ D do p ← sof tmax (f (x)) ŷ ← argmax i p i if p i &lt; 1 c</formula><p>for ∀i ∈ {1, ..., c} \ {ŷ} then Append (x, ŷ) to DP L+ with probability p ŷ else Reject (x, y) end end Calculate L P L+ (f (x), ŷ) for DP L+ by Eq. ( <ref type="formula" target="#formula_9">7</ref>) return</p><formula xml:id="formula_8">1 | DP L+ | x∈ DP L+ L P L+ (f (x), ŷ)</formula><p>vided to more confident data, while a higher gradient is provided to less confident data. Since the goal is to train faster on more confident data, not just training more on less confident data, we propose PL+ loss function to resolve this issue as follows:</p><formula xml:id="formula_9">L P L+ (f, ŷ) = − N n=0 (1 + p 2 n ŷ ) c k=1 y k log p k ,<label>(7)</label></formula><p>and the gradient of PL+ loss is as follows:</p><formula xml:id="formula_10">∇L P L+ = N n=0 (1 + p 2 n ŷ )∇L P L = − N n=0 (1 + p 2 n ŷ )(1 − p ŷ ) = −(1 − p 2 N +1 ŷ ).<label>(8)</label></formula><p>Similar to NL+, N n=0 (1+p 2 n ŷ ) acts as a constant weighting factor. By applying this weight factor, the gradient of PL+ loss function is modified as shown in Eq 8 and visualized in Figure <ref type="figure" target="#fig_5">3 (c)</ref>. It can be seen that higher gradient is being provided to data of high p y as N increases. Figure <ref type="figure" target="#fig_5">3 (d)</ref> proves faster convergence as N increases. We set N = 3 throughout the whole paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>Since our method is the advanced version of NLNL, which is targeted throughout our whole paper, this section further demonstrates the distinct advantage of our method JNPL over NLNL.</p><p>First of all, our method JNPL is a unified step pipeline for filtering noisy data, compared to 3-step pipeline of NLNL. JNPL is trained with two loss functions simultaneously, increasing the efficiency of training CNN. Secondly, NL+ is more capable of handling more diverse noise types compared to NL→SelNL owing to the nature of gradient followed by L N L+ . Although NL applies SelNL to compensate for underfitting problem, we show that this is not an optimal solution for all types of noise. Consider when training data is CIFAR10 mixed with asymm noise, especially when class "dog" is mixed with "cat" in bidirectional manner (DOG ↔ CAT). Overall probability values across all classes are shared between class "dog" and "cat," resulting in distribution of training data as shown in Figure <ref type="figure" target="#fig_8">6</ref> (a), (d). In this case, SelNL shows almost no effect as the noisy data is not under the uniform distribution (Figure <ref type="figure" target="#fig_8">6</ref> (b), (e)). Whereas for NL+, due to the fact that gradient for region (p y &lt; 0.5 &amp; p y &gt; 0.5) is reduced in a smooth manner compared to NL, it eventually enables both classes to be separated, showing distinct advantage of NL+ over SelNL (Figure <ref type="figure" target="#fig_8">6 (c), (f)</ref>).</p><p>Finally, we show that our method JNPL successfully filters noisy data from training data than NLNL. Figure <ref type="figure" target="#fig_7">5</ref> shows overall filtering ability between NLNL and JNPL with average precision (AP). It is compared in diverse environment: CIFAR10/CIFAR100 mixed with different ratio of symm and asymm noise. It shows that our method outperforms NLNL in filtering noisy data on overall cases. Furthermore, it can be observed that gap of AP between NLNL and JNPL increases as the noise ratio increases. This implies that JNPL is more robust to the amount of noise mixed  in training data. Also, JNPL being more robust to asymm noise than NLNL also proves the point made above. This phenomenon is more clearly shown in more difficult data CIFAR100. AP of NLNL drastically decreases as the noise rate gets higher. However, JNPL shows robustness in types and ratios of noise, similar to when training with CIFAR10. Figure <ref type="figure" target="#fig_7">5</ref> demonstrates our method JNPL is capable of being generalized to type and ratio of noise, and even number of classes in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we describe the experiments performed to evaluate our method. Pseudo-labeling is done on a training dataset filtered by JNPL for noisy data classification and resulting accuracies are compared to those of other existing methods. We verify our method by comparing with other recent baseline methods, varying experimental settings in terms of dataset and type and ratio of noise in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment settings</head><p>Baseline methods We compare our method against CE, along with recent state-of-the-art approaches including Coteaching <ref type="bibr" target="#b5">[6]</ref>, JoCoR <ref type="bibr" target="#b29">[30]</ref>, APL <ref type="bibr" target="#b17">[18]</ref>, and NLNL <ref type="bibr" target="#b11">[12]</ref>. Dataset We conduct the experiments on CIFAR10, CI-FAR100 <ref type="bibr" target="#b12">[13]</ref> mixed with two types noises (symm, asymm), and Clothing1M <ref type="bibr" target="#b30">[31]</ref> dataset. Clothing1M is a large-scale real-world dataset with noisy labels, containing 1 million images of clothing obtained from several online shopping websites. It is reported that the overall accuracy of noisy labels in this dataset is 61.54%, and some pairs of classes are often confused with each other (e.g., Knitwear and Sweater). For preprocessing, we performed mean subtraction, horizontal flip, and random crops for CIFAR10 and CIFAR100. For Clothing1M, we resize the image to 256×256, crop 224×224 at the center and perform mean subtraction and horizontal flip.</p><p>Label noise types We generated noisy CIFAR10 and CI-FAR100 datasets according to the following procedures.</p><p>In symmetric (symm) noise experiments, we flipped a portion of the labels by re-sampling each label uniformly from the remaining classes, excluding the ground-truth class. In asymmetric (asymm) noise experiments, we followed the same label transition rule used by Patrini et al. <ref type="bibr" target="#b21">[22]</ref>. For CIFAR10, we mapped TRUCK → AUTOMOBILE, BIRD → PLANE, DEER → HORSE, and CAT ↔ DOG. For CI-FAR100, the noise flipped each class into the next, circularly within super-classes.</p><p>For each noise type, we compared the methods under the symmetric noise rates of η symm ∈ {0.2, 0.4, 0.6, 0.8} and asymmetric noise rates of η asymm ∈ {0.1, 0.2, 0.3, 0.4}. Models For CIFAR10 and CIFAR100 experiments, we used ResNet34. For Clothing1M, we used ResNet50 <ref type="bibr" target="#b7">[8]</ref>, pretrained on ImageNet. Hyperparameters We used stochastic gradient descent (SGD) with momentum of 0.9, weight decay of 10 −4 . For experiments with CIFAR10 and CIFAR100, batch size is set to 128. Moreover, JNPL trains CNN for 1000 epochs with initial learning rate of 10 −2 , and decay by a factor of 10 at 800 epochs. For pseudo labeling, initial learning rate is 0.1, decayed by a factor of 10 at 192, 288 epochs (480 epochs total). For experiments with Clothing1M, batch size is set to 64, and JNPL trains CNN for 40 epochs with initial learning rate of 10 −3 , and decay by a factor of 10 at 30 epochs. For pseudo labeling, initial learning rate is 10 −3 , decayed by a factor of 10 at 10 epochs (15 epochs total).</p><p>For CIFAR100, we adopt the technique NLNL proposed for generalization to the number of classes in training data: providing multi y to each data. We provide 110 y to each data in order to match the training speed to when training with CIFAR10 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the results of our method and other baseline methods in various noise environment and two datasets. Our proposed method outperformed all other comparable baseline methods in overall noise types and ratios. The result shows other baseline methods achieve comparable results in the less-noisy environment, but the performance decreases drastically as the noise ratio increases, which is even more visible at CIFAR100, which is the harder case for noisy data classification. Our method shows a distinct improvement in this situation compared to all other methods. It was shown in Section 5 our method is robust to the amount of noise mixed in training data, regardless of the type of noises. Table <ref type="table" target="#tab_1">1</ref> shows a similar result that our method achieves more distinct best accuracy as the noise rate gets higher. This phenomenon is more emphasized for CIFAR100. Our method outperforms as much as 6 to 7% at both symm and asymm noises in this dataset. It is noteworthy that our method achieved 7% higher state-of-the-art  <ref type="bibr" target="#b20">[21]</ref> 69.84 M-correction <ref type="bibr" target="#b0">[1]</ref> 71.00 LIMIT <ref type="bibr" target="#b6">[7]</ref> 71.39 Joint-Optim <ref type="bibr" target="#b25">[26]</ref> 72.16 MetaCleaner <ref type="bibr" target="#b33">[34]</ref> 72.5 MLNT <ref type="bibr" target="#b15">[16]</ref> 73.47 PENCIL <ref type="bibr" target="#b31">[32]</ref> 73.49</p><p>Ours 74.15 It is shown that Co-teaching and JoCoR method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> exceeds the performance compared to our method for some cases. However, it should be noted that they assume prior knowledge on important statistics about the dataset such as the amount of noise. In reality, this assumption often leaves the method impractical because the ratio of noise mixed in training data is likely to be unknown. On the other hand, our method does not assume any such prior knowledge and therefore does not require extensive tuning of hyperparameters.</p><p>To demonstrate the generalization of our method JNPL to real-world noisy data, we compose an experiment on Clothing1M dataset (Table <ref type="table" target="#tab_2">2</ref>). We brought recent baseline methods which conducted experiment on Clothing1M for comparison. It shows our method achieves comparable performance, outperforming other recent baseline methods. This result clearly proves that JNPL can generalize to training data mixed with various types and ratios of noise, showing the novelty of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose Joint Negative and Positive Learning, the next version of NLNL which is the novel single-step pipeline for filtering noisy training data. Compared to 3step pipeline of NLNL, our method trains CNN with twoloss functions (L N L+ + L P L+ ) in one step. They are developed from previous NL and PL loss functions to enhance convergence and training speed, resulting in better filtering performance than NLNL. We demonstrated that JNPL is stable and robust in various types and ratios of noise mixed in training data. Our method achieves stateof-the-art performance in noisy data classification utilizing pseudo-labeling to our filtered training data, proving our method's excellent filtering ability without referring to any prior knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between Negative Learning for Noisy Labels (NLNL) and Joint Negative and Positive Learning (JNPL) for filtering noisy data from training data, demonstrated with histograms showing the distribution of noisy training data . (a): NLNL is a 3-stage pipeline (NL→SelNL→SelPL). (b): JNPL is a single-stage pipeline, in which two loss functions (NL+ and PL+) train CNN simultaneously.</figDesc><graphic url="image-2.png" coords="2,47.08,179.66,166.28,63.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 (a) shows 2D gradient map of ∇L N L(i =y) , and Figure 2 (b)-(d) shows the distribution of training data after NL in diverse noise ratio. Each training data is distributed in gradient map with respect to its p y (when i = y) and p y max . As the training with NL progresses, clean data tend to have high p y and low p y (lower-right region in Figure 2 (a)), while noisy data tend to have low p y and high p y (upper-left region in Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Plot of ∇L N L (b) η = 0.2 (c) η = 0.4 (d) η = 0.6 Plot of ∇L N L+ (f) η = 0.2 (g) η = 0.4 (h) η = 0.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between NL and NL+ with CIFAR10 with symm noise. (a), (e): Gradient map of NL and NL+, respectively. (b)-(d): Training data distribution with 20%, 40%, 60% noise after training with NL. (f)-(h): Training data distribution with 20%, 40%, 60% noise after training with NL+</figDesc><graphic url="image-30.png" coords="4,448.79,192.88,69.91,68.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 (</head><label>2</label><figDesc>f)-(h) shows the distribution of training data mixed with diverse ratio of noise. It shows that compared to Figure 2 (b)-(d), NL+ results in more convergence. Especially in noise of high ratio (Figure 2 (d), (h)), NL+ successfully divides noisy data from training data, sending noisy data to upperleft region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a), (b): Cases for selecting data for PL+. Data is the candidate for PL+ if confidences at classes other than label of maximum probability is under uniform distribution (1/c). (c): Gradient of PL+ depending on N compared to PL (cross-entropy loss). (d): Accuracy comparison between PL+ with different N . This shows that the flatter version of PL+ (N = 3) generates better training results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy graph of NLNL, NL+, and JNPL (NL+ &amp; PL+) with CIFAR10 mixed with 60% symm noise. (NL→SelNL→SelPL), NL+, and JNPL (NL+&amp;PL+) when training with CIFAR10 mixed with 60% symm noise. Figure 4 clearly indicates that NL+ solely reaches the accuracy of NL→SelNL, proving better convergence of NL+ compared to NL. Furthermore, when PL+ is done simultaneously along with NL+, it results in faster training without the need for additional subsequent step. It also shows overall accuracy of NL+ and JNPL overpasses the accuracy reached by NLNL while preventing overfitting to noisy data, proving the superiority of our method over NLNL.Secondly, NL+ is more capable of handling more diverse noise types compared to NL→SelNL owing to the nature of gradient followed by L N L+ . Although NL applies SelNL to compensate for underfitting problem, we show that this is not an optimal solution for all types of noise. Consider when training data is CIFAR10 mixed with asymm noise, especially when class "dog" is mixed with "cat" in bidirectional manner (DOG ↔ CAT). Overall probability values across all classes are shared between class "dog" and "cat," resulting in distribution of training data as shown in Figure6(a), (d). In this case, SelNL shows almost no effect as the noisy data is not under the uniform distribution (Figure6(b), (e)). Whereas for NL+, due to the fact that gradient for region (p y &lt; 0.5 &amp; p y &gt; 0.5) is reduced in a smooth manner compared to NL, it eventually enables both classes to be separated, showing distinct advantage of NL+ over SelNL (Figure6 (c), (f)).Finally, we show that our method JNPL successfully filters noisy data from training data than NLNL. Figure5shows overall filtering ability between NLNL and JNPL with average precision (AP). It is compared in diverse environment: CIFAR10/CIFAR100 mixed with different ratio of symm and asymm noise. It shows that our method outperforms NLNL in filtering noisy data on overall cases. Furthermore, it can be observed that gap of AP between NLNL and JNPL increases as the noise ratio increases. This implies that JNPL is more robust to the amount of noise mixed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average Precision (AP) for CIFAR10 / CIFAR100 on symm / asymm noises. (a), (b): AP for CIFAR10 on symm / asymm noises, respectively. (c), (d): AP for CI-FAR100 on symm / asymm noises, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between NL and NL+ for asymm 40% noise CIFAR10 "cat" class. (a), (d): Gradient map and histogram of NL, respectively. (b), (e): Gradient map and histogram of NL→SelNL, respectively. (c), (f): Gradient map and histogram of NL+, respectively. Blue indicates clean data whereas orange indicates noisy data in histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ours 70.94 68.11 61.26 17.55 72.03 69.95 68.12 59.51 Comparison with other baseline methods on CIFAR10, CIFAR100 mixed with various types and ratios of noise. Best 2 accuracies are bold faced.</figDesc><table><row><cell>Datasets</cell><cell>Model</cell><cell>Methods</cell><cell>20</cell><cell>40</cell><cell>Symm</cell><cell>60</cell><cell>80</cell><cell>10</cell><cell>Asymm 20 30</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell>CE</cell><cell cols="8">83.95 67.58 43.55 17.32 91.39 87.67 82.73 76.37</cell></row><row><cell></cell><cell></cell><cell cols="9">Co-teaching [6] 91.08 88.08 80.96 21.13 94.20 93.24 90.67 70.20</cell></row><row><cell></cell><cell></cell><cell>JoCoR [30]</cell><cell cols="8">91.84 88.15 59.20 20.72 93.13 91.19 89.01 83.61</cell></row><row><cell cols="2">CIFAR10 ResNet34</cell><cell cols="9">NFL+RCE [18] 90.50 85.16 70.77 19.67 92.35 89.66 84.92 78.30</cell></row><row><cell></cell><cell></cell><cell cols="9">NCE+RCE [18] 90.36 84.57 74.09 26.71 91.89 90.13 85.80 78.49</cell></row><row><cell></cell><cell></cell><cell>NLNL [12]</cell><cell cols="4">94.23 92.43 88.32</cell><cell>-</cell><cell cols="3">94.57 93.35 91.80 89.86</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell cols="8">93.53 91.89 88.45 35.65 94.22 93.45 92.47 90.72</cell></row><row><cell></cell><cell></cell><cell>CE</cell><cell cols="5">57.32 45.64 24.30 8.06</cell><cell cols="3">65.12 62.12 52.77 44.55</cell></row><row><cell></cell><cell></cell><cell cols="9">Co-teaching [6] 69.56 62.81 51.12 10.25 72.52 67.46 61.50 52.86</cell></row><row><cell></cell><cell></cell><cell>JoCoR [30]</cell><cell cols="5">71.75 63.96 37.84 7.32</cell><cell cols="3">72.01 65.05 56.63 45.14</cell></row><row><cell cols="2">CIFAR100 ResNet34</cell><cell cols="9">NFL+RCE [18] 58.70 42.76 24.77 10.57 63.70 56.45 46.96 37.52</cell></row><row><cell></cell><cell></cell><cell cols="6">NCE+RCE [18] 57.41 43.75 25.87 9.94</cell><cell cols="3">64.24 56.48 47.17 36.40</cell></row><row><cell></cell><cell></cell><cell>NLNL [12]</cell><cell cols="4">71.52 66.39 56.51</cell><cell>-</cell><cell cols="3">70.35 63.12 54.87 45.70</cell></row><row><cell>Method</cell><cell></cell><cell>Test Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CE</cell><cell></cell><cell>69.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Forward</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison on Clothing1M with other baseline methods.accuracy in the most difficult setting in Table1, which is 100 class dataset mixed with 40% asymm noise. It is widely known training in general is challenging as the number of classes in the dataset increases. Furthermore, compared to symm noise, asymm noise is the replica of noise that we can actually make in real-life. Achieving such a high accuracy in this setting implies that our method is more capable of generalizing to training data and various types and ratios of noise mixed within compared to other baseline methods.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E O'</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11238</idno>
		<imprint>
			<date type="published" when="2008">2019. 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02679</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="1919">1919-1925, 2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 3, 6, 8</date>
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving generalization by controlling labelnoise information in neural network weights</title>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07933</idno>
		<imprint>
			<date type="published" when="2008">2020. 3, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10456" to="10465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><surname>Mentornet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Regularizing very deep neural networks on corrupted labels</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep networks from noisy labels with dropout regularization</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Nokleby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuewen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="967" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2019. 1, 3, 6, 7, 8</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07131</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1928" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2020. 1, 2, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02612</idno>
		<title level="m">Dimensionality-driven learning with noisy labels</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName><forename type="first">Tailin</forename><surname>Curtis G Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01936</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03683</idno>
		<title level="m">Making neural networks robust to label noise: a loss correction approach</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6575" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2020. 1, 3, 6, 8</date>
			<biblScope unit="page" from="13726" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition</title>
		<author>
			<persName><forename type="first">Weihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07836</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
