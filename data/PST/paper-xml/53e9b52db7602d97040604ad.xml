<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Representation for Signal Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Huang</surname></persName>
							<email>kehuang@egr.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Selin</forename><surname>Aviyente</surname></persName>
							<email>aviyente@egr.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Representation for Signal Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14E9EBE7A0030B55F5E041CE49F63D4F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, application of sparse representation (factorization) of signals over an overcomplete basis (dictionary) for signal classification is discussed. Searching for the sparse representation of a signal over an overcomplete dictionary is achieved by optimizing an objective function that includes two terms: one that measures the signal reconstruction error and another that measures the sparsity. This objective function works well in applications where signals need to be reconstructed, like coding and denoising. On the other hand, discriminative methods, such as linear discriminative analysis (LDA), are better suited for classification tasks. However, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction. In this paper, we present a theoretical framework for signal classification with sparse representation. The approach combines the discrimination power of the discriminative methods with the reconstruction property and the sparsity of the sparse representation that enables one to deal with signal corruptions: noise, missing data and outliers. The proposed approach is therefore capable of robust classification with a sparse representation of signals. The theoretical results are demonstrated with signal classification tasks, showing that the proposed approach outperforms the standard discriminative methods and the standard sparse representation in the case of corrupted signals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sparse representations of signals have received a great deal of attentions in recent years. The problem solved by the sparse representation is to search for the most compact representation of a signal in terms of linear combination of atoms in an overcomplete dictionary. Recent developments in multi-scale and multi-orientation representation of signals, such as wavelet, ridgelet, curvelet and contourlet transforms are an important incentive for the research on the sparse representation. Compared to methods based on orthonormal transforms or direct time domain processing, sparse representation usually offers better performance with its capacity for efficient signal modelling. Research has focused on three aspects of the sparse representation: pursuit methods for solving the optimization problem, such as matching pursuit <ref type="bibr" target="#b0">[1]</ref>, orthogonal matching pursuit <ref type="bibr" target="#b1">[2]</ref>, basis pursuit <ref type="bibr" target="#b2">[3]</ref>, LARS/homotopy methods <ref type="bibr" target="#b3">[4]</ref>; design of the dictionary, such as the K-SVD method <ref type="bibr" target="#b4">[5]</ref>; the applications of the sparse representation for different tasks, such as signal separation, denoising, coding, image inpainting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. For instance, in <ref type="bibr" target="#b5">[6]</ref>, sparse representation is used for image separation. The overcomplete dictionary is generated by combining multiple standard transforms, including curvelet transform, ridgelet transform and discrete cosine transform. In <ref type="bibr" target="#b6">[7]</ref>, application of the sparse representation to blind source separation is discussed and experimental results on EEG data analysis are demonstrated. In <ref type="bibr" target="#b7">[8]</ref>, a sparse image coding method with the wavelet transform is presented. In <ref type="bibr" target="#b8">[9]</ref>, sparse representation with an adaptive dictionary is shown to have state-of-the-art performance in image denoising. The widely used shrinkage method for image desnoising is shown to be the first iteration of basis pursuit that solves the sparse representation problem <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the standard framework of sparse representation, the objective is to reduce the signal reconstruction error with as few number of atoms as possible. On the other hand, discriminative analysis methods, such as LDA, are more suitable for the tasks of classification. However, discriminative methods are usually sensitive to corruption in signals due to lacking crucial properties for signal reconstruction. In this paper, we propose the method of sparse representation for signal classification (SRSC), which modifies the standard sparse representation framework for signal classification. We first show that replacing the reconstruction error with discrimination power in the objective function of the sparse representation is more suitable for the tasks of classification. When the signal is corrupted, the discriminative methods may fail because little information is contained in discriminative analysis to successfully deal with noise, missing data and outliers. To address this robustness problem, the proposed approach of SRSC combines discrimination power, signal reconstruction and sparsity in the objective function for classification. With the theoretical framework of SRSC, our objective is to achieve a sparse and robust representation of corrupted signals for effective classification.</p><p>The rest of this paper is organized as follows. Section 2 reviews the problem formulation and solution for the standard sparse representation. Section 3 discusses the motivations for proposing SRSC by analyzing the reconstructive methods and discriminative methods for signal classification. The formulation and solution of SRSC are presented in Section 4. Experimental results with synthetic and real data are shown in Section 5 and Section 6 concludes the paper with a summary of the proposed work and discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sparse Representation of Signal</head><p>The problem of finding the sparse representation of a signal in a given overcomplete dictionary can be formulated as follows. Given a N × M matrix A containing the elements of an overcomplete dictionary in its columns, with M &gt; N and usually M &gt;&gt; N, and a signal y ∈ R N , the problem of sparse representation is to find an M × 1 coefficient vector x, such that y = Ax and x 0 is minimized, i.e., x = min</p><formula xml:id="formula_0">x x 0 s.t. y = Ax.<label>(1)</label></formula><p>where x 0 is the 0 norm and is equivalent to the number of non-zero components in the vector x.</p><p>Finding the solution to equation ( <ref type="formula" target="#formula_0">1</ref>) is NP hard due to its nature of combinational optimization. Suboptimal solutions to this problem can be found by iterative methods like the matching pursuit and orthogonal matching pursuit. An approximate solution is obtained by replacing the 0 norm in equation ( <ref type="formula" target="#formula_0">1</ref>) with the 1 norm, as follows:</p><formula xml:id="formula_1">x = min x x 1 s.t. y = Ax.<label>(2)</label></formula><p>where x 1 is the 1 norm. In <ref type="bibr" target="#b10">[11]</ref>, it is proved that if certain conditions on the sparsity is satisfied, i.e., the solution is sparse enough, the solution of equation ( <ref type="formula" target="#formula_0">1</ref>) is equivalent to the solution of equation (2), which can be efficiently solved by basis pursuit using linear programming. A generalized version of equation ( <ref type="formula" target="#formula_1">2</ref>), which allows for certain degree of noise, is to find x such that the following objective function is minimized:</p><formula xml:id="formula_2">J 1 (x; λ) = y -Ax 2 2 + λ x 1<label>(3)</label></formula><p>where the parameter λ &gt; 0 is a scalar regularization parameter that balances the tradeoff between reconstruction error and sparsity. In <ref type="bibr" target="#b11">[12]</ref>, a Bayesian approach is proposed for learning the optimal value for λ. Except for the intuitive interpretation as obtaining a sparse factorization that minimizes signal reconstruction error, the problem formulated in equation ( <ref type="formula" target="#formula_2">3</ref>) has an equivalent interpretation in the framework of Bayesian decision as follows <ref type="bibr" target="#b12">[13]</ref>. The signal y is assumed to be generated by the following model:</p><formula xml:id="formula_3">y = Ax + ε (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ε is white Gaussian noise. Moreover, the prior distribution of x is assumed to be super-Gaussian:</p><formula xml:id="formula_5">p(x) ∼ exp -λ M i=1 |x i | p (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where p ∈ [0, 1]. This prior has been shown to encourage sparsity in many situations, due to its heavy tails and sharp peak. Given this prior, maximum a posteriori (MAP) estimation of x is formulated as</p><formula xml:id="formula_7">x MAP = arg max x p(x|y) = arg min x [-log p(y|x) -log p(x)] = arg min x ( y -Ax 2 2 + λ x p )<label>(6)</label></formula><p>when p = 0, equation ( <ref type="formula" target="#formula_7">6</ref>) is equivalent to the generalized form of equation ( <ref type="formula" target="#formula_0">1</ref>); when p = 1, equation ( <ref type="formula" target="#formula_7">6</ref>) is equivalent to equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reconstruction and Discrimination</head><p>Sparse representation works well in applications where the original signal y needs to be reconstructed as accurately as possible, such as denoising, image inpainting and coding. However, for applications like signal classification, it is more important that the representation is discriminative for the given signal classes than a small reconstruction error.</p><p>The difference between reconstruction and discrimination has been widely investigated in literature. It is known that typical reconstructive methods, such as principal component analysis (PCA) and independent component analysis (ICA), aim at obtaining a representation that enables sufficient reconstruction, thus are able to deal with signal corruption, i.e., noise, missing data and outliers.</p><p>On the other hand, discriminative methods, such as LDA <ref type="bibr" target="#b13">[14]</ref>, generate a signal representation that maximizes the separation of distributions of signals from different classes. While both methods have broad applications in classification, the discriminative methods have often outperformed the reconstructive methods for the classification task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. However, this comparison between the two types of method assumes that the signals being classified are ideal, i.e., noiseless, complete(without missing data) and without outliers. When this assumption does not hold, the classification may suffer from the nonrobust nature of the discriminative methods that contains insufficient information to successfully deal with signal corruptions. Specifically, the representation provided by the discriminative methods for optimal classification does not necessarily contain sufficient information for signal reconstruction, which is necessary for removing noise, recovering missing data and detecting outliers. This performance degradation of discriminative methods on corrupted signals is evident in the examples shown in <ref type="bibr" target="#b16">[17]</ref>. On the other hand, reconstructive methods have shown successful performance in addressing these problems. In <ref type="bibr" target="#b8">[9]</ref>, the sparse representation is shown to achieve state-of-the-art performance in image denoising. In <ref type="bibr" target="#b17">[18]</ref>, missing pixels in images are successfully recovered by inpainting method based on sparse representation. In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, PCA method with subsampling effectively detects and excludes outliers for the following LDA analysis.</p><p>All of these examples motivate the design of a new signal representation that combines the advantages of both reconstructive and discriminative methods to address the problem of robust classification when the obtained signals are corrupted. The proposed method should generate a representation that contain discriminative information for classification, crucial information for signal reconstruction and preferably the representation should be sparse. Due to the evident reconstructive properties <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>, the available efficient pursuit methods and the sparsity of representation, we choose the sparse representation as the basic framework for the SRSC and incorporate a measure of discrimination power into the objective function. Therefore, the sparse representation obtained by the proposed SRSC contains both crucial information for reconstruction and discriminative information for classification, which enable a reasonable classification performance in the case of corrupted signals. The three objectives: sparsity, reconstruction and discrimination may not always be consistent. Therefore, weighting factors are introduced to adjust the tradeoff among these objectives, as the weighting factor λ in equation <ref type="bibr" target="#b2">(3)</ref>. It should be noted that the aim of SRSC is not to improve the standard discriminative methods like LDA in the case of ideal signals, but to achieve comparable classification results when the signals are corrupted. A recent work <ref type="bibr" target="#b16">[17]</ref> that aims at robust classification shares some common ideas with the proposed SRSC. In <ref type="bibr" target="#b16">[17]</ref>, PCA with subsampling proposed in <ref type="bibr" target="#b18">[19]</ref> is applied to detect and exclude outliers in images and the rest of pixels are used for calculating LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sparse Representation for Signal Classification</head><p>In this section, the SRSC problem is formulated mathematically and a pursuit method is proposed to optimize the objective function. We first replace the term measuring reconstruction error with a term measuring discrimination power to show the different effects of reconstruction and discrimination. Further, we incorporate measure of discrimination power in the framework of standard sparse representation to effectively address the problem of classifying corrupted signals. The Fisher's discrimination criterion <ref type="bibr" target="#b13">[14]</ref> used in the LDA is applied to quantify the discrimination power. Other well-known discrimination criteria can easily be substituted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>Given y = Ax as discussed in Section 2, we view x as the feature extracted from signal y for classification. The extracted feature should be as discriminative as possible between the different signal classes. Suppose that we have a set of K signals in a signal matrix Y = [y 1 , y 2 , ..., y K ] with the corresponding representation in the overcomplete dictionary as X = [x 1 , x 2 , ..., x K ], of which K i samples are in the class C i , for 1 ≤ i ≤ C. Mean m i and variance s 2 i for class C i are computed in the feature space as follows:</p><formula xml:id="formula_8">m i = 1 K i x∈Ci x , s 2 i = 1 K i x∈Ci x -m i 2 2<label>(7)</label></formula><p>The mean of all samples are defined as:</p><formula xml:id="formula_9">m = 1 K K i=1</formula><p>x i . Finally, the Fisher's discrimination power can then be defined as:</p><formula xml:id="formula_10">F (X) = S B S W = C i=1 K i (m i -m)(m i -m) T 2 2 C i=1 s 2 i . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>The difference between the sample means</p><formula xml:id="formula_12">S B = C i=1 K i (m i -m)(m i -m) T 2 2</formula><p>can be interpreted as the 'inter-class distance' and the sum of variance S W = C i=1 s 2 i can be similarly interpreted as the 'inner-class scatter'. Fisher's criterion is motivated by the intuitive idea that the discrimination power is maximized when the spatial distribution of different classes are as far away as possible and the spatial distribution of samples from the same class are as close as possible.</p><p>Replacing the reconstruction error with the discrimination power, the objective function that focuses only on classification can be written as:</p><formula xml:id="formula_13">J 2 (X, λ) = F (X) -λ K i=1 x i 0 (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where λ is a positive scalar weighting factor chosen to adjust the tradeoff between discrimination power and sparsity. Maximizing J 2 (X, λ) generates a sparse representation that has a good discrimination power. When the discrimination power, reconstruction error and sparsity are combined, the objective function can be written as:</p><formula xml:id="formula_15">J 3 (X, λ 1 , λ 2 ) = F (X) -λ 1 K i=1 x i 0 -λ 2 K i=1 y i -Ax i 2 2<label>(10)</label></formula><p>where λ 1 and λ 2 are positive scalar weighting factors chosen to adjust the tradeoff between the discrimination power, sparsity and the reconstruction error. Maximizing J 3 (X, λ 1 , λ 2 ) ensures that a representation with discrimination power, reconstruction property and sparsity is extracted for robust classification of corrupted signals. In the case that the signals are corrupted, the two terms</p><formula xml:id="formula_16">K i=1 x i 0 and K i=1 y i -Ax i 2 2</formula><p>robustly recover the signal structure, as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. On the other hand, the inclusion of the term F (X) requires that the obtained representation contains discriminative information for classification. In the following discussions, we refer to the solution of the objective function J 3 (X, λ 1 , λ 2 ) as the features for the proposed SRSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problem Solution</head><p>Both the objective function J 2 (X, λ) defined in equation ( <ref type="formula" target="#formula_13">9</ref>) and the objective function J 3 (X, λ 1 , λ 2 ) defined in equation <ref type="bibr" target="#b9">(10)</ref> have similar forms to the objective function defined in the standard sparse representation, as J 1 (x; λ) in equation ( <ref type="formula" target="#formula_2">3</ref>). However, the key difference is that the evaluation of F (X) in J 2 (X, λ) and J 3 (X, λ 1 , λ 2 ) involves not only a single sample, as in J 1 (x; λ), but also all other samples. Therefore, not all the pursuit methods, such as basis pursuit and LARS/Homotopy methods, that are applicable to the standard sparse representation method can be directly applied to optimize J 2 (X, λ) and J 3 (X, λ 1 , λ 2 ). However, the iterative optimization methods employed in the matching pursuit and the orthogonal matching pursuit provide a direct reference to the optimization of J 2 (X, λ) and J 3 (X, λ 1 , λ 2 ). In this paper, we propose an algorithm similar to the orthogonal matching pursuit and inspired by the simultaneous sparse approximation algorithm described in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Taking the optimization of J 3 (X, λ 1 , λ 2 ) as example, the pursuit algorithm can be summarized as follows:</p><p>1. Initialize the residue matrix R 0 = Y and the iteration counter t = 0.</p><p>2. Choose the atom from the dictionary, A, that maximizes the objective function:</p><formula xml:id="formula_17">g = argmax g∈A J 3 (g T R t , λ 1 , λ 2 )<label>(11)</label></formula><p>3. Determine the orthogonal projection matrix P t onto the span of the chosen atoms, and compute the new residue.</p><formula xml:id="formula_18">R t = Y -P t Y<label>(12)</label></formula><p>4. Increment t and return to Step 2 until t is less than or equal to a pre-determined number.</p><p>The pursuit algorithm for optimizing J 2 (X, λ) also follows the same steps. Detailed analysis of this pursuit algorithm can be found in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Two sets of experiments are conducted. In Section 5.1, synthesized signals are generated to show the difference between the features extracted by J 1 (X, λ) and J 2 (X, λ), which reflects the properties of reconstruction and discrimination. In Section 5.2, classification on real data is shown. Random noise and occlusion are added to the original signals to test the robustness of SRSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Example</head><p>Two simple signal classes, f 1 (t) and f 2 (t), are constructed with the Fourier basis. The signals are constructed to show the difference between the reconstructive methods and discriminative methods. The scalar g 1 is uniformly distributed in the interval [0, 5], and the scalar g 2 is uniformly distributed in the interval <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. The scalar h 1 and h 2 are uniformly distributed in the interval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. Therefore, most of the energy of the signal can be described by the sine function and most of the discrimination power is in the cosine function. The signal component with most energy is not necessary the component with the most discrimination power. Construct a dictionary as {sin t, cos t}, optimizing the objective function J 1 (X, λ) with the pursuit method described in Section 4.2 selects sin t as the first atom. On the other hand, optimizing the objective function J 2 (X, λ) selects cos t as the first atom. In the simulation, 100 samples are generated for each class and the pursuit algorithm stops at the first run. The projection of the signals from both classes to the first atom selected by J 1 (X, λ) and J 2 (X, λ) are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The difference shown in the figures has direct impact on the classification.</p><formula xml:id="formula_19">f 1 (t) = g 1 cos t + h 1 sin t (13) f 2 (t) = g 2 cos t + h 2 sin t<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real Example</head><p>Classification with J 1 , J 2 and J 3 (SRSC) is also conducted on the database of USPS handwritten digits <ref type="bibr">[22]</ref>. The database contains 8-bit grayscale images of "0" through "9" with a size of 16 × 16 and there are 1100 examples of each digit. Following the conclusion of <ref type="bibr" target="#b21">[23]</ref>, 10-fold stratified cross validation is adopted. Classification is conducted with the decomposition coefficients (' X' in equation ( <ref type="formula" target="#formula_15">10</ref>)) as feature and support vector machine (SVM) as classifier. In this implementation, the overcomplete dictionary is a combination of Haar wavelet basis and Gabor basis. Haar basis is good at modelling discontinuities in signal and on the other hand, Gabor basis is good at modelling continuous signal components.</p><p>In this experiment, noise and occlusion are added to the signals to test the robustness of SRSC. First, white Gaussian noise with increasing level of energy, thus decreasing level of signal-to-noise ratio (SNR), are added to each image. Table <ref type="table" target="#tab_0">1</ref> summarizes the classification error rates obtained with different SNR. Second, different sizes of black squares are overlayed on each image at a random location to generate occlusion (missing data). For the image size of 16 × 16, black squares with size of 3 × 3, 5 × 5, 7 × 7, 9 × 9 and 11 × 11 are overlayed as occlusion. Table <ref type="table">2</ref> summarizes the classification error rates obtained with occlusion.</p><p>Results in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table">2</ref> show that in the case that signals are ideal (without missing data and noiseless) or nearly ideal, J 2 (X, λ) is the best criterion for classification. This is consistent with the known conclusion that discriminative methods outperform reconstructive methods in classification. However, when the noise is increased or more data is missing (with larger area of occlusion), the accuracy based on J 2 (X, λ) degrades faster than the accuracy base on J 1 (X, λ). This indicates </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>In summary, sparse representation for signal classification(SRSC) is proposed. SRSC is motivated by the ongoing researches in the area of sparse representation in the signal processing area. SRSC incorporates reconstruction properties, discrimination power and sparsity for robust classification. In current implementation of SRSC, the weight factors are empirically set to optimize the performance. Approaches to determine optimal values for the weighting factors are being conducted, following the methods similar to that introduced in <ref type="bibr" target="#b11">[12]</ref>.</p><p>It is interesting to compare SRSC with the relevance vector machine (RVM) <ref type="bibr" target="#b22">[24]</ref>. RVM has shown comparable performance to the widely used support vector machine (SVM), but with a substantially less number of relevance/support vectors. Both SRSC and RVM incorporate sparsity and reconstruction error into consideration. For SRSC, the two terms are explicitly included into objective function. For RVM, the two terms are included in the Bayesian formula. In RVM, the "dictionary" used for signal representation is the collection of values from the "kernel function". On the other hand, SRSC roots in the standard sparse representation and recent developments of harmonic analysis, such as curvelet, bandlet, contourlet transforms that show excellent properties in signal modelling. It would be interesting to see how RVM works by replacing the kernel functions with these harmonic transforms. Another difference between SRSC and RVM is how the discrimination power is incorporated. The nature of RVM is function regression. When used for classification, RVM simply changes the target function value to class membership. For SRSC, the discrimination power is explicitly incorporated by inclusion of a measure based on the Fisher's discrimination. The adjustment of weighting factor in SRSC (in equation ( <ref type="formula" target="#formula_15">10</ref>)) may give some flexibility for the algorithm when facing various noise levels in the signals. A thorough and systemic study of connections and difference between SRSC and RVM would be an interesting topic for the future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributions of projection of signals from two classes with the first atom selected by: J 1 (X, λ) (the left figure) and J 2 (X, λ) (the right figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification error rates with different levels of white Gaussian noise structures recovered by the standard sparse representation are more robust to noise and occlusion, thus yield less performance degradation. On the other hand, the SRSC demonstrates lower error rate by the combination of the reconstruction property and the discrimination power in the case that signals are noisy or with occlusions.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">N oiseless</cell><cell>20db</cell><cell>15db</cell><cell>10db</cell><cell>5db</cell></row><row><cell>J 1 (Reconstruction)</cell><cell></cell><cell>0.0855</cell><cell cols="4">0.0975 0.1375 0.1895 0.2310</cell></row><row><cell cols="2">J 2 (Discrimination)</cell><cell>0.0605</cell><cell cols="4">0.0816 0.1475 0.2065 0.2785</cell></row><row><cell>J 3 (SRSC)</cell><cell></cell><cell>0.0727</cell><cell cols="4">0.0803 0.1025 0.1490 0.2060</cell></row><row><cell cols="7">Table 2: Classification error rates with different sizes of occlusion</cell></row><row><cell cols="3">no occlusion</cell><cell>3 × 3</cell><cell>5 × 5</cell><cell>7 × 7</cell><cell>9 × 9</cell><cell>11 × 11</cell></row><row><cell>J 1 (Reconstruction)</cell><cell cols="2">0.0855</cell><cell cols="4">0.0930 0.1270 0.1605 0.2020</cell><cell>0.2990</cell></row><row><cell>J 2 (Discrimination)</cell><cell cols="2">0.0605</cell><cell cols="4">0.0720 0.1095 0.1805 0.2405</cell><cell>0.3305</cell></row><row><cell>J 3 (SRSC)</cell><cell cols="2">0.0727</cell><cell cols="4">0.0775 0.1135 0.1465 0.1815</cell><cell>0.2590</cell></row><row><cell>that the signal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching pursuits with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Asilomar Conference on Signals, Systems, and Computers</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Solution of L1 minimization problems by LARS/Homotopy methods</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="636" to="639" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The K-SVD: An algorithm for designing of overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Signal Processing</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image decomposition via the combination of sparse representation and a variational approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1570" to="1582" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of sparse representation and blind source separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1193" to="1234" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning sparse image codes using a wavelet pyramid architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sallee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="887" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image denoising via learned dictionaries and sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising with shrinkage and redundant representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Matalon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zibulevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uncertainty principles and ideal atomic decomposition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2845" to="2862" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian L1-Norm sparse learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="605" to="608" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse bayesian learning for basis selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2153" to="2164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern classification</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining reconstructive and discriminative subspace methods for robust classification and regression by subsampling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skocaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="350" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous cartoon and texture image inpainting using morphological component analysis (MCA)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Querre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="340" to="358" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust recognition using eigenimages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="99" to="118" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for simultaneous sparse approximation. part I: Greedy pursuit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing, special issue on Sparse approximations in signal and image processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithms for simultaneous sparse approximation. part II: Convex relaxation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing, special issue on Sparse approximations in signal and image processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="589" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1145" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse bayesian learning and the relevance vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
