<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
							<email>yong.ge@uncc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of North Carolina at Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Leon</forename><surname>Zhao</surname></persName>
							<email>jlzhao@cityu.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B61630E988E0C6CEF484A4E7E76476C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series (particularly multivariate) classification has drawn a lot of attention in the literature because of its broad applications for different domains, such as health informatics and bioinformatics. Thus, many algorithms have been developed for this task. Among them, nearest neighbor classification (particularly 1-NN) combined with Dynamic Time Warping (DTW) achieves the state of the art performance. However, when data set grows larger, the time consumption of 1-NN with DTW grows linearly. Compared to 1-NN with DTW, the traditional feature-based classification methods are usually more efficient but less effective since their performance is usually dependent on the quality of hand-crafted features. To that end, in this paper, we explore the feature learning techniques to improve the performance of traditional feature-based approaches. Specifically, we propose a novel deep learning framework for multivariate time series classification. We conduct two groups of experiments on real-world data sets from different application domains. The final results show that our model is not only more efficient than the state of the art but also competitive in accuracy. It also demonstrates that feature learning is worth to investigate for time series classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a large amount of time series data have been collected in many domains such as finance and bioinformatics, time series data mining has drawn a lot of attention in the literature. Particularly, multivariate time series classification is becoming very important in a broad range of real-world applications, such as health care and activity recognition <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>.</p><p>In recent years, a plenty of classification algorithms for time series data have been developed. Among these classification methods, the distance-based method k-Nearest Neighbor (k-NN) classification has been empirically proven to be very difficult to beat <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Also, more and more evidences have shown that the Dynamic Time Warping (DTW) is the best sequence distance measurement in most domains <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Thus, the simple combination of k-NN and DTW could reach the best performance of classification in most domains <ref type="bibr" target="#b5">[6]</ref>. Other than sequence distance based methods, feature-based classification methods <ref type="bibr" target="#b7">[8]</ref> follow the traditional classification framework. As is known to all, the performance of traditional feature-based methods depends on the quality of hand-crafted features. However, unlike other applications, it is difficult to design good features to capture intrinsic properties embedded in various time series data. Therefore, the accuracy of feature-based methods is usually worse than that of sequence distance based ones, particularly 1-NN with DTW method. On the other hand, although many research works use 1-NN and DTW, both of them cause too much computation for many real-world applications <ref type="bibr" target="#b6">[7]</ref>.</p><p>Motivation. Is it possible to improve the accuracy of feature-based methods? So that the feature-based methods are not only superior to 1-NN with DTW in efficiency but also competitive to it in accuracy.</p><p>Inspired by the deep feature learning for image classification <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, in this paper, we explore a deep learning framework for multivariate time series classification. Deep learning does not need any hand-crafted features by people, instead it can learn a hierarchical feature representation from raw data automatically. Specifically, we propose an effective Multi-Channels Deep Convolution Neural Networks (MC-DCNN) model, each channel of which takes a single dimension of multivariate time series as input and learns features individually. Then the MC-DCNN model combines the learnt features of each channel and feeds them into a Multilayer Perceptron (MLP) to perform classification finally. To estimate the parameters, we utilize the gradient-based method to train our MC-DCNN model. We evaluate the performance of our MC-DCNN model on two real-world data sets. The experimental results on both data sets show that our MC-DCNN model outperforms the baseline methods with significant margins and has a good generalization, especially for weakly labeled data.</p><p>The rest of the paper is organized as follows. Section 2 depicts the definitions and notations used in the paper. In section 3, we present the architecture of MC-DCNN, and describe how to train the neural networks. In section 4, we conduct experiments on two real-world data sets and evaluate the performance of each model. We make a short review of related work in section 5. Finally, we conclude the paper and discuss future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions and Notations</head><p>In this section, we introduce the definitions and notations used in the paper. As previous works shown <ref type="bibr" target="#b11">[12]</ref>, it's common to extract subsequences from long time series to do classification instead of classifying with the whole sequence. Definition 3 Subsequence is a sequence of consecutive points which are extracted from time series T and can be denoted as S = {t i , t i+1 , ..., t i+k-1 }, where k is the length of subsequence. Similarly, multivariate subsequence can be denoted as</p><formula xml:id="formula_0">Y = {m •i , m •i+1 , ..., m •i+k-1 }, where m •i is defined in Definition 2.</formula><p>Since we perform classification on multivariate subsequences in our work, in remainder of the paper, we use subsequence standing for both univariate and multivariate subsequence for short according to the context. For a long-term time series, domain experts may manually label and align subsequences based on experience. We define this type of data as well aligned and labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4</head><p>Well aligned and labeled data: Subsequences are labeled by domain experts, and different subsequences belonging to same pattern are well aligned.</p><p>Fig. <ref type="figure">1</ref> shows a snippet of time series extracted from BIDMC Congestive Heart Failure data set <ref type="bibr" target="#b12">[13]</ref>. Each subsequence is extracted and labeled according to the red dotted line by medical staffs. However, to acquire the well aligned and labeled data, it always needs great manual cost. In contrast to well aligned and labeled data, in practice, weakly labeled data can be obtained more easily <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>. We define it as follows. Definition 5 Weakly labeled data: A long-term time series is associated with a single global label as shown in Fig. <ref type="figure">2</ref>.</p><p>Due to the alignment-free property of weakly labeled data, it requires to extract subsequences by specific algorithm. The most widely used algorithm is sliding window <ref type="bibr" target="#b13">[14]</ref>. By specifying sliding step, we can extract large amount of redundant subsequences from long-term time series.</p><p>In summary, in this paper, we will primarily concentrate on the time series of the same length and conduct experiments on both labeled data that is well aligned and weakly labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Channels Deep Convolutional Neural Networks</head><p>In this section, we will introduce a deep learning framework for multivariate time series classification: Multi-Channels Deep Convolutional Neural Networks (MC-DCNN). Traditional Convolutional Neural Networks (CNN) usually include two parts. One is a feature extractor, which learns features from raw data automatically. And the other is a trainable fully-connected MLP, which performs classification based on the learned features from the previous part. Generally, the feature extractor is composed of multiple similar stages, and each stage is made up of three cascading layers: filter layer, activation layer and pooling layer. The input and output of each layer are called feature maps <ref type="bibr" target="#b10">[11]</ref>. In the previous work of CNN <ref type="bibr" target="#b10">[11]</ref>, the feature extractor usually contains one, two or three such 3-layers stages. Due to space constraint, we only introduce the components of CNN briefly. More details of CNN can be referred to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>In contrast to image classification, the input of multivariate time series classification are multiple 1D subsequences but not 2D image pixels. We modify the traditional CNN and apply it to multivariate time series classification task in this way: we separate multivariate time series into univariate ones and perform feature learning on each univariate series individually. Then we concatenate a normal MLP at the end of feature learning to do classification. To be understood easily, we illustrate the architecture of MC-DCNN in Fig. <ref type="figure" target="#fig_1">3</ref>. Specifically, this is an example of 2-stages MC-DCNN for activity classification. It includes 3-channels inputs and the length of each input is 256. For each channel, the input (i.e., the univariate time series) is fed into a 2-stages feature extractor, which learns hierarchical features through filter, activation and pooling layers. At the end of feature extractor, we flatten the feature maps of each channel and combine them as the input of subsequent MLP for classification. Note that in Fig. <ref type="figure" target="#fig_1">3</ref>, the activation layer is embedded into filter layer in the form of non-linear operation on each feature map. Next, we describe how each layer works.</p><p>Filter Layer. The input of each filter is a univariate time series, which is denoted</p><formula xml:id="formula_1">x l i ∈ n l 2 , 1 ≤ i ≤ n l 1</formula><p>, where l denotes the layer which the time series comes from, n l 1 and n l 2 are number and length of input time series. To capture local temporal information, it requires to restrict each trainable filter k ij with a small size, which is denoted m l 2 , and the number of filter at layer l is denoted m l 1 . Recalling the example described in Fig. <ref type="figure" target="#fig_1">3</ref>, in first stage of channel 1, we have n l 1 = 1, n l 2 = 256, m l 2 = 5 and m l 1 = 8. We compute the output of each filter according to this:</p><formula xml:id="formula_2">i x l-1 i * k l ij + b l j</formula><p>, where the * is convolution operator and b l j is the bias term.</p><p>Activation Layer. The activation function introduces the non-linearity into neural networks and allows it to learn more complex model. The most widely used activation functions are sigmoid(t) = 1 1+e -t and tanh(•). In this paper, we adopt sigmoid(•) function in all activation layers due to its simplicity.</p><p>Pooling Layer. Pooling is also called subsampling because it usually subsamples the input feature maps by a specific factor. The purpose of pooling layer is to reduce the resolution of input time series, and make it robust to small variations for previous learned features. The simplest yet most popular method is to compute average value in each neighborhood at different positions with or without overlapping. The neighborhood is usually constructed by splitting input feature maps into equal length (larger than 1) subsequences. We utilize average pooling without overlapping for all stages in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient-Based Learning of MC-DCNN</head><p>The same as traditional MLP, for multi-class classification task, the loss function of our MC-DCNN model is defined as: E =t k y * k (t) log (y k (t)), where y * k (t) and y k (t) are the target and predicted values of t-th training example at k-th class, respectively. To estimate parameters of models, we utilize gradientbased optimization method to minimize the loss function. Specifically, we use simple backpropagation algorithm to train our MC-DCNN model, since it is efficient and most widely used in neural networks <ref type="bibr" target="#b15">[16]</ref>. We adopt stochastic gradient descent (SGD) instead of full-batch version to update the parameters. Because SGD could converge faster than full-batch for large scale data sets <ref type="bibr" target="#b15">[16]</ref>.</p><p>A full cycle of parameter updating procedure includes three cascaded phases <ref type="bibr" target="#b16">[17]</ref>: feedforward pass, backpropagation pass and the gradient applied.</p><p>Feedforward Pass. The objective of feedforward pass is to determine the predicted output of MC-DCNN on input vectors. Specifically, it computes feature maps from layer to layer and stage to stage until obtaining the output. As shown in the previous content, each stage contains three cascaded layers, and activation layer is embedded into filter layer in form of non-linear operation on each feature map. We compute output feature map of each layer as follows:</p><formula xml:id="formula_3">z l j = i x l-1 i * k l ij + b l j , x l j = sigmoid(z l j ), x l+1 j = down(x l j )</formula><p>where down(•) represents the subsampling function for average pooling, x l-1 i and z l j denote the input and output of filter layer, z l j and x l j denote the input and output of activation layer, x l j and x l+1 j denote the input and output of pooling layer.</p><p>Eventually, a 2-layer fully-connected MLP is concatenated to feature extractor. Since feedforward pass of MLP is standard and also the space is limited, more details of MLP can be referred to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Backpropagation Pass. Once acquiring predicted output y, the predicted error E can be calculated according to the loss function. By taking advantage of chain-rule of derivative, the predicted error propagates back on each parameter of each layer one by one, which can be used to work out the derivatives of them. We still don't present backpropagation pass of final MLP for the same reason of feedforward pass.</p><p>For pooling layer in the second stage of feature extractor, the derivative of x l-1 j is computed by the upsampling function up(•), which is an inverse operation opposite to the subsampling function down(•) for the backward propagation of errors in this layer.</p><formula xml:id="formula_4">∂E ∂x l-1 j = up( ∂E ∂x l j )</formula><p>For filter layer in second stage of feature extractor, derivative of z l j is computed similar to that of MLP's hidden layer:</p><formula xml:id="formula_5">δ l j = ∂E ∂z l j = ∂E ∂x l j ∂x l j ∂z l j = sigmoid (z l j ) • up( ∂E ∂x l+1 j )</formula><p>where • denotes element-wise product. Since the bias is a scalar, to compute its derivative, we should summate over all entries in δ l j as follows:</p><formula xml:id="formula_6">∂E ∂b l j = u (δ l j ) u</formula><p>The difference between kernel weight k l ij and MLP's weight w l ij is the weight sharing constraint, which means the weights between (k l ij ) u and each entry of x l j must be the same. Due to this constraint, the number of parameters is reduced by comparing with the fully-connected MLP, Therefore, to compute the derivative of kernel weight k l ij , it needs to summate over all quantities related to this kernel. We perform this with convolution operation:</p><formula xml:id="formula_7">∂E ∂k l ij = ∂E ∂z l j ∂z l j ∂k l ij = δ l j * reverse(x l-1 i )</formula><p>where reverse(•) is the function of reversing corresponding feature map. Finally, we compute the derivative of x l-1 i as follows:</p><formula xml:id="formula_8">∂E ∂x l-1 i = j ∂E ∂z l j ∂z l j ∂x l-1 i = j pad(δ l j ) * reverse(k l ij )</formula><p>where pad(•) is a function which pads zeros into δ l j from two ends, e.g., if the size of k l ij is n l 2 , then this function will pad each end of δ l j with n l 2 -1 zeros.</p><p>Gradients Applied. Once we obtain the derivatives of parameters, it's time to apply them to update parameters. To converge fast, we utilize decay and momentum strategies <ref type="bibr" target="#b15">[16]</ref>. The weight w l ij in MLP is updated in this way:</p><formula xml:id="formula_9">w l ij = w l ij + Δw l ij Δw l ij = momentum • Δw l ij -decay • • w l ij -• ∂E ∂w l ij</formula><p>where w l ij represents the weight between x l-1 i and x l j , Δw l ij denotes the gradient of w l ij , and denotes the learning rate. The kernel weight k l ij , the bias term b l j in filter layer and b l in MLP are updated similar to the way of w l ij . The same as <ref type="bibr" target="#b17">[18]</ref>, we set momentum = 0.9, decay = 0.0005 and = 0.01 for our experiments. It is noted that <ref type="bibr" target="#b18">[19]</ref> claimed that both the initialization and the momentum are crucial for deep neural networks, hence, we consider how to select these values as a part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we will conduct two groups of experiments on real-world data sets from two different application domains. Particularly, we will show the performance of our methods via comparing with other baseline models in terms of both efficiency and accuracy.</p><p>To the best of our knowledge, indeed, there are many public time series data sets available, e.g., the UCR Suite <ref type="bibr" target="#b19">[20]</ref>. However, we decide not using the UCR Suite for the following reasons. First, we focus on the classification of multivariate time series, whereas most data sets in UCR Suite only contain univariate time series. Second, data sets in UCR Suite are usually small and CNN may not work well on such small data sets <ref type="bibr" target="#b20">[21]</ref>. Thus, we choose two data sets which are collected from real-world applications, and we will introduce the data sets in the next subsections.</p><p>We consider three approaches as baseline methods for evaluation: 1-NN (ED), 1-NN (DTW-5%) and MLP. Here, 1-NN (ED) and 1-NN (DTW-5%) are the methods that combine Euclidean Distance and Window Constraint DTW <ref type="bibr" target="#b6">[7]</ref>) 1 with 1-NN, respectively. Besides these two state-of-the-art methods, MLP is chosen to demonstrate that the feature learning process can improve the classification accuracy effectively. For the purpose of comparison, we record the performance of each method by tuning their parameters. Notice that some other classifiers are not considered here, since it is difficult to construct hand-crafted features for time series and many previous works have claimed that feature-based methods cannot achieve the accuracy as high as 1-NN methods. Also, we do not choose the full DTW due to its expensive time consumption. Actually, at least more than a month will be cost if we use full DTW in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Activity Classification (Weakly Labeled Data)</head><p>Data Set. We use the weakly labeled PAMAP2 data set for activity classification . It records 19 physical activities performed by 9 subjects. On a machine with Intel I5-2410 (2.3GHz) CPU and 8G Memory (our experimental platform), according to the estimation, it will cost nearly a month for 1-NN (DTW-5%) on this data set if we use all the 19 physical activities. Hence, currently, we only consider 4 out of these 19 physical activities in our work, which are 'standing', 'walking', 'ascending stairs' and 'descending stairs'. And each physical activity corresponds to a 3D time series. Moreover, 7 out of these 9 subjects are chosen. Because the other two either have different physical activities or have different dominant hand/foot.   Experiment Setup. We normalize each dimension of 3D time series as x-μ σ , where μ and σ are mean and standard deviation of time series. Then we apply the sliding window algorithm to extract subsequences from 3D time series with different sliding steps. To evaluate the performance of different models, we adopt the leave-one-out cross validation (LOOCV) technique. Specifically, each time we use one subject's physical activities as test data, and the physical activities of remaining subjects as training data. Then we repeat this for every subject. To glance the impact of depths, we evaluate two models: MC-DCNN(1), MC-DCNN <ref type="bibr" target="#b1">(2)</ref>. They are 1-stage and 2-stages feature learning models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>To evaluate efficiency and scalability of each model, we get five data splits with different volumes by setting sliding step as 128, 64,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step = 128</head><p>Step = 64</p><p>Step = 32</p><p>Step = 16</p><p>Step = 8 0.7 0.8 0.9 0.7 0.8 0.9</p><p>Accuracy</p><formula xml:id="formula_10">Models • • MLP 1-NN (ED) 1-NN (DTW-5%) MC-DCNN(1) MC-DCNN(2)</formula><p>Fig. <ref type="figure">6</ref>. Classification accuracy on each subject with different sliding steps 32, 16, 8, respectively. In addition, to ensure each subsequence to cover at least one pattern of time series, we set the sliding window length as 256.</p><p>As is well known, feature-based models have an advantage over lazy classification models (e.g., k-NN) in efficiency. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, the prediction time of 1-NN model increases linearly as the size of training data set grows. In contrast, the prediction time of our MC-DCNN model is almost constant no matter how large the training data is.</p><p>We also evaluate accuracy of each model on these five data splits. Fig. <ref type="figure">6</ref> shows the detailed accuracy comparisons of each subject at different step settings. From this figure we can see that for each subject our MC-DCNN model is either the most accuracy one or very close to the most accuracy one. Especially, for subject 3, the 2-stages MC-DCNN leads to much better accuracy than other approaches. We suppose that 2-stages MC-DCNN may learn high-level and robust feature representations so that it has a good generalization. We also show the average and standard deviation of accuracy in Table <ref type="table" target="#tab_0">1</ref>. From the table we can see that our model leads to the highest average accuracy and the lowest standard deviation. claimed that 1-NN combined DTW is the current state of the art <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. However, the biggest weakness of 1-NN with DTW model is its expensive computation <ref type="bibr" target="#b6">[7]</ref>. To overcome this drawback, a part of researchers explored to speed up the computation of distance measure (e.g., DTW) in certain methods (e.g., with boundary conditions) <ref type="bibr" target="#b6">[7]</ref>. While another part of researchers tried to reduce the computation of 1-NN by constructing data dictionary <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. When the data set grows large, all these approaches improve the performance significantly in contrast to simple 1-NN with DTW. Some feature-based models have been explored for time series classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, however, most of previous works extracted the hand-crafted statistical features based on domain knowledge, and achieved the performance not as well as sequence distance based models. Feature learning (or representation learning) is becoming an important field in machine learning community in recent years <ref type="bibr" target="#b8">[9]</ref>. The most successful feature learning framework is deep neural networks, which build hierarchical representations from raw data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. Particularly, as a supervised feature learning model, deep convolutional neural networks achieve remarkable successes in many tasks such as digit and object recognition <ref type="bibr" target="#b17">[18]</ref>, which motivates us to investigate the deep learning in time series field. In the literature, there are few works on time series classification using deep learning. Ref. <ref type="bibr" target="#b24">[25]</ref> explored an unsupervised feature learning method with convolutional deep belief networks for audio classification, but in frequency domain rather than in time domain. Ref. <ref type="bibr" target="#b2">[3]</ref> adopted a special time delay neural network (TDNN) model for electroencephalography (EEG) classification. However, their TDNN model only included a single hidden layer, which is not deep enough to learn good hierarchical features. To the best of our knowledge, none of existing works on time series classification has considered the supervised feature learning from raw data. In this paper, we explore a MC-DCNN model for multivariate time series classification and intend to investigate this problem in another way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Time series classification is becoming very important in a broad range of realworld applications, such as health care and activity recognition. However, most existing methods have high computational complexity or low prediction accuracy. To this end, we developed a novel deep learning framework (MC-DCNN) to classify multivariate time series in the paper. This model learns features from individual univariate time series in each channel automatically, and combines information from all channels as feature representation at final layer. A traditional MLP is concatenated to perform classification. We evaluated our MC-DCNN model on two real-world data sets. Experimental results show that our MC-DCNN model outperforms the competing baseline methods on both data sets, especially, the improvement of accuracy on weakly labeled data set is significant. Also, we showed that 2-stages MC-DCNN is superior to 1-stage MC-DCNN. It provides the evidence that the deeper architecture can learn more robust highlevel features, which is helpful for improving performance of classification.</p><p>There are several research directions for future work. First, in this paper we simply use the 1-stage and 2-stages feature learning for better illustration, and in the future we plan to study and extend other deep learning models for multivariate time series classification on more data sets. Second, we also intend to perform unsupervised algorithms on unlabeled data to pre-train the networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. A snippet of time series which contains two types of heartbeat: normal (N) and ventricular fibrillation (V)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A 2-stages MC-DCNN architecture for activity classification. This architecture consists of 3 channels input, 2 filter layers, 2 pooling layers and 2 fully-connected layers. This architecture is denoted as 8(5)-2-4(5)-2-732-4 based on the template C1 (Size)-S1 -C2 (Size)-S2 -H -O, where C1 and C2 are numbers of filters in first and second stage, Size denotes the kernel size, S1 and S2 are subsampling factors, H and O denote the numbers of units in hidden and output layers of MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Size of Training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Prediction time of each model on training sets with different size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The box-and-whisker plot of classification accuracy on BIDMC data set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Definition 1</head><label>1</label><figDesc>Univariate time series is a sequence of data points, measured typically at successive points in time spaced at uniform time intervals. A univariate time series can be denoted as T = {t 1 , t 2 , ..., t n }, and n is the length of T.</figDesc><table /><note><p>Definition 2 Multivariate time series is a set of time series with the same timestamps. For a multivariate time series M, each element m i is a univariate time series. At any timestamp t, m •t = {m 1t , m 2t , ..., m lt }, where l is the number of univariate time series in M.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average and standard deviation of accuracy of each model at different sliding step. Bold numbers represent the best results.</figDesc><table><row><cell cols="2">Step 1-NN (DTW-5%)</cell><cell>MLP</cell><cell>1-NN (ED) MC-DCNN(1) MC-DCNN(2)</cell></row><row><cell>128</cell><cell>83.46 (0.063)</cell><cell cols="2">77.89 (0.076) 79.05 (0.076) 88.73 (0.057) 90.34 (0.031)</cell></row><row><cell>64</cell><cell>84.51 (0.070)</cell><cell cols="2">80.09 (0.098) 80.25 (0.089) 90.38 (0.050) 91.00 (0.033)</cell></row><row><cell>32</cell><cell>84.44 (0.080)</cell><cell cols="2">82.49 (0.096) 80.74 (0.094) 90.28 (0.063) 91.14 (0.031)</cell></row><row><cell>16</cell><cell>84.16 (0.094)</cell><cell cols="2">84.34 (0.104) 81.74 (0.096) 90.75 (0.062) 93.15 (0.019)</cell></row><row><cell>8</cell><cell>83.61 (0.104)</cell><cell cols="2">84.83 (0.115) 82.28 (0.103) 90.53 (0.065) 93.36 (0.015)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.physionet.org/physiobank/database/chfdb/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This research was partially supported by grants from the National Science Foundation for Distinguished Young Scholars of China (Grant No. 61325010), the National High Technology Research and Development Program of China (Grant No.2014AA015203), the Anhui Provincial Natural Science Foundation (Grant No. 1408085QF110), the Science and Technology Development of Anhui Province, China (Grants No. 13Z02008-5 and 1301022064), and the International Science &amp; Technology Cooperation Plan of Anhui Province (Grant No. 1303063008).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this experiment, we consider four types of heartbeats to evaluate all the models: 'N', 'V', 'S', 'r'.</p><p>Experiment Setup. We still normalize each univariate of 2D time series as mentioned before. Different from weakly data, we extract subsequences centered at aligned marks (red dotted line in Fig. <ref type="figure">1</ref>). And each subsequence still has a length of 256. Similar to the classification of individuals' heartbeats <ref type="bibr" target="#b11">[12]</ref>, we mix all data of 15 subjects and randomly split it into 10 folds to perform 10-folds cross validation. Because as <ref type="bibr" target="#b11">[12]</ref> noted, it can be able to obtain huge amounts of labeled data in this way and a unhealthy individual may have many different types of heartbeats. To glance the impact of depths, we also evaluate two models: MC-DCNN(1), MC-DCNN(2). The former performs 1-stage feature learning, and the latter performs 2-stages. To determine the epochs, we separate one third of training data as validation set. As shown in Fig. <ref type="figure">7</ref>, we set epoch to 40 and 80 for 1-stage and 2-stages MC-DCNN models respectively. Since the test error is stable when epochs are greater than them.</p><p>Experimental Results. We illustrate the accuracy of each model on BIDMC data set in Fig. <ref type="figure">5</ref>. From this figure, we can see that accuracies of 1-stage MC-DCNN and 2-stages MC-DCNN models are 94.67% and 94.65%, which are also higher than the accuracies of 1-NN(ED) (93.64%), 1-NN(DTW-5%) (92.90%) and MLP (94.22%). Due to the space limit we do not report the prediction time of each model on BIDMC data set. However, the result is similar to Fig. <ref type="figure">4</ref> and it also supports that feature-based models have an advantage over lazy classification models (e.g., k-NN) in efficiency.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Many time series classification methods have been proposed based on different sequence distance measurements. Among these previous works, some researchers</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introducing a modular activity monitoring system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>EMBC</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5621" to="5624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heartbeat time series classification with support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kampouraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Manis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nikou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="512" to="518" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using time-dependent neural networks for EEG classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Haselsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="463" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A complexity-invariant distance measure for time series</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Conf. Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Querying and mining of time series data: experimental comparison of representations and distance measures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1542" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching and mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page">262</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast time series classification using numerosity reduction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A brief survey on sequence classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sigkdd Explorations</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5538</idno>
		<title level="m">Representation learning: A review and new perspectives</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series. The Handbook of Brain Theory and</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3361</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>2010 IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Time Series Classification under More Realistic Assumptions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">578</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A N</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PhysioBank, Phys-ioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="e220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient backProp</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
	<note>NIPS-WS 1996</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
		<title level="m">Notes on convolutional neural networks</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">June 16-21. 2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<ptr target="http://www.cs.ucr.edu/~eamonn/time_series_data/" />
		<title level="m">The UCR Time Series Classification/Clustering Homepage</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why is real-world visual object recognition hard?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A shapelet transform for time series classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature-based classification of time-series data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O B</forename><surname>Alcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Technology</title>
		<imprint>
			<biblScope unit="volume">0056</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1096" to="1104" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
