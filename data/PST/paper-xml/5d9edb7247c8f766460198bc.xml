<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Transferability of Adversarial Examples with Input Diversity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhou</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Wormpex AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Improving Transferability of Adversarial Examples with Input Diversity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples -crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines</head><p>. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https: //github.com/cihangxie/DI-2-FGSM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent success of Convolutional Neural Networks (CNNs) leads to a dramatic performance improvement on various vision tasks, including image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>, object detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref> and semantic segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5]</ref>. However, CNNs are extremely vulnerable to small perturbations to the input images, i.e., humanimperceptible additive perturbations can result in failure predictions of CNNs. These intentionally crafted images are known as adversarial examples <ref type="bibr" target="#b35">[36]</ref>. <ref type="bibr">Learning</ref>  From the first row to the the third row, we plot the top-5 confidence distributions of clean images, FGSM and I-FGSM, respectively. The fourth row shows the result of the proposed Diverse Inputs Iterative Fast Gradient Sign Method (DI 2 -FGSM), which attacks the white-box model and all black-box models successfully. robustness of different models <ref type="bibr" target="#b0">[1]</ref> and understand the insufficiency of current training algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>how to generate adversarial examples can help us investigate the</head><p>Several methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref> have been proposed recently to find adversarial examples. In general, these attacks can be categorized into two types according to the number of steps of gradient computation, i.e., single-step attacks <ref type="bibr" target="#b10">[11]</ref> and iterative attacks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16]</ref>. Generally, iterative attacks can achieve higher success rates than single-step attacks in the white-box setting, where the attackers have a perfect knowledge of the network structure and weights. However, if these adversarial examples are tested on a different network (either in terms of network structure, weights or both), i.e., the black-box setting, single-step attacks perform better. This trade-off is due to the fact that iterative attacks tend to overfit the specific network parameters (i.e., have high white-box success rates) and thus making generated adversarial examples rarely transfer to other networks (i.e., have low black-box success rates), while single-step attacks usually underfit to the network parameters (i.e., have low white-box success rates) thus producing adversarial examples with slightly better transferability. Observing the phenomenon, one interesting question is whether we can generate adversarial examples with high success rates under both white-box and black-box settings.</p><p>In this work, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Our work is inspired by the data augmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> strategy, which has been proven effective to prevent networks from overfitting by applying a set of label-preserving transformations (e.g., resizing, cropping and rotating) to training images. Meanwhile, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12]</ref> showed that image transformations can defend against adversarial examples under certain situations, which indicates adversarial examples cannot generalize well under different transformations. These transformed adversarial examples are known as hard examples <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> for attackers, which can then be served as good samples to produce more transferable adversarial examples.</p><p>We incorporate the proposed input diversity strategy with iterative attacks, e.g., I-FGSM <ref type="bibr" target="#b16">[17]</ref> and MI-FGSM <ref type="bibr" target="#b8">[9]</ref>. At each iteration, unlike the traditional methods which maximize the loss function directly w.r.t. the original inputs, we apply random and differentiable transformations (e.g., random resizing, random padding) to the input images with probability p and maximize the loss function w.r.t. these transformed inputs. Note that these randomized operations were previously used to defend against adversarial examples <ref type="bibr" target="#b37">[38]</ref>, while here we incorporate them into the attack process to create hard and diverse input patterns. Fig. <ref type="figure" target="#fig_0">1</ref> shows an adversarial example generated by our method and compares the success rates to other attack methods under both white-box and black-box settings.</p><p>We test the proposed input diversity on several network under both white-box and black-box settings, and singlemodel and multi-model settings. Compared with traditional iterative attacks, the results on ImageNet (see Sec. 4.2) show that our method gets significantly higher success rates for black-box models and maintains similar success rates for white-box models. By evaluating our attack method w.r.t. the top defense solutions and official baselines from NIPS 2017 adversarial competition <ref type="bibr" target="#b17">[18]</ref>, this enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a benchmark for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generating Adversarial Examples</head><p>Traditional machine learning algorithms are known to be vulnerable to adversarial examples <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, Szegedy et al. <ref type="bibr" target="#b35">[36]</ref> pointed out that CNNs are also fragile to adversarial examples, and proposed a box-constrained L-BFGS method to find adversarial examples reliably. Due to the expensive computation in <ref type="bibr" target="#b35">[36]</ref>, Goodfellow et al. <ref type="bibr" target="#b10">[11]</ref> proposed the fast gradient sign method to generate adversarial examples efficiently by performing a single gradient step. This method was extended by Kurakin et al. <ref type="bibr" target="#b15">[16]</ref> to an iterative version, and showed that the generated adversarial examples can exist in the physical world. Dong et al. <ref type="bibr" target="#b8">[9]</ref> proposed a broad class of momentum-based iterative algorithms to boost the transferability of adversarial examples. The transferability can also be improved by attacking an ensemble of networks simultaneously <ref type="bibr" target="#b20">[21]</ref>. Besides image classification, adversarial examples also exist in object detection <ref type="bibr" target="#b38">[39]</ref>, semantic segmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b5">6]</ref>, speech recognition <ref type="bibr" target="#b5">[6]</ref>, deep reinforcement learning <ref type="bibr" target="#b19">[20]</ref>, etc.. Unlike adversarial examples which can be recognized by human, Nguyen et al. <ref type="bibr" target="#b24">[25]</ref> generated fooling images that are different from natural images and difficult for human to recognize, but CNNs classify these images with high confidences.</p><p>Our proposed input diversity is also related to EOT <ref type="bibr" target="#b1">[2]</ref>. These two works differ in several aspects: <ref type="bibr" target="#b0">(1)</ref> we mainly focus on the challenging black-box setting while <ref type="bibr" target="#b1">[2]</ref> focuses on the white-box setting; (2) our work aims at alleviating overfitting in adversarial attacks, while <ref type="bibr" target="#b1">[2]</ref> aims at making adversarial examples robust to transformations, without any discussion of overfitting; and (3) we do not apply expectation step in each attack iteration, while "expectation" is the core idea in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Defending Against Adversarial Examples</head><p>Conversely, many methods have been proposed recently to defend against adversarial examples. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> proposed to inject adversarial examples into the training data to increase the network robustness. Tramèr et al. <ref type="bibr" target="#b36">[37]</ref> pointed out that such adversarially trained models still remain vulnerable to adversarial examples, and proposed ensemble adversarial training, which augments training data with perturbations transferred from other models, in order to improve the network robustness further. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12]</ref> utilized randomized image transformations to inputs at inference time to mitigate adversarial effects. Dhillon et al. <ref type="bibr" target="#b7">[8]</ref> pruned a random subset of activations according to their magnitude to enhance network robustness. Prakash et al. <ref type="bibr" target="#b26">[27]</ref> proposed a framework which combines pixel deflection with soft wavelet denoising to defend against adversarial examples. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref> leveraged generative models to purify adversarial images by moving them back towards the distribution of clean images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Let X denote an image, and y true denote the corresponding ground-truth label. We use θ to denote the network parameters, and L(X, y true ; θ) to denote the loss. To generate the adversarial example, the goal is to maximize the loss L(X + r, y true ; θ) for the image X, under the constraint that the generated adversarial example X adv = X + r should look visually similar to the original image X and the corresponding predicted label y adv = y true . In this work, we use l ∞ -norm to measure the perceptibility of adversarial perturbations, i.e., ||r|| ∞ ≤ ǫ. The loss function is defined as</p><formula xml:id="formula_0">L(X, y true ; θ) = −✶ y true • log (softmax(l(X; θ))) , (1)</formula><p>where ✶ y true is the one-hot encoding of the ground-truth y true and l(X; θ) is the logits output. Note that all the baseline attacks have been implemented in the cleverhans library <ref type="bibr" target="#b25">[26]</ref>, which can be used directly for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Family of Fast Gradient Sign Methods</head><p>In this section, we give an overview of the family of fast gradient sign methods.</p><p>Fast Gradient Sign Method (FGSM). FGSM <ref type="bibr" target="#b10">[11]</ref> is the first member in this attack family, which finds the adversarial perturbations in the direction of the loss gradient ∇ X L(X, y true ; θ). The update equation is</p><formula xml:id="formula_1">X adv = X + ǫ • sign(∇ X L(X, y true ; θ)).<label>(2)</label></formula><p>Iterative Fast Gradient Sign Method (I-FGSM). Kurakin et al. <ref type="bibr" target="#b16">[17]</ref> extended FGSM to an iterative version, which can be expressed as</p><formula xml:id="formula_2">X adv 0 = X<label>(3)</label></formula><formula xml:id="formula_3">X adv n+1 = Clip ǫ X X adv n + α • sign(∇ X L(X adv n , y true ; θ)) ,</formula><p>where Clip ǫ X indicates the resulting image are clipped within the ǫ-ball of the original image X, n is the iteration number and α is the step size. <ref type="bibr" target="#b8">[9]</ref> proposed to integrate the momentum term into the attack process to stabilize update directions and escape from poor local maxima. The updating procedure is similar to I-FGSM, with the replacement of Eq. ( <ref type="formula" target="#formula_2">3</ref>) by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Momentum Iterative Fast Gradient Sign Method (MI-FGSM). MI-FGSM</head><formula xml:id="formula_4">g n+1 = µ • g n + ∇ X L(X adv n , y true ; θ) ||∇ X L(X adv n , y true ; θ)|| 1 X adv n+1 = Clip ǫ X X adv n + α • sign(g n+1 ) ,<label>(4)</label></formula><p>where µ is the decay factor of the momentum term and g n is the accumulated gradient at iteration n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Motivation</head><p>Let θ denote the unknown network parameters. In general, a strong adversarial example should have high success rates on both white-box models, i.e., L(X adv , y true ; θ) &gt; L(X, y true ; θ), and black-box models, i.e., L(X adv , y true ; θ) &gt; L(X, y true ; θ). On one hand, the traditional single-step attacks, e.g., FGSM, tend to underfit to the specific network parameters θ due to inaccurate linear appropriation of the loss L(X, y true ; θ), thus cannot reach high success rates on white-box models. On the other hand, the traditional iterative attacks, e.g., I-FGSM, greedily perturb the images in the direction of the sign of the loss gradient ∇ X L(X, y true ; θ) at each iteration, and thus easily fall into the poor local maxima and overfit to the specific network parameters θ. These overfitted adversarial examples rarely transfer to black-box models. In order to generate adversarial examples with strong transferability, we need to find a better way to optimize the loss L(X, y true ; θ) to alleviate this overfitting phenomenon.</p><p>Data augmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> is shown as an effective way to prevent networks from overfitting during the training process. Meanwhile, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12]</ref> showed that adversarial examples are no longer malicious if simple image transformations are applied, which indicates these transformed adversarial images can serve as good samples for better optimization. Those facts inspire us to apply random and differentiable transformations to the inputs for the sake of the transferability of adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Diverse Input Patterns</head><p>Based on the analysis above, we aim at generating more transferable adversarial examples via diverse input patterns. DI 2 -FGSM. First, we propose the Diverse Inputs Iterative Fast Gradient Sign Method (DI 2 -FGSM), which applies image transformations T (•) to the inputs with the probability p at each iteration of I-FGSM <ref type="bibr" target="#b16">[17]</ref> to alleviate the overfitting phenomenon.</p><p>In this paper, we consider random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner <ref type="bibr" target="#b37">[38]</ref>, as the instantiation of the image transformations T (•) <ref type="foot" target="#foot_0">1</ref> . The transformation probability p controls the tradeoff between success rates on white-box models and success rates on black-box models, which can be observed from Fig. <ref type="figure" target="#fig_4">4</ref>. If p = 0, DI 2 -FGSM degrades to I-FGSM and leads to overfitting. If p = 1, i.e., only transformed inputs are used for the attack, the generated adversarial examples tend to have much higher success rates on black-box models but lower success rates on white-box models, since the original inputs are not seen by the attackers.</p><p>In general, the updating procedure of DI 2 -FGSM is similar to I-FGSM, with the replacement of Eq. ( <ref type="formula" target="#formula_2">3</ref>) by</p><formula xml:id="formula_5">X adv n+1 = Clip ǫ X {X adv n + α • sign ∇X L(T (X adv n ; p), y true ; θ) },<label>(5)</label></formula><p>M-DI<ref type="foot" target="#foot_1">2</ref> -FGSM DI 2  where the stochastic transformation function T (X adv n ; p) is</p><formula xml:id="formula_6">T (X adv n ; p) = T (X adv n ) with probability p X adv n with probability 1 − p .<label>(6)</label></formula><p>M-DI 2 -FGSM. Intuitively, momentum and diverse inputs are two completely different ways to alleviate the overfitting phenomenon. We can combine them naturally to form a much stronger attack, i.e., Momentum Diverse Inputs Iterative Fast Gradient Sign Method (M-DI 2 -FGSM). The overall updating procedure of M-DI 2 -FGSM is similar to MI-FGSM, with the only replacement of Eq. ( <ref type="formula" target="#formula_4">4</ref>) by</p><formula xml:id="formula_7">g n+1 = µ • g n + ∇ X L(T (X adv n ; p), y true ; θ) ||∇ X L(T (X adv n ; p), y true ; θ)|| 1 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relationships between Different Attacks</head><p>The attacks mentioned above all belong to the family of Fast Gradient Sign Methods, and they can be related via different parameter settings as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. To summarize,</p><p>• If the transformation probability p = 0, M-DI 2 -FGSM degrades to MI-FGSM, and DI 2 -FGSM degrades to I-FGSM.</p><p>• If the decay factor µ = 0, M-DI 2 -FGSM degrades to DI 2 -FGSM, and MI-FGSM degrades to I-FGSM.</p><p>• If the total iteration number N = 1, I-FGSM degrades to FGSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Attacking an Ensemble of Networks</head><p>Liu et al. <ref type="bibr" target="#b20">[21]</ref> suggested that attacking an ensemble of multiple networks simultaneously can generate much stronger adversarial examples. The motivation is that if an adversarial image remains adversarial for multiple networks, then it is more likely to transfer to other networks as well. Therefore, we can use this strategy to improve the transferability even further.</p><p>We follow the ensemble strategy proposed in <ref type="bibr" target="#b8">[9]</ref>, which fuse the logit activations together to attack multiple networks simultaneously. Specifically, to attack an ensemble of K models, the logits are fused by:</p><formula xml:id="formula_8">l(X; θ 1 , ..., θ K ) = K k=1 w k l k (X; θ k )<label>(8)</label></formula><p>where l k (X; θ k ) is the logits output of the k-th model with the parameters θ k , w k is the ensemble weight with w k ≥ 0 and K k=1 w k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Dataset. It is less meaningful to attack the images that are already classified wrongly. Therefore, we randomly choose 5000 images from the ImageNet validation set that are classified correctly by all the networks which we test on, to form our test dataset. All these images are resized to 299×299×3 beforehand.</p><p>Networks. We consider four normally trained networks, i.e., Inception-v3 (Inc-v3) <ref type="bibr" target="#b34">[35]</ref>, Inception-v4 (Inc-v4) <ref type="bibr" target="#b33">[34]</ref>, Resnet-v2-152 (Res-152) <ref type="bibr" target="#b12">[13]</ref> and Inception-Resnet-v2 (IncRes-v2) <ref type="bibr" target="#b33">[34]</ref>, and three adversarially trained networks <ref type="bibr" target="#b36">[37]</ref>, i.e., ens3-adv-Inception-v3 (Inc-v3 ens3 ), ens4-adv-Inception-v3 (Inc-v3 ens4 ) and ens-adv-Inception-ResNet-v2 (IncRes-v2 ens ). All networks are publicly available 2,<ref type="foot" target="#foot_2">3</ref> .</p><p>Implementation details. For the parameters of different attackers, we follow the default settings in <ref type="bibr" target="#b15">[16]</ref> with the step size α = 1 and the total iteration number N = min(ǫ + 4, 1.25ǫ). We set the maximum perturbation of each pixel to be ǫ = 15, which is still imperceptible for human observers <ref type="bibr" target="#b22">[23]</ref>. For the momentum term, decay factor µ is set to be 1 as in <ref type="bibr" target="#b8">[9]</ref>. For the stochastic transformation function T (X; p), the probability p is set to be 0.5, i.e., attackers put equal attentions on the original inputs and the transformed inputs. For transformation operations T (•), the input X is first randomly resized to a rnd × rnd × 3 image, with rnd ∈ [299, 330), and then padded to the size 330 × 330 × 3 in a random manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attacking a Single Network</head><p>We first perform adversarial attacks on a single network. We craft adversarial examples only on normally trained networks, and test them on all seven networks. The success rates are shown in Table <ref type="table" target="#tab_0">1</ref>, where the diagonal blocks indicate white-box attacks and off-diagonal blocks indicate black-box attacks. We list the networks that we attack on in rows, and networks that we test on in columns.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, a first glance shows that M-DI 2 -FGSM outperforms all other baseline attacks by a large margin on all black-box models, and maintains high success rates on all white-box models.  We then compare the success rates of I-FGSM and DI 2 -FGSM to see the effectiveness of diverse input patterns solely. By generating adversarial examples with input diversity, DI 2 -FGSM significantly improves the success rates of I-FGSM on challenging black-box models, regardless whether this model is adversarially trained, and maintains high success rates on white-box models. For example, if adversarial examples are crafted on Res-152, DI 2 -FGSM has success rates of 99.2% on Res-152 (white-box model), 53.8% on Inc-v3 (normally trained blackbox model) and 11.1% on Inc-v3 ens4 (adversarially trained black-box model), while I-FGSM only obtains the corresponding success rates of 99.1%, 20.8% and 4.6%, respectively. Compared with FGSM, DI 2 -FGSM also reaches much higher success rates on the normally trained blackbox models, and comparable performance on the adversarially trained black-box models. Besides, we visualize 5 randomly selected pairs of such generated adversarial images and their clean counterparts in Figure <ref type="figure" target="#fig_2">3</ref>. These visualization results show that these generated adversarial perturbations are human imperceptible.</p><p>It should be mentioned that the proposed input diversity is not merely applicable to fast gradient sign methods. To demonstrate the generalization, we also incorporate C&amp;W attack <ref type="bibr" target="#b3">[4]</ref> with input diversity. The experiment is conducted on 1000 correctly classified images. For the parameters of C&amp;W, the maximal iteration is 250, the learning rate is 0.01 and the confidence is 10. As Table <ref type="table" target="#tab_1">2</ref> suggests, our method D-C&amp;W obtains a significant performance improvement over C&amp;W on black-box models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attacking an Ensemble of Networks</head><p>Though the results in Table <ref type="table" target="#tab_0">1</ref> show that momentum and input diversity can significantly improve the transferability of adversarial examples, they are still relatively weak at attacking an adversarially trained network under the blackbox setting, e.g., the highest black-box success rate on IncRes-v2 ens is only 19.0%. Therefore, we follow the strat-  <ref type="table">3</ref>. The success rates of ensemble attacks. Adversarial examples are generated on an ensemble of six networks, and tested on the ensembled network (white-box setting) and the hold-out network (black-box setting). The sign "-" indicates the hold-out network. We observe that the proposed M-DI 2 -FGSM significantly outperforms all other attacks on all black-box models.</p><p>egy in <ref type="bibr" target="#b20">[21]</ref> to attack multiple networks simultaneously in order to further improve transferability. We consider all seven networks here. Adversarial examples are generated on an ensemble of six networks, and tested on the ensembled network and the hold-out network, using I-FGSM, DI 2 -FGSM, MI-FGSM and M-DI 2 -FGSM, respectively. FGSM is ignored here due to its low success rates on white-box models. All ensembled models are assigned with equal weight, i.e., w k = 1/6.</p><p>The results are summarized in Table <ref type="table">3</ref>, where the top row shows the success rates on the ensembled network (whitebox setting), and the bottom row shows the success rates on the hold-out network (black-box setting). Under the challenging black-box setting, we observe that M-DI 2 -FGSM always generates adversarial examples with better transferability than other methods on all networks. For example, by keeping Inc-v3 ens3 as a hold-out model, M-DI 2 -FGSM can fool Inc-v3 ens3 with an success rate of 44.6%, while I-FGSM, DI 2 -FGSM and MI-FGSM only have success rates of 12.9%, 36.3% and 22.8%, respectively. Besides, compared with MI-FGSM, we observe that using diverse input patterns alone, i.e., DI 2 -FGSM, can reach a much higher success rate if the hold-out model is an adversarially trained network, and a comparable success rate if the hold-out model is a normally trained network.</p><p>Under the white-box setting, we see that DI 2 -FGSM and M-DI 2 -FGSM reach slightly lower (but still very high) success rates on ensemble models compared with I-FGSM and MI-FGSM. This is due to the fact that attacking multiple networks simultaneously is much harder than attacking a single model. However, the white-box success rates can be improved if we assign the transformation probability p with a smaller value, increase the number of total iteration N or use a smaller step size α (see Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>In this section, we conduct a series of ablation experiments to study the impact of different parameters. We only consider attacking an ensemble of networks here, since it is much stronger than attacking a single network and can provide a more accurate evaluation of the network robustness. The max perturbation of each pixel ǫ is set to 15 for all experiments.</p><p>Transformation probability p. We first study the influence of the transformation probability p on the success rates under both white-box and black-box settings. We set the step size α = 1 and the total iteration number N = min(ǫ + 4, 1.25ǫ). The transformation probability p varies from 0 to 1. Recall the relationships shown in Fig. <ref type="figure" target="#fig_1">2</ref>, M-DI 2 -FGSM (or DI 2 -FGSM) degrades to MI-FGSM (or I-FGSM) if p = 0.</p><p>We show the success rates on various networks in Fig. <ref type="figure" target="#fig_4">4</ref>. We observe that both DI 2 -FGSM and M-DI 2 -FGSM achieve a higher black-box success rates but lower white-box success rates as p increase. Moreover, for all attacks, if p is small, i.e., only a small amount of transformed inputs are utilized, black-box success rates can increase significantly, while white-box success rates only drop a little. This phenomenon reveals the importance of adding transformed inputs into the attack process.   The trends shown in Fig. <ref type="figure" target="#fig_4">4</ref> also provide useful suggestions of constructing strong adversarial attacks in practice. For example, if you know the black-box model is a new network that totally different from any existing networks, you can set p = 1 to reach the maximum transferability. If the black-box model is a mixture of new networks and existing networks, you can choose a moderate value of p to maximize the black-box success rates under a pre-defined white-box success rates, e.g., white-box success rates must greater or equal than 90%.</p><p>Total iteration number N . We then study the influence of the total iteration number N on the success rates under both white-box and black-box settings. We set the transformation probability p = 0.5 and the step size α = 1. The total iteration number N varies from 15 to 31, and the results are plotted in Fig. <ref type="figure" target="#fig_5">5</ref>. For DI 2 -FGSM, we see that the black-box success rates and white-box success rates always increase as the total iteration number N increase. Similar trends can also be observed for M-DI 2 -FGSM except for the black-box success rates on adversarially trained models, i.e., performing more iterations cannot bring extra transferability on adversarially trained models. Moreover, we observe that the success rates gap between M-DI 2 -FGSM and DI 2 -FGSM is diminished as N increases.</p><p>Step size α. We finally study the influence of the step size α on the success rates under both white-box and black-box settings. We set the transformation probability p = 0.5.</p><p>In order to reach the maximum perturbation ǫ even for a small step size α, we set the total iteration number be proportional to the step size, i.e., N = ǫ/α. The results are plotted in Fig. <ref type="figure" target="#fig_6">6</ref>. We observe that the white-box success rates of both DI 2 -FGSM and M-DI 2 -FGSM can be boosted if a smaller step size is provided. Under the black-box setting, the success rates of DI 2 -FGSM is insensitive to the step size, while the success rates of M-DI 2 -FGSM can still be improved with smaller step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">NIPS 2017 Adversarial Competition</head><p>In order to verify the effectiveness of our proposed attack methods in practice, we here reproduce the top defense entries and official baselines from NIPS 2017 adversarial competition <ref type="bibr" target="#b17">[18]</ref> for testing transferability. Due to the resource limitation, we only consider the top-3 defense entries, i.e., TsAIL <ref type="bibr" target="#b18">[19]</ref>, iyswim <ref type="bibr" target="#b37">[38]</ref> and Anil Thomas<ref type="foot" target="#foot_3">4</ref> , as well 3 official baselines, i.e., Inc-v3 adv , IncRes-v2 ens and Inc-v3.  <ref type="table">4</ref>. The success rates on top defense solutions and official baselines from NIPS 2017 adversarial competition <ref type="bibr" target="#b17">[18]</ref>. * indicates the official results reported in the competition. Our proposed M-DI 2 -FGSM reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%.</p><p>We note that the No.1 solution and the No.3 solution apply significantly different image transformations (compared to random resizing &amp; padding used in our attack method) for defending against adversarial examples. For example, the No.1 solution, TsAIL, applies an image denoising network for removing adversarial perturbations, and the No.3 solution, Anil Thomas, includes a series of image transformations, e.g., JPEG compression, rotation, shifting and zooming, in the defense pipeline. The test dataset contains 5000 images which are all of the size 299 × 299 × 3, and their corresponding labels are the same as the ImageNet labels.</p><p>Generating adversarial examples. When generating adversarial examples, we follow the procedure in <ref type="bibr" target="#b17">[18]</ref>: (1) split the dataset equally into 50 batches; (2) for each batch, the maximum perturbation ǫ is randomly chosen from the set { 4 255 , 8 255 , 12 255 , 16 255 }; and (3) generate adversarial examples for each batch under the corresponding ǫ constraint.</p><p>Attacker settings. For the settings of attackers, we follow <ref type="bibr" target="#b8">[9]</ref> by attacking an ensemble eight diferent models, i.e., Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens and Inc-v3 adv <ref type="bibr" target="#b16">[17]</ref>. The ensemble weights are set as 1/7.25 equally for the first seven models and 0.25/7.25 for Inc-v3 adv . The total iteration number N is 10 and the decay factor µ is 1. This configuration for MI-FGSM won the 1-st place in the NIPS 2017 adversarial attack competition. For DI 2 -FGSM and M-DI 2 -FGSM, we choose p = 0.4 according to the trends shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>Results. The results are summarized in Table <ref type="table">4</ref>. We also report the official results of MI-FGSM (named MI-FGSM*) as a reference to validate our implementation. The performance difference between MI-FGSM and MI-FGSM* is due to the randomness of the max perturbation magnitude introduced in the attack process. Compared with MI-FGSM, DI 2 -FGSM have higher success rates on top defense solutions while slightly lower success rates on baseline models, which results in these two attack methods having similar average success rates. By integrating both diverse inputs and momentum term, this enhanced attack, M-DI 2 -FGSM, reaches an average success rate of 73.0%, which is far better than other methods. For example, the top-1 attack submission, MI-FGSM, in the NIPS competition only gets an average success rate of 66.4%. We believe this superior transferability can also be observed on other defense submissions which we do not evaluate on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>We provide a brief discussion of why the proposed diverse input patterns can help to generate adversarial examples with better transferability. One hypothesis is that the decision boundaries of different networks share similar inherent structures due to the same training dataset, e.g., Im-ageNet. For example, as shown in Fig <ref type="figure" target="#fig_0">1</ref>, different networks make similar mistakes in the presence of adversarial examples. By incorporating diverse patterns at each attack iteration, the optimization produces adversarial examples that are more robust to small transformations. These adversarial examples are malicious in a certain region at the network decision boundary, thus increasing the chance to fool other networks, i.e., they achieve better black-box success rate than existing methods. In the future, we plan to validate this hypothesis theoretically or empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose to improve the transferability of adversarial examples with input diversity. Specifically, our method applies random transformations to the input images at each iteration in the attack process. Compared with traditional iterative attacks, the results on ImageNet show that our proposed attack method gets significantly higher success rates for black-box models, and maintains similar success rates for white-box models. We improve the transferability further by integrating momentum term and attacking multiple networks simultaneously. By evaluating this enhanced attack against the top defense submissions and official baselines from NIPS 2017 adversarial competition <ref type="bibr" target="#b17">[18]</ref>, we show that this enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a benchmark for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in future. Code is publicly available at https://github.com/cihangxie/DI-2-FGSM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The comparison of success rates using three different attacks. The ground-truth "walking stick" is marked as pink in the top-5 confidence distribution plots. The adversarial examples are crafted on Inception-v3 with the maximum perturbation ǫ = 15. From the first row to the the third row, we plot the top-5 confidence distributions of clean images, FGSM and I-FGSM, respectively. The fourth row shows the result of the proposed Diverse Inputs Iterative Fast Gradient Sign Method (DI 2 -FGSM), which attacks the white-box model and all black-box models successfully.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Relationships between different attacks. By setting setting values of the transformation probability p, the decay factor µ and the total iteration number N , we can relate these different attacks in the family of Fast Gradient Sign Methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visualization of randomly selected clean images and their corresponding adversarial examples. All these adversarial examples are generated on Inception-v3 using our proposed DI 2 -FGSM with the maximum perturbation of each pixel ǫ = 15. are crafted on IncRes-v2, M-DI 2 -FGSM has success rates of 67.4% on Inc-v4 (normally trained black-box model) and 25.1% on Inc-v3 ens3 (adversarially trained black-box model), while strong baselines like MI-FGSM only obtains the corresponding success rates of 45.9% and 15.3%, respectively. This convincingly demonstrates the effectiveness of the combination of input diversity and momentum for improving the transferability of adversarial examples.We then compare the success rates of I-FGSM and DI2 -FGSM to see the effectiveness of diverse input patterns solely. By generating adversarial examples with input diversity, DI 2 -FGSM significantly improves the success rates of I-FGSM on challenging black-box models, regardless whether this model is adversarially trained, and maintains high success rates on white-box models. For example, if adversarial examples are crafted on Res-152, DI2 -FGSM has success rates of 99.2% on Res-152 (white-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The success rates of DI 2 -FGSM (a) and M-DI 2 -FGSM (b) when varying the transformation probability p. "Ensemble" (white-box setting) is with dashed lines and "Hold-out" (black-box setting) is with solid lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The success rates of DI 2 -FGSM (a) and M-DI 2 -FGSM (b) when varying the total iteration number N . "Ensemble" (white-box setting) is with dashed lines and "Hold-out" (blackbox setting) is with solid lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The success rates of DI 2 -FGSM (a) and M-DI 2 -FGSM (b) when varying the step size α. "Ensemble" (white-box setting) is with dashed lines and "Hold-out" (black-box setting) is with solid lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>For example, if adversarial examples The success rates on seven networks where we attack a single network. The diagonal blocks indicate white-box attacks, while the off-diagonal blocks indicate black-box attacks which are much more challenging. Experiment results demonstrate that our proposed input diversity strategy substantially improve the transferability of generated adversarial examples.</figDesc><table><row><cell>Model</cell><cell>Attack</cell><cell cols="6">Inc-v3 Inc-v4 IncRes-v2 Res-152 Inc-v3ens3 Inc-v3ens4 IncRes-v2ens</cell></row><row><cell></cell><cell>FGSM</cell><cell>64.6% 23.5%</cell><cell>21.7%</cell><cell>21.7%</cell><cell>8.0%</cell><cell>7.5%</cell><cell>3.6%</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>99.9% 14.8%</cell><cell>11.6%</cell><cell>8.9%</cell><cell>3.3%</cell><cell>2.9%</cell><cell>1.5%</cell></row><row><cell>Inc-v3</cell><cell>DI 2 -FGSM (Ours)</cell><cell>99.9% 35.5%</cell><cell>27.8%</cell><cell>21.4%</cell><cell>5.5%</cell><cell>5.2%</cell><cell>2.8%</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>99.9% 36.6%</cell><cell>34.5%</cell><cell>27.5%</cell><cell>8.9%</cell><cell>8.4%</cell><cell>4.7%</cell></row><row><cell></cell><cell cols="2">M-DI 2 -FGSM (Ours) 99.9% 63.9%</cell><cell>59.4%</cell><cell>47.9%</cell><cell>14.3%</cell><cell>14.0%</cell><cell>7.0%</cell></row><row><cell></cell><cell>FGSM</cell><cell>26.4% 49.6%</cell><cell>19.7%</cell><cell>20.4%</cell><cell>8.4%</cell><cell>7.7%</cell><cell>4.1%</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>22.0% 99.9%</cell><cell>13.2%</cell><cell>10.9%</cell><cell>3.2%</cell><cell>3.0%</cell><cell>1.7%</cell></row><row><cell>Inc-v4</cell><cell>DI 2 -FGSM (Ours)</cell><cell>43.3% 99.7%</cell><cell>28.9%</cell><cell>23.1%</cell><cell>5.9%</cell><cell>5.5%</cell><cell>3.2%</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>51.1% 99.9%</cell><cell>39.4%</cell><cell>33.7%</cell><cell>11.2%</cell><cell>10.7%</cell><cell>5.3%</cell></row><row><cell></cell><cell cols="2">M-DI 2 -FGSM (Ours) 72.4% 99.5%</cell><cell>62.2%</cell><cell>52.1%</cell><cell>17.6%</cell><cell>15.6%</cell><cell>8.8%</cell></row><row><cell></cell><cell>FGSM</cell><cell>24.3% 19.3%</cell><cell>39.6%</cell><cell>19.4%</cell><cell>8.5%</cell><cell>7.3%</cell><cell>4.8%</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>22.2% 17.7%</cell><cell>97.9%</cell><cell>12.6%</cell><cell>4.6%</cell><cell>3.7%</cell><cell>2.5%</cell></row><row><cell>IncRes-v2</cell><cell>DI 2 -FGSM (Ours)</cell><cell>46.5% 40.5%</cell><cell>95.8%</cell><cell>28.6%</cell><cell>8.2%</cell><cell>6.6%</cell><cell>4.8%</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>53.5% 45.9%</cell><cell>98.4%</cell><cell>37.8%</cell><cell>15.3%</cell><cell>13.0%</cell><cell>8.8%</cell></row><row><cell></cell><cell cols="2">M-DI 2 -FGSM (Ours) 71.2% 67.4%</cell><cell>96.1%</cell><cell>57.4%</cell><cell>25.1%</cell><cell>20.7%</cell><cell>14.9%</cell></row><row><cell></cell><cell>FGSM</cell><cell>34.4% 28.5%</cell><cell>27.1%</cell><cell>75.2%</cell><cell>12.4%</cell><cell>11.0%</cell><cell>6.0%</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>20.8% 17.2%</cell><cell>14.9%</cell><cell>99.1%</cell><cell>5.4%</cell><cell>4.6%</cell><cell>2.8%</cell></row><row><cell>Res-152</cell><cell>DI 2 -FGSM (Ours)</cell><cell>53.8% 49.0%</cell><cell>44.8%</cell><cell>99.2%</cell><cell>13.0%</cell><cell>11.1%</cell><cell>6.9%</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>50.1% 44.1%</cell><cell>42.2%</cell><cell>99.0%</cell><cell>18.2%</cell><cell>15.2%</cell><cell>9.0%</cell></row><row><cell></cell><cell cols="2">M-DI 2 -FGSM (Ours) 78.9% 76.5%</cell><cell>74.8%</cell><cell>99.2%</cell><cell>35.2%</cell><cell>29.4%</cell><cell>19.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The success rates on seven networks where we attack a single network using C&amp;W attack. Experiment results demonstrate that the proposed input diversity strategy can enhance C&amp;W attack for generating more transferable adversarial examples.</figDesc><table><row><cell>Model</cell><cell>Attack</cell><cell>Inc-v3</cell><cell>Inc-v4</cell><cell cols="5">IncRes-v2 Res-152 Inc-v3ens3 Inc-v3ens4 IncRes-v2ens</cell></row><row><cell>Inc-v3</cell><cell>C&amp;W D-C&amp;W (Ours)</cell><cell>100.0% 100.0%</cell><cell>5.7% 16.8%</cell><cell>5.3% 13.0%</cell><cell>5.1% 11.2%</cell><cell>3.0% 5.8%</cell><cell>2.5% 3.9%</cell><cell>1.1% 2.1%</cell></row><row><cell>Inc-v4</cell><cell>C&amp;W D-C&amp;W (Ours)</cell><cell>15.1% 29.3%</cell><cell>100.0% 100.0%</cell><cell>9.2% 20.1%</cell><cell>7.8% 15.4%</cell><cell>4.4% 7.1%</cell><cell>3.5% 5.3%</cell><cell>1.9% 3.1%</cell></row><row><cell>IncRes-v2</cell><cell>C&amp;W D-C&amp;W (Ours)</cell><cell>15.8% 33.9%</cell><cell>11.2% 25.6%</cell><cell>99.9% 100.0%</cell><cell>8.6% 19.4%</cell><cell>6.3% 11.2%</cell><cell>3.6% 7.3%</cell><cell>3.4% 4.0%</cell></row><row><cell>Res-152</cell><cell>C&amp;W D-C&amp;W (Ours)</cell><cell>11.4% 33.0%</cell><cell>6.9% 27.7%</cell><cell>6.1% 24.4%</cell><cell>100.0% 100.0%</cell><cell>4.4% 13.1%</cell><cell>4.1% 9.3%</cell><cell>2.3% 5.7%</cell></row><row><cell>Model</cell><cell>Attack</cell><cell cols="7">-Inc-v3 -Inc-v4 -IncRes-v2 -Res-152 -Inc-v3ens3 -Inc-v3ens4 -IncRes-v2ens</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>96.6%</cell><cell>96.9%</cell><cell>98.7%</cell><cell>96.2%</cell><cell>97.0%</cell><cell>97.3%</cell><cell>94.3%</cell></row><row><cell>Ensemble</cell><cell>DI 2 -FGSM (Ours) MI-FGSM</cell><cell>88.9% 96.9%</cell><cell>89.6% 96.9%</cell><cell>93.2% 98.8%</cell><cell>87.7% 96.8%</cell><cell>91.7% 96.8%</cell><cell>91.7% 97.0%</cell><cell>93.2% 94.6%</cell></row><row><cell></cell><cell>M-DI 2 -FGSM (Ours)</cell><cell>90.1%</cell><cell>91.1%</cell><cell>94.0%</cell><cell>89.3%</cell><cell>92.8%</cell><cell>92.7%</cell><cell>94.9%</cell></row><row><cell></cell><cell>I-FGSM</cell><cell>43.7%</cell><cell>36.4%</cell><cell>33.3%</cell><cell>25.4%</cell><cell>12.9%</cell><cell>15.1%</cell><cell>8.8%</cell></row><row><cell>Hold-out</cell><cell>DI 2 -FGSM (Ours) MI-FGSM</cell><cell>69.9% 71.4%</cell><cell>67.9% 65.9%</cell><cell>64.1% 64.6%</cell><cell>51.7% 55.6%</cell><cell>36.3% 22.8%</cell><cell>35.0% 26.1%</cell><cell>30.4% 15.8%</cell></row><row><cell></cell><cell cols="2">M-DI 2 -FGSM (Ours) 80.7%</cell><cell>80.6%</cell><cell>80.7%</cell><cell>70.9%</cell><cell>44.6%</cell><cell>44.5%</cell><cell>39.4%</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We have also experimented with other image transformations, e.g., rotation or flipping, to create diverse input patterns, and found random resizing &amp; padding yields adversarial examples with the best transferability.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/tensorflow/models/tree/ master/research/slim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/tensorflow/models/tree/ master/research/adv_imagenet_models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/anlthms/nips-2017/tree/ master/mmd</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported by a gift grant from YiTu and ONR N00014-12-1-0883.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09856</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05373</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06081</idno>
		<title level="m">Boosting adversarial attacks with momentum</title>
				<imprint>
			<date type="published" when="2008">2017. 2, 3, 4, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2004">2016. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM workshop on Security and artificial intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 2, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00097</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tactics of adversarial attack on deep reinforcement learning agents</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Foveationbased mechanisms alleviate adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06292</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09064</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Behzadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gierke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">cleverhans v2.1.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08926</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mc-Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Semantic Segmentation and Object Detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Single-shot object detection with enriched semantics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00433</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
