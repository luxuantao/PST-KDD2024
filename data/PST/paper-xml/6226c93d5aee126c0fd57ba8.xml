<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Entity Alignment</title>
				<funder ref="#_r8M6tTP">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_5f5VyJk">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_TTyKgen">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-07">7 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
							<email>lbguo@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba-Zhejiang University Joint Reseach Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuqiang</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
							<email>qiang.zhang.cs@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba-Zhejiang University Joint Reseach Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Entity Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-07">7 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.03315v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are shortsighted, heuristically selecting the closest entity as the target and allowing multiple entities to match the same candidate. To address these limitations, we model entity alignment as a sequential decision-making task, in which an agent sequentially decides whether two entities are matched or mismatched based on their representation vectors. The proposed reinforcement learning (RL)-based entity alignment framework can be flexibly adapted to most embedding-based EA methods. The experimental results demonstrate that it consistently advances the performance of several state-of-the-art methods, with a maximum improvement of 31.1% on Hits@1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity alignment (EA) is one of the most crucial tasks in knowledge graph (KG) studies. It aims to seek the potentially aligned entity pairs between two KGs, such that distributed knowledge can be linked for better supporting downstream applications. Generally, a fact in a KG can be represented by a triplet (e 1</p><p>x , r 1 , e 1 y ), where e 1</p><p>x , e 1 y denote the head and tail entities in the first KG G 1 . r 1 is the relation connecting them. With a small number of known alignment pairs as anchors, embeddingbased entity alignment (EEA) methods can learn the representations of entities belonging to respective KGs in a unified space and exploit the underlying aligned pairs based on the embedding distance. For example, e 2</p><p>x will be chosen as target entity for e 1</p><p>x if its embedding is closest to the embedding of e 1</p><p>x in vector space.</p><p>Although recent EEA methods <ref type="bibr" target="#b5">(Chen et al., 2017;</ref><ref type="bibr" target="#b20">Sun et al., 2017</ref><ref type="bibr" target="#b21">Sun et al., , 2018;;</ref><ref type="bibr" target="#b9">Guo et al., 2019;</ref><ref type="bibr" target="#b34">Wu et al., 2019;</ref><ref type="bibr" target="#b24">Tang et al., 2020;</ref><ref type="bibr" target="#b31">Wang et al., 2020;</ref><ref type="bibr">Sun et al., 2020a)</ref> have made great performance improvement, they rarely consider the evaluation process. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, all three films are directed by Jules White and have similar casts. This makes the EEA methods confused to discriminate the true aligned entities from other candidates. Current ranking strategy heuristically chooses the nearest entities without considering that some entities have already been matched before. An entity with the largest similarity is not always the true target, especially when this candidate has been matched with other entities. In contrast, we can model entity alignment as a sequential decisionmaking task, where the agent sequentially decides whether a candidate embedding is aligned with the input one. Then, the environment will exclude the matched candidates in the subsequent decisions.</p><p>One issue with the sequential strategy is the accumulated errors. Due to the heterogeneity of KGs, a pair of underlying aligned entities may not share an identical neighborhood. This makes their embeddings not as similar as desired with each other (e.g., "Guns a Poppin!" in Figure <ref type="figure" target="#fig_0">1</ref>). But semantics in the embeddings may still indicate the actual target. It is worth estimating the alignment score directly from their embeddings. It is also important to negate a most likely candidate for maximizing the long-term rewards.</p><p>In this paper, we draw on the insights of reinforcement learning (RL) that has recently received great attention in many fields <ref type="bibr" target="#b13">(Mnih et al., 2015;</ref><ref type="bibr" target="#b12">Lillicrap et al., 2016;</ref><ref type="bibr" target="#b18">Silver et al., 2016)</ref>. With the trained embeddings of any existing EEA models as raw input, we train an agent to find as many alignment pairs as possible to maximize the reward. Meanwhile, we adopt a curriculum learning <ref type="bibr" target="#b7">(Elman, 1993)</ref> strategy for the environment to provide candidate entity pairs as observations of increasing difficulty. In sum, our contributions are three-fold:</p><p>? We propose to model entity alignment as a sequential decision-making task. To the best of our knowledge, this is the first method that provides a general solution to improve the evaluation strategy for the EEA task.</p><p>? We implement an end-to-end RL-based entity alignment (RLEA) framework to solve the sequential EEA problem 1 . We elaborate an entity alignment environment to sample candidate pairs as observations efficiently. Besides, we design a policy network that takes self-embedding, neighborhood, and long-term rewards into account.</p><p>? We conduct extensive experiments to show that RLEA can significantly and consistently improve the state-of-the-art EEA methods.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding-based Entity Alignment</head><p>We divide the exiting EEA methods into two categories. The first is based on the well-known KG embedding method TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. TransE models a triplet (e 1 x , r 1 , e 1 y ) as e 1</p><p>x +r 1 ? e 1 y , with the boldfaced as the corresponding embeddings. Many methods use TransE as the KG embedding model for the EA task: MTransE <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> sets a learnable matrix to project the entity embeddings from the source KG to the space of the 1 https://github.com/guolingbing/RLEA target KG. Then, the distance among entity embeddings from different KGs can be used to estimate the similarity. This idea is extended by later works, e.g., KDCoE <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, SEA <ref type="bibr">(Pei et al., 2019a)</ref>, and OTEA <ref type="bibr">(Pei et al., 2019b)</ref>. Specifically, KDCoE learns the triplet embedding model and the description embedding model in a co-training fashion. SEA leverages adversarial learning to learn better projection matrix. It also considers the attribute information. OTEA makes use of optimal transport theories to advance the learning process of MTransE. On the other hand, JAPE <ref type="bibr" target="#b20">(Sun et al., 2017)</ref> and IPransE <ref type="bibr" target="#b39">(Zhu et al., 2017)</ref> adopt a mapping strategy that utterly different from MTransE. They directly set two entities in a known alignment pair to one embedding vector. Therefore, the vector spaces of two KGs are naturally connected. For example, given a known alignment (e 1 , e 2 ), e 1 , e 2 will be mapped to one embedding vector e.</p><p>The other line of EEA research focuses on the design of embedding models. Great efforts were put into graph convolutional networks (GCNs) <ref type="bibr" target="#b11">(Kipf and Welling, 2017)</ref>, e.g., GCN-Align <ref type="bibr" target="#b30">(Wang et al., 2018)</ref>, RDGCN <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, and graph attention networks (GATs) <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref>, e.g., MuGNN <ref type="bibr" target="#b3">(Cao et al., 2019)</ref>, AliNet <ref type="bibr">(Sun et al., 2020a)</ref>. Most of them adopt the mapping strategy to map entities in each known pair to one vector to connect two KGs. Therefore, these methods center on the design of different graph network structures, which is out of the discussion of this paper. We refer the readers to <ref type="bibr">(Sun et al., 2020b;</ref><ref type="bibr" target="#b29">Wang et al., 2017)</ref> for details.</p><p>One unique method, BootEA <ref type="bibr" target="#b21">(Sun et al., 2018)</ref>, iteratively labels likely entity alignment as training data. BootEA is a powerful method that greatly improved the performance of the basic AlignE model. This bootstrapping method is closely related to RLEA as it also assumes that a candidate entity should not be matched more than once. However, BootEA does not have a learning process, similar to its followers <ref type="bibr" target="#b36">(Xu et al., 2020;</ref><ref type="bibr" target="#b37">Zeng et al., 2020;</ref><ref type="bibr" target="#b40">Zhu et al., 2021)</ref>. The entity alignment pairs are computed based on the cosine similarity and further threshed by a hyper-parameter to filter out those with low similarity. On the other hand, the bootstrapping algorithm must run with the embedding model iteratively, making BootEA more sensitive to parameter settings. Nevertheless, there is no contradiction in integrating RLEA with BootEA to achieve better performance (see Section 4.6).</p><p>The above methods have different objectives and investigate diverse techniques. However, RLEA only needs their trained embeddings as input data, which is sufficient to achieve much better performance on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Reinforcement Learning for Knowledge Graphs</head><p>One most relevant work to this paper is CEAFF <ref type="bibr" target="#b38">(Zeng et al., 2021)</ref>, which also leverages RL algorithms and believes in 1-to-1 alignment.</p><p>But CEAFF focuses more on generating and integrating different entity features. The RL part is less explored. From its experimental results <ref type="bibr" target="#b38">(Zeng et al., 2021)</ref>, we can find that RL-based CEAFF only outperformed its heuristic version slightly. Moreover, CEAFF does not provide a general solution for sequential EEA task. It is not applicable to most existing EEA methods. DeepPath <ref type="bibr" target="#b35">(Xiong et al., 2017)</ref> and its followers <ref type="bibr" target="#b6">(Das et al., 2018;</ref><ref type="bibr" target="#b28">Wan et al., 2020)</ref> are also wellknown RL-based KG embedding methods. They leverage RL agents to continually extend paths for multi-hop reasoning. There are two major differences. First, DeepPath focuses more on the design of the reward function. It takes accuracy, diversity, and efficiency into consideration when estimating an action's reward. However, the design of the environment is relatively straightforward, as the next state is certain after receiving an edge as action. By contrast, the reward in our sequential EA task can be simply assigned by comparing the output action (i.e., match or mismatch) with the actual label; nevertheless, any valid entity pair can be set as next state. Therefore, we focus more on formalizing this problem and building a proper environment where the agent can explore efficiently.</p><p>Additionally, some methods like KAGAN <ref type="bibr" target="#b17">(Qu et al., 2019)</ref> only use the policy gradient algorithm <ref type="bibr" target="#b32">(Williams, 1992)</ref> to update their network parameters. They do not really learn a policy to solve a sequential decision-making problem. Therefore, we do not review them in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><formula xml:id="formula_0">Let G 1 = {E 1 , R 1 , T 1 } and G 2 = {E 2 , R 2 , T 2 }</formula><p>be the source and target KGs, with E, R, T denoting the entity, relation, and triplet sets respectively. The proposed RL-based framework consists of two modules, i.e., the agent and the environment. We use the trained entity embeddings E 1 , E 2 of any EEA models as input for the agent. The training set, same to the existing works, is still a small number of known entity alignment S ? E 1 ? E 2 provided by the dataset.</p><p>In each training episode, the states and actions are generated by the environment and the agent in an alternative order, i.e., s 1 , a 1 , s 2 , a 2 , ..., s i , a i . We define a state s as a pair of arbitrary entities [e x , e y ] belonging to respective KGs (we rewrite [e 1</p><p>x , e 2 y ] as [e x , e y ] for readability, the same below). An action a ? {0, 1} represents the decision of the agent that indicates match or mismatch for [e x , e y ].</p><p>Each state also has a label l ? {0, 1}, implying the right decision. It is worth noting that an action may still have a positive effect even if it is not equal to the label. For example, an incorrect match action can also exclude two wrong entities correctly.</p><p>In the following sections, we call the case of a = 0 ? l = 0 a true mismatch, a = 0 ? l = 1 a false mismatch, a = 1 ? l = 0 a false match, and a = 1 ? l = 1 a true match. Therefore, the number of correct aligned entity pairs equals to that of true match, which is proportional to the Hits@1 result in the conventional EEA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Agent</head><p>We start by introducing the agent module, which is modeled by neural networks.</p><p>State A state s = [e x , e y ] is given by the environment. We take the following features into consideration: (1) the embeddings of two entities e x , e y ;</p><p>(2) the neighbor embedding sets of two entities N x , N y ; (3) the opponent entity embedding set O y of e y . We term the k-nearest candidates to e x except e y "opponent entities", as they are also possible aligned entities to e x . These entities can provide additional information for refusing or accepting the input entity pair [e x , e y ].</p><p>Action An action a is a binary number that represents the agent's choice. The binary schema has two advantages. First, the corresponding best policy can be an easier function to be approximated. Selecting one entity from multiple candidates is significantly more complex than judging a pair of entities, especially for the case of existing thousands of different candidates. On the other hand, the binary schema enables the agent to suspend the current candidate pair. For example, if the agent chooses mismatch, the source entity still has a chance to be correctly matched in the following interactions. We first use a GNN model to aggregate the neighbor embeddings of each entity. The output representations of e x , e y are then fed into a linear layer that maps features to an unnormalized estimation of the alignment score. We also leverage a mutual information estimator, which takes the opponent entity representations as negative examples. We combine the output of two types of estimations to obtain the final action distribution.</p><p>By contrast, in the classification schema, the agent must select one entity as the final choice.</p><p>Policy The policy ?(a|s, ?) is parameterized by graph neural networks (GNNs), where ? denotes the parameter set. We illustrate its architecture in Figure <ref type="figure" target="#fig_1">2</ref>. Given a state s = [e x , e y ], we first extract the features by a multi-layer GNN. Here, we use vanilla GCN <ref type="bibr" target="#b11">(Kipf and Welling, 2017)</ref> for graph convolution, but other GNN models like GATs <ref type="bibr" target="#b26">(Velickovic et al., 2018)</ref> can also be employed. The output embedding of e x at layer k is defined as:</p><formula xml:id="formula_1">g k x = ?( e i ?N (ex)?{ex} 1 c x W k g g k-1 i )<label>(1)</label></formula><p>where g k x denotes the output hidden of layer k for e x . c x is the normalization constant. W k g is the weight matrix at layer k. ?(?) is the activation function (ReLU <ref type="bibr" target="#b14">(Nair and Hinton, 2010)</ref> in our implementation). For the first layer, we set g 0 i = e i , where e i denotes the input embedding of e i . GCNs efficiently aggregate the neighborhood and selfinformation into a single vector, which is supposed to be more robust and informative than directly using the trained embeddings. Furthermore, GCNs also allow RLEA to reweight entity embeddings for sequential EEA. For simplicity, we denote the output of the last GCN layer by g x .</p><p>Next, we use a linear layer to combine the output embeddings g x , g y , which can be written as follows:</p><formula xml:id="formula_2">h ex,ey = ?(W h (g x ||g y ) + b h ), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where || is the concatenation operator to concat g x , g y to one hidden vector. W h and b h are the weight matrix and bias vector, respectively.</p><p>We also take the mutual information I(e x , e y ) <ref type="bibr" target="#b1">(Belghazi et al., 2018)</ref> as an additional feature. Unlike the cosine similarity that weights the difference of two vectors at each dimension, mutual information values more on the high-level correlations. Therefore, it is especially appropriate for the EEA task, where two aligned entities may not have identical neighborhoods due to the heterogeneity. Following (van den <ref type="bibr" target="#b25">Oord et al., 2018)</ref>, we leverage a neural function f (g x , g y ) to estimate the density ratio:</p><formula xml:id="formula_4">f (g x , g y ) = exp(g T x W f g y ),<label>(3)</label></formula><p>where W f is the weight matrix. As aforementioned, we consider opponent entities a kind of future information to aid the agent in making decisions. This idea can be naturally reified by viewing the opponents as negative examples:</p><p>?ex,ey = f (g x , g y )</p><formula xml:id="formula_5">e i ?Oy?{ey} f (g x , g i )</formula><p>.</p><p>(4)</p><p>The above equation has a similar form to that used in InfoNCE (van den <ref type="bibr" target="#b25">Oord et al., 2018)</ref>. But from another aspect, ?ex,ey can be also understood as the probability of outputting the action match based on the mutual information estimator (MIE). Finally, we concatenate all estimates to obtain the final action distribution:</p><formula xml:id="formula_6">p ex,ey = Softmax(W p (h e i ,e j || ?ex,ey )), = ?(a|s, ?)<label>(5)</label></formula><p>where p ex,ey is the normalized action distribution . W p is the weight matrix.</p><p>Reward We assign the reward for the given output action a by the following equation:</p><formula xml:id="formula_7">r = ? ? ? ? ?</formula><p>1, a true match, -10, a false mismatch, 0, elsewise.</p><p>(6)</p><p>The goal of the agent is to maximize the overall reward, i.e., output 1 as much as possible for aligned pairs. Therefore, we set a positive reward for a = l when the input pair match and a severe penalty (-10 is most efficient in our implementation) for a false mismatch. For other cases, the agent will receive a reward 0, as they do not directly increase or decrease the number of alignment pairs.</p><p>Optimization We use the policy gradient algorithm REINFORCE <ref type="bibr" target="#b32">(Williams, 1992)</ref> to find the parameters leading to a larger reward. To reduce the variance, we employ a baseline function for comparison. Therefore, the gradient at i-step in an episode is:</p><formula xml:id="formula_8">?? = ?? i ?? ln ?(a|[e x , e y ]), (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where ? is the learning step-size. ? is the discount factor. ? is the relative advantage of policy ?(a|[e x , e y ]) than the baseline, i.e., how much better the output action a is than mean or random. It can be defined as follows:</p><formula xml:id="formula_10">? = G -v([e x , e y ]) = T k=i+1 ? k-i-1 r k -v([e x , e y ]),<label>(8)</label></formula><p>where G is the return based on the future rewards.</p><p>T denotes the episode length. The baseline function v([e x , e y ]) in this paper is an estimate of the state value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Environment</head><p>Generally, the environment for an RL task should conform with three basic properties: dependency, dynamics, and difficulty.</p><p>Dependency The output action may change the later states. For sequential entity alignment, a true match will not only yield a correct alignment, but also exclude some plausible candidates for the following judgments, contributing to higher overall reward. Even a false match also has its value in filtering out two wrong entities. Therefore, we should consider the long-term dependencies.</p><p>To this end, for each entity e x in G 1 , its knearest entities e 1 , e 2 , ..., e k in G 2 are selected as candidates. Those entities are then concatenated with e x to form k candidate pairs [e x , e 1 ], [e x , e 2 ], ..., [e x , e k ].</p><p>The environment maintains a sequence c 1 , c 2 , ..., c j , in which each element is such a candidate pair. At the i-th step, the environment pops a candidate pair [e x , e y ] as s i . If it receives an action a i = 1 from the agent, all candidate pairs containing e x or e y will be removed from the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamics</head><p>The environment is usually dynamic. The state-action sequences are different in different episodes. A dynamic environment makes the agent capable of capturing the general rules of the game, which is crucial to avoid overfitting. For the EEA task, if the state sequence is constant at each training episode, the agent will fit this sequence. However, the states are entirely different at the testing phase.</p><p>To ensure the dynamic property, we set a skip rate p s . The environment randomly skips a candidate pair with probability p s and then pops the next pair. Therefore, the length and elements of the state sequence change in each episode.</p><p>Difficulty Often, the difficulty of a game is improved gradually as step number grows. For example, the health and speed of enemies in video games usually increase over game time. On the other hand, it is also a general strategy to break down complex knowledge by a sequence of learning episodes of increasing difficulty, which is known as curriculum learning <ref type="bibr" target="#b7">(Elman, 1993)</ref>.</p><p>For sequential entity alignment, the difficulty of a candidate pair can be estimated based on the cosine similarity of the two entities and their label, which can be written as follows:</p><formula xml:id="formula_11">d(e x , e y ) = l(C ex,emax -C ex,ey ) + (1 -l)(? -C ex,emax + C ex,ey )<label>(9)</label></formula><p>where C ex,ey is the cosine similarity between e x and e y . e max denotes the entity with the largest similarity to e x . We use the difference between C ex,emax and C ex,ey as the basis to estimate the extent, and the label l as the sign. When l = 1, i.e., the first term in Eq. ( <ref type="formula" target="#formula_11">9</ref>), a large difference between C ex,ey and C ex,emax will result in high difficulty because e x , e y may be too dissimilar with each other. The situation is reversed for l = 0. We add a hyper-parameter ? to balance difficulty scores between these two cases. Then, we can sort candidate pairs by the difficulty in ascending order, such that the agent will always start from the relatively easier states. However, this operation is inapplicable to the testing set where the label information is unknown. To mitigate this problem, we propose a curriculum learning strategy. We do not directly sort the candidate pairs by difficulty score. Instead, we sort them based on the cosine similarity and re-weight the skip rate p s for each pair by its normalized difficulty score. Therefore, for each episode, the agent will start from pairs with high similarity, and the more difficult states will be skipped with larger probabilities. As the policy is optimized, we gradually decrease p s to approximate the testing environment. The final state sequence shall have a similar arrangement to that at the testing phase.</p><p>The skip rate p i,t s at the i-th step in episode t can be written as:</p><formula xml:id="formula_12">p i,t s = max(p min s , ? t-1 p s d i ),<label>(10)</label></formula><p>where p min s is the minimal skip rate to ensure the dynamic property. p s is the basic skip rate. d i denotes the difficulty of state s i at i-th step. As episode number grows, p i,t s decreases with discount factor ? exponentially until it meets the lower bound p min s .</p><p>We illustrate how the environment collaborates with the agent in Figure <ref type="figure" target="#fig_2">3</ref>: a. the environment pops an entity pair c from the candidate pair sequence; b. this entity pair may be skipped with probability p s (Equation ( <ref type="formula" target="#formula_12">10</ref>)); c. the non-skipped pair is outputted by the environment as s i ; d. the agent takes s i as input, and its output action changes the candidate pair sequence reversely. The detailed implementation can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conducted experiments to verify the effectiveness of the proposed RL-based framework. The trained entity embeddings were obtained from the OpenEA project<ref type="foot" target="#foot_0">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Settings</head><p>We used the 15K benchmark proposed by OpenEA. It consists of four subsets: EN-FR, EN-DE, D-W, and D-Y. The former two are cross-lingual datasets, where EN, FR, DE denote English, French, and German versions of DBpedia, respectively. The latter two are cross-source datasets, where D, W, Y denote DBpedia <ref type="bibr" target="#b0">(Auer et al., 2007)</ref>, WikiData <ref type="bibr" target="#b27">(Vrande?i? and</ref><ref type="bibr" target="#b27">Kr?tzsch, 2014), and</ref><ref type="bibr">Yago (Fabian et al., 2007)</ref>, respectively. We used "V1" subsets that has similar distributions to original KGs. Please refer to <ref type="bibr">(Sun et al., 2020b)</ref> for detailed statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>We select the following methods as baselines:</p><p>? JAPE <ref type="bibr" target="#b20">(Sun et al., 2017)</ref>, which learns attribute embeddings and relational embeddings jointly for EEA.</p><p>? SEA <ref type="bibr">(Pei et al., 2019a)</ref>, which adopts adversarial learning to learn the projection matrix.</p><p>? RSN <ref type="bibr" target="#b9">(Guo et al., 2019)</ref>, which leverages recurrent neural networks (RNNs) <ref type="bibr" target="#b33">(Williams and Zipser, 1989)</ref> to learn KG embeddings.</p><p>? RDGCN <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>, which uses GCNs to capture the neighborhood information into entity embeddings.</p><p>We also design a basic sequential strategy called Seq for comparison. We follow the algorithm used in BootEA <ref type="bibr" target="#b21">(Sun et al., 2018)</ref> to implement it. Entity pairs with similarity above a predefined threshold are regarded as match, or the algorithm randomly chooses actions based on cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The results on four datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. Orig denotes the original results of the EEA methods. We can observe that RLEA significantly improved the performance of all baseline methods, including the best-performing one, RDGCN. Therefore, we believe that RLEA provides a better way  to exploit aligned entity pairs from embeddings than the widely-used heuristic strategy.</p><p>Specifically, the performance improvement of JAPE is most notable, with 30.4% and 31.1% increases on EN-FR and D-Y, respectively. Although RSN has minimal performance increase, the difference is still significant. Similarly, RDGCN also achieved better performance, leading to a new stateof-the-art on the benchmark.</p><p>With the basic sequential strategy Seq, four baseline methods also achieved better Hits@1 results on most datasets except D-W. This observation empirically proves the advantages of modeling EA as a sequential decision-making task.</p><p>Note that RLEA also has its limitations. We find there exists a dataset bias. The performance improvement on EN-FR dataset is notable, but that on D-W is less significant. We believe that the sequential evaluation process might cause this bias. For instance, Seq also got worse performance than the original method SEA on D-W. We leave how to mitigate this problem in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of Candidate Number</head><p>In RLEA, the candidate number for each entity is an important hyper-parameter as it decides the length of the candidate sequence. A large value means covering more correct alignment pairs as well as more plausible pairs. Therefore, it is necessary to study how this hyper-parameter influences the performance of RLEA.</p><p>We used the embeddings of the best-performing method RDGCN as input in this experiment. As shown in the left of Figure <ref type="figure" target="#fig_3">4</ref>, the Hits@1 results on four datasets gradually increase with candidate number from 1 to 5, but converge after 10. When candidate number was set to 1, for each entity, only the pair with the highest similarity was added to the sequence, resulting in similar or even worse results compared with the original method. For example, on D-W, the hits@1 of RLEA is 0.478, significantly below that of the original RDGCN (0.541). As the candidate number increased, more aligned pairs were added to the sequence, the performance improved steadily. It then gets saturated due to more unaligned pairs were also added to the sequence. On the right of Figure <ref type="figure" target="#fig_3">4</ref>, we show the runtime of one testing episode w.r.t. candidate number, which, however, grows exponentially. This observation suggests that setting a large candidate number is computationally expensive. Therefore, we decide to use the top-10 candidates in our implementation, for sake of performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of Modules</head><p>We conducted experiments to verify the effectiveness of mutual information estimator (MIE) and the proposed environment. We developed two variants of RLEA: (1) RLEA without MIE (denoted as w/o MIE), and (2) RLEA with a random environment (RandEnv). The random environment still maintains a candidate pair sequence but does not have the difficulty and skipping settings. All candidate pairs in the sequence are randomly reset at the start of each episode.</p><p>We compare the results in Figure <ref type="figure" target="#fig_4">5</ref>, from which we find that the agent does not work in the random environment on all datasets. The alignment num- ber even slowly decreases during training. This is because that the state sequence in the random environment changes irregularly. The agent fails to establish an effective policy to maximize the reward for all episodes. Furthermore, the random environment does not have a curriculum learning strategy to help the agent study from easy to hard. Therefore, the agent is not able to capture the general rules in the random environment.</p><p>On the other hand, we find that MIE slightly improves the performance. It does not have a significant advantage over the final reward or alignment number. This may be because that the output embeddings of GNNs have already included sufficient information to judge entity pairs. Nevertheless, the estimation provided by MIE helps the agent find the best policy rapidly, which is crucial when applying to larger datasets. A more detailed version of Figure <ref type="figure" target="#fig_4">5</ref> is shown in Appendix B, from which we can obtain the consistent observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">A Comparison of RLEA and BootEA</head><p>BootEA <ref type="bibr" target="#b21">(Sun et al., 2018</ref>) is a bootstrapping method that iteratively labels possible entity alignment as training data. Like RLEA, BootEA assumes a candidate entity should not be aligned twice. Therefore, it is interesting to compare and discuss these two methods.</p><p>We illustrate the experimental results on four datasets in Figure <ref type="figure">6</ref>. AlignE is a variant of BootEA without bootstrapping process. We first compare the left four columns. Obviously, BootEA (4th column) has a better effect in improving the performance of AlignE, as it directly participates in training AlignE by iteratively adding plausible align-ment pairs. In contrast, Seq (2nd column) and RLEA (3rd column) only use the trained embeddings as input and do not modify the embeddings or training procedure. They are thus more extensible and applicable to arbitrary EEA methods.</p><p>In fact, it is no contradiction to integrate these two types of methods. The performance improvement (5th and 6th columns) is still significant and consistent on all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Competing with Conventional Methods</head><p>There has always been an argument about the practical use of EEA. Most EEA methods are end-toend and easy to be deployed. The performance also improves when new models are developed. However, a significant performance gap still exists between EEA methods and those conventional methods like Paris <ref type="bibr" target="#b19">(Suchanek et al., 2012)</ref> and LogMap <ref type="bibr" target="#b10">(Jim?nez-Ruiz and Grau, 2011)</ref>. We show in Table <ref type="table" target="#tab_1">2</ref> that RLEA with the embeddings of best EEA methods as input can narrow this gap and even outperform the conventional methods on some datasets. As shown in Table <ref type="table" target="#tab_1">2</ref>, PARIS is the best method that outperformed others on all datasets except D-Y. However, The second method changed from LogMap to RLEA. We can find that RLEA not only outperformed LogMap on EN-FR, but also achieved the best performance on D-Y.</p><p>We should notice that the alignment pairs exploited by EEA methods and conventional methods are not all overlapped <ref type="bibr">(Sun et al., 2020b)</ref>. It is possible to integrate them to achieve better performance <ref type="bibr" target="#b20">(Sun et al., 2017</ref><ref type="bibr">(Sun et al., , 2020b))</ref>. In this sense, RLEA is also the best choice to be combined with conventional methods.</p><p>In this paper, we proposed an RL-based entity alignment framework, which can advance most existing EEA methods without modifying their parameter settings or infrastructures. Our experiments demonstrate consistent and significant improvement on all baseline methods. We plan to study how to jointly train EEA methods and RLEA for further improvement in future work.</p><p>Add s, a to episode sequence;  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Parameter Settings</head><p>For each EEA method, we directly used their trained entity embeddings as input and did not mod-ify these vectors during training. The embeddingsize was identical to that used in OpenEA <ref type="bibr">(Sun et al., 2020b)</ref>. The number of training episodes was set to 500, and the learning step-size was set to 0.0001. The candidate pair number for each entity was set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Results of Ablation Study</head><p>The detailed results of RLEA and its two variants are shown in Figure <ref type="figure" target="#fig_7">7</ref>. Overall, the full RLEA still has the best performance and training speed, especially on D-Y dataset. The method without MIE also have competitive performance on four datasets, which demonstrates the effectiveness of the RL-based sequential EEA.</p><p>From the bottom sub-figures, we find that the agent tries to find a policy to achieve high rewards in the random environment. However, 0 is almost the best reward it can get. The agent fails to establish a good policy in this dynamic environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Results on OpenEA 100K datasets</head><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, the Hits@1 results on Ope-nEA 100K datasets are consistent with those on 15K datasets. RLEA still outperformed the baselines on four datasets. We did not consider RDGCN and RSN in this experiment, as they can only be trained on CPUs (confirmed from the authors of OpenEA).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different evaluation strategies. The ranking strategy (left) heuristically selects the candidate with the largest similarity. The sequential strategy (right) allows each candidate to be matched only once. Deeper color indicates higher similarity. Diagonals are correct matches. Cells with yellow borders are the selected entities, while those with dotted borders denote the excluded entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of the policy network. We first use a GNN model to aggregate the neighbor embeddings of each entity. The output representations of e x , e y are then fed into a linear layer that maps features to an unnormalized estimation of the alignment score. We also leverage a mutual information estimator, which takes the opponent entity representations as negative examples. We combine the output of two types of estimations to obtain the final action distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of how the environment interacts with the agent in RLEA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hits@1 and episode time w.r.t. candidate number on four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Alignment number w.r.t. episode number, on EN-FR and D-W datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Alignment number, episode length, and reward w.r.t. episode number, on four datasets.</figDesc><graphic url="image-44.png" coords="12,425.90,589.16,71.71,57.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hits@1 results on four datasets (5-fold cross-validation).</figDesc><table><row><cell>Methods</cell><cell>EN-FR</cell><cell></cell><cell>EN-DE</cell><cell></cell><cell>D-W</cell><cell></cell><cell>D-Y</cell><cell></cell></row><row><cell></cell><cell cols="8">Orig Seq RLEA Orig Seq RLEA Orig Seq RLEA Orig Seq RLEA</cell></row><row><cell>JAPE (Sun et al., 2017)</cell><cell>.247 .291</cell><cell>.322</cell><cell>.307 .332</cell><cell>.336</cell><cell>.259 .279</cell><cell>.301</cell><cell>.463 .547</cell><cell>.607</cell></row><row><cell>SEA (Pei et al., 2019a)</cell><cell>.280 .317</cell><cell>.365</cell><cell>.530 .556</cell><cell>.571</cell><cell>.360 .359</cell><cell>.414</cell><cell>.500 .564</cell><cell>.643</cell></row><row><cell>RSN (Guo et al., 2019)</cell><cell>.393 .410</cell><cell>.429</cell><cell>.587 .614</cell><cell>.634</cell><cell>.441 .466</cell><cell>.493</cell><cell>.514 .546</cell><cell>.566</cell></row><row><cell cols="2">RDGCN (Wu et al., 2019) .755 .801</cell><cell>.830</cell><cell>.830 .861</cell><cell>.878</cell><cell>.515 .517</cell><cell>.541</cell><cell>.931 .951</cell><cell>.974</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparing RLEA with conventional methods.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell></cell><cell cols="2">EN-FR</cell><cell></cell><cell></cell><cell></cell><cell cols="3">EN-DE</cell><cell></cell><cell></cell><cell cols="2">D-W</cell><cell></cell><cell></cell><cell></cell><cell cols="2">D-Y</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="21">Precision Recall F1-score Precision Recall F1-score Precision Recall F1-score Precision Recall F1-score</cell></row><row><cell></cell><cell>LogMap</cell><cell cols="2">.818</cell><cell cols="2">.729</cell><cell>.771</cell><cell></cell><cell>.925</cell><cell></cell><cell>.725</cell><cell></cell><cell>.813</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="2">.960</cell><cell cols="2">.943</cell><cell cols="2">.951</cell></row><row><cell></cell><cell>PARIS</cell><cell cols="2">.907</cell><cell cols="2">.900</cell><cell>.903</cell><cell></cell><cell>.938</cell><cell></cell><cell>.933</cell><cell></cell><cell>.935</cell><cell>.746</cell><cell>.723</cell><cell></cell><cell>.734</cell><cell cols="2">.875</cell><cell cols="2">.868</cell><cell cols="2">.872</cell></row><row><cell></cell><cell>OpenEA</cell><cell cols="2">.755</cell><cell cols="2">.755</cell><cell>.755</cell><cell></cell><cell>.830</cell><cell></cell><cell>.830</cell><cell></cell><cell>.830</cell><cell>.572</cell><cell>.572</cell><cell></cell><cell>.572</cell><cell cols="2">.931</cell><cell cols="2">.931</cell><cell cols="2">.931</cell></row><row><cell></cell><cell>RLEA</cell><cell cols="2">.830</cell><cell cols="2">.830</cell><cell>.830</cell><cell></cell><cell>.878</cell><cell></cell><cell>.878</cell><cell></cell><cell>.878</cell><cell>.611</cell><cell>.611</cell><cell></cell><cell>.611</cell><cell cols="2">.974</cell><cell cols="2">.974</cell><cell cols="2">.974</cell></row><row><cell></cell><cell></cell><cell cols="2">AlignE</cell><cell cols="3">AlignE+Seq</cell><cell cols="3">AlignE+RLEA</cell><cell cols="3">BootEA</cell><cell cols="2">BootEA+Seq</cell><cell cols="3">BootEA+RLEA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hits@1</cell><cell>0.75 0.55</cell><cell></cell><cell></cell><cell>0.507</cell><cell>0.514</cell><cell>0.525</cell><cell>0.552</cell><cell>0.593</cell><cell>0.623</cell><cell>0.675</cell><cell>0.689</cell><cell>0.699</cell><cell>0.490</cell><cell>0.534</cell><cell>0.572</cell><cell>0.595</cell><cell>0.611</cell><cell>0.571</cell><cell>0.642</cell><cell>0.696</cell><cell>0.739</cell><cell>0.747</cell><cell>0.760</cell></row><row><cell></cell><cell></cell><cell>0.419</cell><cell>0.452</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.450</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.366</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">EN-FR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">EN-DE</cell><cell></cell><cell></cell><cell></cell><cell cols="2">D-W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">D-Y</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">Figure 6: A comparison between RLEA and BootEA.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Hits@1 results on OpenEA 100K datasets.</figDesc><table><row><cell>Methods</cell><cell>EN-FR</cell><cell></cell><cell>EN-DE</cell><cell></cell><cell>D-W</cell><cell></cell><cell>D-Y</cell></row><row><cell></cell><cell cols="8">Orig Seq RLEA Orig Seq RLEA Orig Seq RLEA Orig Seq RLEA</cell></row><row><cell cols="2">JAPE (Sun et al., 2017) .165 .172</cell><cell>.197</cell><cell>.152 .162</cell><cell>.169</cell><cell>.211 .229</cell><cell>.257</cell><cell>.287 .308</cell><cell>.323</cell></row><row><cell cols="2">SEA (Pei et al., 2019a) .225 .229</cell><cell>.261</cell><cell>.341 .345</cell><cell>.376</cell><cell>.291 .293</cell><cell>.338</cell><cell>.490 .525</cell><cell>.545</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/nju-websoft/OpenEA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We want to thank the anonymous reviewers for their invaluable comments.</p><p>This work is funded by <rs type="grantNumber">NSFCU19B2027/NSFC91846204</rs>, <rs type="funder">National Key R&amp;D Program of China</rs> (Funding No.<rs type="grantNumber">SQ2018YFC000004</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (No.<rs type="grantNumber">LGG22F030011</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_r8M6tTP">
					<idno type="grant-number">NSFCU19B2027/NSFC91846204</idno>
				</org>
				<org type="funding" xml:id="_5f5VyJk">
					<idno type="grant-number">SQ2018YFC000004</idno>
				</org>
				<org type="funding" xml:id="_TTyKgen">
					<idno type="grant-number">LGG22F030011</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Algorithm</head><p>We show the training procedure of RLEA by Algorithm 1. The input is two KGs, trained embeddings of an arbitrary EEA method, and parameter settings. If the EEA method has projection matrices <ref type="bibr" target="#b5">(Chen et al., 2017;</ref><ref type="bibr">Pei et al., 2019a)</ref>, the embeddings of G 2 should be projected to the space of G 1 by the corresponding matrix before the training starts. We first initialize all parameters of the policy network. The episode sequence and candidate pair sequence will be reset at the start of each episode. After that, the agent interacts with the environment, which generates a state-action sequence. We then use RE-INFORCE algorithm to update the policy network with the generated sequence. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural network for entity alignment</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment</title>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</title>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: The importance of starting small</title>
		<author>
			<persName><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="99" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Yago: A core of semantic knowledge unifying wordnet and wikipedia</title>
		<author>
			<persName><forename type="first">Kasneci</forename><surname>Ms Fabian</surname></persName>
		</author>
		<author>
			<persName><surname>Gjergji</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum Gerhard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to exploit long-term relational dependencies in knowledge graphs</title>
		<author>
			<persName><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Logmap: Logic-based and scalable ontology matching</title>
		<author>
			<persName><forename type="first">Ernesto</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><forename type="middle">Cuenca</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">2019a. Semi-supervised entity alignment via knowledge graph embedding with awareness of degree difference</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving cross-lingual entity alignment via optimal transport</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Weakly-supervised knowledge graph alignment with adversarial learning</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1907.03179</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Thore Graepel, and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PARIS: Probabilistic alignment of relations, instances, and schema</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>PVLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attributepreserving embedding</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bootstrapping entity alignment with knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge graph alignment network with gated multi-hop neighborhood aggregation</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A benchmarking study of embedding-based entity alignment for knowledge graphs</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>PVLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BERT-INT: A bertbased interaction model for knowledge graph alignment</title>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3174" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wikidata: a free collaborative knowledgebase. Communications of the</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reasoning like human: Hierarchical reinforcement learning for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Guojia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JICAI</title>
		<imprint>
			<biblScope unit="page" from="1926" to="1932" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph alignment with entity-pair embedding</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoju</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1672" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relation-aware entity alignment for heterogeneous knowledge graphs</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coordinated reasoning for crosslingual knowledge graph alignment</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9354" to="9361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collective entity alignment via adaptive features</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1870" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinforcement learningbased collective entity alignment with adaptive features</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Iterative entity alignment via joint knowledge embeddings</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Hao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RAGA: relation-aware graph attention networks for global entity alignment</title>
		<author>
			<persName><forename type="first">Renbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12712</biblScope>
			<biblScope unit="page" from="501" to="513" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
