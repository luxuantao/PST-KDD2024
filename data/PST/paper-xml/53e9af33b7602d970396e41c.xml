<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Energy-Aware Capacity Provisioning for Cloud Computing Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>q8zhang@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Faten Zhani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
							<email>zhangshuo@nudt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Quanyan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raouf</forename><surname>Boutaba</surname></persName>
							<email>rboutaba@uwaterloo.ca</email>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology Changsha</orgName>
								<address>
									<settlement>Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Pohang University of Science and Technology (POSTECH) Pohang</orgName>
								<address>
									<postCode>790-784</postCode>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<settlement>Seattle, Washington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Energy-Aware Capacity Provisioning for Cloud Computing Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">105166EFE698610AD6B50A68B3F661F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Modeling techniques</term>
					<term>I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory</term>
					<term>I.6.3 [Simulation and Modeling]: Applications Management, Performance, Experimentation Cloud Computing</term>
					<term>Resource Management</term>
					<term>Energy Management</term>
					<term>Model Predictive Control</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data centers have recently gained significant popularity as a cost-effective platform for hosting large-scale service applications. While large data centers enjoy economies of scale by amortizing initial capital investment over large number of machines, they also incur tremendous energy cost in terms of power distribution and cooling. An effective approach for saving energy in data centers is to adjust dynamically the data center capacity by turning off unused machines. However, this dynamic capacity provisioning problem is known to be challenging as it requires a careful understanding of the resource demand characteristics as well as considerations to various cost factors, including task scheduling delay, machine reconfiguration cost and electricity price fluctuation.</p><p>In this paper, we provide a control-theoretic solution to the dynamic capacity provisioning problem that minimizes the total energy cost while meeting the performance objective in terms of task scheduling delay. Specifically, we model this problem as a constrained discrete-time optimal control problem, and use Model Predictive Control (MPC) to find the optimal control policy. Through extensive analysis and simulation using real workload traces from Google's compute clusters, we show that our proposed framework can achieve significant reduction in energy cost, while maintaining an acceptable average scheduling delay for individual tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data centers today are home to a vast number and a variety of applications with diverse resource demands and performance objectives. Typically, a cloud application can be divided into one or more tasks executed in one or more containers (e.g., virtual machines (VMs)). At run time, schedulers are responsible for assigning tasks to machines. In today's reality, production data centers such as Google's cloud backend often execute tremendous number (e.g., millions) of tasks on a daily basis <ref type="bibr" target="#b26">[27]</ref>. Such extremely large-scale workload hosted by data centers not only consumes significant storage and computing power, but also huge amounts of energy. In practice, the operational expenditure on energy not only comes from running physical machines, but also from cooling down the entire data center. It has been reported that energy consumption accounts for more than 12% of monthly operational expenditures of a typical data center <ref type="bibr" target="#b5">[5]</ref>. For large companies like Google, a 3% reduction in energy cost can translate into over a million dollars in cost savings <ref type="bibr" target="#b18">[19]</ref>. On the other hand, governmental agencies continue to implement standards and regulations to promote energy-efficient (i.e., "Green") computing <ref type="bibr" target="#b1">[1]</ref>. Motivated by these observations, cutting down electricity cost has become a primary concern of today's data center operators.</p><p>In the research literature, a large body of recent work tries to improve energy efficiency of data centers. A plethora of techniques have been proposed to tackle different aspects of the problem, including the control of power distribution systems <ref type="bibr" target="#b19">[20]</ref>, cooling systems <ref type="bibr" target="#b7">[8]</ref>, computer hardware <ref type="bibr" target="#b24">[25]</ref>, software components such as virtualization <ref type="bibr" target="#b23">[24]</ref> and loadbalancing algorithms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. It is known that one of the most effective approach for reducing energy cost is to dynamically adjust the data center capacity by turning off unused machines, or to set them to a power-saving (e.g., "sleep") state. This is supported by the evidence that an idle machine can consume as much as 60% of the power when the machine is fully utilized <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Unsurprisingly, a number of efforts are trying to leverage this fact to save energy using techniques such as VM consolidation <ref type="bibr" target="#b23">[24]</ref> and migration <ref type="bibr" target="#b22">[23]</ref>. However, these studies have mainly focused on improving the utilization of clusters by improving the "bin-packing" algorithm for VM scheduling. In a production data center where resource requests for tasks can arrive dynamically over time, deciding the number of machines to be switched off is not only affected by the efficiency of the scheduling algorithm, but also time-dependent characteristics of resource demand. While over-provisioning the data center capacity can lead to sub-optimal energy savings, under-provisioning the data center capacity can cause significant performance penalty in terms of scheduling delay, which is the time a task has to wait before it is scheduled on a machine. A high scheduling delay can significantly hurt the performance of some services that must be scheduled as soon as possible to satisfy end user requests (e.g., user-facing applications). On the other hand, tasks in production data centers often desire multiple types of resources, such as CPU, memory, disk and network bandwidth. In this context, devising a dynamic capacity provisioning mechanism that considers demand for multiple types of resources becomes a challenging problem. Furthermore, there are reconfiguration costs associated with switching on and off machines. In particular, turning on and off a machine often consumes large amount of energy due to saving and loading system states to memory and disk <ref type="bibr" target="#b17">[18]</ref>. When turning off a machine with running tasks, it is necessary to consider the performance penalty due to migrating (or terminating) the tasks on the machine. Therefore, the reconfiguration cost due to server switching should be considered as well. Finally, another aspect often neglected in the existing literature is the electricity price. For example, it is known that in many regions of the U.S., the price of electricity can change depending on the time of the day. Electricity price is thus another factor that should be considered when making capacity adjustment decisions.</p><p>In this paper, we present a solution to the dynamic capacity provisioning problem with the goal of minimizing total energy cost while maintaining an acceptable task scheduling delay. Different from existing works on server capacity provisioning problem, we formulate the problem as a convex optimization problem that considers multiple resource types and fluctuating electricity prices. We then analyze the optimality condition of this problem and design a Model Predictive Control (MPC) algorithm that adjusts the number of servers to track the optimality condition while taking into account switching costs of machines. Through analysis and simulation using real workload traces from Google's compute clusters, we show our proposed solution is capable of achieving significant energy savings while minimizing SLA violations in terms of task scheduling delay.</p><p>The remainder of the paper is organized as follows. Section 2 presents a survey of related work in the research literature. In Section 3, we present an analysis of real workload traces for one of Google's production compute clusters and illustrate the benefits of our approach. Section 4 describes the architecture of our proposed system. In Section 5, we present our demand prediction model and control algorithm. In Section 6, we provide our detailed formulation for the optimal control problem and present our solution based on the MPC framework. In Section 7, we evaluate our proposed system using Google workload traces, and demonstrate the benefits under various parameter settings. Finally, we draw our conclusions in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Much effort has been made to achieve energy savings in data centers. Dynamic capacity provisioning is one of the most promising solutions to reduce energy cost that consists of dynamically turning on and off data center servers. For instance, motivated by the time-dependent variation of the number of users and TCP connections in Windows live messenger login servers, Chen et al. <ref type="bibr" target="#b10">[11]</ref> have derived a framework for dynamic server provisioning and load dispatching. They have proposed a technique to evaluate the number of needed servers based on the predicted load in terms of users' login rate and active TCP connections. The load dispatching algorithm ensures that incoming requests are distributed among the servers. However, their framework does not consider the cost of switching on and off machines. Guenter et al. <ref type="bibr" target="#b15">[16]</ref> have proposed another automated server provisioning and load dispatching system based on the predicted demand while considering the cost of transitioning servers between different power states (e.g., "on", "off", "hibernate"). This cost depends on the transition time, the energy cost and the long-term reliability of the server. Different from our work, they analyze the number of requests that can be satisfied instead of request scheduling delay. Furthermore, the multi-dimensional aspect of resource demand and fluctuations of electricity prices are not considered in their work.</p><p>Kusic et al. <ref type="bibr" target="#b17">[18]</ref> have proposed a dynamic resource provisioning framework for virtualized server environments based on lookahead control. The framework minimizes power consumption by adjusting the number of physical and virtual machines. It also estimates the CPU share and the workload directed to every virtual machine. In addition, their controller manages to maximize the number of transactions that satisfy Service Level Agreement (SLA) in terms of average response time while taking into account the cost of turning on and off the machines. However, they mainly consider the performance of application servers rather than the scheduling of VMs. Furthermore, time-dependent variations of electricity prices are not considered in their framework. Abbasi et al. <ref type="bibr" target="#b6">[7]</ref> have proposed a thermal-aware server provisioning technique and a workload distribution algorithm. In this approach, active servers are selected using heuristics in a way that minimizes cooling and computing energy cost. The workload distribution algorithm ensures that servers' utilizations do not exceed a threshold in order to satisfy SLA defined in terms of average response time. However, their approach does not consider switching cost of machines.</p><p>There is also a large body of work that applies control theory to achieve energy savings in data centers. Fu et al. <ref type="bibr" target="#b14">[15]</ref> have proposed a control-theoretic thermal balancing that reduces temperature differences among servers. Hence, the controller acts on the utilization of each processor in order to reduce or increase its temperature. Model predictive control        is used by Wang et al. <ref type="bibr" target="#b25">[26]</ref> to reduce the total power consumption of a cluster by tuning CPU frequency level for the processor of each server. However, most of previous work has focused on capacity provisioning from a service provider perspective, i.e., provisioning server capacities (e.g., number of web servers) to accommodate end user requests. Existing solutions to this problem rely on queuing-theoretic models that consider only a single type of resource (mainly CPU). In contrast, our approach investigates the problem from the cloud provider's perspective, where resource demand and usage are multi-dimensional. Our solution considers resource usage and capacity for multiple resource types, such as CPU and memory. Furthermore, none of the existing work has considered additional factors such as the fluctuating electricity prices. Our approach is also lightweight and independent of the scheduling algorithm, making it more suitable for practical implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WORKLOAD ANALYSIS</head><p>To motivate the problem and justify our solution approach, we have conducted an analysis of workload traces for a production compute cluster at Google <ref type="bibr" target="#b4">[4]</ref> consisting of approximately 12, 000 machines. The dataset was released on November 29, 2011. The workload traces contain scheduling events as well as resource demand and usage records for a total of 672, 003 jobs and 25, 462, 157 tasks over a time span of 29 days. Specifically, a job is an application that consists of one or more tasks. Each task is scheduled on a single physical machine. When a job is submitted, the user can specify the maximum allowed resource demand for each task in terms of required CPU, memory and disk size. At run time, the usage of a task measures the actual consumption of each type of resources. The current Google cluster traces provide task demand and usage for CPU, memory and disk<ref type="foot" target="#foot_0">1</ref> . The usage of each type of resource is reported at 5 minute intervals. Our current analysis mainly focuses on CPU and memory, as they are typically scarce compared to disk. However, we believe it is straightforward to extend our approach to consider other resources such as disk space.</p><p>In addition to resource demand, the user can also specify a scheduling class, a priority and placement constraints for each task. The scheduling class captures the type of the task (e.g., user-facing or batch). The priority determines the importance of each task. The task placement constraints specify additional scheduling constraints concerning the machine configurations, such as processor architecture of the physical machine <ref type="bibr" target="#b20">[21]</ref>. To simplify, we do not consider the scheduling class, priority and task placement constraints in our model. The analysis of these factors is left for future work.</p><p>We first plot the total demand and usage for both CPU and memory over the entire duration. The results are shown in Figure <ref type="figure" target="#fig_1">1</ref> and 2 respectively. The total usage at a given time is computed by summing up the resource usage of all the running tasks at that time. On the other hand, the total demand at a given time is determined by total resource requirement by all the tasks in the system, including the tasks that are waiting to be scheduled. From Figure <ref type="figure" target="#fig_1">1</ref> and 2, it can be observed that both usage and demand for each type of resource can fluctuate significantly over time. Figure <ref type="figure" target="#fig_5">3</ref> shows the number of machines available and used in the cluster. Specifically, a machine is available if it can be turned on to execute tasks. A machine is used if there is at least one task running on it. Figure <ref type="figure" target="#fig_5">3</ref> shows that the capacity of this cluster is not adjusted based on resource demand, as the number of used machines is almost equal to the number of available machines. Combining the observations from Figure <ref type="figure" target="#fig_1">1</ref>, 2 and 3, it is evident that a large number of machines can be turned off to save energy. For instance, we estimated that a perfect energy saving schedule where the provisioned capacity exactly matches the current demand can achieve about 22% and 17% percent resource reduction for CPU and memory, respectively, compared to provisioning capacity according to the peak demand. This indicates that there is great potential for energy savings in this compute cluster using dynamic capacity provisioning.</p><p>However, while turning off active machines can reduce total energy consumption, turning off too many machines can also hurt task performance in terms of scheduling delay. grows exponentially with resource utilization. To quantify this effect, we analyzed the relationship between scheduling delay experienced by each task and the average utilization of the bottleneck resource (e.g., CPU) while the task is waiting to be scheduled. We then plotted the average task scheduling delay as a function of the utilization of the bottleneck resource, as shown in Figure <ref type="figure" target="#fig_7">4</ref>. The error bar in this diagram represents the standard deviation of task scheduling delay with average utilization at each given value. Indeed, we found that there is a direct relationship between task scheduling delay and resource utilization. We also modeled the relationship through curve fitting. The above observations suggest that while the benefits of dynamic capacity provisioning is apparent for production data center environments, designing an effective dynamic capacity provisioning scheme is challenging, as it involves finding an optimal tradeoff between energy savings and scheduling delay. Furthermore, turning off active machines may require killing or migrating tasks running on these machines, which will introduce an additional performance penalty. The goal of this paper is to provide a solution to this dynamic capacity provisioning problem that finds the optimal trade-off between energy savings and the cost of reconfigurations, including cost of turning on and off machines and killing/migrating tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classic queuing theory indicates that task scheduling delay</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SYSTEM ARCHITECTURE</head><p>Our proposed system architecture is depicted in Figure <ref type="figure" target="#fig_8">5</ref>. It consists of the following components:</p><p>• The scheduler is responsible for assigning incoming tasks to active machines in the cluster. It also reports the average number of tasks in the queue during each control period to help the controller make informed decisions.</p><p>• The monitoring module is responsible for collecting CPU and memory usage statistics of every machine in the cluster. The monitoring is performed periodically.</p><p>• The prediction module receives statistics about the usage of all resources (CPU and memory) in the cluster and predicts the future usage for all of them.</p><p>• The controller implements a MPC algorithm that controls the number of machines based on the predicted usage of the cluster and taking into account the reconfiguration cost.</p><p>• The capacity provisioning module gathers the status information of machines from the controller, and decides which machines in particular should be added or removed. It then provides the scheduler with the list of active machines.</p><p>It is worth mentioning that different schedulers may adopt different resource allocation schemes. For example, in a public cloud environment such as Amazon EC2, it is necessary to schedule tasks according to their resource demand (e.g., VM size). However, since the actual resource usage of each task can be much lower than the demand, many advanced schedulers adjust dynamically resource allocation based on task usage <ref type="bibr" target="#b21">[22]</ref>. Even though our framework is applicable to both scenarios, in this work, we use the latter case for illustration, not only because it is more general, but also because it reflects the behavior of Google cluster schedulers.</p><p>In particular, Google's schedulers intentionally over-commit resources on each machine <ref type="bibr" target="#b3">[3]</ref>. Finally, as an initial effort towards solving this problem, we currently consider that all the machines in the cluster are homogenous and with identical resource capacities. It is part of our future work to extend our model to consider machine heterogeneity (e.g., multiple generations of machines <ref type="bibr" target="#b20">[21]</ref>).</p><p>In the following sections, we will describe the design of each component in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">USAGE PREDICTION</head><p>In this section, we describe our model for predicting usage of each resource type. We used the Auto-Regressive Integrated Moving Average (ARIMA) model <ref type="bibr">[9]</ref> to predict the time series G r k which represents the usage of resource type r in all the machines at time k. For convenience, we drop the superscript r and we write simply G k in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">One-step Prediction</head><p>Knowing the last n observations of G k , i.e., G k-n+1 ,... G k , we want to predict G k+1 , which is the expected usage at time k + 1 predicted at time k. The time series G k follows an ARMA(n,q) model if it is stationary and if for every k:</p><formula xml:id="formula_0">G k+1 = φ0G k + .. + φn-1G k-n+1 +ǫ k+1 + θ0ǫ k + .. + θq-1ǫ k+1-q ,<label>(1)</label></formula><p>where the φi and θj are constants estimated from available data. The terms ǫ k are error terms which are assumed to be independent, identically distributed samples from a normal distribution with zero mean and finite variance σ 2 . The parameters n and q are the number of lags used by the model (i.e., the number of last measured values of the usage) and the number of error terms respectively. Equation (1) can also be written in a concise form as:</p><formula xml:id="formula_1">G k+1 = n-1 i=0 φiL i G k + ǫ k+1 + ( q-1 i=0 θiL i )ǫ k , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where L is the backward shift operator defined as follows:</p><formula xml:id="formula_3">L i G k = G k-i .</formula><p>We point out that AR and MA models are special cases of the ARMA model when q = 0 or n = 0. The ARMA model fitting procedure assumes that the data are stationary. If the time series exhibits variations, we use differencing operation in order to make it stationary. It is defined by:</p><formula xml:id="formula_4">(1 -L)G k = G k -G k-1 .<label>(3)</label></formula><p>It can be shown that a polynomial trend of degree k is reduced to a constant by differencing k times, that is, by applying the operator (1-L) k y(t). An ARIMA(n,d,q) model is an ARMA(n,q) model that has been differenced d times. Thus, the ARIMA(n,d,q) can be given by:</p><formula xml:id="formula_5">1 - n-1 i=0 φiL i (1 -L) d G k = 1 + q-1 i=0 θiL i ǫ k (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-step Prediction</head><p>In our model, we aim to predict future resource usage over a time window H ∈ N + . This requires predicting resource usage h ∈ N + steps ahead from an end-of-sample G k for all 1 ≤ h ≤ H. Let G k+h|k denote the h th step prediction of G k knowing the last n observations, i.e., G k-n+1 ,... G k . Thus, we aim to predict G k+1|k ,G k+2|k ....,G k+h|k . The multi-step prediction is obtained by iterating the one-step ahead prediction. The h th step prediction G k+h|k is given by:</p><formula xml:id="formula_6">G k+h|k = f (G k+h-n|k , ..., G k+h-i|k , ..., G k+h-1|k ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">G k-i|k = G k-i ∀i ∈ [0, n],</formula><p>the function f is the prediction model, n is the number of lags used by the model and h is the prediction step. Table <ref type="table">1</ref> illustrates how one-step prediction is iterated to obtain multi-step predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONTROLLER DESIGN</head><p>We formally describe the dynamic capacity provisioning problem in this section. We assume the cluster consists of M k ∈ N + homogeneous machines at time k. The number of machines in the cluster can change due to machine failure and recovery. We assume each machine has d ∈ N types of resources. For example, a physical machine provides CPU, memory and disk. Let R = {1, 2, ..., d} denote the set of Table <ref type="table">1</ref>: Example of multi-step prediction (n = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Inputs of the model Output step</p><formula xml:id="formula_8">1 G k-2|k ,G k-1|k ,G k|k G k+1|k 2 G k-1|k ,G k|k ,G k+1|k G k+2|k 3 G k|k ,G k+1|k ,G k+2|k G k+3|k 4 G k+1|k ,G k+2|k ,G k+3|k G k+4|k</formula><p>resource types. Denote by C r ∈ R + the capacity for resource type r ∈ R of a single machine. To model the system dynamics, we divide time into intervals of equal duration. We assume reconfiguration happens at the beginning of each time interval. At interval k ∈ N + , the measured usage for resource type r in the cluster is denoted by G r k . Let x k denote the number of active machines. Denote by u k ∈ R the change in the number of active machines. A positive value of u k means more machines will be turned on, whereas a negative value of u k means some active machines will be powered off. Therefore, we have the following simple state equation that calculates the number of active machines at time k + 1:</p><formula xml:id="formula_9">x k+1 =x k + u k . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Our objective is to control the number of machines in order to reduce the total operational cost in terms of energy consumption and penalty due to violating the SLA, while taking into consideration the cost of dynamic reconfiguration. In what follows, we describe how to model each of the cost factors in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Modeling SLA penalty cost</head><p>In our model, the SLA is expressed in terms of an upper bound d on the average task scheduling delay. Thus, in order to meet the SLA, the average task scheduling delay d k at time k should not exceed d. As suggested in Section 3, the average task scheduling delay is correlated with the cluster resources' utilization, and more particularly with the utilization of the bottleneck resource. Therefore, we define the bottleneck resource b ∈ R at time k ∈ N + as the resource that has the highest utilization. In our model, the utilization of resource r ∈ R in the cluster at time k ∈ N + is given by:</p><formula xml:id="formula_11">U r k = G r k x k C r .<label>(7)</label></formula><p>Therefore, the utilization of the bottleneck resource b can be calculated as:</p><formula xml:id="formula_12">U b k = max r∈R {U r k } .<label>(8)</label></formula><p>Then the average scheduling delay at time k ∈ N can be expressed as:</p><formula xml:id="formula_13">d k = q b (U b k ),<label>(9)</label></formula><p>where q b (U b k ) denotes the average latency given current utilization U b k for the bottleneck resource b. The function q b (•) can be obtained using various techniques, such as queue theoretic models, or directly from empirical measurements as described in Section 3.</p><p>We adopt a simple penalty cost model for SLA violation. Specifically, if the delay bound is violated, then there will be a SLA penalty cost P SLA k proportional to the degree of violation. Therefore, the penalty function P SLA k (•) can be rewritten as:</p><formula xml:id="formula_14">P SLA k (U b k ) = N k p SLA (q(U b k ) -d) + ,<label>(10)</label></formula><p>where p SLA represents the unit penalty cost for violating the delay upperbound, and N k is the weight factor representing the severity of the violation at time k (e.g., N k can be the number of requests in the scheduling queue at time k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Modeling the total energy cost</head><p>In the research literature, it is known that the total energy consumption of a physical machine can be estimated by a linear function of CPU, memory and disk usage <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Thus, the energy consumption of a machine at time k can be expressed as:</p><formula xml:id="formula_15">e k = E idle + r∈R α r U r k .<label>(11)</label></formula><p>Let p power k denote the electricity price at time k. Then, for a given number of machines x k , the total energy cost P power k at time k can be expressed as</p><formula xml:id="formula_16">P power k (x k ) = p power k x k e k = p power k x k (E idle + r∈R α r G r x k C r ). (<label>12</label></formula><formula xml:id="formula_17">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Formulation of the optimization problem</head><p>As mentioned previously, our objective is to control the number of servers so as to minimize the total operational cost, which is the sum of SLA penalty cost P SLA k (U b k ) and energy cost P power k (x k ). At the same time, we need to ensure that the number of active machines in the cluster must not exceed M k , the total number of physical machines in the cluster. This can be formulated by the following optimization problem: min</p><formula xml:id="formula_18">x k ∈R + N k p SLA q max r∈R G r x k C r -d + + p power k x k E idle + r∈R α r G r x k C r<label>(13)</label></formula><formula xml:id="formula_19">s.t. 0 ≤ x k ≤ M k ,</formula><p>where (x) + = max (x, 0). Notice that p power k</p><formula xml:id="formula_20">x k • r∈R α r G r x k C r</formula><p>does not depend on x k at time k, thus it can be omitted in the optimization formulation. In addition, define</p><formula xml:id="formula_21">w k = maxr{ G r k</formula><p>C r }, we can further simplify the problem to:</p><formula xml:id="formula_22">min 0≤x k ≤M k N k p SLA q w k x k -d + + p power k x k E idle . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>Assuming that q( w k x k ) is a decreasing function of x k (namely, the average queuing delay decreases as the number of machines increases), we can see that the optimal solution x * k of this problem satisfies</p><formula xml:id="formula_24">x * k ≤ w k q -1 ( d) , since if x * k ≥ w k q -1 ( d) , (q( w k x k ) -d) + = 0, in this case we can decrease x * k to w k q -1 ( d)</formula><p>to reduce energy cost while maintaining (q( w k x k ) -d) + = 0. Therefore, we can further simplify the problem to:</p><formula xml:id="formula_25">min 0≤x k ≤ w k q -1 ( d) N k p SLA (q( w k x k ) -d) + p power k x k E idle<label>(15)</label></formula><p>In order to solve this optimization problem, we use the Karush-Kuhn-Tucker (KKT) approach <ref type="bibr" target="#b9">[10]</ref>. The Lagrangian function is</p><formula xml:id="formula_26">L(x k , γ) =N k p SLA q w k x k -d + p power k x k E idle + γ(x k - w k q -1 ( d) ) + µ (0 -x k ) . (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>The KKT conditions are:</p><formula xml:id="formula_28">dL dx = p power k E idle -N k p SLA w k dq w k x k dx 1 x 2 k + γ -µ = 0, µx k = 0, γ( w k q -1 ( d) -x k ) = 0, 0 ≤ x k ≤ w k q -1 ( d)</formula><p>, µ, γ ≥ 0.</p><p>We need to consider three cases: (1) γ &gt; 0, (2) µ &gt; 0, and (3) γ = 0 and µ = 0. The first two cases correspond to boundary conditions whether x * k = w k q -1 ( d) or x * k = 0. In the third case, assuming q(•) is convex and differentiable, we can solve x * k using the first condition. For instance, q(U ) = a • U 1-U + b (which is the case for M/M/1 queuing model), we can obtain</p><formula xml:id="formula_29">x * k = w k + N k p SLA aw k p power k E idle . (<label>17</label></formula><formula xml:id="formula_30">)</formula><p>The above equation reveals many insights. First, the optimal number of servers x * k depends mainly on the cluster utilization, which is captured by the variable w k . Second, x * k is also dependent on the electricity price and the SLA violations. In particular, it increases either when the electricity price drops down or when the SLA penalty cost rises. Therefore, it can be seen that equation <ref type="bibr" target="#b16">(17)</ref> tries to strike a balance between saving electricity cost and SLA penalty cost in a dynamic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Capacity provisioning module</head><p>The capacity provisioning module takes as input the number of machines that should be added or removed from the cluster, and determines which machine should be turned on or off. The decision of switching on a particular machine can be made based on different criteria such as its usage and its location in the cluster. However, choosing which machine to power off is more complicated since some tasks could be running on it. Thus, more criteria should be considered such as the number of running tasks on the machine, their priorities, the cost of migrating or killing those tasks as well as the resource usage in the machine. For simplicity, define ct as the cost for migrating (or terminating) the task t, depending on the scheduling policy applied to task t. For example, if task t is an interactive task such as a web server, it is better to migrate the server to another machine to minimize the service down time. On the other hand, if the task t belongs to a MapReduce job, it is more cost-effective to simply terminate the task and restart it on a different machine <ref type="bibr" target="#b11">[12]</ref>. We define the cost of powering off a particular machine i, Perform the reconfiguration using the capacity provisioning module according to u(k|k) 97:</p><formula xml:id="formula_31">1 ≤ i ≤ M k at time k as c i k = t∈S i k ct,<label>(18)</label></formula><p>k ← k + 1 98: end loop where S i k denotes the set of tasks running on the machine i at time k ∈ N. It is clear that c i k increases with the number of tasks and their costs. Consequently, the capacity provisioning module turns off machines having the lowest cost c i k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Designing the MPC controller</head><p>The goal of our controller is to adjust the number of machines to minimize the violation of KKT conditions, while taking into consideration the reconfiguration cost. As N k , w k , p power k can change over time, we adopt the well-known MPC framework to design an online controller for this problem. The MPC algorithm is illustrated by Algorithm 1. It can be intuitively described as follows: At time k, the prediction module is responsible for predicting the future values of N k , w k , p power k for a prediction window H. The controller will then solve an optimal control problem that will determine the optimal decisions for the entire window H. As only the first step is required, the controller will only carry out the first control decision. The procedure will repeat at the beginning of every time interval k, k + 1, and so on.</p><p>More formally, we can define N k+h|k ,w k+h|k , p power k+h|k as the values of N k , w k , p power k predicted for time k + h, given the historical values up to time k. We also define</p><formula xml:id="formula_32">e k+h|k = x k+h|k -x * k+h|k ,<label>(19)</label></formula><p>as the tracking error at time k, the objective of the controller is to solve the following program:</p><formula xml:id="formula_33">min u k ∈R J k = H h=1 Q(e k+h|k ) 2 + R(u k+h|k ) 2<label>(20)</label></formula><formula xml:id="formula_34">s. t. x k+h+1|k = x k+h|k + u k+h|k , ∀0 ≤ h ≤ H -1 e k+h|k = x k+h|k -x * k+h|k , ∀1 ≤ h ≤ H 0 ≤ x k+h|k ≤ N, ∀1 ≤ h ≤ H</formula><p>where H is the horizon of interest. The first term represents the tracking error, the second term represents the control penalty. The tracking error aims to reduce the error between the actual and the optimal number of machines. The second term is the control penalty which takes into account the cost of adding or removing machines. Thus, J k can be interpreted as a plan of action for next H time intervals. Q and R are weight factors that will control the stability and convergence rate of the controller. If Q is much bigger than R, then the controller will place a higher weight on maximizing power savings and adjust number of servers aggressively. On the other hand, if R is large compared to Q, then the controller will adjust the capacity less aggressively to minimize reconfiguration cost. A standard way to determine the values of Q and R is to normalize both terms. In our case, we normalize them by converting both terms into monetary cost. Define r = 1  2 (c avg,on + c avg,of f ) where c avg,on and c avg,of f are the average cost for turning on and off servers, respectively. Similarly, we define q = 1 2 (c avg over +c avg under ) where c avg over is average cost introduced per machine due to provisioning, and c avg under is average cost introduced per machine due to underprovisining, respectively. Even though it is possible to compute analytically the values of c avg,on , c avg,of f , c avg over , c avg under , it is more practical to estimate their values through empirical measurement. Notice that we set q and r to the average penalty cost of both positive and negative errors, because in practice, the number of occurrences of both positive and negative errors will likely to be the same, if the capacity provisioned by our controller only fluctuates within a fixed range. Finally, although we can set (Q, R) = (q<ref type="foot" target="#foot_1">2</ref> , r2 ) to ensure both terms are in the unit of dollar 2 , to simplify our model, we set (Q, R) = (1, r2 q2 ) in our experiment so that we only need to control R to achieve different trade-offs between solution optimality and reconfiguration cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTAL EVALUATION</head><p>We have implemented our system shown in Figure <ref type="figure" target="#fig_8">5</ref> and evaluated the quality of our solution using trace-driven simulations. In our experiment, we set the CPU and memory capacity of each machine to 1. This represents a majority of machines in the Google cluster 2 . In our simulation we implemented a greedy First-Fit (FF) scheduling algorithm, which is used by many cloud computing platforms such as Eucalyptus <ref type="bibr" target="#b2">[2]</ref>. In our simulations, we set E idle to 200 Watts, α r to 121 and 0 for CPU and memory, respectively, similar to the values used in <ref type="bibr" target="#b17">[18]</ref>. For electricity price, we used the electricity prices for the city of Houston, Texas, obtained from a publicly available source <ref type="bibr">[6]</ref>. Figure <ref type="figure" target="#fig_10">6</ref> shows the fluctuation of electricity price over a duration of 24 hours. It can be observed that the electricity price is generally higher during day time. The fluctuation sometimes can be as large as 20% compared to the average electricity price over the 24 hours. Finally, we set d to 10 seconds as an upperbound on task scheduling delay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Prediction performance</head><p>In our first experiment, we assess the performance of the multi-step prediction for resource usages. We first describe the prediction procedure and the performance criteria. Then   we study the effect of the number of lags and the prediction horizon on the prediction accuracy.</p><p>To evaluate the quality of our prediction technique, the available traces (e.g., measured CPU or memory usage) are divided into a training data set and a validation data set. Training data are used to identify the model parameters n, d, q and the coefficient φi and θj. For given n and q, the coefficients φi, i ≤ n and θj, j ≤ q are estimated using the RPS toolkit <ref type="bibr" target="#b12">[13]</ref>. The validation data set is then used to assess the accuracy of the prediction model. The performance metric used to evaluate the prediction accuracy is the relative squared error (RSE). It is calculated for every prediction step h as:</p><formula xml:id="formula_35">RSE h = T k=1 G k -G k+h|k 2 T k=1 [G k -µ] 2 (21)</formula><p>where T is the size of the validation data set and µ is the average of G k over the T time intervals. The advantage of the RSE h is that it neither depends on the used scale nor on the size of data. Having the RSE h lower than 1 means that the predictor is outperforming the use of the average of the data as prediction for G k (G k+h|h = µ). In addition, the smaller is the RSE h , the more accurate is the prediction. The RSE can also be seen as the ratio of the mean squared error divided by the variance of validation data set.</p><p>Since our model exploits the predicted usage of the cluster in terms of CPU and memory to proactively add and remove servers, we assess the prediction model accuracy. We applied the ARIMA model to the real data collected at the Google cluster and we evaluated the effect of the number of lags used as input for the prediction model (n) and the effect of the prediction horizon (h) on the multi-step squared error (RSE h ). Memory and CPU usage are measured every five minutes. Hence, a one-step prediction is equivalent to predict the cluster usage in the next five minutes.  Figure <ref type="figure" target="#fig_12">7</ref> shows the one-step prediction of the CPU and memory usage compared to the real usage. The graph shows that the predicted values are always close to the real ones even during peaks. The prediction relative squared error RSE1 is close to zero which proves that the ARIMA(2,1,1) provides an accurate prediction of the usage either for CPU (RSE1 ≈ 0.062) or memory (RSE1 ≈ 0.086).</p><p>Figure <ref type="figure" target="#fig_13">8</ref> depicts the effect of increasing the number of lags used in the ARIMA model to predict CPU and memory usage. Regarding CPU usage prediction (Figure <ref type="figure" target="#fig_13">8</ref>(a)), it is apparent from the results that starting from the second lag (n = 2), the prediction error becomes stable around RSE1 ≈ 0.062. If we now turn to memory usage prediction, Figure <ref type="figure" target="#fig_13">8</ref>(b) shows the prediction error remains almost stable regardless of the number of lags used for the ARIMA model (RSE1 ≈ 0.086). Consequently, there is no improvement of the prediction performance beyond two lags (n = 2). This result is interesting since a small number of lags reduces the ARIMA model complexity and allows to implement it online with minimal overhead and high accuracy.</p><p>We also conducted more experiments to examine the impact of the horizon h on the prediction performance. Since using more than two lags does not reduce the prediction error, we only considered two lags as input for the ARIMA model (i.e., n = 2). As expected, when we increase the prediction horizon, the prediction error grows for both CPU and memory usage (Figure <ref type="figure" target="#fig_15">9</ref>). What is interesting in these results is that the error remains small (RSE h ≤ 1) for multiple prediction steps. In particular, the prediction error RSE h remains below 1 for 400 steps ahead (≈ 33 hour) for CPU usage and for 50 steps ahead (≈ 250 min) for memory usage. We also mention that increasing the number of error terms (q) for the ARIMA model does not improve the prediction performance. In summary, these results suggest that we can apply ARIMA(2,1,1) using two lags to predict 12 steps ahead (equivalent to one hour), and this ensures that the prediction error does not exceed 0.3 and 0.5 for CPU and memory, respectively (Figure <ref type="figure" target="#fig_15">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Controller performance</head><p>We conducted several experiments to evaluate the performance of our controller. In our experiment, we set the control frequency to once every 5 minutes to match Google's Cluster measurements frequency <ref type="bibr" target="#b4">[4]</ref>. Typically, a high control frequency implies fast response to demand fluctuations. However, it also incurs a high computational overhead. However, we found the computational overhead of both demand prediction algorithm and controller to be almost negligible, thus, once every 5 minutes is a reasonable control frequency.    Lastly, we set Q = 1 in our experiments. Thus, the reconfiguration cost can be controlled by properly adjusting the value of R.</p><p>In our experiments, we first evaluated the response of our system to usage fluctuation. The number of active servers provisioned over the 24-hour duration is shown in Figure <ref type="figure" target="#fig_1">11</ref> (for R = 0.1). Figure <ref type="figure" target="#fig_17">10</ref> show the capacity and the usage of the cluster. It can be observed that the controller adjusts the number of servers dynamically in reaction to usage fluctuation, while avoiding rapid change in the number of active machines. The cumulative distribution function of task scheduling delay is shown in Figure <ref type="figure" target="#fig_3">12</ref>. It can be seen that more than 60% of tasks are scheduled immediately.</p><p>We performed several other experiments for comparison purpose. In the first experiment, the number of machines is provisioned statically according to peak usage (i.e., 4100 machines). In the remaining experiments, we applied our controller using different values of R. Figures <ref type="figure" target="#fig_21">13</ref> and<ref type="figure" target="#fig_7">14</ref> show the corresponding average scheduling delay and energy consumption for different values of R compared to the static provisioning. It can be observed that the static provisioning achieves the lowest scheduling delay since it significantly overprovisions the cluster capacity. On the other hand, dynamic provisioning with R = 0.5 causes a significant scheduling delay although it allows to reduce the energy consumption (up to 50%). Furthermore, setting R to a small value (e.g., 0.02) does not achieve significant energy reduction. Through experiments, we found that setting R = 0.225 achieves our desired SLA objective of keeping the average scheduling delay around 10 seconds while reducing the energy consumption by 18.5%. Figure <ref type="figure" target="#fig_22">15</ref>    in energy costs, which implies 20% reduction in energy cost, while achieving the desired scheduling delay (for R = 0.225). Furthermore, depending on the desired average scheduling delay (Figure <ref type="figure" target="#fig_21">13</ref>), our proposed approach can reduce total operational cost by about 18.5 -50% (Figure <ref type="figure" target="#fig_22">15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>Data centers have become a cost-effective infrastructure for data storage and hosting large-scale service applications. However, large data centers today consume significant amounts of energy. This not only rises the operational expenses of cloud providers, but also raises environmental concerns with regard to minimizing carbon footprint. In this paper, we mitigate this concern by designing a dynamic capacity provisioning system that controls the number of active servers in the data center according to (1) demand fluctuation, (2) variability in energy prices and (3) the cost of dynamic capacity reconfiguration. Our solution is based on the well-established Model Predictive Control framework, and aims to find a good trade-off between energy savings and capacity reconfiguration cost. Simulations using real traces obtained from a production Google compute clusters demonstrate our approach achieves considerable amount of reduction in energy cost. As such, we believe our approach represents an initial step towards building a full-fledged capacity management framework for cloud data centers.</p><p>There are several promising directions we can pursue in the future. First, our current approach assumes that machines are homogenous. While this is applicable to many situations (cloud providers often buy large quantities of identical machines in bulk), recent literature suggests that production data centers often consists of multiple types (sometimes multiple generations) of machines. Extending our current solution to handle machine heterogeneity requires careful consideration of scheduling capability of each type of machine. Another interesting problem is to understand the interplay between the scheduler and the capacity controller.</p><p>We believe it is possible to further reduce the cost of energy consumption and reconfiguration (i.e., task preemption and migration cost) if the scheduler and the capacity controller can cooperate tightly at a fine-grained level (e.g., interaction of server consolidation algorithms with our capacity controller).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Total CPU demand and usage (29 days).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Total memory demand and usage (29 days).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of machines available and used in the cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average task scheduling delay vs. Resource utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: System architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>It seems that both a linear function (i.e., d = a • U + b) or a delay function for M/M/1 queuing model (i.e., d = a • U 1-U + b)) can fit the curve well. Similar observations have been reported in recent work [27][21].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Electricity price of Houston, TX over 24 hours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Prediction of resource usage in the Google cluster -one-step prediction -ARIMA(2, 1, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of the number of lags on usage prediction -One-step prediction (h = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of multi-step prediction -ARIMA(2, 1, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Capacity vs. Usage over 24 hours (R=0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 11 Figure 12 :</head><label>1112</label><figDesc>Figure 11: Number of machines over 24 hours (R=0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>shows the actual energy cost per hour, taking into consideration the fluctuation of the electricity price. It can be seen that our dynamic capacity provisioning mechanism reduces 7 dollars per hour Static 0.02 0.1 0.225 0.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 13</head><label>13</label><figDesc>Figure 13: Average Queuing Delay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Average cost per hour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 MPC Algorithm for DCP 91: Provide initial state x k , k ← 0 92: loop 93: At the end of control period k: 94: Predict N k+h|k , w k+h|k , p power k+h|k for time h ∈ {1, H} 95: Solve tracking problem to obtain u(k + h|k) for horizons h ∈ {0, • • • , H} 96:</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that the values reported in Google cluster traces were normalized between 0 and 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The values of CPU and memory capacity reported in Google traces were normalized to the configuration of the largest machine.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by a Google Research Award, by the Natural Science and Engineering Council of Canada (NSERC) SAVI Research Network, and by the World Class University (WCU) Program under the Korea Science and Engineering Foundation funded by the Ministry of Education, Science and Technology (Project No. R31-2008-000-10100-0).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Energy star computers specification -feb</title>
		<ptr target="http://www.energystar.gov/ia/partners/proddevelop-ment/revisions/downloads/computer/ESComputers" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Draft 1 Version 6.0 Specification.pdf</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://open.eucalyptus.com/" />
		<title level="m">Eucalyptus community</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://googleclusterdata.googlecode.com/files/Google%20cluster" />
		<title level="m">Google cluster-usage traces: format + schema</title>
		<imprint/>
	</monogr>
	<note>usage%20traces%20-%20format%20%2B %20schema%20%282011.10.27%20external%29.pdf</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://code.google.com/p/googleclusterdata/" />
		<title level="m">Googleclusterdata -traces of google workloads</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="http://www.gartner.com/it/page.jsp?id=1442113" />
		<title level="m">Technology research -Gartner Inc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thermal aware server provisioning and workload distribution for Internet data centers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varsamopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Symposium on High Performance Distributed Computing (HPDC)</title>
		<meeting>the ACM International Symposium on High Performance Distributed Computing (HPDC)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic thermal management of air cooled data centers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intersociety Conference on the Thermal and Thermomechanical Phenomena in Electronics Systems (ITHERM)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<title level="m">Time Series Analysis, Forecasting, and Control</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Energy-aware server provisioning and load dispatching for connection-intensive Internet services</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Design, implementation, and performance of an extensible toolkit for resource prediction in distributed systems</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dinda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2006-02">February 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Power provisioning for a warehouse-sized computer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual international symposium on Computer architecture (ISCA)</title>
		<meeting>the annual international symposium on Computer architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust control-theoretic thermal balancing for server clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Parallel Distributed Processing (IPDPS)</title>
		<imprint>
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Managing cost, performance, and reliability tradeoffs for energy-aware server provisioning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VMware distributed resource management: Design, implementation, and lessons learned</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shanmuganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In VMware Technical Journal</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Power and performance management of virtualized computing environments via lookahead control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kusic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Autonomic Computing (ICAC)</title>
		<meeting>the International Conference on Autonomic Computing (ICAC)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cutting the electric bill for Internet-scale systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">No power struggles: Coordinated multi-level power management for the data center</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling and synthesizing task placement constraints in google compute clusters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chudnovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rifaat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Symposium on Cloud Computing</title>
		<meeting>ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cloudscale: Elastic resource scaling for multi-tenant cloud systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">pMapper: power and migration cost aware application placement in virtualized systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IFIP/USENIX Middleware</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Server workload analysis for power minimization using consolidation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on USENIX Annual technical conference</title>
		<meeting>the conference on USENIX Annual technical conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Power-aware scheduling of virtual machines in DVFS-enabled clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Cluster Computing and Workshops</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cluster-level feedback power control for performance optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Characterizing task usage shapes in Google&apos;s compute clusters</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Large Scale Distributed Systems and Middleware (LADIS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
