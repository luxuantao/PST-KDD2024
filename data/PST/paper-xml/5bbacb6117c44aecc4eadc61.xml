<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A multi-verse optimizer approach for feature selection and optimizing SVM parameters based on a robust system architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hossam</forename><surname>Faris</surname></persName>
							<email>hossam.faris@ju.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="department">Business Information Technology Department</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">A</forename><surname>Hassonah</surname></persName>
							<email>mohammad.a.hassonah@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Business Information Technology Department</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Ala'</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Al-Zoubi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Business Information Technology Department</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seyedali</forename><surname>Mirjalili</surname></persName>
							<email>seyedali.mirjalili@griffithuni.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<postCode>4111</postCode>
									<settlement>Nathan, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ibrahim</forename><surname>Aljarah</surname></persName>
							<email>i.aljarah@ju.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="department">Business Information Technology Department</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">King Abdullah II School for Information Technology</orgName>
								<orgName type="institution">The University of Jordan</orgName>
								<address>
									<settlement>Amman</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Neural Comput &amp; Applic</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A multi-verse optimizer approach for feature selection and optimizing SVM parameters based on a robust system architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">960134145787002830C333A6BE9097DD</idno>
					<idno type="DOI">10.1007/s00521-016-2818-2</idno>
					<note type="submission">Received: 21 September 2016 / Accepted: 19 December 2016 Ó The Natural Computing Applications Forum 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Optimization</term>
					<term>SVM</term>
					<term>Support vector machines</term>
					<term>Multi-verse optimizer</term>
					<term>MVO</term>
					<term>Feature selection</term>
					<term>Metaheuristics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Support vector machine (SVM) is a well-regarded machine learning algorithm widely applied to classification tasks and regression problems. SVM was founded based on the statistical learning theory and structural risk minimization. Despite the high prediction rate of this technique in a wide range of real applications, the efficiency of SVM and its classification accuracy highly depends on the parameter setting as well as the subset feature selection. This work proposes a robust approach based on a recent nature-inspired metaheuristic called multi-verse optimizer (MVO) for selecting optimal features and optimizing the parameters of SVM simultaneously. In fact, the MVO algorithm is employed as a tuner to manipulate the main parameters of SVM and find the optimal set of features for this classifier. The proposed approach is implemented and tested on two different system architectures. MVO is benchmarked and compared with four classic and recent metaheuristic algorithms using ten binary and multi-class labeled datasets. Experimental results demonstrate that MVO can effectively reduce the number of features while maintaining a high prediction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Support vector machine (SVM) is a supervised machine learning model designed for analyzing the data and recognizing certain visible or hidden patterns. SVM can be used for either classification or regression analysis. SVM was first designed and proposed by Vladimir Vapnik <ref type="bibr" target="#b38">[39]</ref>. Besides linear classification, SVM can efficiently perform nonlinear classification by projecting the training dataset into a higher dimensional space so the categories of the training data are separated by a determined hyperplane. In the literature, SVM showed high prediction accuracy and modeling capability in wide range of real classification, pattern recognition and regression problems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>.</p><p>Different kernel functions were used and applied by researchers in the literature (e.g., linear, polynomial, or sigmoid). Radial Basis Function (RBF) kernel (also known as Gaussian function) is the most popular and highly recommended kernel. As reported in several studies, RBF kernel helps SVM to achieve an accurate prediction and reliable performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b48">49]</ref>. In addition, RBF can lead a better analysis of higher dimensional data and it has less parameters to optimize <ref type="bibr" target="#b12">[13]</ref>.</p><p>In order to get the advantage of SVM and achieve the best generalization ability with maximum prediction power, two important problems should be addressed. The first problem is the optimization of the error penalty parameter C of SVM and its kernel parameters. The second one is the selection of the best representative subset of features that will be used in the training process. Regarding the former problem, conventionally, SVM parameters are selected using an exhaustive grid search algorithm. However, this method suffers from a long running time due to the need for a huge number of possible evaluations <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50]</ref>. Therefore, there are researchers who proposed other efficient solutions for optimizing the SVM and the parameters of its kernel. A common type of these approaches is the metaheuristic algorithms. In the literature, Metaheuristic algorithms showed high efficiency in generating acceptable solutions when the problem is very complex and the search space is extremely large <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref>. Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) are very popular examples of the metaheuristic search algorithms. GA was designed by John Holland and inspired by the Darwinian theories of evolution and natural selection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. On the other hand, PSO is a swarm intelligent-based algorithm inspired by the movement of bird and fish flocks in nature <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Some examples that deployed GA, PSO and other natureinspired metaheuristic algorithms in optimizing SVM can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Feature selection is a process of choosing a set of M features from a data set of N features, M\N, so that the value of some evaluation function or criterion is optimized over the space of all possible feature subsets. The goal of the feature selection process is to eliminate the irrelevant features and consequently decreasing the training time and reducing the complexity of the developed classification models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. It was reported in several studies that feature selection occasionally leads to improvements in the predictive accuracy and enhancements in the comprehensibility and generalization of the developed model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. On the other side, selecting the best subset of features from all possible 2 N subsets is not trivial and turns to be NP-hard problem when the search space grows <ref type="bibr" target="#b1">[2]</ref>. SVM is not different from other data mining and machine learning techniques and its performance could be highly improved by applying the feature selection process.</p><p>Previous works proposed different techniques for optimizing SVM parameters and performing feature selection simultaneously. One of the first attempts was made by Huang and Wang <ref type="bibr" target="#b14">[15]</ref>, in which GA was applied to this problem. Based on comparisons with the conventional grid search algorithm using different datasets, they showed that GA is able to optimize SVM to reach better accuracy with a fewer number of features. A similar approach was followed by Lin et al. <ref type="bibr" target="#b23">[24]</ref> where the PSO algorithm was employed instead of GA. They compared their results with those obtained by Huang and Wang <ref type="bibr" target="#b14">[15]</ref>. Their results showed that PSO was very competitive compared to GA outperforming it in six datasets out of ten. Another work was conducted by Zhao et al. <ref type="bibr" target="#b50">[51]</ref> and they used GA with feature chromosome operation for subset feature selection and optimizing SVM parameters. In their work, almost a similar system architecture was experimented and compared to those in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper, we propose, experiment and discuss a robust approach based on the recent multi-verse optimizer (MVO) for feature selection and optimizing the parameters of SVM in order to maximize the accuracy of SVM. According to our knowledge, this is the first time to employ MVO for optimizing SVM. This work also considers the proposal of an improved architecture to improve the generalization power and robustness of the SVM. The proposed model is named as MVO?SVM. MVO is a metaheuristic search algorithm inspired by a number of cosmological theories including the Big Bang theory <ref type="bibr" target="#b27">[28]</ref>. MVO has shown high competency and efficiency when applied to challenging optimization problems such as training feedforward neural networks <ref type="bibr" target="#b6">[7]</ref>. The proposed MVO-based SVM is evaluated based on seven binary datasets and three multi-class datasets selected from the UCI machine learning repository. The MVO?SVM approach is designed and applied using two different system architectures. The first one is identical to those employed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>, while the second architecture is proposed in order to maximize the generalization ability of the model and therefore achieving more robust results. In addition, evaluation results of MVObased SVM are compared with those obtained for GA, PSO and two recent metaheuristic algorithms which are the Firefly Algorithm (FF) <ref type="bibr" target="#b44">[45]</ref> and Bat Algorithm (BAT) <ref type="bibr" target="#b45">[46]</ref>. In the past few years, BAT and FF were investigated for optimizing SVM in many studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48]</ref>. All our experiments are carried out using the two aforementioned system architectures.</p><p>This paper is structured as follows: Sect. 2 briefly describes the SVM algorithm. Section 3 presents the structure of the MVO algorithm. The proposed approach for feature selection and optimizing the parameters of SVM is provided and discussed in Sect. 4. The conducted experiments and the results are discussed and analyzed in Sect. 5. At last, the conclusion and future works are summarized in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Support vector machines</head><p>SVM is a mathematical model and a powerful universal approximator proposed and developed by Vapnik <ref type="bibr" target="#b37">[38]</ref>. SVM can be used for both classification and regression tasks. Recently, in an extensive comparative study <ref type="bibr" target="#b7">[8]</ref>, it was shown that SVM stands among the best classifiers implemented so far. Rather than the minimizing the empirical error like in Neural Networks, the theoretical foundations of SVM are derived from the structural risk minimization idea <ref type="bibr" target="#b38">[39]</ref>.</p><p>As in the majority of classifiers, SVM depends on the training process to build its model. By using the kernel trick, SVM transforms the training data by nonlinear mapping functions to a higher dimensional space where the data can be separated linearly, or to find the best hyperplanes (support vectors) with maximal normalized margin with respect to the data points. Therefore, the goal of the learning process of SVM is to look for the optimal linear hyperplanes in that dimension <ref type="bibr" target="#b9">[10]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> depicts an example of a binary class dataset separated by SVM optimal hyperplanes.</p><p>Suppose we have a dataset fx i ; y j g i¼1;...;n where the x i 2 R d represents the input features, d is number of features in the training dataset and y i 2 R is its corresponding actual output, the main target of the SVM algorithm is to draw the linear decision function given in Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_0">f ðxÞ ¼ hw; / i ðxÞi þ b<label>ð1Þ</label></formula><p>w and b a weight and a constant, respectively, which have to be estimated from the dataset. / is a nonlinear function which maps the input features to higher feature space. h:; :i indicates the dot product in R d . This problem can be represented to minimize the following function: </p><formula xml:id="formula_1">RðCÞ ¼ C n X n i¼1 L e ðf ðx i Þ; y i Þ þ 1 2 w k k 2 L e ðf ðx i Þ; y i Þ</formula><p>By adding the slack variables n i and n Ã i , the problem can be formulated to minimize Eq. (3) in subject to the constraints given in Eq. (4).</p><formula xml:id="formula_3">Rðw; n Ã i Þ ¼ 1 2 w k k 2 þC X n i¼1 ðn i þ n Ã i Þ ð<label>3Þ</label></formula><formula xml:id="formula_4">y i À hw; x i i À b e þ n i hw; x i i þ b À y i e þ n Ã i n i ; n Ã i ! 0 8 &gt; &lt; &gt; :<label>ð4Þ</label></formula><p>C represents a penalty for a prediction error that is greater than e. n i and n Ã i are slack variables that measure the error cost based on the training data.</p><p>This optimization problem with the specified constraints can be handled by means of Lagrangian multipliers as a quadratic optimization problem. The solution can be represented as given in Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_5">f ðxÞ ¼ X n i¼1 ða i À a Ã i ÞKðx i ; xÞ þ b<label>ð5Þ</label></formula><p>where a i and a Ã i are Lagrange multipliers which are subject to the following constraints:</p><formula xml:id="formula_6">X n i¼1 ða i À a Ã i Þ ¼ 0 0 a i C i ¼ 1; . . .; n 0 a Ã i C i ¼ 1; . . .; n K(.</formula><p>) is the kernel function and its value is an inner product of two vectors x i and x j in the feature space /ðx i Þ and /ðx j Þ. K(.) can be represented as shown in Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_7">Kðx i ; x j Þ ¼ /ðx i Þ:/ðx j Þ ð<label>6Þ</label></formula><p>The most popular and used kernel functions in the literature are the Polynomial, Hyperbolic Tangent Kernel and the RBF Kernel as given in Eqs. ( <ref type="formula" target="#formula_8">7</ref>), ( <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref>), respectively.</p><formula xml:id="formula_8">K p ðx i ; x j Þ ¼ hx i ; x j þ 1i d<label>ð7Þ</label></formula><formula xml:id="formula_9">K h ðx i ; x j Þ ¼ tan hðc 1 ðx i :x j Þ þ c 2 Þ ð<label>8Þ</label></formula><formula xml:id="formula_10">K rbf ðx i ; x j Þ ¼ expðÀcjjx j À x i jj 2 Þ; where c [ 0<label>ð9Þ</label></formula><p>One of the key issues here is that selection of kernel functions and the values of their parameters have a great impact on the accuracy of the SVM model. The values of SVM parameters have high influence of its performance. As we discussed in Sect. 1, there are different approaches for optimizing these parameters. Since the metaheuristic algorithms are very efficient to find optimal values for optimization problems, we employ MVO to do that for the first time in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-verse optimizer</head><p>The multi-verse optimizer <ref type="bibr" target="#b27">[28]</ref> is a recent evolutionary metaheuristic algorithm, which mimics the rules in one of the theories of multi-verse. The main inspiration of this algorithm comes from the theory of existence of multiple universes and their interactions via black, white, and worm holes. This algorithm is a population-based stochastic algorithm and approximates the global optimum for optimization problems with a collection of solutions.</p><p>In this algorithm, two parameters should be calculated first to update the solutions: Wormhole Existence Probability (WEP) and Traveling Distance Rate (TDR). Such parameters dictate how often and how much the solutions change during the optimization process and defined as follows:</p><formula xml:id="formula_11">WEP ¼ a þ t Â b À a T<label>ð10Þ</label></formula><p>where a is the minimum, b is the maximum, t is the current iteration, and T represents the maximum number of iterations allowed.</p><formula xml:id="formula_12">TDR ¼ 1 À t 1=p T 1=p<label>ð11Þ</label></formula><p>where p defines the exploitation accuracy.</p><p>The main parameter of TDR is p. The exploitation is emphasized proportional to the value of this parameter.</p><p>After calculating WEP and TDR, the position of solutions can be updated using the following equation:</p><formula xml:id="formula_13">x i j ¼ x j þ TDR þ ððub j À lb j Þ Ã r 4 þ lb j ÞÞ if r 3 \0:5 x j À TDR þ ððub j À lb j Þ Ã r 4 þ lb j ÞÞ if r 3 ! 0:5 if r 2 \WEP x j RouletteWheel if r 2 ! WEP 8 &lt; :<label>ð12Þ</label></formula><p>where X j is the jth element of the best individual, WEP, TDR are coefficients, lb i and ub i are the lower and upper bounds of the jth element, r 2 ; r 3 ; r 4 are randomly generated numbers drawn from the interval of [0, 1], x j i represents the jth parameter in ith individual, and x j RouletteWheel is the jth element of a solution picked by the roulette wheel selection mechanism.</p><p>This equation shows that the position of solution can be updated with respect to the current best individual obtained using the WEP. If the r 3 , which is a random number in [0, 1], less than 0.5, the solution is required to get the value of the jth dimension in the best solution. WEP is increased during optimization, so this is how the MVO increases the exploitation of the best solution obtained so far.</p><p>The exploration and local optima avoidance are guaranteed with the second part of the above equation, in which the jth variable in the solution i is replaced with that in a selected solution using a roulette wheel. When using the second part, the current solution is considered to have a black hole, and the one of the best solutions contain a white hole.</p><p>The white holes are chosen with a roulette wheel proportional to their fitness value. The black holes are created inversely proportional to the fitness value for minimization problems. This mechanism assists the MVO algorithm to improve the poor solutions using the best solutions over the course of iterations. Since the solutions exchange variables, there are sudden changes in the solutions and consequently improved exploration. If a solution stagnates in a local optimum, this approach is able to revolve it as well.</p><p>To balance between exploration and exploitation, WEP and TDR should be changed adaptively using Eqs. ( <ref type="formula" target="#formula_11">10</ref>) and <ref type="bibr" target="#b10">(11)</ref>. In this work we have used a ¼ 0:2, b ¼ 1, and p ¼ 6 in these equations. Figure <ref type="figure" target="#fig_1">2</ref> shows how the WEP and TDR change over the course of iterations.</p><p>The MVO algorithm first generates a set of random solutions and calculates their corresponding objectives. The position of solutions is repeatedly updated using Eq. ( <ref type="formula" target="#formula_13">12</ref>) until the satisfaction of an end condition. Meanwhile, the random parameter (r 2 ; r 3 ; r 4 ), WEP, and TDR are updated for each solution. It has been proved that this algorithm is able to provide very comparative and occasionally superior results compared to the current approaches. In this following section, this algorithm is integrated to the SVM for the first time. In this section, we describe three important points regarding the proposed implementation of MVO for feature selection and SVM parameters optimization. They are the encoding scheme used to represent MVO universes, the fitness function, and the system architectures followed in this work. The key points are described as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding scheme</head><p>In the two architectures investigated in this work, the individuals are encoded as a vector of real numbers. The number of elements in each vector equals number of features in the dataset plus two elements to represent SVM parameters: the Cost (C) and Gamma (c). The implemented encoding scheme is shown in Fig. <ref type="figure">3</ref>. Each element in the vector is a randomly generated number in the interval [0,1]. Therefor, the elements that represent features are rounded: if the element is larger or equal to 0.5, its value is rounded to 1 and the feature is selected, otherwise the value is rounded to 0 and the feature is not selected. For the C and c, those parameters need to be mapped to different scales since their search space is different. For example, the value of the element corresponding to C is mapped to the interval [0, 35000] while the element corresponding to c is mapped to [0, 32]. In our implementation, the values of C and c are linearly transformed using Eq. ( <ref type="formula" target="#formula_14">13</ref>).</p><formula xml:id="formula_14">B ¼ A À min A max A À min A ðmax B À min B Þ þ min B :<label>ð13Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fitness evaluation</head><p>In order to assess the generated universes (solutions), we rely on the confusion matrix shown in Fig. <ref type="figure">4</ref> which is considered as the primary source for evaluating classification models. Based on this confusion matrix, the classification accuracy rate is calculated as given in Eq. ( <ref type="formula" target="#formula_15">14</ref>):</p><formula xml:id="formula_15">Accuracy ¼ TP þ TN TP þ FN þ FP þ TN :<label>ð14Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System architectures</head><p>This subsection describes the main system architectures that are applied to perform feature selection and optimizing the parameters of SVM simultaneously using the MVO algorithm. The term ''system architecture'' was used in the previous studies to describe the processes that are carried out to perform this task and their sequence. In this work two different architectures are utilized. The first system architecture used and implemented in <ref type="bibr" target="#b14">[15]</ref>, while the second is a modified version of the first one. We will refer to the two architectures as ''Architecture I'' and ''Architecture II,'' respectively. In other words, we propose Architecture II as an approach to enhance the generalization ability and robustness of the developed model. To describe the workflow of these architectures, we provide the following bullet points and figures:</p><p>• Data normalization: this is a preprocessing step performed on the features of all datasets. The values of all features are mapped into same scale in order to eliminate the effect of some features that have different range values on the learning process of the algorithm. Therefore, all features are given an equal weight and normalized to fall in the interval [0,1] using a Eq. ( <ref type="formula" target="#formula_16">15</ref>), which is a special form of Eq. ( <ref type="formula" target="#formula_14">13</ref>).</p><formula xml:id="formula_16">B ¼ A À min A max A À min A<label>ð15Þ</label></formula><p>• Decoding of universes: the generated vectors (universes) by MVO are split into two parts: the first two elements of the vector correspond to the SVM parameters and they are converted using Eq. ( <ref type="formula" target="#formula_14">13</ref>). The rest of the elements, which correspond to the selected features, are rounded to form a binary vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>123</head><p>• Select feature subset: after decoding the universe as described earlier to a binary vector, the corresponding features are selected from the training dataset. • Fitness evaluation: every generated solution by MVO which represents the parameters of SVM and a set of selected features is assessed using the fitness function. • Termination criterion: the evolutionary cycle of MVO keeps until a termination condition is met. Here we set a maximum number of iterations as a termination condition. • Reproduction operators: this is a sequence of operators which are applied by MVO in order to evolve the generated universes searching for a better quality solution.</p><p>The main differences between those architectures are the fitness evaluation and the methodology for training and testing. In the objective function of Architecture I, the whole training dataset is deployed to build the SVM model, and then the returned fitness of the objective function is the evaluation result of the trained SVM based on the testing part. On the other side, in the objective function of Architecture II, the training part is split again into a number of smaller parts to perform k-folds cross-validation. So the SVM is trained k times and the average evaluation is returned. In the latter case, the testing part is not presented to the SVM during the iteration of the metaheuristic but used to assess the final selected subset of features and the best obtained parameters. Figures <ref type="figure" target="#fig_3">5</ref> and<ref type="figure" target="#fig_4">6</ref> clearly show the details and differences between the two architectures.</p><p>5 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments setup</head><p>The experiments in this work are conducted on a personal machine with Intel Core i7 processor, 2.40 GHz, 8 GB RAM, using Windows 10 as the operating system. We also used Matlab R2015a (8.5.0.197613) environment as an implementation for our experiment. The LIBSVM implementation is used for the SVM classifier <ref type="bibr" target="#b2">[3]</ref>. The proposed MVO-SVM approach is tested and evaluated based on ten datasets drawn from the UCI repository<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b22">[23]</ref>. Seven of the datasets belong to binary class labeled: Heart, Ionosphere, German, Sonar, Breast cancer, Parkinsons and Spectf; while Vowel, Wine and Vehicle are multiclass labeled. Number of features and instances in each dataset is given in Table <ref type="table" target="#tab_1">1</ref>.</p><p>The initial parameters of MVO, GA, PSO, BAT and FF algorithms are set as listen in Table <ref type="table" target="#tab_2">2</ref>. The number of universes, individuals and swarm size is identical in all algorithms and set to 30. Generally speaking, there should be special considerations when setting the maximum iterations of the algorithm. This is because that the type of the incorporated feature selection method is a wrapper one. Although wrapper-based feature selection methods are powerful due to the interaction between the selection of the features and the classifier wrapped within the search method, they have the disadvantage of being computationally expensive and they can overfit <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>. Therefore, empirically, it was found that running the algorithm up to a small number of iterations (e.g., 50) can reduce the computation time of the metaheuristic algorithms, and they can converge to a solution.</p><p>As mentioned earlier, system Architecture I and II differ in the training/testing methodology implemented in each one. In Architecture I, the cross-validation is set to 10. This means that SVM is trained 10 times where in each time SVM is trained using different 9-folds, and then, the fitness function returns a fitness value based on the 10th testing fold. In Architecture II, however, the outer cross-validation is set to 10-folds and the inner one is set to 3-folds. The experiments are repeated 10 times to get statistically meaningful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of optimizing SVM with feature selection</head><p>In this part of the experiments, MVO is evaluated and compared to GA, PSO, BAT and FF in performing feature selection and optimizing the parameters of SVM. The five algorithms are evaluated using the ten datasets described earlier. The average of the accuracy rate and the average number of selected features along with the standard deviation are listed in Tables <ref type="table">3</ref> and<ref type="table">4</ref> for Architecture I and Architecture II, respectively. We use avg AE std deviation to represent these values. The results of Architecture I show that MVO achieved the highest average accuracy rates compared to GA, PSO, BAT and FF in 8 out 10 datasets exceeding 99% accuracy rate in six of them. Also, MVO shows lower standard deviation values for most of the datasets. It can also be noticed that the three optimizers have very close results regarding the number of selected features. Figure <ref type="figure" target="#fig_5">7</ref> shows the convergence curves of all optimizers based on the averages of accuracy rates for the 10 runs. In this figure MVO clearly shows its higher convergence speed in 8 out the 10 datasets. The best results and parameters obtained based on Architecture I are listed in Table <ref type="table" target="#tab_3">5</ref>. It can be seen that MVO and GA achieved 100% accuracy  Inspecting the results of Architecture II in Table <ref type="table">4</ref>, it may be observed that the obtained accuracy rates by all optimizers are lower than those for Architecture I. This decrease is due to the incorporated training/testing scheme in this architecture. As it was mentioned before, the testing part in this architecture was not represented or seen during the optimization process. Therefore, the results are expected to be lower but more credible. According to the results obtained by Architecture II in Table <ref type="table">4</ref>, the MVO algorithm shows higher average accuracy than the other optimizers in all datasets except in the German and Spectf datasets in which it is ranked second after GA. Moreover,  In this experiment, we compare MVO with the gird search for optimizing the parameters of SVM. To make the comparison fair, MVO is applied just for parameters optimization without the feature selection part since the grid search does not have this capability. Both techniques were applied with a 10-folds cross-validation. Grid search is used as described in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. Table <ref type="table">8</ref> shows the results of comparison based on the aforementioned Arch I and Arch II. The results from Arch I show that MVO is noticeably better than the grid search in 9 datasets. For the Wine dataset, MVO is slightly better while German dataset was the only one that shows better performance for grid search. With checking the average classification accuracies obtained by Arch II, it can been seen that the accuracy rates obtained by MVO are higher than those for the grid search even in the German dataset. The only exception is for Spectf and Breast cancer datasets.</p><p>Comparing the results of Arch II to Arch I, two facts can be observed. First, the accuracy rates decreased for MVO and grid search-based SVM models. As mentioned the previous section, this is expected due to the adopted testing strategy. Second, the difference between the accuracy of MVO and the grid search has increased. This can be an evidence that MVO is more robust than the grid search in optimizing SVM when new and unseen data are presented to the model. Overall, the results and findings of this section show the merits of the MVO algorithm in improving the performance of SVM. The more accurate results of MVO-based SVM are due to the high exploitation of MVO. In order for SVM to be very accurate, the parameters should be tuned accurately as well. This paper showed that the MVO can be very efficient, which is due the bold role of the best individual obtained using the worm holes in improving the quality of other solutions. The reliability and robustness of the MVO-based SVM originate from the high exploration and local optima avoidance of the MVO algorithm. The sudden changes in the solutions using white/black holes emphasize the exploration process and help in resolving the local optima stagnation. Moreover, the WEP and TDR parameters assist MVO to first explore the search space broadly and, then, exploit the promising regions accurately over the course of iterations. This dynamic control of the exploration and exploitation processes enabled MVO to achieve better results over the other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this research work we proposed the application of a recent nature-inspired metaheuristic called multi-verse optimizer for feature selection and optimizing the parameters of SVM simultaneously. Two system architectures were implemented for the proposed approach: the first architecture is commonly used in the literature while the second is proposed in this work to increase the credibility of the SVM prediction results. The developed approach is assessed and benchmarked with four well-regarded metaheuristic algorithms (GA, PSO, BAT and Firefly) and the grid search. Experiments show that MVO was able to optimize SVM achieving the highest accuracy compared with the other optimizers based on the two investigated architectures. The findings and analysis of this work proved the merits of the MVO algorithm in improving the performance of SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with ethical standards</head><p>Conflict of interest The authors declare that there is no conflict of interest regarding the publication of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Optimal hyperplane in support vector machine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 WEP and TDR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Fig. 3 Encoding scheme of individuals for SVM optimization and feature selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 System architecture I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 System architecture II</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Convergence curves of MVO, GA, PSO, BAT and FF in optimizing SVM and feature selection based on Architecture I. a Breast cancer. b Heart. c Ionosphere. d Sonar. e Vehicle. f Vowel. g Wine. h German. i Parkinsons. j Spectf</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is known as eÀintensive loss function which can be represented as shown in Eq. (2):</figDesc><table><row><cell>L e ðf ðxÞ; yÞ ¼</cell><cell>jf ðxÞ À yj À e jf ðxÞ À yj ! e 0 otherwise</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>List of datasets</figDesc><table><row><cell>Dataset</cell><cell>Features</cell><cell>Instances</cell><cell>Classes</cell></row><row><cell>Heart</cell><cell>13</cell><cell>270</cell><cell>2</cell></row><row><cell>Ionosphere</cell><cell>34</cell><cell>351</cell><cell>2</cell></row><row><cell>Sonar</cell><cell>60</cell><cell>208</cell><cell>2</cell></row><row><cell>German</cell><cell>24</cell><cell>1000</cell><cell>2</cell></row><row><cell>Vowel</cell><cell>10</cell><cell>528</cell><cell>11</cell></row><row><cell>Wine</cell><cell>13</cell><cell>178</cell><cell>3</cell></row><row><cell>Vehicle</cell><cell>18</cell><cell>846</cell><cell>4</cell></row><row><cell>Breast Cancer</cell><cell>10</cell><cell>683</cell><cell>2</cell></row><row><cell>Parkinsons</cell><cell>22</cell><cell>195</cell><cell>2</cell></row><row><cell>Spectf</cell><cell>44</cell><cell>276</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Initial parameters of the MVO, GA, PSO, BAT and FF</figDesc><table><row><cell>Algorithm</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>MVO</cell><cell>Min wormhole existence ratio</cell><cell>0.2</cell></row><row><cell></cell><cell>Max wormhole existence ratio</cell><cell>1</cell></row><row><cell></cell><cell>Universes</cell><cell>30</cell></row><row><cell></cell><cell>Iterations</cell><cell>50</cell></row><row><cell>GA</cell><cell>Crossover ratio</cell><cell>0.9</cell></row><row><cell></cell><cell>Mutation ratio</cell><cell>0.1</cell></row><row><cell></cell><cell>Selection mechanism</cell><cell>Roulette wheel</cell></row><row><cell></cell><cell>Population size</cell><cell>30</cell></row><row><cell></cell><cell>Generations</cell><cell>50</cell></row><row><cell>PSO</cell><cell>Acceleration constants</cell><cell>[2.1, 2.1]</cell></row><row><cell></cell><cell>Inertia w</cell><cell>[0.9, 0.6]</cell></row><row><cell></cell><cell>Number of particles</cell><cell>30</cell></row><row><cell></cell><cell>Generations</cell><cell>50</cell></row><row><cell>BAT</cell><cell>Loudness</cell><cell>0.5</cell></row><row><cell></cell><cell>Pulse rate</cell><cell>0.5</cell></row><row><cell></cell><cell>Frequency minimum</cell><cell>0</cell></row><row><cell></cell><cell>Frequency maximum</cell><cell>1</cell></row><row><cell>FF</cell><cell>Alpha</cell><cell>0.2</cell></row><row><cell></cell><cell>Beta</cell><cell>1</cell></row><row><cell></cell><cell>Gamma</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Best obtained results based on Architecture I Sonar, Vowel and Wine. Figure8shows the convergence curves of all optimizers based on Architecture II. In this figure, MVO shows a higher convergence speed in most of the datasets. The best results and parameters obtained based on Architecture II are presented in Table6. It is shown that only MVO and PSO achieved 100% accuracy rate in 5 out of 10 datasets while BAT, GA and FF comes next with 4, 3, 2 datasets, respectively.In order to verify the significance of the differences between MVO results and the other optimizers, the nonparametric statistical test Wilcoxon's rank-sum test is Convergence curves of MVO, GA, PSO, BAT and FF in optimizing SVM and feature selection based on Architecture II. a Breast cancer. b Heart. c Ionosphere. d Sonar. e Vehicle. f Vowel. g Wine. h German. i Parkinsons. j Spectf</figDesc><table><row><cell>Dataset</cell><cell>MVO?SVM</cell><cell>GA?SVM</cell><cell>PSO?SVM</cell><cell>BAT?SVM</cell><cell>FF?SVM</cell></row><row><cell>Heart</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>96.3</cell><cell>100</cell><cell>96.3</cell></row><row><cell>No. of selected features</cell><cell>8</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>5</cell></row><row><cell>Cost (C)</cell><cell>31941.4</cell><cell>28267.28</cell><cell>18562.92</cell><cell>16395.42</cell><cell>16265.8</cell></row><row><cell>c</cell><cell>8.43</cell><cell>0.0001</cell><cell>27.01</cell><cell>19.43</cell><cell>12.63</cell></row><row><cell>Ionosphere</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>17</cell><cell>12</cell><cell>14</cell><cell>15</cell><cell>13</cell></row><row><cell>Cost (C)</cell><cell>7487.26</cell><cell>1637.64</cell><cell>12386.94</cell><cell>34665.56</cell><cell>22565.59</cell></row><row><cell>c</cell><cell>0.81</cell><cell>1.53</cell><cell>0.87</cell><cell>2.18</cell><cell>0.0001</cell></row><row><cell>Sonar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>22</cell><cell>16</cell><cell>30</cell><cell>24</cell><cell>20</cell></row><row><cell>Cost (C)</cell><cell>10711.62</cell><cell>35000</cell><cell>10102.66</cell><cell>35000</cell><cell>12168.71</cell></row><row><cell>c</cell><cell>1.44</cell><cell>0.9</cell><cell>0.87</cell><cell>0.0001</cell><cell>0.21</cell></row><row><cell>German</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>85</cell><cell>89</cell><cell>84</cell><cell>84</cell><cell>83</cell></row><row><cell>No. of selected features</cell><cell>15</cell><cell>12</cell><cell>11</cell><cell>15</cell><cell>13</cell></row><row><cell>Cost (C)</cell><cell>32517.1</cell><cell>10044.12</cell><cell>32700.54</cell><cell>35000</cell><cell>19536.01</cell></row><row><cell>c</cell><cell>0</cell><cell>0.0001</cell><cell>15.76</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>Vowel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>5</cell><cell>6</cell><cell>5</cell><cell>5</cell><cell>6</cell></row><row><cell>Cost (C)</cell><cell>9470.25</cell><cell>8710.15</cell><cell>1322.08</cell><cell>34526.26</cell><cell>10891.24</cell></row><row><cell>c</cell><cell>12.06</cell><cell>13.8</cell><cell>16</cell><cell>23.16</cell><cell>4.17</cell></row><row><cell>Wine</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>5</cell><cell>4</cell></row><row><cell>Cost (C)</cell><cell>5149.58</cell><cell>13789.51</cell><cell>32715.96</cell><cell>35000</cell><cell>14755.23</cell></row><row><cell>c</cell><cell>1.48</cell><cell>5.15</cell><cell>3.31</cell><cell>0.0001</cell><cell>7.76</cell></row><row><cell>Vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>91.76</cell><cell>92.86</cell><cell>92.94</cell><cell>94.05</cell><cell>92.86</cell></row><row><cell>No. of selected features</cell><cell>11</cell><cell>12</cell><cell>10</cell><cell>12</cell><cell>13</cell></row><row><cell>Cost (C)</cell><cell>26775.73</cell><cell>19883.93</cell><cell>7429.6</cell><cell>989.31</cell><cell>11950.47</cell></row><row><cell>c</cell><cell>0.39</cell><cell>0.0001</cell><cell>4.17</cell><cell>0.09</cell><cell>0.58</cell></row><row><cell>Breast cancer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>3</cell></row><row><cell>Cost (C)</cell><cell>30124.21</cell><cell>9562.28</cell><cell>34005.7</cell><cell>35000</cell><cell>26687.13</cell></row><row><cell>c</cell><cell>24.29</cell><cell>18.56</cell><cell>3.62</cell><cell>0.0001</cell><cell>17.89</cell></row><row><cell>Parkinsons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>7</cell><cell>7</cell><cell>10</cell><cell>8</cell><cell>8</cell></row><row><cell>Cost (C)</cell><cell>4691.48</cell><cell>4071.45</cell><cell>21470.18</cell><cell>2635.98</cell><cell>4275.59</cell></row><row><cell>c</cell><cell>16.17</cell><cell>2.14</cell><cell>10.15</cell><cell>8.24</cell><cell>16.16</cell></row><row><cell>Spectf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>96.3</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>17</cell><cell>14</cell><cell>18</cell><cell>20</cell><cell>16</cell></row><row><cell>Cost (C)</cell><cell>2634.96</cell><cell>8045.49</cell><cell>27837.72</cell><cell>3342.66</cell><cell>16232.75</cell></row><row><cell>c</cell><cell>6.66</cell><cell>16.08</cell><cell>28.52</cell><cell>13.82</cell><cell>13.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Best obtained results based on Architecture II</figDesc><table><row><cell>Dataset</cell><cell>MVO?SVM</cell><cell>GA?SVM</cell><cell>PSO?SVM</cell><cell>BAT?SVM</cell><cell>FF?SVM</cell></row><row><cell>Heart</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>88.89</cell><cell>88.89</cell><cell>88.89</cell><cell>88.89</cell><cell>92.59</cell></row><row><cell>No. of selected features</cell><cell>3</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>9</cell></row><row><cell>Cost (C)</cell><cell>8090.17</cell><cell>16488.46</cell><cell>28811.03</cell><cell>34962.8</cell><cell>28465.52</cell></row><row><cell>c</cell><cell>9.5</cell><cell>0.33</cell><cell>0.0001</cell><cell>0.0002</cell><cell>0.0002</cell></row><row><cell>Ionosphere</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>97.14</cell><cell>97.14</cell><cell>100</cell><cell>97.14</cell></row><row><cell>No. of selected features</cell><cell>19</cell><cell>17</cell><cell>16</cell><cell>15</cell><cell>12</cell></row><row><cell>Cost (C)</cell><cell>19636.16</cell><cell>6596.45</cell><cell>33320.6</cell><cell>28035.14</cell><cell>22814.34</cell></row><row><cell>c</cell><cell>1.54</cell><cell>0.64</cell><cell>1.63</cell><cell>1.44</cell><cell>1.92</cell></row><row><cell>Sonar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>95.24</cell><cell>100</cell><cell>95.24</cell><cell>95.24</cell></row><row><cell>No. of selected features</cell><cell>28</cell><cell>32</cell><cell>34</cell><cell>26</cell><cell>26</cell></row><row><cell>Cost (C)</cell><cell>23023.17</cell><cell>35000</cell><cell>29043.67</cell><cell>31331.58</cell><cell>19709.92</cell></row><row><cell>c</cell><cell>0.38</cell><cell>0.25</cell><cell>0.19</cell><cell>0.53</cell><cell>0.64</cell></row><row><cell>German</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>78</cell><cell>82</cell><cell>82</cell><cell>84</cell><cell>79</cell></row><row><cell>No. of selected features</cell><cell>16</cell><cell>15</cell><cell>15</cell><cell>11</cell><cell>10</cell></row><row><cell>Cost (C)</cell><cell>18981.34</cell><cell>6585.16</cell><cell>35000</cell><cell>34948.39</cell><cell>13712.41</cell></row><row><cell>c</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0007</cell><cell>15.94</cell></row><row><cell>Vowel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>Cost (C)</cell><cell>2567.54</cell><cell>3086.54</cell><cell>35000</cell><cell>854.76</cell><cell>14842.37</cell></row><row><cell>c</cell><cell>2.59</cell><cell>6.16</cell><cell>4.32</cell><cell>2.64</cell><cell>8</cell></row><row><cell>Wine</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>No. of selected features</cell><cell>5</cell><cell>7</cell><cell>7</cell><cell>8</cell><cell>6</cell></row><row><cell>Cost (C)</cell><cell>17425.11</cell><cell>11212.74</cell><cell>4106.05</cell><cell>870.14</cell><cell>16346.93</cell></row><row><cell>c</cell><cell>3.57</cell><cell>1.34</cell><cell>1.23</cell><cell>3.09</cell><cell>2.62</cell></row><row><cell>Vehicle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>88.1</cell><cell>83.53</cell><cell>87.06</cell><cell>88.1</cell><cell>87.06</cell></row><row><cell>No. of selected features</cell><cell>13</cell><cell>13</cell><cell>11</cell><cell>11</cell><cell>13</cell></row><row><cell>Cost (C)</cell><cell>16452.13</cell><cell>29894.9</cell><cell>4262.44</cell><cell>6808.75</cell><cell>12268.22</cell></row><row><cell>c</cell><cell>0.11</cell><cell>0.28</cell><cell>1.77</cell><cell>0.09</cell><cell>0.11</cell></row><row><cell>Breast cancer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>98.55</cell><cell>98.55</cell><cell>100</cell><cell>98.53</cell><cell>98.53</cell></row><row><cell>No. of selected features</cell><cell>8</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>6</cell></row><row><cell>Cost (C)</cell><cell>34011.26</cell><cell>925.34</cell><cell>10457.39</cell><cell>34285.81</cell><cell>15715.01</cell></row><row><cell>c</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.0001</cell><cell>21.96</cell><cell>0.005</cell></row><row><cell>Parkinsons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>94.74</cell></row><row><cell>No. of selected features</cell><cell>10</cell><cell>7</cell><cell>12</cell><cell>11</cell><cell>10</cell></row><row><cell>Cost (C)</cell><cell>7936.23</cell><cell>23834.89</cell><cell>14260.76</cell><cell>15299.07</cell><cell>10308.68</cell></row><row><cell>c</cell><cell>9.11</cell><cell>11.91</cell><cell>5.22</cell><cell>3.09</cell><cell>4.12</cell></row><row><cell>Spectf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Best accuracy</cell><cell>92.59</cell><cell>96.15</cell><cell>88.46</cell><cell>92.59</cell><cell>85.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 P</head><label>7</label><figDesc>values of the Wilcoxon test of MVO classification results versus other algorithms (p ! 0.05 are italized)</figDesc><table><row><cell cols="2">Neural Comput &amp; Applic</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Table 6 continued</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">MVO?SVM</cell><cell>GA?SVM</cell><cell>PSO?SVM</cell><cell>BAT?SVM</cell><cell>FF?SVM</cell></row><row><cell cols="2">No. of selected features</cell><cell>21</cell><cell></cell><cell>27</cell><cell>21</cell><cell>20</cell><cell>14</cell></row><row><cell>Cost (C)</cell><cell></cell><cell>35000</cell><cell></cell><cell>9511.23</cell><cell>1634.7</cell><cell>6281.27</cell><cell>13492.21</cell></row><row><cell>c</cell><cell></cell><cell>4.65</cell><cell></cell><cell>7.86</cell><cell>15.61</cell><cell>12</cell><cell>27.96</cell></row><row><cell></cell><cell>GA</cell><cell>PSO</cell><cell>BAT</cell><cell>FF</cell><cell></cell></row><row><cell>Heart</cell><cell>0.0780</cell><cell>0.0038</cell><cell>8.66e-10</cell><cell>8.21e-08</cell><cell></cell></row><row><cell>Ionosphere</cell><cell>5.03e-05</cell><cell>0.0011</cell><cell>2.29e-05</cell><cell>0.1671</cell><cell></cell></row><row><cell>Sonar</cell><cell>8.27e-07</cell><cell>0.0192</cell><cell>0.0135</cell><cell>1.60e-05</cell><cell></cell></row><row><cell>German</cell><cell>9.39e-06</cell><cell>0.6211</cell><cell>0.5390</cell><cell>5.55e-21</cell><cell></cell></row><row><cell>Vowel</cell><cell>0.0101</cell><cell>6.38e-10</cell><cell>7.94e-04</cell><cell>0.4848</cell><cell></cell></row><row><cell>Wine</cell><cell>1.38e-08</cell><cell>0.0575</cell><cell>1.31e-08</cell><cell>0.0061</cell><cell></cell></row><row><cell>Vehicle</cell><cell>0.0013</cell><cell>0.9031</cell><cell>0.0199</cell><cell>9.45e-07</cell><cell></cell></row><row><cell>Breast cancer</cell><cell>0.0760</cell><cell>0.5367</cell><cell>0.7092</cell><cell>6.52e-06</cell><cell></cell></row><row><cell>Parkinsons</cell><cell>1.00</cell><cell>4.82e-05</cell><cell>0.0170</cell><cell>0.2080</cell><cell></cell></row><row><cell>Spectf</cell><cell>0.2702</cell><cell>0.9032</cell><cell>1.00</cell><cell>0.5390</cell><cell></cell></row><row><cell cols="2">Table 8 Comparison between MVO and Grid search in</cell><cell></cell><cell></cell><cell>Arch I</cell><cell></cell><cell>Arch II</cell></row><row><cell cols="2">optimizing SVM parameters</cell><cell></cell><cell></cell><cell>MVO</cell><cell>Grid</cell><cell>MVO</cell><cell>Grid</cell></row><row><cell></cell><cell></cell><cell>Heart</cell><cell></cell><cell>87.41 ± 5.02</cell><cell>86.3 ± 4.98</cell><cell>82.96 ± 4.44</cell><cell>80.74 ± 2.22</cell></row><row><cell></cell><cell></cell><cell cols="2">Ionosphere</cell><cell>96.86 ± 3.25</cell><cell>96.3 ± 1.81</cell><cell>94.03 ± 5.89</cell><cell>92.87 ± 3.21</cell></row><row><cell></cell><cell></cell><cell>Sonar</cell><cell></cell><cell>93.69 ± 6.17</cell><cell>89.88 ± 7.25</cell><cell>87.02 ± 5.73</cell><cell>86 ± 5.64</cell></row><row><cell></cell><cell></cell><cell>German</cell><cell></cell><cell>75.4 ± 5.99</cell><cell>78.4 ± 3.58</cell><cell>75.8 ± 5.25</cell><cell>75.2 ± 4.45</cell></row><row><cell></cell><cell></cell><cell>Vowel</cell><cell></cell><cell>99.81 ± 0.57</cell><cell>91.3 ± 4.33</cell><cell>99.24 ± 0.93</cell><cell>89.97 ± 4.3</cell></row><row><cell></cell><cell></cell><cell>Wine</cell><cell></cell><cell>99.44 ± 1.67</cell><cell>99.41 ± 1.76</cell><cell>97.75 ± 3.72</cell><cell>97.19 ± 2.81</cell></row><row><cell></cell><cell></cell><cell>Vehicle</cell><cell></cell><cell>88.42 ± 2.74</cell><cell>77.79 ± 3.03</cell><cell>84.99 ± 4.12</cell><cell>75.18 ± 3.25</cell></row><row><cell></cell><cell></cell><cell cols="2">Breast cancer</cell><cell>98.1 ± 1.96</cell><cell>97.65 ± 1.89</cell><cell>96.63 ± 2.38</cell><cell>96.63 ± 1.61</cell></row><row><cell></cell><cell></cell><cell cols="2">Parkinsons</cell><cell>97.42 ± 3.49</cell><cell>88.63 ± 5.76</cell><cell>94.89 ± 4.65</cell><cell>86.68 ± 8.71</cell></row><row><cell></cell><cell></cell><cell>Spectf</cell><cell></cell><cell>84.72 ± 7.41</cell><cell>79.8 ± 7.19</cell><cell>77.88 ± 5.94</cell><cell>77.91 ± 6.15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://archive.ics.uci.edu/ml/.Neural Comput &amp; Applic</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A nested heuristic for parameter tuning in support vector machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Carrizosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martı ´n-Barraga ´n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Oper Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="328" to="334" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on feature selection methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sahin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Electr Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Intell Syst Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The construction of support vector machine classifier using the firefly algorithm</title>
		<author>
			<persName><forename type="first">C-F</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-H</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Intell Neurosci</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell Data Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anomaly intrusions detection based on support vector machines with an improved bat algorithm</title>
		<author>
			<persName><forename type="first">A-C</forename><surname>Enache</surname></persName>
		</author>
		<author>
			<persName><surname>Sga ˆrciu V</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 20th international conference on control systems and computer science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="317" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training feedforward neural networks using multi-verse optimizer for binary classification problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferna ´ndez-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><forename type="middle">J</forename></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Am</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="72" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName><forename type="first">C-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/*cjlin/papers/guide/guide.pdf" />
		<imprint>
			<date type="published" when="2003-08-01">2003. 1 Aug 2016</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, National Taiwan University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parameter selection of support vector regression based on particle swarm optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X-H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on granular computing</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="834" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A GA-based feature selection and parameters optimizationfor support vector machines</title>
		<author>
			<persName><forename type="first">C-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new hybrid modified firefly algorithm and support vector regression model for accurate short term load forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kavousi-Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marzbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="6047" to="6056" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The particle swarm: social adaptation of knowledge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 international conference on evolutionary computation</title>
		<meeting>the 1997 international conference on evolutionary computation<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Service Center</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The behavior of particles. Evolutionary programming VII</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="581" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, IEEE international conference on</title>
		<meeting>IEEE international conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Control system optimization using genetic algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krishnakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Guid Control Dyn</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="735" to="740" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting motor vehicle crashes using support vector machine models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accid Anal Prev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1611" to="1618" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013-08-01">2013. 1 Aug 2016</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Irvine, School of Information and Computer Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for parameter determination and feature selection of support vector machines</title>
		<author>
			<persName><forename type="first">S-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z-J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1817" to="1824" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature extraction, construction and selection: a data mining perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Kluwer, Norwell</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Neural Comput &amp; Applic</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An intelligent system for automated breast cancer diagnosis and prognosis using svm based classifiers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zafiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anagnostopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-verse optimizer: a nature-inspired algorithm for global optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamlou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="513" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A SVM method trained by improved particle swarm optimization for image classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Echo state network with SVM-readout for customer churn prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A support vector machine approach for churn prediction in telecom industry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alsakran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Al-Kadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf-In Interdiscip J</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3961" to="3970" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional reservoir networks trained using SVM? privileged information for manufacturing process modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Sheta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00500-016-2232-9</idno>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Larran ˜aga P (2007) A review of feature selection techniques in bioinformatics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Inza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Least squares twin parametric-margin support vector machine for classification</title>
		<author>
			<persName><forename type="first">Y-H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N-Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="464" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comparison between regression, artificial neural networks and support vector machines for predicting stock market index</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Sheta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sem</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An application of support vector machines in bankruptcy prediction model</title>
		<author>
			<persName><forename type="first">K-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="135" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parameter selection for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Staelin</surname></persName>
		</author>
		<idno>HPL-2002-354R1</idno>
	</analytic>
	<monogr>
		<title level="j">Hewlett-Packard Company</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">G-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deb</forename><forename type="middle">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<title level="m">Monarch butterfly optimization. Neural computing and applications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stud krill herd algorithm</title>
		<author>
			<persName><forename type="first">G-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="363" to="370" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hybrid krill herd algorithm with differential evolution for global numerical optimization</title>
		<author>
			<persName><forename type="first">G-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-S</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="308" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hybridizing harmony search algorithm with cuckoo search for global numerical optimization</title>
		<author>
			<persName><forename type="first">G-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hce</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="285" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Earthworm optimization algorithm: a bio-inspired metaheuristic algorithm for global optimization problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lds</forename><surname>Coelho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Bio-Inspired Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Firefly algorithm, stochastic test functions and design optimisation</title>
		<author>
			<persName><forename type="first">X-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Bio-Inspired Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A new metaheuristic bat-inspired algorithm</title>
		<author>
			<persName><forename type="first">X-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nature inspired cooperative strategies for optimization</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<author>
			<persName><forename type="first">X-S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, chapter Accelerated Particle Swarm Optimization and Support Vector Machine for Business Optimization and Applications</title>
		<meeting>chapter Accelerated Particle Swarm Optimization and Support Vector Machine for Business Optimization and Applications<address><addrLine>Macau, China; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-07-11">2011. 2011. July 11-13, 2011</date>
			<biblScope unit="page" from="53" to="66" />
		</imprint>
	</monogr>
	<note>Networked digital technologies: third international conference</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Texture image classification based on support vector machine and bat algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent data acquisition and advanced computing systems: technology and applications (IDAACS), 2015 IEEE 8th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">EC-SVM approach for real-time hydrologic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xinying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Babovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Hydroinform</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An ACO-based algorithm for parameter optimization of support vector machines</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6618" to="6628" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature selection and parameter optimization for support vector machines: a new approach based on genetic algorithm with feature chromosomes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5197" to="5204" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling No x emissions from coal-fired utility boilers using support vector regression with ant colony optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Cen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="158" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neural Comput &amp; Applic</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
