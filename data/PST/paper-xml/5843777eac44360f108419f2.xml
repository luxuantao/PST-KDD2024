<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Web User Profiling using Data Redundancy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>zhangjing12@mails.tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Web User Profiling using Data Redundancy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The study of Web user profiling can be traced back to 30 years ago, with the goal of extracting "semantic"-based user profile attributes from the unstructured Web. Despite slight differences, the general method is to first identify relevant pages of a specific user and then use machine learning models (e.g., CRFs) to extract the profile attributes from the page. However, with the rapid growth of the Web volume, such a method suffers from data redundancy and error propagation between the two steps. In this paper, we revisit the problem of Web user profiling in the big data era, trying to deal with the new challenges. We propose a simple but very effective approach for extracting user profile attributes from the Web using big data. To avoid error propagation, the approach processes all the extraction subtasks in one unified model. To further incorporate human knowledge to improve the extraction performance, we propose a markov logic factor graph (MagicFG) model. The MagicFG model describes human knowledge as first-order logics and combines the logics into the extraction model. Our experiments on a real data set show that the proposed method significantly improves (+4-6%; p 0.01, t-test) the extraction performance in comparison with several baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Web user profiling, also referred to as user profile extraction and user profile mining, has long been viewed as an important and challenging problem in Web mining and natural language processing. Related studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> can be traced back to 30 years ago. The general task of web user profiling is to extract "semantic"-based user profile attributes (e.g., contact information, educational history, experience, and biography) from the unstructured Web. Web user profiling can be applied in many applications, and is becoming necessary in most social-related systems. With a large and highquality profile database, we could easily identify what kind of information we should recommend to a specific user. In e-commerce, one could also leverage the profile information to locate targeted customers for a new product. There are also several products such as, Email Breaker<ref type="foot" target="#foot_0">1</ref> , Email Hunter<ref type="foot" target="#foot_1">2</ref> , and Sidekick<ref type="foot" target="#foot_2">3</ref> offering services to help users find email addresses of target people from the Web.</p><p>Despite much research conducted in this field to automate the process of profile extraction, the problem remains largely unsolved <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>. To generate the profile for a specific user, a usual approach is to first find relevant Web pages of this user from the Web, and then use machine learning models (e.g., CRFs) to extract profile attributes from the pages. State-ofthe-art accuracy achieved in the two stages are around 90.0%, respectively. For example, F1 score is reported as 92% for the task of homepage finding conducted by Tang et al. <ref type="bibr" target="#b8">[9]</ref>, and when 87% for extracting profile attributes from the homepage <ref type="bibr" target="#b4">[5]</ref>. From independent views, the performance sounds good. However, taking error propagation into consideration, the overall accuracy of the approach that combines the two stages together drops to 80%. More seriously, with the rapid growth of the data volume on the Web, the problem becomes even more challenging, as the big Web data contains much more noisy and redundant data.</p><p>In this paper, we try to revisit this problem from the perspective of big data. Specifically, to avoid error propagation, we propose a unified approach framework to process all the extraction subtasks together in one step. Moreover, rather than figuring out new ways of reducing noise (or redundancy), we develop a simple but very effective approach for extracting user profile attributes using the redundant information in the big Web data. To incorporate redundant information to improve the extraction accuracy, we propose a markov logic factor graph (MagicFG) model to formalize human knowledge as first-order logics in the model.</p><p>We compare our approach with several state-of-the-art methods for profiling (Cf. Section III for detailed comparisons). As shown in Figure <ref type="figure" target="#fig_1">2</ref> We apply the proposed model to an academic search and mining system AMiner.org <ref type="foot" target="#foot_3">4</ref> to extract profile for researchers from the Web. Figure Figure <ref type="figure" target="#fig_0">1</ref> shows an example of researcher profile in AMiner.org. The profile contains basic information of Dr. Jiawei Han such as affiliation, position, picture, email, and homepage. With the proposed model, we have successfully extracted more than 100,000,000 researcher profiles. We also quantitatively evaluate the proposed model on a ground-truth dataset. The proposed method achieves significant improvement (+4-6% in terms of F1 score; p 0.01, t-test) over several alternative methods.</p><p>Organization In Section II, we describe the proposed approach for user profiling. In Section III, we present experimental results to evaluate the effectiveness of the proposed approach. In Section IV, we review the related work and finally, Section V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPROACH FRAMEWORK</head><p>In this section, we first give the basic idea of the proposed framework to solve the profiling problem, and then introduce three methods to extract profile attributes from the Web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Idea</head><p>Given a person v, referred to as query person, our goal is to extract profile attributes of the person and construct a researcher profile. For example, in the academic search system, the researcher profile consists of position, picture, address, phone, email, homepage, research interest, etc. A detailed definition can be found in <ref type="bibr" target="#b4">[5]</ref>. We aim to design a general method to automatically extract the profile attributes from the Web with high accuracy. The method should be also flexible enough to extended to handle new profile attributes.</p><p>Traditional methods usually deal with the problem by first finding relevant Web pages for the query person from the Web, and then using models such as SVM or CRF to extract the required profile attributes from the pages. In both steps, the state-of-the-art performances achieved by traditional methods are around 90% <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, the overall accuracy by combining the two steps inevitably drops to 80% due to error propagation between the two steps. Meanwhile, the required profile attributes may distribute in different Web pages, which leads to two new challenges: 1) extraction from distributed pages and 2) extraction with data redundancy.</p><p>To tackle the problem of error propagation and data redundancy, we propose a unified framework to process all the extraction subtasks together from the big Web data. The approach is simple but very effective. Specifically, for each profile attribute, we first construct a "smart" query and use a search engine to retrieve relevant snippets with the query, finally an extraction model is applied to the returned snippets to extract the profile attribute. The idea behind is to leverage data redundancy to help the extraction. Suppose we are going to extract the affiliation of "Philip S. Yu". The constructed query can be "Philip S. Yu affiliation". In a similar way, to extract the email address of "Philip S. Yu", we can construct "Philip S. Yu email". Figure <ref type="figure" target="#fig_2">3</ref> shows two example snippets returned by Google with two constructed queries. We see that from 3(a) we can easily identify two different affiliations: "University of Illinois at Chicago" and "IBM T. J. Watson Research Center" (after normalization), and from 3(b) we can also identify two email addresses: "psyu@cs.uic.edu" and "hanj[at]cs.uiuc.edu". We call the identified affiliations/emails from the snippets as candidate affiliations/emails. Now the problem is how to rank the identified information. Our basic idea is to leverage the redundancy information-e.g., "University of Illinois at Chicago" occurs four times in the snippets and two times for "IBM T. J. Watson Research Center". More precisely, we propose a MAkov loGIC factor graph (MagicFG) to rank the obtained candidates by leveraging the redundancy information. The model is flexible and can easily incorporate any domain human knowledge to further improve the extraction accuracy.</p><p>It is noteworthy that we are restricted to the two example profile attributes. In fact, the proposed method is in general flexible. To extract a new profile attribute, what we need to do is to construct the "smart" query and to train the MagicFG model. For some profile attributes, it is easy to construct the query. For example, we found that for email, we can achieve a high accuracy by simply using name+email. For some other profile attributes, for example, Gender and Position, the situation may be more complicated. We will introduce how we construct the smart query for general profile attributes in Section II-B. Please also note that there are generally two types of profile attributes: the categorical attributes and the non-categorical attributes. For example, Gender is a categorical attribute. Position is also a categorical attribute with multiple values, such as professor, student, researcher, and engineer. While Email and Age are two non-categorical attributes. The two kinds of attributes will be treated slightly differently in the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Smart Query Construction</head><p>We construct the queries for the categorical and noncategorical attributes in different ways. For the categorical attributes, we construct the query by automatically identifying representative keywords in each candidate category and combine them together as the query. To find the representative keywords for each category, we first collect several person names (e.g., 1000) for each category from professional websites such as AMiner and LinkedIn. We then submit the corresponding person names as queries to search engines like Google to obtain top-k (e.g., 10) snippets. Among all the words in the snippets, we identify the most representative keyword as that with the highest TF-IDF scores <ref type="bibr" target="#b10">[11]</ref>. The TF-IDF score of a word w in a category c is calculated as follows:</p><formula xml:id="formula_0">TF-IDF(w, c) = (1 + log n(S c , w)) log(1 + |S| n(S, w) )<label>(1)</label></formula><p>where S c denotes the snippets that belongs to category c.</p><p>Notation n(S c , w) denotes the number of snippets in category c that contains the word w. Notation n(S, w) indicates the number of snippets in all the categories that contains the word w and |S| is the number of all the snippets in all the categories. Take Gender as an example, using the above method, we found that the most representative keyword is "her" for females, and is "his" for males . The query is then constructed as "name his|her".</p><p>For a non-categorical attribute, we directly use the keywords in the attribute name to construct the query. For example, the query for Email extraction can be "name email|mail".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Extraction Models</head><p>We first introduce two baseline models for extracting the profile attributes.</p><p>1) Rule-based Model: In the rule-based model, for extracting Email of the query person v, we simply construct the query by combing the person name and word "email". Once obtained the returned snippets from a search engine (e.g., Google), we can use rule-based heuristics to extract the Email of the query person. The rules are defined as follows: we first extract the candidate email addresses from the searched snippets, and if we find the first name or the last name of the queried person name is contained in the prefix of a candidate email address, then the extracted Email will be selected as the result. One thing worth noting is the recognition of the email address, because in many Web pages, especially some person's homepages, the email addresses may be encoded in special patterns such as "firstname [dot] lastname [at] cmu [dot] edu". Heuristic rules are defined to recognize potential email addresses <ref type="foot" target="#foot_4">5</ref> .</p><p>Our preliminary experiments show that such a simple method could be able to result in an accuracy of 88%comparable with the state-of-the-art performance obtained by traditional two-step approach (Cf. Section III for detailed comparisons).</p><p>2) Classification-based Model: We make use of Logistic Regression (LR) as the classification model. Let us first consider a two class classification problem. Let {(x 1 , y 1 ), • • • , (x N , y N )} be a training data set, in which x i denotes a feature vector of a candidate information and y i ∈ {−1, +1} denotes a classification label (whether the candidate is correct or not). The classification-based extraction model consists of two stages: learning and extraction. In learning, one attempts to find an optimal weight configuration to maximize the log-likelihood function of the observed instances). In extraction, we use the learned model to classify which candidate information is we want to extract.</p><p>Regarding features in the classification model, we use the same attribute features as the attribute factors defined in our proposed model (Cf. Section II-C for details). The classification can adjust the weights of different features and combine the feature together, thus obtains a better performance (90% in terms of F1-score) than the rule-based method. However, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, the returned snippets usually contain much redundant information that might be helpful for the extraction. Both the rule-based and the classification-based models consider each candidate instance as independent and thus cannot leverage such redundant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Markov Logic Factor Graph (MagicFG) Model</head><p>The rule-based method is comparable with traditional methods and the classification-based method outperforms many existing methods. Both of the above methods treat the candidate email addresses as independent classification objects, while both the rule-based and the classification-based methods ignore the correlations among the candidate instances identified in the returned snippets. However, this redundant information can be leveraged. In practice, the redundant information resided in the snippets can be helpful to improve the extraction accuracy. For example, in Figure <ref type="figure" target="#fig_2">3(b)</ref>, for Email extraction, the same prefix "psyu" before "@" in the two candidate email addresses, "psyu@cs.uic.edu" and "psyu@uic.edu", indicates that the two email addresses belong to the same person. Modeling Non-categorical Attributes. When extracting noncategorical attributes, for each query person, we construct a factor graph model with each node representing a candidate instances, and each edge corresponding to the dependency between two candidate instances. We optimize the factor graph model for all query persons simultaneously. We explain the modeling process of categorical attributes using Email as the example. For a query person v, we denote each candidate Email as e i . As the example in Figure <ref type="figure" target="#fig_3">4</ref>, we could extract four candidates {e 1 , e 2 , e 3 , e 4 }. For each candidate Email, we create an instance (e i , v) and associate with a latent variable y i . To model the correlations between the candidate instances, we can construct a factor graph model as presented in Fig- <ref type="figure" target="#fig_3">ure 4</ref>. The model is referred to as Markov logic factor graph (MagicFG) model. In MagicFG, the correlation is represented as the first-order logic as it defines prior knowledge and all the other correlations as first-order logics. We will explain how we define the first-logic based correlation later. At the high-level, in MagicFG, we define two types of functions.</p><p>• Attribute factor function: It captures characteristics of the Email-person pair and is defined as an exponential function:</p><formula xml:id="formula_1">f (v, e i , y i ) = 1 Z a exp{ k α k φ k (y i , x i )},<label>(2)</label></formula><p>where φ k (.) is the k th feature function defined between v and e i with respect to the value of y i ; α k is the weight of the corresponding attribute feature; x i is the i th feature vector. Z a is the normalization factor.</p><p>• Logic factor function: It captures the correlations between latent variables. It is also defined as an exponential function:</p><formula xml:id="formula_2">g(y i , y j ) = 1 Z b exp{ m β m ψ m (y i , y j )},<label>(3)</label></formula><p>where ψ m (.) is the m th correlation factor function defined between y i and y j according to a first-order logic knowledge base; β m is the weight of the corresponding correlation factor . For the attribute factor function, we can define multiple feature functions {φ k (y i , x i )} k to characterize each candidate instance. For extracting Email, we define features such as whether v's first name, last name or full name is contained in e i 's prefix. <ref type="foot" target="#foot_5">6</ref> Another kind of feature is defined between person v and the context c i from which the candidate e i is extracted. For example, whether v's first name, last name or full name is contained in context c i , and whether v's affiliation is contained in context c i . Here we use the affiliation information to disambiguate persons with the same names.</p><p>Regarding the logic factor function, we mainly consider three kinds of first-order logic relationships between latent variables: complete consistency, partial consistency and prior knowledge. First-order logic is the standard for the formalization of mathematics into axioms and is studied in the foundations of mathematics. In our problem, we use first-order logic to encode user-specific correlations between candidate instances and domain human knowledge about the extraction. For a general introduction of first-order logic, please refer to <ref type="bibr" target="#b11">[12]</ref>.</p><p>Complete consistency describes the fact that the values of two latent variables y i and y j should be consistent under some given conditions. For example, the following first-order logic Equals(e i , e j ) ⇒ Equals(y i , y j ) indicates that y i equals y j if the corresponding Email candidates are same with each other. The logic is straightforward because two same email addresses are highly likely to be credible or incredible at the same time. Correspondingly, we define the factor function as</p><formula xml:id="formula_3">ψ(y i , y j ) =</formula><p>1, e i = e j and y i = y j 0. otherwise Partial consistency describes the fact that the values of two latent variables y i and y j should be partially consistent under some given conditions. For example, the following first-order logic SamePrefix(e i , e j ) ⇒ True(y i ) ∧ True(y j ) indicates that y i and y j both equal to 1 if their prefixes are the same. This logic can be explained as follows. When two email addresses share the same prefix, they are very likely to mention the same person, because people usually use the same prefix in different email addresses. In this case, If one email address is correct, the other one is also likely to be also correct. We define the corresponding factor function as</p><formula xml:id="formula_4">ψ(y i , y j ) =   </formula><p>1, e i and e j have the same prefix and y i = y j = 1 0. otherwise Prior knowledge describes the prior knowledge that can be formalized into useful first-order logics for a specific task. For example, when we search for someone's email address by Google, we find that many candidates starting with the word "email", like "email@gmail.com". This is due to the security policy of the search engine, which blocks or modifies the prefixes of some email addresses from several sensitive sources. We can still observe the domain information. We found that when another candidate shares the same domain with a blocked candidate, it is more likely that the other candidate is a correct Email. We define the corresponding firstorder logic as IsBlocked(e j ) ∧ SameDomain(e i , e j ) ⇒ True(y i ) ∧ False(y j ).</p><p>The corresponding factor function is defined as</p><formula xml:id="formula_5">ψ(y i , y j ) =       </formula><p>1, c j is blocked, c i and c j have the same domain, y i = 1 and y j = 0 0. otherwise For each profiling task, we build a knowledge base according to the defined first-order logics and summarize it in Table <ref type="table">I</ref>. In general, the attribute factors capture the characteristics on each potential person-Email pair and the logic correlation factors capture the dependencies between two person-Email pairs. Modeling Categorical Attributes. When dealing with categorical attributes, for all the queried persons, we build one factor graph with each node representing a query person, and the edges representing dependencies between two query persons. We use Gender as the example to explain the modeling process for the categorical attributes.</p><p>Different from the non-categorical attributes, each person can only have one Gender, either male or female. Thus in this task, we directly assign a label to each query person. We construct a query by combing the person name and the representative keywords for each Gender("his" for male and "her" for female, as mentioned before). The query finally looks like "name his|her". Then we formulate the MagicFG based on the returned snippets.</p><p>The formulation of MagicFG model is also a little different from that of non-categorical attributes. We feed the model with each observation variable as a person v i . The corresponding latent variable y i to each person v i represents v i 's attribute values, e.g., whether v i is male or female.</p><p>For attribute factor functions, we first extract features for each person from his/her search context. For example, whether a snippet in the search results contains both the person name and the word "his/her", whether a snippet contains both the affiliation and the word "his/her", whether "his/her" appears in the snippets of the top 3 returned search results, and the number "his/her" in all the search results. For logic factor functions, we define a correlation feature of the type of complete consistency logic as follows:</p><formula xml:id="formula_6">SameFirstname(v i , v j ) ⇒ Equals(y i , y j ).</formula><p>The logic indicates that the Gender of two persons are more likely to be the same if they have the same first name.</p><p>In summary, the factor graphs built for the non-categorical and categorical attributes are slightly different. For noncategorical attributes, multiple graphs are build, of which each graph is build for each person with each candidate attribute being formed as a node and the dependency between two candidate attributes being formed as an edge. While for categorical attributes, only one graph is build, with each person being formed as a node and the dependency between two persons being formed as an edge.</p><p>Model Training and Extraction. Once we formulated the MagicFG model for either non-categorical or categorical attributes, we can combine the defined factor functions and define the following log-likelihood objective function by following the Markov assumption <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_7">log P (Y |X, θ) = yi∈Y k α k φ k (y i , x i ) + ei∼ej m β m ψ m (y i , y j ) − log Z,<label>(4)</label></formula><p>where Z = Z a Z b is the normalization factor; e i ∼ e j indicates that there is a (directed or indirected) correlation between e i and e j ; θ = (α, β) are parameters to estimate, representing the weights of the defined feature functions.</p><p>Training a MagicFG is to estimate a parameter configuration θ = (α, β) from a given historical dataset, such that the loglikelihood objective function L(θ) can be maximized,</p><formula xml:id="formula_8">θ * = arg max θ log P (Y |X, θ). (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>We use a gradient ascent algorithm to solve the objective function. The gradient for parameter α k can be written as:</p><formula xml:id="formula_10">∂L(θ) ∂α k = E[φ(y i , x i )] − E P (y i , x i )[φ(y i , x i )].<label>(6)</label></formula><p>The parameter β m can be obtained in the same way. In the above equation, the first term E[φ(y i , x i )], representing the expectation of features values under the uniform distribution, can be easily calculated, while it is usually intractable to directly estimate the marginal probability in the second term as the graphical structure can be arbitrary and may contain cycles. In this work, we use loopy belief propagation (LBP) <ref type="bibr" target="#b13">[14]</ref> to approximate the marginal probability in the second term and accordingly calculate the gradient. The learning algorithm can be divided into two steps: we first perform the LBP algorithm to calculate marginal distribution for each latent variable, and then update each parameter to maximize the objective loglikelihood function by :</p><formula xml:id="formula_11">θ new = θ old + η • O(θ) θ , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where η is the learning step. The process repeats updating marginal probabilities and parameters until the convergence or until the number of iterations is large enough. Given the observed feature vectors X v for all candidates of person v and the learned parameter configuration θ, the extraction can be done by finding the most likely configuration of Y v = {y 1 , ..., y I } for all the person-Email pairs {(e i , v)}:</p><formula xml:id="formula_13">Y v = arg max Yv P (Y v |X v , θ),<label>(8)</label></formula><p>where the LBP algorithm is again used to solve this problem.</p><p>Discussions. Different from traditional methods that crawled each of the relevant pages, we only use the snippet information to extract the profile attributes. It is much faster and more stable, as different servers that host the relevant pages may have very different network speed. Also we found with the constructed "smart" queries, more than 90% of the profile attributes are already contained in the snippets returned by the search engine. One additional advantage is that we do not need to maintain a large database to record all the relevant pages for all the query persons. This is very important, as, for example, in AMiner, we have more than 130,000,000 researchersmaintaining such a big database for all researchers itself is a challenging task. Moreover, the profile information is very dynamic. Our method avoids this problem by directly querying the search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>In this section, we demonstrate the effectiveness of our approach for both categorical and non-categorical attributes. For quantitative evaluation, we take Gender as an example of categorical attributes, and Email as an example of noncategorical ones. Please note that our framework is very flexible and have already been applied to an online academic search and mining system AMiner.org to extract the profiles for researchers. All datasets and codes used in this work are publicly available. <ref type="foot" target="#foot_6">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup</head><p>Dataset. To construct a ground-truth dataset for quantitative evaluation, we randomly choose 2,000 researchers from AMiner.org <ref type="bibr" target="#b8">[9]</ref>. Specifically, for extracting Email of each researcher, we search the Web using search engine by querying the person name and the word "email". This way results in 4,528 Email candidates. Human annotations are applied to identify the correct Email addresses. In an analogous way, for inferring Gender, we search the Web by querying the person name and the word "his" or "her". Human annotations were also conducted to identify the Gender of these 2,000 researchers. For disagreements in the annotation, we conducted "majority voting". Finally, for the 2,000 researchers, we identify 34% of the researchers are female researchers; and about 40% of the Email candidates are correct Email, which means that our framework can find the correct Email for over 90% users.</p><p>Evaluation Metrics. To quantitatively evaluate our model, we divide the dataset into training set and test set. We perform five-fold cross-validation and report the extraction performance in terms of precision, recall, and F1-score.</p><p>Comparison Methods. We compare the MagicFG with following methods for extracting Email and Gender on the ground-truth dataset.</p><p>• Rule. Uses several simple defined rules to extract profile attributes. For example, for extracting Gender, we count the number of common names for girls and boys. For extracting Emails, we find whether the prefix of the Email contains the person name. • Support Vector Machine (SVM). Uses the same attribute factors as features and employs SVM-Light <ref type="bibr" target="#b14">[15]</ref> to train and predict Email and Gender.  MagicFG captures the dependencies between different Email candidates by incorporating them as features of first-order logics, in addition to the independent attribute features.</p><p>Gender inference. Under the same framework we propose, the MagicFG model outperforms the best method LR (+2.21% in terms of F1-score). Because MagicFG additionally incorporates one logic relationship of complete consistence logic and capture the dependencies between different person candidates.</p><p>Effect of factors. We further present an in-depth analysis of how different logic correlation factors affect the performance of user profiling. Figure <ref type="figure" target="#fig_4">5</ref> shows the different evaluation metrics of the proposed MagicFG by considering different levels of logic factors. It can be clearly seen from Figure <ref type="figure" target="#fig_4">5</ref>(a) that for Email extraction, the accuracy performance drops significantly without the logic correlations. In addition, adding the factors of prior knowledge logic can further improve the performance significantly. Figure <ref type="figure" target="#fig_4">5</ref>(b) also show that the factor of complete consistence logic improves the performance of Gender inference significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Existing Methods</head><p>We now compare our approach with several state-of-the-art methods for the task of Email extraction and Gender inference:</p><formula xml:id="formula_14">• TCRF</formula><p>For Email extraction, we use the method proposed in <ref type="bibr" target="#b4">[5]</ref> as the baseline (to hereafter refer to as: TCRF), which is one of the state of art approaches to extract homepages and Emails from the Web. This method has two steps, where it first finds the user's homepage and then extracts Email from the homepage with a high precision using an extraction model named TCRF.</p><p>Table <ref type="table" target="#tab_2">II</ref> shows the classification performance of Email extraction by different methods. We can see from the results that our method consistently outperforms the baseline (TCRF) on F1-score by +5.78%. As you can see, the recall of our system is clearly much better (+9.53%). This is because the TCRF method only chooses the homepage as its data source, which is a little narrow and ignores useful information from other sources on the Web. From the careful construction of query, our approach can effectively find out rich sources related to the target attribute, reducing the risk of missing the right choice. It is noteworthy that our approach also achieves better precision for Email extraction. • Facebook Generated Name List Predictor(FGNL).</p><p>For Gender inference, we use a method proposed by <ref type="bibr" target="#b9">[10]</ref> as the baseline (to hereafter refer to as: FGNL). Most state of the art methods for inferring Gender depend on a list of common names for males and females. In <ref type="bibr" target="#b9">[10]</ref>, the authors proposed an approach which used data from Facebook to construct an expanded and high-quality name list. They match the user's first name with the list to make the inference. If the first name is matched with one of the male names, the user is treated as a male, and vice versa. While if the first name is found in neither the male names nor the female names, or in both the name lists, they make a random guess about the user's Gender.</p><p>Table <ref type="table" target="#tab_2">III</ref> shows the classification performance of Gender inference by different methods. We can see that our method outperforms the baseline (FGNL) on F1score by +6.49%. Our method performs much better than the FGNL method in recall (+13.13%). This is because the FGNL method depends greatly on the name list. However, you can never list all those names, no matter how large the list is. On the contrary, our approach can automatically find the representative keywords for documents describing a user with specific Gender, and infer Gender from the big Web data with less limitation. So we seldom have the problem that the FGNL has to face when they cannot find the name in their list. As the table shows, the FGNL method performs slightly better in precision (+1.22%), which is clearly an advantage of using the name list. However, our approach achieves a close precision while raising the recall to a different level. Taking the limits of the FGNL method into consideration, our approach is much more generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATED WORK</head><p>Previously considerable efforts have been made for obtaining user profiles. Back in 1900s, <ref type="bibr" target="#b1">[2]</ref> discussed algorithms for learning and revising user profiles that could determine which Web sites on a given topic would be interesting to a user. It used a Naive Bayes classifier to incrementally learn profiles from user feedback on the Web sites. <ref type="bibr" target="#b15">[16]</ref> had developed a personalized web browser. It learned a user profile, and aimed at helping user navigate the Web by searching for potentially interesting pages for recommendations. <ref type="bibr" target="#b2">[3]</ref> described an experimental work to study whether user interests could be automatically classified through heuristics. The results highlighted the need for user feedbacks and machine learning methods.</p><p>Nowadays, with the rapid development of the Internet, especially that of social networks, we are capable of fetching user profiles, with different methods and from different sources. For example, Yu et al. propose a cascaded information extraction framework for identifying personal information from resumes <ref type="bibr" target="#b16">[17]</ref>. Tang et al. propose a conditional random field to extract user profiles from one's homepages <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr">Li et al.</ref> propose a weakly supervised method to extract user profiles from Twitter in 2014 <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr">Merler et al.</ref> propose a method to extract user attributes from the pictures posted in social media feeds <ref type="bibr" target="#b17">[18]</ref>, especially gender information. <ref type="bibr" target="#b18">[19]</ref> and inferred user's profile by analysing the user's Twitter posts, which is a little difficult to generalize to other applications. However, these methods are highly dependent on the quanlity of the data sources, and thus their performance may be sufferred from error propagation.</p><p>To reduce such risks, efforts have been made to combine several data sources. In 2015, <ref type="bibr" target="#b19">[20]</ref> presented an initial study of user profile learning via integration of multiple data sources, including Twitter, Foursquare and Instagram. They present that multiple data sources of the same users can enhance the final performance. <ref type="bibr" target="#b20">[21]</ref> proposed an effective method to link different social network accounts for a specific user. The correlation between redundancy and correctness of retrieved information has also been well studied in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref>. These interesting ideas give us a different insight into the profiling problem, and inspire us to design a more generalizable framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We study an interesting problem of web user profiling using big data and propose an approach framework to extract user profile attributes directly from the Web. For a given profiling task, the approach first constructs a meaningful query to retrieve relevant information. Without downloading any Web data, we present a Markov logic factor graph (MagicFG) model to directly model and extract user profile from the search results. The MagicFG incorporates the redundant information in the big data. We test the proposed method on two real data sets. Our experiments show that the proposed method significantly improves the profiling accuracy in comparison with several comparison methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of researcher profile in AMiner.org. The profile contains basic information such as affiliation, position, picture, email, and homepage.</figDesc><graphic url="image-1.png" coords="1,316.82,243.01,234.17,112.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance comparison between our approach with existing methods. (a) Comparison with TCRF [5], a two-step method for Email extraction. (b) Comparison with FGNL [10] for Gender inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Snippets returned by Google by the two constructed queries. From (a) we can easily identify two affiliations and from (b) we can also identify two email addresses.</figDesc><graphic url="image-4.png" coords="2,435.52,50.54,129.82,152.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Graphical representation of logic factor graph model based on a real search example. Notation (e i , v) represents a Email-person pair, and yi indicates its corresponding label; Notations f (.) and g(.) represent the attribute factor function and logic factor function respectively.</figDesc><graphic url="image-36.png" coords="4,159.72,140.14,144.67,63.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Effect of logic correlation factors in Email extraction and Gender inference. Basic stands for the MagicFG model that only consider the attribute factors. +CC stands for adding the factors of complete consistency logic. +PC adds factors of partial consistency logic. +PK adds the factors of prior knowledge logic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>First-order logicExample Complete Consistency Equals(e i , e j ) ⇒ Equals(y i , y j ) Partial Consistency SamePrefix(e i , e j ) ⇒ True(y i ) ∧ True(y j )</figDesc><table><row><cell></cell><cell>TABLE I</cell></row><row><cell cols="2">FIRST-ORDER LOGIC KNOWLEDGE BASE.</cell></row><row><cell>Prior Knowledge</cell><cell>IsBlocked(e</cell></row></table><note>j ) ∧ SameDomain(e i , e j ) ⇒ True(y i ) ∧ False(y j )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>•</head><label></label><figDesc>Random Forest (RF). Uses the same attribute factors as features and employs sklearn package to conduct train and predict.• Logistic Regression (LR). Uses the same attribute factors as features and employs sklearn package to conduct train and predict. The MagicFG model is implemented in C++. All experiments are conducted on a Macbook Pro with Intel Core i5 CPU 2.9GHz(2 cores) and 8 GB memory. In all the experiments, we set L = 10 and search top 10 results by Google search, and conduct a five-fold cross validation for each method. Under the same framework we propose, the MagicFG model outperforms the best extraction method, namely RF (+2.12% in terms of F1-score). This is because</figDesc><table><row><cell>94</cell></row><row><cell>93</cell></row><row><cell>92</cell></row><row><cell>91</cell></row><row><cell>90</cell></row><row><cell>89</cell></row><row><cell>88</cell></row><row><cell>87</cell></row><row><cell>B. Extraction Performance Email extraction. 86 Precision Recall F1-score Basic Basic+CC Basic+CC+PC Basic+CC+PC+PK</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON OF EMAIL EXTRACTION (%)</figDesc><table><row><cell>Method</cell><cell>Precision</cell><cell cols="2">Recall F1-score</cell></row><row><cell>TCRF</cell><cell>90.20</cell><cell>83.83</cell><cell>86.90</cell></row><row><cell>Rule</cell><cell>87.81</cell><cell>89.64</cell><cell>88.72</cell></row><row><cell>SVM</cell><cell>88.26</cell><cell>89.25</cell><cell>88.75</cell></row><row><cell>RF</cell><cell>90.76</cell><cell>90.58</cell><cell>90.56</cell></row><row><cell>LR</cell><cell>89.07</cell><cell>91.14</cell><cell>90.11</cell></row><row><cell>MagicFG</cell><cell>92.00</cell><cell>93.36</cell><cell>92.68</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="4">PERFORMANCE COMPARISON OF GENDER INFERENCE (%)</cell></row><row><cell>Method</cell><cell>Precision</cell><cell cols="2">Recall F1-score</cell></row><row><cell>FGNL</cell><cell>94.66</cell><cell>80.88</cell><cell>87.23</cell></row><row><cell>Rule</cell><cell>92.12</cell><cell>88.32</cell><cell>90.18</cell></row><row><cell>SVM</cell><cell>91.98</cell><cell>90.60</cell><cell>91.29</cell></row><row><cell>RF</cell><cell>90.17</cell><cell>89.99</cell><cell>90.08</cell></row><row><cell>LR</cell><cell>91.48</cell><cell>91.54</cell><cell>91.51</cell></row><row><cell>MagicFG</cell><cell>93.44</cell><cell>94.01</cell><cell>93.72</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://emailbreaker.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://emailhunter.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://www.getsidekick.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://aminer.org (a) Affiliation (b) Email</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">One example of the heuristic rule: "(([a − z0 − 9]+)(\.|dot|\.)?) + (@|at|\[at\]|\[at\])(([a − z0 − 9\]+)(\.|dot|\.\[dot\])) + ([a − z]+)"</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">We call the string before "@" of an Email candidate as the prefix of the Email, and the string after "@" as the domain of it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://aminer.org/profiling/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">User modeling in intelligent information retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brajnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing &amp; Management</title>
				<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="305" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning and revising user profiles: The identification of interesting web sites</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="313" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic learning of user profilestowards the personalisation of agent services</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Soltysiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Crabtree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT Technology Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="110" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Social network extraction of academic researchers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;07</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A combination approach to web user profiling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised user profile extraction from twitter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lifestyle finder: Intelligent user profiling using large-scale demographic data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krulwich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ontological user profiling in recommender systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Middleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Shadbolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>De Roure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="88" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Whats in a name: a study of names, gender inference, and gender behavior in facebook</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database Systems for Adanced Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="344" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<title level="m">Modern Information Retrieval</title>
				<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Markov fields on finite graphs and lattices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hammersley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clifford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized belief propagation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="689" to="695" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Making large scale svm learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Universität Dortmund</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A non-invasive learning approach to building web user profiles</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD-99 Workshop on Web Usage Analysis and User Profiling</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resume information extraction with cascaded hybrid model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;05</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You are what you tweet pic! gender prediction based on semantic analysis of social media images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME), 2015 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Twitter user modeling and tweets recommendation based on wikipedia concept graph</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Harvesting multiple sources for user profile learning: a big data study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
				<meeting>the 5th ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cosnet: Connecting heterogeneous social networks with local and global consistency</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1485" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A probabilistic model of redundancy in information extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
