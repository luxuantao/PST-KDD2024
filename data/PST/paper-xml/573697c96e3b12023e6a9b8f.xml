<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hopper: Decentralized Speculation-aware Cluster Scheduling at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoqi</forename><surname>Ren</surname></persName>
							<email>xren@caltech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Wierman</surname></persName>
							<email>adamw@caltech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">California Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minlan</forename><surname>Yu</surname></persName>
							<email>minlanyu@usc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hopper: Decentralized Speculation-aware Cluster Scheduling at Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DEEA9C89932384400E7D3A19833FE12</idno>
					<idno type="DOI">10.1145/2785956.2787481</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speculation</term>
					<term>decentralized scheduling</term>
					<term>straggler</term>
					<term>fairness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As clusters continue to grow in size and complexity, providing scalable and predictable performance is an increasingly important challenge. A crucial roadblock to achieving predictable performance is stragglers, i.e., tasks that take significantly longer than expected to run. At this point, speculative execution has been widely adopted to mitigate the impact of stragglers. However, speculation mechanisms are designed and operated independently of job scheduling when, in fact, scheduling a speculative copy of a task has a direct impact on the resources available for other jobs. In this work, we present Hopper, a job scheduler that is speculationaware, i.e., that integrates the tradeoffs associated with speculation into job scheduling decisions. We implement both centralized and decentralized prototypes of the Hopper scheduler and show that 50% (66%) improvements over state-of-the-art centralized (decentralized) schedulers and speculation strategies can be achieved through the coordination of scheduling and speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>•Networks → Cloud computing; •Computer systems organization → Distributed architectures;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data analytics frameworks have successfully realized the promise of "scaling out" by automatically composing user-submitted scripts into jobs of many parallel tasks and executing them on large clusters. However, as clusters increase in size and complexity, providing scalable and predictable performance is an important ongoing challenge for interactive analytics frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. Indeed, production clusters at Google and Microsoft <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> acknowledge this as a prominent goal.</p><p>As the scale and complexity of clusters increase, hardto-model systemic interactions that degrade the performance of tasks become common <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>. Consequently, many tasks become "stragglers", i.e., running slower than expected, leading to significant unpredictability (and delay) in job completion times -tasks in Facebook's Hadoop cluster can run up to 8× slower than expected <ref type="bibr" target="#b11">[12]</ref>. The most successful and widely deployed straggler mitigation solution is speculation, i.e., speculatively running extra copies of tasks that have become stragglers (or likely to), and then picking the earliest copy that finishes, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50]</ref>. Speculation is commonplace in production clusters, e.g., in our analysis of Facebook's Hadoop cluster, speculative tasks account for 25% of all tasks and 21% of resource usage.</p><p>Speculation is intrinsically intertwined with job scheduling because spawning a speculative copy of a task has a direct impact on the resources available for other jobs. Aggressive speculation can improve the performance of the job at hand but hurt the performance of other jobs. Despite this, speculation policies deployed today are all designed and operated independently of job scheduling; schedulers simply allocate slots to speculative copies in a "best-effort" fashion, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Coordinating speculation and scheduling decisions is an opportunity for significant performance improvement. However, achieving such coordination is challenging, particularly as schedulers themselves scale out. Schedulers are increasingly becoming decentralized in order to scale to hundreds of thousands of machines with each machine equipped with tens of compute slots for tasks. This helps them make millions of scheduling decisions per second, a requirement about two orders of magnitude beyond the (already highly-optimized) centralized schedulers, e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49]</ref>. In decentralized designs multiple schedulers operate autonomously, with each of them scheduling only a subset of the jobs, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Thus, the coordination between speculation and scheduling must be achieved without maintaining central information about all the jobs. Contribution of this paper: In this paper we present the design of the first speculation-aware job scheduler, Hopper, which dynamically allocates slots to jobs keeping in mind the speculation requirements necessary for predictable performance. Hopper incorporates a variety of factors such as data locality, estimates of task execution times, fairness, dependencies (DAGs) between tasks, etc. Further, Hopper is compatible with all current speculation algorithms and can operate as either a centralized or decentralized scheduler; achieving scalability by not requiring any central state.</p><p>The key insight behind Hopper is that a scheduler must anticipate the speculation requirements of jobs and dynamically allocate capacity depending on the marginal value (in terms of performance) of extra slots which are likely used for speculation. A novel observation that leads to the design of Hopper is that there is a sharp "threshold" in the marginal value of extra slotsan extra slot is always more beneficial for a job below its threshold than it is for any job above its threshold. The identification of such a threshold then allows Hopper to use different resource allocation strategies depending on whether the system capacity is such that all jobs can be allocated more slots than their threshold or not. This leads to a dynamic, adaptive, online scheduler that reacts to the current system load in a manner that appropriately weighs the value of speculation.</p><p>Importantly, the core components of Hopper can be decentralized effectively. The key challenge to avoiding the need to maintain a central state is the fact that stragglers create heavy-tailed task durations, e.g., see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Hopper handles this by adopting a "power of many choices" viewpoint to approximate the global state, which is fundamentally more suited than the traditional "power of two choices" viewpoint due to the durations and frequency of stragglers.</p><p>To demonstrate the potential of Hopper, we have built three demonstration prototypes by augmenting the centralized scheduling frameworks Hadoop <ref type="bibr" target="#b2">[3]</ref> (for batch jobs) and Spark <ref type="bibr" target="#b48">[49]</ref> (for interactive jobs), and the decentralized framework Sparrow <ref type="bibr" target="#b35">[36]</ref>. Hopper incorporates many practical features of jobs into its scheduling. Among others, it estimates the amount of intermediate data produced by the job and accounts for their pipelining between phases, integrates data locality requirements of tasks, and provides fairness guarantees.</p><p>We have evaluated our three prototypes on a 200 node private cluster using workloads derived from Facebook's and Microsoft Bing's production traces. The decentralized and centralized implementations of Hopper reduce the average job completion time by up to 66% and 50% compared to state-of-the-art scheduling and straggler mitigation techniques. The gains are consis-tent across common speculation algorithms (LATE <ref type="bibr" target="#b49">[50]</ref>, GRASS <ref type="bibr" target="#b13">[14]</ref>, and Mantri <ref type="bibr" target="#b14">[15]</ref>), DAGs of tasks, and locality constraints, while providing fine-grained control on fairness. Importantly, the gains do not result from improving the speculation mechanisms but from improved coordination of scheduling and speculation decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND &amp; RELATED WORK</head><p>We begin by presenting a brief overview of existing cluster schedulers: how they allocate resources across jobs, both centralized and decentralized ( §2.1), and how they handle straggling tasks ( §2.2). This overview highlights the lack of coordination that currently exists between scheduling and straggler mitigation strategies such as speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cluster Schedulers</head><p>Job scheduling -allotting compute slots to jobs for their tasks -is a classic topic with a large body of work.</p><p>The most widely-used scheduling approach in clusters today is based on fairness which, without loss of generality, can be defined as equal sharing (or weighted sharing) of the available resources among jobs (or their users) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Fairness, of course, comes with performance inefficiencies, e.g., <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>In contrast, the performance-optimal approach for job scheduling is Shortest Remaining Processing Time (SRPT), which assigns slots to jobs in ascending order of their remaining duration (or, for simplicity, the remaining number of tasks). SRPT's optimality in both single <ref type="bibr" target="#b38">[39]</ref> and multi-server <ref type="bibr" target="#b36">[37]</ref> settings motivates a focus on prioritizing small jobs and has led to many schedulers such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>The schedulers mentioned above are all centralized; however, motivated by scalability, many clusters are beginning to adopt decentralized schedulers, e.g., at Google <ref type="bibr" target="#b22">[23]</ref>, Apollo <ref type="bibr" target="#b16">[17]</ref> at Microsoft, and the recently proposed Sparrow <ref type="bibr" target="#b35">[36]</ref> scheduler. The scalability of decentralized designs allows schedulers to cope with growing cluster sizes and increasing parallelism of jobs (due to smaller tasks <ref type="bibr" target="#b33">[34]</ref>), allowing them to scale to millions of scheduling decisions (for tasks) per second.</p><p>Importantly, the literature on cluster scheduling (both centralized and decentralized) ignores an important aspect of clusters: straggler mitigation via speculation. No current schedulers coordinate decisions with speculation mechanisms, while our analysis shows that speculative copies account for a sizeable fraction of all tasks in production clusters, e.g., in Facebook's Hadoop cluster, speculative tasks account for 25% of all tasks and 21% of resource usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Straggler Mitigation via Speculation</head><p>Dealing with straggler tasks, i.e., tasks that take significantly longer than expected to complete, is an important challenge for cluster schedulers, one that was called out in the original MapReduce paper <ref type="bibr" target="#b23">[24]</ref>, and a topic of significant subsequent research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Clusters already blacklist problematic machines (e.g., faulty disks or memory errors) and avoid scheduling tasks on them. However, despite blacklisting, stragglers occur frequently, often due to intrinsically complex causes such as IO contention, interference by periodic maintenance operations, and hardware behaviors which are hard to model and circumvent <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref>. Straggler prevention based on comprehensive root-cause analyses is an open research challenge.</p><p>The most effective, and indeed the most widely deployed, technique has been speculative execution. Speculation techniques, monitor the progress of running tasks, compare them to the progress of completed tasks of the job, and spawn speculative copies for those progressing much slower, i.e., straggling. It is then a race between the original and speculative copies of the task and on completion of one, the other copies are killed. 1  There is considerable (statistical and systemic) sophistication in speculation techniques, e.g., ensuring early detection of stragglers <ref type="bibr" target="#b14">[15]</ref>, predicting duration of new (and running) tasks <ref type="bibr" target="#b15">[16]</ref>, and picking lightly loaded machines to spawn speculative copies <ref type="bibr" target="#b49">[50]</ref>. The techniques also take care to avoid speculation when a new copy is unlikely to benefit, e.g., when the single input source's machine is the cause behind the straggling <ref type="bibr" target="#b45">[46]</ref>. Speculation has been highly effective in mitigating stragglers, bringing the ratio of the progress rates of the median task of a job to its slowest down from 8× (and 7×) to 1.08× (and 1.1×) in Facebook's production Hadoop cluster (and Bing's Dryad cluster).</p><p>Speculation has, to this point, been done independently of job scheduling. This is despite the fact that when a speculative task is scheduled it takes resources away from other jobs; thus there is an intrinsic tradeoff between scheduling speculative copies and scheduling new jobs. In this paper, we show that integrating these two via speculation-aware job scheduling can speed up jobs by considerably, even on average. Note that these gains are not due to improving the speculative execution techniques, but instead come purely from the integration between speculation and job scheduling decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MOTIVATION</head><p>The previous section highlights that speculation and scheduling are currently designed and operated independently. Here, we illustrate the value of coordinated speculation and scheduling using simple examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Strawman Approaches</head><p>We first explore two baselines that characterize how scheduling and speculation interact today. In our ex-1 Schedulers avoid checkpointing a straggling task's current output and spawning a new copy for just the remaining work due to the overheads and complexity of doing so. In general, even though the speculative copy is spawned on the expectation that it would be faster than the original, it is extremely hard to guarantee that in practice. Thus, both are allowed to run until the first completes.    <ref type="figure">*+," $-" $-" $-"</ref> &amp;<ref type="figure">-" #-"  ( ./0" #-" #-" #-" #-" #-"</ref> 1" 1#"1$"1%"1&amp;" ( )*+," #-" #-" #-" %-" ( ./0" #-" #-" #-" #-" amples we assume that stragglers can be detected after a task has run for 2 time units and that, at this point, a speculation is performed if the remaining running time (t rem ) is longer than the time to run a new copy (t new ). When the fastest copy of a task finishes, other running copies of the same task are killed. Note that while these examples have all jobs arrive at time 0, Hopper is designed to work in an online setting. Best-Effort Speculation: A simple approach, which is also the most common in practice, is to treat speculative tasks the same as regular tasks. The job scheduler allocates resources for speculative tasks in a "best effort" manner, i.e., whenever there is an open slot.</p><p>Consider the example in Figure <ref type="figure">1a</ref> with two jobs A (4 tasks) and B (5 tasks) that are scheduled using the SRPT policy. The scheduler has to wait until time 10 to find an open slot for the speculative copy of A4, despite detecting it was straggling at time 2. <ref type="foot" target="#foot_0">2</ref> Clearly, the scheduler can do better. If it had allocated a slot to A's speculative task at time 2 (instead of letting B use it), then job A's completion time would have reduced, without slowing job B (see Table <ref type="table" target="#tab_0">1</ref> for task durations).</p><p>Note that similar inefficiencies occur under Fair scheduling in this example. Budgeted Speculation: The main problem for besteffort speculation is a lack of available slots for speculation when needed. Thus, an alternative approach is to have the job scheduler reserve a fixed "budget" of slots for speculative tasks. Budgeting the right size of the resource pool for speculation, however, is challeng-ing because of time-varying straggler characteristics and fluctuating cluster utilizations. If the resource pool is too small, it may not be enough to immediately support all the tasks that need speculation. If the pool is too large, resource are left idle.</p><p>Figure <ref type="figure">1b</ref> illustrates budgeted speculation with three slots (slot 5 -7) being reserved for speculation. This, unfortunately, leads to slots 6 and 7 lying fallow from time 0 to 12. If the wasted slot had been used to run a new task, say B1, then job B's completion time would have been reduced. It is easy to see that similar wastage of slots occurs with the Fair scheduler. Note that reserving one or two instead of three slots will not solve the problem, since three speculative copies are required to run simultaneously at a later time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Challenges in Coordination</head><p>In contrast to the two baselines discussed above, Figure <ref type="figure">2</ref> shows the benefit of coordinated decision making.</p><p>At time 0 -10, we allocate 1 extra slot to job A (for a total of 5 slots), thus allowing it to speculate task A4 promptly. After time 10, we can dynamically reallocate the slots to job B. This reduces the average completion time compared to both the budgeted and best-effort strategies. The joint design budgeted slot 5 until time 2 but after task A4 finished, it used all the slots. Doing such dynamic allocation is already challenging in a centralized environment, and it becomes more so in a decentralized setting. In particular, decentralized speculation-aware scheduling has additional constraints. Since the schedulers are autonomous, there is no central state and thus, no scheduler has complete information about all the jobs in the cluster. Further, every scheduler has information about only a subset of the cluster (the machines it probed). Since decentralization is mainly critical for interactive jobs (sub-second or a few seconds), time-consuming gossiping between schedulers is infeasible. Finally, running all the schedulers on one multi-core machine cramps that machine and caps scalability, the original drawback they aim to alleviate.</p><p>In the above example, this means making the allocation as in Figure <ref type="figure">2</ref> when jobs A and B autonomously schedule their tasks without complete knowledge of utilizations of the slots or even each other's existence.</p><p>Thus, the challenges for speculation-aware job scheduling are: (i) dynamically allocating/budgeting slots for speculation based on the distribution of stragglers and cluster utilization while being (approximately) fair and, in decentralized settings, (ii) using incomplete information about the machines and jobs in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hopper: SPECULATION-AWARE SCHEDULING</head><p>The central question in the design of a speculationaware job scheduler is how to dynamically (online) balance the slots used by speculative and original copies of tasks across jobs. A given job will complete more quickly if it is allowed to do more speculation, but this comes at the expense of other jobs in the system.</p><p>Hopper's design is based on the insight that the balance between speculative and original tasks must dynamically depend on cluster utilization. The design guidelines that come out of this insight are supported by theoretical analysis in a simple model <ref type="bibr" target="#b7">[8]</ref>. We omit the analytic support due to space constraints and focus on providing an intuitive justification for the design choices. Pseudocode 1 shows the basic structure. <ref type="foot" target="#foot_2">3</ref>We begin our discussion of speculation-aware job scheduling by introducing the design features of Hopper in a centralized setting. We focus on single-phased jobs in §4.1, and then generalize the design to incorporate DAGs of tasks ( §4.2), data locality ( §4.4), and fairness ( §4.3). Finally, in §5, we discuss how to adapt the design to a decentralized setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dynamic Resource Allocation</head><p>The examples in §3 illustrate the value of dynamic allocation of slots for speculation. Our analysis indicates that this dynamic allocation can be separated into two regimes: whether the cluster is in "high" or "low" load.</p><p>The distinction between these two regimes follows from the behavior of the marginal return (in terms of performance) that jobs receive from being allocated slots. It is perhaps natural to expect that the performance of a job will always improve when it is given additional slots (because these can be used for additional speculative copies) and that the value of additional slots has a decreasing marginal return (because an extra slot is more valuable when the job is given few slots than when the job already has many slots). However, surprisingly, a novel observation that leads to the design of Hopper is that the marginal return of an extra slot has a sharp threshold (a.k.a., knee) where, below the threshold, the marginal return is large and (nearly) constant</p><formula xml:id="formula_0">1: procedure Hopper( Job J(t), int S, float β) totalVirtualSizes ← 0 2:</formula><p>for each Job j in J(t) do j.V (t) = (2/β) j.Trem j.Trem: remaining number of tasks j.V (t): virtual job size totalVirtualSizes += j.V (t) 3:</p><p>SortAscending(J(t), V (t)) 4:</p><p>if S &lt; totalVirtualSizes then 5:</p><p>for each Job j in J(t) do j.slots ← min(S, j.V (t)) S ← max(S -j.slots, 0) 6: else 7:</p><p>for each Job j in J(t) do j.slots ← (j.V (t)/totalVirtualSizes) × S Pseudocode 1: Hopper (centralized) allocating S slots to the set of jobs present at time t, J(t), with task distribution parameter β.</p><p>and, above the threshold, the marginal return is small and decreasing. Figure <ref type="figure">3</ref> illustrates this threshold using a simulation of a sample job with 200 tasks (with Pareto sizes, common in production traces) and LATE <ref type="bibr" target="#b49">[50]</ref> speculation when assigned various numbers of slots. Crucially, there is a marked change in slope beyond the vertical dashed line, indicating the change in the marginal value of a slot. Note, that such a threshold exists for different job sizes, speculation algorithms, etc. Further, in the context of a simple model, we can prove the existence of a sharp threshold <ref type="bibr" target="#b7">[8]</ref>.</p><p>The most important consequence of the discussion above is that it is desirable to ensure every job is allocated enough slots to reach the threshold (if possible) before giving any job slots beyond this threshold. Thus, we refer to this threshold as the "desired (minimum) allocation" for a job or simply the "virtual job size". Guideline 1. It is better to give resources to a job that has not reached the desired (minimum) allocation than a job that has already reached the point. This guideline yields the key bifurcation in the Hopper design, as illustrated in line 4 of Pseudocode 1. Additionally, it highlights that there are three important design questions, which we address in the following sections: (i) How can we determine the desired allocation (virtual size) of a job? (ii) How should slots be allocated when there are not enough to give each job its desired allocation, i.e., when the cluster is highly utilized? (iii) How should slots be allocated when there are more than enough to give each job its desired allocation, i.e., when the cluster is lightly utilized? (i) Determining the virtual size of a job Determination of the "desired (minimum) allocation", a.k.a., the "virtual" size, of a job is crucial to determining which regime the system is in, and thus how slots should be allocated among jobs. While the virtual job size is learned empirically by Hopper through measurements of the threshold point during operation, it is important to point out that it is also possible to derive a useful static rule of thumb analytically, which can give intuition for the design structure.</p><p>In particular, the task durations in production traces (e.g., Facebook and Bing traces described in §7) typically follow a heavy-tailed Pareto distribution, where the Pareto tail parameter β (which is often 1 &lt; β &lt; 2) represents the likelihood of stragglers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Roughly, smaller β means that stragglers are more damaging, i.e., if a task has already run for some time, there is higher likelihood of the task running longer.</p><p>Given the assumption of Pareto task durations, we can prove analytically (in a simple model) that the threshold point defining the "desired (minimum) allocation" is max(2/β, 1), which corresponds exactly to the vertical line in Figure <ref type="figure">3</ref> (see <ref type="bibr" target="#b7">[8]</ref> for details). While we show only two examples here, the estimate this provides is robust across varying number of tasks, β, etc. <ref type="foot" target="#foot_3">4</ref>Thus, we formally define the "virtual job size" V i (t) for job i at any time t, as its number of remaining tasks (T i (t)) multiplied by 2/β (since β &lt; 2 in our traces), i.e., V i (t) = 2 β T i (t). This virtual job size determines which regime the scheduler should use; see line 2 in Pseudocode 1. In practice, since β may vary over time, it is learned online by Hopper (see §7) making it adaptive to different threshold points as in Figure <ref type="figure">3</ref>.</p><p>(ii) Allocation when the cluster is highly utilized When there are not enough slots to assign every job its virtual size, we need to decide how to distribute this "deficiency" among the jobs. The scheduler could either spread the deficiency across all jobs, giving them all less opportunity for speculation, or satisfy as many jobs as possible with allocations equaling their virtual sizes.</p><p>Hopper does the latter. Specifically, Hopper processes jobs in ascending order of their virtual sizes V i (t), giving each job its desired (minimum) allocation until all the slots are exhausted (see lines 3 -5 in Pseudocode 1). This choice is in the spirit of SRPT, and is motivated both by the optimality of SRPT and the decreasing marginal return of additional slots, which magnifies the value of SRPT. Additionally, our theoretical analysis (in <ref type="bibr" target="#b7">[8]</ref>) shows the optimality of this choice in the context of a simple model. Guideline 2. At all points in time, if there are not enough slots for every job to get its desired (minimum) allocation, i.e., a number of slots equal to its virtual size, then slots should be dedicated to the smallest jobs and each should be given a number of slots equal to its virtual size.</p><p>Note that prioritization of small jobs may lead to unfairness for larger jobs, an issue we address shortly in §4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(iii) Allocation when the cluster is lightly utilized</head><p>At times when there are more slots in the cluster than the sum of the virtual sizes of jobs, we have slots left over even after allocating every job its virtual size. The scheduler's options for dividing the extra capacity are to either split the slots across jobs, or give all the extra slots to a few jobs in order to complete them quickly.</p><p>In contrast to the high utilization setting, in this situation Hopper allocates slots proportionally to the virtual job sizes, i.e., every job i receives (V i (t)/ j V j (t))S slots, where S is the number of slots available in the system and V i (t) is the virtual size; see line 7 in Pseudocode 1. Note that this is, in a sense, the opposite of the prioritization according to SRPT.</p><p>The motivation for this design is as follows. Given that all jobs are already receiving their (minimum) desired level of speculation, scheduling is less important than speculation. Thus, prioritization of small jobs is not crucial, and the goal should be to extract the maximum value from speculation. Since stragglers are more likely to occur in larger jobs (stragglers occur in proportion to the number of tasks in a job, on average<ref type="foot" target="#foot_4">5</ref> ), the marginal improvement in performance due to an additional slot is proportionally higher for large jobs. Thus, they should get prioritization in proportion to their size when allocating the extra slots. Our analytic work in <ref type="bibr" target="#b7">[8]</ref> highlights that this allocation is indeed optimal in a simple model. Guideline 3. At all points in time, if there are enough slots to give every job its desired (minimum) allocation, then, the slots should be shared "proportionally" to the virtual sizes of the jobs.</p><p>Since the guidelines specify allocations at the granularity of every job, it is easy to cope with any fluctuations in cluster load (say, from lightly to highly utilized) in an online system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorporating DAGs of Tasks</head><p>The discussion to this point has focused on singlephased jobs. In practice, many jobs are defined by multiple-phased DAGs, where the phases are typically pipelined. That is, downstream tasks do not wait for all the upstream tasks to finish but read the upstream outputs as the tasks finish, e.g., <ref type="bibr" target="#b5">[6]</ref>. Pipelining is beneficial because the upstream tasks are typically bottlenecked on other non-overlapping resources (CPU, memory), while the reading downstream takes network resources. The additional complexity DAGs create for our guidelines is the need to balance the gains due to overlapping network utilization with the improvements that come from favoring upstream phases with fewer remaining tasks.</p><p>We integrate this tradeoff into Hopper using a weighting factor, α per job, set to be the ratio of remaining work in the downstream phase's network transfer to the remaining work in the upstream phase. Specifically, α favors jobs with higher remaining communication and lower remaining tasks in the current phase. The exact details of estimating α are deferred to §6.3.</p><p>Given the weighting factor α, there are two key adjustments that we make to the guidelines discussed so far. First, in Guideline 2, the prioritization of jobs based on the virtual size V i (t) is replaced by a prioritization based on max{V i (t), V i (t)}, where V i (t) is the virtual remaining number of tasks in the current phase and V i (t) is the virtual remaining work in communication in the downstream phase. <ref type="foot" target="#foot_5">6</ref> Second, we redefine the virtual size itself as</p><formula xml:id="formula_1">V i (t) = 2 β T i (t)</formula><p>√ α i . This form follows from the analysis in <ref type="bibr" target="#b7">[8]</ref> and is similar in spirit to the optimality of square-root proportionality in load balancing across heterogeneous servers <ref type="bibr" target="#b20">[21]</ref>.</p><p>For DAGs that are not strict chains, but are wide and "bushy", we calculate α by summing over all the running and their respective downstream phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incorporating Fairness</head><p>While fairness is an important constraint in clusters, conversations with data center operators reveal that it is not an absolute requirement. Thus, we relax the notion of fairness currently employed by cluster schedulers, e.g., <ref type="bibr" target="#b46">[47]</ref>, which enforce that if there are N (t) active jobs and S available slots at time t, then each job is assigned S/N (t) slots.</p><p>Specifically, to allow some flexibility while still tightly controlling unfairness, we define a notion of approximate fairness as follows. We say that a scheduler is -fair if it guarantees that every job receives at least (1-)S/N (t) slots at all times t. The fairness knob → 0 indicates absolute fairness while → 1 focuses on performance.</p><p>Hopper can be adjusted to guarantee -fairness in a very straightforward manner. In particular, if a job receives slots less than its fair share, i.e., fewer than (1 -)S/N (t) slots, the job's capacity assignment is increased to (1 -)S/N (t). Next, the remaining slots are allocated to the remaining jobs according to Guidelines 2 or 3, as appropriate. Note that this is a form of projection from the original (unfair) allocation into the feasible set of allocations defined by the fairness constraints.</p><p>Our experimental results ( §7.3) highlight that even at moderate values of , nearly all jobs finish faster than they would have under fair scheduling. This fact, though initially surprising, is similar to the conclusions about SRPT-like policies. Despite being intuitively unfair to large job sizes, it in fact improves the average response time of every job size (when job sizes are heavytailed) compared to fair schedulers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Incorporating Data Locality</head><p>As such, the guidelines presented does not consider data locality <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48]</ref> in the scheduling of tasks. Tasks reading their data from remote machines over the network run slower. In addition, such remote reads also increase contention with other intermediate tasks (like reduce tasks) that are bound to read over the network.</p><p>We devise a simple relaxation approach for balancing adherence to our guidelines and locality. Specifically, we adjust the ordering of jobs in Guideline 2 to include information about locality. Instead of allotting slots to the jobs with the smallest virtual sizes, we allow for picking any of the smallest k% of jobs whose tasks can run with data locality on the available slots. In practice, a small value of k (≤ 5%) suffices due to high churn in task completions and slot availabilities ( §7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DECENTRALIZED Hopper</head><p>In this section, we adapt the guidelines described in §4 to design a decentralized (online) scheduler. Decentralized schedulers are increasingly prominent as cluster sizes grow. As we explain in this section, a key benefit of our guidelines in §4 is that they can be decentralized with little performance loss.</p><p>Decentralized schedulers, like the recently proposed Sparrow <ref type="bibr" target="#b35">[36]</ref>, broadly adopt the following design (see Figure <ref type="figure" target="#fig_4">4</ref>). There are multiple independent schedulers each of which is responsible for scheduling one or a subset of jobs; for simplicity, a single job never spans across schedulers. Every scheduler assigns the tasks of its jobs to machines in the cluster (referred to as workers) that executes the tasks. The architecture allows for an incoming job to be assigned to any of the available schedulers, while also seamlessly allowing new schedulers to be dynamically spawned.</p><p>A scheduler first pushes reservation requests for its tasks to workers; each request contains the identifier of the scheduler placing the request along with the remaining number of unscheduled tasks in the job. When a worker is vacant, it pulls a task from the corresponding scheduler based on the reservation requests in its waiting queue. In this framework, workers decide which job's task to run and the scheduler for the corresponding job decides which task to run within the chosen job.  Though we adopt an overall design structure similar to Sparrow for the decentralization of Hopper, it is important to note that Hopper's design is fundamentally different because it integrates straggler mitigation based on the guidelines behind Hopper introduced in §4.</p><p>Decentralizing Hopper involves the following steps: approximating worker-wide information at each scheduler ( §5.1), deciding if the number of slots are constrained ( §5.2), and calculating virtual sizes ( §5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Power of Many Choices</head><p>Decentralized schedulers have to approximate the global state of the cluster -the states of all the workers -since they are unaware of other jobs in the system. A common way to accomplish this is via the "power of two choices" <ref type="bibr" target="#b37">[38]</ref>. This celebrated and widely used result highlights that, in many cases, one nearly matches the performance of a centralized implementation by querying two workers for their queue lengths, and choosing the shorter of the queues. In fact, this intuition underlies the design of Sparrow as well, which combines the idea with a form of "late binding"; schedulers send reservation requests for every task to two workers and then let workers pull a task from the corresponding scheduler when they have a free slot. We adopt "late binding", as used in Sparrow, but replace the "power of two choices" with the "power of many choices".</p><p>The reason for this change is that the effectiveness of the "power of two choices" relies on having lighttailed task size distributions. The existence of stragglers means that, in practice, task durations are heavy-tailed, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Recent theoretical results have proven that, when task sizes are heavy-tailed, probing d &gt; 2 choices can provide orders-of-magnitude improvements <ref type="bibr" target="#b17">[18]</ref>. The value in using d &gt; 2 comes from the fact that large tasks, which are more likely under heavytailed distributions, can cause considerable backing up of worker queues. Two choices may not be enough to avoid such backed-up queues, given the high frequency of straggling tasks. More specifically, d &gt; 2 allows the schedulers to have a view of the jobs that is closer to the global view.</p><p>We use simulations in Figure <ref type="figure" target="#fig_6">5a</ref> to highlight the benefit of using d &gt; 2 probing choices in Hopper and to contrast this benefit with Sparrow, which relies on the power of two choices. Our simulation considers a cluster of 50 schedulers and 10,000 workers and jobs with Pareto distributed (β = 1.5) task sizes. Job performance with decentralized Hopper is within just 15% of the centralized scheduler; the difference plateaus beyond d = 4. Note that Sparrow (which does not coordinate scheduling and speculation) is &gt; 100% off for medium utilizations and even further off for high utilizations (not shown on the figure in order to keep the scale visible). Further, workers in Sparrow pick tasks in their waiting queues in a FCFS fashion. The lack of coordination between scheduling and speculation results in a long waiting time for speculative copies in the queues which diminishes the benefits of multiple probes. Thus parrow cannot extract the same benefit Hopper has from using more than two probes. Of course, these are rough estimates since the simulations do not capture overheads due to increased message processing, which are included in the evaluations in §7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Is the system capacity constrained?</head><p>In the decentralized setting workers implement our scheduling guidelines. Recall that Guideline 2 or Guideline 3 is applied depending on whether the system is constrained for slots or not. Thus, determining which to follow necessitates comparing the sum of virtual sizes of all the jobs and the number of slots in the cluster, which is trivial in a centralized scheduler but requires communication in an decentralized setting.</p><p>To keep overheads low, we avoid costly gossiping protocols among schedulers regarding their states. Instead, we use the following adaptive approach. Workers start with the conservative assumption that the system is capacity constrained (this avoids overloading the system with speculative copies), and thus each worker implements Guideline 2, i.e., enforces an SRPT priority on its queue. Specifically, when a worker is idle, it sends a refusable response to the scheduler corresponding to the reservation request of the job it chooses from its queue. However, since the scheduler queues many more reservation requests than tasks, it is possible that its tasks may have all been scheduled (with respect to virtual sizes). A refusable response allows the scheduler to refuse sending any new task for the job if the job's tasks are all already scheduled to the desired speculation level (ResponseProcessing in Pseudocode 2). In its refusal, it sends information about the job with the smallest virtual size in its list which still has unscheduled tasks (if such an "unsatisfied" job exists). Subsequently, the worker sends a refusable response to the scheduler corresponding to second smallest job in its queue, and so forth till it gets a threshold number of refusals. Note that the worker avoids probing the same scheduler more than once. Several consecutive refusals from schedulers without information about any unsatisfied jobs suggests that the system is not capacity constrained. At that point, it switches to implementing Guideline 3. Once it is following Guideline 3, the worker randomly picks a job from the waiting queue based on the distribution of job virtual sizes. If there are still unsatisfied jobs at the end of the refusals, the worker sends a non-refusable response (which cannot be refused) to the scheduler whose unsatisfied job is the smallest. Pseudocode 3 explains the Response method.</p><p>The higher the threshold for refusals, the better the view of the schedulers for the worker. Our simulations (with 50 schedulers and 10,000 workers) in Figure <ref type="figure" target="#fig_6">5b</ref> show that performance with two or three refusals is within 10% -15% of the centralized scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Updating Virtual Job Sizes</head><p>Computing the remaining virtual job size at a scheduler is straightforward. However, since the remaining virtual size of a job changes as tasks complete, virtual sizes need to be updated dynamically. Updating virtual sizes accurately at the workers that have queued reservations for tasks of this job would require frequent message exchanges between workers and schedulers, which would create significant overhead in communication and processing of messages. So, our approach is to piggyback updates for virtual sizes on other communication messages that are anyway necessary between a scheduler and a worker (e.g., schedulers send-ing reservation requests for new jobs, workers sending responses to probe system state and ask for new tasks). While this introduces a slight error in the virtual remaining sizes, our evaluation shows that the approximation provided by this approach is enough for the gains associated with Hopper.</p><p>Crucially, the calculation of virtual sizes is heavily impacted by the job specifics. Job specific properties of the job DAG and the likelihood of stragglers are captured through α and β, respectively, which are learned online. Note that jobs from different applications may have heterogeneous α and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">IMPLEMENTATION OVERVIEW</head><p>We now give an overview of the implementation of Hopper in decentralized and centralized settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Decentralized Implementation</head><p>Our decentralized implementation uses the Sparrow <ref type="bibr" target="#b35">[36]</ref> framework, which consists of many schedulers and workers (one each on every machine) <ref type="bibr" target="#b8">[9]</ref>. Arbitrarily many schedulers can operate concurrently; though we use 10 in our experiments. Schedulers allow submissions of jobs using Thrift RPCs <ref type="bibr" target="#b0">[1]</ref>.</p><p>A job is broken into a set of tasks with their dependencies (DAG), binaries and locality preferences. The scheduler places requests at the workers for its tasks; if a task has locality constraints, its requests are only placed on the workers meeting its constraints <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>. The workers talk to the client executor processes (e.g., Spark executor). The executor processes are responsible for executing task binaries and are long-lived to avoid startup overheads (see <ref type="bibr" target="#b35">[36]</ref> for a more detailed explanation).</p><p>Our implementation modifies the scheduler as well as the worker. The workers implement the core of the guidelines in §4 -determining if the system is slotconstrained and accordingly prioritizing jobs as per their virtual sizes. This required modifying the FIFO queue at the worker in Sparrow to allow for custom ordering of the queued requests. The worker, nonetheless, augments its local view by coordinating with the scheduler. This involved modifying the "late binding" mechanism both at the worker and scheduler. The worker, when it has a free slot, works with the scheduler in picking the next task (using Pseudocode 3). The scheduler deals with a response from the worker as per Pseudocode 2.</p><p>Even after all the job's tasks have been scheduled (including its virtual size), the job scheduler does not "cancel" its pending requests; there will be additional pending requests with any probe ratio over one. Thus, if the system is not slot-constrained, it would be able to use more slots (as per Guideline 3).</p><p>In our decentralized implementation, for tasks in the input phase (e.g., map phase), when the number of probes exceeds the number of data replicas, we queue up the additional requests at randomly chosen machines.</p><p>Consequently, these tasks may run without data locality, and our results in §7 include such loss in locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Centralized Implementation</head><p>We implement Hopper inside two centralized frameworks: Hadoop YARN (version 2.3) and Spark (version 0.7.3). Hadoop jobs read data from HDFS <ref type="bibr" target="#b4">[5]</ref> while Spark jobs read from in-memory RDDs.</p><p>Briefly, these frameworks implement two level scheduling where a central resource manager assigns slots to the different job managers. When a job is submitted to the resource manager, a job manager is started on one of the machines, that then executes the job's DAG of tasks. The job manager negotiates with the resource manager for resources for its tasks.</p><p>We built Hopper as a scheduling plug-in module to the resource manager. This makes the frameworks use our design to allocate slots to the job managers. We also piggybacked on the communication protocol between the job manager and resource manager to communicate the intermediate data produced and read by the phases of the job to vary α accordingly; locality and other preferences are already communicated between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Estimating Intermediate Data Sizes</head><p>Recall from §4.2 that our scheduling guidelines recommend scaling every job's allocation by √ α in the case of DAGs. The purpose of the scaling is to capture pipelining of the reading of upstream tasks' outputs.</p><p>The key to calculating α is estimating the size of the intermediate output produced by tasks. Unlike the job's input size, intermediate data sizes are not known upfront. We predict intermediate data sizes based on similar jobs in the past. Clusters typically have many recurring jobs that execute periodically as newer data streams in, and produce intermediate data of similar sizes.</p><p>Our simple approach to estimating α works sufficiently well for our evaluations (accuracy of 92%, on average). However, we realize that workloads without many multiwaved or recurring jobs and without tasks whose duration is dictated by their input sizes, need more sophisticated models of task executions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EVALUATION</head><p>We evaluate our prototypes of Hopper -with both decentralized and centralized scheduling -on a 200 machine cluster. We focus on the overall gains of the decentralized prototype of Hopper in §7.2 and evaluate the design choices that led to Hopper in §7.3. Then, in §7.4 we evaluate the gains with Hopper in a centralized scheduler in order to highlight the value of coordinating scheduling and speculation. The key highlights are:</p><p>1. Hopper's decentralized prototype improves the average job duration by up to 66% compared to an aggressive decentralized baseline that combines Sparrow with SRPT ( §7.2).</p><p>2. Hopper ensures that only 4% of jobs slow down compared to Fair scheduling, and jobs which do slow down do so by ≤ 5% ( §7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Centralized</head><p>Hopper improves job completion times by 50% compared to centralized SRPT ( §7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Setup</head><p>Cluster Deployment: We deploy our prototypes on a 200-node private cluster. Each machine has 16 cores, 34GB of memory, 1Gbps network and 4 disks. The machines are connected using a network with no oversubscription. 7  Workload: Our evaluation runs jobs in traces from Facebook's production Hadoop <ref type="bibr" target="#b2">[3]</ref> cluster (3, 500 machines) and Microsoft Bing's Dryad cluster (O(1000) machines) from Oct-Dec 2012. The traces consist of a mix of experimental and production jobs. Their tasks have diverse resource demands of CPU, memory and IO, varying by a factor of 24× (refer to <ref type="bibr" target="#b26">[27]</ref> for detailed quantification). We retain the inter-arrival times of jobs, their input sizes and number of tasks, resource demands, and job DAGs of tasks. Job sizes follow a heavy-tailed distribution (quantified in detail in <ref type="bibr" target="#b11">[12]</ref>). Each experiment is a replay of a representative 6 hour slice from the trace. It is repeated five times and we report the median.</p><p>To evaluate our prototype of decentralized Hopper, we use in-memory Spark <ref type="bibr" target="#b48">[49]</ref> jobs. These jobs are typical of interactive analytics whose tasks vary from sub-second durations to a few seconds. Since the performance of any decentralized scheduler depends on the cluster utilization, we speed-up the trace appropriately, and evaluate on (average) utilizations between 60% and 90%, consistent with Sparrow <ref type="bibr" target="#b35">[36]</ref>. Stragglers: The stragglers in our experiments are those that occur naturally, i.e., not injected via any model of a probability distribution or via statistics gathered from the Facebook and Bing clusters. Importantly, the frequency and lengths of stragglers observed in our evaluations are consistent with prior studies, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b49">50]</ref>. While Hopper's focus is not on improving straggler mitigation algorithms, our experiments certainly serve to emphasize the importance of such mitigation. Baseline: We compare decentralized Hopper to Sparrot-SRPT, an augmented version of Sparrow <ref type="bibr" target="#b35">[36]</ref>. Like Sparrow, it performs decentralized scheduling using a "batched" power-of-two choices. In addition, it also includes an SRPT heuristic. In short, when a worker has a slot free, it picks the task of the job that has the least unfinished tasks (instead of the standard FIFO ordering in Sparrow). Finally, we combine Sparrow with LATE <ref type="bibr" target="#b49">[50]</ref> using "best effort"speculation ( §3); we do not consider "budgeted" speculation due to the difficulty of picking a fixed budget.</p><p>The combination of Sparrow-SRPT and LATE performs strictly better than Sparrow, and serves as an ag- 7 Results with a 10Gbps network are qualitatively similar.   gressive baseline. Our improvements over this aggressive benchmark highlight the importance of coordinating scheduling and speculation.</p><p>We compare centralized Hopper to a centralized SRPT scheduler with LATE speculation. Again, this is an aggressive baseline since it sacrifices fairness for performance. Thus, improvements can be interpreted as coming solely from better coordination of scheduling and speculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Decentralized Hopper's Improvements</head><p>In our experiments, unless otherwise stated, we set the fairness allowance as 10%, probe ratio as 4 and speculation algorithm in every job to be LATE <ref type="bibr" target="#b49">[50]</ref>. Our estimation of α ( §6.3) has an accuracy of 92% on average. As the workload executes, we also continually fit the parameter β of task durations based on the completed tasks (including stragglers); the error in β's estimate falls to ≤ 5% after just 6% of the jobs have executed. Overall Gains: Figure <ref type="figure" target="#fig_8">6</ref> plots Hopper's gains for varying utilizations, compared to stock Sparrow and Sparrow-SRPT. Jobs, overall, speedup by 50% -60% at utilization of 60%. The gains compared to Sparrow are marginally better than Sparrow-SRPT. When the utilization goes over 80%, Hopper's gains compared to both are similar. An interesting point is that Hopper's gains with the Bing workload in Figure <ref type="figure" target="#fig_8">6b</ref> are a touch higher (difference of 7%), perhaps due to the larger difference in job sizes between small and large jobs, allowing more opportunity for Hopper. Gains fall to &lt; 20% when utilization is high (≥ 80%), naturally because there is not much room for any optimization at that occupancy.  While not plotted, gains at utilizations ≤ 30% are no more than 14%. Expectedly, at such low utilizations, there is little requirement for smarter speculation or probing.</p><p>Note that the above utilizations are on average and there is considerable variation. At 80% average utilization, Hopper allocates 53% of jobs using Guideline 2 (high utilization) and the remaining 47% of jobs using Guideline 3 (low utilization). This indicates that 53% of jobs in the experimental run arrived such that the cluster did not have enough slots to allocate every job its virtual size.</p><p>The results so far highlight that Sparrow-SRPT is a more aggressive baseline than Sparrow, and so we compare only to it for the rest of our evaluation. Job Bins: Figure <ref type="figure" target="#fig_9">7</ref> dices the gains by job size (number of tasks). Gains for small jobs are less compared to large jobs. This is expected given that our baseline of Sparrow-SRPT already favors the small jobs. Nonetheless, Hopper's smart allocation of speculative slots offers 18% -32% improvement. Gains for large jobs, in contrast, are over 50%. This not only shows that there is sufficient room for the large jobs despite favoring small jobs (due to the heavy-tailed distribution of job sizes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>) but also that the value of deciding between speculative tasks and unscheduled tasks of other jobs increases with the number of tasks in the job. With trends of smaller tasks and hence, larger number of tasks per job <ref type="bibr" target="#b33">[34]</ref>, Hopper's allocation becomes important. Distribution of Gains: Figure <ref type="figure" target="#fig_11">8a</ref> plots the distribution of gains across jobs. While the median gains are just higher than the average, there are &gt; 70% gains at higher percentiles. Encouragingly, gains even at the 10 th percentile are 15% and 10%, which shows Hopper's ability to improve even worse case performance. DAG of Tasks: The scripts in our Facebook (Hive scripts <ref type="bibr" target="#b6">[7]</ref>) and Bing (Scope <ref type="bibr" target="#b19">[20]</ref>) workloads produce DAGs of tasks which often pipeline data transfers of downstream phases with upstream tasks <ref type="bibr" target="#b5">[6]</ref>. The communication patterns in the DAGs are varied (e.g., allto-all, many-to-one etc.) and thus the results also serve to underscore Hopper's generality. As Figure <ref type="figure" target="#fig_11">8b</ref> shows, Hopper's gains hold across DAG lengths.  Speculation Algorithm: We now experimentally evaluate Hopper's performance with different speculation mechanisms. LATE <ref type="bibr" target="#b49">[50]</ref> is deployed in Facebook's clusters, Mantri <ref type="bibr" target="#b14">[15]</ref> is in operation in Microsoft Bing, and GRASS citegrass is a recently reported straggler mitigation system that was demonstrated to perform nearoptimal speculation. Our experiments still use Sparrow-SRPT as the baseline but pair with the different straggler mitigation algorithms. Figure <ref type="figure" target="#fig_13">9</ref> plots the results.</p><p>While the earlier results were achieved in conjunction with LATE, a remarkable point about Figure <ref type="figure" target="#fig_13">9</ref> is the similarity in gains even with Mantri and GRASS. This indicates that as long as the straggler mitigation algorithms are aggressive in asking for speculative copies, Hopper will appropriately balance speculation and scheduling. Overall, it emphasizes the aspect that resource allocation across jobs (with speculation) has a higher performance value than straggler mitigation within jobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluating Hopper's Design Decisions</head><p>We now evaluate the sensitivity of decentralized Hopper to our key design decisions: fairness and probe ratio. Fairness: As we had described in §4.3, the fairness knob decides the leeway for Hopper to trade-off fairness for performance. Thus far, we had set to be 10% of the perfectly fair share of a job (ratio of total slots to jobs), now we analyze its sensitivity to Hopper's gains.</p><p>Figure <ref type="figure" target="#fig_15">10a</ref> plots the increase in gains as we increase from 0 to 30%. The gains quickly rise for small values of , and beyond = 15% the increase in gains are flatter with both the Facebook as well as Bing workloads. Conservatively, we set to 10%.</p><p>An important concern, nonetheless, is the amount of slowdown of jobs compared to a perfectly fair allocation ( = 0), i.e., when all the jobs are guaranteed their fair share at all times. Any slowdown of jobs is because of receiving fewer slots. Figure <ref type="figure" target="#fig_15">10b</ref> measures the number of jobs that slowed down, and for the slowed jobs, Figure <ref type="figure" target="#fig_15">10c</ref> plots their average and worst slowdowns. Note that fewer than 4% of jobs slow down with Hopper compared to a fair allocation at = 10%. The corresponding number for the Bing workload is 3.8%. In fact, both the average and worst slowdowns are limited at = 10%, thus demonstrating that Hopper's focus on performance does not unduly slow down jobs. Probe Ratio: An important component of decentralized scheduling is the probe ratio -the number of re-    quests queued at workers to number of tasks in the job. A higher probe ratio reduces the chance of a task being stuck in the queue of a busy machine, but also increases messaging overheads. While the power-of-two choices <ref type="bibr" target="#b37">[38]</ref> and Sparrow <ref type="bibr" target="#b35">[36]</ref> recommend a probe ratio of 2, we adopt a probe ratio of 4 based on our analysis in §5.</p><p>Figure <ref type="figure" target="#fig_17">11</ref> confirms that higher probe ratios are indeed beneficial. As the probe ratio increase from 2 onwards, the payoff due to Hopper's scheduling and straggler mitigation results in gains increasing until 4; at utilizations of 70% and 80%, using 3.5 works well too. At 90% utilization, however, gains start slipping even at a probe ratio of 2.5. However, the benefits at such high utilizations are smaller to begin with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Centralized Hopper's Improvements</head><p>To highlight the fact that Hopper is a unified design, appropriate for both decentralized and centralized systems, we also evaluate Hopper in a centralized setting using Hadoop and Spark prototypes. Figure <ref type="figure" target="#fig_2">12</ref> plots the gains for the two prototypes with Facebook and Bing workloads. We achieve gains of ∼ 50% with the  two workloads, with individual job bins improving by up to 80%.</p><p>As with the decentralized setting, gains for small jobs are lower due to the baseline of SRPT already favoring small jobs. Between the two prototypes, gains for Spark are consistently higher (albeit, modestly). Spark's small task durations makes it more sensitive to stragglers and thus it spawns many more speculative copies. This makes Hopper's scheduling more crucial. DAG of Tasks: Like in the decentralized implementation, Hopper's gains hold consistently over varying DAG lengths, see Figure <ref type="figure" target="#fig_2">12</ref>. Note that there is a contrast between Spark jobs and Hadoop jobs. Spark jobs have fast in-memory map phases, thus making intermediate data communication the bottleneck. Hadoop jobs are less bottlenecked on intermediate data transfer, and spend more of their time in the map phase <ref type="bibr" target="#b12">[13]</ref>. This difference is captured via α, which is learned as described in §6.3. Data Locality: Recall from §4.4 that we achieve data locality using a relaxation heuristic to allow any k subsequent jobs (as a % of total jobs).</p><p>As Figure <ref type="figure" target="#fig_22">13a</ref> shows, a small relaxation of k = 3%, which is what we have used so far, achieves appreciable increase in locality in Spark. Gains are steady for a bit but then start dropping beyond a k value of 7%. This is because the deviation from the theoretical guidelines overshadows any increase in gains from locality. The fraction of data local tasks, naturally, increases with k (Figures 13a). Note that even when we enhance a centralized SRPT scheduler to include the above locality heuristic, it gains no more than 20% compared to centralized SRPT (without the locality heuristic). This indicates that Hopper's gains are predominantly due to coordinated speculation and scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>With launching speculative copies of tasks being a common approach for mitigating the impact of stragglers, schedulers face a decision between scheduling speculative copies of some jobs versus original copies of other jobs. While this question is seemingly simple, we find  that the problem is not only unsolved thus far, but also has significant performance implications. This paper proposes Hopper, the first speculation-aware job scheduler, and implements both decentralized and centralized prototypes. We deploy our prototypes (built in Sparrow <ref type="bibr" target="#b35">[36]</ref>, Spark <ref type="bibr" target="#b48">[49]</ref> and Hadoop <ref type="bibr" target="#b2">[3]</ref>) on a 200 machine cluster, and see job speed ups of 66% in decentralized settings and 50% in centralized settings compared to current state-of-the-art schedulers. In addition to providing performance improvements in both centralized and decentralized settings, Hopper is compatible with all current speculation algorithms and incorporates data locality, fairness, DAGs of tasks, etc.; thus, it represents a unified speculation-aware scheduling framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Best-effort Speculation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Combining SRPT scheduling and speculation for two jobs A (4 tasks) and B (5 tasks) on a 7-slot cluster. The + suffix indicates speculation. Copies of tasks that are killed are colored red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 ( 6 Figure 3 :</head><label>263</label><figDesc>Figure 3: The impact of number of slots on single job performance. The number of slots is normalized by job size (number of tasks within the job). β is the Pareto shape parameter for the task size distribution. (In our traces 1 &lt; β &lt; 2.) The red vertical line shows the threshold point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Decentralized scheduling architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The impact of number of probes and number of refusals on Hopper's performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hopper's gains with cluster utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hopper's gains by job bins over Sparrow-SRPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (a) CDF of Hopper's gains, and (b) gains as the length of the job's DAG varies; both at 60% utilization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>. LATE + Sparrow-SRPT Mantri + Hopper vs. Mantri + Sparrow-SRPT GRASS + Hopper vs. GRASS + Sparrow-SRPT Job Bin (Number of tasks) Reduction (%) in Average Job Duration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Hopper's results are independent of the straggler mitigation strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Fairness. Figure (a) shows sensitivity of gains to . Figure (b) shows the fraction of jobs that slowed down compared to a fair allocation, and (c) shows the magnitude of their slowdowns (average and worst).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Power of d choices: Impact of the number of probes on job completion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Centralized Hopper's gains over SRPT, overall and broken by DAG length (Facebook workloads).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Hadoop results are similar (13b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Centralized Hopper: Impact of Locality Allowance (k) (see §6.2) with Facebook workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>torig and tnew</figDesc><table><row><cell>are durations of the</cell></row><row><cell>original and specula-</cell></row><row><cell>tive copies of each task.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>At time 10, when A1 finishes, the job scheduler allocates the slot to job A because its remaining processing is smaller than job B's. Job A speculates task A4 because A4's trem = torig-currentTime =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>30-10 = 20 &gt; tnew = 10 (see Table1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For ease of exposition, Pseudocode 1 considers the (online) allocation of all slots to jobs present at time t. Of course, in the implementation, slots are allocated as they become available. See Pseudocode 2 and 3 for more details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We make the simplifying assumption that task durations of each job are also Pareto distributed. A somewhat surprising aspect given the typical values of β (1 &lt; β &lt; 2) is that even when so many slots are allocated for redundant speculative copies, faster "clearing" of tasks is overall beneficial.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Machines in the cluster are equally likely to cause a straggler<ref type="bibr" target="#b11">[12]</ref>; known problematic machines are already blacklisted (see §2).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Results in<ref type="bibr" target="#b30">[31]</ref> show that picking the max{Ti(t), T i (t)} is 2speed optimal for completion times when stragglers are not considered.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGMENT</head><p>We would like to thank Michael Chien-Chun Hung, Shivaram Venkataraman, Masoud Moshref, Niangjun Chen, Qiuyu Peng, and Changhong Zhao for their insightful discussions. We would like to thank the anonymous reviewers and our shepherd, Lixin Gao, for their thoughtful suggestions. This work was supported in part by National Science Foundation (NSF) with Grants (CNS-1319820, CNS-1423505).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Thrift</surname></persName>
		</author>
		<ptr target="https://thrift.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cloudera</forename><surname>Impala</surname></persName>
		</author>
		<ptr target="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://hadoop.apache.org" />
		<title level="m">Hadoop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://hadoop.apache.org/docs/r1.2.1/capacityscheduler.html" />
		<title level="m">Hadoop Capacity Scheduler</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://hadoop.apache.org/hdfs" />
		<title level="m">Hadoop Distributed File System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hadoop</forename><surname>Slowstart</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/MAPREDUCE-1184/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="http://wiki.apache.org/hadoop/Hive" />
		<title level="m">Hive</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hopper</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/sigcommhoppertechreport/" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sparrow</surname></persName>
		</author>
		<ptr target="https://github.com/radlab/sparrow" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/" />
		<title level="m">The Next Generation of Apache Hadoop MapReduce</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coping with Skewed Popularity Content in MapReduce Clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><surname>Scarlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective Straggler Mitigation: Attack of the Clones</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PACMan: Coordinated Memory Caching for Parallel Jobs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GRASS: Trimming Stragglers in Approximation Analytics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reining in the Outliers in Map-Reduce Clusters Using Mantri</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting Execution Bottlenecks in Map-Reduce Clusters</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bortnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotCloud</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Apollo</surname></persName>
		</author>
		<title level="m">Scalable and Coordinated Scheduling for Cloud-Scale Computing. In USENIX OSDI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randomized load balancing with general service time distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sigmetrics</title>
		<meeting>Sigmetrics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SCOPE: Easy and Efficient Parallel Processing of Massive Datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the Impact of Heterogeneity and Back-end Scheduling in Load Balancing Designs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Achieving Rapid Response Times in Large Online Services</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Berkeley AMPLab Cloud Seminar</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decentralized Task-aware Scheduling for Data Center Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dogar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dominant Resource Fairness: Fair Allocation of Multiple Resource Types</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Resource Packing for Cluster Schedulers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Size-based scheduling to improve web performance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quincy: Fair Scheduling for Distributed Computing Clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint Optimization of Overlapping Phases in MapReduce</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dremel: Interactive Analysis of Web-Scale Datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gubarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vassilakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On Scheduling in Map-reduce and Flow-shops</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SPAA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Case for Tiny Tasks in Compute Clusters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotOS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Making Sense of Performance in Data Analytics Frameworks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparrow: Distributed, Low Latency Scheduling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Online scheduling. Handbook of scheduling: algorithms, models, and performance analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pruhs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sitaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The power of two random choices: A survey of techniques and results</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A proof of the optimality of the shortest remaining processing time discipline</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schrage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="687" to="690" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling and Synthesizing Task Placement Constraints in Google Compute Clusters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chudnovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rifaat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOCC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Delay Tails in MapReduce Scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Preemptive ReduceTask Scheduling for Fast and Fair Job Completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>USENIX ICAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fairness and scheduling in single server queues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surveys in Operations Research and Management Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classifying scheduling policies with respect to unfairness in an m/gi/1</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="238" to="249" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">FLEX: a Slot Allocation Scheduling Optimizer for MapReduce Workloads</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balmin</surname></persName>
		</author>
		<editor>Middleware</editor>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wrangler: Predictable and Faster Jobs using Fewer Resources</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SoCC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Job scheduling for multi-user mapreduce clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno>UCB/EECS-2009-55</idno>
	</analytic>
	<monogr>
		<title level="m">UC Berkeley</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM EuroSys</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improving MapReduce Performance in Heterogeneous Environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
