<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modality Person Re-Identification via Modality-aware Collaborative Ensemble Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qingming</forename><surname>Leng</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">Cross-Modality Person Re-Identification via Modality-aware Collaborative Ensemble Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8AC49F777FA6E7BCC2459189B5D061A3</idno>
					<idno type="DOI">10.1109/TIP.2020.2998275</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visible thermal person re-identification (VT-ReID) is a challenging cross-modality pedestrian retrieval problem due to the large intra-class variations and modality discrepancy across different cameras. Existing VT-ReID methods mainly focus on learning cross-modality sharable feature representations by handling the modality-discrepancy in feature level. However, the modality difference in classifier level has received much less attention, resulting in limited discriminability. In this paper, we propose a novel modality-aware collaborative ensemble (MACE) learning method with middle-level sharable two-stream network (MSTN) for VT-ReID, which handles the modality-discrepancy in both feature level and classifier level. In feature level, MSTN achieves much better performance than existing methods by capturing sharable discriminative middlelevel features in convolutional layers. In classifier level, we introduce both modality-specific and modality-sharable identity classifiers for two modalities to handle the modality discrepancy. To utilize the complementary information among different classifiers, we propose an ensemble learning scheme to incorporate the modality sharable classifier and the modality specific classifiers. In addition, we introduce a collaborative learning strategy, which regularizes modality-specific identity predictions and the ensemble outputs. Extensive experiments on two cross-modality datasets demonstrate that the proposed method outperforms current state-of-the-art by a large margin, achieving rank-1/mAP accuracy 51.64%/50.11% on the SYSU-MM01 dataset, and 72.37%/69.09% on the RegDB dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Person re-identification (Re-ID) is a specific pedestrian retrieval task, which aims at matching person images captured from different non-overlapping cameras <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It has gained increasing attention due to its importance in computer vision research community and practical video surveillance applications <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Existing person Re-ID mainly focuses on single-modality module, where all the person images are captured by visible cameras in the daytime. Encouraging performance with deep neural networks has been achieved in both image-based <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref> and video-based person Re-ID tasks [10]- <ref type="bibr" target="#b11">[12]</ref>, achieving more than 95% rank-1 recognition accuracy in most benchmarks. However, the general visible RGB cameras cannot capture valid appearance information under low-illumination environment, e.g., at night (Fig. <ref type="figure" target="#fig_0">1 (a)</ref>). In comparison, many new-generation surveillance cameras can M. Ye and J. Shen are with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. E-mail: mangye16@gmail.com, shenjianbingcg@gmail.com. (Corresponding Author: Jianbing Shen) X. Lan is with the Department of Computer Science, Hong Kong Baptist University, Hong Kong. E-mail: xiangyuanlan@life.hkbu.edu.hk Q. Leng is with the Jiujiang University, Jiangxi, China. E-mail: lengqingming@126.com. automatically switch to infrared mode to capture the person images at night <ref type="bibr" target="#b12">[13]</ref>. Therefore, in this paper, we focus on the cross-modality visible thermal person re-identification (VT-ReID) <ref type="foot" target="#foot_0">1</ref> problem <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which plays an important role in practical night-time video surveillance applications.</p><p>Given a query thermal image captured at night by a thermal/infrared camera, VT-ReID aims at searching out the corresponding visible images from a gallery set captured in the daytime, which represent the same identity as query. An illustration about the VT-ReID problem is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. To our best knowledge, VT-ReID has been rarely studied due to the significant visual differences between two modalities caused by modality discrepancy and large intra-class variations. The modality discrepancy is usually caused by different wavelength ranges in different camera modules, which results in huge visual difference in the visual information between two modalities. In addition, different viewpoints, poses and self-occlusions yield very large intra-class variations for crossmodality VT-ReID. In addition, the person images are usually captured in different environments, i.e., indoor or outdoor applications, which brings in additional difficulties. Related cross-modality matching problem has been extensively studied in VIS-NIR face recognition <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, the visual appearance variations of the person images in VT-ReID are much larger than that of face images, which makes their methods less competitive for VT-ReID task <ref type="bibr" target="#b13">[14]</ref>.</p><p>For VT-ReID, several pioneer works have been proposed</p><p>to address the modality discrepancy and large intra-class variations. A deep zero-padding network is introduced to learn modality-sharable features by adaptively handling the modality input in <ref type="bibr" target="#b13">[14]</ref>. cmGAN is proposed in <ref type="bibr" target="#b12">[13]</ref> to simultaneously discriminate the identities and modalities with adversarial training. Ye et al. introduced a dual-constrained top-ranking loss with a two-stream network in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>. D 2 RL <ref type="bibr" target="#b20">[21]</ref> is the current state-of-the-art by handling the modality discrepancy in both pixel level and feature level. However, all of them usually learn the cross-modality feature representations with modality-sharable classifier <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The modality discrepancy issue in classifier level is not well addressed in their methods, resulting in limited performance for VT-ReID <ref type="bibr" target="#b61">[62]</ref>. In addition, previous works usually adopt two-stream network with shared embedding layer to learn modalitysharable features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, which can only capture high-level sharable information. The discriminative sharable information in middle-level convolutional layers is ignored.</p><p>To address above limitations, we introduce modality-aware collaborative ensemble (MACE) learning with a middle-level sharable two-stream network (MSTN) for VT-ReID. Our basic idea is to handle the modality discrepancy in both feature-level and classifier-level. Specifically, MSTN aims at learning modality-aware feature representations with partially shared network structures. The improvement mainly lies in the sharable convolutional blocks to capture discriminative middle-level features, not just high-level features. In classifier level, we introduce both modality-sharable and modalityspecific classifiers to guide the feature learning. On one hand, the modality-sharable classifier aims at capturing the sharable information. On the other hand, the modality-specific classifiers learn two separate identity classifiers for two different modalities to handle the modality discrepancy. In addition, we introduce an ensemble learning strategy by combining all the prediction outputs of different classifiers to formulate an enhanced teacher ensemble. To facilitate knowledge transfer among different classifiers, we adopt the knowledge distillation technique introduced in <ref type="bibr" target="#b23">[24]</ref> for collaborative learning. It improves the performance by utilizing the relationship between the modality-specific classifiers and the teacher ensemble with a consistency regularization.</p><p>The main contributions can be summarized as follows:</p><p>• We propose a novel modality-aware collaborative ensemble (MACE) learning method with an improved middle-level sharable two-stream network (MSTN) for cross-modality VT-ReID. We demonstrate that handling modality-discrepancy in both feature level and classifier level consistently is important for VT-ReID. And the proposed MSTN also greatly improves performance of other VT-ReID methods. • We introduce a collaborative ensemble learning scheme to utilize the relationship among different classifiers. It enhances the discriminability with the ensemble outputs and their consistency. • We outperform current state-of-the-arts by a large margin on two cross-modality person Re-ID datasets, which greatly accelerates the cross-modality Re-ID research.</p><p>A preliminary conference version has been published in <ref type="bibr" target="#b24">[25]</ref>. We have made three major improvements in this journal version: Firstly, we present a middle-level sharable two-stream network structure to learn better multi-modality sharable features, which provides a strong baseline for the cross-modality person Re-ID task by learning sharable middle-level convolutional features. It also greatly improves previous state-of-theart methods. Secondly, we introduce a collaborative ensemble learning strategy to improve the performance by facilitating the knowledge transfer among different classifiers. The ensemble provides a better comprehensive model learning guidance for different classifiers. Meanwhile, the improved method also contains fewer hyper parameters but achieves much better performance than our previous conference version. Finally, more comprehensive analysis is conducted to discuss the superiority and limitations of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Single-Modality Person Re-ID. Person re-identification (Re-ID) addresses the problem of matching person images across non-overlapping visible cameras <ref type="bibr" target="#b25">[26]</ref>. The key challenges of person Re-ID task mainly lie in the large intra-class variation caused by different camera views, poses variations, illuminations changes and occlusions <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Existing methods can be categorized into feature learning and metric learning methods. The former feature learning methods mainly focus on discriminative and robust feature representation learning by utilizing the human body structure <ref type="bibr" target="#b29">[30]</ref>. The latter metric learning methods usually aim at learning discriminative distance measurements to make sure the positive distance is much smaller the negative distance <ref type="bibr" target="#b30">[31]</ref>. Recently, Re-ID works have achieved inspiring performance with the deep end-to-end learning CNN network <ref type="bibr" target="#b31">[32]</ref>, and some of them have already outperformed the human-level performance on the widely-used datasets <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. However, most of existing methods are developed for single visible modality module, i.e., the person images are collected by RGB cameras in the daytime under well lighting conditions, and they usually cannot perform well for the night-time cross-modality person Re-ID task <ref type="bibr" target="#b13">[14]</ref>, which limits applicability in practical surveillance.</p><p>Multi-Modality Person Re-ID. Multi-modality person Re-ID has been extensively studied by combing the multiple modality information to improve the single modality person Re-ID <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Specifically, the depth information is adopted to improve the single RGB modality person Re-ID in <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In addition, some researchers also try to combine the thermal information to provide additional appearance information under low-lighting conditions in <ref type="bibr" target="#b14">[15]</ref>. Considering the semantic attributes as another modality cue, it has been widely used to improve the performance with single visual feature representation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. Usually, combining additional modality information achieves better performance than solely using single modality information. However, it usually require additional cost to collect or mine the multi-modality information. In comparison, this paper mainly focuses on cross-modality person Re-ID. The main target is to match person images across different modalities rather than combing different modality information.</p><p>1057-7149 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing</p><p>Cross-Modality Person Re-ID. Cross-modality person Re-ID matching person images across different modalities, i.e., text-to-image pedestrian retrieval <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> or visible-tothermal matching <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Different from the text-toimage retrieval, the modality discrepancy in VT-ReID is totally different. Therefore, the methods designed from text-to-image retrieval are usually unsuitable for our VT-ReID problem.</p><p>For VT-ReID, a zero-padding strategy with one-stream network is proposed to adaptively learn the cross-modality feature representations in <ref type="bibr" target="#b13">[14]</ref>. Later on, a two stream network with dual-constrained top-ranking loss is introduced in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref> to simultaneously handle the cross-and intra-modality variations. Besides, an adversarial learning framework with triplet loss is presented in <ref type="bibr" target="#b12">[13]</ref>, which jointly discriminates the identity information and the modality information. Recently, a dual-level discrepancy method is proposed to reduce the modality discrepancy in both feature level and image image level <ref type="bibr" target="#b20">[21]</ref>. Meanwhile, some other papers also try to investigate a better loss function for this cross-modality person Re-ID task <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Most of these methods have ignored the classifier discrepancy in different modalities, which limits their performance <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Meanwhile, their baseline networks are not well designed for this task (usually less than 35% rank-1 accuracy on the large-scale SYSU-MM01 dataset). In this paper, we present a two-stream network with modality-aware learning in both feature and classifier level.</p><p>Heterogeneous Face Recognition. In a more general perspective, heterogeneous face recognition has been extensively studied in photo-to-sketch <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref> and NIR-VIS module <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. To reduce the modality discrepancy, early research mainly focuses on learning modality-sharable or modality specific metrics or dictionaries <ref type="bibr" target="#b47">[48]</ref>. With deep learning, most of them try to learn the modality-sharable feature representations or cross-modality matching models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Compared to the NIR-VIS face recognition problem, VT-ReID also shares the same module by matching visible and thermal images of the same identity <ref type="bibr" target="#b18">[19]</ref>. However, VT-ReID suffers from much larger modality difference due to the different camera environments and the visual difference. The modality discrepancy is much more challenging than the NIR-VIS face recognition problem. Therefore, the methods designed for NIR-VIS face recognition usually have limited performance for our cross-modality person Re-ID task <ref type="bibr" target="#b13">[14]</ref>.</p><p>Collaborative Ensemble Learning. Collaborative learning aims at training an improved network with multiple classifiers, where these classifiers collaboratively improve the feature learning performance by using the same network structure <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. The output predictions of multiple classifiers can provide supplementary information for each other. Inspired by this idea, we propose to formulate a teacher ensemble by combing the outputs of modality-sharable classifiers and modality-specific classifiers. In addition, we introduce collaborative learning scheme to incorporate the the teacher ensemble with the modality-specific classifier output to improve the cross-modality person Re-ID performance.  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> 1 Conv Block Comparison between the widely-used two-stream network in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> and our improved middle-level sharable two-stream network (MSTN). We use the widely-used ResNet50 for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. Overview</head><p>Our proposed method mainly contains three parts: 1) Feature-Level Modality-aware Learning, we introduce a middle-level sharable two-stream network for feature learning, which addresses the feature-level discrepancy with partially independent and sharable network structures. 2) Classifier-Level Modality-aware Learning, we propose a modality-aware classifier learning strategy, which simultaneously uses the modality-sharable and modality-specific classifiers to handle the modality discrepancy in classifier level. 3) Collaborative Ensemble Learning, we design a collaborative ensemble learning method to facilitate the feature learning by utilizing the relationship among different classifiers. Finally, we will present our overall loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature-Level Modality-aware Learning</head><p>We firstly introduce the feature-level modality-aware learning with an improved two-stream network, termed as MST-N. To simultaneously handle the modality discrepancy and mine modality-sharable information at feature level, we use a two-stream CNN network with partially shared structures for feature learning. Specifically, the network parameters of shallow convolutional layers are specific to capture modalityspecific low-level feature patterns. Meanwhile, the network parameters of deep convolutional layers are shared to learn modality-sharable middle-level feature representations. After the convolutional layers with adaptive pooling, a shared batch normalization layer is added to learn the shared feature embedding. Note that the output of shared batch normalization layer is used for the feature representation in testing process. In this manner, MSTN learns modality-sharable middle-level features while capturing the modality-specific low-level information.</p><p>Different from the two-stream network used in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, our proposed MSTN has two main modifications: A modality sharable-classifier loss L 0 is adopted to guide the learning process. To further handle the modality discrepancy, we introduce two modality-specific classifier losses (L v id and L t id ). To facilitate knowledge transfer among different classifiers, we introduce a collaborative ensemble learning scheme, which contains an ensemble learning loss L e and a consistency collaboration loss L c .</p><p>• Shared convolutional blocks. In <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, all the network parameters in the convolution blocks are specific. However, this strategy cannot capture the shared middle-level feature patterns in convolutional layers. In comparison, we only utilize one single domain-specific convolutional block 2 to capture the modality-specific information and the left four residual blocks are shared in both modalities.</p><p>Our improved MSTN learns better features by mining sharable information in middle-level convolutional blocks for cross-modality person Re-ID. • Feature embedding layer. Similar to the Batch Normalization Neck (BNNeck) in introduced in <ref type="bibr" target="#b53">[54]</ref> for single-modality person Re-ID, we directly add a batch normalization layer after the pooling layer as the feature embedding for cross-modality person Re-ID. Compared to the two-stream network with another fully connected layer for feature embedding learning in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, the improved structure also achieves better performance for the cross-modality person Re-ID task. Experiments in Section IV-C demonstrate that our proposed MSTN has achieved quite competitive performance when configured with a simple baseline learning objective, using a modality-sharable classifier. Note that learning a sharable classifier is also widely used in existing cross-modality person Re-ID <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Generally, we define a set of training images by X v and X t with identity labels Y = {y i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It contains images from visible modality (denoted by</head><formula xml:id="formula_0">X v = {x v i |i = 1, 2, • • • , N 1 }</formula><p>) and thermal modality (denoted by</p><formula xml:id="formula_1">X t = {x t i |i = 1, 2, • • • , N 2 }). N 1 (N 2</formula><p>) represents the number of visible (thermal) images in the training set.</p><p>We use the combination of triplet loss with hard mining <ref type="bibr" target="#b54">[55]</ref> (L 0 tri ) and softmax identity loss <ref type="bibr" target="#b55">[56]</ref> (L 0 id ) as the baseline learning objective function L 0 . Specifically, the triplet loss constrains the feature learning process by utilizing the bi-directional relationship (visible-to-thermal and thermal-to- 2 We adopt the widely-used ResNet50 <ref type="bibr" target="#b52">[53]</ref> as the backbone network.</p><p>visible <ref type="bibr" target="#b19">[20]</ref>) among different person identities across two modalities. The identity loss aims at learning an identity invariant feature representation by treating the images of each identity captured from two different modalities as the same class. Mathematically, the baseline learning objective with modality-sharable classifier is a combination of two parts:</p><formula xml:id="formula_2">L 0 = L 0 tri + L 0 id .<label>(1)</label></formula><p>The bi-directional triplet loss with hard mining is represented by</p><formula xml:id="formula_3">L 0 tri = n i=1 [ρ + min ∀yj =yi D(f v i , f t j ) -min ∀yi =y k D(f v i , f t k )] + + n i=1 [ρ + min ∀yj =yi D(f t i , f v j ) -min ∀yi =y k D(f v i , f t k )] + ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">[•] + = max(•, 0), ρ is the margin parameter, n is the number of visible (thermal) samples in each training batch. f v i (f t i ) represents the extracted features of input visible (thermal) image x v i (x t i )</formula><p>, and y i is the corresponding identity label. D(•) represents the squared Euclidean distance between the extracted features of two samples <ref type="bibr" target="#b54">[55]</ref>. Note that we also adopt a bi-directional training strategy as introduced in <ref type="bibr" target="#b19">[20]</ref> to enhance the performance, which considers both visible-tothermal and thermal-to-visible relationships.</p><p>The modality-sharable identity classifier learns the feature representation with sharable parameters θ 0 to calculate identity loss for two different modalities <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>. With the modality-sharable classifier θ 0 , we calculate the probability p 0 (y j |x v i ) of a visible sample x v i being recognized as identity j. Mathematically, the probability is computed by a softmax function</p><formula xml:id="formula_5">p 0 (y j |x v i ) = exp(z 0 i,j ) C k=1 exp(z 0 i,k ) , j = 1, • • • , C.<label>(3)</label></formula><p>where z 0 i,j represents the output classification logit of an input sample x v i being recognized as identity j through the modality-sharable classifier θ 0 . C is the total number of identities. Similarly, we calculate the probability of an input thermal sample x t i being recognized as identity j, denoted by p 0 (y j |x t i ). With the calculated probabilities, the modalitysharable identity loss is denoted by</p><formula xml:id="formula_6">L 0 id = - 1 n n i=1 log(p 0 (y i |x v i )) - 1 n n i=1 log(p 0 (y i |x t i )),<label>(4)</label></formula><p>where x v i (x t i ) represents the input visible (thermal) image, and y i is the corresponding label. n is the number of visible (thermal) images at each training batch. In our proposed model, we random select n-pair visible-thermal images to construct the batch, where each visible-thermal pair represents the same identity, as introduced in Section III-D.</p><p>Note that the modality-sharable classifier learns identity discriminative classifier for two different modalities with the same parameters θ 0 , which is also widely used in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. This strategy may lose modality-specific information in the classifier level, which cannot well reduce the cross-modality discrepancy. Therefore, it results in less discriminative cross-modality feature representations in the backward propagation learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classifier-Level Modality-aware Learning</head><p>To address above issue, we propose a novel modalityspecific classifier learning strategy to improve the performance. Our basic idea is that two sets of modality-specific identity classifiers (θ v for visible modality and θ t for thermal modality) are learned for two different modalities, as illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>. Given the modality-specific identity classifier for visible modality represented by θ v , the output logits of an input visible image x v i are calculated by z v i . Correspondingly, we calculate the probability p v (y i |x v i ) of visible sample x v i being correctly recognized as identity i with the softmax function. Similar to the modality-sharable identity loss, the identity loss of visible modality-specific classifier is then calculated by</p><formula xml:id="formula_7">L v id = - 1 n n i=1 log(p v (y i |x v i )).<label>(5)</label></formula><p>Similarly, we can compute the identity loss of the modalityspecific identity classifier θ t for thermal modality. We denote the corresponding output logits of an input thermal image x t i by θ t . Meanwhile, the probability of thermal sample x t i being correctly recognized as identity i is represented by p t (y i |x t i ). The modality-specific identity loss for thermal modality is then calculated by</p><formula xml:id="formula_8">L t id = - 1 n n i=1 log(p t (y i |x t i )).<label>(6)</label></formula><p>In summary, we define the modality-specific loss L s as a combination of L v id and L t id . Mathematically, it is represented by</p><formula xml:id="formula_9">L s = L v id + L t id .<label>(7)</label></formula><p>Note that the modality-specific identity classifiers share the same structure with the modality-sharable identity classifier θ 0 , but they are optimized separately to capture different modalityspecific information in classifier level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Collaborative Ensemble Learning</head><p>Above modality-specific classifiers (θ v , θ t ) and modalitysharable identity classifier (θ 0 ) share most convolutional layers, but they are optimized separately to learn high-level semantic representations. This learning strategy may lose the complementary information among different classifiers. To address this issue, we introduce a collaborative ensemble learning scheme, which aims at collaboratively optimizing the feature learning with multiple classifiers. Motivated by teacher-ensemble model <ref type="bibr" target="#b51">[52]</ref>, we take the ensemble of different classifier output to generate an enhanced teacher model for n identities in each batch. We assume that different classifiers contribute equally in the ensemble. For each visible-thermal image pair x v i and x t i , we calculate the average prediction of all the classifiers as the ensemble z e i , which is represented by</p><formula xml:id="formula_10">z e i = 1 4 (z 0,1 i + z 0,2 i + z v i + z t i ), i = 1, 2, • • • , n,<label>(8)</label></formula><p>where z e i is a C-dim vector, representing the calculated ensemble of each pair identity {x v i , x t i }. z 0,1 i and z 0,2 i represents the output logits of x v i and x t i with the shared classifier θ 0 , respectively. We guide the ensemble training with the crossentropy loss, which is represented by</p><formula xml:id="formula_11">L e L e = - 1 n n i=1 log(p e (y i |x v i , x t i )),<label>(9)</label></formula><p>where p e (y i |x v i , x t i ) is calculated with the softmax function in Eq. 4, representing the probability of pair {x v i , x t i } being recognized as identity y i with the teacher ensemble.</p><p>Collaborative Consistency. To facilitate knowledge transfer among different classifiers, we adopt the knowledge distillation technique introduced in <ref type="bibr" target="#b23">[24]</ref> for collaborative learning. Following <ref type="bibr" target="#b23">[24]</ref>, we add a temperature parameter T to smooth the probability distributions for different classifiers. Mathematically, we compute smoothed probability of the teacher ensemble by</p><formula xml:id="formula_12">pe (y k |x v i , x t i ) = exp(z e i,j /T ) C k=1 exp(z e i,k /T ) , j = 1, • • • , C.<label>(10)</label></formula><p>Similarly, we could compute the smoothed probability of pv (y k |x v i ) and pt (y k |x t i ). Following <ref type="bibr" target="#b23">[24]</ref>, we set T = 3 in our experiments. Note that T controls the concentration level of the softened distributions <ref type="bibr" target="#b56">[57]</ref>.</p><p>To align the distributions between the modality-specific identity classifier and the teacher ensemble, we adopt the Kullback Leibler divergence to measure the distribution difference. It is formulated by</p><formula xml:id="formula_13">L c = 1 n n i=1 C k=1 pe (y k |x v i , x t i ) log pe (y k |x v i , x t i ) pv (y k |x v i ) + 1 n n i=1 C k=1 pe (y k |x v i , x t i ) log pe (y k |x v i , x t i ) pt (y k |x t i ) .<label>(11)</label></formula><p>n-pair Batch Sampling. This part introduces our n-pair batch sampling training strategy <ref type="bibr" target="#b57">[58]</ref> for cross-modality person Re-ID, which is designed to match the rationale of collaborative ensemble learning. In particular, at each training batch, Fig. <ref type="figure">4</ref>. Sampled visible-thermal image pairs from SYSU-MM01 dataset <ref type="bibr" target="#b13">[14]</ref> and RegDB dataset <ref type="bibr" target="#b14">[15]</ref>. Each column represents the same identity from two different modalities. Note that night-time images are captured by near-infrared cameras on SYSU-MM01 dataset and by thermal cameras on RegDB dataset, respectively.</p><p>we firstly randomly select p person identities, and then we select k visible and k thermal images for each identity to feed into the two-stream network. It is easy to infer that n is equal to p × k. Therefore, n-pair images are fed into the network at each step and each visible-thermal pair represents the same identity from two modalities.</p><p>On one hand, in our teacher ensemble, we learn a Cdim ensemble for each visible-thermal pair by combining the outputs of modality-sharable and modality-specific classifiers, and then the ensemble learning loss could guarantee that the ensemble is correctly classified. In this manner, we could learn an enhanced ensemble for each visible-thermal pair by considering the relationship between two images. In a random sampling mechanism, all the possible positive visiblethermal pairs are constrained to be correlated in the ensemble learning process, resulting in better performance. On the other hand, the collaborative consistency loss calculates the difference between the ensemble output and modality-specific classifier output. The collaborative consistency loss aims at transferring the learned information among multiple classifiers, provides more reliable gradient information in backpropagation process. Experimentally, we demonstrate that both constraints improve the Re-ID performance consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overall Loss Function.</head><p>The total loss L of our modality-aware collaborative ensemble (MACE) learning is then defined by</p><formula xml:id="formula_14">L c = L 0 + λ 1 L s + L e + w(t) • T 2 L c ,<label>(12)</label></formula><p>where λ 1 is the coefficient to adjust the contribution of modality-specific classifier loss L s . Note that the gradient magnitudes of the collaborative consistency loss is scaled by 1/T 2 due to the temperature T . Therefore, we multiply a factor T 2 for the collaborative consistency loss L c to ensure that it shares similar contribution with the ensemble learning loss L e . w(t) is a ramps up sigmoid function, where the weight value increases from zero to one gradually according to the training epoch t <ref type="bibr" target="#b58">[59]</ref>. The main reason is that the initial predictions of different classifiers might be quite different and it is quite difficult to guarantee the predictions are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Datasets and settings. To evaluate our proposed method, we adopt two publicly available cross-modality person Re-ID datasets (SYSU-MM01 <ref type="bibr" target="#b13">[14]</ref> and RegDB <ref type="bibr" target="#b14">[15]</ref>) for experiments. We also plot some example visible-thermal image pairs randomly sampled from two datasets in Fig. <ref type="figure">4</ref>.</p><p>SYSU-MM01 dataset <ref type="bibr" target="#b13">[14]</ref> is a large-scale cross-modality person Re-ID dataset. It is collected by 4 general RGB cameras and 2 near-infrared cameras in SYSU campus. Note that this dataset contains images captured in both indoor and outdoor environment, which makes this dataset extremely challenging. SYSU-MM01 contains 491 person identities, and each identity appears in more than two different modality cameras. Specifically, it contains 395 identities for training and 96 identities for testing. Totally, the training set contains 22,258 visible and 11,909 near-infrared images for 395 identities, which are captured from both indoor and outdoor cameras. For testing, it contains two different evaluation settings, all-search mode and indoor-search mode. The query set contains 3803 images captured from IR camera 3 and 6 in both settings. The gallery set contains all the visible images captured from four RGB cameras in all-search mode, while the indoor-search mode only contains the images captured by two indoor cameras. Details description of the evaluation settings is in <ref type="bibr" target="#b13">[14]</ref>.</p><p>RegDB dataset <ref type="bibr" target="#b14">[15]</ref> is a small-scale dataset collected by a dual-camera system, including one visible camera and one thermal camera. Totally, this dataset contains 412 person identities, in which each identity has 10 visible and 10 thermal images. Following the cross-modality pedestrian retrieval evaluation protocol in <ref type="bibr" target="#b15">[16]</ref>. We randomly select 206 identities for training and the rest 206 identities are used for testing. Following <ref type="bibr" target="#b15">[16]</ref>, we use the images from visible modality as query and the images from thermal modality as gallery. Naturally, the query set contains 2,060 visible images and the gallery set contains 2,060 thermal images. The average performance of ten times randomly training/testing splits is reported following <ref type="bibr" target="#b15">[16]</ref>. Note that we also evaluate the performance by changing the query setting to thermal (query) to visible (gallery).</p><p>Evaluation metrics. To evaluate our proposed method and competing methods, we use Cumulative Matching Characteristics (CMC) and mean Average Precision (mAP) as the evaluation metrics. CMC measures the matching probability of the groundtruth person occurs in the top-k retrieved results (Rank-k accuracy). mAP is adopted to measure the retrieval performance when multiple matching images occur in the gallery set for a given query image <ref type="bibr" target="#b59">[60]</ref>.</p><p>Implementation details. Our algorithm is implemented on PyTorch framework. Following most existing person Re-ID works, ResNet50 <ref type="bibr" target="#b52">[53]</ref> is adopted as our backbone network for cross-modlaity feature learning. The stride of the last convolutional block is set to 1 following <ref type="bibr" target="#b33">[34]</ref> to obtain finegrained feature maps. We initialize the convolutional blocks of our two-stream network with the pre-trained ImageNet parameters, as done in <ref type="bibr" target="#b16">[17]</ref>. All the input images are firstly resized to 288 × 144. We adopt random cropping with zeropadding and horizontal flipping for data argumentation. SGD 1057-7149 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing optimizer is adopted to optimize the network, and the momentum parameter is set to 0.9. We set the initial learning rate as 0.1 for both datasets. The learning rate is decayed by 0.1 at 30 epoch with totally 60 epochs on both datasets. We set the margin parameter ρ in Eq.2 to 0. We set the weighting coefficient of the modality-aware identity classifier as λ 1 = 5 on both datasets. The collaborative consistency loss is added to the total loss with a ramps up sigmoid function 3 . Mathematically, it is represented by w(t) = exp(-5.0 * (1 -t tm ) 2 ), where t is the current epoch number and t m is set to 100 in our experiments. This function aims at increasing the weights from zero to one gradually <ref type="bibr" target="#b58">[59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Self Evaluation</head><p>Evaluation of MSTN. In this subsection, we firstly evaluate the effectiveness of our improved two-stream network, MSTN. We compare our performance with the two-stream network used in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Specifically, both methods use the combination of softmax loss and bi-directional triplet loss <ref type="bibr" target="#b19">[20]</ref> as our baseline learning objective. The results on the largescale SYSU-MM01 dataset are shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>We observe that our MSTN achieves much better performance than the widely-used two-stream network baselines <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The main improvement is brought by the shared convolutional blocks. The main reason is that the middle-level convolutional blocks usually capture the middle-level features, which is quite important for finegrained recognition task. Therefore, our MSTN learns sharable middle-level feature representations rather than optimizing them independently for cross-modality person Re-ID. This 3 Available at https://github.com/benathi/fastswa-semi-sup modification results in significant improvment for VT-ReID task, even outperforming most of the current state-of-the-art <ref type="bibr" target="#b20">[21]</ref>. In addition, we also compare the performance with onestream network under our settings as shown in Table <ref type="table" target="#tab_0">I</ref>. Results show that our two-stream network achieves slightly better performance than the one-stream network. This experiment demonstrates that modeling modality discrepancy in feature level is also quite important for VT-ReID.</p><p>Evaluation of Each Component. We evaluate the effectiveness of each component on the SYSU-MM01 dataset. The results of adding/removing each component are shown in Table <ref type="table" target="#tab_1">II</ref>. Specifically, "B" represents the baseline results by combing the identity loss and triplet loss L 0 . "S" denotes the modalityspecific identity loss L s . "E" means the ensemble learning loss L e . 'C" demonstrates the collaborative learning loss L c . 1) Effectiveness of L s : Compared to the baseline model (B), the proposed modality-specific classifier loss (S) greatly improves the performance on both query settings. The improvement is about 10% for rank-1 accuracy and 6% for mAP on this large-scale dataset. This experiment demonstrates that handling the modality discrepancy in classifier level is important for VT-ReID. 2) Effectiveness of L e : When we further combine the modality-specific loss with the ensemble learning loss, the performance is further improved by about 2% for rank-1 accuracy. It shows the importance of learning a teacher ensemble based on the outputs of different classifiers, which enhances the similarity between the two images in each visible-thermal image pair. 3) Effectiveness of L c : We also evaluate the collaborative consistency loss. We observe that facilitating the knowledge transfer between different classifiers also consistently improves the performance. After combing all the terms together, the final performance is further improved, which shows that all these components work well together. Finally, we achieve rank-1/mAP accuracy 51.64%/50.11 for the challenging single-shot all search on SYSU-MM01 dataset. This experiment verifies the effectiveness of the proposed modality-aware collaborative ensemble learning.  Analysis of Collaborative Consistency Loss. We also evaluate the collaborative consistency loss on the large-scale SYSU-MM01 dataset under the challenging single-shot all search mode (w KL loss or w/o KL loss). The results are shown in Fig. <ref type="figure" target="#fig_4">5</ref>. We calculate the KL loss between the teacher ensemble and the modality-specific classifier output at different epochs (left). We also report the cross-modality person Re-ID performance at different epochs (right).</p><p>Results shown in Fig. <ref type="figure" target="#fig_4">5</ref> demonstrate that the divergence between the teacher ensemble and the modality-specific classier outputs is very large if without the collaborative consistency loss L c . Specifically, the KL divergence drops dramatically when combined with L c . Meanwhile, we observe that the rank-1 accuracy and mAP at different epochs also perform better than the baseline results. It also achieves faster learning speed in terms of the VT-ReID accuracy. This experiment verifies the idea to constrain the consistency between different classifiers. It facilitates the knowledge transfer among different classifiers, which is similar to knowledge distillation <ref type="bibr" target="#b23">[24]</ref>.</p><p>Analysis of Ramps Up Sigmoid Function. We evaluate the effect of the ramp up function by simply setting the weight of the collaborative consistency loss as 1. The results are shown in Table <ref type="table" target="#tab_3">III</ref>. We observe that the performance is slightly lower than our full model. Meanwhile, it performs closely to the results when this loss is not included in the model. The reason is that the predictions of different classifiers at the early stage are different due to the random initialization of the classifier weights. Forcibly including this constraint too early may lead a trivial solution, i.e.,, all the classifiers model the same information. Thus we progressively add this constraint in the overall learning process, ensuring that modality-specific information is captured by the classifiers at the early stage.</p><p>Parameter Analysis. We also evaluate the weighting pa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indoor search</head><p>Rank-1 mAP Fig. <ref type="figure">6</ref>. Evaluation of the weighting parameter λ1 SYSU-MM01 dataset (single-shot all search/indoor search mode). Note that we only combine the baseline loss L 0 and the evaluated component (L s for demonstration. Rank-1 matching accuracy (%) and mAP (%) are reported. rameters λ 1 of modality-specific classifier loss L s in the proposed method. Note that we only have one hyper-parameter in our collaborative ensemble learning method, which is suitable for real applications. Specifically, we only adopt the baseline loss L 0 to evaluate the performance to better illustrate the influence. The rank-1 accuracy and mAP on the large-scale SYSU-MM01 dataset with different λ 1 are reported in Fig. <ref type="figure">6</ref>.</p><p>Fig. <ref type="figure">6</ref> demonstrates that integrating L s with L 0 consistently improves the cross-modality person Re-ID performance. The improvement is obvious under both query settings. This experiment verifies the importance of addressing the modality discrepancy in classifier level. We also observe that we achieve the best performance when λ 1 is close to 5. When we keep on increasing λ 1 , the performance is almost unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-arts</head><p>In this subsection, we compare our proposed method (MACE) with the state-of-the-art methods on two different 1057-7149 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP. <ref type="bibr">2020</ref>  datasets. All existing cross-modality VT-ReID methods are included for comparison, including the one-stream Zero-Padding <ref type="bibr" target="#b13">[14]</ref> network in ICCV 2017, TONE + HCML <ref type="bibr" target="#b15">[16]</ref> with two-stage learning in AAAI 2018, bi-directional dualconstrained top-ranking (BDTR) <ref type="bibr" target="#b19">[20]</ref> in IJCAI 2018, centerconstrained top-ranking (eBDTR) <ref type="bibr" target="#b16">[17]</ref> in TIFS 2019, crossmodality GAN (cmGAN) <ref type="bibr" target="#b12">[13]</ref> in IJCAI 2018, Hypersphere Manifold Embedding (HSME) <ref type="bibr" target="#b22">[23]</ref> in AAAI 2019, dual-level discrepancy learning (D 2 RL) <ref type="bibr" target="#b20">[21]</ref> in CVPR 2019, modalityaware collaborative learning (MAC <ref type="bibr" target="#b24">[25]</ref>) in ACM MM2019, (DFE <ref type="bibr" target="#b60">[61]</ref>) in ACM MM2019 and (MSR <ref type="bibr" target="#b61">[62]</ref>) in TIP 2020. In addition, we also compare some unpublished arXiv papers, including EDFL <ref type="bibr" target="#b21">[22]</ref>, HPILN <ref type="bibr" target="#b43">[44]</ref> and LZM <ref type="bibr" target="#b42">[43]</ref>. Note that the numbers of these methods are all taken from their original papers. The results are shown in Table IV and V.</p><p>We have the following observations in Table <ref type="table" target="#tab_6">IV</ref> and V: 1) Compared to traditional hand-crafted features learning methods, we achieve much better performance for the crossmodality person Re-ID. The main reason is that the domain knowledge in cross-modality person Re-ID is ignored in their methods. Meanwhile, we observe that deep learning also performs much better than the hand-crafted features and dictionary/metric learning methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b62">[63]</ref>. 2) Our proposed method outperforms the current state-of-the-art cross-modality person Re-ID method D 2 RL [21] by a large margin on both datasets. Note that D 2 RL <ref type="bibr" target="#b20">[21]</ref> needs to generate visible-tothermal and thermal-to-visible images for unified training, and the augmented training set is three times larger than the original dataset. Meanwhile, their testing process also needs to generate more images for feature extraction. In comparison, our proposed method does not need any cumbersome image generation process but achieves much better performance. In addition, compared to the cmGAN method <ref type="bibr" target="#b12">[13]</ref>, it trains more than 2,000 epochs for adversarial modality discrimination to achieve good performance, while our proposed method only needs 60 epochs. We achieve much better performance in a more efferent and simpler way. Compared to MSR <ref type="bibr" target="#b61">[62]</ref>, which also adopts a similar idea with modality-specific classifier learning, we achieve much higher accuracy than MSR in most settings. The comparison demonstrates the effectiveness of our proposed MACE method, which is more suitable for real applications. This experiment shows the superiority by simultaneously handling the modality discrepancy in feature level and classifier level with modality-aware collaborative ensemble learning. 3) In addition, we also observe that our MSTN achieves quite competitive performance when only configured with the baseline learning objective, outperforming most counterparts.</p><p>In addition, we find that our proposed method performs much better than the other counterparts on the RegDB dataset, usually about 40% improvement for the rank-1 accuracy. Table V also demonstrates that MACE is robust to different query settings. The main reason is that we can learn much better modality-sharable middle-level feature representations with our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Analysis</head><p>Retrieved Examples. We also visualize some retrieved results on the large-scale SYSU-MM01 dataset. Two different searching modes are demonstrated: thermal to visible search and visible to thermal search. For each query setting, five query samples are randomly selected and their corresponding top ten retrieved cross-modality results are visualized in Fig. <ref type="figure" target="#fig_6">7</ref>. Note that we use the all-search gallery for visualization. Meanwhile, we also report the cosine similarity scores.</p><p>The results demonstrate that our method method can get good retrieval results when the person appearance has rich structure information (e.g., bags or stripes) or conspicuous part (e.g., logo). This observation is also consistent with the cross-modality person Re-ID task since the thermal images at 1057-7149 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.   night cannot capture the color information but they preserve rich texture information. Interestingly, we find that when some persons changing their clothes (e.g., 3rd example in the left), we can still get the correct results by mining the discriminative visual cues, maybe the T-shirt or the shorts. Another interesting observation is that using the visible-to-thermal query setting usually performs better than that of thermal-to-visible query setting. The main reason is that using the visible images as query provides richer appearance information for the query image, which is useful for cross-modality person Re-ID. However, there are still many errors and the performance is still far from the requirements in real applications for cross-modality person Re-ID. In addition, we observe that the similarity scores are distributed differently for different query examples, it would be interesting to study how to define a similarity threshold for the VT-ReID problem.</p><p>MSTN for Other Methods. In this subsection, we evaluate the performance of two state-of-the-art methods when configured with our proposed MSTN. Two methods are selected for evaluation, including eBDTR <ref type="bibr" target="#b16">[17]</ref> and MAC <ref type="bibr" target="#b24">[25]</ref>. The results on the large-scale SYSU-MM01 dataset under both query settings are shown in Table <ref type="table" target="#tab_8">VI</ref>.</p><p>We observe that both methods are significantly improved when using our MSTN as the backbone network. We achieve nearly 50%-70% rank-1 accuracy improvement under both query settings on the large-scale SYSU-MM01 dataset. We can draw two important conclusions according to the results in Table <ref type="table" target="#tab_8">VI</ref>: 1) The middle-level sharable features play an important role in cross-modality person Re-ID to bridge the modality gap. By learning middle-level sharable feature representations with MSTN, we can achieve much better VT-ReID performance than learning high-level sharable features in the final embedding layers. 2) Our improved MSTN works well for other counterparts, which provides an important insight for researchers in this field. It can greatly accelerate the crossmodality Re-ID research, which is very important for practical person Re-ID applications.</p><p>Cross-Dataset Evaluation. In this subsection, we conduct the cross-dataset evaluation experiments, which is ignored in previous cross-modality person Re-ID works. Specifically, we use the trained model on the large-scale SYSU-MM01 dataset and test it on the small-scale RegDB dataset. We evaluate our baseline method and the proposed MACE method. The results under two different query settings are shown in Table <ref type="table" target="#tab_8">VI</ref>.</p><p>Although we find that we perform better than the baseline method, a thought-provoking observation is that the performance drops dramatically under the cross-dataset evaluation setting. The main reason is that these two datasets use different light spectrums to capture the night-time person images (infrared camera on the SYSU-MM01 dataset and thermal camera 1057-7149 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing  on the RegDB dataset), and the collected night-time person images are totally visually different. However, this would be the practical scenario in real application by applying a trained model in different environments, but importance of this cross-dataset evaluation setting is ignored in previous works. Our observation in this experiment provides an important but unexplored direction for future research in VT-ReID.</p><p>Results with Different Backbone Networks. We evaluate the proposed MSTN and MACE by applying AlexNet, ResNet50 and DenseNet121 as backbones on the large-scale SYSU-MM01 dataset. Similar to our design for ResNet50, the first convolutional block (convolutional layer for AlexNet, Dense block for DenseNet121) is specific for modality-specific feature learning, while the rest layers are shared for modalitysharable feature learning. Other training parameters are exactly the same with ResNet50 as described in Sec IV-A. The results are shown in Table <ref type="table" target="#tab_10">VIII</ref>. We also include the baseline performance by using the two-stream network in <ref type="bibr" target="#b24">[25]</ref>. We observe that the proposed methods (MSTN and MACE) perform well in improving the accuracy on different backbone networks, and consistently outperform the two-stream network structure in <ref type="bibr" target="#b24">[25]</ref>. This experiment further verifies the flexibility of our proposed method for different backbone networks.</p><p>MACE with One-stream Network Baseline. We have applied our method to the one-stream network, and the results shown in Table <ref type="table" target="#tab_10">VIII</ref>. We observe that the baseline performance of one-stream network is also consistently improved by our MACE method. However, the improvement is not as significant as our proposed MSTN network backbone. The main reason is that the one-stream network provides limited ability to mine the modality-specific information in feature level, since all the network parameters are the same for both modalities to extract the features. In comparison, our method addresses the modality discrepancy in both feature level and classifier level under a collaborative ensemble learning framework, resulting in better performance.</p><p>V. CONCLUSIONS In this paper, we propose a modality-aware collaborative ensemble learning (MACE) method with an improved middlelevel sharable two-stream network (MSTN) for cross-modality VT-ReID. We firstly introduce MSTN for modality-aware feature learning, which learns modality-sharable features in middle-level convolutional layers. It achieves much better performance compared than current state-of-the-arts when using a simple combination of softmax loss and triplet loss. Experiments also demonstrate that our proposed MSTN also greatly improves the performance of other methods. Besides the feature-level modality discrepancy with MSTN, we also propose to handle the modality difference in classifier-level. Extensive experiments demonstrate that the modality-specific classifier is essential for a good cross-modality person Re-ID system. To incorporate different classifiers, we introduce a collaborative ensemble learning scheme to further improve the performance. By facilitating the knowledge transfer among different classifiers, we outperform the state-of-the-arts by a large margin on two public cross-modality person Re-ID datasets. It provides new insights and greatly accelerates the cross-modality Re-ID research, which is very important for real applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of cross-modality visible-thermal person reidentification (VT-ReID). (a) The visible cameras usually cannot capture valid appearance information at night; The infrared(thermal) cameras could capture person images with rich appearance information at night; (c) The gallery images are usually collected by visible cameras in the daytime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Two-stream network in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig.2. Comparison between the widely-used two-stream network in<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b19">[20]</ref>,<ref type="bibr" target="#b21">[22]</ref> and our improved middle-level sharable two-stream network (MSTN). We use the widely-used ResNet50 for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The framework of our proposed method. The parameters of the first convolutional block are different to address the modality difference in feature level. Meanwhile, we use four shared convolutional blocks and one BN layer to learn modality-sharable middle-level feature. A modality sharable-classifier loss L 0 is adopted to guide the learning process. To further handle the modality discrepancy, we introduce two modality-specific classifier losses (L v id and L t id ). To facilitate knowledge transfer among different classifiers, we introduce a collaborative ensemble learning scheme, which contains an ensemble learning loss L e and a consistency collaboration loss L c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Evaluation of the collaborative consistency loss L c on the large-scale SYSU-MM01 dataset (single-shot all search mode). We calculate the KL loss between the teacher ensemble and the modalityspecific classifier output at different epochs (left). We also report the person Re-ID performance at different epochs (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Retrieved results visualization. Two different query settings: visible to thermal and thermal to visible. For each setting, we randomly five query examples and visualize their corresponding top-10 retrieved results (All-search Mode) from SYSU-MM01 dataset. Corrected retrieved samples are in green boxes and wrong matchings are in red boxes (best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>OF THE PROPOSED MSTN ON THE LARGE-SCALE SYSU-MM01 DATASET. NOTE THAT BOTH OF THEM UTILIZE THE COMBINATION OF SOFTMAX LOSS AND BI-DIRECTIONAL TRIPLET LOSS<ref type="bibr" target="#b19">[20]</ref> AS BASELINE LEARNING OBJECTIVE. RANK AT r MATCHING ACCURACY(%) AND MAP (%)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ARE REPORTED.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell></cell><cell></cell><cell>All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Indoor Search</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>Two-Stream [25]</cell><cell>30.67</cell><cell>60.73</cell><cell>74.76</cell><cell>87.57</cell><cell>32.90</cell><cell>33.50</cell><cell>67.28</cell><cell>81.64</cell><cell>93.38</cell><cell>44.83</cell></row><row><cell>MSTN (Ours)</cell><cell>45.22</cell><cell>74.22</cell><cell>84.51</cell><cell>93.15</cell><cell>45.79</cell><cell>49.53</cell><cell>78.75</cell><cell>88.56</cell><cell>95.49</cell><cell>58.15</cell></row><row><cell>One-Stream (Ours)</cell><cell>44.68</cell><cell>73.85</cell><cell>84.75</cell><cell>92.24</cell><cell>44.91</cell><cell>48.68</cell><cell>78.02</cell><cell>87.62</cell><cell>94.95</cell><cell>57.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATION</head><label>II</label><figDesc>OF EACH COMPONENT ON THE LARGE-SCALE SYSU-MM01 DATASET. "B" REPRESENTS THE BASELINE LEARNING OBJECTIVE WITH THE COMBINATION OF IDENTITY LOSS AND TRIPLET LOSS L 0 . "S" MEANS RESULTS WITH THE MODALITY-SPECIFIC CLASSIFIER LOSS L s . "E" MEANS RESULTS WITH THE ENSEMBLE LEARNING LOSS L e . 'C" MEANS THE COLLABORATIVE LEARNING WITH CONSISTENCY REGULARIZATION L c . RANK AT r MATCHING ACCURACY(%) AND MAP (%) ARE REPORTED.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell>All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Indoor Search</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>B</cell><cell>45.22</cell><cell>74.22</cell><cell>84.51</cell><cell>93.15</cell><cell>45.79</cell><cell>49.53</cell><cell>78.75</cell><cell>88.56</cell><cell>95.49</cell><cell>58.15</cell></row><row><cell>B + S</cell><cell>49.79</cell><cell>78.28</cell><cell>87.77</cell><cell>94.87</cell><cell>48.54</cell><cell>54.66</cell><cell>83.11</cell><cell>91.59</cell><cell>97.25</cell><cell>62.22</cell></row><row><cell>B + S + E</cell><cell>50.38</cell><cell>78.52</cell><cell>88.46</cell><cell>95.29</cell><cell>49.68</cell><cell>55.04</cell><cell>84.44</cell><cell>92.50</cell><cell>97.37</cell><cell>62.62</cell></row><row><cell>B + S + C</cell><cell>50.48</cell><cell>79.31</cell><cell>88.25</cell><cell>94.87</cell><cell>49.35</cell><cell>56.05</cell><cell>85.26</cell><cell>92.65</cell><cell>97.31</cell><cell>63.98</cell></row><row><cell>B + S + C + E</cell><cell>51.64</cell><cell>78.24</cell><cell>87.25</cell><cell>94.44</cell><cell>50.11</cell><cell>57.35</cell><cell>85.67</cell><cell>93.02</cell><cell>97.47</cell><cell>64.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EVALUATION</head><label>III</label><figDesc>OF THE PROPOSED MACE WITH THE ONE-STREAM NETWORK BASELINE ON THE LARGE-SCALE SYSU-MM01 DATASET. RANK AT r MATCHING ACCURACY(%) AND MAP (%) ARE REPORTED.</figDesc><table><row><cell>Datasets</cell><cell cols="2">All Search</cell><cell cols="2">Indoor Search</cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>mAP</cell><cell>r = 1</cell><cell>mAP</cell></row><row><cell>without ramp</cell><cell>50.42</cell><cell>49.48</cell><cell>55.24</cell><cell>62.84</cell></row><row><cell>Full Model</cell><cell>51.64</cell><cell>50.11</cell><cell>57.35</cell><cell>64.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>WITH THE STATE-OF-THE-ARTS ON THE REGDB DATASET. RANK AT r ACCURACY (%) AND MAP (%).</figDesc><table><row><cell>Method</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>Setting</cell><cell></cell><cell cols="2">Visible to Thermal</cell><cell></cell></row><row><cell>TONE + HCML [16]</cell><cell>24.44</cell><cell>47.53</cell><cell>56.78</cell><cell>20.08</cell></row><row><cell>Zero-Padding [14]</cell><cell>17.75</cell><cell>34.21</cell><cell>44.35</cell><cell>18.90</cell></row><row><cell>BDTR [20]</cell><cell>33.56</cell><cell>58.61</cell><cell>67.43</cell><cell>32.76</cell></row><row><cell>eBDTR [17]</cell><cell>34.62</cell><cell>58.96</cell><cell>68.72</cell><cell>33.46</cell></row><row><cell>HSME [23]</cell><cell>50.85</cell><cell>73.36</cell><cell>81.66</cell><cell>47.00</cell></row><row><cell>D 2 RL [21]</cell><cell>43.4</cell><cell>66.1</cell><cell>76.3</cell><cell>44.1</cell></row><row><cell>MAC [25]</cell><cell>36.43</cell><cell>62.36</cell><cell>71.63</cell><cell>37.03</cell></row><row><cell>EDFL  † [22]</cell><cell>52.58</cell><cell>72.10</cell><cell>81.47</cell><cell>52.98</cell></row><row><cell>MSR [62]</cell><cell>48.43</cell><cell>70.32</cell><cell>79.95</cell><cell>48.67</cell></row><row><cell>DFE [61]</cell><cell>70.13</cell><cell>86.32</cell><cell>91.96</cell><cell>69.14</cell></row><row><cell>MACE (Ours)</cell><cell>72.37</cell><cell>88.40</cell><cell>93.59</cell><cell>69.09</cell></row><row><cell>Setting</cell><cell></cell><cell cols="2">Thermal to Visible</cell><cell></cell></row><row><cell>TONE + HCML [16]</cell><cell>21.70</cell><cell>45.02</cell><cell>55.58</cell><cell>22.24</cell></row><row><cell>BDTR [20]</cell><cell>32.92</cell><cell>58.46</cell><cell>68.43</cell><cell>31.96</cell></row><row><cell>Zero-Padding [14]</cell><cell>16.63</cell><cell>34.68</cell><cell>44.25</cell><cell>17.82</cell></row><row><cell>eBDTR [17]</cell><cell>34.21</cell><cell>58.74</cell><cell>68.64</cell><cell>32.49</cell></row><row><cell>HSME [23]</cell><cell>50.15</cell><cell>72.40</cell><cell>81.07</cell><cell>46.16</cell></row><row><cell>MAC [25]</cell><cell>36.20</cell><cell>61.68</cell><cell>70.99</cell><cell>36.63</cell></row><row><cell>EDFL  † [22]</cell><cell>51.89</cell><cell>72.09</cell><cell>81.04</cell><cell>52.13</cell></row><row><cell>DFE [61]</cell><cell>67.99</cell><cell>85.56</cell><cell>91.41</cell><cell>66.70</cell></row><row><cell>MACE (Ours)</cell><cell>72.12</cell><cell>88.07</cell><cell>93.07</cell><cell>68.57</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>† Arxiv papers, not yet published.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH THE STATE-OF-THE-ART METHODS ON THE SYSU-MM01 DATASET. ACCURACY(%) AT RANK r AND MAP (%).</figDesc><table><row><cell>Datasets</cell><cell></cell><cell cols="2">All Search</cell><cell></cell><cell></cell><cell cols="2">Indoor Search</cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>HOG</cell><cell>2.76</cell><cell>18.25</cell><cell>31.91</cell><cell>4.24</cell><cell>3.22</cell><cell>24.7</cell><cell>44.6</cell><cell>7.25</cell></row><row><cell>MLBP</cell><cell>2.12</cell><cell>16.23</cell><cell>28.32</cell><cell>3.86</cell><cell>3.43</cell><cell>26.42</cell><cell>45.36</cell><cell>7.72</cell></row><row><cell>LOMO [30]</cell><cell>1.75</cell><cell>14.14</cell><cell>26.63</cell><cell>3.48</cell><cell>2.24</cell><cell>22.52</cell><cell>41.53</cell><cell>6.64</cell></row><row><cell>GSM [63]</cell><cell>5.29</cell><cell>33.71</cell><cell>52.95</cell><cell>8.00</cell><cell>9.46</cell><cell>48.98</cell><cell>72.06</cell><cell>15.57</cell></row><row><cell>One-stream [14]</cell><cell>12.04</cell><cell>49.68</cell><cell>66.74</cell><cell>13.67</cell><cell>16.94</cell><cell>63.55</cell><cell>82.10</cell><cell>22.95</cell></row><row><cell>Two-stream [14]</cell><cell>11.65</cell><cell>47.99</cell><cell>65.50</cell><cell>12.85</cell><cell>15.60</cell><cell>61.18</cell><cell>81.02</cell><cell>21.49</cell></row><row><cell>Zero-Padding [14]</cell><cell>14.80</cell><cell>54.12</cell><cell>71.33</cell><cell>15.95</cell><cell>20.58</cell><cell>68.38</cell><cell>85.79</cell><cell>26.92</cell></row><row><cell>TONE [16]</cell><cell>12.52</cell><cell>50.72</cell><cell>68.60</cell><cell>14.42</cell><cell>20.82</cell><cell>68.86</cell><cell>84.46</cell><cell>26.38</cell></row><row><cell>HCML [16]</cell><cell>14.32</cell><cell>53.16</cell><cell>69.17</cell><cell>16.16</cell><cell>24.52</cell><cell>73.25</cell><cell>86.73</cell><cell>30.08</cell></row><row><cell>cmGAN [13]</cell><cell>26.97</cell><cell>67.51</cell><cell>80.56</cell><cell>31.49</cell><cell>31.63</cell><cell>77.23</cell><cell>89.18</cell><cell>42.19</cell></row><row><cell>BDTR [17]</cell><cell>27.32</cell><cell>66.96</cell><cell>81.07</cell><cell>27.32</cell><cell>31.92</cell><cell>77.18</cell><cell>89.28</cell><cell>41.86</cell></row><row><cell>eBDTR [17]</cell><cell>27.82</cell><cell>67.34</cell><cell>81.34</cell><cell>28.42</cell><cell>32.46</cell><cell>77.42</cell><cell>89.62</cell><cell>42.46</cell></row><row><cell>HSME [23]</cell><cell>20.68</cell><cell>32.74</cell><cell>77.95</cell><cell>23.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>D 2 RL [21]</cell><cell>28.9</cell><cell>70.6</cell><cell>82.4</cell><cell>29.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAC [25]</cell><cell>33.26</cell><cell>79.04</cell><cell>90.09</cell><cell>36.22</cell><cell>36.43</cell><cell>62.36</cell><cell>71.63</cell><cell>37.03</cell></row><row><cell>EDFL  † [22]</cell><cell>36.94</cell><cell>84.52</cell><cell>93.22</cell><cell>40.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HPILN  † [44]</cell><cell>41.36</cell><cell>84.78</cell><cell>94.31</cell><cell>42.95</cell><cell>45.77</cell><cell>91.82</cell><cell>98.46</cell><cell>56.52</cell></row><row><cell>LZM  † [43]</cell><cell>45.00</cell><cell>89.06</cell><cell>-</cell><cell>45.94</cell><cell>49.66</cell><cell>92.47</cell><cell>-</cell><cell>59.81</cell></row><row><cell>MSR [62]</cell><cell>37.35</cell><cell>83.40</cell><cell>93.34</cell><cell>38.11</cell><cell>39.64</cell><cell>89.29</cell><cell>97.66</cell><cell>50.88</cell></row><row><cell>DFE [61]</cell><cell>48.71</cell><cell>88.86</cell><cell>95.27</cell><cell>48.59</cell><cell>52.25</cell><cell>89.86</cell><cell>95.85</cell><cell>59.68</cell></row><row><cell>(Ours)</cell><cell>51.64</cell><cell>87.25</cell><cell>94.44</cell><cell>50.11</cell><cell>57.35</cell><cell>93.02</cell><cell>97.47</cell><cell>64.79</cell></row><row><cell>† Arxiv papers, not yet published.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Citation information: DOI 10.1109/TIP.2020.2998275, IEEE Transactions on Image Processing</figDesc><table><row><cell cols="6">IEEE TRANSACTIONS ON IMAGE PROCESSING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>0.395</cell><cell>0.347</cell><cell>0.336</cell><cell>0.321</cell><cell>0.307</cell><cell>0.301</cell><cell>0.292</cell><cell>0.254</cell><cell>0.227</cell><cell>0.227</cell><cell>0.490</cell><cell>0.464</cell><cell>0.462</cell><cell>0.457</cell><cell>0.428</cell><cell>0.427</cell><cell>0.411</cell><cell>0.410</cell><cell>0.402</cell><cell>0.389</cell></row><row><cell>0.454</cell><cell>0.429</cell><cell>0.411</cell><cell>0.403</cell><cell>0.319</cell><cell>0.290</cell><cell>0.271</cell><cell>0.263</cell><cell>0.250</cell><cell>0.248</cell><cell>0.461</cell><cell>0.423</cell><cell>0.411</cell><cell>0.402</cell><cell>0.391</cell><cell>0.390</cell><cell>0.386</cell><cell>0.386</cell><cell>0.386</cell><cell>0.377</cell></row><row><cell>0.507</cell><cell>0.479</cell><cell>0.479</cell><cell>0.473</cell><cell>0.465</cell><cell>0.416</cell><cell>0.337</cell><cell>0.332</cell><cell>0.319</cell><cell>0.297</cell><cell>0.477</cell><cell>0.446</cell><cell>0.434</cell><cell>0.426</cell><cell>0.409</cell><cell>0.408</cell><cell>0.405</cell><cell>0.401</cell><cell>0.396</cell><cell>0.393</cell></row><row><cell>0.360</cell><cell>0.337</cell><cell>0.311</cell><cell>0.261</cell><cell>0.255</cell><cell>0.239</cell><cell>0.233</cell><cell>0.222</cell><cell>0.217</cell><cell>0.217</cell><cell>0.344</cell><cell>0.338</cell><cell>0.330</cell><cell>0.307</cell><cell>0.295</cell><cell>0.286</cell><cell>0.279</cell><cell>0.277</cell><cell>0.272</cell><cell>0.270</cell></row><row><cell>0.435</cell><cell>0.422</cell><cell>0.421</cell><cell>0.417</cell><cell>0.351</cell><cell>0.335</cell><cell>0.332</cell><cell>0.330</cell><cell>0.325</cell><cell>0.318</cell><cell>0.441</cell><cell>0.430</cell><cell>0.416</cell><cell>0.415</cell><cell>0.412</cell><cell>0.387</cell><cell>0.382</cell><cell>0.380</cell><cell>0.379</cell><cell>0.377</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) Thermal to visible</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI EVALUATION</head><label>VI</label><figDesc>OF THE PROPOSED MSTN CONFIGURED WITH PEER METHODS ON THE LARGE-SCALE SYSU-MM01 DATASET. RANK AT r MATCHING ACCURACY(%) AND MAP (%) ARE REPORTED.</figDesc><table><row><cell>Settings</cell><cell></cell><cell></cell><cell>All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Indoor Search</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>eBDTR [17]</cell><cell>27.82</cell><cell>58.32</cell><cell>67.34</cell><cell>81.34</cell><cell>28.42</cell><cell>32.46</cell><cell>66.72</cell><cell>77.42</cell><cell>89.62</cell><cell>42.46</cell></row><row><cell>eBDTR (Ours)</cell><cell>47.32</cell><cell>76.24</cell><cell>87.02</cell><cell>94.26</cell><cell>47.92</cell><cell>51.26</cell><cell>81.32</cell><cell>91.22</cell><cell>96.73</cell><cell>60.22</cell></row><row><cell>Improvements</cell><cell>70.09↑</cell><cell>30.73↑</cell><cell>29.22↑</cell><cell>15.88↑</cell><cell>68.61↑</cell><cell>57.92↑</cell><cell>21.88↑</cell><cell>17.82↑</cell><cell>7.93↑</cell><cell>41.83↑</cell></row><row><cell>MAC [25]</cell><cell>33.26</cell><cell>65.10</cell><cell>79.04</cell><cell>90.09</cell><cell>36.22</cell><cell>33.37</cell><cell>67.02</cell><cell>82.49</cell><cell>93.69</cell><cell>44.95</cell></row><row><cell>MAC (Ours)</cell><cell>50.08</cell><cell>78.46</cell><cell>88.04</cell><cell>94.16</cell><cell>48.96</cell><cell>55.06</cell><cell>84.65</cell><cell>92.32</cell><cell>97.02</cell><cell>62.72</cell></row><row><cell>Improvements</cell><cell>50.57↑</cell><cell>20.52↑</cell><cell>11.39↑</cell><cell>4.52↑</cell><cell>35.17↑</cell><cell>65.00↑</cell><cell>26.31↑</cell><cell>11.92↑</cell><cell>3.65↑</cell><cell>39.53↑</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII CROSS</head><label>VII</label><figDesc>-DATASET EVALUATION. THE MODELS ARE TRAINED ON SYSU-MM01 DATASET AND TESTED ON THE REGDB DATASET. RANK AT r MATCHING ACCURACY(%) AND MAP (%) ARE REPORTED.</figDesc><table><row><cell>Settings</cell><cell></cell><cell></cell><cell cols="2">Visible to Thermal</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Thermal to Visible</cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>3.12</cell><cell>7.01</cell><cell>10.31</cell><cell>15.22</cell><cell>4.29</cell><cell>2.72</cell><cell>5.66</cell><cell>8.33</cell><cell>12.24</cell><cell>4.11</cell></row><row><cell>MAC [25]</cell><cell>3.42</cell><cell>7.89</cell><cell>11.23</cell><cell>17.62</cell><cell>4.62</cell><cell>3.28</cell><cell>7.62</cell><cell>10.75</cell><cell>16.32</cell><cell>4.78</cell></row><row><cell>MACE (Ours)</cell><cell>4.43</cell><cell>8.99</cell><cell>12.96</cell><cell>19.09</cell><cell>5.57</cell><cell>4.44</cell><cell>9.13</cell><cell>12.63</cell><cell>19.07</cell><cell>5.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII EVALUATION</head><label>VIII</label><figDesc>OF THE PROPOSED MSTN ON THE LARGE-SCALE SYSU-MM01 DATASET WITH DIFFERENT BACKBONE NETWORKS. NOTE THAT ALL OF THEM UTILIZE THE COMBINATION OF SOFTMAX LOSS AND BI-DIRECTIONAL TRIPLET LOSS AS BASELINE LEARNING OBJECTIVE. RANK AT r MATCHING ACCURACY(%) AND MAP (%) ARE REPORTED.</figDesc><table><row><cell>Settings</cell><cell></cell><cell></cell><cell>All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Indoor Search</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell></row><row><cell>Baseline [25] (AlexNet)</cell><cell>20.42</cell><cell>56.48</cell><cell>62.42</cell><cell>76.42</cell><cell>19.88</cell><cell>28.84</cell><cell>60.52</cell><cell>73.42</cell><cell>88.23</cell><cell>34.52</cell></row><row><cell>MSTN (AlexNet)</cell><cell>30.32</cell><cell>61.22</cell><cell>73.58</cell><cell>86.72</cell><cell>31.48</cell><cell>32.88</cell><cell>66.82</cell><cell>80.38</cell><cell>92.02</cell><cell>41.26</cell></row><row><cell>MACE (AlexNet)</cell><cell>38.43</cell><cell>65.12</cell><cell>76.42</cell><cell>88.14</cell><cell>36.11</cell><cell>42.62</cell><cell>71.03</cell><cell>84.42</cell><cell>94.28</cell><cell>48.42</cell></row><row><cell>Baseline [25] (ResNet50)</cell><cell>30.67</cell><cell>60.73</cell><cell>74.76</cell><cell>87.57</cell><cell>32.90</cell><cell>33.50</cell><cell>67.28</cell><cell>81.64</cell><cell>93.38</cell><cell>44.83</cell></row><row><cell>MSTN (ResNet50)</cell><cell>45.22</cell><cell>74.22</cell><cell>84.51</cell><cell>93.15</cell><cell>45.79</cell><cell>49.53</cell><cell>78.75</cell><cell>88.56</cell><cell>95.49</cell><cell>58.15</cell></row><row><cell>MACE (ResNet50)</cell><cell>51.64</cell><cell>78.24</cell><cell>87.25</cell><cell>94.44</cell><cell>50.11</cell><cell>57.35</cell><cell>85.67</cell><cell>93.02</cell><cell>97.47</cell><cell>64.79</cell></row><row><cell>Baseline [25] (DenseNet121)</cell><cell>31.82</cell><cell>61.78</cell><cell>75.84</cell><cell>88.47</cell><cell>33.42</cell><cell>34.56</cell><cell>69.02</cell><cell>82.42</cell><cell>93.84</cell><cell>46.51</cell></row><row><cell>MSTN (DenseNet121)</cell><cell>46.52</cell><cell>76.03</cell><cell>85.82</cell><cell>93.08</cell><cell>46.92</cell><cell>50.42</cell><cell>79.63</cell><cell>89.42</cell><cell>95.28</cell><cell>57.72</cell></row><row><cell>MACE (DenseNet121)</cell><cell>52.72</cell><cell>78.92</cell><cell>88.03</cell><cell>94.28</cell><cell>52.08</cell><cell>58.48</cell><cell>86.73</cell><cell>93.28</cell><cell>97.06</cell><cell>66.32</cell></row><row><cell>One-stream Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>One-Stream (Baseline)</cell><cell>44.68</cell><cell>73.85</cell><cell>84.75</cell><cell>92.24</cell><cell>44.91</cell><cell>48.68</cell><cell>78.02</cell><cell>87.62</cell><cell>94.95</cell><cell>57.43</cell></row><row><cell>One-Stream (MACE)</cell><cell>49.63</cell><cell>77.12</cell><cell>86.54</cell><cell>93.16</cell><cell>48.21</cell><cell>54.26</cell><cell>83.01</cell><cell>91.42</cell><cell>96.76</cell><cell>61.82</cell></row><row><cell>MSTN (MACE)</cell><cell>51.64</cell><cell>78.24</cell><cell>87.25</cell><cell>94.44</cell><cell>50.11</cell><cell>57.35</cell><cell>85.67</cell><cell>93.02</cell><cell>97.47</cell><cell>64.79</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We name the person images captured at night with special spectrum cameras (either infrared<ref type="bibr" target="#b13">[14]</ref> or thermal<ref type="bibr" target="#b14">[15]</ref> cameras) as thermal images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Authorized licensed use limited to: Carleton University. Downloaded on June 29,2020 at 14:06:32 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of open-world person reidentification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by deep asymmetric metric embedding</title>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic Structural Latent Representation for Unsupervised Embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamical hyperparameter optimization via deep reinforcement learning in tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Paying attention to video object pattern understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transferable joint deep learning for unsupervised person reidentification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jingya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shaogang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Purifynet: A robust person re-identification model with noisy labels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning view-specific deep networks for person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3472" to="3483" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic graph comatching for unsupervised video-based person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2976" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive learning for person re-identification with one example</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with generative adversarial training</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="677" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rgb-infrared crossmodality person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-S. Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person recognition system based on a combination of body images from visible light and thermal cameras</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">605</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical discriminative learning for visible thermal person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bi-directional centerconstrained top-ranking for visible thermal person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dictionary alignment with re-ranking for low-resolution nir-vis face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mudunuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="886" to="896" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coupled deep learning for heterogeneous face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visible thermal person reidentification via dual-constrained top-ranking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1092" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person reidentification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enhancing the discriminative feature learning for visible-thermal cross-modality person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09659</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hsme: Hypersphere manifold embedding for visible thermal person re-identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8385" to="8392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modality-aware collaborative learning for visible thermal person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="347" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visible-Infrared Person Re-Identification via Homogeneous Augmented Tri-Modal Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local semantic siamese networks for fast tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing (TIP), volume=27, number=5</title>
		<imprint>
			<date type="published" when="2017">pages=2368-2378, year=2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video person reidentification by temporal residual learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1366" to="1377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reidentification with rgb-d sensors</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>ECCVW</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust depth-based person reidentification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2588" to="2603" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>arX- iv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Incremental deep hidden attribute learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantically selective augmentation for deep compact person re-identification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ponce-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hannunna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops (ECCVW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identity-aware textualvisual matching with latent co-attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Specific person retrieval via incomplete text description</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An efficient framework for visible-infrared cross modality person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06498</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hpiln: A feature learning framework for crossmodality person re-identification</title>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03142</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian face sketch synthesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1264" to="1274" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning invariant deep representation for nir-vis face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2000" to="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Re-ranking high-dimensional deep local representation for nir-vis face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with cnn visual features: A new baseline</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics (TCYB)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="460" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving night-time pedestrian retrieval with distribution alignment and contextual distance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics (TII)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep perceptual mapping for crossmodal face recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="438" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative learning for deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1837" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7528" to="7538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image in IEEE Conference on Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08332</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dual-alignment feature embedding for cross-modality person re-identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations for visible-infrared person re-identification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="579" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He is currently a Research Scientist at Inception Institute of Artificial Intelligence</title>
	</analytic>
	<monogr>
		<title level="m">His research interests focus on multimedia retrieval, computer vision and pattern recognition</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<publisher>Mang Ye received the B.S. and M.S. degrees from Wuhan University</publisher>
			<date type="published" when="2013">2013. 2016. 2019</date>
		</imprint>
		<respStmt>
			<orgName>D degree in Computer Science from Hong Kong Baptist University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. He obtained the Ph</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">He is currently a Research Assistant Professor with Hong Kong Baptist University. His current research interests include intelligent video surveillance and biometric security</title>
		<author>
			<persName><forename type="first">Xiangyuan</forename><surname>Lan Received The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009, and the Ph.D degree in National Engineering Research Center for Multimedia Software from Wuhan University</title>
		<meeting><address><addrLine>China; Nanchang, China; Wuhan, China; Wuhan, China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2016. 2014</date>
		</imprint>
		<respStmt>
			<orgName>South China University of Technology ; D. degree from the Department of Computer Science, Hong Kong Baptist University ; Information Science and Technology, Jiujiang University</orgName>
		</respStmt>
	</monogr>
	<note>2007, M.S degree in International School of Software from Wuhan University. His research interests include person re-identification, image retrieval and machine learning</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">He has published more than 100 journal and conference papers, eight papers are selected as the ESI Hightly Cited. His current research interests include deep learning and computer vision. He serves as an Associate Editor for IEEE Trans. on Image Processing</title>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks and Learning Systems, Neurocomputing and other journals</title>
		<imprint/>
		<respStmt>
			<orgName>Computer Science, Beijing Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>M&apos;11-SM&apos;12) is currently acting as the Lead Scientist at the Inception Institute of Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
