<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selecting Concise Training Sets from Clean Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Plutowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Halbert</forename><surname>White</surname></persName>
						</author>
						<title level="a" type="main">Selecting Concise Training Sets from Clean Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D8E4E8BE50F7354240A7CC657492D0C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We derive a method for selecting exemplars for training a multilayer feedfoward network architecture to estimate an unknown (deterministic) mapping from clean data, i.e., data measured either without error or with negligible error. Active data selection chooses from a given set of available examples a concise subset of training "exemplars." In practice, this amounts to incrementally growing the training set as necessary to achieve the desired level of accuracy. Our selection criterion does not depend on using a neural network estimator, thus it may be used for general purpose nonlinear regression using any statistical estimator.</p><p>The objective is to minimize the data requirement of learning. In a particular sense, we are performing a kind of data compression, by selecting exemplars representative of the set of all available examples. Towards this end, we choose a criterion for selecting training examples that works well in conjunction with the criterion used for learning, here, least squares. We proceed sequentially, selecting an example that, when added to the previous set of training examples and learned, maximizes the decrement of network squared error over the input space. When dealing with clean data and deterministic relationships, we desire concise training sets that minimize the Integrated Squared Bias (ISB). We use the ISB to derive a selection criterion for evaluating individual training examples, the AISB, that we maximize to select new exemplars.</p><p>We conclude with graphical illustrations of the method, and demonstrate its use during network training. Several benefits are apparent for practical use in a variety of applications. Experimental results indicate that training upon exemplars selected in this fashion can save computation in general purpose use as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Statement of the Problem</head><p>HE learning task is to train mutlilayer networks to T estimate a deterministic function from clean data. We use a gradient descent learning rule to modify the network weights. Rather than using all of the training examples, we utilize information from a partially trained network to select a concise subset of exemplars.</p><p>If this approach turns out to be generally applicable, several benefits are immediately apparent for a wide range of situations. First, if the set of available training examples is too large to contain in working memory, our method provides a feasible method of selecting a concise subset of exemplars representative (in a precise sense) of the entire set. Second, our derivation of the selection criterion does not depend on using a neural network estimator, so our method may be used for any statistical estimator of a deterministic function. Third, <ref type="bibr">Manuscript received February 26, 1991;</ref><ref type="bibr">revised March 29, 1992</ref> the method could be used in data-driven learning techniques where the model complexity required to achieve functional interpolation is directly related to the number of training examples. For example, Platt allocates the basis functions at a rate directly associated with the number of exemplars, which are chosen randomly from the set of candidates <ref type="bibr">[35]</ref>. Our principled way of choosing the exemplars could make Platt's algorithm more accurate or efficient by maximizing the information content of each exemplar, resulting in a smaller network.</p><p>Maximizing the information density of the training set may have the side benefit of making training more reliable and efficient. Second order techniques that require evaluation of the entire training set at each iteration may consequently be made more efficient, e.g., methods involving the Hessian matrix and conjugate gradient techniques. Currently, heuristics are often employed with conjugate gradient algorithms for the purpose of reducing the number of examples that need to be considered at each iteration [32]. Applications that require a set of estimators to be trained repeatedly upon a set of data may also be made more efficient, once a concise set of exemplars is obtained. For example, some approaches to complexity regularization require several networks to be trained upon the same data set.</p><p>We demonstrate that our method selects concise training sets, and that it can be used to reduce the cost of network training. The method works well on networks with multiple inputs and outputs. Theoretically, it should work on problems of any dimension. However, experience in high-dimensional domains is required to determine whether the benefits of our technique outweigh the overhead of the selection process. We have confirmed this conjecture on small problems, showing a reduction in the number of floating point operations required to train a network on several learning tasks. In fact, we found that once a concise set of exemplars is found, the cost of retraining another network to fit the entire set of candidates can be done at a fraction of the cost of training over all the examples. We can also grow the network while simultaneously growing the training set, again resulting in a reduction of floating point operations on the problems we tried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Approach</head><p>Our approach applies to the noiseless learning task exclusively. We choose a criterion for evaluating sets of training examples appropriate for use with the squared error performance criterion. It is intractable to select a set of exemplars that minimize this criterion directly, but it is feasible to maximize its decrement while growing the training set. Therefore, we sequentially add exemplars to the training set, training the 1045-9227/93$03.00 0 1993 IEEE network in between selection of successive exemplars. We derive a criterion for evaluating individual examples that allows us to select exemplars maximizing the decrement in squared error that would result from adding the new exemplar to the training set and training upon the resulting training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Previous Work</head><p>The problem of efficiently selecting data points for use in learning has been studied extensively. The problem has been referred to variously as "active learning," "sequential design," or "query-based learning." In a loose sense, we can think of a concise set of exemplars as analogous to the notion of "extreme points" employed by Cover for classification tasks [ 141. In computational learning theory, deduction programs can make queries to an oracle <ref type="bibr">[40]</ref>, <ref type="bibr">[20]</ref>, <ref type="bibr">[21]</ref>, <ref type="bibr">[29]</ref>. In the connectionist literature, there is a related approach in which a network is trained by selective attention <ref type="bibr">[2]</ref>.</p><p>Active data selection assumes that the model is allow to play an active role in either generating new exemplars or selecting exemplars from available examples. In the first approach (often called query learning), we are allowed to select the input values where future data will be gathered. In this case this task is to minimize the number of exemplars one has to gather, or to corroborate information crucial to the estimate. This is desirable when data measurements are costly, or when there are critical gaps in the data due to sampling variation. The second approach is the one we address in this paper, and we refer to it as active exemplar selection. This is concerned with selecting a concise subset of the available examples for use as training exemplars. This approach is concerned with minimizing the number of exemplars one has to train upon, given that many are available. Alternatively, we may view this approach as selecting a concise subset of exemplars representative of the entire set of available "candidate" training examples. This is also desirable when dealing with very abundant data, or when using data-driven learning methods. Note that active exemplar selection may utilize information about the targets since this information is available in the set of candidate examples, whereas query learning may select new exemplars based upon the input information only.</p><p>Active exemplar selection is inspired by, and in some respects, is similar to, query learning. Therefore, we provide a brief overview of query learning with references for the interested reader. Some approaches to query learning use a partially trained network to determine regions of uncertainty in the environment and then query the oracle for values of the function in these regions <ref type="bibr">[12]</ref>. Others use statistical sampling techniques to ensure that the environment is sampled sufficiently often in regions of the input space where the data is deemed to have high utility <ref type="bibr">[43]</ref>. Hwang et al. present a query-based approach for neural network learning of classification tasks that generates exemplars in the vicinity of classification boundaries [25], <ref type="bibr">[24]</ref>. Baum proposes another query-based learning algorithm for iteratively seeking out classification boundaries for which a convergence proof is supplied <ref type="bibr">[5]</ref>. MacKay uses a principled approach from a Bayesian perspective to obtain an information-based query strategy for training classification networks. The performance of the resulting technique corroborates the intuitive strategy of concentrating queries near classification boundaries <ref type="bibr">[31]</ref>.</p><p>Using queries for the more general task of estimating an unknown mapping from real-valued, noisy data has been studied extensively in the statistics and econometrics literature, in the theory of response surface methodology, optimal experiments, and sequential design <ref type="bibr">[17]</ref>, <ref type="bibr">[33]</ref>, [15], [9], <ref type="bibr">[28]</ref>. These techniques typically query in regions where the network estimate has high uncertainty. Stringent conditions are typically required to ensure that second order derivative information (or approximation thereof) is well-behaved. It may be necessary to ensure that the model is the correct one for the task, which is translated into our situation as meaning that the network must have the appropriate complexity <ref type="bibr">[30]</ref>. A significant amount of data may be required to "prime" the techniques <ref type="bibr">[15]</ref>. Edge effects can also be a problem, since the borders of the input domain can tend to the regions of high uncertainty <ref type="bibr">[30]</ref>, <ref type="bibr">[17]</ref>.</p><p>If it is known that the function to be learned is reasonably smooth, a query method exists for a kernal-based smoothing approach. Faraway uses the integrated mean squared error (IMSE) criterion to sequentially query the environment for training examples for a nonparametric regression [ 151. For more on the IMSE criterion the interested reader may refer to [17], [3], [9], <ref type="bibr">[28], 1131, [34]</ref>, <ref type="bibr">[39]</ref>. Fedorov obtains several querying methods for performing interpolation and extrapolation from noisy real-valued data using a correctly specified model, including a method for obtaining new data which is maximally informative with respect to comparing competing models.</p><p>Our approach is inspired by the work of Fedorov and Faraway, where the IMSE criterion is used to evaluate training sets. Our sampling criterion, the ISB, is derived directly from the IMSE as a special case appropriate when dealing with clean data and deterministic relationships <ref type="bibr">[36]</ref>. Note that the querybased methods discussed above can be used in conjunction with our technique, to obtain more data from the environment once the current set has been learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">DEFINITION OF THE LEARNING TASK</head><p>We consider the learning task of determining the relationship between two (possibly random) vectors X and Y , where X takes values in X c W, and Y takes values in Y c Rs, for integers r and s. We refer to X as the input space. For concreteness we take X to represent a vector of input variables and Y a vector of target variables. For simplicity, let s = 1.</p><p>The learning task is thus one of training a neural network with r inputs and one output.</p><p>We adhere to the framework provided by White <ref type="bibr">[41]</ref>. We draw n observations on X , z" = (XI,. . . , xn), along with n corresponding observations on Y, yn = (y1, . . . , yn). For convenience let 2 = (X',Y')', and zi = (z:,yi)', where the prime denotes transpose. We denote our sample, or, training set, as zn = (zl,... ,zn). We suppose that the measurement process could be continued indefinitely, to obtain a sequence of observations { z i } = { z i , i = 1,2, . . .}, as a realization of the random sequence {Zi = ( X l , yZ')'}, where X i and Y, have the distributions of X and Y , respectively for all i = 1 , 2 , . . . .</p><p>In our simulations we will have complete control in determining xi. We define a (fixed) "environmental" probability law If yi and xi can be measured with perfect precision, and the relation between Y and X is deterministic, then there is an exact functional relationship between xi and yi such that yi = g(xi) for some mapping g : %' -+ %. We also write Y = g ( X ) and Y, = g ( X i ) . This is also the limit of the case in which the relation between X and Y is random but measurement at the sample points is repeated many times, and the measurement averaged. In this case g represents the average relationship, g ( X ) = E ( Y ( X ) , the conditional expectation of Y given X .</p><p>Let f denote the output function of a neural network parameterized by weights w, where w is an element of a weight space W c ! X P (we assume W compact). Then, for some particular input vector x, and weight vector w, the network output is f(x, w). We will measure network performance using squared error. Our objective is to find a set of weights</p><formula xml:id="formula_0">w* E argmin,, J (g(x) -f ( x , ~) ) ~p ( d x ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">DERIVATION OF METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Integrated Squared Bias tor</head><p>We consider a (measurable) nonlinear least-squares estima-Given appropriate conditions, lit, displays a strong form of stochastic convergence to optimal weights w*, in that lit, + w* as.-P. [41, Theorem 11. One of these conditions dictates that { Z;} be independent and identically distributed (i.i.d.) random variables. Here, we violate this condition. Given zn = (zi, i = 1, . . . , n ) , we choose x,+1 according to some specific criterion that will depend upon 2". Therefore, the zi are obtained from random variables that are neither independent, nor identically distributed. Nevertheless, we expect that if { x i } is properly chosen, then a similar convergence result can be established.</p><p>A criterion for selecting training examples should work well in conjunction with the criterion used for learning (here, squared error.) We take as a suitable choice of criterion the Integrated Squared Bias (ISB), defined below. The ISB is the special case of the Integrated Mean Squared Error (IMSE) appropriate for use on clean data [9], <ref type="bibr">[28]</ref>.</p><p>Define I , as the learning rule giving the realization w, of lit, as a function of the realizations xn, yn:</p><p>Because we consider the case in which we learn from clean data, y" is completely determined by xn, as is w,. Therefore, for any xn the training sample becomes zn = ( x n , g ( x n ) ) ,</p><p>where we abuse notation by writing g(x") = (g(x1), g(x2), . . . , g(x,))'. Integrating the squared error over the input space gives the ISB:</p><p>where p is an appropriately chosen measure on ( X , B(X)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Minimizing ISB for Nonlinear Least Squares Regression</head><p>For a sample of size n, the optimal training set satisfies</p><p>x: = argminZn ISB(xn). Finding such a training set is computationally impractical. Here we pursue a more modest goal.</p><p>Given a training set xn, we shall seek x,+1 that maximizes the decrement in ISB, ISB(x") -ISB(x"+l), xn+l = (x", x,+1).</p><p>Note that this property for x n f l is a necessary condition for solving the computationally impractical global problem, but will not necessarily deliver the globally optimal solution. Nevertheless, this approach permits a computationally feasible and attractive method for sequential selection of exemplars. Let ?,+I be a candidate input, and ?, +' = {x",&amp;+l). Let X,(x") = l, <ref type="bibr">(xn,g(xn)</ref>), (where g(z") is as defined in the previous section), so that w, = Xn(xn), and put wn+l = X,+l(?i."+l).</p><p>Thus we seek to maximize where P2(w</p><formula xml:id="formula_1">) = s [ g ( x ) -f ( ~, ~) ] ~p ( d x ) .</formula><p>(The variable X is introduced to make explicit the role of x" in the noiseless environment; the variable P2 is introduced to allow a change of argument emphasizing the role of the network weights.)</p><p>We could pick xn+l to maximize (2) directly, albeit at great expense. Fortunately, a much simpler procedure can be obtained by exploiting certain properties of the learning algorithm.</p><p>To proceed, we take a first order Taylor expansion of O2(w,+1) around P2(w,), yielding <ref type="bibr">By Bartle,</ref><ref type="bibr">([4]</ref>, Corollary 5.9), the gradient and integral can be interchanged to yield provided in addition to the compactness of W that a f l d w exists on X x W , and that f ( x , . ) and the elements of V, f (x, .) are dominated by square integrable functions of x only. We have already assumed implicitly that g is square integrable. Consequently, Equation (4) provides a computationally simpler estimate of (2). For each successive selection of an example from a set of candidates, the integra in (4) need be computed once, and this can be approximated by summing over the set of candidate examples. Then, to evaluate each individual example in the set of candidates, we compute (wnw,+1), and take its inner product with the vector quantity obtained from the summation over the set of candidates.</p><p>To calculate (4), we need to estimate (w, -wn+1), given 2"+1. When n is large, it is generally true that wn+l is in a neighborhood of tu,. The particular weight update we assume satisfies this requirement. For example, as described in [39], we may use the Gauss-Newton weight update:</p><p>. V , f ( P + l , W") (g(2i."+1)f ( P + l , w , ) ) .</p><p>(5)</p><p>Here,</p><formula xml:id="formula_2">f ( k " + l , U l n ) = (f(x1, wn), f ( x 2 , w n ) , . ' . &gt; f ( z n , wn), Vwf(2"+1,wn) = (V7,f(z1,wn),..., V w f ( x n , W , ) , H(E"+1, wJ1 = ( V , j ( P + l , w,) f ( &amp; + l , %I)'.</formula><p>V w f ( 5 n + l , w,)), a matrix withp rows and n f l columns.</p><p>V w f ( P f l , wn)')-' acts as an approximation to the socalled Hessian matrix of second derivative information <ref type="bibr">[171, [371, [391.</ref> As an aside, note that the Gauss-Newton weight update we use here may be replaced. For example, we might use a steepest descent update, which would save computation at the cost of reduced accuracy. On the other hand, we could use a Newton weight update, which would provide higher accuracy but would require the Hessian matrix of second derivatives.</p><p>The estimate of w, -w,+l, (5), can be simplified by</p><formula xml:id="formula_3">noting that for n = 1 , 2 , . . . ,1/n V,f(zi, wn)(g(xi) - f ( z i , 7 ~~) )</formula><p>= 0. This is by definition of w,, which is optimal for xn. (Note that this remains a valid assumption when 20, is approximated by a locally optimal estimate 6, obtained by gradient descent.) As a result,</p><formula xml:id="formula_4">V w f ( P + 1 ; w n ) (g(5."+') -f ( P + l , W")) = VWf(&amp;l+l, wn)(d&amp;+l) -f ( L + l , wn)).</formula><p>We also employ the approximation Collecting the above approximations together, define our estimate of U), -w,+l to be giving as one implementation of (4) For use in comparing candidate examples with each other, note that the constant factors of ( <ref type="formula">6</ref>) may be eliminated, giving A&amp;+i = [H(x", wn&gt;]-'V,f(&amp;+i, wn)(g(&amp;+i)</p><p>f ( G + l , W " ) ) <ref type="bibr" target="#b7">(8)</ref> Ignoring the constant factor in <ref type="bibr" target="#b6">(7)</ref> and combining with <ref type="bibr" target="#b7">(8)</ref> leads to the criterion that we propose for selecting the next training example,</p><formula xml:id="formula_5">AISB(*,+1(x") = (9(&amp;+1) -f(&amp;+l, w,))' . V w f ( L + l , wn)'[H(x", wn)l-l . 1 V,f(., .Wn&gt;(dx) -f ( x , wn))pu(dz). (<label>9</label></formula><formula xml:id="formula_6">)</formula><p>In practice the distribution p is unknown, but can be well approximated by the empirical distribution of the candidate examples. This means that the integral appearing in ( <ref type="formula" target="#formula_5">9</ref>) is approximated by an average over all the candidate examples. This average need be computed only once. The training example x,+1 is selected so that zn+l E argmaxAISB(k,+llx").</p><p>(10)</p><p>x n + 1</p><p>Although we have arrived at this criterion by making use of approximations valid for large n, this criterion has an appealing interpretation apart from large sample considerations. Specifically, AISB can be interpreted directly as the square of the weighted Euclidean distance between the gradient of squared error with respect to network weights integrated over all candidate examples, and the gradient of squared error with respect to network weights for a single candidate example. (The matrix W-' performs the weighting.) Thus, picking %,+I to maximize AISB can be viewed as picking the single example having individual error gradient most highly correlated with error gradient of the entire set of examples. Learning with this example is therefore likely to be especially informative, relative to other available examples. The AISB criterion thus possesses heuristic appeal in training sets of any size. As n becomes large, the derivation above establishes that AISB approaches ISB(x") -ISB (P+'), providing further support for its use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION</head><p>We now consider the practical use of the AISB criterion be accomplished efficiently. Given a priori knowledge about rule to utilize to fit a single example, it is possible to calculate</p><formula xml:id="formula_7">p 2 ( w n ) -p2(w"+1) 2Aw'+1 V w f ( z 7 W a ) ( g ( z )</formula><p>the class of functions that we can expect the network learning</p><formula xml:id="formula_8">-f ( z , W n ) ) P ( d Z ) .<label>(7)</label></formula><p>the optimal location of x1 by employing summary statistics over the set of available examples. A single example is fit perfectly with a constant function (see Fig. <ref type="figure" target="#fig_6">3</ref>(a) and 6(a).) It is then straightforward to choose the single example minimizing AISB(z'). Calculate m = (l/N) E:=, g(x,),x, E xN, and select z 1 = argminXEZN (g(z) -m12.</p><p>Given z1 , we proceed to grow the training set by successive optimization of AISB. For n = 1 , 2 , . . . . we then choose</p><p>x , + 1 = arg max "r AISB(x(x").</p><p>(11)</p><p>In growing the training set, a number of practical matters must be addressed. Because we are trying to learn a deterministic mapping, it is appropriate to specify a tolerance tolN representing the desired accuracy desired for the fit over all of the candidates as measured by</p><p>We desire rmselv 5 tolL$r. Clearly, before selecting a new exemplar, we should always check whether rmseN 5 tolN, and quit if it does. We refer to a set of exemplars that give this condition as "sufficient." If this condition does not hold, we must determine when the current exemplars have been fit well enough to select a new one, as the underlying theory and practical experience suggest that, for a given network complexity, the selection criterion will work best if the fit is optimal for the current set of exemplars. Optimality for the given network complexity is ensured either by the finding that rmse, is sufficiently small, where</p><formula xml:id="formula_9">1'2 rmse, = - ( g ( x ) -f(z,w,)) [' X E X "</formula><p>or by finding that repeated optimization attempts do not give further improvement in rmse,. In implementing the requirement that rmse, be sufficiently small, we specify that rmse, 5 tolN when selecting x,+1. This requirement follows from the observation that if the given model cannot adequately fit the training points, then it is unlikely it will adequately fit the entire set of candidates, since xn c xN.</p><p>Our experience indicates that fitting zn to an even closer tolerance can result in a more concise set of exemplars, i.e., a sufficient set of smaller size. When the minimal number of exemplars is desired, rmse, should be stringently minimized.</p><p>If a somewhat larger than minimal set of exemplars is acceptable, then it may not necessary to obtain the globally minimal value for rmse, before selecting a new exemplar. The task of seeking the most concise training set can require more total overhead (which includes training as well as selection cost) than for another sufficient but larger set. Relaxing the tolerance on rmse, allows a new exemplar to be selected sooner. This reduces the cost of training over the exemplars, and can thus reduce total overhead as well, even though more exemplars may need to be selected to comprise a sufficient set. We regulate this by using a variable tol, to determine when rmse, is sufficiently small to select a new exemplar.</p><p>A problem that can arise during this process is that an exemplar may be selected that is close to a previous choice, in the sense that both input and target values are close together. This can indicate that optimization in the prior exemplars was not complete (i.e., rmse, can be reduced further) but also is observed to occur when network complexity is insufficient to achieve rmse, 5 to1N. Such exemplars too close to a previous selection are easy to detect, and in such cases we simply discard the new exemplar and continue training on the current set. To avoid the cost of repetitively selecting a new exemplar only to reject it, we modify tol, adaptively during training by reducing it when a selection is rejected. This also makes the method less sensitive to initial setting of tol,, in that we may set it to a larger value initially, say, to], = tolN, and then adaptively reduce it during training as necessary. Practical implementation of exemplar rejection is based on selecting a small number d. In our experiments the input and output were scaled equivalently, so we let d = O(to1N). If the input vector a new exemplar is within Euclidean distance d of a previous exemplar in input space, we compute the distance between their output values, and if their output values are closer than tolN, we reject the new exemplar. This permits us to keep exemplars in regions of the input space where the output varies rapidly with the input.</p><p>If the learning algorithm reaches a local minima with tolN &lt; rmse,, then either the learning rule needs to continue its search within the current weight space in order to obtain a better optimum, or network complexity is insufficient to fit the current set of exemplars. Therefore, at this point we either restart network training or modify network complexity and continue.</p><p>If the learning algorithm reaches a local minima with tol, &lt; rmse, 5 tolN &lt; rniseN, we attempt to add a new exemplar, even though rmsell 5 tol, has not been met, since we have found that the method will often select an informative exemplar anyway (i.e., one not too close to previous choices.)</p><p>If the new exemplar is rejected, we either restart network training or modify network complexity and continue. It may be difficult or expensive to determine whether a given network can fit a set of exemplars within tolerance. Although we may not know in general when an estimate is sufficiently close to a globally optimal solution, in particular circumstances we may employ capacity results for neural networks to determine whether this is the case. (cf., Vapnik-Chervonenkis dimension [l], [6].) For example, assume that each unit employs the usual sigmoid squashing function. If h is the number of hidden units in the neural network, then, given any set of n &lt; h examples, there exist weights w, such that Er=l (g(x,)f ( x t , w , ) ) ~ = 0. [23, Theorem 2.51. Thus, in this case we know in advance that a set of weights exists that can fit the training examples exactly.</p><p>To summarize, we now present an algorithm for implementing the AISB criterion that embodies each of the considerations addressed above. The general algorithm begins with a network of arbitrary complexity, and increases the complexity appropriately as the exemplars become too difficult for the network to fit. In our experiments we always begin with a network consisting of a single hidden unit. We use multistarts to determine when the network is having difficulty fitting the data, by restarting the network a number of times on the data before increasing network complexity. Let restarts count the number of restarts, and let mux be the maximum number of restarts we allow before increasing network complexity.  If restarts &lt; max, restart the network, increment restarts, and continue training.</p><p>Otherwise, increase network complexity, set restarts = 0, and continue training. This algorithm could be modified to utilize more sophisticated ways of regulating network complexity. It is useful in its present form for many practical situations, and is sufficient for demonstrating the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. DEMONSTRATING THE TECHNIQUE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Illustrating the Behavior of AISB</head><p>Here we wish to convey the qualitative behavior of the AISB criterion. We illustrate this using the network illustrated in Fig. <ref type="figure" target="#fig_5">2</ref> to obtain the target function g ( x ) illustrated in Fig. <ref type="figure">l(a)</ref>. This is a one-dimensional map with upper and lower plateaus at about 0.2 and 0.8, respectively, and steep drop off. The drop off is shifted left of center, so that the lower plateau is more extensive than the upper plateau. The diagram illustrates a feedfonvard network with 1 input, 1 output, and 1 hidden unit. Each unit employs for its transfer function the usual sigmoidal output activation .(U) = (1 + exp ( -U ) ) -' , where n is the weighted sum of the inputs to the unit. The arcs are labelled by real values, denoting the weights. The values of the weights in this figure give a set of weights for which f(z, w ) = g(x). This set of weights is w* = (-20.0,70.0,1.5, -3.0).</p><p>Note the size of weights w1 and 202 relative to w3 and wg. This is a manifestation of the region in g of high first derivative, which the network function fits by employing large magnitude weights on the hidden unit. In what follows, we will illustrate the AISB for three different network functions obtained by perturbing the weights of this network slightly. For clarity of illustration, we will assume all inputs are equally likely, so that 1-1 is the uniform distribution.</p><p>In Fig. l(b)-(d), we modify one weight at a time to obtain network functions slightly perturbed from g(z). In each figure, the network function so obtained is given by the dashed line. This has the incidental benefit of illustrating the role each weight plays in the network mapping. For example, in Fig. <ref type="figure">l(b</ref>) weight w3 was perturbed, by putting w3 = 1.9. This results in an upward shifting of the entire mapping.</p><p>The dotted line gives the plot of AISB, normalized to allow it to be plotted along with f ( x , w ) and g. Plotting f and g ( x ) together allows a comparison of the two functions, and provides a visualization as to how the AISB criterion varies as the difference between f and 9. <ref type="bibr">Fig. l(b)</ref> corroborates the intuition that the sampling criterion indicates the potential of a candidate example for reducing squared error over the input space. Although both plateaus are shifted upwards an equal distance, a training example chosen from the region of input space corresponding to the lower plateau is weighted more heavily than one chosen from the upper plateau. Presumably, that is because an example from the lower plateau might result in bringing the lower plateau of f back in line with g, and this would be better in comparison with choosing an example from the upper plateau, which covers a smaller region of the input space. To see where the next exemplar would be selected by (lo), observe on the coordinate axis the location of the maximum value of the Delta ISB curve.</p><p>Fig. <ref type="figure">l(c</ref>) and (d) illustrate that the criterion gives high value to candidate examples lying in regions where more information is necessary. The criterion displays an intuitively pleasing behavior in these figures, but is not obvious that the criterion will always act in accordance to our immediate intuition in more general settings. For example, the criterion may place high value on examples in regions where network error is substantially lower than the network error over other regions. This can occur by the effect of , U which essentially weights each example according to the environmental probability law. It may also occur due to the form of the basis functions. This is because small changes in the network function in one region resulting from the attempt to fit a particular example can have a global impact upon the network function far away from the new exemplar. This is illustrated in the next sections, where we next apply the AISB criterion in a sequential manner during network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training a Fixed Size Network on Actively Selected Exemplars</head><p>We investigate what sequence of examples will be chosen by the criterion while learning the mapping of Discussion of some implementation details is in order. We use the conjugate gradient algorithm to set the network weights, in order to make the computational comparisons more fair. We discuss this in more detail below in Section V-B-1). As our selection criterion, we use the implementation of (10) given by (9). In the interest of computational simplicity, we modified the equation for the purpose of demonstrating the technique. The particular gradient descent algorithm we used (conjugate gradient) does not calculate H ( z n , w,), so we used an approximation to it obtained by calculating only the diagonal elements of H ( x " , w,). This has the additional benefit of resulting in a matrix that is guaranteed to be positive definite, and inexpensive to invert. When the number of current exemplars was less than the number of parameters in the network, we set H ( x n , w,) to the identify matrix to avoid the situation where elements of the diagonal matrix become large due to ill-conditioning. Although we recommend using H ( x n , w , ) whenever possible, we note that the performance of the AISB criterion did not seem overly sensitive to the approximations we made here.</p><p>We followed the algorithm given above in Section IV. We began with a training set consisting of a single example. In After obtaining w1 on xl, we computed AISB(z1x') for</p><p>x E z N . AISB is normalized by its maximum absolute value, so that it may be plotted along with f and g. Note that AISB(x1d) may take on negative values.</p><p>The next exemplar selected by (10) is located by the maximum value of the AISB curve. This exemplar is appended to the initial training set x1 to give the second training set, x2. <ref type="bibr">Fig. 3(b)</ref> shows the result of training on x2, the two examples in x2 being indicated, as before, by markers superimposed upon g(x). Note how training on x2 resulting in improvement along the entire lower plateau. After obtaining w2, AISB was maximized at a point 2 3 near the top of the steep portion of g. Note that this point is not one for which network error was maximal. Hence, AISB need not coincide with the heuristic of training on examples according to maximal error. We expect this to be the case for the squared error learning rule we have assumed. After training on x3, the network function shows only a slight improvement, but this is enough to allow the selection of the final exemplar which provides enough information to complete the learning task. The final exemplar was also selected on the steep portion, near, but not quite on the upper plateau. However, due to the sigmoidal basis functions used in the network, the information provided by 5 3 and x4 is sufficient for the network to learn the upper plateau, even though no exemplars are located in that region. The result is a network function f (., w4) within tolN of g. An alternative selection method can be obtained by selecting exemplars uniformly distributed in the input space. We chose a sequence of training sets in this fashion and found that this particular learning task required 16 evenly spaced exemplars in order to fit the function within the desired tolerance. The set of exemplars selected in this fashion is displayed along with the resulting trained network function in Fig. <ref type="figure" target="#fig_7">4</ref>. Note the sparsity of exemplars on the steep portion of the curve, and the relative density of exemplars along the plateaus.</p><p>1) Computational cost comparison: Here we illustrate how active selection may save computation in general purpose use. We compare it with the baseline approach of training over all available examples. Before reporting the results, we first discuss some relevant implementation details.</p><p>We measured computational cost in terms of floating point operations. We counted every multiply, divide, and add used during every phase of network training. It suffices to report only on the mutiplies here, since the number of adds and divides have been proportional to the multiplies in all of our experiments.</p><p>We compare the computational cost of the algorithm in Section 1V with the cost of using all of the candidate examples. The question is whether or not the overhead due to exemplar selection will be compensated for by savings due to a lower cost of processing a concise training set. We also keep track of the cost of selecting exemplars, in order to determine the percentage of the total cost that is required in overhead. This allows us to analyze whether or not it is important to try to reduce the overhead of the selection process. However, the bottom line is whether or not the total cost of fitting the given set of candidates may be reduced by active selection.</p><p>The AISB method requires a close fit over the exemplars. This may be obtained using any kind of gradient descent, such as a stochastic approximation method (e.g., backpropagation), or steepest descent. We tried using a steepest descent algorithm in previous experiments, and found that several restarts were typically necessary for the learning rule to converge to weights near the optimal. This made the experiments costly to run, and comparisons difficult. Using steepest descent upon actively selected exemplars sometimes required restarts, but the cost of each restart was much less, due to the smaller training set size. Our intuitive judgment is that the AISB method would be far cheaper in this case, as we expect our technique to become more cost effective as the number of restarts increases. In order to make more reliable comparisons and to obtain the best baseline we chose to use conjugate gradient, which requires far fewer restarts to find a good set of weights. This largely avoided the case in which a large number of restarts introduced an experimental bias favoring our technique. While we do not do so here, it would be interesting to explore whether active selection could make steepest descent competitive with more sophisticated techniques by reducing the cost of restarts. Now we present the results of comparing the cost of active selection with the baseline on the learning task illustrated in Fig. <ref type="figure" target="#fig_6">3(a)-(d)</ref>. In this case, the network is static, needing only one hidden unit. We used d = tolN = 0.01, and tol, = ( 1 / 2 ) t o l ~. We experimented with candidate sets of size 200 and 500, to illustrate how the number of available examples affects the cost benefit. For each size candidate set we compared the total cost of training using active selection with the total cost of training on all of the candidates. This gives four cases. We evaluated five runs for each case. To ensure each training run started from the same initial conditions, we generated five sets of random initial weights, using the same set of five initial weights to initialize the training runs in each of the four cases.   <ref type="table" target="#tab_2">I</ref> and<ref type="table" target="#tab_1">I1</ref> the method selected sets of 4 exemplars on all but one of the runs, indicating the consistent performance of the method. Table <ref type="table" target="#tab_2">I</ref> shows the comparison of the cost of learning the shifted sigmoid function using 200 candidate training examples. The "baseline" column gives the cost of training over all 200 examples. the next set of columns pertain to active exemplar selection. The total cost of training is given along with the ratio of this cost with the baseline cost in parenthesis. The "overhead" gives the portion of the total cost due to exemplar selection. The number of exemplars selected is given by n. cost is measured in millions of multiplies. The bottom row</p><p>gives the median values. Table <ref type="table" target="#tab_1">I1</ref> shows the comparison of the cost of learning the shifted sigmoid function using 500 candidate training examples. The "baseline" column gives the cost of training over all 500 examples. The next set of columns pertain to active exemplar selection. The total cost of training is given next. The ratio of this cost to the baseline cost is given in parentheses. The "overhead" is the portion of the total cost due to exemplar selection. The number of exemplars selected is given by n. Cost is measured in measured in millions of multiplies. The bottom row gives the median values. The average overhead of the selection method comprised 58.8% of the total cost on the runs using 200 candidates, and 43.5% on the runs using 500 candidates. Observe that if the overhead due to exemplar selection is subtracted from the total cost, we see that the cost due to training is significantly less than the cost of training over all available examples, corroborating our intuition that concise training sets can reduce computation. The important question, however, is whether or not training by active selection will reduce computation when the overhead is included in the total cost. As indicated on the results, this was indeed the case for this experiment. The median cost of the AISB method required 17.9% of the median baseline cost on 200 candidates, and only 10.4% of the baseline on 500</p><p>candidates. Note that when N is increased from 200 to 500, the median cost of active selection grew less than the median baseline cost, which more than doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Active Selection and Complexity Regularization</head><p>We now illustrate how the sampling criterion behaves upon a network which does not necessarily have the correct number of hidden units to fit the desired function exactly. We generated a mapping using the 3-hidden unit feedforward network with  the connectivity and weight values shown in Fig. <ref type="figure" target="#fig_8">5</ref>, (which we refer to as Network 2) using the same type of sigmoidal units used in Network 1. Network 2 generates the skewed unimodal mapping g(z) illustrated in Fig. <ref type="figure" target="#fig_9">6</ref>(a). This choice of g(z) gives a function difficult for a network of this particular architecture to fit with less than three hidden units. For the first demonstration we used Network 2 to generate N = 200 candidate examples uniformly at random over the input spac '.</p><p>The training regime is the same as for the experiment reported in the previous section, except that here we are allowed to grow the network. The network initially has one hidden unit. The goal is to fit the candidate examples to within a rmseNtolerance of tolN = 0.01. We used tol, = O.tjO1. As training progresses, the algorithm automatically modifies network complexity by adding another hidden unit when the network can no longer fit the exemplars. When the network reached a local minima without attaining rmse, 2 tolnr, we restarted the network, randomizing the weights to small values. A maximum of 5 restarts was allowed before growing the network. Examining Fig. <ref type="figure" target="#fig_9">6</ref>(c), we expect that this network could not fit the given x3 with a single hidden unit, since le basis function used here is a sigmoidal function. Our complexity regularization technique detected this automatically, growing the network at the appropriate time. This also occurs when n = 5, (see Fig. <ref type="figure" target="#fig_9">6(e)</ref>) where a thil(1 hidden unit was added to fit the 5 exemplars. The AISB method worked well along with this complexity regularization technique. The experiment concludes with the close fit illustrated in Fig. <ref type="figure">7</ref>.</p><p>We compared this selection method with uniformly spaced exemplars. This task required 31 uniformly spaced exemplars in order to fit the candidates within the desired tolerance. The set of exemplars selected in this fashion is displayed along with the resulting trained network function in Fig. <ref type="figure" target="#fig_11">8</ref>. This figure illustrates how the uniformly spaced distribution required a close spacing of exemplars in order to sample rapidly changing portions of the function sufficiently well.</p><p>1) Cost comparison: We now compare the cost of active selection with the baseline of training over all of the candidate examples upon the skewed hump learning task. This task required choosing the correct sized network for the task, so we applied the network growing method described above to the baseline method. We started each method with a single hidden unit, and added an extra unit if the exemplars have not  been fit within tolerance after 10 restarts (see Table <ref type="table">111</ref>). To evaluate how much of the benefit of active selection is due to the number of restarts, we ran two more experiments in which the baseline was allowed to grow without any restarts if conjugate gradient fell into a local minima without fitting the exemplars within tolerance after a single run. This is not unreasonable, given the reliability of conjugate gradient, as we found that conjugate gradient would quite often fit a given candidate set on the first run, given a correctly specified network. However, occasionally conjugate gradient will fall into local minima, and so this approach is admittedly less reliable than using restarts. We grew the network trained by active selection using 5 restarts. This is the run illustrated in Fig. <ref type="figure" target="#fig_9">6</ref>(i) and Fig. <ref type="figure">7</ref>; the results are tabulated in the second row of Table <ref type="table">111</ref>. On this particular run, conjugate gradient did select the correct size network using zero restarts, fitting the candidates on the first try with 3 hidden units. Active selection still required only 58.3% of the computation required by the baseline.</p><p>To evaluate the effect of having more candidate examples, we performed by experiment again using 500 candidates, again allowing the baseline method 0 restarts, while the active selection method used 5 restarts. However, in this case we had to run the baseline again to allow it to converge to the correct size network, as it was unable to fit the candidates on the first try when it grew to 3 hidden units. (We mark this result with an asterisk in Table <ref type="table">111</ref> to remind that the cost of the previous run is not included.) This run gives a comparison of the typical cost of using active selection with the cost for the baseline when it does converge. The benefit of the AISB method was even greater than it was on the smaller set of candidates, requiring only 22.2% of the baseline cost to fit the 500 candidates. In this case, whereas the baseline failed to fit the candidates with smallest network on the first run, the AISB algorithm determined it correctly the first time.</p><p>To get a better comparison of average results, we ran 5 runs of each method upon a set of 200 candidates, and then repeated the comparison with 5 runs each upon a set of 500 candidates. Each of the four sets training runs was initialized using the same set of 5 initial randomized weights. For a conservative comparison we require the network trained by active selection determine the correct network complexity, but give the baseline method the advantage of being provided with the correct number of hidden units. The baseline method is restarted if it fails to fit the candidates. The cost of the restarts is summed into the total cost for each run. To eliminate any possible influence of our experience biasing the experiment through the initial choice of rmse,, we set tol, = t o l ~ in all of these runs. In our experience this is not the best initial setting for rmse, but it is reasonable. The results are tabulated in Tables <ref type="table" target="#tab_7">IV</ref> and<ref type="table" target="#tab_7">V</ref>. Table <ref type="table" target="#tab_6">IV</ref> compares the cost of learning the skewed hump function using 200 candidate training examples. The "baseline" column gives the total cost for all 200 examples. The next set of columns pertain to active exemplar selection. The total cost is given along with the ratio of the total cost with the baseline cost in parentheses. The "overhead" gives the portion of the total cost due to exemplar selection. The number of exemplars selected is given by n. Cost is measured in millions of multiplies. The bottom row gives medians over the five runs. Baseline runs 1 and 2 required 1 and 2 restarts, respectively. All the other baseline runs fit the candidates within tolerance on the first attempt. For the comparison of the cost of learning the skewed hump function using 500 candidate training examples, the "baseline" column gives the total cost of training over all 500 examples. The next set of columns pertain to active exemplar selection. The total cost is given along with the ratio of the total cost with the baseline cost in parentheses. The "overhead" is the portion of the total cost due to exemplar selection. The number of exemplars selected is given by n. Cost is measured in measured in millions of multiplies. The bottom row gives medians over the five runs. Baseline runs  2 and 5 fit the candidates on the first attempt; baseline runs 1, 3, and 4 required 3,1, and 2 restarts, respectively. Note that active selection typically required much less computation overall, even though it is responsible for the additional task of determining the appropriate network complexity. Note further that while the cost of the baseline method for N = 500 was typically much greater than for N = 200, the cost of active selection was typically only slightly higher. (The average cost of active selection actually decreased.) These runs (and in particular, Run 4 in Table <ref type="table" target="#tab_6">IV</ref>) indicate how the choice of rinse, can affect the cost of active selection. Observe that several of the runs selected more than 10 exemplars, the number that was found when tol, = tolN/10 = 0.001 (see Table <ref type="table">111</ref>). However, the typical cost of these runs was still on par with the cost that resulted from using tol, = 0.001. The exception is Run 4 in Table <ref type="table" target="#tab_6">IV</ref>, which cost more than the baseline. We repeated this particular run, reducing tol, by half to 0.005. this improved the cost substantially, resulting in 32.24 million multiplies instead of 78.28 million, selecting 12 exemplars rather than 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>We now turn to issues relevant to the practical application of our selection method. We emphasize the importance of obtaining a good solution to the training problem at each stage. We experimented briefly with an iterative training regime, in which little optimization was performed between successive examples. There was a tendency for the criterion to sample repetitively near previously sampled points when the optimization between successive examples was not stringent.</p><p>In some such cases a pathological condition emerged in which the technique entered a cycle, endlessly alternating between two examples. For this reason, we advocate stringent optimization at each step.</p><p>We now address the overhead due to the selection process. The cost of selecting a new exemplar is about the same as that of making two iterations through the entire set of candidates with a steepest descent learning rule. This can be cut in half, at the expense of allocating space equal to the size of the set of candidates, by storing partial results and making one iteration through the set of candidates. However, we expect that this will be unreasonable for large data sets. When dealing with large data sets the cost of exemplar selection could be reduced by appropriately subsampling the set of candidates <ref type="bibr">[32]</ref>.</p><p>Observe that the selection cost is incurred only occasionally, because typically network training will process a given set of exemplars for numerous iterations before selecting a new exemplar. When multistarts are used to determine the appropriate network complexity for the task, several restarts might be performed before a new exemplar is selected. In our experience we have noticed that when multistarts are used the overhead of exemplar selection tends to be a small percentage of the total cost of training.</p><p>In our experience with the technique, we have noticed that the AISB criterion can have multiple maxima. It may be possible to exploit this to try to reduce the cost of obtaining exemplars further by selecting more than one exemplar at a time. Rather than just selecting the single exemplar maximizing our criterion, we might select a set of exemplars that locally maximize the criterion <ref type="bibr">[22]</ref>. This reduces to the task of finding the modes of the selection criterion. Arbitrary decisions were made in the derivation of the active selection method we obtained. We now discuss the effects of these choices, and how they might be modified to obtain variants of the method for practical use on particular applications. It is an open question as to how sensitive the method is to H ( x n , w,), the estimate of the Hessian appearing in (9). Observe that the appearance of H is due to our choice of weight update. This choice could be modified, depending upon the degree of approximation that is deemed necessary for the particular learning task. Our experiments demonstrate that the method can work well even when H is approximated. Recall that in our demonstrations we approximated or replaced H for computation simplicity. This was sufficient for the intended purpose of illustrating the method, although we advocate calculating H directly whenever feasible. That the method still selected concise training sets and saved computation indicates that even the crude approximation of H we used may be adequate on some tasks.</p><p>When obtaining the most concise set of exemplars is the highest priority, we expect the calculating H directly will be desirable. We may go even further, calculating the Hessian directly, taking appropriate steps to detect when it becomes ill-behaved. The numerical optimization literature offers considerable guidance in this regard. It is a straightforward modification to use the actual Hessian in place of H . This is equivalent to replacing the Gauss-Newton weight update in our derivation with the Newton update.</p><p>We employ the least squares performance criterion. It would be straightforward to replace it with another choice. For example, one might use cross entropy for use on classification tasks [38]. The estimator could be modified to include a (differentiable) complexity regularization term as well, such as weight decay <ref type="bibr">[lo]</ref>. In this case, exemplars would be selected according to their potential worth in minimizing the sum of network complexity and estimation error.</p><p>Finally, we summarize our initial experience with the technique on more difficult learning tasks. Space limitations prevent us from discussing all of the experiments we have done so far, but we have applied the technique to learning tasks requiring networks with multiple inputs and outputs with similarly favorable results. More extensive experiments are necessary to corroborate these preliminary results, especially on learning tasks requiring high dimensional data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>The goal of this work is to explore the benefits of selective data handling. The AISB criterion we obtain is reasonable to compute, exhibits satisfying behavior, and is nonparametric in the sense that the complexity of the network need not be prespecified. Parameterization of the network function is necessary to evaluate training examples, but modifying the number of network parameters (weights) between successive training examples is allowed.</p><p>We demonstrate that the technique selects concise training sets, and that it works well in conjunction with a particular complexity regularization technique. The technique resulted in an overall computational savings for network training on the problems we tried. As might be expected, we found the cost benefit to depend upon the abundance of data. We expect the benefit to be amplified when used with a complexity regularization technique requiring a large number of training runs over a given data set, due to the low cost of training over a concise sets of exemplars.</p><p>The result we obtain has immediate practical applications. It may provide an advantage in domains where data is abundant, by pruning the training set to a manageable size, or with use in conjunction with data-driven network training algorithms. Our preliminary results indicate it may be useful in general purpose use as well, by reducing computation. More extensive benchmark tests are required on problems involving highdimensional data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. M. Plutowski is with the Department of Computer Science and Engineering, H. White is with the Institute for Neural Computation and the Department IEEE Log Number 9200941 University of California, San Diego, CA. of Economics, University of California, San Diego, CA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>,U</head><label></label><figDesc>that describes the relative frequencies with which different values for xi are set; p : B(Rr) -+ [0,1]. B(!Xr) is the Bore1 0-algebra generated by the open sets of Rr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 9 )</head><label>9</label><figDesc>for evaluating examples for training a one-hidden layer feedfonvard network. We have available a set of N examples from which to select training examples. Call this set z N , the set of candidate examples. Suppose we have trained the network using a subset of n examples 5". Given xn, we evaluate AISB(5,+,Izn) for all &amp;+I E z N . For simplicity, we begin by putting n = 1 and find x1 such that x1 = {XI} = arg min,l ISB(xl). This is computationally appealing, since fitting a single example by a neural network can -.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) Vwf(z",w,)(g(xn) -f(x", w,)) = 0 and rmse, &gt; tolN:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1.(a) Shows the desired function, g ( s ) , here a scaled and translated sigmoid generated by the network of Fig.2with weights U' = (-20, 7' 0.1.5. -3.0). The next three figures illustrate the AISB upon networks obtained by perturbing these weights. The perturbed weight vector is given at the bottom of each figure, with the perturbed weight in bold. The network function f(s. w ) is given by the dashed line, g ( s ) is given by the solid line, and the AISB, normalized for plotting purposes, is given by the dotted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. A simple one hidden layer neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. These figures illustrate the sequential selection of exemplars during learning. The solid line gives g(.r), here, the scaled and translated sigmoid of the previous figure. The dashed line gives f ( x , u s n ) , n = 1. 2 . 3 , 4 , and the dotted line gives the AISB(.rI.r") for n = 1. 2 . 3 . The exemplars are given by the cross hatch marks on the solid line. For each 71, the new exemplar . I ' , &gt; + I is given by the lowest valued x maximizing hISB(z(s").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Selecting uniformly spaced exemplars required 16 exemplars to fit the sigmoidal function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Network 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref>. The solid line shows the desired function, g, here a skewed hump. The exemplars are given by markers superimposed upon g. For each R, The network function f(.. uln) is given by the dashed line. For each R , sn+l is given by the smallest z maximizing AISB, which is given by the dotted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Final ht with n = 10 Fig. 7 .</head><label>107</label><figDesc>Fig. 7. Final result of active selection on the skewed hump. 10 exemplars were required overall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Selecting uniformly spaced exemplars required 31 exemplars to fit the skewed hump.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Note that we reset restarts after adding a new exemplar, since each new exemplar results in a new training set, xn, n = 1 , 2 , . . . .</figDesc><table><row><cell>1) Begin by selecting x1 minimizing ISB(d), and set</cell></row><row><cell>restarts = 0.</cell></row><row><cell>2) Continue training until one of the following occurs and</cell></row><row><cell>take the actions listed, in sequence:</cell></row><row><cell>(a) rmse, &lt; toln;</cell></row><row><cell>If rmseN &lt; toln;, quit.</cell></row><row><cell>Select x,+1 = arg max, AISB(xIx").</cell></row><row><cell>Check whether x,+1 is too close to previous</cell></row><row><cell>exemplars. If x,+1 is rejected, reduce tol, and</cell></row><row><cell>continue training.</cell></row><row><cell>Otherwise, append x,+1 to 9, set reslarts = 0</cell></row><row><cell>and continue training.</cell></row><row><cell>(b) V w f ( z n , w n ) ( g ( x n )</cell></row></table><note><p><p><p><p>f ( z : " , w,)) = 0 and rmse, 5 toln;:</p>If rmseN 5 tolN, quit.</p>Otherwise, select x,+1 = arg max, AISB(xlx"). Check whether z, +1 is too close to previous exemplars. If x,+1 is not rejected, append it to xn, set restarts = 0, and continue training.</p>Otherwise, if restarts &lt; max, restart the network, increment restarts, and continue training. Otherwise, increase network complexity, set restarts = 0, and continue training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I1</head><label>I1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc>Shifted sigmoid, -1-= 200.Shifted sigmoid, -Y = 500.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="2">Active selection</cell><cell></cell><cell></cell><cell>Baseline</cell><cell></cell><cell>Active selection</cell><cell></cell></row><row><cell>Run</cell><cell>cost</cell><cell>cost</cell><cell>overhead</cell><cell>n</cell><cell>Run</cell><cell>cost</cell><cell>cost</cell><cell>overhead</cell><cell>II</cell></row><row><cell>1.</cell><cell>2.94</cell><cell>0.47 (16.0%)</cell><cell>46.0%</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.</cell><cell>2.63</cell><cell>0.58 (22.1%)</cell><cell>35.6%</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.</cell><cell>2.02</cell><cell>0.58 (28.7%)</cell><cell>35.6%</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.</cell><cell>9.64</cell><cell>0.46 ( 4.8%)</cell><cell>46.8%</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.</cell><cell>1.85</cell><cell>0.45 (24.3%)</cell><cell>57.5%</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Med:</cell><cell>2.63</cell><cell>0.47 (17.9%)</cell><cell>46.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">As indicated by the results of Tables</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I11</head><label>I11</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">Learning skewed hump</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell cols="2">Active selection</cell><cell></cell></row><row><cell>N</cell><cell>cost</cell><cell>restarts</cell><cell>cost</cell><cell>restarts</cell><cell>overhead</cell><cell>R</cell></row><row><cell>200</cell><cell>495</cell><cell>10</cell><cell>55.5</cell><cell>10</cell><cell>2.5%</cell><cell>11</cell></row><row><cell>200</cell><cell>56.1</cell><cell>0</cell><cell>32.7</cell><cell>5</cell><cell>3.2%</cell><cell>10</cell></row><row><cell>500</cell><cell>286*</cell><cell>0</cell><cell>63.4</cell><cell>5</cell><cell>4.5%</cell><cell>11</cell></row><row><cell cols="7">Note*: the third baseline run entered a local minima without fitting rY on</cell></row><row><cell cols="7">the first try, and required a restart to converge; the number entered here is</cell></row><row><cell cols="3">the cost of the second run.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table111compares the cost of active selection with the baseline of using all of the available examples. The task is to learn the skewed hump generated by network 2. The first column gives the number of candidates, N . The second pair of columns pertain to the baseline of training on all N candidates. Cost is measured in millions of multiplies. This task involves choosing the correct size network, so the maximum number of restarts is given. The next set of columns apply to active exemplar selection. The total cost is given first; the portion of this used by active selection is given next, followed by the number of exemplars selected, n, and finally the number of restarts used during training. Likely due to the number of restarts, the overhead of the AISB method in the first row was quite low, at 2.5%. Training upon actively selected exemplars required only 11.2% of the multiplies required by the baseline. The ten restarts gave active selection a clear advantage, as the bulk of the baseline cost was due to the high cost of the restarts when the network did not have sufficient complexity for the task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Skewed hump,</cell><cell>= 200.</cell><cell></cell></row><row><cell></cell><cell>Bas e I i n e</cell><cell cols="2">Active selection</cell><cell></cell></row><row><cell>Run</cell><cell>cost</cell><cell>cost</cell><cell>overhead</cell><cell>n</cell></row><row><cell>1.</cell><cell>83.30</cell><cell>33.46 (40.2%)</cell><cell>2.60%</cell><cell>11</cell></row><row><cell>2.</cell><cell>381.43</cell><cell>31.46 (8.2%)</cell><cell>5.73%</cell><cell>16</cell></row><row><cell>3.</cell><cell>80.37</cell><cell>18.52 (23.0%)</cell><cell>6.49%</cell><cell>12</cell></row><row><cell>4.</cell><cell>61.08</cell><cell>78.28 (128.2%)</cell><cell>2.16%</cell><cell>15</cell></row><row><cell>5.</cell><cell>70.96</cell><cell>34.5 (48.6%)</cell><cell>4.81%</cell><cell>14</cell></row><row><cell>Med:</cell><cell>80.37</cell><cell>33.46 (41.6%)</cell><cell>4.81%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V</head><label>V</label><figDesc>Skewed hump, S = 500</figDesc><table><row><cell></cell><cell>Baseline</cell><cell cols="2">Active selection</cell><cell></cell></row><row><cell>Run</cell><cell>cost</cell><cell>cost</cell><cell>overhead</cell><cell>17</cell></row><row><cell>1.</cell><cell>940.68</cell><cell>37.26 (4.0%)</cell><cell>8.56%</cell><cell>12</cell></row><row><cell>2.</cell><cell>182.67</cell><cell>33.17 (18.2%)</cell><cell>8.86%</cell><cell>11</cell></row><row><cell>3.</cell><cell>778.51</cell><cell>30.52 (3.9%)</cell><cell>17.20%</cell><cell>17</cell></row><row><cell>4.</cell><cell>1287.39</cell><cell>38.05 (3.0%)</cell><cell>12.51%</cell><cell>16</cell></row><row><cell>5.</cell><cell>241.18</cell><cell>44.43 (18.4%)</cell><cell>9.48%</cell><cell>15</cell></row><row><cell>Med:</cell><cell>778.51</cell><cell>37.26 (4.8%)</cell><cell>9.48%</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Vapnik-Chervonenkis dimension</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A network for extracting the locations of point clusters using selective attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omohundro</surname></persName>
		</author>
		<imprint>
			<publisher>Conf Cognitive Science Society</publisher>
		</imprint>
		<respStmt>
			<orgName>Int. Computer Science Institute, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 90411</note>
	<note>also to appear in 12th Ann</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to design optimality with an overview of the literature</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hedayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics, Part A-Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1295" to="1325" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Elements of Integration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Bartle</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural net algorithms that learn in polynomial time from examples and queries</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="1966">1991. 1966</date>
			<pubPlace>New York, Wiley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What size network gives valid generalization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Probability and Measure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Billingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>New York Wiley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is NP-complete</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory (COLT), Haussler (UCSC) and Pitt (Illinois)</title>
		<editor>
			<persName><forename type="first">Acm</forename><surname>Sigact</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">/</forename><surname>Sigart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Santa</forename><surname>Uc</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cruz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-08">Aug. 1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Draper</surname></persName>
		</author>
		<title level="m">Emirical Model-Building and Response Surfaces</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian back-propagation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>to appear in Complex Systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training connectionist networks with queries and selective sampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2, Proc. Neural Information Processing Systems Conf. D. Cohn</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Diego</surname></persName>
		</editor>
		<editor>
			<persName><surname>Touretzky</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>Proc. 1990 Summuer School in San</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooray-Wijesinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Khuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electronic Computers</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="1965">1987. 1965</date>
		</imprint>
	</monogr>
	<note>Comm. Statistics-Simulation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sequential design for the nonparametric regression of curves and surfaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Faraway</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, The University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 177</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting chaotic time series</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sidorowich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="845" to="848" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How neural nets work</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Fedorov ; Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, (Denver 1987)</title>
		<title level="s">Theory of Optimal Experiments</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Anderson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York; New York</addrLine></address></meeting>
		<imprint>
			<publisher>American Institute of Physics</publisher>
			<date type="published" when="1972">1972. 1988</date>
			<biblScope unit="page" from="442" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological control systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">287</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning via queries to an oracle</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">I</forename><surname>Gasarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Pleszkoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLT&apos;89 (Second Annual Workshop on Computational Learning Theory)</title>
		<meeting>COLT&apos;89 (Second Annual Workshop on Computational Learning Theory)</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning read-once formulas using membership queries</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLT39 (Second Annual Workshop on Computational Learning Theory)</title>
		<meeting>COLT39 (Second Annual Workshop on Computational Learning Theory)</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-layer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Pergamon</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query-based learning applied to partially trained multi-layer perceptrons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="136" />
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Query learning based on boundary search and gradient computation of trained multilayer perceptrons</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN 1990</title>
		<meeting>IJCNN 1990<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>The Int</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Joint Con5 Neural Networks.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the complexity of loading shallow neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Judd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Complexity</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning in neural networks (extended abstract)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Judd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory (COLT), Haussler (UCSC) and Pitt (Illinois)</title>
		<editor>
			<persName><forename type="first">Acm</forename><surname>Sigactisigart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Santa</forename><surname>Uc</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cruz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988-08">Aug. 1988. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Response Surfaces (Designs andAnalyses)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Khuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cornell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Marcel Dekker, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the role of search for learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLT&apos;89 (Second Annual Workshop on Computational Learning Theory</title>
		<meeting>COLT&apos;89 (Second Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Information-based objective functions for active data selection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
	<note>The evidence framework applied to classification networks. submitted to Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Supervised learning on large redundant training sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal designs for nonparametric kernel regression</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Probability Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="290" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A resource-allocating neural network for function interpolation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Khuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Carter</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="1989">1989. 1991. 1990</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Technometrics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
