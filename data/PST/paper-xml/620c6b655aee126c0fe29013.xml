<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">G-Mixup: Graph Data Augmentation for Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-15">15 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
							<email>&lt;han@tamu.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sci-ence&amp;Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sci-ence&amp;Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Georgia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">G-Mixup: Graph Data Augmentation for Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-15">15 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.07179v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently deep learning has been widely adopted to graph analysis. Graph Neural Networks (GNNs) <ref type="bibr" target="#b38">(Wu et al., 2020;</ref><ref type="bibr" target="#b50">Zhou et al., 2020a;</ref><ref type="bibr" target="#b47">Zhang et al., 2020;</ref><ref type="bibr" target="#b40">Xu et al., 2018)</ref> have shown promising performance on graph classification. Meanwhile, data augmentation (e.g., DropEdge <ref type="bibr" target="#b26">(Rong et al., 2020)</ref>, Subgraph <ref type="bibr" target="#b42">(You et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020a)</ref> ) has also been adopted to graph analysis by generating synthetic graphs to create more training data for improving the generalization of graph classification models. However, existing graph data augmentation strategies typically aim to augment graphs at a within-graph level by either modifying</p><p>In parallel with the development of graph neural networks, Mixup <ref type="bibr" target="#b44">(Zhang et al., 2017)</ref> and its variants (e.g., Manifold Mixup <ref type="bibr" target="#b32">(Verma et al., 2019a)</ref>), as data augmentation methods, have been theoretically and empirically shown to improve the generalization and robustness of deep neural networks in image recognition <ref type="bibr" target="#b44">(Zhang et al., 2017;</ref><ref type="bibr" target="#b32">Verma et al., 2019a;</ref><ref type="bibr" target="#b45">Zhang et al., 2021)</ref> and natural language processing <ref type="bibr" target="#b12">(Guo et al., 2019a;</ref><ref type="bibr" target="#b11">Guo, 2020)</ref>. The basic idea of Mixup is to linearly interpolate continuous values of random sample pairs to generate more synthetic training data. The formal mathematical expression of Mixup is x new = λx i + (1 − λ)x j , y new = λy i +(1−λ)y j , where (x i , y i ) and (x j , y j ) are two samples drawn at random from training data and the target y are one-hot labels. With graph neural networks and mixup in mind, the following question naturally arises:</p><p>Can we mix up graph data to improve the generalization and robustness of GNNs?</p><p>It remains an open and challenging problem to mix up graph data due to the characteristics of graphs and the requirements of applying Mixup. Typically, Mixup requires that original data instances are regular and well-aligned in Euclidean space, such as image data and table data. However, graph data is distinctly different from them due to the following reasons: (i) graph data is irregular, since the number of nodes in different graphs are typically different from each other; (ii) graph data is not well-aligned, where nodes in graphs are not naturally ordered and it is hard to match up nodes between different graphs; (iii) graph topology between classes are divergent, where the topologies of a pair of graphs from different classes are usually different while the topologies of those from the same class are usually similar. Thus, it is nontrivial to directly adopt the Mixup strategy to graph data.</p><p>To tackle the aforementioned problems, we propose G-Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons. The graphs within one class are produced under the same generator (i.e., graphon). We mix up the graphons of different classes and then generate synthetic graphs. Informally, a graphon can be thought of as a probability matrix (e.g., the matrix W G and W H in Figure <ref type="figure" target="#fig_0">1</ref>), where W (i, j) represents the probability of edge between node i and j. The real-world graphs can be regraded as generated from graphons. Since the graphons of different graphs is regular, well-aligned, and is defined in Euclidean space, it is easy and natural to mix up the graphons and then generate the synthetic graphs therefrom.</p><p>On this basis, we can achieve graphs mixup by mixing their generators. We also provide theoretical analysis of graphons mixup, which guarantees that the generated graphs will preserve the key characteristics of both original classes. Our proposed method is illustrated in Figure <ref type="figure" target="#fig_0">1</ref> with an example. Given two graph training sets</p><formula xml:id="formula_0">G = {G 1 , G 2 , • • • , G m } and H = {H 1 , H 2 , • • • , H m }</formula><p>with different labels and distinct topologies (i.e., G has two communities while H has eight communities), we estimate graphons W G and W H respectively from G and H. We then mix up the two graphons and obtain a mixed graphon W I . After that, we sample synthetic graphs from W I as additional training graphs. The generated synthetic graphs have two major communities and each of them have four sub-communities, which is a mixture of the two graph sets. It thus shows that G-Mixup is capable of mixing up graphs.</p><p>Our main contributions are highlighted as follows: Firstly, we propose G-Mixup to augment the training graphs for graph classification. Since directly mixing up graphs is intractable, G-Mixup mixes the graphons of different classes of graphs to generate synthetic graphs. Secondly, we theoretically prove that the synthetic graph will be the mixture of the original graphs, where the key topology (i.e., discriminative motif) of source graphs will be mixed up. Thirdly, we demonstrate the effectiveness of the proposed G-Mixup on various graph neural networks and datasets. Extensive experimental results show that G-Mixup substantially improves the performance of graph neural networks in terms of enhancing their generalization and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we first go over the notations used in this paper, and then introduce graph related concepts including graph homomorphism and graphons, which will be used for theoretical analysis in this work. Finally, we briefly review the graph neural networks for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations</head><p>Given a graph G, we use V (G) and E(G) to denote its nodes and edges, respectively. The number of nodes is </p><formula xml:id="formula_1">v(G) = |V (G)|,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Homomorphism and Graphons</head><p>Graph Homomorphism. A graph homomorphism is an adjacency-preserving mapping between two graphs, i.e., mapping adjacent vertices in one graph to adjacent vertices in the other. Formally, a graph homomorphism φ : </p><formula xml:id="formula_2">F → G is a map from V (F ) to V (G), where if {u, v} ∈ E(F ), then {φ(u), φ(v)} ∈ E(G).</formula><formula xml:id="formula_3">, G) = hom(H,G) |V (G)| |V (H)| . For example, t( , G) = |V (G)|/N 1 = 1, t( , G) = 2|E(G)|/N 2 .</formula><p>Graphon. A graphon <ref type="bibr" target="#b0">(Airoldi et al., 2013)</ref> is a continuous, bounded and symmetric function W : [0, 1] 2 → [0, 1] which may be thought of as the weight matrix of a graph with infinite number of nodes. Then, given two points u i , u j ∈ [0, 1], W (i, j) represents the probability that nodes i and j be related with an edge. Various quantities of a graph can be calculated as a function of the graphon. For example, the degree of nodes in graphs can be easily extended to a degree distribution function in graphons, which is characterized by its graphon marginal d W (x) = 1 0 W (x, y)dy. Similarly, the concept of homomorphism density can be naturally extended from graphs to graphons. Given an arbitrary graph motif F , its homomorphism density with respect to graphon W is defined by</p><formula xml:id="formula_4">t(F, W ) = [0,1] V (F ) i,j∈E(F ) W (x i , x j ) i∈V (F ) dx i .</formula><p>For example, the edge density of graphon W is t( , W ) = [0,1] 2 W (x, y) dxdy, and the triangle density of graphon</p><formula xml:id="formula_5">W is t( , W ) = [0,1] 3 W (x, y)W (x, z)W (y, z) dxdydz.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph Classification with Graph Neural Networks</head><p>Given a set of graphs, graph classification aims to assign a class label for each graph G. Recently, graph neural networks have become the state-of-the-art approach for graph classification. Without loss of generalization, we present the formal expression of a graph convolution network (GCN) <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016)</ref>. The forward propaga- G and H have different graphons. We mix up the graphons WG and WH to obtain a mixed graphon WI, and then sample new graphs from the mixed graphon. Intuitively, the synthetic graphs have two major communities and each of which has four sub-communities, demonstrating that the generated graphs preserve the structure of original graphs from both classes.</p><formula xml:id="formula_6">W I W H W G I = {I 1 , I 2 , • • • , I k } with label (0.5, 0.5) H = {H 1 , H 2 , • • • , H k } with label (0, 1) G = {G 1 , G 2 , • • • , G k } with</formula><p>tion at k-th layer is described as the following:</p><formula xml:id="formula_7">a (k) i = AGG (k) h (k−1) j : j ∈ N (i) , h (k) i = COMBINE (k) h (k−1) i , a (k) i ,<label>(1) where h (k) i</label></formula><p>∈ R n×d k is the intermediate representation of node i at the k-th layer, N (i) denotes the neighbors of node i. AGG(•) is an aggregation function to collect embedding representations from neighbors, and COMBINE(•) combines neighbors' representation and its representation at (k − 1)-th layer. For graph classification, a graph-level representation is obtained by summarizing all node-level representations in the graph by a readout function:</p><formula xml:id="formula_8">h G = READOUT h (k) i : i ∈ E(G) , ŷ = softmax(h G ),<label>(2)</label></formula><p>where READOUT(•) is the readout function, which can be a simple function such as average or sophisticated pooling function <ref type="bibr" target="#b10">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b41">Ying et al., 2018)</ref>, h G is the representation of graph G, and ŷ ∈ R C is the predicted probability that G belongs to each of the C classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we formally introduce the proposed G-Mixup and elaborate its implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">G -Mixup</head><p>Different from the interpolation of image data in Euclidean space, adopting Mixup to graph data is nontrivial since graphs are irregular, unaligned and non-Euclidean data. In this work, we show that the challenges could be tackled with graphon theory. Intuitively, a graphon can be regarded as a graph generator. Graphs of the same class can be seen as being generated from the same graphon. With this in mind, we propose G-Mixup, a class-level data augmentation method via graphon interpolation. Specifically, G-Mixup interpolates different graph generators to obtain a new mixed one. Then, synthetic graphs are sampled based on the mixed graphon for data augmentation. The graphs sampled from this generator partially possess properties of the original graphs. Formally, G-Mixup is formulated as:</p><formula xml:id="formula_9">Graphon Estimation: G → W G , H → W H (3)</formula><p>Graphon Mixup:</p><formula xml:id="formula_10">W I = λW G + (1 − λ)W H (4) Graph Generation: {I 1 , I 2 , • • • , I m } i.i.d ∼ G(K, W I ) (5)</formula><p>Label Mixup:</p><formula xml:id="formula_11">y I = λy G + (1 − λ)y H (6)</formula><p>where W G , W H are graphons of the graph set G and H. The mixed graphon is denoted by W I , and λ ∈ [0, 1] is the trade-off hyperparameter to control the contributions from different source sets. The set of synthetic graphs generated from </p><formula xml:id="formula_12">W I is I = {I 1 , I 2 , • • • , I m }.</formula><formula xml:id="formula_13">I ∈ R C .</formula><p>As illustrated in Figure <ref type="figure" target="#fig_0">1</ref> and the above equations, the proposed G-Mixup includes three key steps: i) estimate a graphon for each class of graphs, ii) mix up the graphons of different graph classes, and iii) generate synthetic graphs based on the mixed graphons. Specifically, suppose we have two graph sets</p><formula xml:id="formula_14">G = {G 1 , G 2 , • • • , G m } with label y G , and H = {H 1 , H 2 , • • • , H m } with label y H . Graphons W G</formula><p>and W H are estimated from graph sets G and H, respectively. Then, we mix them up by linearly interpolating the two graphons and their labels, and obtain W I and y I . Finally, a set of synthetic graphs I is sampled based on W I , which will be used as additional training graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>In this section, we introduce the implementation details of graphon estimation and synthetic graphs generation. We provide the pseudo-code of G-Mixup in Appendix E.</p><p>Graphon Estimation and Mixup. Estimating graphons from observed graphs is a prerequisite for G-Mixup. However, it is intractable because a graphon is an unknown function without a closed-form expression for real-world graphs. Therefore, we use the step function <ref type="bibr" target="#b22">(Lovász, 2012;</ref><ref type="bibr" target="#b39">Xu et al., 2021)</ref> to approximate graphons 1 . In general, the step function can be seen as a matrix</p><formula xml:id="formula_15">W = [w kk ] ∈ [0, 1] K×K ,</formula><p>where W ij is the probability that an edge exists between node i and node j. In practice, we use the matrix-form step function as graphon to mix up and generate synthetic graphs. The step function estimation methods are wellstudied, which first align the nodes in a set of graphs based on node measurements (e.g., degree) and then estimate the step function from all the aligned adjacency matrices. The typical step function estimation methods includes sortingand-smoothing (SAS) method <ref type="bibr" target="#b5">(Chan &amp; Airoldi, 2014)</ref>, stochastic block approximation (SBA) <ref type="bibr" target="#b0">(Airoldi et al., 2013)</ref>, "largest gap" (LG) <ref type="bibr" target="#b6">(Channarond et al., 2012)</ref>, matrix completion (MC) <ref type="bibr" target="#b19">(Keshavan et al., 2010)</ref>, universal singular value thresholding (USVT) <ref type="bibr" target="#b7">(Chatterjee et al., 2015)</ref>. 2 Formally, a step function</p><formula xml:id="formula_16">W P : [0, 1] 2 → [0, 1] is defined as W P (x, y) = K k,k =1</formula><p>w kk 1 P k ×P k (x, y), where P = (P 1 , .., P K ) denotes the partition of [0, 1] into K adjacent intervals of length 1/K, w kk ∈ [0, 1], and indicator func-</p><formula xml:id="formula_17">tion 1 P k ×P k (x, y) equals to 1 if (x, y) ∈ P k × P k and</formula><p>otherwise it is 0. For binary classification, we have</p><formula xml:id="formula_18">G = {G 1 , G 2 , • • • , G m } and H = {H 1 , H 2 , • • • , H m } with dif-</formula><p>ferent labels, we estimate their step functions W G ∈ R K×K and W H ∈ R K×K , where we let K be the average number of nodes in all graphs. For multi-class classification, we first estimate the step function for each class of graphs and then randomly select two to perform mix-up. The resultant step function is</p><formula xml:id="formula_19">W I = λW G + (1 − λ)W H ∈ R K×K , which</formula><p>serves as the generator of synthetic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Graphs Generation.</head><p>A graphon W provides a distribution to generate arbitrarily sized graphs. Specifically, a K-node random graph G(K, W I ) can be generated following the process:</p><formula xml:id="formula_20">u 1 , . . . , u K iid ∼ Unif [0,1] , G(K, W ) ij iid ∼ Bern(W (u i , u j )), ∀i, j ∈ [K].</formula><p>Since we use the step function W to approximate the 1 Because weak regularity lemma of graphon <ref type="bibr" target="#b9">(Frieze &amp; Kannan, 1999)</ref> indicates that an arbitrary graphon can be approximated well by step function. Detailed discussion is in Appendix A.4.</p><p>2 The details about these step function estimation methods are presented in Appendix B. graphon W , we set </p><formula xml:id="formula_21">W (u i , u j ) = W[ 1/u i , 1/u j ],</formula><formula xml:id="formula_22">(G) = {(i, j) | a ij = 1}.</formula><p>A set of synthetic graphs can be generated by conducting the above process multiple times.</p><p>The generation of node features of synthetic graphs includes two steps: 1) build the graphon node features based on the original node features, 2) generate node features of synthetic graphs based on the graphon node features. Specifically, at the graphon estimation phase, we align original node features while aligning the adjacency matrices, so we have a set of aligned original node features for each graphon, then we conduct pooling (average pooling in our experiments) on the aligned original node features to obtain the graphon node features. The node features of generated graphs are the same as graphon features. </p><formula xml:id="formula_23">USVT O(N 3 ) LG O(mN 2 ) SBA O(mKN log N ) SAS O(mN log N + K 2 log K 2 )</formula><p>Computational Complexity Analysis. We hereby discuss computational complexity of G-Mixup. The major computation costs come from graphon estimation and synthetic graph generation. For graphon estimation, suppose we have m graphs and each of them has N nodes, and estimate step function with K partitions to approximate a graphon, the complexity of used graphon estimation methods <ref type="bibr" target="#b39">(Xu et al., 2021)</ref> is in Table <ref type="table" target="#tab_3">1</ref>. For graph generation, suppose we need to generate l graphs with K nodes, the computational complexity is O(lK) for node generation and O(lK 2 ) for edge generation, so the overall complexity of graph generation is O(lK 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Justification</head><p>In the following, we theoretically prove that: the synthetic graphs generated by G-Mixup will be a mixture of original graphs. We first define the discriminative motif, and then we justify the graphon mixup operation (Equation ( <ref type="formula">4</ref>)) and graph generation operation (Equation ( <ref type="formula">5</ref>)) by analysing the homomorphism density of discriminative motifs of the original graphs and the synthetic graphs. Definition 4.1 (Discriminative Motif). A discriminative motif F G of graph G is the subgraph, with the minimal number of nodes and edges, that can decide the class the graph G. Furthermore, F G is the set of discriminative motifs for graphs in the set G.</p><p>Intuitively, the discriminative motif is the key topology of a graph. We assume that (i) every graph G has a discrimina-tive motif F G , and (ii) a set of graphs G has a finite set of discriminative motifs F G . The goal of graph classification is to filter out structural noise in graphs <ref type="bibr" target="#b8">(Fox &amp; Rajamanickam, 2019)</ref> and recognize the key typologies (discriminative motifs) to predict the class label. For example, benzene (a chemical compound) is distinguished by the motif (benzene ring). In the following, we analyze G-Mixup from the perspective of discriminative motifs.</p><formula xml:id="formula_24">4.1. Will discriminative motifs F G and F H exist in λW G + (1 − λ)W H ?</formula><p>We answer this question by exploring the difference in homomorphism density of discriminative motifs between the original and mixed graphon, as the following theorems, Theorem 4.2. Given two sets of graphs G and H, the corresponding graphons are W G and W H , and the corresponding discriminative motif set F G and F H . For every discriminative motif F G ∈ F G and F H ∈ F H , the difference between the homomorphism density of F G /F H in the mixed graphon Ideally, the generated graphs should inherit the homomorphism density of discriminative motifs from the graphon.</p><formula xml:id="formula_25">W I = λW G + (1 − λ)W H and that of the graphon W H /W G is upper bounded by |t(F G , W I ) − t(F G , W G )| ≤ (1 − λ)e(F G )||W H − W G || , |t(F H , W I ) − t(F H , W H )| ≤ λe(F H )||W H − W G || where e(F )</formula><p>To verify this, we propose the following theorem. Theorem 4.3. Let W I be the mixed graphon, n ≥ 1, 0 &lt; ε &lt; 1, and let F I be the mixed discriminative motif, then the W I -random graph G = G(n, W I ) satisfies</p><formula xml:id="formula_26">P (|t(F I , G) − t(F I , W I )| &gt; ε) ≤ 2exp − ε 2 n 8v(F I ) 2 .</formula><p>Theorem 4.3 states that for any specified nonzero margin ε, with a sufficient number of graphs sampled from the mixed graphon, the homomorphism density of discriminative motif in synthetic graphs will approximately equal to that in graphon t(F I , G) ≈ t(F I , W I ) with high probability. In other words, the synthetic graphs will preserve the discriminative motif of the mixed graphon with a very high probability if the sample number n is large enough. The detailed proof is in Appendix A.3. Therefore, G-Mixup can preserve the discriminative motifs of the two different graphs into one mixed graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>We discuss the differences and relations between G-Mixup and other augmentation strategies.</p><p>Relation to Edge Perturbation Methods. The commonly used edge perturbation methods are spacial cases of G-Mixup. Edge perturbation methods randomly perturb the edges to improve the GNNs, inlcuding DropEdge <ref type="bibr" target="#b26">(Rong et al., 2020)</ref>, and Graphon-based edge perturbation <ref type="bibr" target="#b16">(Hu et al., 2021)</ref>. DropEdge removes graph edges independently with a specified probability, aiming to prevent oversmoothing and over-fitting issues in GNNs. Graphon-based edge perturbation <ref type="bibr" target="#b26">(Rong et al., 2020)</ref> improves the Dropedge by dropping edge based on an estimated probability. One limitation of such methods is that the edge permutation is based on one individual graph, so the graphs will not mix up. DropEdge and Graphon-based edge perturbation <ref type="bibr" target="#b16">(Hu et al., 2021)</ref> are special cases of G-Mixup while setting different hyperparameter λ. i) G-Mixup will degenerate into Graphon-based edge perturbation, if λ = 0 in Equation ( <ref type="formula">4</ref>), where the mathematical expression is</p><formula xml:id="formula_27">W I = W H , {I 1 , I 2 , • • • , I m } i.i.d ∼ G(k, W I ), y I = y H . ii)</formula><p>G-Mixup will degenerate into DropEdge, if λ = 0 and using the element-wise product of graphons W and adjacency matrix A in Equation ( <ref type="formula">4</ref>) as edge probability. The expression is</p><formula xml:id="formula_28">W I = A W H , {I 1 , I 2 , • • • , I m } i.i.d ∼ G(k, W I ), y I = y H</formula><p>, where is element-wise multiplication.</p><p>Relation to Manifold Mixup. As a model-agnostic augmentation method, G-Mixup has broader applications, e.g., creating graphs for graph contrastive learning, than Manifold Mixup. Manifold Mixup <ref type="bibr" target="#b37">(Wang et al., 2021)</ref> is proposed to mix up graphs in the embedding space, which interpolates hidden representations of graphs. Interpolating hidden representation could limit its applications because: 1) algorithms must have hidden representation of graphs, and 2) models must be modified to adapt it. In contrast, G-Mixup generates synthetic graphs without modifying models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the performance of G-Mixup in this section. First, we visualize graphons and graph generation results in Sections 5.1 and 5.2 to investigate what G-Mixup actually do on real-world datasets. Then, we evaluate the effectiveness of G-Mixup in graph classification with various datasets and GNN backbones in Section 5.3, as well as how it improves the robustness of GNNs against label corruption and adversarial attacks in Section 5.4. The experiment setting and more experiments are in Appendices F and G. The observations are highlighted with # boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Do different classes of real-world graphs have different graphons?</head><p>We visualize the estimated graphons in Figure <ref type="figure">2</ref>. It shows that, 1 the graphons of different class of graphs in one dataset are distinctly different. The graphons of IMDB-BINAERY in Figure <ref type="figure">2</ref> shows that the graphon of class 1 has larger dense area, which indicates that the graphs in this class have a more large communities than the graphs of class 0. The graphons of REDDIT-BINARY in Figure <ref type="figure">2</ref> shows that graphs of class 0 have one high-degree nodes while the graphs of class 1 have two. This observation validates that real-world graphs of different classes have distinctly different graphons, which lays a solid foundation for generating the mixture of graphs by mixing up graphons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">What is G-Mixup doing? A case study</head><p>To investigate the outcome of G-Mixup in real-world scenarios, we visualize the generated synthetic graphs in REDDIT-BINARY dataset in Figure <ref type="figure">3</ref>. We observed that 2 The synthetic graphs are indeed the mixture of the original graphs. Original graphs and the generated synthetic graphs are visualized in Figure <ref type="figure">3</ref>(a)(b) and Figure <ref type="figure">3</ref>(c)(d)(e), respectively. Figure <ref type="figure">3</ref> demonstrates that mixed graphon 0.5 * W G + 0.5 * W H is able to generate graphs with a highdegree node and a dense subgraph, which can be regarded as the mixture of graphs with one high-degree node and two high-degree nodes. It validates that G-Mixup prefer to preserve the discriminative motifs from the original graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Can G-Mixup improve the performance and generalization of GNNs?</head><p>To validate the effectiveness of G-Mixup, we compare the performance of GNNs with various backbones on differ-  <ref type="table" target="#tab_6">2 and 3</ref> as well as the training curves in Figure <ref type="figure">4</ref>. We make the following observations: 3 G-Mixup can improve the performance of graph neural networks on various datasets. From Table <ref type="table" target="#tab_5">2</ref>, G-Mixup gain 12 best performances among 15 reported accuracies, which substantially improve the performance of GNNs. Overall, G-Mixup performs 2.84% better than the vanilla model. Note that G-Mixup and baseline models adopt the same architecture of GNNs (e.g., layers, activation functions) and the same training hyperparameters (e.g., optimizer, learning rate). From Table <ref type="table" target="#tab_6">3</ref>, G-Mixup gains 7 best performances among 8 cases, which substantially improve the performance of DiffPool and Min-cutPool. Meanwhile, 4 G-Mixup can improve the generalization of graph neural networks. From the loss curve on test data (green line) in Figure <ref type="figure">4</ref>, the loss of test data of G-Mixup (dashed green lines) are consistently lower than the vanilla model (solid green lines). Considering both the better performance and the better test loss curves, G-Mixup is able to substantially improve the generalization of GNNs. Also, 5 G-Mixup could stabilize the model training. As shown in Table <ref type="table" target="#tab_5">2</ref>, G-Mixup achieves 11 lower standard deviation among total 15 reported numbers than the vanilla model. Additionally, the train/validation/test curves of G-Mixup (dashed line) in Figure <ref type="figure">4</ref> are more stable than vanilla model (solid line), indicating that G-Mixup sta-  <ref type="bibr" target="#b15">(Hu et al., 2020)</ref> and more pooling method (GMT) are in Appendices G.2 and G.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB-BINARY IMDB-MULTI REDDIT-BINARY REDDIT-MULTI-5K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Can G -Mixup improve the robustness of GNNs?</head><p>We investigate the two kinds of robustness of G-Mixup, including Label Corruption Robustness and Topology Corruption Robustness, and report the results in Table <ref type="table" target="#tab_7">4</ref> and Table <ref type="table" target="#tab_8">5</ref>, respectively. More experimental settings are presented in Appendix F.4. 6 G-Mixup improves the robustness of graph neural networks. Table <ref type="table" target="#tab_7">4</ref> shows G-Mixup gains better performance in general, indicating it is more robust to noisy labels than the vanilla baseline. Table <ref type="table" target="#tab_8">5</ref> shows that G-Mixup is more robust when graph topology is corrupted since the accuracy is consistently better than baselines. This can be an advantage of G-Mixup when graph label or topology are noisy.   different numbers (hyperparameters K) of nodes and use them to train graph neural networks. We observed form Figure 5 that 7 using the average node number of all the original graphs is a better choice for hyperparameter K in G-Mixup, which is in line with the intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2.">IMPACT ON DEEPER MODELS</head><p>We investigate the performance of G-Mixup when GCN goes deeper. We experiment with different numbers (2 − 9) of layers and report the results in Figure <ref type="figure" target="#fig_5">6</ref>. 8 G-Mixup improves the performance of graph neural networks with varying layers. In Figure <ref type="figure" target="#fig_5">6</ref>, the left figure shows G -Mixup gains better performance while the depth of GCNs is 2 − 6.</p><p>The performance with deeper GCNs (7 − 9) are comparable to baselines, however, the accuracy is much lower than shallow ones. The right figure shows G-Mixup gains better performance by a significant margin while the depth of GCNs is 2 − 9. This validates the effectiveness of G-Mixup when graph neural network goes deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Works</head><p>Graph Data Augmentation.</p><p>Graph neural networks (GNNs) achieve the state-of-the-art performance on graph classification tasks <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b31">Veličković et al., 2017;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b40">Xu et al., 2018;</ref><ref type="bibr">Zhang et al.</ref>, 2018). In parallel, graph data augmentation methods improve the performance of GNNs. There are three categories of graph data augmentation, including node perturbation <ref type="bibr" target="#b42">(You et al., 2020;</ref><ref type="bibr" target="#b17">Huang et al., 2018)</ref>, edge perturbation <ref type="bibr" target="#b26">(Rong et al., 2020;</ref><ref type="bibr" target="#b42">You et al., 2020)</ref>, and subgraph sampling <ref type="bibr" target="#b42">(You et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020a)</ref>. However, the major limitation of the existing graph augmentation methods is that they are based on one single graph while G-Mixup leverages multiple input graphs. Besides, there are a line of works focusing on graph data augmentation methods for node classification <ref type="bibr" target="#b48">(Zhao et al., 2021;</ref><ref type="bibr" target="#b36">Wang et al., 2020b;</ref><ref type="bibr" target="#b30">Tang et al., 2021;</ref><ref type="bibr" target="#b25">Park et al., 2021;</ref><ref type="bibr" target="#b33">Verma et al., 2019b)</ref>. The more discussion are in Appendix D.</p><p>Graphon Estimation. Graphons and convergent graph sequences have been broadly studied in mathematics <ref type="bibr" target="#b22">(Lovász, 2012;</ref><ref type="bibr" target="#b23">Lovász &amp; Szegedy, 2006;</ref><ref type="bibr" target="#b4">Borgs et al., 2008)</ref> and have been applied to network science <ref type="bibr" target="#b1">(Avella-Medina et al., 2018;</ref><ref type="bibr" target="#b34">Vizuete et al., 2021)</ref> and graph neural networks <ref type="bibr" target="#b27">(Ruiz et al., 2020a;</ref><ref type="bibr">b)</ref>. There are tow lines of works to estimate step functions, one is based on stochastic block models, such as stochastic block approximation (SBA) <ref type="bibr" target="#b0">(Airoldi et al., 2013)</ref>, "largest gap" (LG) <ref type="bibr" target="#b6">(Channarond et al., 2012)</ref> and sortingand-smoothing (SAS) <ref type="bibr" target="#b5">(Chan &amp; Airoldi, 2014)</ref>; another one is based on low-rank matrix decomposition, such as matrix completion (MC) <ref type="bibr" target="#b19">(Keshavan et al., 2010)</ref>, universal singular value thresholding (USVT) <ref type="bibr" target="#b7">(Chatterjee et al., 2015)</ref>. More discussion about graphon estimation are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This work develops a novel graph augmentation method called G-Mixup. Unlike image data, graph data is irregular, unaligned and in non-Euclidean space, making it hard to be mixed up. However, the graphs within one class have the same generator (i.e., graphon), which is regular, wellaligned and in Euclidean space. Thus we turn to mix up the graphons of different classes to generate synthetic graphs. G-Mixup is mix up and interpolate the topology of different classes of graphs. Comprehensive experiments show that GNNs trained with G-Mixup achieve better performance and generalization, and improve the model robustness to noisy labels and corrupted topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem</head><p>In the appendix, we first present the preliminaries in Appendix A.1. And then we present complete proof for Theorems 4.2 and 4.3 in Appendices A.2 and A.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Preliminaries</head><p>Cut norm <ref type="bibr" target="#b22">(Lovász, 2012;</ref><ref type="bibr" target="#b49">Zhao, 2019)</ref> is used to measure structural similarity of two graphons. The definition of cut norm is as follow:</p><p>Definition A.1. The cut norm of grapon W is defined as</p><formula xml:id="formula_29">W = sup S,T⊂[0,1] S×T W (x, y)dxdy ,<label>(7)</label></formula><p>where the supremum is taken over all measurable subsets S and T.</p><p>The following lemma follows the derivation of counting lemma for graphons, are known in the paper <ref type="bibr" target="#b22">(Lovász, 2012)</ref>. It will be used to prove the Theorem 4.2.</p><p>Lemma A.2. Let F be a simple graph and let W, W ∈ W. Then</p><formula xml:id="formula_30">|t(F, W ) − t(F, W )| ≤ e(F )||W − W || (8)</formula><p>Proof of Lemma A.2: The proof follows <ref type="bibr" target="#b49">Zhao (2019)</ref>. For an arbitrary simple graph F , by the triangle inequality we have</p><formula xml:id="formula_31">|t(F, W ) − t(F, W )| = uivi∈E W (u i , v i ) − uivi∈E W (u i , v i ) v∈V dv ≤ |E| i=1   i−1 j=1 W (u j , v j ) (W (u i , v i ) − W (u i , v i )) |E| k=i+1 W (u k , v k )   v∈V dv<label>(9)</label></formula><p>Here, each absolute value term in the sum is bounded by the cut norm W − W if we fix all other irrelavant variables (everything except u i and v i for the i-th term), altogether implying that</p><formula xml:id="formula_32">| t(F, W ) − t(F, W )| ≤ e(F )||W − W ||<label>(10)</label></formula><p>Lemma A.3 (Corollary 10.4 in <ref type="bibr" target="#b23">(Lovász &amp; Szegedy, 2006)</ref>). Let W be a graphon, n ≥ 1, 0 &lt; ε &lt; 1, and let F be a simple graph, then the W -random graph G = G(n, W ) satisfies</p><formula xml:id="formula_33">P (|t(F, G) − t(F, W )| &gt; ε) ≤ 2exp − ε 2 n 8v(F ) 2<label>(11)</label></formula><p>A.2. Proof of Theorem 1</p><p>We have the mixed graphon</p><formula xml:id="formula_34">W I = λW G + (1 − λ)W H . Let W = W I , W = W G , and F = F G in Lemma A.2, we have, |t(F G , W I ) − t(F G , W G )| ≤ e(F G )||W I − W G || |t(F G , λW G + (1 − λ)W H ) − t(F, W G )| ≤ e(F G )||λW G + (1 − λ)W H − W G || ≤ e(F G )||(1 − λ)(W H − W G )||<label>(12)</label></formula><p>Recall that the cut norm W = sup S,T ⊆[0,1] S×T W .</p><p>obviously, suppose α ∈ R, we have</p><formula xml:id="formula_35">αW = sup S,T ⊆[0,1] S×T αW = sup S,T ⊆[0,1] α S×T W = α W (13)</formula><p>Based on Equation ( <ref type="formula" target="#formula_34">12</ref>) and Equation ( <ref type="formula">13</ref>), we have</p><formula xml:id="formula_36">|t(F G , λW G + (1 − λ)W H ) − t(F G , W G )| ≤ e(F G )||(1 − λ)(W H − W G )|| ≤ (1 − λ)e(F G )||W H − W G ||<label>(14)</label></formula><p>Similarly, let W = W I , W = W H and F = F H in Lemma A.2, We can also easily obtain</p><formula xml:id="formula_37">|t(F H , λW G + (1 − λ)W H ) − t(F H , W H )| ≤ λe(F H )||W H − W G ||<label>(15)</label></formula><p>Equation ( <ref type="formula" target="#formula_36">14</ref>) and Equation ( <ref type="formula" target="#formula_37">15</ref>) produce the upper bound in Equation (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Theorem 2</head><p>Let F and W be the discriminative motif F I and the mixed graphon W I in Lemma A.3, we will have</p><formula xml:id="formula_38">P (|t(F I , G) − t(F I , W I )| &gt; ε) ≤ 2exp − ε 2 n 8v(F I ) 2<label>(16)</label></formula><p>which produces the result in Equation (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Graphons Estimation by Step Function</head><p>The proof follows <ref type="bibr" target="#b39">Xu et al. (2021)</ref>. A graphon can always be approximated by a step function in the cut norm <ref type="bibr" target="#b9">(Frieze &amp; Kannan, 1999)</ref>.</p><p>Let P = (P 1 , .., P K ) be a partition of Ω into K measurable sets. We define a step function W P : Ω 2 → [0, 1] as</p><formula xml:id="formula_39">W P (x, y) = K k,k =1 w kk 1 P k ×P k (x, y),<label>(17)</label></formula><p>where each w kk ∈ [0, 1] and the indicator function 1 P k ×P k (x, y) is 1 if (x, y) ∈ P k × P k , otherwise it is 0. The weak regularity lemma <ref type="bibr" target="#b22">(Lovász, 2012)</ref> shown below guarantees that every graphon can be approximated well in the cut norm by step functions.</p><p>Theorem A.4 (Weak Regularity Lemma (Lemma 9.9 in <ref type="bibr" target="#b22">(Lovász, 2012)</ref>) ). For every graphon W and K ≥ 1, there always exists a step function W with |P| = K steps such that</p><formula xml:id="formula_40">W − W ≤ 2 √ log K W L2 .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graphons Estimation Methods</head><p>The adopted graphon estimated methods (e.g., LG, USVT, SBA) are well-studied methods. Typically they have rigorous mathematical proof to upper bound the graphon estimation error. For example, Theorem 2.10 in <ref type="bibr" target="#b7">(Chatterjee et al., 2015)</ref> shows the graphon estimation error of USVT is strictly upper bounded. And we also copy the results of graphon estimation methods on synthetic graphon from <ref type="bibr" target="#b39">(Xu et al., 2021)</ref> in Table <ref type="table" target="#tab_9">6</ref>. The results show the graphon estimation methods in our work can precisely estimate graphon. The details of them are listed as the following:</p><p>• SBA <ref type="bibr" target="#b0">(Airoldi et al., 2013)</ref> The Stochastic Block Approximation learns stochastic block models to approximate graphons. This method can consistently estimate the graphon with extremely small error and the estimation error vanishes provably as the node number of the graph goes infinity. 5.0±9.5 23.1±3.2 64.6±0.5 37.3±0.6 73.3±0.7</p><p>• LG <ref type="bibr" target="#b6">(Channarond et al., 2012)</ref> The "largest gap" algorithm improve the SBA method, which can be used for both large-scale and small graphs.</p><p>• SAS <ref type="bibr" target="#b5">(Chan &amp; Airoldi, 2014)</ref> The smoothing-and-sorting (SAS) is a improved variant of SBA, which first sorts the graphs based on the node degree, then smooths the sorted graph using total variation minimization.</p><p>• MC and USVT <ref type="bibr" target="#b19">(Keshavan et al., 2010;</ref><ref type="bibr" target="#b7">Chatterjee et al., 2015)</ref> Matrix Completion and Universal Singular Value Thresholding are matrix decomposition based methods, which learn low-rank matrices to approximate graphons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion about Manifold Intrusion in G-Mixup</head><p>In this appendix, we discuss that manifold intrusion in G-Mixup and argue that G-Mixup does not suffer from manifold intrusion issue. The manifold intrusion may be harmful for mixup method. Manifold intrusion in mixup is a form of under-fitting resulting from conflicts between the labels of the synthetic examples and the labels of original training data <ref type="bibr" target="#b13">(Guo et al., 2019b)</ref>. The manifold intrusion in graph learning represents that the generated graphs have identical topology but different labels. In our method, the adjacency matrix A ∈ R K×K of generated graphs are generated from the matrix-from graphon W ∈ R K×K , thus we have</p><formula xml:id="formula_41">A ij iid ∼ Bern(W ij ), ∀i, j ∈ [K].</formula><p>In the graph generation phase, G-Mixup may cause manifold intrusion in two cases: 1) two generated two graphs are identical, 2) a generated graph is identical to an original graph. We hereby show that graph manifold intrusion issue will not happen with a very high probability in G-Mixup as follows:</p><p>• Two generated two graphs are identical. The probability of generating two identical graphs from the same graphon</p><formula xml:id="formula_42">W is Π K i=1 Π K j=1 (W 2 ij + (1 − W ij )</formula><p>2 ), which is extremely small since 0 &lt; W 2 ij + (1 − W ij ) 2 &lt; 1 and K is large enough in the real-world graphs. The probability that two generated two graphs are identical are extremely small.</p><p>• A generated graph is identical to an original graph. The probability of generating a new graph that is identical to an original graph (the adjacency matrix is</p><formula xml:id="formula_43">Ã) is Π K i=1 Π K j=1 (W Ãij ij (1 − W ij ) 1− Ãij ), which is extremely small since 0 &lt; W Ãij ij (1 − W ij ) 1− Ãij &lt; 1</formula><p>and K is large enough in the real-world graphs. The probability that a generated graph is identical to an original graph are identical are extremely small too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Discussion about Related Works</head><p>In this appendix, we discuss two categories of related works. The first one is graph data augmentation for node classification, and the second is model-dependent graph data augmentation for graph classification. Both of them are different to our proposed G-Mixup.</p><p>Graph Data Augmentation for Node Classification. There is another line of works targeting graph data augmentation for node classification <ref type="bibr" target="#b48">(Zhao et al., 2021;</ref><ref type="bibr" target="#b36">Wang et al., 2020b;</ref><ref type="bibr" target="#b30">Tang et al., 2021;</ref><ref type="bibr" target="#b25">Park et al., 2021;</ref><ref type="bibr" target="#b33">Verma et al., 2019b)</ref>. <ref type="bibr" target="#b51">Zhou et al. (2020b)</ref> leverage information inherent in the graph to predict edge probability to augment a new graph for node classification task. <ref type="bibr" target="#b33">Verma et al. (2019b)</ref> proposed GraphMix to augment the vanilla GNN with a Fully-Connected Network </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Sensitivity Analysis to Mixup Ratio λ</head><p>To further investigate the performance of G-Mixup, we provide experimental results of G-Mixup to analyse the sensitivity to hyperparameter mixup ratio λ. Specifically, we use the different mixing ratio λ in W I = λW G +(1−λ)W H , λy G +(1−λ)y H on molecular property prediction task (i.e., ogbg-molbbbp, ogbg-molbace). The p-value<ref type="foot" target="#foot_6">17</ref> is computed with the best performance compared to the Vanilla GCN (last column in Table <ref type="table" target="#tab_10">9 and Table 9</ref>). We can observed that G-Mixup significantly improves graph neural networks' performance while we tune the hyperparameter of G-Mixup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An overview of G-Mixup. The task is binary graph classification. We have two classes of graphs G and H with different topologies (G has two communities while H has eight communities).G and H have different graphons. We mix up the graphons WG and WH to obtain a mixed graphon WI, and then sample new graphs from the mixed graphon. Intuitively, the synthetic graphs have two major communities and each of which has four sub-communities, demonstrating that the generated graphs preserve the structure of original graphs from both classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and • is the floor function. The first step samples K nodes independently from a uniform distribution Unif [0,1] on [0, 1]. The second step generates an adjacency matrix A = [a ij ] ∈ {0, 1} K×K , whose element values follow the Bernoulli distributions Bern(•) determined by the step function. A graph is thus obtained as G where V (G) = {1, ..., K} and E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Estimated graphons on IMDB-BINARY, REDDIT-BINARY, and IMDB-MULTI. Obviously, graphons of different graph classes are quiet different. This observation validates the divergence of graphons between different classes of graphs, which is the basis of the G-Mixup. The graphons are estimated by LG. More estimated graphons via various methods are in Appendix G.1.</figDesc><graphic url="image-13.png" coords="6,55.44,74.38,65.84,65.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.1. THE NODES NUMBER OF GENERATED GRAPHS We investigate the impact of the nodes number in generated synthetic graphs by G-Mixup and present the results in Figure 5. Specifically, G-Mixup generates synthetic graphs with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The impact of the node numbers of generated synthetic graphs. The red vertical line indicates the average number of all the original training graphs. The blue line represents that classification accuracy with different number of nodes of generated graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The performance of G-Mixup using GCNs with different layers on IMDB-BINARY and REDDIT-BINARY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The motifs in graph G is denoted as F G . The set of motifs in graph set G is denoted as F G . W G denotes the graphon of graph set G. W denotes the step function. G(K, W ) denotes the random graph with K nodes based on graphon W .</figDesc><table /><note>and the number of edges is e(G) = |E(G)|. We use m, l to denote the number of graphs and N, K to denote the number of nodes. We use G, H, I/G, H, I to denote graphs/graph set. y G ∈ R C denotes the label of graph set G, where C is number of classes of graphs. A graph could contain some frequent subgraphs which are called motifs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The y G ∈ R C and y H ∈ R C are vectors containing ground-truth labels for graph G and H, respectively, where C is the number of classes. The label vector of synthetic graphs in graph set I is denoted as y</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Computational complexity of graphon estimation.</figDesc><table><row><cell>Method</cell><cell>Complexity</cell></row><row><cell>MC</cell><cell>O(N 3 )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>H ) and the cut norm ||W H − W G || . Since the e(F G )/e(F H ) and the cut norm ||W H − W G || are decided by the dataset (can be seen as a constant), the difference in homomorphism densities will be decided by λ. On this basis, the label of the mixed graphon is set to λy G + (1 − λ)y H . Therefore, G-Mixup can preserve the different discriminative motifs of the two different graphons into one mixed graphon. 4.2. Will the generated graphs from graphon W I preserve the mixture of discriminative motifs?</figDesc><table /><note>is the number of nodes in graph F , and ||W H − W G || denotes the cut norm 3 . Proof Sketch. The proof follows the derivation of Counting Lemma for Graphons (Lemma 10.23 in Lovász (2012)), which associates the homomorphism density with the cut norm ||W H − W G || of graphons. Specifically, we take the two graphons in this Lemma to deduce the bound of the difference of homomorphism densities of W I and W G /W H . Detailed proof are in Appendix A.2.Theorem 4.2 suggests that the difference in the homomorphism densities of the mixed graphon and original graphons is upper bounded. Note that difference depends on the hyperparameter λ, the edge number e(F G )/e(F</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons of G-Mixup with different GNNs on different datasets. The metric is the classification accuracy. Experimental settings are in Appendix F. Dropedge 72.20±1.82 48.83±3.02 92.00±1.13 55.10±0.44 49.77±0.76 w/ DropNode 72.16±0.28 48.33±0.98 90.25±0.98 53.26±4.99 49.95±1.70 w/ Subgraph 68.50±0.86 47.25±3.78 90.33±0.87 54.60±3.15 49.67±0.90 w/ M-Mixup 70.83±1.04 49.88±1.34 90.75±1.78 54.95±0.86 49.81±0.80 w/ G-Mixup 71.94±3.00 50.46±1.49 92.90±0.87 55.49±0.53 50.50±0.41 ent datasets, and summarize results in Tables</figDesc><table><row><cell>Cross-entropy Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell>Epoch</cell><cell>Epoch</cell><cell>Epoch</cell></row><row><cell cols="7">Figure 4. The training/validation/test curves on IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI-5K with GCN</cell></row><row><cell cols="6">as backbone. The curves are depicted on ten runs.</cell></row><row><cell></cell><cell>Dataset</cell><cell>IMDB-B</cell><cell cols="4">IMDB-M REDD-B REDD-M5 REDD-M12</cell></row><row><cell></cell><cell>#graphs</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>4999</cell><cell>11929</cell></row><row><cell></cell><cell>#classes</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>5</cell><cell>11</cell></row><row><cell></cell><cell>#avg.nodes</cell><cell>19.77</cell><cell>13.00</cell><cell>429.63</cell><cell>508.52</cell><cell>391.41</cell></row><row><cell></cell><cell>#avg.edges</cell><cell>96.53</cell><cell>65.94</cell><cell>497.75</cell><cell>594.87</cell><cell>456.89</cell></row><row><cell>GCN</cell><cell cols="6">vanilla w/ Dropedge 72.50±0.31 49.08±1.89 81.25±8.15 51.35±1.54 47.08±0.55 72.18±1.55 48.79±2.72 78.82±1.33 45.07±1.70 46.90±0.73 w/ DropNode 72.00±4.09 48.58±2.85 79.25±0.35 49.35±1.80 47.93±0.64</cell></row><row><cell></cell><cell cols="6">w/ Subgraph 68.50±4.76 49.58±2.61 74.33±2.88 48.70±1.63 47.49±0.93</cell></row><row><cell></cell><cell cols="6">w/ M-Mixup 72.83±1.75 49.50±1.97 75.75±4.53 49.82±0.85 46.92±1.05</cell></row><row><cell></cell><cell cols="6">w/ G-Mixup 72.87±3.85 51.30±2.14 89.81±0.74 51.51±1.70 48.06±0.53</cell></row><row><cell>GIN</cell><cell>vanilla w/</cell><cell cols="5">71.55±3.53 48.83±2.75 92.59±0.86 55.19±1.02 50.23±0.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Performance comparisons of G-Mixup with different Pooling methods. The metric is classification accuracy.</figDesc><table><row><cell></cell><cell>Method</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>REDD-B</cell><cell>REDD-M5k</cell></row><row><cell>TopKPool</cell><cell cols="4">vanilla w/ Dropedge w/ DropNode 69.16±1.04 48.50±2.50 81.33±4.48 72.37±5.01 50.57±1.62 90.30±1.47 71.75±2.18 48.75±2.94 88.96±1.90 w/ Subgraph 67.83±4.01 50.83±2.38 86.08±2.12</cell><cell>45.07±1.70 47.43±1.82 46.15±2.28 45.75±2.47</cell></row><row><cell></cell><cell>w/ M-Mixup</cell><cell cols="3">71.83±3.03 51.22±1.17 87.58±3.16</cell><cell>45.60±2.35</cell></row><row><cell></cell><cell>w/ G-Mixup</cell><cell cols="3">72.80±3.33 51.30±2.14 90.40±0.89</cell><cell>46.48±1.70</cell></row><row><cell>DiffPool</cell><cell cols="4">vanilla w/ Dropedge w/ DropNode 70.25±3.01 46.83±1.34 76.68±2.57 71.68±3.40 47.75±2.34 78.40±4.38 69.16±2.51 49.44±2.50 76.00±5.50</cell><cell>31.61±5.95 34.46±6.80 33.10±5.53</cell></row><row><cell></cell><cell>w/ Subgraph</cell><cell cols="3">69.50±2.16 46.00±4.43 76.06±2.81</cell><cell>31.65±4.43</cell></row><row><cell></cell><cell>w/ M-Mixup</cell><cell cols="3">66.50±4.04 45.16±4.63 78.37±2.29</cell><cell>34.46±6.80</cell></row><row><cell></cell><cell>w/ G-Mixup</cell><cell cols="3">73.25±3.89 50.70±2.79 78.87±2.27</cell><cell>38.42±6.51</cell></row><row><cell>MincutPool</cell><cell cols="4">vanilla w/ Dropedge w/ DropNode 73.50±3.89 49.91±2.83 85.68±2.04 73.25±3.27 49.04±3.57 84.95±3.25 69.16±2.51 49.66±1.73 81.37±1.59 w/ Subgraph 70.25±1.84 48.18±1.10 84.91±2.50 w/ M-Mixup 70.62±2.09 49.96±1.86 85.12±2.29</cell><cell>49.32±2.67 47.20±1.10 46.82±4.60 49.22±2.49 47.20±1.10</cell></row><row><cell></cell><cell>w/ G-Mixup</cell><cell cols="3">73.93±2.84 50.29±2.30 85.87±1.37</cell><cell>50.12±2.47</cell></row><row><cell cols="6">bilize the training of graph neural networks. Experiments</cell></row><row><cell cols="2">on OGB</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Robustness to label corruption with different ratios. Dropedge 72.00±2.44 69.52±3.25 64.12±3.44 48.50±0.00 w/ M-Mixup 71.87±3.56 69.03±4.85 65.62±9.89 48.50±0.00 w/ G-Mixup 72.56±3.08 69.87±5.41 65.50±8.90 52.56±6.97</figDesc><table><row><cell>Models</cell><cell>Methods</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell cols="2">IMDB-B vanilla</cell><cell cols="4">72.30±3.67 69.43±4.80 63.65±8.87 55.21±8.75</cell></row><row><cell cols="2">w/ REDD-B vanilla</cell><cell cols="4">73.90±1.43 75.68±2.75 68.12±0.81 46.50±0.00</cell></row><row><cell></cell><cell cols="5">w/ Dropedge 73.75±1.28 72.06±1.42 46.50±0.00 46.50±0.00</cell></row><row><cell></cell><cell cols="5">w/ M-Mixup 71.96±1.97 76.00±2.24 54.43±1.09 46.50±0.00</cell></row><row><cell></cell><cell cols="5">w/ G-Mixup 71.94±3.00 76.34±1.49 74.21±1.85 53.50±0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Robustness to topology corruption with different ratios.</figDesc><table><row><cell>Models</cell><cell>Methods</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell cols="2">Removing vanilla</cell><cell cols="4">77.96±3.71 67.59±5.73 64.96±8.87 65.71±8.31</cell></row><row><cell>edges</cell><cell cols="5">w/ Dropedge 74.40±2.26 65.12±3.51 65.93±2.32 57.87±4.14</cell></row><row><cell></cell><cell cols="5">w/ M-Mixup 75.62±1.59 65.81±3.84 59.81±9.45 57.31±3.15</cell></row><row><cell></cell><cell cols="5">w/ G-Mixup 81.46±3.08 71.12±7.47 67.46±8.90 66.25±7.78</cell></row><row><cell>Adding</cell><cell>vanilla</cell><cell cols="4">76.12±5.73 74.37±6.48 72.31±2.69 72.00±2.92</cell></row><row><cell>edges</cell><cell cols="5">w/ Dropedge 70.53±1.47 70.18±1.29 71.18±1.53 70.90±1.53</cell></row><row><cell></cell><cell cols="5">w/ M-Mixup 73.41±2.40 71.87±1.28 71.50±2.03 71.21±2.00</cell></row><row><cell></cell><cell cols="5">w/ G-Mixup 84.31±3.21 82.21±4.31 77.00±2.25 75.56±3.05</cell></row><row><cell></cell><cell>IMDB-BINARY</cell><cell></cell><cell></cell><cell cols="2">REDDIT-BINARY</cell></row><row><cell></cell><cell>Avg. #Nodes of</cell><cell></cell><cell></cell><cell>Avg. #Nodes of</cell><cell></cell></row><row><cell></cell><cell>Original Graphs</cell><cell></cell><cell></cell><cell>Original Graphs</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>The MSE error of graphon estimation methods on synthetic graphs<ref type="bibr" target="#b39">(Xu et al., 2021)</ref>. The graphon estimation is based on 10 graphs, the error is Mean Square Error, and the resolution of graphon is 1000 × 1000.</figDesc><table><row><cell>W(x, y)</cell><cell>SBA</cell><cell>LG</cell><cell>MC USVT</cell><cell>SAS</cell></row><row><cell>xy</cell><cell cols="4">65.6±6.5 29.8±5.7 11.3±0.8 31.7±2.5 125.0±1.3</cell></row><row><cell>e −(x 0.7 +y 0.7 )</cell><cell cols="4">58.7±7.8 22.9±3.1 71.7±0.5 12.2±1.5 77.7±0.8</cell></row><row><cell>x 2 +y 2 + √ x+ √ y 4</cell><cell cols="4">63.4±7.6 24.1±2.5 73.2±0.7 33.8±1.1 99.3±1.2</cell></row><row><cell>1 2 (x + y)</cell><cell cols="4">66.2±8.3 24.0±2.5 71.9±0.6 40.2±0.8 108.3±1.0</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1+exp(−10(x 2 +y 2 ))</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>The sensitivity of G-Mixup to Mixup Ratio λ on ogbg-molbbbp dataset. The p-value is 0.00515, 0.0994, 0.0109, 0.0471 , indicating the 3 improvements are statistically significant (p &lt; 0.05). 23±0.75 68.23±1.81 68.45±0.84 67.54±3.09 69.51±1.20 67.79±0.82 67.60±1.31 69.48±2.62 67.86±1.02 68.78±2.61 68.05±1.52 GCN-virtual 68.57±2.61 68.81±1.57 67.20±1.30 68.64±2.09 70.05±1.78 68.77±2.31 69.11±1.12 68.82±0.98 69.07±1.48 68.37±0.95 65.13±1.11 GIN 68.20±1.04 69.37±1.38 69.28±1.24 68.89±2.70 70.17±1.03 66.95±0.92 69.86±1.05 70.01±1.14 68.65±1.03 69.73±1.32 68.42±2.31 GIN-virtual 70.58±1.55 69.44±1.88 70.02±1.68 69.77±0.88 69.18±0.87 68.17±1.67 68.62±1.15 69.16±1.87 70.15±1.32 68.66±0.68 67.10±2.10</figDesc><table><row><cell>λ</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell><cell>0.30</cell><cell>0.35</cell><cell>0.40</cell><cell>0.45</cell><cell>0.50</cell><cell>Vanilla</cell></row><row><cell>GCN</cell><cell>68.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>The sensitivity of G-Mixup to Mixup Ratio λ on ogbg-molbace dataset. The p-value is 0.0227, 0.0375, 0.0401, 0.0427, indicating the 4 improvements are statistically significant (p &lt; 0.05). 41±2.24 77.33±2.10 80.73±2.06 78.42±2.25 77.98±2.03 79.25±1.64 75.80±4.31 78.40±1.88 79.54±1.25 77.90±2.67 80.36±1.56 GCN-virtual 75.64±4.03 76.80±1.74 73.55±4.79 76.46±1.05 73.97±4.11 76.55±2.28 75.91±2.73 77.99±2.59 78.34±1.10 72.84±5.52 74.49±3.04 GIN 76.44±2.19 75.55±4.05 77.79±3.34 75.20±2.91 74.79±2.64 76.27±4.61 73.02±3.68 76.29±3.55 75.77±2.30 74.12±4.12 75.91±1.01 GIN-virtual 74.51±4.91 74.07±2.76 73.53±3.98 78.85±1.98 77.15±2.44 76.85±3.42 79.69±1.37 75.13±5.46 77.04±1.37 78.63±2.04 74.19±4.99</figDesc><table><row><cell>λ</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell><cell>0.30</cell><cell>0.35</cell><cell>0.40</cell><cell>0.45</cell><cell>0.50</cell><cell>Vanilla</cell></row><row><cell>GCN</cell><cell>77.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">Cut norm is used to measure the similarity between graphs, Details about cut norm are in Appendix A.1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1">https://github.com/DropEdge/DropEdge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_2">https://github.com/Shen-Lab/GraphCL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_3">https://github.com/Shen-Lab/GraphCL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_4">https://github.com/vanoracai/MixupForGraph</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_5">https://github.com/snap-stanford/ogb/tree/master/examples/graphproppred/mol</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_6">A p-value less than 0.05 (typically ≤ 0.05) is statistically significant.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(FCN) and the FCN loss is computed using Manifold Mixup. <ref type="bibr" target="#b33">Verma et al. (2019b)</ref> proposed to generate augmented graphs from an explicit target distribution for semi-supervised learning, which has flexible control of the strength and diversity of augmentation. Many graph augmentation methods are proposed to solve node classificaiton task. However, the node classification task is a different task in graph learning from graph classification task. The node classification task usually has one input graph, thus the graph augmentation methods for node classification is limited to one graph while the graph augmentation for graph classification can manipulate multiple graphs. Thus graph data augmentation for node-level task is not applicable to our scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-Dependent Graph Data Augmentation for Graph Classification.</head><p>There are some model-dependent graph augmentation methods <ref type="bibr" target="#b29">(Suresh et al., 2021;</ref><ref type="bibr" target="#b43">You et al., 2022;</ref><ref type="bibr" target="#b51">Zhou et al., 2020b)</ref> for graph classification task. <ref type="bibr" target="#b29">Suresh et al. (2021)</ref> proposed to enable GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in graph contrastive learning during the training phase. <ref type="bibr" target="#b43">You et al. (2022)</ref> proposed to learn a continuous prior parameterized by a neural network from data during contrastive training, which is used to augment graph. The difference between our proposal and these methods is that G-Mixup an general model-agnostic graph data augmentation methods for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>In this appendix, we present the pseudo code for G-Mixup. We first present the pseudo code for graphon estimation in Algorithm 1, which depicts how to generate the graphon and the node features. Since our proposed method is a modelagnostic method, which can be conducted before the model training. Then we present the pseudo code G-Mixup. The graphon estimation is based on the one class of graphs, thus we can estimate on graphon using all the graphs in the same class or a random batch of graphs in the same class. On this basis, we have two version of concrete implementations: 1) estimating graphon on graphon using all the graphs in the same class (Algorithm 2), 2) estimating graphon on graphon using a random batch of graphs in the same class (Algorithm 3). The first implementation provide more accurate estimated graphons while the second encourages more diversity of the synthetic graphs. Note that all these two versions can be done as a pre-processing before model training.  <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref>. The initial learning rate is 0.01 and will drop the learning rate by half every 100 epochs. The batch size is set to 128. We split the dataset into train/val/test data by 7 : 1 : 2. Note that best test epoch is selected on a validation set, and we report the test accuracy on ten runs. For hyperparemeter in G-Mixup, we generate 20% more graph for training graph. The graphons are estimated based on the training graphs. We use different λ ∈ [0.1, 0.2] to mix up the graphon and generate synthetic with different strength of mixing up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Architectures of Graph Neural Networks</head><p>We adopted two categories of graph neural networks as our baselines, The first category is Graph Convolutional Network (GCN) and Graph Isomorphism Network (GIN). The second category is graph polling methods, including TopK Pooling (TopKPool), Differentiable Pooling (DiffPool), MinCut Pooling (MincutPool) and Graph Multiset Pooling (GMT). The details of the GNNs are listed as follows:</p><p>• GCN 4 <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016)</ref>. Four GNN layers and global mean pooling are applied. All the hidden units is set to 64.</p><p>The activation is ReLU <ref type="bibr" target="#b24">(Nair &amp; Hinton, 2010)</ref>.</p><p>• GIN 5 <ref type="bibr" target="#b40">(Xu et al., 2018)</ref>. We apply five GNN layers and all MLPs have two layers. Batch normalization <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015)</ref> is applied on every hidden layer. All hidden units are set to 64. The activation is ReLU <ref type="bibr" target="#b24">(Nair &amp; Hinton, 2010)</ref>.</p><p>• TopKPool 6 <ref type="bibr" target="#b10">(Gao &amp; Ji, 2019)</ref>. Three GNN layers and three TopK pooling are applied. A there-layer percetron are adopted to predict the labels. All the hidden units is set to 64. The activation is ReLU <ref type="bibr" target="#b24">(Nair &amp; Hinton, 2010)</ref>.</p><p>• DiffPool 7 <ref type="bibr" target="#b41">(Ying et al., 2018</ref>) is a differentiable graph pooling methods that can be adapted to various GNN architectures, which maps nodes to clusters based on their learned embeddings.</p><p>• MincutPool 8 <ref type="bibr" target="#b3">(Bianchi et al., 2020</ref>) is a differentiable pooling baselines. It learns a clustering function that can be quickly evaluated on out-of-sample graphs.</p><p>• GMT 9 <ref type="bibr" target="#b2">(Baek et al., 2020</ref>) is a multi-head attention based global pooling layer to generate graph representation, which captures the interaction between nodes according to their structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Baseline Methods</head><p>We adopted three mainstream graph data augmentation methods as our baselines, including DropEdge, DropNode, Subgraph and Manifold-Mixup. The details of the baselines are listed as follows,</p><p>• DropEdge 10 ( <ref type="bibr" target="#b26">Rong et al., 2020)</ref>. DropEdge randomly removes a certain ratios of edges from the input graph at each training epoch, which can prevent over-fitting and alleviate over-smoothing.</p><p>• DropNode 11 <ref type="bibr" target="#b42">(You et al., 2020)</ref>. DropNode randomly remove certain portion of nodes as well as their connections, which under a underlying assumption that missing part of nodes will note affect the semantic meaning of original graph.</p><p>• Subgraph 12 <ref type="bibr" target="#b42">(You et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2020a)</ref>. Subgraph method samples a subgraph from the original graph using random walk The generated graph will keep part of the the semantic meaning of original graphs.</p><p>• M-Manifold 13 <ref type="bibr" target="#b37">(Wang et al., 2021)</ref> Manifold-Mixup conducts Mixup operation for graph classification in the embedding space, which interpolates graph-level embedding after the READOUT function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Experimental Setting of Robustness</head><p>The graph neural network adopted in this experiment is GCN, the architecture of which is as above. For label corruption, we randomly corrupt the graph labels with different corruption ratio 10%, 20%, 30%, 40%. For topology corruption, we we randomly remove/add edges with different corruption ratio 10%, 20%, 30%, 40%. The dataset for topology corruption is REDDIT-BINARY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Experiments</head><p>In this appendix, we conduct additional experiments to further investigate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Visualization of Graphons on More Real-world Dataset</head><p>G-Mixup explores five graphon estimation methods, including sorting-and-smoothing (SAS) method <ref type="bibr" target="#b5">(Chan &amp; Airoldi, 2014)</ref>, stochastic block approximation (SBA) <ref type="bibr" target="#b0">(Airoldi et al., 2013)</ref>, "largest gap" (LG) <ref type="bibr" target="#b6">(Channarond et al., 2012)</ref>, matrix completion (MC) <ref type="bibr" target="#b19">(Keshavan et al., 2010)</ref> and the universal singular value thresholding (USVT) <ref type="bibr" target="#b7">(Chatterjee et al., 2015)</ref>. We present the estimated graphon by LG in Figure <ref type="figure">2</ref>. Here we present more visualization of graphons on IMDB-BINARY, REDDIT-BINARY and IMDB-MULTI dataset. An obvious observation is that graphons of different classes of graphs are different. This observation further validates the divergence of graphon between different classes of graphs. To further validate the effectiveness of G-Mixup on more graph neural networks, we experiment with GMT <ref type="bibr" target="#b2">(Baek et al., 2020)</ref>, a modern pooling method. To reproduce GMT results, we the released code and the recommended hyperparameters for their used datasets (D&amp;D, MUTAG, PROTEINS, IMDB-B, IMDB-M) in their paper. The results are presented in Table <ref type="table">7</ref>. 9 G-Mixup can significantly improve the performance of GMT. Table <ref type="table">7</ref> shows that G-Mixup outperform all the baselines on all datasets. Overall, G-Mixup outperform vanilla, Dropedge, ManifoldMixup by 1.44%, 1.28%, 2.01%, respectively. This indicates the superiority of G-Mixup for graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Experiment on Molecular Property Prediction</head><p>We experiment on molecular property prediction task <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>, including ogbg-molhiv, ogbg-molbace, ogbgmolbbbp. In these dataset, each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. We adopte official reference graph neural network backbones (gcn, gcn-vitual, gin, gin-vitual) 14 as our backbones, and we generate the edge attributes randomly for synthetic graphs. The results are presented in Table <ref type="table">8</ref>. 10 G-Mixup can improve the performance of GNNs on molecular property prediction task with the experimental setting for a fair comparison. Table <ref type="table">8</ref> shows that G-Mixup gains 9 best performances among 12 reported AUCs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
				<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="692" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Centrality measures for graphons: Accounting for uncertainty in networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Avella-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Parise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Segarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="520" to="537" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate learning of graph representations with graph multiset pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Sós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesztergombi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Mathematics</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1801" to="1851" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A consistent histogram estimator for exchangeable graph models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification and estimation in the stochastic blockmodel based on the empirical degrees</title>
		<author>
			<persName><forename type="first">A</forename><surname>Channarond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Daudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2574" to="2601" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matrix estimation by universal singular value thresholding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="214" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajamanickam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10206</idno>
		<title level="m">How robust are graph neural networks to structural noise?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quick approximation to matrices and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="220" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05178</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph u-nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear mixup: Out-of-manifold data augmentation for text classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training graph neural networks by graphon estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matrix completion from a few entries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2980" to="2998" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large networks and graph limits</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Limits of dense graph sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="933" to="957" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metropolis-hastings data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graphon neural networks and the transferability of graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12529</idno>
		<title level="m">Graph and graphon neural network stability</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05819</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data augmentation for graph convolutional network on semi-supervised classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Dharejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08848</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graphmix: Improved training of gnns for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11715</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The laplacian spectrum of large graphs sampled from graphons</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vizuete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Garin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><surname>Graphcrop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10564</idno>
		<title level="m">Subgraph cropping for graph classification</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nodeaug: Semi-supervised node classification with data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mixup for node and graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3663" to="3674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning graphons via structured gromov-wasserstein barycenters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10505" to="10513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bringing your own view: Graph contrastive learning without prefabricated data augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01702</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How does mixup help with robustness and generalization?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An end-toend deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11015" to="11023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph theory and additive combinatorics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://yufeizhao.com/gtac/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications. AI Open</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data augmentation for graph classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="2341" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
