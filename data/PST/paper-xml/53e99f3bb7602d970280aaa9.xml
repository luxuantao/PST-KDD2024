<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Practical Off-chip Meta-data for Temporal Memory Streaming</title>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_9qFNvSV">
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder ref="#_A5JjJH8">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Ecole Polytechnique F?d?rale de Lausanne and Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">http://www.ece.cmu.edu/~stems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Practical Off-chip Meta-data for Temporal Memory Streaming</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior research demonstrates that temporal memory streaming and related address-correlating prefetchers improve performance of commercial server workloads though increased memory level parallelism. Unfortunately, these prefetchers require large on-chip meta-data storage, making previously-proposed designs impractical. Hence, to improve practicality, researchers have sought ways to enable timely prefetch while locating meta-data entirely off-chip. Unfortunately, current solutions for off-chip meta-data increase memory traffic by over a factor of three.</p><p>We observe three requirements to store meta-data off chip: minimal off-chip lookup latency, bandwidthefficient meta-data updates, and off-chip lookup amortized over many prefetches. In this work, we show:</p><p>(1) minimal off-chip meta-data lookup latency can be achieved through a hardware-managed main memory hash table, (2) bandwidth-efficient updates can be performed through probabilistic sampling of meta-data updates, and (3) off-chip lookup costs can be amortized by organizing meta-data to allow a single lookup to yield long prefetch sequences. Using these techniques, we develop Sampled Temporal Memory Streaming (STMS), a practical address-correlating prefetcher that keeps predictor meta-data in main memory while achieving 90% of the performance potential of idealized on-chip meta-data storage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Memory access latency continues to pose a crucial performance bottleneck for commercial server workloads <ref type="bibr" target="#b10">[11]</ref>. System designers employ a variety of strategies to bridge the processor-memory performance gap. On the software side, efforts are under way to restructure server workloads to increase on-chip data sharing and reuse <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. Although these efforts reduce off-chip accesses, application working sets continue to exceed available cache capacity. On the hardware side, multi-threading is effective in improving throughput when abundant software threads are available, but does not improve response time <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Prefetching improves both throughput and response time by increasing memory level parallelism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> and remains an essential strategy to address the processor-memory performance gap. Today's systems employ spatial/stride based prefetchers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref> because they are practical to implement. These prefetchers require simple hardware additions and minimal on-chip area <ref type="bibr" target="#b16">[17]</ref>. However, the effectiveness of these prefetchers is limited in commercial workloads (e.g., online transaction processing), which are dominated by pointer-chasing access patterns <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In contrast to stride-based approaches, addresscorrelating prefetchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> are effective for repetitive, yet arbitrarily-irregular access patterns, such as the pointer-chasing access patterns of commercial workloads <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Addresscorrelating prefetchers associate a miss address with a set of possible successor misses, or, in Temporal Memory Streaming (TMS) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and similar recent proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, a sequence of successors. For modern commercial workloads, early address-correlating prefetcher designs are impractical because the onchip meta-data size required to capture correlations is proportional to an application's data set and requires megabytes of storage <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To improve practicality, recent address-correlating prefetchers store meta-data off chip in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Shifting correlation tables to main memory eliminates on-chip storage costs, but creates two new challenges. First, correlation table lookups incur one or more main memory access latencies, which delay prefetches. The correlation algorithm must be designed to tolerate this long lookup latency by targeting prefetches several misses ahead in the anticipated future miss sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Second, the extra memory traffic used to lookup and maintain meta-data increases memory bandwidth pressure. Existing designs require correlation table lookups and updates on nearly every cache miss, incurring overhead traffic three times larger than the base system's read traffic.</p><p>We observe three key requirements to make offchip meta-data practical: (1) minimal off-chip metadata lookup latency, <ref type="bibr" target="#b1">(2)</ref> bandwidth-efficient meta-data updates, and (3) off-chip lookup amortized over many accurate prefetches. We propose hash-based lookup to achieve the first requirement. In hash-based lookup, we use a hardware-managed hash table to index previ-ously-recorded miss-address sequences within a log of prior misses. Hash-based lookup enables retrieval of the corresponding miss addresses with only two mainmemory round-trips (one for the hashed index table lookup, and the second for the address sequence). We propose probabilistic update to achieve the second requirement. Probabilistic update applies only a randomly-selected subset of updates to the hashed index table, reducing index table maintenance bandwidth to practical levels. Because recurring miss-address sequences either tend to be long or repeat frequently, stale or missing index table entries do not sacrifice significant coverage. We address the third requirement by applying these two mechanisms to a previously-proposed prefetch meta-data organization where misses are logged continuously in a circular buffer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. By separating indexing and logging, this prefetcher organization allows a single lookup to predict long missaddress sequences of up to hundreds of misses <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. We evaluate our practical design, Sampled Temporal Memory Streaming (STMS), through cycle-accurate full-system simulation of scientific and commercial multiprocessor workloads, to demonstrate: ? Performance potential. Ideal on-chip lookup would enable TMS to eliminate 19-99% of offchip misses (40-60% in online transaction processing and web workloads), improving performance by up to 80% (5-18% for OLTP and Web). ? Storage efficiency. Because meta-data is located off chip, STMS requires only 2KB of on-chip prefetch buffers per core. For maximum effectiveness, STMS needs 64MB of meta-data in main memory, a small fraction of memory in servers. ? Latency efficiency. By using hash-based lookup to prefetch sequences of tens of misses, STMS mitigates main-memory meta-data access latency. A practical lookup mechanism achieves 90% of the performance potential of idealized lookup. ? Bandwidth efficiency. We show that probabilistic update reduces the memory traffic overhead of meta-data updates by a geometric mean factor of 3.4 with a maximum coverage loss of 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>An address-correlating prefetcher learns temporal relationships between accesses to specific addresses. For instance, if address B tends to be accessed shortly after address A, an address-correlating prefetcher can learn this relationship and use the occurrence of A to trigger a prefetch of B. Address-correlating prefetchers succinctly capture pointer-chasing relationships, and thus substantially improve the performance of pointerintensive commercial workloads <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Pair-wise-correlating prefetchers. The Markov prefetcher <ref type="bibr" target="#b15">[16]</ref> is the simplest prefetcher design for predicting pair-wise correlation between an address and its successor (i.e., two addresses that tend to cause consecutive cache misses). The Markov prefetcher hardware is organized as a set-associative table that maps an address to several recently-observed possibilities for the succeeding miss. On each miss, the table is searched for the miss address, and if an entry is found, the likely successors are prefetched. Several pair-wisecorrelating prefetchers build upon this simple design to optimize correlation table storage <ref type="bibr" target="#b12">[13]</ref>, or trigger prefetchers earlier to improve lookahead <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The key limitation of pairwise-correlating prefetchers is that they attempt to predict correctly only a single miss per prediction, limiting memory level parallelism and prefetch lookahead. More recent addresscorrelating prefetchers use a single correlation to predict a sequence of successor misses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. We adopt the terminology of <ref type="bibr" target="#b25">[26]</ref> and refer to these successor sequences as temporal streams-extended sequences of memory accesses that recur over the course of program execution.</p><p>Temporal streaming. The observation that memory access sequences recur was first quantified in memory trace analysis by Chilimbi and Hirzel <ref type="bibr" target="#b3">[4]</ref>. To exploit this observation, researchers initially proposed correlation tables that store a temporal stream (i.e., a short sequence of successors) rather than only a single future access in each set-associative correlation table entry <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. The primary shortcoming of this set-associative organization is that temporal stream length is fixed to the size of the table entry, typically three to six successor addresses. However, offline analyses of miss repetition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> have shown that temporal streams vary drastically in length, from two to hundreds of misses. Fixing stream length in the prefetcher design leads either to inefficient use of correlation table storage (if the entries are too large) or sacrifices lookahead and prefetch coverage (if entries are too small).</p><p>To support variable length temporal streams while maintaining storage efficiency, several designs separate the storage of address sequences and correlation data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. A history buffer records the application's recent miss-address sequence, typically in a circular buffer. An index table correlates a particular miss address (or other lookup criteria) to a location in the history buffer. The split-table approach allows a single index-table entry to point to a stream of arbitrary length, allowing maximal lookahead and prefetch coverage without substantial storage overheads.</p><p>In this study, our goal is not to improve the prediction accuracy of state-of-the-art address-correlating prefetchers. Instead, we seek to identify and solve the key implementation barriers that make these prefetchers unattractive for practical deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Practicality Challenges</head><p>More than a decade of research has repeatedly shown that address-correlating prefetchers can eliminate about half of all off-chip misses in pointerintensive commercial server workloads, whereas stride prefetchers provide only minimal benefit <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Nevertheless, stride prefetchers are widely implemented, while, to date, no commercial design has implemented an address-correlating prefetcher. In this section, we enumerate the major practicality challenges of prior address-correlating prefetcher designs.</p><p>On-chip storage requirements. Initial addresscorrelating prefetcher designs located correlation tables entirely on-chip <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. However, correlation table storage requirements are proportional to the application's working set. Hence, for commercial workloads, even the most storage-efficient design <ref type="bibr" target="#b12">[13]</ref> requires megabytes of correlation table storage to be effective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. Figure <ref type="figure">1</ref> (left) shows the number of correlation table entries required for a given average coverage across commercial workloads for the idealized address-correlating prefetcher we analyze in detail in Section 5.2. Our result corroborates prior work <ref type="bibr" target="#b5">[6]</ref>: to achieve maximum coverage in commercial workloads, correlation tables must store more than one million entries, which can require as much as 64MB of storage. High storage requirements make on-chip correlation tables impractical.</p><p>More recent prefetchers locate correlation metadata in main memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>, where multi-megabyte tables can easily be accommodated. However, offchip tables lead to two new challenges: high lookup latency and increased memory bandwidth pressure.</p><p>High lookup latency. When correlation tables are off chip, each lookup requires at least one main memory access before prefetching can proceed. Unlike other predictors that can make use of the on-chip cache hierarchy to provide "virtual" on-chip lookup <ref type="bibr" target="#b1">[2]</ref>, address correlation tables are substantially larger than the on-chip caches and correlation entries exhibit minimal temporal locality. A correlation table entry is not reused until the address it corresponds to is evicted from on-chip caches. By that time, the correlation entry is also likely to be evicted.</p><p>Instead, the prefetching mechanism must be designed to account for long correlation table lookup latency. Epoch-based correlation prefetching (EBCP) <ref type="bibr" target="#b5">[6]</ref> explicitly accounts for off-chip lookup latency and the memory level parallelism already obtained by outof-order processing in choosing prefetch addresses. Rather than correlate an address to its immediate successors, EBCP skips over successor addresses that will be requested while the correlation table lookup is in progress. However, EBCP employs a set-associative correlation table, which, as noted in Section 2, bounds maximum stream length, limiting memory level parallelism, lookahead, and bandwidth efficiency.</p><p>With STMS, we instead mitigate lookup latency by following arbitrarily long streams to maximize the number of prefetches per lookup operation-a single lookup may lead to tens or hundreds of prefetches. The split-table meta-data organization and hash-based lookup mechanism are key to this strategy.</p><p>Memory bandwidth requirements. Address-correlating prefetchers with off-chip meta data substantially increase pressure on memory bandwidth. First, as with any prefetching mechanism, erroneous prefetches (cache blocks that are prefetched but never accessed) consume memory bandwidth, as prefetchers inherently trade increased memory bandwidth requirements to reduce effective access latency. However, off-   chip correlation tables make matters worse: both lookups and updates require off-chip memory accesses. Figure <ref type="figure">1</ref> (right) shows the average memory traffic overheads for three existing address-correlating prefetchers that store meta-data in main memory, the User Level Memory Thread (ULMT <ref type="bibr" target="#b22">[23]</ref>), the Epoch-Based Correlation Prefetcher (EBCP <ref type="bibr" target="#b5">[6]</ref>), and the Temporal Streaming Engine (TSE <ref type="bibr" target="#b26">[27]</ref>), based on their published results. Overhead traffic is normalized to the number of memory reads without a prefetcher. "Erroneous Prefetches" are calculated directly from published accuracy and coverage. ULMT and TSE incur "Meta-data Lookup" traffic on each off-chip read miss (i.e., the remaining misses after prefetching), requiring one and three memory accesses per lookup, respectively. EBCP performs a single memory access to lookup its meta-data at the start of each off-chip miss epoch-that is, when the number of outstanding (nonprefetch) off-chip misses transitions from zero to one. ULMT and EBCP perform "Meta-data Update" following each lookup, both requiring three memory accesses per update. TSE updates its correlation tables on both off-chip misses and prefetcher hits, requiring slightly over one memory access per update on average.</p><p>As the figure shows, overhead traffic is triple the baseline read traffic without a prefetcher. Several factors mitigate the performance impact of massive traffic overheads in the existing designs. All three designs issue correlation table lookups and updates as low-priority traffic, prioritizing processor-initiated requests. ULMT collocates its prefetcher with an off-chip memory controller, and, hence, its meta-data traffic does not cross the processor's pins. TSE's meta-data lookups are embedded in existing cache coherence traffic. When unused memory bandwidth is abundant, overhead traffic can be absorbed by the memory system with minimal performance impact. However, as available memory bandwidth must be shared among cores in a multi-core system, memory bandwidth utilization is growing rapidly with chip multiprocessor scaling. Hence, the high traffic overhead of current main-memory correlation table designs limits their applicability.</p><p>If it were possible to store correlation tables on chip, lookup and update traffic would be of little concern-though large relative to off-chip traffic, required bandwidth can be easily sustained in dedicated on-chip structures. However, storage requirements preclude onchip meta-data, requiring new solutions to improve bandwidth-efficiency for off-chip meta-data storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">STMS Design</head><p>We leverage prior work in temporal streaming to reuse mechanisms and terminology whenever possible.</p><p>We begin by enumerating the three key requirements for effective temporal streaming in Section 4.1. We then provide an overview of STMS in Section 4.2, constructing a generalized temporal streaming prefetcher that draws heavily from the stream-following mechanisms of the Temporal Streaming Engine (TSE) <ref type="bibr" target="#b26">[27]</ref> and the predictor organization of the Global History Buffer (GHB) <ref type="bibr" target="#b20">[21]</ref>. After we describe the basic hardware operation, the following sections provide details of how the proposed STMS mechanisms meet the requirements for efficient temporal streaming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Requirements for Effective Temporal Streaming</head><p>Minimize Lookup Latency. Temporal-streaming prefetchers initiate predictor lookup on a trigger event (typically a cache miss). However, prefetches cannot be issued until an address sequence is located and retrieved. Cache misses incurred during this time result in lost prefetch opportunity, even if they comprise a predictable temporal stream <ref type="bibr" target="#b5">[6]</ref>. Prior prefetchers targeting desktop/engineering applications rely on long temporal streams to overcome the startup cost <ref type="bibr" target="#b8">[9]</ref>. However, unlike those applications, prior research <ref type="bibr" target="#b26">[27]</ref> and our results (see Section 5.4) indicate that half of the temporal streams in commercial workloads are shorter than ten cache blocks. Therefore, effective streaming for commercial workloads requires minimal lookup latency to ensure timely prefetch.</p><p>Bandwidth-efficient Index Table <ref type="table">Updates</ref>. Maintaining predictor meta-data in off-chip storage induces additional traffic across the memory interface. Spatial locality allows amortization of history buffer updates by storing multiple consecutive addresses with one off-chip write. Conversely, any index table updates are directed to randomized addresses and exhibit neither spatial nor temporal locality. Furthermore, each index table update incurs both a read and a write operation. Performing all updates on an un-optimized mainmemory index table will therefore triple memory bandwidth consumption over a base system. Efficient bandwidth utilization is growing even more important in chip multiprocessor, where scaling in the number of cores per die will continue increasing strain on off-chip bandwidth <ref type="bibr" target="#b13">[14]</ref>.</p><p>Amortized Lookups. Another key requirement for effective temporal streaming is to amortize off-chip lookups over many successful prefetches, thereby keeping off-chip bandwidth low by reducing the number of index table lookups and history buffer reads. The static stream lengths imposed in previous single-table prefetcher designs fragment long temporal streams into short prefetch sequences (see Section 3), limiting prefetcher effectiveness, as half of the temporal streams found in commercial workloads are long (in excess of 10 misses). Furthermore, the bandwidth and congestion overhead of accessing the history structures for each short sub-sequence can match or exceed the bandwidth required to retrieve data. Widening correlation table entries to target long streams in a single-table prefetcher is also bandwidth-inefficient. Large correlation table entries that can contain many miss addresses result in considerable bandwidth overhead when useless addresses or empty space is retrieved from the history structures in the case of shorter streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Design Overview</head><p>Figure <ref type="figure" target="#fig_3">2</ref> shows a block diagram of a four-core single-chip processor equipped with STMS. STMS comprises on-chip prefetch buffers and queues and offchip index table <ref type="bibr" target="#b20">[21]</ref> and history buffers <ref type="bibr" target="#b20">[21]</ref> 1 . The prefetch buffers and queues, located along side the L1 victim caches <ref type="bibr" target="#b16">[17]</ref>, orchestrate the streaming process and act as temporary holding space for a small number of blocks that have been prefetched, but not yet accessed by the core. The off-chip structures are allocated in a private region of main memory, each core receives its own history buffer but all cores share a unified index table. The index table contains a mapping from physical addresses to pointers within the history buffer, facilitating lookup.</p><p>Recording temporal streams. STMS records correct-path off-chip misses and prefetched hits in the corresponding core's history buffer. To avoid polluting the history buffer with wrong-path accesses, instructions observing prefetched hits and off-chip loads are marked in the load-store queue <ref type="bibr" target="#b16">[17]</ref>. Later, when a marked instruction retires, the effective physical address is appended to the history buffer. To minimize pin-bandwidth overhead, a cache-block-sized buffer accumulates entries which are then written to main memory as a group as proposed in <ref type="bibr" target="#b8">[9]</ref>. As history buffer entries are created, the index table entry for the corresponding address may be updated to point to the new history buffer entry. As we discuss in Section 4.4, the majority of index table updates are skipped by probabilistic update to conserve memory bandwidth.</p><p>Each core's miss-address sequence is logged in a separate history buffer. Whereas each core's misses individually form temporal streams, when accesses from multiple cores are interleaved, repetitive sequences are obscured. In the case of multi-threaded cores, each thread requires its own history buffer. The index table is shared by all cores.</p><p>Lookup. Upon an off-chip read miss, STMS searches for a previously-recorded temporal stream that begins with the miss address. STMS performs a pointer lookup in the index table. If a pointer is found, the address sequence is read from the history buffer, starting at the pointer location. It is important to note that the STMS shared index table can locate a temporal stream from another core's history buffer.</p><p>Streaming cache blocks. Miss addresses read from the history buffer are held in a FIFO address queue. STMS prefetches addresses from the queue in order. Prefetched data are stored in the small, fullyassociative prefetch buffer, avoiding cache pollution on erroneous prefetches. On correct prefetches, L1 misses are satisfied directly from the prefetch buffer while STMS continues to populate the address queue with addresses from the off-chip history buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Latency-Efficient Hash-based Index Lookup</head><p>Any associative lookup structure can be used to implement an index table. We examined many possible structures (e.g., red-black trees, open address hash tables, direct-mapped tables), however these structures have unacceptable latency, bandwidth, or storage characteristics. To achieve low lookup latency, we elect to implement the index table as a bucketized probabilistic hash table <ref type="bibr" target="#b7">[8]</ref> pictured in Figure <ref type="figure" target="#fig_3">2</ref> (right).   Physical addresses are hashed to select a bucket. Each bucket contains n {physical address, history buffer pointer} pairs, with n sized to the memory system interface width (i.e., one cache line). On lookup, the entire bucket is retrieved with one memory access, and searched linearly (linear search is negligible relative to the off-chip read latency). On update, the bucket is similarly retrieved and searched. If the trigger address is found in the bucket, the entry's history buffer pointer is updated. If the trigger address is missing, the least recently used entry of the bucket is replaced. Before being written back to memory, the elements are reshuffled to maintain LRU order. We find that assigning a low priority to predictor memory traffic is essential to minimize queueing-related stalls. To facilitate index table updates and to delay writeback until memory bandwidth is available, STMS employs a small (8 KB) bucket buffer.</p><p>The key advantage of our hash table design is lowlatency index table lookup. In addition, the design results in high storage density (i.e., it can be fully loaded) and supports an arbitrary number of parallel reads and updates without synchronization, enabling independent parallel access from multiple cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Bandwidth-Efficient Probabilistic Index Update</head><p>Although only a small fraction of index table entries yields the vast majority of prefetch coverage, to date, no known property allows a priori distinction of useful index table entries at the time they are recorded. Rather than filtering index table updates, we propose probabilistic update. For every potential index table update, a coin flip, biased to a predetermined sampling probability, determines whether the update will or will not be performed. Index table update bandwidth is directly proportional to the sampling probability. For example, a 50% sampling probability halves bandwidth requirements. Probabilistic update is highly effective in reducing index table update bandwidth, while leading to only a small coverage loss.</p><p>Intuitively, sampling does not significantly reduce coverage for most cases: long temporal streams and short frequent temporal streams. Figure <ref type="figure">3</ref> illustrates both cases. For long temporal streams, probabilistic update likely skips several addresses before creating an index table entry pointing into the body of the stream. However, coverage loss on the first few blocks is negligible relative to the stream's length. For a sampling probability of one eighth, the probability of inserting at least one address into the index table reaches 50% within the first five blocks. For frequent temporal streams, the probability of inserting the first address into the index table grows with the number of stream recurrences. For short temporal streams, the index table may remain without a pointer for the first few occurrences, however a high appearance frequency results in an index update within a small number of occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Variable-Length Streams via Split Tables</head><p>The drawbacks of statically pre-determined stream length motivate our desire to support variable-length temporal streams. Variable-length temporal streams are made possible by splitting the history buffer and index table into separate structures, as outlined in Section 2. Short streams are accommodated with minimal storage overhead, while longer streams are accommodated by reading consecutive sections of the history buffer. A key result of this design is that long streams require only a single index lookup. We quantitatively contrast the latency-and bandwidth-efficiency of single-table and split-table organizations in Section 5.4.</p><p>To avoid streaming erroneous blocks past the end of a temporal stream within the history buffer, STMS annotates the history buffer entry following the last contiguous successfully-prefetched address. Whenever STMS encounters a marked entry, it pauses streaming, continuing only if the annotated address is explicitly requested by the core. In contrast to TSE <ref type="bibr" target="#b26">[27]</ref>, the STMS stream-end detection is highly bandwidth-efficient, requiring to read only one location from the history buffer to determine the end of stream. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long temporal streams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short, frequent temporal streams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>The goal of our evaluation is to demonstrate that STMS (1) matches the coverage and performance of idealized temporal memory streaming while storing meta-data in off-chip memory, and (2) that it does not reduce performance of workloads that derive no benefit from temporal streaming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Methodology</head><p>We evaluate STMS using a combination of tracebased and cycle-accurate full-system simulation using the FLEXUS infrastructure. FLEXUS builds on the Virtutech Simics functional simulator. We include four workload classes in our study: online transaction processing (OLTP), decision support (DSS), web server (Web), and scientific (Sci) applications. Table <ref type="table" target="#tab_3">1</ref> (left) details our workload suite.</p><p>We model a four-core chip multiprocessor with private L1 caches and a shared L2 cache configured to represent an aggressive near-future high-performance processor. The minimum L2 hit latency is 20 cycles, but accesses may take longer due to bank conflicts or queueing delays. A total of at most 64 L2 accesses and off-chip misses may be in flight at any time. Further configuration details appear in Table <ref type="table" target="#tab_3">1</ref>.</p><p>We include a stride-based prefetcher in our base system <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29]</ref>. All results report only coverage in excess of that provided by the stride prefetcher.</p><p>We measure performance using the SIMFLEX sampling methodology <ref type="bibr" target="#b27">[28]</ref>. We report confidence intervals on change in performance using matched-pair sample comparison. Our samples are drawn over an interval of from 10s to 30s of simulated time for OLTP and web server workloads, over the complete query execution for DSS, and over a single iteration for scientific applications. We launch measurements from checkpoints with warmed caches, branch predictors, history buffer, and index table state, then warm queue and interconnect state for 100,000 cycles prior to measuring 200,000 cycles. We use the aggregate number of user instructions committed per cycle (i.e., committed user instructions summed over the 4 cores divided by total elapsed cycles) as our performance metric, which is proportional to overall system throughput <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Potential of Idealized Prefetcher</head><p>We begin by demonstrating the performance potential of an idealized version of TMS with on-chip meta-data. The idealized prefetcher records a sequence of cache miss addresses in a "magic" on-chip history buffer that has impractically large storage capacity and zero-latency infinite lookup bandwidth.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> presents the coverage (left) and speedup (right) achieved by the idealized prefetcher over our baseline system. Coverage is defined as the fraction of L2 cache misses eliminated by the prefetcher.</p><p>We corroborate prior work, showing that temporal memory streaming is an effective mechanism for eliminating cache misses in OLTP, web serving, and scientific computing workloads, and that temporal memory streaming is ineffective for DSS workloads because they exhibit non-repetitive access sequences where data is visited only once throughout execution. We also observe that, despite achieving high predictor coverage, minimal speedup opportunity is available in workloads whose dominant bottlenecks are not main memory accesses (in the case of OLTP Oracle, the primary bottlenecks are L1 instruction and data misses that hit in L2 and on-chip core-to-core coherence traffic). However, prefetch coverage indicates that even for such workloads, once other (on-chip) bottlenecks in the system are eliminated, temporal memory streaming offers a benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Transaction Processing (TPC-C)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oracle</head><p>Oracle 10g, 100 warehouse (10 GB), <ref type="bibr" target="#b15">16</ref>  Although the potential gains are evident in Figure <ref type="figure" target="#fig_4">4</ref>, on-chip storage required to implement TMS is impractical (see Figure <ref type="figure">1</ref>). We recognize that advances in the field will improve the coverage of stream-based address-correlating prefetchers beyond our idealized implementation of the proposed prior techniques <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. However, improved predictors can leverage the basic mechanisms we propose in this work to facilitate off-chip lookup. Our aim is therefore not to improve over the idealized prior design that we describe, but instead to match the performance of idealized TMS. To this end, our goal is two-fold: (1) maintain performance improvement for the workloads where streaming is effective, (2) avoid adverse performance impact in workloads where streaming is ineffective. Accomplishing this goal requires bandwidth-, latency-, and storage-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Achieving Storage Efficiency</head><p>On-chip structures. As demonstrated in Section 3, on-chip correlation meta-data are impractical, and, hence, STMS locates its history buffer and index table in main memory. On chip, STMS requires a prefetch buffer and address queue collocated with each core to track pending prefetch addresses and buffer prefetched data. Storage requirements for the address queue are negligible (under 128 bytes), while prefetch buffers each require 2KB. We do not report sensitivity to prefetch buffer size, as it has been studied extensively in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>. In addition, STMS uses a shared 8KB bucket buffer to store index table entries between lookup, update, and write back.</p><p>History buffer. The history buffer's main-memory storage requirements are driven by the prefetcher's meta-data reuse distance. The history buffer must be at least large enough to fit all intervening miss addresses between a recorded temporal stream and its previous occurrence. For commercial workloads, the reuse dis-tance depends on the frequency at which data structures are revisited, resulting in a spectrum of distances, and giving rise to a smooth improvement in coverage as the size of the history buffer is increased. Conversely, scientific computing workloads typically exhibit a reuse distance proportional to the length of a single computational iteration and varies only with the size of the application's dataset.</p><p>Figure <ref type="figure">5</ref> (left) plots predictor coverage as a function of history buffer size. For our commercial workloads, the history buffer must be on the order of 32MB to achieve maximal coverage. For scientific workloads, coverage is bimodal: if the history buffer is sufficiently large to capture an entire iteration, coverage is nearly perfect; if the history buffer is insufficiently large, coverage is negligible. For both classes of workloads, the history buffer storage requirements are at least an order of magnitude greater than can be allocated on chip. However, relative to a server's main memory, history buffer footprint is small. Index table. An ideal index table can locate the most recent occurrence of any miss address in the history buffers. Hence, in the worst case, if all addresses in the history buffer are distinct, then there must exist one index table entry per history buffer entry. In practice, far fewer index table entries are required.</p><p>To optimize index-table storage efficiency, we propose hash-based lookup. Hash-based lookup spreads miss addresses over buckets, applying the LRU replacement policy within each bucket. The LRU policy brings useful history buffer pointers to the top, naturally aging out unneeded entries. Figure <ref type="figure">5</ref> (right) plots predictor coverage as a function of the hash-table size for an ideal (unbounded) history buffer. Hashbased lookup achieves maximum coverage with a 16MB main-memory index table, retaining only a fraction of the index entries of an idealized prefetcher. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Achieving Latency Efficiency</head><p>An idealized prefetcher would have instant lookup, experiencing no latency between the cache miss that triggers a prediction and the predicted prefetch. However, a realistic implementation may lose prefetch opportunity on each lookup because time must be spent to locate and retrieve the miss address sequence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We estimate lost prefetch opportunity by examining workload and lookup mechanism characteristics. A latency-efficient predictor will perform a lookup once per temporal stream recurrence, losing prediction opportunity only during retrieval of the initial addresses within the stream. Naturally, longer streams reduce opportunity loss, however temporal stream length is workload dependent and cannot be controlled. Figure <ref type="figure" target="#fig_5">6</ref> (left) shows a cumulative distribution of prefetches arising from streams of various lengths in our commercial workloads. In the scientific applications, the length of the single temporal stream depends on iteration length. For our configurations, this length is approximately 400,000 misses in em3d, 21,000 in ocean, and 81,000 in moldyn.</p><p>Lookup opportunity loss also depends on a workload's memory level parallelism (MLP <ref type="bibr" target="#b6">[7]</ref>), the average number of off-chip loads issued while at least one such load is outstanding. As discussed in prior work <ref type="bibr" target="#b5">[6]</ref>, a predictor that retrieves meta-data from main memory loses coverage for each followed temporal stream, proportional to the number of round-trip memory accesses needed for a single lookup, multiplied by the workload's MLP (shown in Table <ref type="table" target="#tab_5">2</ref>). Like stream length, MLP is an inherent property of a workload, and is typically low in pointer-chasing applications such as the studied commercial workloads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>As discussed in Section 3, when temporal streams are stored in a single set-associative correlation table (as in EBCP <ref type="bibr" target="#b5">[6]</ref> and ULMT <ref type="bibr" target="#b22">[23]</ref>), lookups require only a single memory access before prefetching can proceed. However, the set-associative table design limits maximum prefetch sequence length (referred to as the prefetch depth <ref type="bibr" target="#b20">[21]</ref>) based on the size of a table entry. Because storage cost, complexity, and update-band-   We propose an efficient organization for index table entries by packing a hash bucket into a single memory block (64 bytes). Our organization limits the maximum number of index entries per bucket (to 12 in our design), but ensures that the index table can be searched with a single access. We performed an extensive analysis of alternative organizations for the index table (e.g., open address hashing, larger hash bucket chains, tree structures), and found that these organizations were either less storage efficient or sacrificed additional coverage due to increased lookup latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Achieving Bandwidth Efficiency</head><p>Placing predictor meta-data off chip introduces several sources of pin-bandwidth overheads: recording miss addresses in the history buffer, index table updates, lookup operations (index table and history buffer accesses), and prefetch of erroneously-predicted addresses. Figure <ref type="figure" target="#fig_6">7</ref> shows the relative importance of each overhead source normalized to the base system off-chip traffic (which includes demand-triggered cache block fetches and writebacks).</p><p>The largest bandwidth overhead in an un-optimized system arises due to index table maintenance (see the bars labelled 100%). Each index table update calls for a read and write operations on the corresponding index table entry, resulting in bandwidth utilization that exceeds the base system traffic for many workloads. The second largest contributor is index table lookups, performed on each demand-read-miss from the L2, searching for a temporal stream to follow. Although lookup traffic is substantial, it decreases as the system makes more correct predictions and eliminates demand misses. Accurate detection of the end-ofstream curtails the bandwidth consumed by erroneous prefetches. Finally, the bandwidth utilized for recording off-chip sequences is negligible (not visible in the graphs), as a single densely-packed history buffer write is performed for every twelve off-chip read misses.</p><p>The high traffic overheads shown in Figure <ref type="figure" target="#fig_6">7</ref> demonstrate the need for probabilistic update. We compare an un-optimized system (100% sampling probability) to one with a probabilistic update sampling probability of 1/8th (12.5%). Probabilistic update drastically reduces the off-chip traffic required for index updates.</p><p>Probabilistic update reduces traffic of index-table maintenance at the cost of predictor coverage. Figure <ref type="figure">8</ref> shows the traffic overhead of streaming (left) and predictor coverage (right) as function of the sampling probability. Index update traffic is proportional to sampling probability. Hence, Figure <ref type="figure" target="#fig_6">7</ref> shows that reducing sampling probability from 100% to 12.5% reduces total traffic. Below 12.5%, other sources of overhead (particularly lookup traffic) dominate.</p><p>Whereas traffic overhead decreases rapidly, predictor coverage decreases logarithmically with sampling probability. Omitting index table updates has only a small effect on coverage because temporal streams either tend to be long (in which case, a later address in the stream can be used to locate it) or to recur frequently (in which case an index entry from a 0 0.5  prior occurrence likely exists). Hence, probabilistic update is highly effective at reducing off-chip traffic for index table management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Performance Impact of Practical Streaming</head><p>We conclude our evaluation by comparing the performance impact of STMS to idealized TMS. We employ a 12.5% update sampling probability, as it offers the best balance between bandwidth-efficiency and coverage for our workloads. Our mechanisms allow STMS to obtain on average 90% of idealized TMS coverage. Figure <ref type="figure">9</ref> (left) compares coverage of STMS and idealized TMS. For STMS, we subdivide coverage into fully-covered misses (off-chip latency is fully-hidden) and partially-covered misses (a core requested the block before the prefetch completed).</p><p>Because STMS maintains high coverage, it achieves 90% of the performance improvement possible with idealized lookup. Figure <ref type="figure">9</ref> (right) compares the performance improvement of STMS and idealized TMS. We conclude that our proposed mechanismshash-based lookup and probabilistic update-enable a bandwidth-, latency-, and storage-efficient design, meeting our stated goals of (1) matching the coverage and performance of an idealized prefetcher while stor-ing meta-data in off-chip memory, and doing so (2) without penalizing the performance of workloads that derive no benefit from temporal streaming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Address-correlating prefetchers are known to improve performance of commercial server workloads. To be effective, these prefetchers must maintain metadata that cannot fit on chip. In this work, we identified the key requirements for implementing address-correlating prefetchers with off-chip meta-data storage. To satisfy these requirements, we proposed two techniques: hash-based lookup and probabilistic sampling of meta-data updates, and applied these techniques to an address-correlating prefetcher design with split history and index tables. Hash-based lookup achieves low off-chip meta-data lookup latency. Probabilistic sampling achieves bandwidth-efficient meta-data updates. Split index and history tables amortize each meta-data lookup over multiple prefetches. Our evaluation demonstrates that these techniques yield a bandwidth-, latency-, and storage-efficient temporal memory streaming design that keeps predictor meta-data in main memory while achieving 90% of the performance potential of idealized on-chip meta-data storage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 2 :</head><label>2</label><figDesc>FIGURE 2: STMS block diagram.</figDesc><graphic url="image-2.png" coords="5,296.69,76.52,245.41,115.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 4 :</head><label>4</label><figDesc>FIGURE 4: Prefetching potential. The left graph shows the prefetch coverage achieved by an idealized temporal streaming prefetcher. The right graph shows the corresponding performance impact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 6 :</head><label>6</label><figDesc>FIGURE 6: Amortizing lookup. The left graph shows the fraction of coverage arising from streams of a particular length (commercial workloads only). The right graph shows the prefetch coverage loss of restricted prefetch depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 7 :</head><label>7</label><figDesc>FIGURE 7: Overhead traffic. The left bar in each pair shows memory traffic overheads of off-chip index lookup without probabilistic update (sampling probability = 100%). The right bar shows overheads with a 12.5% probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The left graph shows the number of correlation table entries required for a given coverage in commercial server workloads. One million correlation table entries can require up to 64MB of storage<ref type="bibr" target="#b5">[6]</ref>.The right graph shows the memory traffic overheads of existing designs based on their published results<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Erroneous Prefetches</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Meta-data Lookup</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Meta-data Update</cell></row><row><cell>10 4</cell><cell>10 5</cell><cell>10 6</cell><cell>10 7</cell></row></table><note><p>FIGURE 1: Practicality challenges.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 : Application and system model parameters.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Cores UltraSPARC III ISA</cell></row><row><cell></cell><cell>clients, 1.4 GB SGA</cell><cell></cell><cell>4 GHz 8-stage pipeline; out-of-order 4-wide dispatch / retirement</cell></row><row><cell>DB2</cell><cell>DB2 v8, 100 warehouse (10 GB), 64 clients, 2 GB buffer pool</cell><cell></cell><cell>96-entry ROB, LSQ</cell></row><row><cell></cell><cell>Decision Support (TPC-H on DB2 v8 ESE)</cell><cell cols="2">L1 D-Cache 64KB 2-way, 2-cycle load-to-use</cell></row><row><cell>Qry 2</cell><cell>Join-dominated, 480 MB buffer pool</cell><cell></cell><cell>3 ports, 32 MSHRs</cell></row><row><cell>Qry 17</cell><cell>Balanced scan-join, 480 MB buffer pool</cell><cell>Instruction Fetch</cell><cell>64KB 2-way L1 I-cache</cell></row><row><cell>Apache</cell><cell>Web Server (SPECweb99) Apache 2.0, 4K connect, FastCGI, worker threading model</cell><cell>Unit</cell><cell>16-entry pre-dispatch queue Hybrid 16K gShare/bimodal branch pred. Next line prefetcher</cell></row><row><cell>Zeus</cell><cell>Zeus v4.3, 4K connections, FastCGI</cell><cell cols="2">Shared L2 Cache 8MB 16-way, 20-cycle access latency</cell></row><row><cell></cell><cell>Scientific</cell><cell></cell><cell>16 banks, 64 MSHRs</cell></row><row><cell>em3d</cell><cell>768K nodes, degree 2, span 5, 15% remote</cell><cell cols="2">Main Memory 3 GB total memory, 45 ns access latency</cell></row><row><cell>ocean</cell><cell>258x258 grid, 9600s relaxations, 20K res., err tol 1e-07</cell><cell></cell><cell>28.4 GB/s peak bandwidth, 64-byte transfers</cell></row><row><cell>moldyn</cell><cell>19652 molecules, boxsize 17, 2.56M max interactions</cell><cell cols="2">Stride Prefetcher 32-entry buffer, max 16 distinct strides</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The left graph shows the storage requirements for the history buffer. The right graph shows the storage requirements for the index table.</figDesc><table><row><cell>% Coverage</cell><cell cols="5">0% 20% 40% 60% 80% 100% FIGURE 5: Storage requirements. 100% 0.1 1 10 100 Aggregate History Buffer Size (MB) 20% 40% 60% 80% 100% % Coverage 0%</cell><cell>0.1</cell><cell>1 Index Table Size (MB) 10</cell><cell>100</cell><cell>Web Apache Web Zeus OLTP Oracle OLTP DB2 DSS DB2 Sci em3d Sci moldyn Sci ocean</cell></row><row><cell cols="2">Cum. % Streamed Blocks</cell><cell>0% 20% 40% 60% 80% 100%</cell><cell></cell><cell>ss % Coverage Los</cell><cell cols="2">pth) (vs. unbounded dep</cell><cell>50% 0% 40% 10% 20% 30%</cell><cell>Web -Apache Web -Zeus OLTP -DB2 OLTP -Oracle DSS -DB2 Sci</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>10 Temporal Stream Length (blocks) 100 1000</cell><cell>10000</cell><cell></cell><cell>0</cell><cell>5 Fixed Prefetch Depth 10</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 : Memory-level parallelism of off-chip reads (without STMS).</head><label>2</label><figDesc>STMS instead employs a split-table meta-data organization, with separate history buffer and index table, allowing it to follow temporal streams for tens or hundreds of misses. However, the split-table approach implies a minimum of two round-trip memory accesses per lookup (one to each table). The expected coverage loss STMS incurs due to the additional memory round trip (equal to MLP reported in Table2) is less than the fragmentation losses incurred by single-table designs.</figDesc><table><row><cell cols="2">Benchmark</cell><cell>MLP</cell><cell cols="2">Benchmark</cell><cell>MLP</cell></row><row><cell>Web</cell><cell>Apache</cell><cell>1.5</cell><cell>DSS</cell><cell>DB2</cell><cell>1.6</cell></row><row><cell></cell><cell>Zeus</cell><cell>1.5</cell><cell>Sci</cell><cell>em3d</cell><cell>1.7</cell></row><row><cell>OLTP</cell><cell>DB2</cell><cell>1.3</cell><cell></cell><cell>moldyn</cell><cell>1.0</cell></row><row><cell></cell><cell>Oracle</cell><cell>1.3</cell><cell></cell><cell>ocean</cell><cell>1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Probabilistic update sampling sensitivity.</head><label></label><figDesc>The left graph shows the traffic overhead impact of probabilistic update, while the right graph shows its coverage impact. The left graph compares the coverage of idealized temporal streaming (ideal) and a practical STMS design with off-chip lookup (off-chip). The right graph compares the performance of the two designs, relative to a design with stride prefetching only.</figDesc><table><row><cell>Overhead traffic O</cell><cell>ead bytes / data byte) (overhe</cell><cell cols="3">1% FIGURE 8: 0.5 Sampling Probability 10% 1.0 1.5 2.0 0.0</cell><cell>100%</cell><cell cols="2">OLTP Oracle OLTP DB2 Web Apache Web Zeus DSS DB2 Sci em3d Sci moldyn Sc o dy Sci ocean</cell><cell>% Coverage</cell><cell>0% 20% 40% 60% 80% 100%</cell><cell>1%</cell><cell></cell><cell cols="2">10% Sampling Probability</cell><cell>100%</cell></row><row><cell>% off-chip read misses</cell><cell cols="2">0% 20% 40% 60% 80% 100%</cell><cell>Ideal Apache Off chip O</cell><cell>Ideal Partially covered Off chip Ideal Off chip Ideal O O Zeus DB2 Oracle Off chip O</cell><cell>Ideal DB2 Off chip O</cell><cell>Ideal Fully covered Off chip Ideal O em3d moldyn Off chip O</cell><cell cols="2">Ideal ocean Off chip O</cell><cell>% Speedup</cell><cell>5% 10% 15% 20% 70% 75% 80% 0% 5%</cell><cell>Apache</cell><cell cols="2">Idealized Lookup Off-chip Lookup Zeus DB2 Oracle DB2</cell><cell>em3d</cell><cell>moldyn</cell><cell>ocean</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Web</cell><cell>OLTP</cell><cell>DSS</cell><cell>Sci</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Web</cell><cell>OLTP DSS</cell><cell>Sci</cell></row></table><note><p>% FIGURE 9: Performance.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Brian Gold</rs> and the anonymous reviewers for their feedback. This work was supported by grants from <rs type="funder">Intel</rs>, two Sloan research fellowships, an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery Grant</rs>, an <rs type="funder">IBM</rs> faculty partnership award, and <rs type="funder">NSF</rs> grant <rs type="grantNumber">CCR-0509356</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9qFNvSV">
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
				<org type="funding" xml:id="_A5JjJH8">
					<idno type="grant-number">CCR-0509356</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory system characterization of commercial workloads</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kourosh</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Symposium on Computer Architecture</title>
		<meeting>of the 25th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Andreas Moshovos, and Babak Falsafi. Predictor virtualization</title>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th International Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>of the 13th International Conf. on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalized correlation-based hardware prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">P</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><surname>Reeves</surname></persName>
		</author>
		<idno>TR EE-CEG-95-1</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>School of Electrical Engineering, Cornell University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient representations and abstractions for quantifying and exploiting data reference locality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGPLAN &apos;01 Conf. on Programming Language Design and Implementation</title>
		<meeting>of the SIGPLAN &apos;01 Conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic hot data stream prefetching for general-purpose programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGPLAN 2002 Conf. on Programming language design and implementation</title>
		<meeting>of the ACM SIGPLAN 2002 Conf. on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-cost epoch-based correlation prefetching for commercial applications</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th IEEE/ ACM International Symposium on Microarchitecture</title>
		<meeting>of the 40th IEEE/ ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st International Symposium on Computer Architecture</title>
		<meeting>of the 31st International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Last-touch correlated data streaming</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 41st IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>of the 41st IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Database servers on Chip Multiprocessors: Limitations and Opportunities</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ippokratis</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naju</forename><surname>Mancheril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Biennial Conf. on Innovative Data Systems Research</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Timekeeping in the memory system: predicting and optimizing memory behavior</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th International Symposium on Computer Architecture</title>
		<meeting>of the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tcp: Tag correlating prefetchers</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th IEEE Symposium on High-Performance Computer Architecture</title>
		<meeting>of the 9th IEEE Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the design space of future cmps</title>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 International Conf. on Parallel Architectures and Compilation Techniques</title>
		<meeting>of the 2001 International Conf. on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anastassia Ailamaki, and Babak Falsafi. To share or not to share?</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ippokratis</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naju</forename><surname>Mancheril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kivanc</forename><surname>Sabirli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Very Large Data Bases Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th International Symposium on Computer Architecture</title>
		<meeting>of the 24th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Symposium on Computer Architecture</title>
		<meeting>of the 17th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dead-block prediction &amp; dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">An-Chow</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th International Symposium on Computer Architecture</title>
		<meeting>of the 28th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using cohort scheduling to enhance server performance</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Larus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Technical Conference</title>
		<meeting>of the USENIX Technical Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of database workload performance on simultaneous multithreaded processors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kourosh</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><forename type="middle">S</forename><surname>Parekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Symposium on Computer Architecture</title>
		<meeting>of the 25th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Tenth IEEE Symposium on High-Performance Computer Architecture</title>
		<meeting>of the Tenth IEEE Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predictor-directed stream buffers</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suleyman</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd IEEE/ ACM International Symposium on Microarchitecture</title>
		<meeting>of the 33rd IEEE/ ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MI-CRO</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using a userlevel memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th International Symposium on Computer Architecture</title>
		<meeting>of the 29th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd International Symposium on Computer Architecture</title>
		<meeting>of the 33rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The memory performance of DSS commercial workloads in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep-L</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Third IEEE Symposium on High-Performance Computer Architecture</title>
		<meeting>of the Third IEEE Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal streams in commercial server applications</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal streaming of shared memory</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 32nd International Symposium on Computer Architecture</title>
		<meeting>of the 32nd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SimFlex: statistical sampling of computer system simulation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hardware-only stream prefetching and dynamic access ordering</title>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Conf. on Supercomputing</title>
		<meeting>of the 14th International Conf. on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
