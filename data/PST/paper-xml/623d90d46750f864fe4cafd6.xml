<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NEURAL STRUCTURED PREDICTION FOR INDUCTIVE NODE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-15">15 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiyu</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">HEC</orgName>
								<address>
									<settlement>Montr?al</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Canadian Institute for Advanced Research (CIFAR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NEURAL STRUCTURED PREDICTION FOR INDUCTIVE NODE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-15">15 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.07524v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a nearoptimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data are ubiquitous in the real world, covering a variety of applications. This paper studies node classification, a fundamental problem in the machine learning community. Most existing efforts focus on the transductive setting <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b41">Veli?kovi? et al., 2018)</ref>, i.e., using a small set of labeled nodes in a graph to classify the rest of nodes. In this paper, we study node classification in the inductive setting <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, which is receiving growing interest. Given some training graphs with all nodes labeled, we aim to classify nodes in unlabeled test graphs. This problem has been recently studied with graph neural networks (GNNs) <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b13">Gilmer et al., 2017;</ref><ref type="bibr" target="#b41">Veli?kovi? et al., 2018)</ref>. GNNs infer the marginal label distribution of each node by learning useful node representations based on node features and edges. Once a GNN is learned on training graphs, it can be further applied to test graphs to infer node labels. Owing to the high capacity of nonlinear neural architectures, GNNs achieve impressive results on many datasets. However, one limitation of GNNs is that they ignore the joint dependency of node labels, and therefore node labels are predicted separately without modeling structured output. Indeed, modeling structured output has been widely explored by the literature of structured prediction <ref type="bibr" target="#b1">(BakIr et al., 2007)</ref>. Structured prediction methods predict node labels collectively, so the label prediction of each node can be improved according to the predicted labels of neighboring nodes. One representative approach is the conditional random field (CRF) <ref type="bibr" target="#b19">(Lafferty et al., 2001)</ref>. A CRF models the joint distribution of node labels with Markov networks, and thus training CRFs becomes a learning task in graphical models, while predicting node labels corresponds to an inference task. Typically, the potential functions in CRFs are parameterized as log-linear functions, which suffer from low model capacities. One remedy for this is to define potential functions with GNNs <ref type="bibr" target="#b25">(Ma et al., 2018;</ref><ref type="bibr" target="#b30">Qu et al., 2019)</ref>. However, most of the effective methods for learning CRFs involve a There are also some recent works trying to combine GNNs and CRFs. Some works use GNNs to solve inference problems in graphical models <ref type="bibr" target="#b8">(Dai et al., 2016;</ref><ref type="bibr" target="#b34">Satorras et al., 2019;</ref><ref type="bibr" target="#b50">Zhang et al., 2020;</ref><ref type="bibr">Chen et al., 2020b;</ref><ref type="bibr" target="#b33">Satorras &amp; Welling, 2020)</ref>. In contrast, our approach uses GNNs to parameterize the potential functions in CRFs, which is in a similar vein to <ref type="bibr" target="#b25">Ma et al. (2018)</ref>; <ref type="bibr" target="#b30">Qu et al. (2019)</ref>; <ref type="bibr" target="#b23">Ma et al. (2019;</ref><ref type="bibr">2021)</ref>; <ref type="bibr" target="#b44">Wang et al. (2021)</ref>. Among them, <ref type="bibr" target="#b25">Ma et al. (2018)</ref> and <ref type="bibr" target="#b30">Qu et al. (2019)</ref> optimize the pseudolikelihood <ref type="bibr" target="#b2">(Besag, 1975)</ref> for model learning, and <ref type="bibr" target="#b44">Wang et al. (2021)</ref> optimizes a cross-entropy loss on each single node, which can yield poor approximation of the true joint likelihood <ref type="bibr" target="#b18">(Koller &amp; Friedman, 2009;</ref><ref type="bibr" target="#b38">Sutton &amp; McCallum, 2012)</ref>. Our approach instead solves a proxy problem, which yields a near-optimal solution to the original problem of maximizing likelihood, and thus gets superior results. For <ref type="bibr" target="#b23">Ma et al. (2019)</ref> and <ref type="bibr" target="#b24">Ma et al. (2021)</ref>, they focus on transductive node classification and continuous labels respectively, which are different from our work.</p><p>Lastly, learning CRFs has also been widely studied. Some works solve a maximin game as a surrogate for learning <ref type="bibr" target="#b38">(Sutton &amp; McCallum, 2012)</ref> and some others maximize a lower bound of the likelihood function <ref type="bibr" target="#b37">(Sutton &amp; McCallum, 2009)</ref>. However, these maximin games are often hard to optimize and the lower bounds are often loose. Different from them, we follow <ref type="bibr" target="#b43">Wainwright et al. (2003)</ref> and build an approximate optimization problem as a proxy, which is easier to solve and yields better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>This paper focuses on inductive node classification <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, a fundamental problem in both graph machine learning and structured prediction. We employ a probabilistic formalization for the problem with some labeled training graphs and unlabeled test graphs. Each training graph is given as (y *</p><p>V , x V , E), where x V and y * V are features and labels of a set of nodes V , and E is a set of edges. For each test graph (x ? , ?), only features x ? and edges ? are given. Then we aim to solve:</p><p>? Learning. On training graphs, learn a probabilistic model to approximate p(y V |x V , E).</p><p>? Inference. For each test graph, infer node labels y * ? according to the distribution p(y ? |x ? , ?). The problem has been extensively studied in both graph machine learning and structured prediction fields, and representative methods are GNNs and CRFs respectively. Next, we introduce the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GRAPH NEURAL NETWORKS</head><p>For inductive node classification, graph neural networks (GNNs) learn node representations to predict marginal label distributions of nodes. GNNs assume all node labels are independent conditioned on node features and edges, so the joint label distribution is factorized into a set of marginals as below:</p><formula xml:id="formula_0">p ? (y V |x V , E) = s?V p ? (y s |x V , E).</formula><p>(1)</p><p>Each marginal distribution p ? (y s |x V , E) is modeled as a categorical distribution over label candidates, and the label probabilities are computed by applying a linear softmax classifier to the representation of node s. In general, node representations are learned via the message passing mechanism <ref type="bibr" target="#b13">(Gilmer et al., 2017)</ref>, which brings high capacity to GNNs. Also, owing to the factorization in Eq. ( <ref type="formula">1</ref>), learning and inference can be easily solved in GNNs, where we simply need to compute loss and make prediction on each node separately. However, GNNs approximate only the marginal label distributions of nodes on training graphs, which may generalize badly and result in poor approximation of node marginal label distributions on test graphs. Also, the labels of different nodes are separately predicted according to their own marginal label distributions, yet the joint dependency of node labels is ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONDITIONAL RANDOM FIELDS</head><p>For inductive node classification, conditional random fields (CRFs) build graphical models for node classification. A popular model is the pair-wise CRF, which formalizes the joint label distribution as:</p><formula xml:id="formula_1">p ? (y V |x V , E) = 1 Z ? (x V , E) exp{ s?V ? s (y s , x V , E) + (s,t)?E ? st (y s , y t , x V , E)}<label>(2)</label></formula><p>where Z ? (x V , E) is the partition function. ? s (y s , x V , E) and ? st (y s , y t , x V , E) are scalar scores contributed by each node s and each edge (s, t). In practice, these ?-functions can be either defined as simple linear functions or complicated GNNs. To make the notation concise, we will omit x V and E in the ?-functions, e.g., simplifying ? s (y s , x V , E) as ? s (y s ). With these ?-functions, CRFs are able to model the joint dependency of node labels and therefore achieve structured prediction.</p><p>However, learning CRFs to maximize likelihood p ? (y * V |x V , E) on training graphs is nontrivial in general, as the partition function Z ? (x V , E) is typically intractable in graphs with loops. Thus, a major line of research instead optimizes a maximin game equivalent to likelihood maximization <ref type="bibr" target="#b42">(Wainwright &amp; Jordan, 2008)</ref>. The maximin game for each training graph (y * V , x V , E) is formalized as follows:</p><formula xml:id="formula_2">max ? log p ? (y * V |x V , E) = max ? min q L(?, q), with L(?, q) = s?V {? s (y * s ) -E qs(ys) [? s (y s )]} + (s,t)?E {? st (y * s , y * t ) -E qst(ys,yt) [? st (y s , y t )]} -H[q(y V )].<label>(3)</label></formula><p>Here, q(y V ) is a variational distribution on node labels, q s (y s ) and q st (y s , y t ) are its marginal distributions on nodes and edges. H[q(y V )] := -E q(y V ) [log q(y V )] is the entropy of q(y V ). Given the maximin game, q and ? can be alternatively optimized via coordinate descent <ref type="bibr" target="#b38">(Sutton &amp; McCallum, 2012)</ref>.</p><p>In each iteration, we first update the node and edge marginals {q s (y s )} Figure <ref type="figure">1</ref>: Framework overview of the SPN. Our approach formulates a proxy optimization problem for learning, which is much easier to solve. Given a graph, a node GNN and an edge GNN are used to predict the pseudomarginal label distributions on each node and each edge respectively. Then these pseudomarginals serve as building blocks to construct a near-optimal joint label distribution.</p><p>towards those defined by p ? . This can be done by MCMC, but the time cost is high, so approximate inference is often used, such as loopy belief propagation <ref type="bibr" target="#b27">(Murphy et al., 1999)</ref>. After q is optimized, we further update ?-functions with the node and edge marginals defined by q via gradient descent.</p><p>The optimal ?-functions are characterized by the following moment-matching conditions:</p><formula xml:id="formula_3">p ? (y s |x V , E) = I y * s {y s } ?s ? V, p ? (y s , y t |x V , E) = I (y * s ,y * t ) {(y s , y t )} ?(s, t) ? E,<label>(4</label></formula><p>) where I a {b} is an indicator function whose value is 1 if a = b and 0 otherwise. See Sec. A and Sec. B in appendix for detailed derivation of the maximin game as well as the moment-matching conditions.</p><p>Once the ?-functions are learned, they can be further applied to each test graph (x ? , ?) to predict the joint label distribution as p ? (y ? |x ? , ?). Then the best label assignment y * ? can be inferred by using approximate inference algorithms, such as loopy belief propagation <ref type="bibr" target="#b27">(Murphy et al., 1999)</ref>.</p><p>The major challenge of CRFs lies in learning. On the one hand, learning relies on inference, meaning that we have to update {q s (y s )} s?V , {q st (y s , y t )} (s,t)?E to approximate the node and edge marginals of p ? at each step, which can be expensive. On the other hand, as learning involves a maximin game and the optimal q of the inner minimization problem in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is intractable, we can only maximize an upper bound of the likelihood function for ?, making learning unstable. The problem becomes even more severe when ? is parameterized by highly nonlinear neural models, e.g. GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL</head><p>In this section, we introduce our proposed approach Structured Proxy Network (SPN). The general idea of SPN is to combine GNNs and CRFs by parameterizing potential functions in CRFs with GNNs, and therefore SPN enjoys high capacity and can model the joint dependency of node labels.</p><p>However, as elaborated in Sec. 3.2, learning such a model on training graphs is challenging due to the maximin game in optimization. Inspired by the connection between the joint and marginal distributions of CRFs, we instead construct a new optimization problem, which serves as a proxy for model learning. Compared with the original maximin game, the proxy problem is much easier to solve, where we can simply train two GNNs to approximate the marginal label distributions on nodes and edges, and further combine these pseudomarginals (defined in Prop. 1) into a near-optimal joint label distribution. This joint label distribution can be further refined by optimizing the maximin game, although it is optional and often unnecessary, as this distribution is often close enough to the optimal one. With this proxy problem for model learning, learning becomes more stable and efficient.</p><p>Afterwards, the learned model is used to predict the joint label distribution on test graphs. Then we run loopy belief propagation to infer node labels. Now, we introduce the details of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LEARNING</head><p>The learning task aims at training ? to maximize the log-likelihood function log p ? (y * V |x V , E) for each training graph (y * V , x V , E), which is highly challenging. Therefore, instead of directly optimizing this goal, we solve an approximate version of the problem as a proxy, which is training a node GNN and an edge GNN to maximize the log-likelihood of observed labels on nodes and edges.</p><p>The Proxy Problem. The proxy problem is inspired by <ref type="bibr" target="#b42">Wainwright &amp; Jordan (2008)</ref>, which points out that the marginal label distributions on nodes and edges defined by a Markov network have inherent connections with the joint distribution. This connection is stated in the proposition below.</p><p>Proposition 1 Consider a set of nonzero pseudomarginals {? s (y s )} s?V and {? st (y s , y t )} (st)?E which satisfy ys ? st (y s , y t ) = ? t (y t ) and yt ? st (y s , y t ) = ? s (y s ) for all (s, t) ? E.</p><p>If we parameterize the ?-functions of p ? in Eq. (2) in the following way:</p><formula xml:id="formula_4">? s (y s ) = log ? s (y s ) ?s ? V, ? st (y s , y t ) = log ? st (y s , y t ) ? s (y s )? t (y t ) ?(s, t) ? E,<label>(5)</label></formula><p>then {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E are specified by a fixed point of the sum-product loopy belief propagation algorithm when applied to the joint distribution p ? , which implies that:</p><formula xml:id="formula_5">? s (y s ) ? p ? (y s ) ?s ? V, ? st (y s , y t ) ? p ? (y s , y t ) ?(s, t) ? E. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>The  <ref type="formula" target="#formula_3">4</ref>) for the optimal ?-functions are roughly satisfied. This implies the joint distribution p ? (y V |x V , E) derived in this way is a near-optimal one. With the observation, rather than directly using GNNs to parameterize the ?-functions, we use a node GNN and an edge GNN to parameterize the pseudomarginals {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E . For the pseudomarginal ? s (y s ) on node s, we apply the node GNN to node features x V and edges E, yielding a representation u s for node s. Then we apply a softmax classifier to u s to compute ? s (y s ):</p><formula xml:id="formula_7">{u s } u?V = GNN node (x V , E), ? s (y s ) = softmax(f (u s ))[y s ],<label>(7)</label></formula><p>where f maps a node representation to a |Y|-dimensional logit and Y is the node label set. Similarly, we apply the edge GNN to compute a representation v s for each node s, and model ? st (y s , y t ) as:</p><formula xml:id="formula_8">{v s } s?V = GNN edge (x V , E) ? st (y s , y t ) = softmax(g(v s , v t ))[y s , y t ],<label>(8)</label></formula><p>where g is a function mapping a pair of representations to a (|Y| ? |Y|)-dimensional logit.</p><p>Given the parameterization, we construct the following problem as a proxy for learning ?-functions:</p><formula xml:id="formula_9">min ?,? s?V d I y * s {y s }, ? s (y s ) + (s,t)?E d I (y * s ,y * t ) {(y s , y t )}, ? st (y s , y t ) , subject to ? s = log ? s (y s ), ? st (y s , y t ) = log ? st (y s , y t ) ? s (y s )? t (y t ) ,</formula><p>and</p><formula xml:id="formula_10">ys ? st (y s , y t ) = ? t (y t ), yt ? st (y s , y t ) = ? s (y s ),<label>(9)</label></formula><p>for all nodes and edges, where d can be any divergence measure between two distributions. By solving the above problem, {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E will be valid pseudomarginals which can well approximate the true labels, i.e., ? s (y s ) ? I y * s {y s } and ? st (y s , y t ) ? I (y * s ,y * t ) {(y s , y t )}. Then according to the constraint in the second line of Eq. ( <ref type="formula" target="#formula_10">9</ref>), ?-functions are formed in a way to enable ? s (y s ) ? p ? (y s ) and ? st (y s , y t ) ? p ? (y s , y t ) as stated in the Prop. 1. Combining these two sets of formula results in p ? (y s ) ? I y * s {y s } and p ? (y s , y t ) ? I y * s {y s }. We see that the moment-matching conditions in Eq. ( <ref type="formula" target="#formula_3">4</ref>) for the optimal joint label distribution are roughly achieved, implying that the derived joint distribution p ? (y V |x V , E) is a near-optimal solution to the original learning problem.</p><p>One good property of the proxy problem is that it can be solved easily. The last consistency constraint (i.e.</p><p>ys ? st (y s , y t ) = ? t (y t ) and yt ? st (y s , y t ) = ? s (y s )) can be ignored during optimization, since by optimizing the objective function, the optimal pseudomarginals ? should well approximate the observed node and edge marginals, i.e., ? s (y s ) ? I y * s {y s } and ? st (y s , y t ) ? I (y * s ,y * t ) {(y s , y t )}, and hence ? will almost naturally satisfy the consistency constraint. We also tried some constrained optimization methods to handle the consistency constraint, but they yield no improvement. See Sec. D of appendix for more details. Thus, we can simply train the pseudomarginals parameterized by GNNs to approximate the true node and edge labels on training graphs, i.e., minimizing d(I y * s {y s }, ? s (y s )) and d(I (y * s ,y * t ) {(y s , y t )}, ? st (y s , y t )). Then we build ?-functions as in Eq. ( <ref type="formula" target="#formula_4">5</ref>) to obtain a near-optimal joint distribution. In practice, we choose d to be the KL divergence, yielding an objective for ? as:</p><formula xml:id="formula_11">max ? s?V log ? s (y * s ) + (s,t)?E log ? st (y * s , y * t ). (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>This objective function is very intuitive, where we simply try to optimize the node GNN and edge GNN to maximize the log-likelihood function of the observed labels on nodes and edges.</p><p>Refinement. By solving the proxy problem, we can obtain a near-optimal joint distribution. In practice, we observe that when we have a large amount of training data, further refining this joint distribution by solving the maximin game in Eq. ( <ref type="formula" target="#formula_2">3</ref>) for a few iterations can lead to further improvement. Formally, each iteration of refinement has two steps. In the first step, we run sum-product loopy belief propagation <ref type="bibr" target="#b27">(Murphy et al., 1999)</ref>, which yields a collection of node and edge marginals (i.e., {q s (y s )} s?V and {q st (y s , y t )} (s,t)?E ) as approximation to the marginals defined by p ? . In the second step, we update the ?-functions parameterized by the node and edge GNNs to maximize:</p><formula xml:id="formula_13">s?V ? s (y * s ) -E qs(ys) [? s (y s )] + (s,t)?E ? st (y * s , y * t ) -E qst(ys,yt) [? st (y s , y t )] .<label>(11)</label></formula><p>Intuitively, we treat the true label y * s and (y * s , y * t ) of each node and edge as positive examples, and encourage the ?-functions to raise up their scores. Meanwhile, those labels sampled from q s (y s ) and q st (y s , y t ) act as negative examples, and the ?-functions are updated to decrease their scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INFERENCE</head><p>After learning, we apply the node and edge GNNs to each test graph (x ? , ?) to compute the ?functions, which are integrated into an approximate joint label distribution p ? (y ? |x ? , ?). Then we use this distribution to infer the best label y * s for each node s ? ? , where two settings are considered. Node-level Accuracy. Typically, we care about the node-level accuracy, i.e., how likely we can correctly classify a node in test graphs. Intuitively, the best label y * s for each test node s ? ? should be predicted as y * s = arg max ys p ? (y s|x ? , ?), where p ? (y s|x ? , ?) is the marginal label distribution of node s induced by the joint p ? (y ? |x ? , ?). In practice, the exact marginal is intractable, so we apply loopy belief propagation <ref type="bibr" target="#b27">(Murphy et al., 1999)</ref> for approximate inference. For each edge (s, t) in test graphs, we introduce a message function m t?s (y s) and iteratively update all messages as:</p><formula xml:id="formula_14">m t?s (y s) ? yt {exp(? t(y t) + ? st (y s, y t)) s ?N ( t)\s m s ? t(y t)},<label>(12)</label></formula><p>where N (s) denotes the set of neighboring nodes for node s. Once the above process converges or after sufficient iterations, the label of each node s can be inferred in the following way:</p><formula xml:id="formula_15">y * s = arg max ys [exp(? s(y s)) t?N (s) m t?s (y s)].<label>(13)</label></formula><p>Graph-level Accuracy. In some other cases, we might care about the graph-level accuracy, i.e., how likely we can correctly classify all nodes in a given test graph. In this case, the best prediction of node labels is given by y * ? = arg max y ? p(y ? |x ? , ?). This problem can be approximately solved by the max-product variant of loopy belief propagation, which simply replaces the sum over y t in Eq. ( <ref type="formula" target="#formula_14">12</ref>) with max <ref type="bibr" target="#b45">(Weiss &amp; Freeman, 2001)</ref>. Afterwards, the best node label can be still decoded via Eq. (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCUSSION</head><p>In practice, many structured prediction problems can be viewed as special cases of inductive node classification, where the graphs between nodes have some special structures. For example in sequence labeling tasks (e.g., named entity recognition), the graphs between nodes have sequential structures. Thus, SPN can be applied to these tasks as well. In order for better results, one might replace GNNs with other neural models which are specifically designed for the studied task to better estimate the pseudomarginals. For example in sequence labeling tasks, recurrent neural networks can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASETS</head><p>We consider datasets in two settings, which focus on node-level and graph-level accuracy respectively.</p><p>Node-level Accuracy. The node-level accuracy measures how likely a model can predict the correct label of a node in test graphs. We use the PPI dataset <ref type="bibr" target="#b51">(Zitnik &amp; Leskovec, 2017;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017)</ref>, which has 20 training graphs. To make the dataset more challenging, we also try using only the first 1/2/10 training graphs, yielding another three datasets PPI-1, PPI-2, and PPI-10. Besides, we also build a DBLP dataset from the citation network in <ref type="bibr" target="#b39">Tang et al. (2008)</ref>. Papers from eight conferences are treated as nodes, and we split them into three categories for classification according to conference domains<ref type="foot" target="#foot_0">2</ref> . For each paper, we compute the mean GloVe embedding <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref> of words in the title and abstract as node features. The training/validation/test graph is formed as the citation graph of papers published before 1999, from 2000 to 2009, after 2010 respectively.</p><p>Graph-level Accuracy. The graph-level accuracy measures how likely a model can correctly classify all the nodes for a given test graph. We construct three datasets from the Cora, Citeseer, and Pubmed datasets used for transductive node classification <ref type="bibr" target="#b47">(Yang et al., 2016)</ref>. Each raw dataset has a single graph. For each training/validation/test node of the raw dataset, we treat its ego network<ref type="foot" target="#foot_1">3</ref> as a training/validation/test graph. We denote the datasets as Cora*, Citeseer*, Pubmed*.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">COMPARED ALGORITHMS</head><p>Graph Neural Networks. For GNNs, we choose a few well-known model architectures for comparison, including GCN (Kipf &amp; Welling, 2017), GraphSage <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b41">(Veli?kovi? et al., 2018)</ref>, Graph U-Net <ref type="bibr" target="#b11">(Gao &amp; Ji, 2019)</ref> and GCNII <ref type="bibr">(Chen et al., 2020a)</ref>.</p><p>Conditional Random Fields. For CRFs, we consider three variants. (1) CRF-linear. This variant uses linear ?-functions in Eq. ( <ref type="formula" target="#formula_1">2</ref>), which takes the features on nodes and edges for computation. (2) CRF-GNN. This variant parameterizes the ?-functions as ? s (y s ) = f (u s ) and ? st (y s , y t ) = g(v s , v t ), with f and g defined in Eq. ( <ref type="formula" target="#formula_7">7</ref>) and Eq. ( <ref type="formula" target="#formula_8">8</ref>), where the node representations are generated by different GNN architectures (e.g., CRF-GAT). We train these models via the maximin game as in Eq. ( <ref type="formula" target="#formula_2">3</ref>) with sum-product loopy belief propagation. (3) GMNN. We also consider GMNN <ref type="bibr" target="#b30">(Qu et al., 2019)</ref>, an approach combining GNNs and CRFs, which optimizes the pseudolikelihood function for learning.</p><p>Our Approach. For SPNs, we try different GNN architectures for defining the node and edge GNNs (e.g., SPN-GAT). By default, we only solve the proxy problem without performing refinement. We systematically compare the results with and without refinement in part 2 of Sec. 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EVALUATION METRICS</head><p>On Cora*, Citeseer*, and Pubmed*, we report the percentage of test graphs where all the nodes are correctly classified (i.e., graph-level accuracy). On DBLP and PPI, we report accuracy and micro-F1 based on the percentage of test nodes which are correctly classified (i.e., node-level accuracy). For Cora*, Citeseer*, and Pubmed*, we run each compared method with 10 different seeds to report the mean accuracy and the standard deviation. For DBLP and PPI, we run each method with 5 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">EXPERIMENTAL SETUP</head><p>For GNNs, by default we use the same architectures (e.g., number of neurons, number of layers) as used in the original papers. Adam <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> is used for training. For the edge GNN in Eq. ( <ref type="formula" target="#formula_8">8</ref>), we add a hyperparameter ? to control the annealing temperature of the logit g(v s , v t ) before the softmax function during belief propagation. Empirically, we find that max-product belief propagation works better than the sum-product variant in most cases, so we use the max-product version by default. By default, we do not run refinement when training SPNs. See Sec. F for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Comparison with other methods. The main results in the two settings are presented in Tab. 1 and Tab. 2. Compared against different GNN models, our approach achieves consistent improvement (the relative underperformance of SPN-GCN and SPN-SAGE is related to the capacity of the backbone GNNs and is explained in Sec. G.1) by using these GNNs as backbone networks for approximating marginal label distributions on nodes and edges, which demonstrates SPNs are able to model the structured output of node labels by combining with CRFs, and thus achieve better results.</p><p>Besides, SPNs also achieve superior results to CRF-GNNs which are trained by directly solving the maximin game in Eq. ( <ref type="formula" target="#formula_2">3</ref>), as well as GMNN which optimizes the pseudolikelihood function. This observation proves the advantage of our proposed proxy optimization problem for learning CRFs. 2. Effect of refinement. By solving the proxy optimization problem in Eq. ( <ref type="formula" target="#formula_10">9</ref>), we can obtain a near-optimal joint label distribution on training graphs, based on which we may optionally refine the distribution with the maximin game in Eq. ( <ref type="formula" target="#formula_2">3</ref>). Next, we study the effect of refinement, and we present the results in Tab. 4. By only solving the proxy problem, our approach already achieves impressive results, showing that the proxy problem can well approximate the original learning problem. Only on datasets with sufficient labeled data (e.g., PPI-10, PPI), refinement leads to some improvement.</p><p>3. Model architecture. SPN uses a node GNN and an edge GNN for computing node and edge marginals independently. In practice, we can also use a shared GNN for both node and edge marginals.</p><p>We show results of this variant in Tab. 6, where it also achieves significant improvement over GNNs.</p><p>4. Efficiency comparison. We have seen SPNs achieve better classification results than GNNs and CRFs. Next, we further compare their efficiency by showing the run time on DBLP and PPI. For PPI, which has 121 labels, we only report the training times on a single label. We use GAT as the backbone network for CRFs and SPNs. GAT and CRF are trained for 1000 epochs to ensure convergence. For the SPN, we train the node GNN and edge GNN for node/edge classification as in Eq. ( <ref type="formula" target="#formula_11">10</ref>) with 1000 epochs. The run times are presented in Tab. 3. SPNs take twice as long for training than GAT, as a SPN needs to train a node GNN and an edge GNN. Compared with CRFs, we can see that SPNs are much more efficient, because the proxy optimization problem in SPNs is much easier to solve.   5. Comparison of learning methods. Next, we investigate different methods for learning SPNs, including directly solving the maximin game, optimizing pseudolikelihood, and solving our proposed proxy problem. We show the results for optimizing SPN-GAT in Tab. 5. We see solving maximin game yields poor results due to unstable training. Although the pseudolikelihood method performs much better, the result is still unsatisfactory as it is not a good approximation of the true likelihood. By solving our proposed proxy problem, SPN achieves the best result, which proves its effectiveness.</p><p>6. Convergence analysis. To better illustrate the advantage of the proxy problem for learning CRFs, we look into the training curves of SPNs, SPNs w/o proxy, and CRFs when optimizing the maximin game in Eq. (3). For SPNs, we optimize the node and edge GNNs on the proxy optimization problem in Eq. ( <ref type="formula" target="#formula_10">9</ref>) before doing refinement with the maximin game, while for SPNs w/o proxy we directly perform refinement with the maximin game without solving the proxy problem. We show the results in Fig. <ref type="figure" target="#fig_1">2</ref>. CRFs and SPNs w/o proxy suffer from high variance and low accuracy. In contrast, owing to the near-optimal joint distribution found by solving the proxy problem, SPNs get much higher accuracy with lower variance even without refinement (see initial results of SPNs at epoch 0). Also, the refinement process quickly converges after only a few epochs, showing good efficiency of SPNs.</p><p>7. Case study. To intuitively see how SPNs outperform GNNs, we conduct some case studies on Cora*. We use GAT as backbone networks, and show the prediction made by the GAT (the node GNN), the edge GNN, and SPN in Fig. <ref type="figure" target="#fig_2">3</ref>. In all three cases shown in the figure, GAT (left column) makes inconsistent predictions on linked nodes, as it fails to model the structured output. The edge GNN (middle column) also makes a mistake in the bottom case. Finally, by combining GAT and edge GNN with a CRF, the SPN (right column) is able to predict the correct labels for all nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper studied inductive node classification, and we proposed SPN to combine GNNs and CRFs. Inspired by the connection of joint and marginal distributions defined by Markov networks, we designed a proxy problem for efficient model learning. In the future, we plan to explore more advanced GNNs to model the pseudomarginals on edges, which are key to improving node classification results in SPNs. In addition, SPNs model joint dependency of node labels by defining potential functions on nodes and edges, and we also plan to further explore high-order local structures, e.g., triangles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DERIVATION OF THE MAXIMIN GAME</head><p>As discussed in the Sec. 3, optimizing the joint label distribution p ? to maximize the log-likelihood log p ? (y * V |x V , E) on a training graph (y * V , x V , E) is equivalent to solving a maximin game. In this section, we provide the detailed derivation.</p><p>Let ? ? (y V , x V , E) be the potential function as below:</p><formula xml:id="formula_16">? ? (y V , x V , E) = exp ? ? ? s?V ? s (y s , x V , E) + (s,t)?E ? st (y s , y t , x V , E) ? ? ? . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>For each training graph (y * V , x V , E), we aim at maximizing the following log-likelihood function:</p><formula xml:id="formula_18">log p ? (y * V |x V , E) = log 1 Z ? (x V , E) ? ? (y * V , x V , E) = log ? ? (y * V , x V , E) -log Z ? (x V , E) = log ? ? (y * V , x V , E) -log y V ? ? (y V , x V , E).</formula><p>(15)</p><p>However, the term log y V ? ? (y V , x V , E) is computationally intractable, as we need to sum over all the possible y V . To solve the problem, we introduce a variational joint distribution q(y V ) defined on all node labels y V , and use the Jensen's inequality to derive an estimation of the term log y V ? ? (y V , x V , E) as follows:</p><formula xml:id="formula_19">log y V ? ? (y V , x V , E) = log E q(y V ) ? ? (y V , x V , E) q(y V ) ? E q(y V ) log ? ? (y V , x V , E) q(y V ) = E q(y V ) [log ? ? (y V , x V , E)] -E q(y V ) [log q(y V )]. (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>The equation holds if and only if q(y V ) = p ? (y V |x V , E), and hence:</p><formula xml:id="formula_21">log y V ? ? (y V , x V , E) = max q(y V ) E q(y V ) [log ? ? (y V , x V , E)] -E q(y V ) [log q(y V )] . (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>By taking the above result into Eq. ( <ref type="formula">15</ref>), we obtain:</p><formula xml:id="formula_23">log p ? (y * V |x V , E) = log ? ? (y * V , x V , E) -log y V ? ? (y V , x V , E) = min q(y V ) log ? ? (y * V , x V , E) -E q(y V ) [log ? ? (y V , x V , E)] + E q(y V ) [log q(y V )] .<label>(18)</label></formula><p>As</p><formula xml:id="formula_24">? ? (y V , x V , E) = exp{ s?V ? s (y s , x V , E) + (s,t)?E ? st (y s , y t , x V , E)}, we have: log p ? (y * V |x V , E) = min q L(?, q),<label>(19)</label></formula><p>with:</p><formula xml:id="formula_25">L(?, q) = -H[q(y V )] + (s,t)?E {? st (y * s , y * t ) -E qst(ys,yt) [? st (y s , y t )]} + s?V {? s (y * s ) -E qs(ys) [? s (y s )]}. (20)</formula><p>Therefore, optimizing ? to maximize the log-likelihood function is equivalent to solving the following maximin game:</p><formula xml:id="formula_26">max ? log p ? (y * V |x V , E) = max ? min q L(?, q).<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DERIVATION OF THE MOMENT-MATCHING CONDITIONS</head><p>In the CRF model defined in the preliminary section, the parameter ? consists of the output values of all ?-functions. In other words, ? = {? s (y s )} ys?Y,s?V ? {? st (y s , y t )} ys?Y,yt?Y,s?V , where Y is the set of all the possible node labels.</p><p>By definition, p ? (y V |x V , E) belongs to the exponential family. According to properties of exponential family distributions, log p ? (y * V |x V , E) is strictly concave with respect to ?. Therefore, the optimal ? is unique, which is characterized by the condition of ? ?? log p ? (y</p><formula xml:id="formula_27">* V |x V , E) = 0. Formally, ? ?? log p ? (y * V |x V , E) can be computed as below: ? ?? s (? s ) log p ? (y * V |x V , E) = ? ?? log ? ? (y * V , x V , E) - ? ?? log Z ? (x V , E). (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>For ? ?? log Z ? (x V , E), we have:</p><formula xml:id="formula_29">? ?? log Z ? (x V , E) = ? ?? log y V ? ? (y V , x V , E) = y V ? ?? ? ? (y V , x V , E) y V ? ? (y V , x V , E) = y V ? ? (y V , x V , E) ? ?? log ? ? (y V , x V , E) y V ? ? (y V , x V , E) = y V ? ? (y V , x V , E) y V ? ? (y V , x V , E) ? ?? log ? ? (y V , x V , E) = y V ? ? (y V , x V , E) Z ? ? ?? log ? ? (y V , x V , E) = E p ? (y V |x V ,E) ? ?? log ? ? (y V , x V , E) . (<label>23</label></formula><formula xml:id="formula_30">)</formula><p>By combining the above two equations, we have:</p><formula xml:id="formula_31">? ?? log p ? (y * V |x V , E) = ? ?? log ? ? (y * V , x V , E) -E p ? (y V |x V ,E) ? ?? log ? ? (y V , x V , E) . (<label>24</label></formula><formula xml:id="formula_32">)</formula><p>The potential function ? ? above is defined as</p><formula xml:id="formula_33">? ? (y V , x V , E) = exp{ s?V ? s (y s , x V , E) + (s,t)?E ? st (y s , y t , x V , E)}.</formula><p>If we consider each specific scalar ? s (? s ), and taking the derivative with respect to the scalar to 0, we obtain:</p><formula xml:id="formula_34">0 = ? ?? s (? s ) log p ? (y * V |x V , E) = ? ?? s (? s ) log ? ? (y * V , x V , E) -E p ? (y V |x V ,E) ? ?? s (? s ) log ? ? (y V , x V , E) = I y * s {? s } ? ?? s (? s ) ? s (? s ) - y V p ? (y V |x V , E) I y * s {? s } ? ?? s (? s ) ? s (? s ) = I y * s {? s } ? ?? s (? s ) ? s (? s ) -p ? (? s |x V , E) ? ?? s (? s ) ? s (? s ) = I y * s {? s } -p ? (? s |x V , E),<label>(25)</label></formula><p>which implies p ? (? s |x V , E) = I y * s {? s } for the optimal ?. Moreover, this equation holds for all s ? V and all ?s ? Y.</p><p>Similarly, for each scalar ? st (? s , ?t ), we have that</p><formula xml:id="formula_35">? ??st(?s,?t) log p ? (y * V |x V , E) = 0 is equivalent to p ? (? s , ?t |x V , E) = I y * s ,y * t {? s , ?t }.</formula><p>This equation holds for all (s, t) ? E and all the choices of (? s , ?t ) ? Y ? Y.</p><p>Therefore, the optimal ?-functions are characterized by the moment-matching conditions as below:</p><formula xml:id="formula_36">p ? (y s |x V , E) = I y * s {y s } ?s ? V, p ? (y s , y t |x V , E) = I y * s ,y * t {y s , y t } ?(s, t) ? E.<label>(26)</label></formula><p>C PROOF OF PROPOSITION 1</p><p>Next, we prove Prop. 1. We first restate the proposition as follows:</p><p>Proposition Consider a set of nonzero pseudomarginals {? s (y s )} s?V and {? st (y s , y t )} (st)?E which satisfy ys ? st (y s , y t ) = ? t (y t ) and yt ? st (y s , y t ) = ? s (y s ) for all (s, t) ? E.</p><p>If we parameterize the ?-functions of p ? in Eq. ( <ref type="formula" target="#formula_1">2</ref>) in the following way:</p><formula xml:id="formula_37">? s (y s ) = log ? s (y s ) ?s ? V, ? st (y s , y t ) = log ? st (y s , y t ) ? s (y s )? t (y t ) ?(s, t) ? E,<label>(27)</label></formula><p>then {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E are specified by a fixed point of the sum-product loopy belief propagation algorithm when applied to the joint distribution p ? , which implies that:</p><formula xml:id="formula_38">? s (y s ) ? p ? (y s ) ?s ? V, ? st (y s , y t ) ? p ? (y s , y t ) ?(s, t) ? E.</formula><p>(28) Proof: To prove the proposition, we first summarize the workflow of the sum-product loopy belief propagation algorithm. In sum-product loopy belief propagation, we introduce a message function m t?s (y s ) for each edge (s, t) ? E. Then we iteratively update all message functions as follows:</p><formula xml:id="formula_39">m t?s (y s ) ? yt ? ? ? exp(? t (y t ) + ? st (y s , y t )) s ?N (t)\s m s ?t (y t ) ? ? ? ,<label>(29)</label></formula><p>where N (t) represents the set of neighbors for node t.</p><p>Once the process converges or after sufficient iterations, the approximation of the node marginals and the edge marginals (i.e., {q s (y s )} s?V and {q st (y s , y t )} (s,t)?E ) can be recovered by the message functions {m t?s (y s )} (s,t)?E as follows:</p><formula xml:id="formula_40">q s (y s ) ? exp(? s (y s )) t?N (s) m t?s (y s ),<label>(30)</label></formula><p>q st (y s , y t ) ? exp(? s (y s ) + ? t (y t ) + ? st (y s , y t ))</p><formula xml:id="formula_41">t ?N (s)\t m t ?s (y s ) s ?N (t)\s m s ?t (y t ).<label>(31)</label></formula><p>Next, let us move back to our case, where we parameterize the ?-functions with a set of pseudomarginals as in Eq. ( <ref type="formula" target="#formula_37">27</ref>). For such a specific parameterization of the ?-functions, we claim that one fixed point of Eq. ( <ref type="formula" target="#formula_39">29</ref>) is achieved when m t?s (y s ) = 1 for all (s, t) ? E. To prove that, we notice that when all the message functions equal to 1, the left side of Eq. ( <ref type="formula" target="#formula_39">29</ref>) is apparently 1. The right side of Eq. ( <ref type="formula" target="#formula_39">29</ref>) can be computed as below:</p><formula xml:id="formula_42">yt exp(? t (y t ) + ? st (y s , y t )) s ?N (t)\s m s ?t (y t ) = yt exp(? t (y t ) + ? st (y s , y t )) = yt exp log ? t (y t ) + log ? st (y s , y t ) ? s (y s )? t (y t ) = yt exp log ? st (y s , y t ) ? s (y s ) = yt ? st (y s , y t ) ? s (y s ) = ? s (y s ) ? s (y s ) =1.<label>(32)</label></formula><p>We can see that both the left side and the right side of Eq. ( <ref type="formula" target="#formula_39">29</ref>) are 1, and hence {m t?s (y s ) = 1} (s,t)?E specifies a fixed point of sum-product loopy belief propagation. For this fixed point, q s (y s ) can be computed as follows:</p><formula xml:id="formula_43">q s (y s ) ? exp(? s (y s )) t?N (s) m t?s (y s ) = exp(? s (y s )) = ? s (y s ).<label>(33)</label></formula><p>Similarly, we can compute q st (y s , y t ) as:</p><formula xml:id="formula_44">q st (y s , y t ) ? exp(? s (y s ) + ? t (y t ) + ? st (y s , y t )) t ?N (s)\t m t ?s (y s ) s ?N (t)\s m s ?t (y t )</formula><p>= exp(? s (y s ) + ? t (y t ) + ? st (y s , y t ))</p><p>= exp log ? s (y s ) + log ? t (y t ) + log ? st (y s , y t ) ? s (y s )? t (y t ) = ? st (y s , y t ).</p><p>(34)</p><p>From the above two equations, we can see that {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E are specified by a fixed point (i.e., m t?s (y s ) = 1 for all (s, t) ? E) of sum-product loopy belief propagation. As sum-product loopy belief propagation often works well in practice to approximate the marginal distributions on nodes and edges, we thus have ? s (y s ) ? p ? (y s ) for each node and ? st (y s , y t ) ? p ? (y s , y t ) for each edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SOLVING THE PROXY PROBLEM WITH CONSTRAINED OPTIMIZATION</head><p>The key innovation of our proposed approach is on the proxy optimization problem which is used to approximate the original learning problem. Formally, the proxy optimization problem is stated as:</p><formula xml:id="formula_45">min ?,? s?V d I y * s {y s }, ? s (y s ) + (s,t)?E d I (y * s ,y * t ) {(y s , y t )}, ? st (y s , y t ) , subject to ? s = log ? s (y s ), ? st (y s , y t ) = log ? st (y s , y t ) ? s (y s )? t (y t ) ,</formula><p>and</p><formula xml:id="formula_46">ys ? st (y s , y t ) = ? t (y t ), yt ? st (y s , y t ) = ? s (y s ),<label>(35)</label></formula><p>for all nodes and edges, where d can be any divergence measure between two distributions.</p><p>In our implementation, we ignore these consistency constraints, i.e., ys ? st (y s , y t ) = ? t (y t ) and yt ? st (y s , y t ) = ? s (y s ). This ie because by by optimizing the objective, the obtained pseudomarginals ? would well approximate the observed node and edge marginals, i.e., ? s (y s ) ? I y * s {y s } and ? st (y s , y t ) ? I (y * s ,y * t ) {(y s , y t )}, and hence ? would almost naturally satisfy the constraints. To demonstrate ignoring the consistency constraint makes sense, we also tried a constrained optimization method for solving the proxy problem. Specifically, we add a quadratic term to penalize the inconsistency between ? st (y s , y t ) and ? s (y s ) as well as ? t (y t ), resulting in the following problem: </p><formula xml:id="formula_47">min ?,</formula><p>for all nodes and edges. Again, d is a divergence measure between two distributions, and we choose to use the KL divergence. ? is a hyperparameter deciding the weight of the penalty term. We conduct empirical comparison of this constrained optimization method and our default implementation where the consistency constraint is ignored. The results are presented in Tab. 7. We can see that the constrained optimization method does not lead to improvement, which shows that ignoring the consistency constraint is empirically reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E UNDERSTANDING SPNS AS OPTIMIZING A SURROGATE FOR THE LOG-LIKELIHOOD FUNCTION</head><p>In the model section, we motivate SPNs from the moment-matching conditions of the optimal ?functions. Specifically, we initialize the ?-functions at a state where the moment-matching conditions are approximately satisfied, yielding a near-optimal joint distribution. Then we further tune the ?-functions to solve the maximin game. Besides this perspective, SPNs can also be understood as optimizing a surrogate for the log-likelihood function. Next, we introduce the details.</p><p>Remember that maximizing the log-likelihood function is equivalent to solving a maximin game as:</p><formula xml:id="formula_49">max ? log p ? (y * V |x V , E) = max ? min q L(?, q), with L(?, q) = -H[q(y V )] + s?V {? s (y * s ) -E qs(ys) [? s (y s )]} + (s,t)?E {? st (y * s , y * t ) -E qst(ys,yt) [? st (y s , y t )]}.<label>(37)</label></formula><p>Here, q(y V ) is a joint distribution on all the node labels. q s (y s ) and q st (y s , y t ) are the corresponding marginal distributions.</p><p>Although the above maximin game is equivalent to the original problem of maximizing likelihood, solving the maximin game is nontrivial. In particular, there are two key challenges, i.e., (1) how to specify constraints to characterize a valid joint distribution q(y V ) and ( <ref type="formula" target="#formula_1">2</ref>) how to compute its entropy H(q) = -E q(y V ) [log q(y V )]. To deal with the challenge, a common practice used in loopy belief propagation is to make the following two approximations:</p><p>(1) Instead of specifying constraints to let q(y V ) be a valid joint distribution, we introduce a set of pseudomarginals as approximation to a valid joint distribution. Specifically, these pseudomarginals are denoted as q = {q s (y s )} s?V ? {q st (y s , y t )} (s,t)?E , and they satisfy ys q st (y s , y t ) = q t (y t ) and yt q st (y s , y t ) = q s (y s ) for all (s, t) ? E.</p><p>(2) We approximate the entropy H(q) with Bethe entropy approximation H Bethe (q), which is defined as follows:</p><formula xml:id="formula_50">H Bethe (q) = - s?V E qs(ys) [log q s (y s )] - (s,t)?E</formula><p>E qst(ys,yt) log q st (y s , y t ) q s (y s )q t (y t ) .</p><p>With the two approximations, we get the following maximin game as a surrogate for the likelihood maximization problem:</p><formula xml:id="formula_52">max ? log p ? (y * V |x V , E) ? max ? min q L Bethe (?, q),<label>(39)</label></formula><p>with:</p><formula xml:id="formula_53">L Bethe (?,q) = -H Bethe (q) + (s,t)?E {? s,t (y * s , y * t ) -E qst(ys,yt) [? s,t (y s , y t )]} + s?V {? s (y * s ) -E qs(ys) [? s (y s )]}. (<label>40</label></formula><formula xml:id="formula_54">)</formula><p>This problem is known as the Bethe variational problem (BVP) <ref type="bibr" target="#b42">(Wainwright &amp; Jordan, 2008)</ref>.</p><p>Such a problem can be solved by coordinate descent, where we alternate between updating q to minimize L Bethe (?, q) and updating ? to maximize L Bethe (?, q). According to <ref type="bibr" target="#b48">Yedidia et al. (2005)</ref>, updating q to minimize L Bethe (?, q) can be exactly achieved by running sum-product loopy belief propagation on p ? , where a fixed point of the belief propagation algorithm yields a local optima of q.</p><p>On the other hand, updating ? to maximize L Bethe (?, q) can be easily achieved by gradient ascent.</p><p>In addition to that, a stationary point (? * , q * ) of the above BVP is specified by following conditions:</p><formula xml:id="formula_55">?L Bethe (? * , q * ) ? q * = 0 ?L Bethe (? * , q * ) ?? * = 0.<label>(41)</label></formula><p>According to <ref type="bibr" target="#b48">Yedidia et al. (2005)</ref> and <ref type="bibr" target="#b42">Wainwright &amp; Jordan (2008)</ref>, the first condition is equivalent to the condition that q * is specified by a fixed-point of sum-product loopy belief propagation. The second condition states that the moment-matching conditions are satisfied, i.e., q s (y s ) = I y * s {y s } on each node and q st (y s , y t ) = I (y * s ,y * t ) {(y s , y t )} on each edge.</p><p>For our proposed approach SPN, it can be viewed as solving the BVP as defined in Eq. ( <ref type="formula" target="#formula_52">39</ref>). Through solving the proxy problem, SPN initializes ? at a state where the conditions of stationary points in Eq. ( <ref type="formula" target="#formula_55">41</ref>) are approximately satisfied. Then the fine-tuning stage of SPN further adjusts ? to solve the maximin game by alternatively updating ? and q.</p><p>More specifically, when solving the proxy optimization problem, by initializing ? in the way defined by Eq. ( <ref type="formula" target="#formula_37">27</ref>), the collection of pseudomarginal distributions {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E is specified by a fixed point of sum-product loopy belief propagation according to Prop. 1. This implies that ? ? q L Bethe (?, q) = 0 for q = {? s (y s )} s?V ? {? st (y s , y t )} (s,t)?E . Meanwhile, as {? s } s?V and {? st } (s,t)?E are learned to match the true labels y * V on each training graph, we thus have ? s (y s ) ? I y * s {y s } on each node and ? st (y s , y t ) ? I (y * s ,y * t ) {(y s , y t )} on each edge. Therefore, the conditions in Eq. ( <ref type="formula" target="#formula_55">41</ref>) are approximately satisfied by (?, q) with q = {? s (y s )} s?V ? {? st (y s , y t )} (s,t)?E , which means that solving the proxy problem yields a ? to roughly match the conditions of stationary points for the BVP in Eq. ( <ref type="formula" target="#formula_52">39</ref>). Afterwards, the refinement stage of SPN is exactly trying to solve the maximin game of BVP in Eq. ( <ref type="formula" target="#formula_52">39</ref>), where we alternate between updating q to minimize L Bethe (?, q) via sum-product loopy belief propagation and updating ? to maximize L Bethe (?, q) via gradient ascent.</p><p>As a result, we see that the SPN can also be understood as solving the Bethe variational problem in Eq. ( <ref type="formula" target="#formula_52">39</ref>), which acts as a surrogate for the log-likelihood function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EXPERIMENTAL DETAILS</head><p>Next, we describe our experimental setup in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 DATASETS</head><p>The statistics of the datasets used in our experiment are summarized in Tab. 8. For the Cora*, Citeseer*, Pubmed*, and PPI datasets, they are under the MIT license. For the DBLP dataset, it is constructed from the citation network<ref type="foot" target="#foot_2">4</ref> in <ref type="bibr" target="#b39">Tang et al. (2008)</ref>. Scientific papers from eight conferences are treated as nodes, which are divided into three categories based on conference domains<ref type="foot" target="#foot_3">5</ref> for classification. For each paper, we compute the mean GloVe embedding<ref type="foot" target="#foot_4">6</ref>  <ref type="bibr" target="#b29">(Pennington et al., 2014)</ref>  For the PPI datasets, there are 121 binary labels, and we treat each binary label as an independent task. For each compared algorithm, we train a separate model for each task, and report the overall results across all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 ARCHITECTURE CHOICES</head><p>To facilitate reproducibility, we use the GNN module implementations of PyTorch Geometric <ref type="bibr" target="#b9">(Fey &amp; Lenssen, 2019)</ref>, and follow the GNN models provided in the examples of the repository, unless otherwise mentioned. Note that most architecture choices are not optimal on the benchmark datasets, but we did not tune them since we only aim to show that our method brings consistent and significant improvement.</p><p>GCN <ref type="bibr" target="#b17">(Kipf &amp; Welling, 2017)</ref>. We set the number of hidden neurons to 16, and the number of layers to 2. ReLU <ref type="bibr" target="#b28">(Nair &amp; Hinton, 2010)</ref> is used as the activation function. We do not dropout between GNN layers.</p><p>GraphSage <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>. We set the number of hidden neurons to 64, and the number of layers to 2. ReLU <ref type="bibr" target="#b28">(Nair &amp; Hinton, 2010)</ref> is used as the activation function. We do not dropout between GNN layers.</p><p>GAT <ref type="bibr" target="#b41">(Veli?kovi? et al., 2018)</ref>. We set the number of hidden neurons to 256 per attention head, and the number of layers to 3. The number of heads for each layer is set to 4, 4 and 6. ELU <ref type="bibr" target="#b7">(Clevert et al., 2016)</ref> is used as the activation function. We do not dropout between GNN layers.</p><p>Graph U-Net <ref type="bibr" target="#b11">(Gao &amp; Ji, 2019)</ref>. We set the number of hidden neurons to 64 and the number of layers to 3. We randomly dropout 20% of the edges from the adjacency matrix. We do not dropout node features or between layers.</p><p>GCNII <ref type="bibr">(Chen et al., 2020a)</ref>. We set the number of hidden neurons to 2048 for the citation datasets (Cora*, Citeseer*, Pubmed* and DBLP) and 256 for the PPI dataset. We set the number of layers to 9. ReLU <ref type="bibr" target="#b28">(Nair &amp; Hinton, 2010)</ref> is used as the activation function. For PPI, layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> is applied between the GCNII layers. We do not dropout between GNN layers. We set the strength ? of the initial residual connection to 0.5, and the hyperparameter ? to compute the strength of the identity mapping to 1.</p><p>The g function. In Eq. ( <ref type="formula" target="#formula_8">8</ref>) of the model section, we define g as a function mapping a pair of Ldimensional representations to a (|Y| ? |Y|)-dimensional logit. Two variants of this function are used in our experiment. For the PPI and DBLP dataset, we use the linear variant, where the pair of node representations are concatenated and plugged into a linear layer:</p><formula xml:id="formula_56">g linear (v s , v t ) = W[v s ; v t ] + b,<label>(42)</label></formula><p>where W ? R (|Y|?|Y|)?2L is the weight matrix and b ? R |Y|?|Y| is the bias. For the citation datasets (Cora*, Citeseer*, Pubmed*), we use the bilienar variant, where the pair of node representations are plugged in a bilinear mapping:</p><formula xml:id="formula_57">g bilinear (v s , v t ) = (Wv s )(Wv t ) T ,<label>(43)</label></formula><p>where W ? R |Y|?L is a weight matrix.</p><p>SPN with a shared GNN. By default, the SPN uses a node GNN and an edge GNN to approximate the pseudomarginals on nodes and edges respectively. In the experiment, we also consider using a shared GNN for both pseudomarginals on nodes and edges. In other words, u s = v s , ?s ? V (see Eq. ( <ref type="formula" target="#formula_7">7</ref>) and Eq. ( <ref type="formula" target="#formula_8">8</ref>)). All the other components are the same as the default SPN. The results of this variant are shown in Tab. 6 of the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 HYPERPARAMETER CHOICES</head><p>GNNs and SPNs. For node classification, the learning rate of the node GNN ? s in GNNs and SPNs is presented in Tab. 9. For edge classification, the learning rate of the edge GNN ? st is presented in Tab. 10. For the temperature ? used in the edge GNN ? st of SPNs, we report its values in Tab. 11.</p><p>CRF-linear. For CRF-linear training, we set the learning rate to 5 ? 10 -4 .</p><p>CRF-GNNs and SPN. For CRF and the refinement stage of SPN, we set learning rates to 1 ? 10 -5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GMNN.</head><p>For GMNN training, we set the learning rate to 5 ? 10 -3 .</p><p>G.2 NODE-LEVEL ACCURACY ON CORA*, CITESEER*, AND PUBMED*</p><p>In the experiment, we report the graph-level accuracy on the Cora*, Citeseer*, and Pubmed* datasets, where SPNs consistently outperform other methods. Besides the graph-level accuracy, we also compute the node-level accuracy on these datasets, and the results are reported in Tab. 12. We can see that our approach still consistently outperforms other methods in terms of node-level accuracy. As explained in section 4.2, the sum-product belief propagation algorithm is more applicable to the case of node-level accuracy, as it aims at inferring the marginal label distribution on each node. Nevertheless, in practice we find that the max-product algorithm usually achieves better empirical node-level accuracy. For example, the results on the PPI-10 dataset are presented in Tab. 13. Because of the better empirical results, we choose to use max-product belief propagation by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 HYPERPARAMETER ANALYSIS</head><p>Finally, We present analysis of the hyperparameter ? (i.e., edge temperature) in Fig. <ref type="figure" target="#fig_5">5</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence curves for solving the maximin game in Eq. (3) during model learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Case study. SPN correctly predicts all node labels than GAT and the edge GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Hyperparameter analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>proof is provided in Sec. C. With the proposition, we observe that if we parameterize the ?functions by combining a set of pseudomarginals {? s (y s )} s?V and {? st (y s , y t )} (s,t)?E in the way defined by Eq. (5), then those pseudomarginals can well approximate the true marginals of the joint distribution p ? , i.e., ? s (y s ) ? p ? (y s ) and ? st (y s , y t ) ? p ? (y s , y t ) for all nodes s and edges (s, t). Given this precondition, if we further have ? s (y s ) ? I y * s {y s } and ? st (y s , y t ) ? I (y *</figDesc><table><row><cell>then the moment-matching conditions in Eq. (</cell><cell>s ,y  *  t ) {(y s , y t )},</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Result on PPI datasets (in %). SPNs get consistent improvement on GNNs and CRFs. ? 0.10 54.55 ? 0.29 77.48 ? 0.12 56.10 ? 0.36 80.43 ? 0.10 62.48 ? 0.27 82.28 ? 0.24 66.52 ? 0.89 GraphSAGE 81.02 ? 0.07 67.30 ? 0.11 84.13 ? 0.04 72.93 ? 0.04 95.34 ? 0.03 92.18 ? 0.05 98.51 ? 0.02 97.51 ? 0.03 GAT 77.49 ? 0.20 60.72 ? 0.25 81.35 ? 0.19 68.55 ? 0.30 96.14 ? 0.15 93.53 ? 0.24 98.85 ? 0.05 98.06 ? 0.08 Graph U-Net 77.17 ? 0.07 55.54 ? 0.33 78.22 ? 0.04 59.12 ? 0.30 83.15 ? 0.04 68.70 ? 0.08 86.29 ? 0.04 75.57 ? 0.18 GCNII 80.99 ? 0.07 65.79 ? 0.25 84.81 ? 0.06 74.54 ? 0.14 97.53 ? 0.01 95.86 ? 0.01 99.39 ? 0.00 98.97 ? 0.00 CRF-linear 65.33 ? 2.77 48.30 ? 0.35 67.20 ? 2.24 49.45 ? 0.97 69.72 ? 0.65 50.17 ? 0.39 69.98 ? 0.30 50.61 ? 0.35 CRF-GCN 76.33 ? 0.21 50.79 ? 0.74 76.27 ? 0.10 49.47 ? 0.63 77.08 ? 0.07 52.36 ? 0.72 77.34 ? 0.07 53.60 ? 0.36 CRF-GraphSAGE 77.43 ? 0.28 54.57 ? 1.07 77.25 ? 0.36 53.48 ? 1.00 77.65 ? 0.38 54.44 ? 1.34 77.21 ? 0.19 54.50 ? 3.09 CRF-GAT 76.50 ? 0.49 52.95 ? 0.40 76.76 ? 0.61 55.01 ? 0.93 74.58 ? 0.92 54.98 ? 1.13 70.42 ? 0.72 53.27 ? 0.42 CRF-GCNII 79.98 ? 0.32 61.22 ? 1.10 81.73 ? 0.33 66.37 ? 0.56 92.11 ? 0.28 87.10 ? 0.40 96.94 ? 0.12 94.95 ? 0.19 GMNN 77.55 ? 0.53 57.20 ? 2.63 81.21 ? 0.87 67.46 ? 2.92 94.67 ? 2.77 90.72 ? 5.28 97.00 ? 2.98 94.69 ? 5.60 SPN-GCN 77.07 ? 0.05 54.15 ? 0.17 78.02 ? 0.05 55.73 ? 0.15 80.59 ? 0.04 61.36 ? 0.11 82.56 ? 0.20 66.70 ? 0.77 SPN-GraphSAGE 82.11 ? 0.03 68.56 ? 0.07 85.40 ? 0.05 74.45 ? 0.07 95.28 ? 0.02 91.99 ? 0.04 98.55 ? 0.02 97.56 ? 0.03 SPN-GAT 79.01 ? 0.17 64.02 ? 0.40 83.55 ? 0.12 72.37 ? 0.18 96.68 ? 0.13 94.41 ? 0.21 99.04 ? 0.06 98.38 ? 0.10 SPN-GCNII 82.01 ? 0.03 67.80 ? 0.11 85.83 ? 0.04 75.96 ? 0.05 97.55 ? 0.01 95.87 ? 0.02 99.41 ? 0.00 99.02 ? 0.00</figDesc><table><row><cell>Algorithm</cell><cell>Accuracy</cell><cell>PPI-1</cell><cell>Micro-F1</cell><cell>Accuracy</cell><cell>PPI-2</cell><cell>Micro-F1</cell><cell>PPI-10 Accuracy Micro-F1</cell><cell>Accuracy</cell><cell>PPI</cell><cell>Micro-F1</cell></row><row><cell>GCN</cell><cell>76.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on Cora*, Citeseer*, Pubmed*, DBLP (in %). SPNs achieve the best result. ? 5.75 38.27 ? 4.82 41.71 ? 4.79 60.55 ? 2.23 GMNN 54.30 ? 1.15 48.46 ? 1.06 51.70 ? 1.23 76.54 ? 2.93 SPN-GAT 58.78 ? 1.21 49.02 ? 0.78 52.91 ? 0.54 84.84 ? 0.73 SPN-UNet 58.03 ? 0.54 46.97 ? 1.06 53.36 ? 0.67 80.11 ? 1.59 SPN-GCNII 60.47 ? 0.49 48.34 ? 0.50 54.35 ? 0.64 83.57 ? 1.33</figDesc><table><row><cell>Algorithm</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell><cell>DBLP</cell></row><row><cell>GCN</cell><cell cols="4">57.26 ? 0.66 46.24 ? 0.61 51.84 ? 0.45 76.60 ? 2.32</cell></row><row><cell>GraphSAGE</cell><cell cols="4">49.02 ? 2.37 41.32 ? 2.41 48.61 ? 1.28 73.81 ? 0.90</cell></row><row><cell>GAT</cell><cell cols="4">51.99 ? 3.51 47.94 ? 0.46 50.89 ? 0.52 79.16 ? 1.44</cell></row><row><cell>Graph U-Net</cell><cell cols="4">56.07 ? 0.57 45.91 ? 1.65 51.77 ? 0.97 75.21 ? 2.68</cell></row><row><cell>GCNII</cell><cell cols="4">59.15 ? 0.67 46.39 ? 0.92 53.54 ? 0.98 81.79 ? 0.88</cell></row><row><cell>CRF-linear</cell><cell cols="4">42.78 ? 3.94 40.60 ? 0.81 43.90 ? 2.91 54.26 ? 1.27</cell></row><row><cell>CRF-GAT</cell><cell cols="4">49.10 ? 3.80 42.89 ? 1.30 47.79 ? 1.33 59.14 ? 4.15</cell></row><row><cell>CRF-UNet</cell><cell cols="4">53.49 ? 2.47 43.66 ? 2.12 50.02 ? 0.88 57.46 ? 3.07</cell></row><row><cell>CRF-GCNII</cell><cell>36.18</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Run time comparison (in sec).</figDesc><table><row><cell>Algorithm</cell><cell>DBLP</cell><cell>PPI</cell></row><row><cell>GAT</cell><cell>23.15</cell><cell>460.81</cell></row><row><cell>CRF (GAT)</cell><cell>500.43</cell><cell>27136.90</cell></row><row><cell>SPN(GAT)</cell><cell>46.86</cell><cell>962.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Micro-F1 with and w/o refinement (in %).</figDesc><table><row><cell>Algorithm</cell><cell>Refine</cell><cell>PPI-2</cell><cell>PPI-10</cell><cell>PPI</cell></row><row><cell>SPN-</cell><cell>w/o</cell><cell cols="3">71.52 ? 0.21 94.41 ? 0.21 98.38 ? 0.10</cell></row><row><cell>GAT</cell><cell>with</cell><cell cols="3">71.58 ? 0.20 94.63 ? 0.20 98.68 ? 0.09</cell></row><row><cell>SPN-</cell><cell>w/o</cell><cell cols="3">73.93 ? 0.08 91.99 ? 0.04 97.56 ? 0.03</cell></row><row><cell>GraphSAGE</cell><cell>with</cell><cell cols="3">73.68 ? 0.10 92.49 ? 0.02 97.77 ? 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of learning methods (in %).</figDesc><table><row><cell>Algorithm</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>PPI-10</cell></row><row><cell>Maximin Game</cell><cell cols="3">49.10 ? 3.80 42.89 ? 1.30 54.98 ? 1.13</cell></row><row><cell>Pseudolikelihood</cell><cell cols="3">54.30 ? 1.15 48.46 ? 1.06 90.72 ? 5.28</cell></row><row><cell>Proxy Problem</cell><cell cols="3">58.78 ? 1.21 49.02 ? 0.78 95.87 ? 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>.</figDesc><table><row><cell>Algorithm</cell><cell>PPI-1</cell><cell>PPI-2</cell><cell>PPI-10</cell></row><row><cell>GAT</cell><cell cols="3">60.72 ? 0.25 68.55 ? 0.30 93.53 ? 0.24</cell></row><row><cell>SPN-GAT node and edge GNNs</cell><cell cols="3">64.02 ? 0.40 72.37 ? 0.18 94.41 ? 0.21</cell></row><row><cell>SPN-GAT a shared GNN</cell><cell cols="3">63.72 ? 0.38 70.99 ? 0.25 95.19 ? 0.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>st (y s , y t ) -? s (y s )} 2 , subject to ? s = log ? s (y s ), ? st (y s , y t ) = log ? st (y s , y t ) ? s (y s )? t (y t ) ,</figDesc><table><row><cell>?</cell><cell>s?V</cell><cell cols="3">d I y  *  s {y s }, ? s (y s ) +</cell><cell>(s,t)?E</cell><cell>d I (y  *  s ,y  *  t ) {(y s , y t )}, ? st (y s , y t )</cell></row><row><cell></cell><cell>+ ?</cell><cell></cell><cell>{</cell><cell cols="2">? st (y s , y t ) -? t (y t )} 2 +</cell></row><row><cell></cell><cell></cell><cell>(s,t)?E</cell><cell>yt</cell><cell>ys</cell></row></table><note><p>ys { yt ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Analysis of constrained optimization methods for solving the proxy problem.</figDesc><table><row><cell>Algorithm</cell><cell>Constrained Optimization</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell></row><row><cell>SPN-GAT</cell><cell>w/o with</cell><cell cols="3">49.10 ? 3.80 42.89 ? 1.30 47.79 ? 1.33 48.83 ? 3.51 42.04 ? 1.23 47.55 ? 1.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Dataset statistics. ML and MC stand for multi-label classification and multi-class classification respectively.</figDesc><table><row><cell cols="4">Dataset Task # Features # Labels</cell><cell cols="9">Training Graphs # Graphs Avg. # Nodes Avg. # Edges # Graphs Avg. # Nodes Avg. # Edges # Graphs Avg. # Nodes Avg. # Edges Validation Graphs Test Graphs</cell></row><row><cell>PPI</cell><cell>ML</cell><cell>50</cell><cell>121</cell><cell>20</cell><cell>2245.3</cell><cell>61318.4</cell><cell>2</cell><cell>3257</cell><cell>99460.0</cell><cell>2</cell><cell>2762</cell><cell>80988.0</cell></row><row><cell>Cora*</cell><cell>MC</cell><cell>1433</cell><cell>7</cell><cell>140</cell><cell>5.6</cell><cell>7.0</cell><cell>500</cell><cell>4.9</cell><cell>5.8</cell><cell>1000</cell><cell>4.7</cell><cell>5.3</cell></row><row><cell cols="2">Citeseer* MC</cell><cell>3703</cell><cell>6</cell><cell>120</cell><cell>4.0</cell><cell>4.3</cell><cell>500</cell><cell>3.8</cell><cell>4.0</cell><cell>1000</cell><cell>3.8</cell><cell>3.8</cell></row><row><cell cols="2">Pubmed* MC</cell><cell>500</cell><cell>3</cell><cell>60</cell><cell>6.0</cell><cell>6.7</cell><cell>500</cell><cell>5.4</cell><cell>5.8</cell><cell>1000</cell><cell>5.6</cell><cell>6.7</cell></row><row><cell>DBLP</cell><cell>MC</cell><cell>100</cell><cell>3</cell><cell>1</cell><cell>6488</cell><cell>10262</cell><cell>1</cell><cell>14142</cell><cell>48631</cell><cell>1</cell><cell>26813</cell><cell>155899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>of words in the title and abstract as features. We split the dataset into three disjoint graphs for training/validation/test. The training graph contains papers published before 1999 (with 1999 included). The validation graph contains papers published between2000 and  2009 (with 2000 and 2009 included). The test graph contains papers published after 2010 (with 2010 included). There exists an undirected edge between two papers if one cites the other one. Cross-split edges (e.g., an edge between a paper in the training set and a paper in the validation set) are removed.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Node-level accuracy on Cora*, Citeseer*, Pubmed* (in %). ? 0.24 72.25 ? 0.71 78.05 ? 0.55 GraphSAGE 73.43 ? 1.67 62.48 ? 2.19 73.99 ? 1.26 GAT 79.65 ? 1.25 74.15 ? 0.12 78.62 ? 0.52 Graph U-Net 78.72 ? 0.63 71.36 ? 1.37 77.93 ? 0.60 GCNII 82.84 ? 0.37 72.61 ? 0.49 79.47 ? 0.55 CRF-linear 68.47 ? 2.13 65.88 ? 0.85 65.93 ? 2.18 CRF-GAT 77.75 ? 1.24 69.13 ? 1.10 75.96 ? 1.06 CRF-UNet 78.32 ? 1.51 70.78 ? 1.15 77.91 ? 0.56 CRF-GCNII 35.98 ? 7.40 33.73 ? 5.87 60.55 ? 4.17 GMNN 79.90 ? 0.93 72.18 ? 0.48 78.00 ? 1.04 SPN-GAT 83.13 ? 0.48 74.50 ? 0.36 79.23 ? 0.33 SPN-UNet 81.11 ? 0.55 72.28 ? 0.94 78.70 ? 0.37 SPN-GCNII 83.54 ? 0.27 74.04 ? 0.29 79.95 ? 0.38 G.3 COMPARISON OF SUM-PRODUCT AND MAX-PRODUCT BELIEF PROPAGATION</figDesc><table><row><cell>Algorithm</cell><cell>Cora*</cell><cell>Citeseer*</cell><cell>Pubmed*</cell></row><row><cell>GCN</cell><cell>79.85</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Micro-F1 on PPI-10 (in %).</figDesc><table><row><cell>Algorithm</cell><cell>Micro-F1</cell></row><row><cell>Sum-product BP</cell><cell>94.50 ? 0.16</cell></row><row><cell>Max-product BP</cell><cell>94.65 ? 0.13</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>ML: ICML/NeurIPS. CV: ICCV/CVPR/ECCV. NLP: ACL/EMNLP/NAACL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The local subgraph formed by a node and its direct neighbors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://originalstatic.aminer.cn/misc/dblp.v12.7z</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>ML: ICML/NeurIPS. CV: ICCV/CVPR/ECCV. NLP: ACL/EMNLP/NAACL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://nlp.stanford.edu/data/glove.6B.zip</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>5 ? 10 -3 5 ? 10 -3 1 ? 10 -2 1 ? 10 -2 1 ? 10 -2 GraphSage 5 ? 10 -3 5 ? 10 -3 5 ? 10 -3 5 ? 10 -3 5 ? 10  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 COMPUTATIONAL RESOURCES</head><p>We run the experiment by using NVIDIA Tesla V100 GPUs with 16GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ADDITIONAL RESULTS</head><p>In this section, we present some additional experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 ADDITIONAL ANALYSIS OF GNN ARCHITECTURES</head><p>In this analysis, we study the effect of node/edge GNN architectures on SPNs. We fix one of the GNNs and change the capacity of the other (Fig. <ref type="figure">4</ref>), then evaluate SPN-GAT on PPI-1-0, a subset of PPI-1 that only contains its first label. The results show that our model benefit from capacity gain in both node and edge GNNs, highlighting their effective synergy. This also explains the underperformance of SPN-GCN in Tab. 1, where the edge GCN backbone with only two layers and 16 hidden neurons is incapable of modeling the edge label dependencies and thus drags the performance behind. We also find that the node and edge GNNs need not share the same backbone, and in many cases SPNs with different node and edge GNNs perform superior to those with same backbone (Fig. <ref type="figure">4</ref>). The expressiveness of edge GNNs is crucial to the performance of SPN. Though we did not optimize the design of our edge GNNs, they have shown to be helpful in boosting the performance once plugged into our approach. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Predicting structured data</title>
		<author>
			<persName><forename type="first">G?khan</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical analysis of non-lattice data. The statistician</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Besag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="179" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding deep architecture with reasoning layer</title>
		<author>
			<persName><forename type="first">Xinshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A stochastic parts program and noun phrase parser for unrestricted text</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLC</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training structural svms when exact inference is intractable</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">?</forename><surname>Carreira-Perpi??n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A flexible generative framework for graph-based semi-supervised learning</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Copulagnn: Towards integrating representational and correlational roles of graphs in graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Cgnf</surname></persName>
		</author>
		<title level="m">Conditional graph neural fields. ICLR Submission</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName><forename type="first">Kevin P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gmnn: Graph markov neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename><surname>De</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate max-margin training for structured output spaces</title>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural enhanced belief propagation on factor graphs</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satorras</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Combining generative and discriminative models for hybrid inference</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Piecewise training for structured prediction</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An Introduction to Conditional Random Fields</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Irwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised node classification on graphs: Markov random fields vs. graph neural networks</title>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the optimality of solutions of the max-product beliefpropagation algorithm in arbitrary graphs</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Continuous graph neural networks</title>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Constructing free-energy approximations and generalized belief propagation algorithms</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient probabilistic logic reasoning with graph neural networks</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
