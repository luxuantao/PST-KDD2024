<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient kNN Classification Algorithm for Big Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shichao</forename><surname>Zhang</surname></persName>
							<email>zhangsc@mailbox.gxnu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhenyun</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Debo</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoshu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">PII</orgName>
								<address>
									<postCode>S0925-2312(16)00113-2</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Efficient kNN Classification Algorithm for Big Data</orgName>
								<address>
									<settlement>Neurocomputing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Guangxi Key Lab of Multi-source Information Mining &amp; Security</orgName>
								<orgName type="institution">Guangxi Normal University</orgName>
								<address>
									<postCode>541004</postCode>
									<settlement>Guilin, Guangxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient kNN Classification Algorithm for Big Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7A6428DB2FAE2C7509A4D3E53C608307</idno>
					<idno type="DOI">10.1016/j.neucom.2015.08.112</idno>
					<note type="submission">Received date: 17 March 2015 Revised date: 1 July 2015 Accepted date: 7 August 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Classification</term>
					<term>kNN</term>
					<term>big data</term>
					<term>data cluster</term>
					<term>cluster center</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>K nearest neighbors (kNN) is an efficient lazy learning algorithm and has successfully been developed in real applications. It is very natural to scale the previous kNN method to the large scale datasets. In this paper, we propose to first conduct a k-means clustering to separate the whole dataset into several parts, each of which is then conducted kNN classification. We conduct sets of experiments on big data and medical imaging data. The experimental results show that the proposed kNN classification works well in terms of accuracy and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the era of big data, it is the most important to efficiently learn from large scale in all kinds of real applications, such as classification and clustering. Therefore, it is very obvious to scale traditional classification algorithms, such as decision trees, support vector machine, Naive Bayes, neutral network, and k nearest neighbors (kNN), so that these methods can be easily used in big data. Due to the simplicity, easy-understand and relatively high performance of kNN, this paper focuses on scaling the kNN classification into the application of big data <ref type="bibr" target="#b12">[13]</ref>.</p><p>The previous kNN method first selects k nearest training samples for a test sample, and then predicts the test sample with the major class among k nearest training samples. However, kNN needs to compute the distance (or similarity) of all training samples for each test sample in the process of selecting k nearest neighbors <ref type="bibr" target="#b23">[24]</ref>[25].</p><p>The high cost (i.e., linear time complexity over the sample size) prohibits the traditional kNN method to be used in big data <ref type="bibr" target="#b20">[21]</ref>. Obviously, conducting kNN algorithm in the memory of a PC should be an interesting issue.</p><p>Inspired by the recent progress on big data, this paper devised a new kNN method for dealing with big data. Specifically, we propose to first conduct a k-means clustering to separate the whole dataset into several parts <ref type="bibr" target="#b25">[26]</ref>. And then we select the nearest cluster as the training samples and conduct the kNN classification. The time complexity of the proposed algorithm is linear to the sample size. The experimental results on real datasets including medical imaging datasets indicated that the proposed method achieved better performance than conventional kNN method in terms of both classification performance and time cost <ref type="bibr" target="#b14">[15]</ref>.</p><p>The rest of paper is organized as follows: in Section 2, we provide a brief review the previous fast kNN methods. We then propose the new kNN classification in Section 3. The experiment results are presented in Section 4. Finally, we give the conclusions and our future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The research of kNN method has been becoming a hot research topic in data mining and machine learning since the algorithm was proposed in 1967. To apply for the traditional kNN method in big data, the previous literatures can be often categorized into two parts, i.e., fast finding the nearest samples <ref type="bibr" target="#b20">[21]</ref> and selecting representatives samples (or removing some samples) to reduce the calculation of kNN <ref type="bibr" target="#b21">[22]</ref>. For instance, Zhang proposed a Certainly Factor (CF) measure to deal with the unsuitability of skewed class distribution in kNN methods <ref type="bibr" target="#b13">[14]</ref>.  <ref type="bibr" target="#b20">[21]</ref>. In particularly, the samples within a cluster has high similarity. Thus, comparing to the traditional kNN method, the proposed algorithm not only reduces the time complexity of kNN, but also does not add significantly effect on classification accuracy <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce two processes in our algorithm, namely training process and testing process. The training process is designed to select a nearest cluster for each test sample as its new training dataset, and the testing process is used to classify each test sample by kNN algorithm within its nearest cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training process</head><p>Clustering is one of the fundamental technique in data mining because I can be used for database segmentation and data compression and can also be employed for data preprocessing of data mining. Clustering is designed to group a set of samples in such a way that samples in the same group (cluster) are more similar to each other than to those in other groups. That is, samples is with high similarity within a cluster and low similarity between clusters.</p><p>The recent clustering methods can be parted into the following categories: density-based clustering, grid-based clustering, partitioning clustering and hierarchical clustering, respectively. Even though the previous clustering methods showed good performance, but they are limited in its applicability to big data due to theirs high computational complexity. To address this, this paper considers to employ a clustering algorithm satisfying the following two advantages: low complexity;</p><p>scales linearly <ref type="bibr">[1][5]</ref>, respectively. To this end, we used the Landmark-based Spectral Clustering (LSC) <ref type="bibr" target="#b3">[4]</ref> in this paper. The rationale of LSC is to select p (&lt;&lt;n) representative sample as landmarks and represents the original samples as the linear combinations of these landmarks. Different from that traditional spectral clustering method use the entire samples to represent each sample, the LSC significant reduces the complexity of affinity matrix. At the same time, the complexity of solving the eigenvalue down to scales linearly <ref type="bibr" target="#b23">[24]</ref>.</p><p>LSC tries to compress the original samples by finding a set of basis vectors and the representation with respect to the bases for each sample, i.e., searching for p representative samples. In this way, we have two simple and effective methods to select landmark sample from original sample, such as random sampling and k-means-based method. Random sampling randomly selects samples as landmark samples while the k-means-based method first conducts clustering on all samples several times (no need to convergence) and then uses the cluster centers as the landmark samples. In this paper, we repeat k-means algorithm 10 times and then use the cluster centers as the landmark samples.</p><p>First, we treat every sample as a basis vector to construct landmark matrix Z. Note that LSC uses p landmarks to represent each sample</p><formula xml:id="formula_0">n m n R    ] , , , [ 2 1 x x x X </formula><p>. Thus, we need to find the matrix W which is projection matrix of X at the landmark matrix Z <ref type="bibr" target="#b9">[10]</ref>. The projection function can be defined as follows:</p><formula xml:id="formula_1">        i Z j j i h j i h ji Z j K K w i ， ' ' ) , ( ) , ( z x z x . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>Where i z is j-th column vector of Z, and</p><formula xml:id="formula_3"> i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z</head><p>denote a sub-matrix of Z composed of r nearest landmarks of i x . Here we need O(pmn) to construct W.</p><formula xml:id="formula_4">) ( h K is a kernel function with a bandwidths h. The Gaussian kernel ) 2 exp( ) , ( 2 2 h K j i j i h z x z x   </formula><p>is one of the most commonly used. Then we conduct spectral analysis on landmark-based graph and compute the graph matrix as:</p><formula xml:id="formula_5">   W W G T . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>which has a very efficient eigen-decomposition. In this method, we choose</p><formula xml:id="formula_7">W D W 2 / 1   </formula><p>where D is the row sum of W. Note that each column of W sums up to 1 and thus the degree matrix of G is I.</p><p>Let the Singular value Decomposition (SVD) of  W is as follows:</p><formula xml:id="formula_8">T V U W    . (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where</p><formula xml:id="formula_10">p p k R    ] , , , [ 2 1 u u u U </formula><p>is called as the left singular vectors of the first k eigenvectors of</p><formula xml:id="formula_11">T   Z Z , p n k R    ] , , , [ 2 1 v v v V  is called as right singular vectors of the k eigenvectors of   Z Z T .</formula><p>We compute U within O(p 3 ), linear to the sample size. V can be compute as:</p><formula xml:id="formula_12">   W U V T T 1 - . (<label>4</label></formula><formula xml:id="formula_13">)</formula><p>The overall time complexity of V is O(p 3 +p 2 n), which is a significant reduction from O(n 3 ) by considering p&lt;&lt;n. Each row of V is a sample and apply k-means to get the clusters. Due to the time complexity from O(n 3 ) to O(n), the LSC algorithm substantially reduces computational time. Thus, the proposed algorithm with low-complexity is very suitable to be applied in the domain of big data.</p><p>Finally, the pseudo of LSC is presented in Algorithm 1.</p><p>Algorithm 1: The pseudo of LSC Input: n data points From algorithm 2, the number of NewX i is much smaller than the size of the training dataset. When the size of m is large, LC-kNN is easy to reduce the calculation of kNN and improves classification quality. However, with the increase of m, the overhead of clustering will increase and the classification accuracy will be lower at the same time. Therefore, in order to avoid this situation, the number of cluster m needs to be set in a reasonable value.</p><formula xml:id="formula_14">m n R x x x  , , ,<label>2 1 </label></formula><p>In general, the larger the value of m, the higher the classification efficiency, which lead to a higher classification accuracy. However, if the training dataset distribution is relatively concentrated, which will lead to low accuracy. In addition, if m is small, i.e., m=1, that is standard kNN algorithm, which prohibits the kNN to be used in big data.</p><p>Thus we set m in a suitable range. Assuming that the proposed algorithm need M memory space, the memory of PC is M 1 and the smallest class of training sample is n 0 .</p><p>To this end, the value range of m is expressed as follows:</p><formula xml:id="formula_15">0 1 n m M M   . (<label>5</label></formula><formula xml:id="formula_16">)</formula><p>Note that we conduct a k-means clustering to separate the whole dataset into several parts, each of which we conduct kNN classification, the selection of k value is important in the later process. For example, Lall mentioned that the suitable setting of k value should satisfy n k </p><p>for the datasets with sample size larger than 100 <ref type="bibr" target="#b7">[8]</ref>.</p><p>However, such a setting has been proved to not suitable for all cases of datasets.</p><p>Considering that the samples has a strong correlation in sub-cluster, so the k value not be setting too large. Thus, the selection of k value should be set as small as possible in the case of high classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>As mentioned in the previous sections, our proposed algorithm is an extension of kNN. Thus, in order to show the effectiveness of LC-kNN algorithm, we took the kNN as the baseline and made a comparison between kNN, LC-kNN and RC-kNN (random clustering kNN), and several real datasets from UCI <ref type="bibr" target="#b1">[2]</ref>, medical imaging datasets <ref type="bibr" target="#b5">[6]</ref>[7], and LIBSVM <ref type="bibr" target="#b2">[3]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental results of LC-kNN and RC-kNN with different value of m</head><p>The value of m is very important to the LC-kNN algorithm, because it directly affects the final performance and real applications.  <ref type="table" target="#tab_10">1~9</ref>. From Table <ref type="table" target="#tab_10">1~9</ref>, we found that the proposed two algorithms needed less time with the larger number of clusters m, while more time cost for the smaller number of m.</p><p>For example, when m value is 10, i.e., we separated the whole dataset into 10 parts.</p><p>We then conducted kNN classification for each part, and obtained about one-tenth time cost of the method conducting in the whole dataset.</p><p>Besides, considering that the proposed two algorithms are the extension of kNN, their classification accuracies should as close as possible kNN. We found that two algorithms have high classification accuracy with the lager number of clusters m, while these two algorithms have low classification accuracy for the small number of m. In addition, from Table <ref type="table" target="#tab_11">10</ref>, we found that two algorithms were closer to kNN classification accuracy with m =10. Thus, we set m=10 in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">To determine parameter k</head><p>In this section, in order to select the appropriate k value, a group of experiments were conducted on nine datasets with m=1 to 20, each point represent the mean of 10 results in the figures. From Fig. <ref type="figure" target="#fig_1">1</ref>, with the increase of k value, the overall of classification accuracy decreases. That is because that the smaller the training dataset is, the more the classification accuracy is. Hence, the difference between the samples is significant and the classification accuracy will reduce. We can make a conclusion that we should choose a suitable k value in this case. According to the previous analysis, k value should be set as small as possible in the case of the higher accuracy of the proposed algorithm. From Fig. <ref type="figure" target="#fig_1">1</ref>, the classification accuracy was 0.8986 with k=3, which was higher than k=1 on satimage dataset. But the LC-kNN performance has higher than kNN with k=1 in Table <ref type="table" target="#tab_11">10</ref>. Thus we set k=1 in the next experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance comparison of kNN, RC-kNN and LC-kNN</head><p>In this experiment, according to previous analysis, we set m=10, k=1. Then, we use classification accuracy and running time as the evaluations for the classification task <ref type="bibr" target="#b22">[23]</ref>. The shorter time and higher accuracy the algorithm is, the better the performance is. We report our experimental results in Table <ref type="table" target="#tab_11">10</ref>. From Table <ref type="table" target="#tab_11">10</ref>, we can observe that the proposed RC-kNN and LC-kNN improved by 7~9 times than the kNN (close to the number of cluster), in terms of time cost. In the evaluation of classification accuracy, the LC-kNN and RC-kNN is lower by 1%~2.6% and 3.3%~14% than kNN. Therefore, according to the experimental results, we may the conclusion that the LC-kNN works well in terms of classification accuracy and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we have proposed an efficient kNN classification to conduct a k-means clustering to separate the whole dataset into several parts. We then conducted kNN classification for each part. To do this, we parted the conventional kNN method into two processes, namely training process and testing process. Moreover, we analyzed the suitable value for the parameters, such as m and k. Furthermore, we took the kNN as the baseline and conducted a groups of compared experiments among kNN, LC-kNN and RC-kNN. The experimental results showed that the proposed kNN classification worked well in terms of accuracy and efficiency, and it is appropriate to deal with big data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Thus, in order to select appropriate m for bringing great effect to LC-kNN, a group of experiments were conducted on datasets by choosing different values of m for LC-kNN and RC-kNN. Specifically, the LC-kNN and RC-kNN in this group experiments were carried out on the datasets with m=10, 15, 20, 25 and 30, respectively. The comparison of classification performance and time cost of two algorithms (i.e., RC-kNN and LC-kNN, respectively) with different values of m are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Classification accuracy of LC-kNN on nine dataset with different k</figDesc><graphic coords="13,124.80,248.30,118.20,79.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,95.00,53.38,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Produce m cluster centers using LSC algorithm, denoted by C 1 ,C 2 ,C 3 ,......,C m ; 2 Compute the distance D(y,C i ) between test sample y and each cluster center, denoted by D(y,C i ), i=1,2,...,m; 3 Compute the nearest cluster center C i to y, C i =min{D(y,C i )}, i=1,2,...,m; 4 Using the corresponding cluster of C i as the training dataset, denoted by NewX</figDesc><table><row><cell>corresponding cluster as the new training dataset for each test sample. Due to</cell></row><row><cell>conducting the clusters with high similarity within a cluster, we apply kNN to classify</cell></row><row><cell>test samples in the new training dataset. In this way, the proposed algorithm still</cell></row><row><cell>guarantees relatively high classification accuracy. We list the pseudo of our proposed</cell></row><row><cell>method in Algorithm 2.</cell></row><row><cell>Algorithm 2: The pseudo of LC-kNN algorithm.</cell></row><row><cell>Input: Training dataset, test samples Y;</cell></row><row><cell>Output: Class label;</cell></row><row><cell>1</cell></row></table><note><p><p><p> ; Cluster number k; Output: k clusters; 1 Produce p landmark points using the k-means method; 2 Construct a landmark matrix Z between data points and landmark samples, with the affinity calculated according to Eq.(1); 3 Compute the first k eigenvectors of WW T , denoted by U=[u 1 , u 2 , ..., u k ]; 4 Compute V=[v 1 , v 2 , ..., v k ] according to Eq.(4); 5 Each row of V is a data point and apply k-means to get the clusters.</p>3.2 Testing process</p>Supposing that we already produce k clusters and cluster centers by LSC algorithm, we then find out the nearest cluster center for each test sample and use the i ; 5 Apple to kNN algorithm to predict y in the training dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.9027±1.6498e-005</cell><cell>0.9355±7.1306e-006</cell></row><row><cell></cell><cell>Time</cell><cell>3.5589±0.0107</cell><cell>3.7605±0.0242</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.8964±5.1803e-005</cell><cell>0.9338±4.1625e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.4857±0.0032</cell><cell>2.7260±0.0077</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.8770±7.4889e-005</cell><cell>0.9300±4.9238e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.3202±0.0010</cell><cell>2.5157±0.01928</cell></row></table><note><p>. Classification accuracy (mean±standard deviation) and Time cost (seconds: mean± standard deviation) on USPS dataset at different values of m.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean± standard deviation) on MNIST dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.7221±4.8878e-005</cell><cell>0.8389±3.1656e-005</cell></row><row><cell></cell><cell>time</cell><cell>2.9369±0.0508</cell><cell>3.5504±0.0927</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.6840±2.3333e-004</cell><cell>0.8364±2.3136e-005</cell></row><row><cell></cell><cell>time</cell><cell>2.8905±0.0456</cell><cell>3.1222±0.01397</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.6657±2.4739e-004</cell><cell>0.8353±3.3233e-005</cell></row><row><cell></cell><cell>time</cell><cell>2.0564±0.0011</cell><cell>2.1490±0.0065</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.6478±2.2689e-004</cell><cell>0.8338±8.7844e-005</cell></row><row><cell></cell><cell>time</cell><cell>1.8240±0.0020</cell><cell>2.1148±0.0094</cell></row><row><cell>30</cell><cell>Accuracy</cell><cell>0.6396±6.9156e-005</cell><cell>0.8313±3.8678e-005</cell></row><row><cell></cell><cell>time</cell><cell>1.5457±0.0002</cell><cell>1.7274±0.0011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.9311±5.0989e-005</cell><cell>0.9526±1.4511e-005</cell></row><row><cell></cell><cell>time</cell><cell>23.3933±0.9677</cell><cell>28.5940±3.2405</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.9252±1.0573e-004</cell><cell>0.9494±1.3378e-005</cell></row><row><cell></cell><cell>time</cell><cell>18.0106±0.2434</cell><cell>23.1904±1.0894</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.9166±2.8267e-005</cell><cell>0.9411±5.4699e-004</cell></row><row><cell></cell><cell>time</cell><cell>12.7685±0.0966</cell><cell>16.2759±0.8880</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.9150±7.0000e-005</cell><cell>0.9321±6.4810e-004</cell></row><row><cell></cell><cell>time</cell><cell>9.9201±0.3696</cell><cell>13.8645±1.5093</cell></row><row><cell>30</cell><cell>Accuracy</cell><cell>0.9079±1.0366e-004</cell><cell>0.9192±5.3796e-004</cell></row><row><cell></cell><cell>time</cell><cell>8.4064±0.0784</cell><cell>11.3922±0.0658</cell></row></table><note><p>. Classification accuracy (mean±standard deviation) and Time cost (seconds: mean± standard deviation) on GISETTE dataset at different values of m.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean± standard deviation) on LETTER dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean± standard deviation) on PENDIGITS dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.9452±3.5382e-005</cell><cell>0.9721±4.7991e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.3380±0.0041</cell><cell>2.4056±0.0101</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.9316±1.0341e-004</cell><cell>0.9711±6.0196e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.5451±0.0011</cell><cell>2.5709±0.0089</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.9163±1.5515e-004</cell><cell>0.9700±2.5390e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.2233±6.4795e-005</cell><cell>2.2554±2.1569e-004</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.9216±1.5677e-004</cell><cell>0.9687±3.5642e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.5270±0.0056</cell><cell>2.5468±0.0083</cell></row><row><cell>30</cell><cell>Accuracy</cell><cell>0.9088±1.8409e-004</cell><cell>0.9683±1.5809e-006</cell></row><row><cell></cell><cell>time</cell><cell>2.1805±7.4785e-005</cell><cell>2.2022±8.9611e-005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Classification accuracy (mean±standard deviation) and Time (seconds: mean± standard deviation) on SATIMAGE dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.8603±8.9122e-005</cell><cell>0.8883±8.1139e-006</cell></row><row><cell></cell><cell>time</cell><cell>1.2868±6.5495e-005</cell><cell>1.3027±1.1429e-004</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.7917±3.1680e-005</cell><cell>0.9468±3.7244e-006</cell></row><row><cell></cell><cell>time</cell><cell>3.8583±0.0332</cell><cell>3.9337±0.0152</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.8418±8.8847e-005</cell><cell>0.8884±6.8028e-006</cell></row><row><cell></cell><cell>time</cell><cell>1.2292±3.2277e-005</cell><cell>1.2463±4.3126e-005</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.7283±1.1039e-004</cell><cell>0.9421±8.8449e-006</cell></row><row><cell></cell><cell>time</cell><cell>3.5062±0.0061</cell><cell>3.6287±0.0052</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean ±standard deviation) on ADNC dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.7024±0.0010</cell><cell>0.7667±0.0019</cell></row><row><cell></cell><cell>time</cell><cell>0.0310±2.7390e-004</cell><cell>0.0333±1.9201e-005</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.6857±0.0014</cell><cell>0.7500±0.0013</cell></row><row><cell></cell><cell>time</cell><cell>0.0308±7.9360e-006</cell><cell>0.0325±3.3635e-005</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.6619±0.0029</cell><cell>0.7143±0.0028</cell></row><row><cell></cell><cell>time</cell><cell>0.0295±3.0063e-006</cell><cell>0.0313±6.8908e-006</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.6548±0.0039</cell><cell>0.7071±0.0038</cell></row><row><cell></cell><cell>time</cell><cell>0.0281±4.8219e-007</cell><cell>0.0286±7.0881e-007</cell></row><row><cell>30</cell><cell>Accuracy</cell><cell>0.6619±0.0015</cell><cell>0.7190±0.0031</cell></row><row><cell></cell><cell>time</cell><cell>0.0306±1.2641e-005</cell><cell>0.0326±5.3013e-006</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean ±standard deviation) on psMCI dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.4792±0.0082</cell><cell>0.5833±0.0089</cell></row><row><cell></cell><cell>time</cell><cell>0.0168±4.5147e-007</cell><cell>0.0171±1.4130e-006</cell></row><row><cell>15</cell><cell>Accuracy</cell><cell>0.5125±0.0019</cell><cell>0.6042±0.0040</cell></row><row><cell></cell><cell>time</cell><cell>0.0167±9.4536e-007</cell><cell>0.0173±5.6957e-007</cell></row><row><cell>20</cell><cell>Accuracy</cell><cell>0.5458±0.0060</cell><cell>0.6500±0.0035</cell></row><row><cell></cell><cell>time</cell><cell>0.0170±1.2416e-006</cell><cell>0.0188±1.1793e-005</cell></row><row><cell>25</cell><cell>Accuracy</cell><cell>0.5333±0.0053</cell><cell>0.6417±0.0035</cell></row><row><cell></cell><cell>time</cell><cell>0.0163±4.9822e-007</cell><cell>0.0286±7.0881e-007</cell></row><row><cell>30</cell><cell>Accuracy</cell><cell>0.5000±0.0042</cell><cell>0.6125±0.0019</cell></row><row><cell></cell><cell>time</cell><cell>0.0166±8.6385e-007</cell><cell>0.0250±2.2840e-004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Classification accuracy (mean±standard deviation) and Time cost (seconds: mean ±standard deviation) on MCINC dataset at different values of m.</figDesc><table><row><cell>m</cell><cell>Criterion</cell><cell>RC-kNN</cell><cell>LC-kNN</cell></row><row><cell>10</cell><cell>Accuracy</cell><cell>0.5476±0.0049</cell><cell>0.6159±0.0045</cell></row><row><cell></cell><cell>time</cell><cell>0.0468±2.8577e-005</cell><cell>0.0506±5.9543e-005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Classification Accuracy and Time Cost of three algorithm on nine dataset</figDesc><table><row><cell>Dataset</cell><cell cols="2">RC-kNN</cell><cell cols="2">LC-kNN</cell><cell cols="2">kNN</cell></row><row><cell></cell><cell>Accuracy</cell><cell>time</cell><cell>Accuracy</cell><cell>time</cell><cell>Accuracy</cell><cell>time</cell></row><row><cell>USPS</cell><cell>0.9027</cell><cell>3.5589</cell><cell>0.9355</cell><cell>3.7605</cell><cell>0.9482</cell><cell>32.8764</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are supported by the China 863 Program (Grant No: 2012AA011005), the China 973 Program (Grant No: 2013CB329404), the Natural Science Foundation of China (Grants No: 61170131, 61450001 and 61263035), the Guangxi Natural Science Foundation for Teams of Innovation and Research (Grant No: 2012GXNSFGA06000 4), the funding of Guangxi 100 Plan, and the Guangxi "Bagui" Teams for Innovation and Research, Innovation Project of Guangxi Graduate Education (Grant NO: YCSZ2 015095, YCSZ2015096).</p><p>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and and algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advance in Neural Information Processing Systems 14</title>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans-on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Scale Spectral Clustering with Landmark-Based Representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of kernel and spectral methods for clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Filippone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Camestra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="176" to="190" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D Object Retrieval and Recognition with Hypergraph Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A nearest neighbor bootstrap for resampling hydrologic time series</title>
		<author>
			<persName><forename type="first">U</forename><surname>Lall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Resources Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="679" to="693" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A density-based method for reducing the amount of training data in kNN text classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Research and Development</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="545" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large graph construction for scalable semi-supervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27 th International Conference on Machine Learning</title>
		<meeting>the 27 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Database classification for multi-database mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information System</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="88" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthesizing High-Frequency Rules from Different Data Sources</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient mining of both positive and negative association rules</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="405" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">KNN-CF approach: Incorporating certainty factor to knn classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Informatics Bulletin</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized dimension-reduction framework for recent-biased time series analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Fast image Classification Algorithm using Support vector Machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCTD</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparse hashing for fast multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Information Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linear cross-modal hashing for efficient multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by mixed kernel canonical correlation analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3003" to="3016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-taught dimensionality reduction on the high-dimensional small sized data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="229" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Sparse Embedding and Least Variance Encoding Approach to Hashing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Missing Value Estimation for Mixed-Attribute Datasets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-to-Shot Tag Propagation by Graph Sparse Group Lasso</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="633" to="646" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel matrix-similarity based loss function for joint regression and classification in ad diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matrix-similarity based loss function and feature selection for alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3089" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Block-Row Sparse Multiview Multilabel Learning for Image Classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">His research interests include data analysis and smart pattern discovery</title>
	</analytic>
	<monogr>
		<title level="m">He has published over 50 international journal papers and over 60 international conference papers. He has won over 10 nation-class grants, such as the China NSF, China 863 Program, China 973 Program, and Australia Large ARC. He is an Editor-in-Chief for International Journal of Information Quality and Computing, and is served as</title>
		<meeting><address><addrLine>Guilin, China; Australia</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Institute of School of Computer Science and Information Technology at the Guangxi Normal University ; D. degree in Computer Science from Deakin University</orgName>
		</respStmt>
	</monogr>
	<note>He holds a Ph</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
