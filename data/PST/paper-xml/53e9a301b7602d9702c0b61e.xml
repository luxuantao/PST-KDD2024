<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive and Resource-Aware Mining of Frequent Sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><surname>Orlando</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università Ca&apos; Foscari</orgName>
								<address>
									<settlement>Venezia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Palmerini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università Ca&apos; Foscari</orgName>
								<address>
									<settlement>Venezia</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Istituto ISTI</orgName>
								<orgName type="institution">Consiglio Nazionale delle Ricerche (CNR)</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Perego</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Istituto ISTI</orgName>
								<orgName type="institution">Consiglio Nazionale delle Ricerche (CNR)</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Istituto ISTI</orgName>
								<orgName type="institution">Consiglio Nazionale delle Ricerche (CNR)</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive and Resource-Aware Mining of Frequent Sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">70065B5E43097C261D2CAF57E02F410A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of an algorithm that mines frequent sets from transactional databases may severely depend on the specific features of the data being analyzed. Moreover, some architectural characteristics of the computational platform used -e.g. the available main memorycan dramatically change its runtime behavior. In this paper we present DCI (Direct Count &amp; Intersect), an efficient algorithm for discovering frequent sets from large databases. Due to the multiple heuristics strategies adopted, DCI can adapt its behavior not only to the features of the specific computing platform, but also to the features of the dataset being mined, so that it results very effective in mining both short and long patterns from sparse and dense datasets. Finally we also discuss the parallelization strategies adopted in the design of ParDCI, a distributed and multi-threaded implementation of DCI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Association Rule Mining (ARM), one of the most popular topic in the KDD field, regards the extractions of association rules from a database of transactions D. Each rule has the form X ⇒ Y , where X and Y are sets of items (itemsets), such that (X ∩ Y ) = ∅. A rule X ⇒ Y holds in D with a minimum confidence c and a minimum support s, if at least the c% of all the transactions containing X also contains Y , and X ∪ Y is present in at least the s% of all the transactions of the database. In this paper we are interested in the most computationally expensive phase of ARM, i.e the Frequent Set Counting (FSC) one. During this phase, the set of all the frequent itemsets is built. An itemset of k items (k-itemset) is frequent if its support is greater than the fixed threshold s, i.e. the itemset occurs in at least minsup transactions (minsup = s/100 • n, where n is the number of transactions in D).</p><p>The computational complexity of the FSC problem de-rives from the exponential size of its search space P(M ), i.e. the power set of M , where M is the set of items contained in the various transactions of D. A way to prune P(M ) is to restrict the search to itemsets whose subsets are all frequent. The Apriori algorithm <ref type="bibr" target="#b4">[5]</ref> exactly exploits this pruning technique, and visits breadth-first P(M ) for counting itemset supports. At each iteration k, Apriori generates C k , a set of candidate k-itemsets, and counts the occurrences of these candidates in the transactions. The candidates in C k for which the the minimum support constraint holds are then inserted into F k , i.e. the set of frequent k-itemsets, and the next iteration is started. Other algorithms adopt instead a depth-first visit of P(M ) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref>. In this case the goal is to discover long frequent itemsets first, thus saving the work needed for discovering frequent itemsets included in long ones. Unfortunately, while it is simple to derive all the frequent itemsets from the maximal ones, the same does not hold for their supports, which require a further counting step. In the last years several variations to the original Apriori algorithm, as well as many parallel implementations, have been proposed. We can recognize two main methods for determining the supports of the various itemsets present in P(M ): a counting-based approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, and an intersection-based one <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>. The former one, also adopted in Apriori, exploits a horizontal dataset and counts how many times each candidate k-itemset occurs in every transaction. The latter method, on the other hand, exploits a vertical dataset, where a tidlist, i.e. a list of transaction identifiers (tids), is associated with items (or itemsets), and itemset supports are determined through tidlist intersections. The support of a k-itemset c can thus be computed either by a k-way intersection, i.e. by intersecting the k tidlists associated with the k items included in c, or by a 2-way intersection, i.e. by intersecting the tidlists associated with a pair of frequent (k -1)-itemsets whose union is equal to c. Recently another category of methods, i.e. the pattern growth ones, have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. FP-growth <ref type="bibr" target="#b10">[11]</ref> is the best representant of this kind of algorithms. It is not based on candidate generation as Apriori, but builds in memory a compact representation of the dataset, where repeated patterns are represented once along with the associated repetition counters. FP-growth does not perform well on sparse datasets <ref type="bibr" target="#b16">[17]</ref>, so the same authors recently proposed a new pattern-growth algorithm, H-mine <ref type="bibr" target="#b13">[14]</ref>, based on an innovative hyper-structure that allows the in-core dataset to be recursively projected by selecting those transactions that include a given pattern prefix.</p><p>In this paper we discuss DCI (Direct Count &amp; Intersect), a new algorithm to solve the FSC problem. We also introduce ParDCI, a parallel version of DCI, which explicitly targets clusters of SMPs. Several considerations concerning the features of real datasets to be mined and the characteristics of modern hw/sw system have motivated the design of DCI. On the one hand, transactional databases may have different peculiarities in terms of the correlations among items, so that they may result either dense or sparse. Hence, a desirable characteristic of a new algorithm should be the ability to adapt its behavior to these features. DCI, which supports this kind of adaptiveness, thus constitutes an innovation in the arena of previously proposed FSC algorithms, which often outperformed others only for specific datasets. On the other hand, modern hw/sw systems need high locality for effectively exploiting memory hierarchies and achieving high performances. Large dynamic data structures with pointers may lack in locality due to unstructured memory references. Other sources of performance limitations may be unpredictable branches. DCI tries to take advantages of modern systems by using simple array data structures, accessed by tight loops which exhibit high spatial and temporal locality. In particular, DCI exploits such techniques for intersecting tidlists, which are actually represented as bit-vectors that can be intersected very efficiently with primitive bitwise and instructions. Another issue regards I/O operations, which must be carefully optimized in order to allow DM algorithms to efficiently manage large databases. Even if the disk-stored datasets to be mined may be very large, DM algorithms usually access them sequentially with high spatial locality, so that suitable out-ofcore techniques to access them can be adopted, also taking advantage of prefetching and caching features of modern OSs <ref type="bibr" target="#b5">[6]</ref>. DCI adopts these out-of-core techniques to access large databases, prunes them as execution progresses, and starts using in-core strategies as soon as possible.</p><p>Once motivated the design requirements of DCI, we can now detail how it works. As Apriori, at each iteration DCI generates the set of candidates C k , determines their supports, and produces the set F k of the frequent k-itemsets. However, DCI adopts a hybrid approach to determine the support of the candidates. During its first iterations, DCI exploits a novel counting-based technique, accompanied by an effective pruning of the dataset, stored to disk in horizon-tal form. During the following iterations, DCI adopts a very efficient intersection-based technique. DCI starts using this technique as soon as the pruned dataset fits into the main memory.</p><p>DCI deals with dataset peculiarities by dynamically choosing between distinct heuristic strategies. For example, when a dataset is dense, identical sections appearing in several bit-vectors are aggregated and clustered, in order to reduce the number of intersections actually performed. Conversely, when a dataset is sparse, the runs of zero bits in the bit-vectors to be intersected are promptly identified and skipped. We will show how the sequential implementation of DCI significantly outperforms previously proposed algorithms. In particular, under a number of different tests and independently of the dataset peculiarities, DCI results to be faster than Apriori and FP-growth. By comparing our experimental results with the published ones obtained on the same sparse dataset, we deduced that DCI is also faster than H-mine <ref type="bibr" target="#b13">[14]</ref>. DCI performs very well on both synthetic and real-world datasets characterized by different density features, i.e. datasets from which, due to the different correlations among items, either short or long frequent patterns can be mined.</p><p>The rest of the paper is organized as follows. Section 2 describes the DCI algorithm and discusses the various adaptive heuristics adopted, while Section 3 sketches the solutions adopted to design ParDCI, the parallel version of DCI. In Section 4 we report our experimental results. Finally in Section 5 we present some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The DCI algorithm</head><p>During its initial counting-based phase, DCI exploits an out-of-core, horizontal database with variable length records. DCI, by exploiting effective pruning techniques inspired by DHP <ref type="bibr" target="#b12">[13]</ref>, trims the transaction database as execution progresses. In particular, a pruned dataset D k+1 is written to disk at each iteration k, and employed at the next iteration. Let m k and n k be the number of items and transactions that are included in the pruned dataset D k , where m k ≥ m k+1 and n k ≥ n k+1 . Pruning the dataset may entail a reduction in I/O activity as the algorithm progresses, but the main benefits come from the reduced computation required for subset counting at each iteration k, due to the reduced number and size of transactions. As soon as the pruned dataset becomes small enough to fit into the main memory, DCI adaptively changes its behavior, builds a vertical layout database in-core, and starts adopting an intersection-based approach to determine frequent sets. Note, however, that DCI continues to have a levelwise behavior. At each iteration, DCI generates the candidate set C k by finding all the pairs of (k -1)-itemsets that are included in F k-1 and share a common (k -2)-prefix. Since F k-1 is lexicographically ordered, the various pairs occur in close positions, and candidate generation is performed with high spatial and temporal locality. Only during the DCI counting-phase, C k is further pruned by checking whether also all the other subsets of a candidate are included in F k-1 . Conversely, during the intersection-based phase, since our intersection method is able to quickly determine the support of a candidate itemset, we found much more profitable to avoid this further check. While during its counting-based phase DCI has to maintain C k in main memory to search candidates and increment associated counters, this is no longer needed during the intersection-based phase. As soon as a candidate kitemset is generated, DCI determines its support on-the-fly by intersecting the corresponding tidlists. This is an important improvement over other Apriori-like algorithms, which suffer from the possible huge memory requirements due to the explosion of the size of C k <ref type="bibr" target="#b10">[11]</ref>.</p><p>DCI makes use of a large body of out-of-core techniques, so that it is able to adapt its behavior also to machines with limited main memory. Datasets are read/written in blocks, to take advantage of I/O prefetching and system pipelining <ref type="bibr" target="#b5">[6]</ref>. The outputs of the algorithm, e.g. the various frequent sets F k , are written to files that are mmap-ped into memory during the next iteration for candidate generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Counting-based phase</head><p>Since the counting-based approach is used only for few iterations, in the following we only sketch its main features. Further details about DCI counting technique can be found in <ref type="bibr" target="#b11">[12]</ref>, where we proposed an effective algorithm for mining short patterns. In the first iteration, similarly to all FSC algorithms, DCI exploits a vector of counters, which are directly addressed through item identifiers. For k ≥ 2, instead of using complex data structures like hash-trees or prefixtrees, DCI uses a novel Direct Count technique that can be thought as a generalization of the technique used for k = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The technique uses a prefix table, PREFIX</head><formula xml:id="formula_0">k [ ], of size m k 2 .</formula><p>In particular, each entry of PREFIX k [ ] is associated with a distinct ordered prefix of two items. For k = 2, PREFIX k [ ] can directly contain the counters associated with the various candidate 2-itemsets, while, for k &gt; 2, each entry of PREFIX k [ ] contains the pointer to the contiguous section of ordered candidates in C k sharing the same prefix. To permit the various entries of PREFIX k [ ] to be directly accessed, we devised an order preserving, minimal perfect hash function. This prefix table is thus used to count the support of candidates in C k as follows. For each transaction t = {t 1 , . . . , t |t| }, we select all the possible 2-prefixes of all k-subsets included in t. We then exploit PREFIX k [ ] to find the sections of C k which must be visited in order to check set-inclusion of candidates in transaction t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intersection-based phase</head><p>Since the counting-based approach becomes less efficient as k increases <ref type="bibr" target="#b14">[15]</ref>, DCI starts its intersection-based phase as soon as possible. Unfortunately, the intersectionbased method needs to maintain in memory the vertical representation of the pruned dataset. So, at iteration k, k ≥ 2, DCI checks whether the pruned dataset D k may fit into the main memory. When the dataset becomes small enough, its vertical in-core representation is built on the fly, while the transactions are read and counted against C k . The intersection-based method thus starts at the next iteration.</p><p>The vertical layout of the dataset is based on fixed length records (tidlists), stored as bit-vectors. The whole vertical dataset can thus be seen as a bidimensional bit-array VD[ ][ ], whose rows correspond to the bit-vectors associated with non pruned items. Therefore, the amount of memory required to store</p><formula xml:id="formula_1">VD[ ][ ] is m k × n k bits.</formula><p>At each iteration of its intersection-based phase, DCI computes F k as follows. For each candidate k-itemset c, we and-intersect the k bit-vectors associated with the items included in c (k-way intersection), and count the 1's present in the resulting bit-vector. If this number is greater or equal to minsup, we insert c into F k . Consider that a bit-vector intersection can be carried out efficiently and with high spatial locality by using primitive bitwise and instructions with word operands. As previously stated, this method does not require C k to be kept in memory: we can compute the support of each candidate c on-the-fly, as soon as it is generated.</p><p>The strategy above is, in principle, highly inefficient, because it always needs a k-way intersection to determine the support of each candidate c. Nevertheless, a caching policy could be exploited in order to save work and speed up our k-way intersection method. To this end, DCI uses a small "cache" buffer to store all the k -2 intermediate intersections that have been computed for the last candidate evaluated. Since candidate itemsets are generated in lexicographic order, with high probability two consecutive candidates, e.g. c and c , share a common prefix. Suppose that c and c share a prefix of length h ≥ 2. When we process c , we can avoid performing the first h -1 intersections since their result can be found in the cache.</p><p>To evaluate the effectiveness of our caching policy, we counted the actual number of intersections carried out by DCI on two different datasets: BMS, a real-world sparse dataset, and connect-4, a dense dataset (the characteristics of these two datasets are reported in Table <ref type="table">1</ref>). We compared this number with the best and the worst case. The best case corresponds to the adoption of a 2-way intersection approach, which is only possible if we can fully cache the tidlists associated with all the frequent (k -1)-itemsets in F k-1 . The worst case regards the adoption of a pure kway intersection method, i.e. a method that does not exploit caching at all. Figure <ref type="figure" target="#fig_0">1</ref>.(a) plots the results of this analysis on the sparse dataset for support threshold s = 0.06%, while Figure <ref type="figure" target="#fig_0">1</ref>.(b) regards the dense dataset mined with support threshold s = 80%. In both the cases the caching policy of DCI turns out to be very effective, since the actual number of intersections performed results to be very close to the best case. Moreover, DCI requires orders of magnitude less memory than a pure 2-way intersection approach, thus better exploiting memory hierarchies. We have to consider that while caching reduces the number of tidlist intersections, we also need to reduce intersection cost. To this end, further heuristics, differentiated w.r.t. sparse or dense datasets, are adopted by DCI. In order to apply the right optimization, the vertical dataset is tested for checking its density as soon as it is built. In particular, we compare the bit-vectors associated with the most frequent items, i.e., the vectors which likely need to be intersected several times since the associated items occur in many candidates. If large sections of these bit-vectors turn out to be identical, we deduce that the items are highly correlated and that the dataset is dense. In this case we adopt a specific heuristics which exploits similarities between these vectors. Otherwise the technique for sparse datasets is adopted. In the following we illustrate the two heuristics in more detail.</p><p>Sparse datasets. Sparse or moderately dense datasets originate bit-vectors containing long runs of 0's. To speedup computation, while we compute the intersection of the bit-vectors relative to the first two items c 1 and c 2 of a generic candidate itemset c = {c 1 , c 2 , . . . , c k } ∈ C k , we also identify and maintain information about the runs of 0's appearing in the resulting bit-vector stored in cache. The further intersections that are needed to determine the support of c (as well as intersections needed to process other kitemsets sharing the same 2-item prefix) will skip these runs of 0's, so that only vector segments which may contain 1's are actually intersected. Since information about the runs of 0's are computed once, and the same information is reused many times, this optimization results to be very effective. Moreover, sparse and moderately dense datasets offer the possibility of further pruning vertical datasets as computation progresses. The benefits of pruning regard the reduction in the length of the bit-vectors and thus in the cost of intersections. Note that a transaction, i.e. a column of VD, can be removed from the vertical dataset when it does not contain any of the itemsets included in F k . This check can simply be done by or-ing the intersection bit-vectors computed for all the frequent k-itemsets. However, we observed that dataset pruning is expensive, since vectors must be compacted at the level of single bits. Hence DCI prunes the dataset only if turns out to be profitable, i.e. if we can obtain a large reduction in the vector length, and the number of vectors to be compacted is small with respect to the cardinality of C k . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense datasets.</head><p>If the dataset turns out to be dense, we expect to deal with a dataset characterized by strong correlations among the most frequent items. This not only means that the bit-vectors associated with the most frequent items contain long runs of 1's, but also that they turn out to be very similar. The heuristic technique adopted by DCI for dense dataset thus works as follows: A) we reorder the columns of the vertical dataset, in order to move identical segments of the bit-vectors associated with the most frequent items to the first consecutive positions; B) since each Table <ref type="table">1</ref>. Datasets used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T25I10D10K</head><p>Synthetic dataset with 1K items and 10K transactions <ref type="bibr" target="#b10">[11]</ref>. The average size of transactions is 25, and the average size of the maximal potentially frequent itemsets is 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T25I20D100K</head><p>Synthetic dataset with 10K items and 100K transactions <ref type="bibr" target="#b10">[11]</ref>. The average size of transactions is 25, and the average size of the maximal potentially frequent itemsets is 20. 400k t10 p8 m10k 10K items and 400K transactions. The average size of transactions is 10, and the average size of the maximal potentially frequent itemsets is 8. Synthetic dataset created with the IBM dataset generator <ref type="bibr" target="#b4">[5]</ref>.</p><p>400k t30 p16 m1k 1K items and 400K transactions. The average size of transactions is 30, and the average size of the maximal potentially frequent itemsets is 16. Synthetic dataset created with the IBM dataset generator <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t20 p8 m1k</head><p>With this notation we identify a series of synthetic datasets characterized by 1K items. The average transaction size is 20, and the average size of maximal potentially frequent itemsets is 8. The number of transactions is varied for scaling measurements. t50 p32 m1k A series of three synthetic datasets with the same number of items (1K), average transaction size of 50, and average size of maximal potentially frequent itemsets equal to 32. We used three datasets of this series with 1000k, 2000k and 3000k transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>connect-4</head><p>Publicly available dense dataset with 130 items and about 60K transactions. The maximal transaction size is 45.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BMS</head><p>Publicly available sparse dataset also known as Gazelle. 497 items and 59K transactions containing click-stream data from an e-commerce web site gazelle.com.</p><p>candidate is likely to include several of these most frequent items, we avoid repeatedly intersecting the identical segments of the corresponding vectors. This technique may save a lot of work because (1) the intersection of identical vector segments is done once, (2) the identical segments are usually very large, and (3), long candidate itemsets presumably contains several of these most frequent items.</p><p>The plots reported in Figure <ref type="figure" target="#fig_1">2</ref> show the effectiveness of the heuristic optimizations discussed above in reducing the average number of bitwise and operations needed to intersect a pair of bit-vectors. In particular, Figure <ref type="figure" target="#fig_1">2</ref>.(a) regards the sparse BMS dataset mined with support threshold s = 0.06%, while Figure <ref type="figure" target="#fig_1">2</ref>.(b) regards the dense dataset connect-4 mined with support threshold s = 80%. In both cases, we plotted the per-iteration cost of each bit-vector intersection in terms of bitwise and operations when either our heuristic optimizations are adopted or not. The two plots show that our optimizations for both sparse and dense datasets have the effect of reducing the intersection cost up to an order of magnitude. Note that when no optimizations are employed, the curves exactly plot the bit-vector length (in words). Finally, from the plot reported in Figure <ref type="figure" target="#fig_1">2</ref>.(a), we can also note the effect of the pruning technique used on sparse datasets. Pruning has the effect of reducing the length of the bit-vectors as execution progresses. On the other hand, when datasets are dense, the vertical dataset is not pruned, so that the length of bit-vectors remains the same for all the DCI iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ParDCI</head><p>In the following we describe the different parallelization techniques exploited for the counting-and intersectionbased phases of ParDCI, the parallel version of DCI. Since our target architecture is a cluster of SMP nodes, in both phases we distinguish between intra-node and inter-node levels of parallelism. At the inter-node level we used the message-passing paradigm through the MPI communication library, while at the intra-node level we exploited multithreading through the Posix Thread library. A Count Distribution approach is adopted to parallelize the counting-based phase, while the intersection-based phase exploits a very effective Candidate Distribution approach <ref type="bibr" target="#b3">[4]</ref>.</p><p>The counting-based phase. At the inter-node level, the dataset is statically split in a number of partitions equal to the number of SMP nodes available. The size of partitions depend on the relative powers of nodes. At each iteration k, an identical copy of C k is independently generated by each node. Then each node p reads blocks of transactions from its own dataset partition D p,k , performs subset counting, and writes pruned transactions to D p,k+1 . At the end of the iteration, an all-reduce operation is performed to update the counters associated to all candidates of C k , and all the nodes produce an identical set F k .</p><p>At the intra-node level each node uses a pool of threads, each holding a private set of counters associated with candidates. They have the task of checking in parallel candidate itemsets against chunks of transactions read from D p,k .</p><p>At the end of each iteration, a global reduction of counters take place, and a copy of F k is produced on each node.</p><p>The intersection-based phase. During the intersectionbased phase, a Candidate Distribution approach is adopted at both the inter-and intra-node levels. This parallelization schema makes the parallel nodes completely independent: inter-node communications are no longer needed for all the following iterations of ParDCI. Let us first consider the inter-node level, and suppose that the intersection-based phase is started at iteration k + 1. Therefore, at iteration k the various nodes build on-the-fly the bit-vectors representing their own in-core portions of the vertical dataset. Before starting the intersection-based phase, the partial vertical datasets are broadcast to obtain a complete replication of the whole vertical dataset on each node. The frequent set F k (i.e., the set computed in the last counting-based iteration) is then partitioned on the basis of itemset prefixes. A disjoint partition F p,k of F k is thus assigned to each node p, where p F p,k = F k . It is worth remarking that this partitioning entails a Candidate Distribution schema for all the following iterations, according to which each node p will be able to generate a unique C p k (k &gt; k) independently of all the other nodes, where</p><formula xml:id="formula_2">C p k ∩ C p k = ∅ if p = p , and p C p k = C k .</formula><p>At the intra-node level, a similar Candidate Distribution approach is employed, but at a finer granularity by using dynamic scheduling to ensure load balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>The DCI algorithm is currently available in two versions, a MS-Windows one, and a Linux one. ParDCI, which exploits the MPICH MPI and the pthread libraries, is currently available only for the Linux platform. We used the MS-Windows version of DCI to compare its performance with other FSC algorithms. For test comparisons we used the FP-growth algorithm<ref type="foot" target="#foot_0">1</ref> , and the Christian Borgelt's implementation of Apriori<ref type="foot" target="#foot_1">2</ref> . For the sequential tests we used a Windows-NT workstation equipped with a Pentium II 350 MHz processor, 256 MB of RAM memory and a SCSI-2 disk. For testing ParDCI performance, we employed a small cluster of three Pentium II 233MHz 2-way SMPs, for a total of six processors. Each SMP is equipped with 256 MBytes of main memory and a SCSI disk. For the tests, we used both synthetic and real datasets by varying the minimum support threshold s. The characteristics of the datasets used are reported in Table <ref type="table">1</ref>.</p><p>DCI performances and comparisons. Figure <ref type="figure" target="#fig_2">3</ref> reports the total execution times obtained running Apriori, FPgrowth, and our sequential DCI algorithm on some datasets described in Table <ref type="table">1</ref> as a function of the support threshold s. In all the tests conducted, DCI outperforms FP-growth with speedups up to 8. Of course, DCI also remarkably outperforms Apriori, in some cases for more than one order of magnitude. For connect-4, the dense dataset, the curve of Apriori is not shown, due to the relatively too long execution times. Note that, accordingly to <ref type="bibr" target="#b16">[17]</ref>, on the real-world sparse dataset BMS (also known as Gazelle), Apriori turned out to be faster than FP-growth. To overcome such bad performance results on sparse datasets, the same authors of FPgrowth recently proposed a new pattern-growth algorithm, H-mine <ref type="bibr" target="#b13">[14]</ref>. By comparing our experimental results with the published execution times on the BMS dataset, we deduced that DCI is also faster than H-mine. For s = 0.06%, we obtained an execution time of about 7 sec., while Hmine completes in about 40 sec. on a faster machine.</p><p>The encouraging results obtained with DCI are due to both the efficiency of the counting method exploited during early iterations, and the effectiveness of the intersectionbased approach used when the pruned vertical dataset fits into the main memory. For only a dataset, namely T25I10D10K, FP-growth turns out to be slightly faster than DCI for s = 0.1%. The cause of this behavior is the size of C 3 , which in this specific case results much larger than the final size of F 3 . Hence, DCI has to carry out a lot of useless work to determine the support of many candidate itemsets, which will eventually result to be not frequent. In this case FP-growth is faster than DCI since it does not require candidate generation.  We also tested the scale-up behavior of DCI when both the size of the dataset and the size of RAM installed in the PC vary. The datasets employed for these tests belong to the series t20 p8 m1k (see Table <ref type="table">1</ref>) mined with support threshold s = 0.5%, while the available RAM was changed from 64MB to 512MB by physically plugging additional memory into the PC main board. PC equipped with a different amount of memory. As it can be seen from Figure <ref type="figure" target="#fig_4">4</ref>.(a), DCI scales linearly also on machines with a few memory. Due to its adaptiveness and the use of efficient out-of-core techniques, it is able to modify its behavior in function of the features of the dataset mined and the computational resources available. For example, in the tests conducted with the largest dataset containing two millions of transactions, the in-core intersection-based phase was started at the sixth iteration when only 64MB of RAM were available, and at the third iteration when the available memory was 512MB. On the other hand the results reported in Figure <ref type="figure" target="#fig_4">4</ref>.(b) show that FP-growth requires much more memory than DCI, and is not able to adapt itself to memory availability. For example, in the tests conducted with 64MB of RAM, FP-growth requires less than 30 seconds to mine the dataset with 200k transactions, but when we double the size of the dataset to 400k transactions, FPgrowth execution time becomes 1303 seconds, more than 40 times higher, due to an heavy page swapping activity. For what regard sparse datasets, we used the synthetic dataset series identified as t50 p32 m1k in Table <ref type="table">1</ref>. We varied the total number of transactions from 1000k to 3000k. In the following we will identify the various synthetic datasets on the basis of their number of transactions, i.e. 1000k, 2000k, and 3000k. Figure <ref type="figure" target="#fig_6">5</ref>.(b) plots the speedups obtained on the three synthetic datasets for a given support threshold (s = 1.5%), as a function of the number of processors used. Consider that, since our cluster is composed of three 2-way SMPs, we mapped tasks on processors always using the minimum number of SPMP nodes (e.g., when we used 4 processors, we actually employed 2 SMP nodes). This implies that experiments performed on either 1 or 2 processors actually have identical memory and disk resources available, whereas the execution on 4 processors benefit from a double amount of such resources. According to the tests above, ParDCI showed a speedup that, in some cases, is close to the optimal one. Considering the results obtained with one or two processors, one can note that the slope of the speedup curve is relatively worse than its theoretical limit, due to resource sharing and thread implementation overheads at the inter-node level. Nevertheless, when additional SMPs are employed, the slope of the curve improves. The strategies adopted for partitioning dataset and candidates on our homogeneous cluster of SMPs sufficed for balancing the workload. In our tests we observed a very limited imbalance. The differences in the execution times of the first and last node to end execution were always below the 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>DCI uses different approaches for extracting frequent patterns: counting-based during the first iterations and intersection-based for the following ones. Adaptiveness and resource awareness are the main innovative features of the algorithm. On the basis of the characteristics of the dataset mined, DCI chooses at run-time which optimization to adopt for reducing the cost of mining. Dataset pruning and effective out-of-core techniques are exploited during the counting-based phase, while the intersection-based phase works in core, and is started only when the pruned dataset can fit into the main memory. As a result, our algorithm can manage efficiently, also on machines with limited physical memory, very large datasets from which, due to the different correlations among items, either short or long frequent patterns can be mined.</p><p>The experimental evaluations demonstrated that DCI significantly outperforms Apriori and FP-growth on both synthetic and real-world datasets. In many cases the performance improvements are impressive. Moreover, ParDCI, the parallel version of DCI, exhibits excellent scaleups and speedups on our homogeneous cluster of SMPs. The variety of datasets used and the large amount of tests con-ducted permit us to state that the performances of DCI are not influenced by dataset characteristics, and that our optimizations are very effective and general. To share our efforts with the data mining community, we made the DCI binary code available for research purposes at http://www.miles.cnuce.cnr.it/∼palmeri/datam/DCI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Evaluation of DCI intersection caching policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Evaluation of DCI optimization heuristics for sparse and dense datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Total execution times for DCI, Apriori, and FP-growth on various datasets as a function of the support threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Total execution times of (a) DCI, and (b) FP-growth, on datasets in the series t20 p8 m1k (s = 0.5%) on a PC equipped with different RAM sizes as a function of the number of transactions (ranging from 100K to 2M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>(a) and 4.(b) plot several curves representing the execution times of DCI and FP-growth, respectively, as a function of the number of transactions contained in the dataset processed. Each curve plotted refers to a series of tests conducted with the same</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a): Dense dataset connect-4: completion times of DCI and ParDCI varying the minimum support threshold. (b): Speedup for sparse datasets 1000K, 2000K and 3000K with s = 1.5% Performance evaluation of ParDCI. We evaluated ParDCI on both dense and sparse datasets. First we compared the performance of DCI and ParDCI on the dense dataset connect-4, for which we obtained very good speedups. Figure 5.(a) plots total execution times as functions of the support thresholds s. ParDCI-2 corresponds to the pure multithread version running on a single 2-way SMP, while ParDCI-4 and ParDCI-6 also exploit internode parallelism, and run, respectively, on two and three 2-way SMPs. For what regard sparse datasets, we used</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We acknowledge Prof. Jiawei Han for kindly providing us the latest, fully optimized, binary version of FP-growth.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://fuzzy.cs.uni-magdeburg.de/∼borgelt</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Tree Projection Algorithm for Generation of Frequent Itemsets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V V</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPDC</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Special Issue on High Performance Data Mining</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth first generation of long patterns</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V V</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 6th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="108" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Discovery of Association Rules in Large Databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Inkeri</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="307" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel mining of association rules</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="962" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Mining Association Rules in Large Databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th VLDB Conf</title>
		<meeting>of the 20th VLDB Conf</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Implementation issues in the design of I/O intensive data mining applications on clusters of workstations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baraglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laforenza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palmerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd HPDM Workshop, (IPDPS-2000)</title>
		<meeting>of the 3rd HPDM Workshop, (IPDPS-2000)<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS 1800 Spinger-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="350" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining frequent patterns with counting inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficiently Mining Long Patterns from Databases</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bayardo</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of the ACM SIGMOD Int. Conf. on Management of Data<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data organization and access for efficient data mining</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Dunkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandit</forename><surname>Soparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Int. Conf. on Data Engineering</title>
		<meeting>of the 15th Int. Conf. on Data Engineering<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="522" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable Parallel Data Mining for Association Rules</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="352" />
			<date type="published" when="2000-06">May/June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining Frequent Patterns without Candidate Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of the ACM SIGMOD Int. Conf. on Management of Data<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing the Apriori Algorithm for Frequent Set Counting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palmerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3 rd Int. Conf. on Data Warehousing and Knowledge Discovery</title>
		<meeting>of the 3 rd Int. Conf. on Data Warehousing and Knowledge Discovery<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">2114</biblScope>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Effective Hash Based Algorithm for Mining Association Rules</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1995 ACM SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of the 1995 ACM SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hyper-Structure Mining of Frequent Patterns in Large Databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>H-Mine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 IEEE ICDM Conf</title>
		<meeting>of the 2001 IEEE ICDM Conf<address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Efficient Algorithm for Mining Association Rules in Large Databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Savasere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Omiecinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Navathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21th VLDB Conf</title>
		<meeting>of the 21th VLDB Conf<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="432" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable algorithms for association mining</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="372" to="390" />
			<date type="published" when="2000-06">May/June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real World Performance of Association Rule Algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD-2001</title>
		<meeting>of KDD-2001</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
