<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Speech-Driven Facial Animation with Temporal GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07-19">19 Jul 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iBUG Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iBUG Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">iBUG Group</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Speech-Driven Facial Animation with Temporal GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07-19">19 Jul 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1805.09313v4[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech-driven facial animation is the process which uses speech signals to automatically synthesize a talking character. The majority of work in this domain creates a mapping from audio features to visual features. This often requires post-processing using computer graphics techniques to produce realistic albeit subject dependent results. We present a system for generating videos of a talking head, using a still image of a person and an audio clip containing speech, that doesn't rely on any handcrafted intermediate features. To the best of our knowledge, this is the first method capable of generating subject independent realistic videos directly from raw audio. Our method can generate videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements 1 . We achieve this by using a temporal GAN with 2 discriminators, which are capable of capturing different aspects of the video. The effect of each component in our system is quantified through an ablation study. The generated videos are evaluated based on their sharpness, reconstruction quality, and lip-reading accuracy. Finally, a user study is conducted, confirming that temporal GANs lead to more natural sequences than a static GAN-based approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial Sythesizer</head><p>Figure <ref type="figure" target="#fig_3">1</ref>: The proposed end-to-end face synthesis model, capable of producing realistic sequences of faces using one still image and an audio track containing speech. The generated sequences exhibit smoothness and natural expressions such as blinks and frowns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial animation plays a major role in computer generated imagery because the face is the primary outlet of information. The problem of generating realistic talking heads is multifaceted, requiring highquality faces, lip movements synchronized with the audio, and plausible facial expressions. This is especially challenging because humans are adept at picking up subtle abnormalities in facial motion and audio-visual synchronization.</p><p>Of particular interest is speech-driven facial animation since speech acoustics are highly correlated with facial movements <ref type="bibr" target="#b26">[27]</ref>. These systems could simplify the film animation process through automatic generation from the voice acting. They can also be applied in movie dubbing to achieve better lip-syncing results. Moreover, they can be used to generate parts of the face that are occluded or missing in a scene. Finally, this technology can improve band-limited visual telecommunications by either generating the entire visual content based on the audio or filling in dropped frames.</p><p>The majority of research in this domain has focused on mapping audio features (e.g. MFCCs) to visual features (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces <ref type="bibr" target="#b11">[12]</ref>. Some methods avoid the use of CG by selecting frames from a person-specific database and combining them to form a video <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. Regardless of which approach is used these methods are subject dependent and are often associated with a considerable overhead when transferring to new speakers.</p><p>Subject independent approaches have been proposed that transform audio features to video frames <ref type="bibr" target="#b4">[5]</ref> but there is still no method to directly transform raw audio to video. Furthermore, many methods restrict the problem to generating only the mouth. Even techniques that generate the entire face are primarily focused on obtaining realistic lip movements, and typically neglect the importance of generating natural facial expressions. Some methods generate frames based solely on present information <ref type="bibr" target="#b4">[5]</ref>, without taking into account the facial dynamics. This makes generating natural sequences, which are characterized by a seamless transition between frames, challenging. Some video generation methods have dealt with this problem by generating the entire sequence at once <ref type="bibr" target="#b24">[25]</ref> or in small batches <ref type="bibr" target="#b19">[20]</ref>. However, this introduces a lag in the generation process, prohibiting their use in real-time applications and requiring fixed length sequences for training.</p><p>We propose a temporal generative adversarial network (GAN), capable of generating a video of a talking head from an audio signal and a single still image (see Fig. <ref type="figure" target="#fig_3">1</ref> ). First, our model captures the dynamics of the entire face producing not only synchronized mouth movements but also natural facial expressions, such as eyebrow raises, frowns and blinks. This is due to the use of an RNN-based generator and sequence discriminator, which also gives us the added advantage of handling variable length sequences. Natural facial expressions play a crucial role in achieving truly realistic results and their absence is often a clear tell-tale sign of generated videos. This is exploited by methods such as the one proposed in <ref type="bibr" target="#b14">[15]</ref>, which detects synthesized videos based on the existence of blinks.</p><p>Secondly, our method is subject independent, does not rely on handcrafted audio or visual features, and requires no post-processing. To the best of our knowledge, this is the first end-to-end technique that generates talking faces directly from the raw audio waveform.</p><p>The third contribution of this paper is a comprehensive assessment of the performance of the proposed method. An ablation study is performed on the model in order to quantify the effect of each component in the system. We measure the image quality using popular reconstruction and sharpness metrics, and compare it to a non-temporal approach. Additionally, we propose using lip reading techniques to verify the accuracy of the spoken words and face verification to ensure that the identity of the speaker is maintained throughout the sequence. Evaluation is performed in a subject independent way on the GRID <ref type="bibr" target="#b5">[6]</ref> and TCD TIMIT <ref type="bibr" target="#b10">[11]</ref> datasets, where our model achieves truly natural results. Finally, the realism of the videos is assessed through an online Turing test, where users are shown videos and asked to identify them as either real or generated.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech-Driven Facial Animation</head><p>The problem of speech-driven video synthesis is not new in computer vision and has been the subject of interest for decades. Yehia et al. <ref type="bibr" target="#b26">[27]</ref> first examined the relationship between acoustics, vocal-tract and facial motion, showing a strong correlation between visual and audio features and a weak coupling between head motion and the fundamental frequency of the speech signal <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some of the earliest methods for facial animation relied on hidden Markov models (HMMs) to capture the dynamics of the video and speech sequences. Simons and Cox <ref type="bibr" target="#b20">[21]</ref> used vector quantization to achieve a compact representation of video and audio features, which were used as the states for their HMM. The HMM was used to recover the most likely mouth shapes for a speech signal. A similar approach is used in <ref type="bibr" target="#b25">[26]</ref> to estimate the sequence of lip parameters. Finally, the Video Rewrite <ref type="bibr" target="#b2">[3]</ref> method relies on the same principals to obtain a sequence of triphones that are used to look up mouth images from a database.</p><p>Although HMMs were initially preferred to neural networks due to their explicit breakdown of speech into intuitive states, recent advances in deep learning have resulted in neural networks being used in most modern approaches. Like past attempts, most of these methods aim at performing a featureto-feature translation. A typical example of this is <ref type="bibr" target="#b22">[23]</ref>, which uses a deep neural network (DNN) to transform a phoneme sequence into a sequence of shapes for the lower half of the face. Using phonemes instead of raw audio ensures that the method is subject independent. Similar deep architectures based on recurrent neural networks (RNNs) have been proposed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>, producing realistic results but are subject dependent and require retraining or re-targeting steps to adapt to new faces.</p><p>Convolutional neural networks (CNN) are used in <ref type="bibr" target="#b11">[12]</ref> to transform audio features to 3D meshes of a specific person. This system is conceptually broken into sub-networks responsible for capturing articulation dynamics and estimating the 3D points of the mesh. Finally, Chung et al. <ref type="bibr" target="#b4">[5]</ref> proposed a CNN applied on Mel-frequency cepstral coefficients (MFCCs) that generates subject independent videos from an audio clip and a still frame. The method uses an L 1 loss at the pixel level resulting in blurry frames, which is why a deblurring step is also required. Secondly, this loss at the pixel level penalizes any deviation from the target video during training, providing no incentive for the model to produce spontaneous expressions and resulting in faces that are mostly static except for the mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GAN-Based Video Synthesis</head><p>The recent introduction of GANs in <ref type="bibr" target="#b9">[10]</ref> has shifted the focus of the machine learning community to generative modelling. GANs consist of two competing networks: a generative network and a discriminative network. The generator's goal is to produce realistic samples and the discriminator's goal is to distinguish between the real and generated samples. This competition eventually drives the generator to produce highly realistic samples. GANs are typically associated with image generation since the adversarial loss produces sharper, more detailed images compared to L 1 and L 2 losses. However, GANs are not limited to these applications and can be extended to handle videos <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Straight-forward adaptations of GANs for videos are proposed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>, replacing the 2D convolutional layers with 3D convolutional layers. Using 3D convolutions in the generator and discriminator networks is able to capture temporal dependencies but requires fixed length videos. This limitation was overcome in <ref type="bibr" target="#b19">[20]</ref> but constraints need to be imposed in the latent space to generate consistent videos.</p><p>The MoCoGAN system proposed in <ref type="bibr" target="#b23">[24]</ref> uses an RNN-based generator, with separate latent spaces for motion and content. This relies on the empirical evidence shown in <ref type="bibr" target="#b17">[18]</ref> that GANs perform better when the latent space is disentangled. MoCoGAN uses a 2D and 3D CNN discriminator to judge frames and sequences respectively. A sliding window approach is used so that the 3D CNN discriminator can handle variable length sequences.</p><p>GANs have also been used in a variety of cross-modal applications, including text-to-video and audioto-video. The text-to-video model proposed in <ref type="bibr" target="#b13">[14]</ref> uses a combination of variational auto encoders (VAE) and GANs in its generating network and a 3D CNN as a sequence discriminator. Finally, Chen et al. <ref type="bibr" target="#b3">[4]</ref> propose a GAN-based encoder-decoder architecture that uses CNNs in order to convert audio spectrograms to frames and vice versa.</p><p>3 End-to-End Speech-Driven Facial Synthesis</p><p>The proposed architecture for speech-driven facial synthesis is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The system is made up of a generator and 2 discriminators, each of which evaluates the generated sequence from a different perspective. The capability of the generator to capture various aspects of natural sequences is directly proportional to the ability of each discriminator to discern videos based on them.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generator</head><p>The inputs to the generator networks consist of a single image and an audio signal, which is divided into overlapping frames each corresponding to 0.16 seconds. The generator network in this architecture can be conceptually divided into subnetworks as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Using an RNN-based generator allows us to synthesize videos frame-by-frame, which is necessary for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Identity Encoder</head><p>The speaker's identity is encoded using a 6 layer CNN. Each layer uses strided 2D convolutions, followed by batch normalization and ReLU activation functions. The Identity Encoder network reduces the input image to a 50 dimensional encoding z id .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Context Encoder</head><p>Audio frames are encoded using a network comprising of 1D convolutions followed by batch normalization and ReLU activations. The initial convolutional layer starts with a large kernel, as recommended in <ref type="bibr" target="#b6">[7]</ref>, which helps limit the depth of the network while ensuring that the low-level features are meaningful. Subsequent layers use smaller kernels until an embedding of the desired size is achieved. The audio frame encodings are input into a 2 layer GRU, which produces a context encoding z c with 256 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Frame Decoder</head><p>The identity encoding z id is concatenated to the context encoding z c and a noise component z n to form the latent representation. The 10-dimensional z n vector is obtained from a Noise Generator, which is a 1-layer GRU that takes Gaussian noise as input. The Frame Decoder is a CNN that uses strided transposed convolutions to produce the video frames from the latent representation. A U-Net <ref type="bibr" target="#b18">[19]</ref> architecture is used with skip connections between the Identity Encoder and the Frame Decoder to help preserve the identity of the subject.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminators</head><p>Our system has two different types of discriminator. The Frame Discriminator helps achieve a highquality reconstruction of the speakers' face throughout the video. The Sequence Discriminator ensures that the frames form a cohesive video which exhibits natural movements and is synchronized with the audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Frame Discriminator</head><p>The Frame Discriminator is a 6-layer CNN that determines whether a frame is real or not. Adversarial training with this discriminator ensures that the generated frames are realistic. The original still frame is used as a condition in this network, concatenated channel-wise to the target frame to form the input as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. This enforces the person's identity on the frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sequence Discriminator</head><p>The Sequence Discriminator presented in Fig. <ref type="figure" target="#fig_2">3</ref> distinguishes between real and synthetic videos. The discriminator receives a frame at every time step, which is encoded using a CNN and then fed into a 2-layer GRU. A small (2-layer) classifier is used at the end of the sequence to determine if the sequence is real or not. The audio is added as a conditional input to the network, allowing this discriminator to classify speech-video pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The Frame discriminator (D img ) is trained on frames that are sampled uniformly from a video x using a sampling function S(x). The Sequence discriminator (D seq ), classifies based on the entire sequence x and audio a. The loss of our GAN is an aggregate of the losses associated with each discriminator as shown in Equation <ref type="formula" target="#formula_0">1</ref>.</p><formula xml:id="formula_0">L adv (D img , D Seq , G) = E x∼P d [log D img (S(x), x 1 )] + E z∼Pz [log(1 − D img (S(G(z)), x 1 ))]+ E x∼P d [log D seq (x, a)] + E z∼Pz [log(1 − D seq (G(z), a))]<label>(1)</label></formula><p>An L 1 reconstruction loss is also used to improve the synchronization of the mouth movements. However we only apply the reconstruction loss to the lower half of the image since it discourages the generation of facial expressions. For a ground truth frame F and a generated frame G with dimensions W × H the reconstruction loss at the pixel level is:</p><formula xml:id="formula_1">L L 1 = p∈[0,W ]×[ H 2 ,H] |F p − G p |<label>(2)</label></formula><p>The final objective is to obtain the optimal generator G * , which satisfies Equation 3. The model is trained until no improvement is observed on the reconstruction metrics on the validation set for 10 epochs. The λ hyperparameter controls the contribution of each loss factor and was set to 400 following a tuning procedure on the validation set. arg min</p><formula xml:id="formula_2">G max D L adv + λL L 1<label>(3)</label></formula><p>We used Adam <ref type="bibr" target="#b12">[13]</ref> for all the networks with a learning rate of 0.0002 for the Generator and 0.001 Frame Discriminator which decay after epoch 20 with a rate of 10%. The Sequence Discriminator uses a smaller fixed learning rate of 5 • 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our model is implemented in PyTorch and takes approximately a week to train using an Nvidia GeForce GTX 1080 Ti GPU. During inference the average generation time per frame is 7ms on the GPU, permitting the use of our method use in real time applications. A sequence of 75 frames can be synthesized in 0.5s. The frame and sequence generation times increase to 1s and 15s respectively when processing is done on the CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The GRID dataset has 33 speakers each uttering 1000 short phrases, containing 6 words taken from a limited dictionary. The TCD TIMIT dataset has 59 speakers uttering approximately 100 phonetically rich sentences each. We use the recommended data split for the TCD TIMIT dataset but exclude some of the test speakers and use them as a validation set. For the GRID dataset speakers are divided into training, validation and test sets with a 50%−20%−30% split respectively. As part of our preprocessing all faces are aligned to the canonical face and images are normalized. We increase the size of the training set by mirroring the training videos. The amount of data used for training and testing is presented in Table <ref type="table" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Samples (Tr) Hours (Tr) Samples (V) Hours (V) Samples (T) Hours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We use common reconstruction metrics such as the peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) index to evaluate the generated videos. During the evaluation it is important to take into account the fact that reconstruction metrics penalize videos for any spontaneous expression. The frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure <ref type="bibr" target="#b16">[17]</ref>, which determines blur based on the presence of edges in the image and the frequency domain blurriness measure (FDBM) proposed in <ref type="bibr" target="#b7">[8]</ref>, which is based on the spectrum of the image. For these metrics larger values imply better quality. The content of the videos is evaluated based on how well the video captures identity of the target and on the accuracy of the spoken words. We verify the identity of the speaker using the average content distance (ACD) <ref type="bibr" target="#b23">[24]</ref>, which measures the average Euclidean distance of the still image representation, obtained using OpenFace <ref type="bibr" target="#b0">[1]</ref>, from the representation of the generated frames. The accuracy of the spoken message is measured using the word error rate (WER) achieved by a pre-trained lip-reading model. We use the LipNet model <ref type="bibr" target="#b1">[2]</ref>, which surpasses the performance of human lipreaders on the GRID dataset. For both content metrics lower values indicate better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In order to quantify the effect of each component of our system we perform an ablation study on the GRID dataset (see Table <ref type="table" target="#tab_2">2</ref>). We use the metrics from section 4.2 and a pre-trained LipNet model which achieves a WER of 21.4% on the ground truth videos. The average value of the ACD for ground truth videos of the same person is 0.74 • 10 −4 whereas for different speakers it is 1.4 • 10 −3 . The L 1 loss achieves slightly better PSNR and SSIM results, which is expected as it does not generate spontaneous expressions, which are penalized by these metrics unless they happen to coincide with those in ground truth videos. This variation introduced when generating expressions is likely the reason for the small increase in ACD. The blurriness is minimized when using the adversarial loss as indicated by the higher FDBM and CPBD scores and Fig. <ref type="figure" target="#fig_4">4</ref>. Finally, the effect of the sequence discriminator is shown in the lip-reading result achieving a low WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>Our method is capable of producing realistic videos of previously unseen faces and audio clips taken from the test set. The examples in Fig. <ref type="figure" target="#fig_6">5</ref> show the same face animated using sentences from different subjects (male and female). The same audio used on different identities is shown in Fig. <ref type="figure" target="#fig_7">6</ref>   inspection it is evident that the lips are consistently moving similarly to the ground truth video. Our method not only produces accurate lip movements but also natural videos that display characteristic human expressions such as frowns and blinks, examples of which are shown in Fig. <ref type="figure" target="#fig_9">7</ref>.  Coarticulation is evident in (a) where "bin" is followed by the word "blue".   The works that are closest to ours are those proposed in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b4">[5]</ref>. The former method is subject dependent and requires a large amount of data for a specific person to generate videos. For the latter method there is no publicly available implementation so we compare our model to a static method that produces video frames using a sliding window of audio samples like that used in <ref type="bibr" target="#b4">[5]</ref>. This is a GAN-based method that uses a combination of an L 1 loss and an adversarial loss on individual frames. We will also use this method as the baseline for our quantitative assessment in the following section. This baseline produces less coherent sequences, characterized by jitter, which becomes worse in cases where the audio is silent (e.g. pauses between words). This is likely due to the fact that there are multiple mouth shapes that correspond to silence and since the model has no knowledge of its past state generates them at random. Fig. <ref type="figure" target="#fig_11">8</ref> shows a comparison between our approach and the baseline in such cases.  </p><note type="other">Ground Truth Identity 1 Identity 2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quantitative Results</head><p>We measure the performance of our model on the GRID and TCD TIMIT datasets using the metrics proposed in section 4.2 and compare it to the static baseline. Additionally, we present the results of a 30-person survey, where users were shown 30 videos from each method and were asked to pick the more natural ones. The results in Table <ref type="table" target="#tab_3">3</ref> show that our method outperforms the static baseline in both frame quality and content accuracy. Although the difference in performance is slight for frame-based measures (e.g. PSNR, ACD) it is substantial in terms of user preference and lipreading WER, where temporal smoothness of the video and natural expressions play a significant role.</p><p>We further evaluate the realism of the generated videos through an online Turing test. In this test users are shown 10 videos, which were chosen at random from GRID and TIMIT consisting of 6 fake videos and 4 real ones. Users are shown the videos in sequence and are asked to label them as real or fake. Responses from 153 users were collected with the average user labeling correctly 63% of the videos. The distribution of user scores is shown in Fig. <ref type="figure" target="#fig_12">9</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><note type="other">PSNR SSIM FDBM CPBD ACD User WER GRID Proposed Model 27</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work we have presented an end-to-end model using temporal GANs for speech-driven facial animation. Our model is capable of producing highly detailed frames scoring high in terms of PSNR, SSIM and in terms of the sharpness measures on both datasets. According to our ablation study this can be mainly attributed to the use of a Frame Discriminator. Furthermore, our method produces more coherent sequences and more accurate mouth movements compared to the static approach, as demonstrated by a resounding user preference and the difference in the WER. We believe that these improvements are not only a result of using a temporal generator but also due to the use of the conditional Sequence Discriminator. Unlike previous approaches <ref type="bibr" target="#b4">[5]</ref> that prohibit the generation of facial expressions, the adversarial loss on the entire sequence encourages spontaneous facial gestures. This has been demonstrated with examples of blinks and frowns. All of the above factors make the videos generated using our approach difficult to separate from real videos as revealed from the Turing test results, with the average user scoring only slightly better than chance. It is also noteworthy that no user was able to perfectly classify the videos.</p><p>This model has shown promising results in generating lifelike videos. Moving forward, we believe that different architectures for the sequence discriminator could help produce more natural sequences. Finally, at the moment expressions are generated randomly by the model so a natural extension of this method would attempt to also capture the mood of the speaker from his voice and reflect it in the facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work has been funded by the European Community Horizon 2020 under grant agreement no. 645094 (SEWA).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The deep model for speech-driven facial synthesis. This uses 2 discriminators to incorporate the different aspects of a realistic video. Details about the architecture are presented in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of the (a) Generator which consists of a Context Encoder (audio encoder and RNN), an Identity Encoder, a Frame Decoder and Noise Generator (b) Sequence Discriminator, consisting of an audio encoder, an image encoder, GRUs and a small classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(a) L 1</head><label>1</label><figDesc>loss on entire frame (b) Proposed loss on frames</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Frames using (a) only an L 1 loss on the entire face compared to (b) frames produced using the proposed method. Frames generated using an L 1 loss on the entire face (a) are blurrier than those produced from the proposed method (b).</figDesc><graphic url="image-37.png" coords="8,44.18,168.70,52.77,70.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>Female voice uttering the word "bin" (b) Male voice uttering the word "white"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Generated sequences for (a) the word "bin" (b) the word "white" from the GRID test set.Coarticulation is evident in (a) where "bin" is followed by the word "blue".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Animation of different faces using the same audio. The movement of the mouth is similar for both faces as well as for the ground truth sequence. Both audio and still image are unseen during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )</head><label>a</label><figDesc>Example of generated frown (b) Example of generated blink</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Facial expressions generated using our framework include (a) frowns and (b) blinks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Audio-visual inconsistency during silence Ground Truth (b) Extraneous frames that break continuity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of consecutive frames showcasing the failures of the static baseline including (a) opening the mouth when words are not spoken (b) producing irrelevant frames that do not take into account the previous face state, thus breaking the sequence continuity.</figDesc><graphic url="image-161.png" coords="9,309.24,311.10,255.69,145.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Distribution of user scores for the Turing test.</figDesc><graphic url="image-162.png" coords="10,45.11,170.22,521.81,157.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The samples and hours of video in the training (Tr), validation (V) and test (T) sets.</figDesc><table><row><cell>(T)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Adv img 27.71 0.840 0.114 0.274 1.04 •10 −4 27.94% L 1 + Adv img + Adv seq 27.98 0.844 0.114 ¯0.277 ¯1.02 •10 −4 25.45% Assessment of each model in the ablation study performed on the GRID dataset</figDesc><table><row><cell>. From visual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>.98 ¯0.844 ¯0.114 ¯0.277 1.02 ¯•10 −4 79.77 ¯% 25.4 ¯% Baseline 27.39 0.831 0.113 0.280 ¯1.07 •10 −4 20.22% 37.2% TCD Proposed Model 23.54 ¯0.697 ¯0.102 ¯0.253 ¯2.06 ¯•10 −4 77.03% ¯N/A Baseline 23.01 0.654 0.097 0.252 2.29 •10 −4 22.97%N/A Performance comparison of the proposed method against the static baseline. The pretrained LipNet model is not available for the TCD TIMIT so the WER metric is omitted.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Videos are available here: https://sites.google.com/view/facialsynthesis/home</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>Details regarding the network architecture that were not included in the paper due to lack of space are included here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Audio Preprocessing</head><p>The sequence of audio samples is divided into overlapping audio frames in a way that ensures a one-toone correspondence with the video frames. In order to achieve this we pad the audio sequence on both ends and use the following formula for the stride:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Network Architecture</head><p>This section describes, in detail, the architecture of the networks used in our temporal GAN. All our networks use ReLU activations except for the final layers. The encoders and generator use the hyperbolic tangent activation to ensure that their output lies in the set [−1, 1] and the discriminator uses a Sigmoid activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Audio Encoder</head><p>The Audio Encoder network obtains features for each audio frame. It is made up of 7 Layers and produces an encoding of size 256. This encoding is fed into a 2 layer GRU which will produce the final context encoding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Noise Generator</head><p>The Noise Generator is responsible for producing noise that is sequentially coherent. The network is made up of GRUs which take as input at every instant a 10 dimensional vector sampled from a Gaussian distribution with mean 0 and variance of 0.6. The Noise Generator is shown in Fig. <ref type="figure">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Identity Encoder and Frame Decoder</head><p>The Identity Encoder is responsible for capturing the identity of the speaker from the still image. The Identity Encoder is a 6 layer CNN which produces an identity encoding z id of size 50. This information is concatenated to the context encoding z c and the noise vector z n at every instant and fed as input to the Frame Decoder, which will generate a frame of the sequence. The Frame Decoder is a 6 layer CNN that uses strided transpose convolutions to generate frames. The Identity Encoder -Frame Decoder architecture is shown in Fig. <ref type="figure">12</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Datasets</head><p>The model is evaluated on the GRID and TCD TIMIT datasets. The subjects used for training, validation and testing are shown in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OpenFace: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<idno>118</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<title level="m">LipNet: End-to-End Sentence-level Lipreading</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video Rewrite</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Graphics and Interactive Techniques</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-Visual Integration in Multimodal Communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="837" to="852" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Neural Networks for Raw Waveforms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image Sharpness Measure for Blurred Images in Frequency Domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Masilamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photo-real talking head with deep bidirectional lstm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4884" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: An audio-visual corpus of continuous speech</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">94</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00421</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Video Generation From Text. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02877</idno>
		<title level="m">Ictu Oculi : Exposing AI Generated Fake Face Videos by Detecting Eye Blinking</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A no-reference perceptual image sharpness metric based on a cumulative probability of blur detection</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Quality of Multimedia Experience (QoMEx)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="87" to="91" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal Generative Adversarial Nets with Singular Value Clipping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generation of mouthshapes for a synthetic talking head</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Institute of Acoustics, Autumn Meeting</title>
				<meeting>the Institute of Acoustics, Autumn Meeting</meeting>
		<imprint>
			<date type="published" when="1990-01">January. 1990</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">475482</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: Learning Lip Sync from Audio Output Obama Video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">95</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">93</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MoCoGAN: Decomposing Motion and Content for Video Generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04993</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating Videos with Scene Dynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lip movement synthesis from speech based on hidden Markov Models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantitative association of vocal-tract and facial behavior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yehia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linking facial animation, head motion and speech acoustics</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Yehia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuratate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vatikiotis-Bateson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="568" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
