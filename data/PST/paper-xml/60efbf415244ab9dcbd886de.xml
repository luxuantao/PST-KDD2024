<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deduplicating Training Data Makes Language Models Better</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-24">24 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution. † Google Research</orgName>
								<orgName type="institution">Brain Team. ‡ University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
							<email>daphnei@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution. † Google Research</orgName>
								<orgName type="institution">Brain Team. ‡ University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yehong</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shao- Jie</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingyue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shanzhi</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gaojun</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaowei</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuefeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">2021</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><surname>Pangu</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yi Liao</orgName>
								<address>
									<addrLine>Zhiwei Wang, ZhenZhang Yang, Kaisheng Wang</addrLine>
									<settlement>Xin Jiang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Xiaoda Zhang</orgName>
								<orgName type="institution" key="instit2">Chen Li</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ziyan Gong</orgName>
								<address>
									<addrLine>Yifan Yao, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang</addrLine>
									<settlement>Xinjing Huang, Jun Wang, Hengtao Tao, Han Zhang, Lingfeng</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deduplicating Training Data Makes Language Models Better</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-24">24 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.06499v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.</p><p>As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets-for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. Code for deduplication is released at https://github.com/goog e-research/ dedup icate-text-datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key factor behind the recent progress in natural language processing is the development of largescale text corpora used to train increasingly large language models. These datasets have grown from single gigabytes to as much as a terabyte over the past few years <ref type="bibr" target="#b9">(Chelba et al., 2013;</ref><ref type="bibr">Xue et al., 2020;</ref><ref type="bibr" target="#b18">Graff et al., 2003;</ref><ref type="bibr">Brown et al., 2020)</ref>. Because it is so expensive to perform manual review and curation on massive datasets, they tend to suffer in quality compared to their smaller predecessors. This has implications far beyond metrics like perplexity and validation loss, as learned models reflect the biases present in their training data <ref type="bibr" target="#b2">(Bender et al., 2021;</ref><ref type="bibr" target="#b38">Wallace et al., 2019;</ref><ref type="bibr" target="#b30">Sheng et al., 2020)</ref>. Quantitatively and qualitatively understanding these datasets is therefore a research challenge in its own right <ref type="bibr" target="#b12">(Dodge et al., 2021a)</ref>.</p><p>We show that one particular source of bias, duplicated training examples, is pervasive: all four common NLP datasets we studied contained duplicates. Additionally, all four corresponding validation sets contained text duplicated in the training set. While naive deduplication is straightforward (and the datasets we consider already perform some naive form of deduplication), performing thorough deduplication at scale is both computationally challenging and requires sophisticated techniques.</p><p>We propose two scalable techniques to detect and remove duplicated training data. Exact substring matching identifies verbatim strings that are repeated. This allows us to identify cases where only part of a training example is duplicated ( §4.1). Approximate full document matching uses hashbased techniques <ref type="bibr" target="#b5">(Broder, 1997)</ref> to identify pairs of documents with high n-gram overlap ( §4.2).</p><p>We identify four distinct advantages to training on datasets that have been thoroughly deduplicated.</p><p>1. Over 1% of tokens emitted unprompted from a model trained on standard datasets (e.g., C4) are part of a memorized sequence (See §6.2)even though the 1.5 billion parameter model is much smaller than the 350GB dataset it was trained on. By deduplicating the training dataset we reduce the rate of emitting memorized training data by a factor of 10×.</p><p>2. Train-test overlap is common in nondeduplicated datasets. For example, we find a 61-word sequence<ref type="foot" target="#foot_0">1</ref> in C4 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> that is repeated 61,036 times verbatim in the training dataset and 61 times in the validation set (0.02% of the samples in each dataset).</p><p>This train-test set overlap not only causes researchers to over-estimate model accuracy, but also biases model selection towards models and hyperparameters that intentionally overfit their training datasets.</p><p>3. Training models on deduplicated datasets is more efficient. Processing a dataset with our framework requires a CPU-only linear-time algorithm. And so because these datasets are up to 19% smaller, even including the deduplication runtime itself, training on deduplicated datasets directly reduces the training cost in terms of time, dollar, and the environment <ref type="bibr" target="#b2">(Bender et al., 2021;</ref><ref type="bibr" target="#b33">Strubell et al., 2019;</ref><ref type="bibr" target="#b26">Patterson et al., 2021)</ref>.</p><p>4. Deduplicating training data does not hurt perplexity: models trained on deduplicated datasets have no worse perplexity compared to baseline models trained on the original datasets. In some cases deduplication reduces perplexity by up to 10%. Further, because recent LMs are typically limited to training for just a few epochs <ref type="bibr" target="#b27">(Radford et al., 2019;</ref><ref type="bibr" target="#b28">Raffel et al., 2020)</ref>, by training on higher quality data the models can reach higher accuracy faster.</p><p>To summarize, data duplication offers significant advantages and no observed disadvantages. In the remainder of this paper we present our text deduplication framework in §4, and study the extent of duplicate content in common NLP datasets (e.g., C4, Wiki-40B, and LM1B) in §5. We then examine the impact of deduplication on test perplexity ( §6.1) and on the frequency of emitting memorized content ( §6.2). Finally, we analyze to what extent perplexity on existing, released models are skewed as a result of overlap between the train and test/validation splits ( §6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large language model datasets. While we believe our results are independent of model architecture, we perform our analysis on Transformerbased decoder-only language models <ref type="bibr" target="#b36">(Vaswani et al., 2017</ref>) trained for open-ended text generation. These current state-of-the-art models are trained on internet text. For example, the GPT-2 family of models <ref type="bibr" target="#b27">Radford et al. (2019)</ref> is trained on Web-Text, a dataset of web documents highly ranked on Reddit-however this dataset was not made available publicly. A common dataset starting point is CommonCrawl, an index of public webpages. Among the models trained on CommonCrawl include GPT-3 <ref type="bibr">(Brown et al., 2020)</ref> with the addition of book datasets, GROVER <ref type="bibr" target="#b43">(Zellers et al., 2019)</ref> on a restricted subset filtered to news domains called RealNews, and T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> on a cleaned version of common crawl called C4. Other models are trained on more curated Internet sources-for example <ref type="bibr" target="#b19">Guo et al. (2020)</ref> used high quality processed Wikipedia text from 40 different languages to train monolingual 141.4M parameter language models. Non-English models necessarily use different datasets; <ref type="bibr">Zeng et al. (2021)</ref> for instance introduced PANGU-α, a family of models with up to 200B parameters that were trained on a non-public corpus of cleaned and filtered Chinese-language documents from CommonCrawl and other sources. Since many of these datasets are not public, we deduplicate three that are: Wiki-40B, C4, and RealNews-as well as the One Billion Word Language Model Benchmark <ref type="bibr" target="#b9">(Chelba et al., 2013)</ref>, a smaller dataset commonly used for evaluation. In our research, we do not focus on the impact of duplicate text in pretrained models on downstream benchmark tasks; instead we address how duplicate text in the LM training and validation sets impacts model perplexity and the extent to which generated text included memorized content.</p><p>Memorizing training data. The privacy risks of data memorization, for example the ability to extract sensitive data such as valid phone numbers and IRC usernames, are highlighted by <ref type="bibr" target="#b8">Carlini et al. (2020)</ref>. While their paper finds 604 samples that GPT-2 emitted from its training set, we show that over 1% of the data most models emit is memorized training data. In computer vision, memorization of training data has been studied from various angles for both discriminative and generative models (e.g. <ref type="bibr" target="#b0">Arpit et al., 2017;</ref><ref type="bibr" target="#b39">Webster et al., 2019;</ref><ref type="bibr" target="#b14">Feldman and Zhang, 2020;</ref><ref type="bibr" target="#b32">Stephenson et al., 2021;</ref><ref type="bibr" target="#b34">Teterwak et al., 2021)</ref>.</p><p>Duplicate text in training data. The Book Corpus <ref type="bibr">(Zhu et al., 2015)</ref>, which was used to train popular models such as BERT, has a substantial amount of exact-duplicate documents according to <ref type="bibr">Bandy and Vincent (2021)</ref>. <ref type="bibr">Allamanis (2019)</ref> shows that duplicate examples in code datasets cause worsened performance on code understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Modeling Datasets</head><p>We analyze the presence of duplicate text in four datasets of varying sizes that have been used for training natural language generation systems, producing general-purpose pre-trained models, and for language model benchmarking. While this paper restricts itself to English datasets, we expect that non-English datasets suffer from similar issues and could likewise benefit from de-duplication.</p><p>Wikipedia (Wiki-40B) consists of multi-lingual cleaned Wikipedia text <ref type="bibr" target="#b19">(Guo et al., 2020)</ref>. We take the English portion, which contains 2.9M Wikipedia pages with an average length of 768 BPE tokens. The dataset creators do not indicate any deduplication was performed aside from removing redirect-pages (e.g., "sunflower" to "Helianthus").</p><p>One-Billion Word benchmark (LM1B) contains 30M sentences of news commentary <ref type="bibr" target="#b9">(Chelba et al., 2013)</ref>. Unlike the other datasets we analyze, LM1B's examples are one sentence long rather than multi-sentence documents. The average example length is 32 BPE tokens. While this dataset is extremely standard for benchmarking language models, <ref type="bibr">Radford et al. (2019, Sec 4)</ref> note it has 13.2% overlap of the test set with the train set.</p><p>Colossal Cleaned Common Crawl (C4) is made up of 360M web documents, with an average length of 486 BPE tokens <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>. C4 was introduced as a pre-training dataset for T5, a set of encoder-decoder models which have been widely used in fine-tuned downstream tasks. The dataset was previously deduplicated in a more sophisticated process than the prior two datasets. Each paragraph was hashed and paragraphs resulting in hash collisions were removed. This was followed by a pass that removed placeholder text, code, and prohibited words. See <ref type="bibr" target="#b12">Dodge et al. (2021a)</ref> for a detailed breakdown of the source text in C4.</p><p>RealNews is a subset of the Common Crawl consisting of articles from news domains <ref type="bibr" target="#b43">(Zellers et al., 2019)</ref>. It contains 31M documents with average length 793 BPE tokens. RealNews was deduplicated by inserting a hash of the first 100 characters of each document into a bloom filter <ref type="bibr" target="#b4">(Bloom, 1970)</ref> and then excluding any document which resulted in a hash collision. Like C4, examples with duplicate URLs were excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods for Identifying Duplicates</head><p>The simplest technique to find duplicate examples would be to perform exact string matching between all example pairs, but as we will show, this is insufficient. We introduce two complementary methods for performing deduplication. First, using a suffix array <ref type="bibr" target="#b24">(Manber and Myers, 1993)</ref>, we remove duplicate substrings from the dataset if they occur verbatim in more than one example. Second, we use MinHash <ref type="bibr" target="#b5">(Broder, 1997)</ref>, an efficient algorithm for estimating the n-gram similarity between all pairs of examples in a corpus, to remove entire examples from the dataset if they have high n-gram overlap with any other example.</p><p>We consider a dataset D = {x i } N i=1 as a collection of examples x i . Each of these examples is itself a sequence of tokens:</p><formula xml:id="formula_0">x i = x 1 i , x 2 i , • • • , x s i i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exact Substring Duplication</head><p>Due to the diversity of possibilities in human language, it is rare for the same idea to be expressed identically in multiple documents unless one expression is derived from the other, or both are quoting from a shared source. This observation motivates deduplicating exact substrings. We call our approach EXACTSUBSTR. When two examples x i and x j share a sufficiently long substring (that is, a substring for which x a..a+k i = x b..b+k j ), that substring is removed from one of them. Based on statistical analyses ( §B), we select k = 50 tokens as the minimum matching substring length.</p><p>A breakdown of the computation needed for this approach can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Suffix Arrays</head><p>This exact-substring-matching criterion, while conceptually simple, is computationally prohibitive with naive (quadratic) all-pair matching. To improve the efficiency, we concatenate all the examples of the entire dataset D into a giant sequence S, and construct a Suffix Array A of S. A suffix array <ref type="bibr" target="#b24">(Manber and Myers, 1993)</ref> is a representation of a suffix tree <ref type="bibr" target="#b40">(Weiner, 1973)</ref> that can be constructed in linear time in S <ref type="bibr" target="#b22">(Kärkkäinen and Sanders, 2003)</ref> and enables efficient computation of many substring queries; in particular, they allow us to identify duplicated training examples in linear time. Suffix arrays have the advantage over suffix trees in that they are 10-100× more memory efficient <ref type="bibr" target="#b24">(Manber and Myers, 1993)</ref>, requiring just 8 bytes per input token, though they are asymptotically less efficient for some query types. They have been used widely in NLP, such as for efficient TF-IDF computation <ref type="bibr" target="#b42">(Yamamoto and Church, 2001)</ref> and document clustering <ref type="bibr" target="#b10">(Chim and Deng, 2007)</ref>.</p><p>The suffix array A for a sequence S is a lexicographically-ordered list of all suffixes contained in the sequence. Formally, A(S) = arg sort all_suffixes(S) For example, the suffixes of the sequence "banana" are ("banana", "anana", "nana" "ana", "na", "a") and so the suffix array is the sequence (6 4 2 1 5 3). In practice, we construct S from the bytes of the BPE tokenization of the text ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Substring matching</head><p>After constructing A, it is straightforward to identify duplicated training examples. Suppose that the sequence s was repeated exactly twice in the training dataset S at positions i and j, that is, S i..i+|s| = S j..j+|s| . Then the indices i, j will occur adjacent to each other in the suffix array A.</p><p>Finding all repeated sequences is thus a matter of linearly scanning the suffix array from beginning to end and looking for sequences A i , A i+1 that share a common prefix of at least some threshold length. Any satisfying sequences are recorded. This algorithm is embarrassingly parallel, and so we can efficiently process the dataset. Based on experimentation (Appendix B), we choose a threshold length of 50 BPE tokens for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Matching with MinHash</head><p>We also perform approximate deduplication based on matching entire examples. This method, which we call NEARDUP, is a good complement to the exact substring matching, especially for web crawl text, as it handles the very common case of documents being identical except for interspersed templated fields (such as the last row of Table <ref type="table">1</ref>).</p><p>MinHash <ref type="bibr" target="#b5">(Broder, 1997</ref>) is an approximate matching algorithm widely used in large-scale deduplication tasks <ref type="bibr" target="#b37">(Versley and Panchenko, 2012;</ref><ref type="bibr" target="#b15">Gabriel et al., 2018;</ref><ref type="bibr" target="#b20">Gyawali et al., 2020)</ref>, including to deduplicate the training set for a large Chinese-language LM <ref type="bibr">(Zeng et al., 2021)</ref>. Given two documents x i and x j , the main idea is to represent each document by its respective set of n-grams d i and d j . We can then use hash functions to approximate the Jaccard Index <ref type="bibr" target="#b21">(Jaccard, 1912)</ref>:</p><formula xml:id="formula_1">Jaccard(d i , d j ) = |d i ∩d j | /|d i ∪d j |</formula><p>If the Jaccard Index between d i and d j is sufficiently high, it is likely that documents are approximate matches of each other. To efficiently approximate the Jaccard index, MinHash constructs document signatures by sorting each of the n-grams via a hash function, and then keeping only the k smallest hashed n-grams. There are multiple ways to construct estimators of the Jaccard index from these kinds of signatures <ref type="bibr" target="#b11">(Cohen, 2016)</ref>.</p><p>In our implementation, we use 5-grams and a signature of size 9,000. The probability that two documents are considered a potential match is</p><formula xml:id="formula_2">Pr(d i , d j | Jaccard(d i , d j ) = s i,j ) = 1−(1−s b i,j ) r</formula><p>where b = 20 and r = 450 are user-settable parameters to control the strength of the filter. See Appendix A for more details.</p><p>For each pair of documents identified as a potential match, more computationally expensive similarity metrics can be employed as a subsequent filtering step. In particular, we identify two documents as duplicates if they are matched by the MinHash algorithm and their edit similarity is greater than 0.8. The edit similarity between token sequences x i and x j is defined as:</p><formula xml:id="formula_3">EditSim(x i , x j ) = 1 − EditDistance(x i , x j ) max(|x i |, |x j |)</formula><p>To build clusters of similar documents, we construct a graph that has an edge between two documents if they are considered a match. Then, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C4</head><p>Affordable and convenient holiday flights take off from your departure country, "Canada". From May 2019 to October 2019, Condor flights to your dream destination will be roughly 6 a week! Book your Halifax (YHZ) -Basel (BSL) flight now, and look forward to your "Switzerland" destination! Affordable and convenient holiday flights take off from your departure country, "USA". From April 2019 to October 2019, Condor flights to your dream destination will be roughly 7 a week! Book your Maui Kahului (OGG) -Dubrovnik (DBV) flight now, and look forward to your "Croatia" destination! Table <ref type="table">1</ref>: Qualitative examples of near-duplicates identified by NEARDUP from each dataset. The similarity between documents is highlighted. Note the small interspersed differences that make exact duplicate matching less effective. Examples ending with "[...]" have been truncated for brevity. More data available in Appendix.</p><p>010 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9</p><p>Number of groups  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deduplication Results</head><p>We deduplicate each of the four datasets with both of our two techniques. When text was duplicated across multiple data splits, we prioritized keeping a copy in the test or validation set and removing it from the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Amount of Text Removed</head><p>With NEARDUP, we found that the web-scrape datasets contain between 3.04% (on C4) to 13.63% (on RealNews) near duplicates (Table <ref type="table" target="#tab_4">2</ref>). Nearduplicate text is much less common in Wiki-40B, forming only 0.39% of the train set. 2 In C4, the majority (1.8M) of near-duplicate clusters consisted of just a single pair of examples that matched against each other, but there were 280 clusters with over 5,000 examples in them (Figure <ref type="figure" target="#fig_0">1</ref>), including one cluster of size 250,933.</p><p>2 Most duplicates we saw were automatically generated pages, such as the outcomes of sports games. This shows the strength of manual curation for creating high-quality datasets.   On average with EXACTSUBSTR, we remove more total content than with NEARDUP (despite EXACTSUBSTR not removing any examples outright)-for example removing 7.18% of the tokens in C4. The exception is LM1B, where EX-ACTSUBSTR removes 8× less data than NEARDUP. On investigation, we find this is due to the fact that LM1B documents are significantly shorter: 90% of all documents are under 50 tokens, and so are not even candidates for potential matches even if the entire sequence matched verbatim. We find that both NEARDUP and EXACTSUBSTR remove similar content-77% of the training examples that NEARDUP removes from C4 have at least one verbatim length-50 match found by EXACTSUBSTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Properties of Duplicated Text</head><p>While the authors of both RealNews and C4 explicitly attempted deduplication during dataset construction, the methods were insufficient to capture the more subtle types of duplicate text commonly found on the internet. In C4 and Wiki-40B, we qualitatively observe that much of the text identified as near-duplicated is computer-generated. The text is identical except for the names of places, businesses, products, dates, and so on. Because these examples frequently differ by just a few words at a time, deduplication strategies relying on exact string matching would fail to identify a match. Example duplicate pairs from each dataset can be found in Table <ref type="table">1</ref> (more examples in the Appendix).</p><p>For RealNews and LM1B, derived from news sites, we observe that many near-duplicates occur because the same news article appears on multiple news sites with slightly different formatting. For example, in LM1B, there is one example that starts "MINEOLA , N.Y. -New York officials say [...]" and another that starts "( AP ) -New York officials say [...]". The two examples are otherwise identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Train / Test Set Leakage</head><p>Both deduplication methods identify overlap between the train set and the validation set (Table <ref type="table" target="#tab_4">2</ref>). For example, 4.6% of the C4 validation set and 14.4% of the RealNews validation set examples had an approximate duplicate in their respective training sets. Such duplication is problematic since it could cause evaluation metrics to be unfairly inflated for models that are better at memorizing their train sets. We evaluate the effect of this leakage on publicly released models in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Impact on Trained Models</head><p>. We trained 1.5B parameter "XL", decoderonly, Transformer-based language models similar to GPT-2, on C4-ORIGINAL, C4-NEARDUP, and C4-EXACTSUBSTR, respectively. We use the T5 codebase and model architecture from <ref type="bibr" target="#b28">Raffel et al. (2020)</ref>, and each model was trained for about two epochs on its respective dataset. To better understand the amount of variance in the perplexities of trained models, we also trained three different random seeds of the 110M parameter "base" model for each of the above three datasets-for a total of nine base-sized models.</p><p>For all experiments, we used a Byte Pair Encoding (BPE) vocabulary trained on C4-NEARDUP with a budget of 50K tokens, which resulted in a vocabulary the same size as GPT-2's. We trained with a maximum sequence length of 512 tokens (for longer documents, we randomly extracted subsequences of this length.) Further training details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Perplexity</head><p>We computed the perplexity of our trained models on the validation sets of LM1B and Wiki-40B, and on subsets of the C4 validation set (Figure <ref type="figure">2</ref>). For the base size, we observe that all models have similar perplexity on the original C4 validation set and on validation set examples that were identified as unique (no near-duplicate in either train or validation). However, both models trained on deduplicated data have significantly higher perplexity on validation set examples that have duplicates in the training set than the model trained on the original C4. EXACTSUBSTR-deduplicated results in higher perplexity than NEARDUP-deduplicated. These trends holds true for the XL sized model as well. While this may suggest EXACTSUBSTR duplication results in models least overfit on the train set, note that both of these techniques have used separate duplicate thresholds and a different choice of thresholds could change the results. When evaluating on the validation sets of LM1B and Wiki-40B, we found that models trained on NEARDUP-deduplicated C4 consistently achieved lowest perplexity (for LM1B eval with base models, see Appendix Figure <ref type="figure" target="#fig_2">7</ref>). EXACTSUBSTR deduplication decreases perplexity of the XL model by almost 3 points perplexity on Wiki-40B which is  much larger than the variation of about 1 point perplexity we observed in the base models. This is despite seeing fewer tokens of training data overall. Lastly, we note all our XL models achieved &lt;35 perplexity on LM1B, which is less than the 42.16 perplexity reported for the 1.5B GPT-2 using a vocabulary the same size as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generated Text</head><p>Data duplication has the effect of biasing the trained LM towards particular types of examples. This can contribute to a lower diversity of generations, and increased likelihood that the generated content is copied from the training data <ref type="bibr" target="#b8">(Carlini et al., 2020)</ref>. For our generation experiments, we use top-k random sampling with k = 50 and experiment with prompted and unprompted generation.</p><p>No prompt. We first evaluate memorization tendencies in the case where the model is asked to generate text without any prompt sequence. We generate 100,000 samples, each up to 512 tokens in length (examples provided in the Appendix). For each generated token, we say the token is memorized if it is part of a 50-token substring that is exactly contained in the training data. On XL-ORIGINAL, over 1% of the generated tokens belong to memorized sub-sequences (see Table <ref type="table" target="#tab_8">4</ref>). This is ∼ 10× more memorization than XL-EXACTSUBSTR or XL-NEARDUP. Some example subsequences that were copied verbatim from the train set can be found in Table <ref type="table" target="#tab_16">9</ref>  amples identified as unique across all splits (valid unique). We select the first 32 tokens of each example as the prompt, which means we can evaluate the fraction of generations which are near-duplicates with the ground-truth continuation for the prompt (Figure <ref type="figure">3</ref>). When the prompt comes from duplicate examples in the train set, XL-ORIGINAL reproduces the groundtruth continuation over 40% of the time. XL-EXACTSUBSTR and XL-NEARDUP still copy the groundtruth more often when the prompt comes from a duplicate example than when the prompt comes from a unique example, suggesting that more stringent deduplication may be necessary to remove memorization tendencies entirely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Impact on Existing Models</head><p>Train-test leakage does not just impact models trained on C4. Table <ref type="table" target="#tab_9">5</ref> shows that the presence of near-duplicates of the evaluation set in the train set has a significant impact on model perplexity for two standard models: Transformer-XL <ref type="bibr" target="#b11">(Dai et al., 2019)</ref>, which was trained on LM1B, and GROVER <ref type="bibr" target="#b43">(Zellers et al., 2019)</ref>, which was trained on RealNews. For Transformer XL, the perplexity halves on examples identified as near-duplicates.</p><p>For GROVER, the difference, though not quite as stark, is present in both model sizes considered.</p><p>Existing models also suffer from the problem of generating text from their train sets. We find that 1.38% of the tokens in the official release of 25k GROVER-Mega outputs<ref type="foot" target="#foot_1">3</ref> are part of verbatim matches in RealNews of at least length 50. Likewise, more than 5% of the tokens in ~200k sequences outputted by GPT-Neo 1.3B <ref type="bibr" target="#b3">(Black et al., 2021)</ref> are part of a 50 token matches of its training data, the Pile <ref type="bibr" target="#b16">(Gao et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>The focus of this paper is on the datasets used to train language models. While recent work focused on documenting the potential harms that could arise from problematic datasets <ref type="bibr" target="#b1">(Bender and Friedman, 2018;</ref><ref type="bibr" target="#b17">Gebru et al., 2020)</ref>, less work has been done to quantitatively analyze properties of real language modelling datasets, like <ref type="bibr" target="#b12">Dodge et al. (2021a)</ref> has done for C4. Our paper provides analysis on one particular axis, that of data duplication.</p><p>Our experiments measured what could be quantified: the amount of duplicate content in common datasets, the effect of deduplication on trained model perplexity, and the reduction of memorized content in trained models through deduplication. We do not focus on the nature of the data being removed by deduplication or memorized by LMs.</p><p>Privacy is an important subject for future work, as memorized training data has significant privacy consequences. By this, we mean the standard privacy definition that a model should not reveal anything particular to the specific dataset it was trained on, as opposed to another training dataset from a similar distribution <ref type="bibr" target="#b31">(Shokri et al., 2017)</ref>. <ref type="foot" target="#foot_2">4</ref> Training on standard datasets that have not yet been deduplicated results in models that are particularly sensitive to examples that happened to be repeated multiple times, and this has negative privacy implications. For instance, it could violate a person's expectations of privacy if their publicly available personal data appeared in a different, surprising context. Downstream applications of LMs, such as the game AI Dungeon<ref type="foot" target="#foot_3">5</ref> , should also not output memorized content like adverts for real products.</p><p>We stress that in our experiments, we do not distinguish between undesired memorized text (such as phone numbers), innocuous memorized text (common phrases), and text we may want to be memorized (such as a quote by a public figure), and instead treat all instances of the LM generating text that closely matches the training set as problematic. While we qualitatively observed that much of the identified memorized content was relatively innocuous, a more systematic study of the risks associated with the detected memorization was beyond the scope of this work.</p><p>We also do not investigate the negative consequences of deduplication. Some language tasks explicitly require memorization, like document retrieval or closed-book question answering. Also, text that gives attribution is often duplicated across documents, so removing duplicate substrings could correspond to removing just the attribution, which could result in models that learn the content without its attached attribution. Deduplication is also not sufficient to remove privacy-sensitive data like bank passwords and medical records which should never be used in training data <ref type="bibr" target="#b6">(Brown et al., 2022)</ref>.</p><p>Ultimately, whether memorization is a desired property of a language model, or else risky and unwanted, depends both on the nature of the text that has been memorized and on the downstream applications of the trained model. However, since the trend has been towards creating datasets and models that are application-agnostic, we encourage researchers to think carefully about the limitations of the data they have collected and the how the model's intended usage constrains what should be part of the training set. Developing techniques to memorize or forget specific sequences depending on the end application is a promising research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We encourage future language model research to perform dataset deduplication, either by training on the deduplicated datasets we release, using the deduplication tools we release, or following our approach to deduplicate datasets with new tools.</p><p>The exact technique used to perform deduplication is less important than performing stringent deduplication in the first place. On the whole, dedu-plication does not harm, and sometimes improves, model perplexity, despite the fact that the deduplicated datasets are smaller and faster to train on. It is especially important that there are no duplicates between the training and testing sets, because overlap here explicitly encourages selecting models that memorize the training data. Lastly, deduplication helps to reduce some of the privacy concerns around LMs memorizing their training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics</head><p>The developers of large language models typically attempt to create training data that reflects natural human communication, but current methods to collect and curate such datasets are fallible. There are multiple reasons some text ends up over-represented. For example, bot replies, auto-generated templates, and licenses are repeated for structural (e.g., legal, economical) reasons (as was also observed by <ref type="bibr" target="#b12">Dodge et al. (2021a)</ref>). Additionally, common techniques for acquiring and "cleaning" data can result in an over-representation of particular subsets of world users, often those who are English-speaking and publishing in established forums. This effectively under-represents non-English speakers as well as groups whose communication mostly occurs outside of the public web. In this paper, we focus on the problem of over-representation of some types of text (structural duplicates) but do not address the problem of under-representation of others.</p><p>Additionally, while we discuss when memorized content might be desired and when it might not be desired, our analysis does not disambiguate these two cases. Work to disambiguate helpful from harmful memorization is tremendously complex and would require a different set of research methodologies than are presented in this work.</p><p>Brain women who have given us continuous support.</p><p>Chris Callison-Burch and Daphne Ippolito's research is supported in part by the DARPA KAIROS Program (contract FA8750-19-2-1004), the DARPA LwLL Program (contract FA8750-19-2-0201), and the IARPA BETTER Program <ref type="bibr">(contract 2019-19051600004)</ref>. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, IARPA, or the U.S. Government.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>Each of the authors on this paper significantly contributed to the final results.</p><p>• Katherine trained the models used in the paper, built and ran the eval and text generation pipelines, contributed significantly to writing, analysis, and project organization and management.</p><p>• Daphne ran the approximate matching data deduplication pipelines, extracted prompts and evaluation datasets, ran eval pipelines, and contributed significantly to planning, writing, and analysis.</p><p>• Andrew wrote the code to perform deduplication with approximate matching, helped evaluate energy expenditure, and helped with analysis.</p><p>• Chiyuan helped generate plots and contributed to project scoping, writing, and data analysis.</p><p>• Chris offered mentorship and guidance throughout the project and contributed to writing.</p><p>• Doug offered mentorship and guidance throughout the project and contributed to writing.</p><p>• Nicholas wrote the suffix array implementation, ran all EXACTSUBSTR deduplication experiments, contributed significantly to planning, writing, and analysis, as well as scoping the project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Miltiadis </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Details on NEARDUP</head><p>For our MinHash based deduplication method, documents are first space tokenized, then each consecutive 5-gram is hashed using tabulation hashing. The set of these hashes is the signature for the document. For each element in a document's signature, the element is hashed using k other hash functions.</p><p>The minimum hashed element for each of the k hash functions is stored. These minimum hashes are then partitioned into r buckets, with b hashes per bucket. These b hashes are augmented into a single value, then if two documents have the same value in at least one bucket, they'll be marked as a potential match. The probability that two documents are considered a potential match is equal to</p><formula xml:id="formula_4">Pr(d i , d j | Jaccard(d i , d j ) = s i,j ) = 1−(1−s b i,j ) r</formula><p>where s i,j is the Jaccard index between the two documents i and j. For document pairs that were identified as potential matches, we computed their actual Jaccard index, and if that was above 0.8, we computed their edit similarity. Document pairs with edit similarity higher than 0.8 were identified as duplicates. After some experimentation, we chose to use b = 20, and r = 450, so k = 9, 000, so as to make sure a collision at the desired Jaccard index threshold of 0.8 had a high probability of occurring.</p><p>We also tested an alternative configurationfiltering to document pairs with Jaccard index of at least 0.9 and edit similarity of at least 0.9. In this case, we used b = 20, r = 40, and k = 800. Figure <ref type="figure">4</ref> shows the histogram of Jaccard similarities and edit similarities for all document pairs which collided in min-hash space, for our chosen configuration (blue) and for the alternative configuration (orange). This allows us verify if the threshold chosen has few comparisons around the chosen threshold, then we've likely captured the majority of actual near duplicates above that threshold. To verify that yourself, look at the left hand tails of the distributions. Since both 0.8 and 0.9 begin to vanish at the same point (in spite of the fact that the two thresholds are optimized for accuracy around different thresholds), we feel comfortable saying that we're capturing the majority of actual near duplicates.</p><p>Computational Analysis Let N be the number of documents and T be the maximal number of to-kens in a document. Edit similarity has a worst case complexity of T 2 , so the worst case complexity is</p><formula xml:id="formula_5">O(N + bk 2 T 2 N ) = O(N )</formula><p>since b, k, and T are all N . The left term is the complexity of grouping by the signatures, and the right represents the pathological worst case of all documents falling into the same B buckets.</p><p>The highly distributed NEARDUP implementation we employed is one used for large-scale production tasks at Google. On the English C4 dataset, the algorithm consumed approximately 41.5 kWh of energy. Note that our choices of k and b were designed to produce very high recall, and with different parameters, the algorithm could be made much more energy efficient while producing similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Details on EXACTSUBSTR</head><p>Parallel linear time construction. We build a parallelized linear time suffix array algorithm. As a building block, we make black-box use of the SA-IS algorithm for constructing a suffix array in linear time <ref type="bibr" target="#b25">Nong et al. (2009)</ref>; <ref type="bibr" target="#b23">Ko and Aluru (2003)</ref>. Unfortunately, this algorithm is not easily parallelized directly, so we introduce a simple divide and conquer approach to parallelizing the array construction.</p><p>We build our implementation in Rust and extend an existing suffix array library<ref type="foot" target="#foot_4">6</ref> with three modification. The first two are straightforward implementation differences: we modify the code to allow datasets larger than 4GB, and we remove the requirement that strings parse as valid UTF-8 sequences in favor of raw byte sequences. Our third change is more significant: we re-implement the algorithm so that we can stream the suffix array itself off disk.</p><p>Parallel partial suffix array construction. Our divide and conquer suffix array construction algorithm starts by partitioning the dataset into K different "splits" with SA-IS run over independently on each split in parallel. This algorithm still requires O(N ) work but runs in O(N/K) wall-clock time. This gives us N separate suffix arrays A i .</p><p>Given two suffix arrays A 1 and A 2 for two sequences S 1 and S 2 it's not completely trivial to construct a single suffix array A for S = S 1 || S 2 because of the boundary conditions. Instead, we Observe that in the general case this algorithm is O(N m log(K)) where N is the length of the dataset, m is the average length of a prefix match, and K is the number of splits. It is therefore incorrect to call this algorithm linear time in the general case, for ours it is. Because the length of the longest match is bounded above by the length of the longest sequence, as long as the size of the dataset is independent of the length of the longest sequence in the dataset, this algorithm remains efficient.</p><p>Again, we can parallelize this operation among L simultaneous jobs (in practice we set K = L as the number of threads on our machine). In the K = 2 case, job l processes i ∈ [jN/L, (j + 1)N/L], choosing the bounds of j by binary searching into C so that S B i &lt; S C j &lt; S B j+1 . The case where K &gt; 2 is identical except that we repeat this over all K partial suffix arrays.</p><p>Computational Analysis. We run our algorithm on a single VM on the cloud with 96 cores and 768GB of memory. Our algorithm is efficient, for example processing the Wiki-40B training set (3 million examples containing 4GB of text) in 2.3 minutes wall-clock time (2.1 CPU-hours of work). The 350GB C4 dataset takes under 12 hours (wallclock) to build a suffix array; although we are still memory constrained and so this corresponds to ∼ 1000 CPU-hours. Once the suffix array has been constructed, it takes under an hour to deduplicate the C4 dataset.</p><p>Note that this algorithm still requires that the dataset itself fits in memory (so that we can efficiently index in arbitrary positions), but we do not need to fit the entire suffix array into memory. This is fortunate since our suffix array requires an 8× space overhead. For example, the suffix array for the 350GB C4 is 1.5TB.</p><p>Compared to the cost of training a language model on this dataset, the additional work required to deduplicate the training dataset is negligible.</p><p>Setting a threshold of duplicates. An important question is how long must a substring match be before it is counted as a duplicate. In Figure <ref type="figure">5</ref>, we plot the frequency of substring matches within the four datasets we will consider. For each substring of length k, we compute the probability that there exists another sequence of length k identical to this</p><formula xml:id="formula_6">LM1B C4 RealNews Wiki-40B</formula><p>Figure <ref type="figure">5</ref>: For each substring of length k, we plot the probability that there exists a second identical lengthk substring in the same train set. Matches with length under 10 subword tokens are common, and account for 90% of tokens. We choose a threshold of 50 for experiments.</p><p>one; formally:</p><formula xml:id="formula_7">m(k) = Pr i∈[N ] ∃j = i : S i..i+k = S j..j+k .</formula><p>We choose 50 tokens as the threshold to be conservative: the "bend in the knee" occurs at 10 tokens, and manual inspection of length-25 matches found no false positives. We then doubled this value to have an exceptionally large margin for error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Details on Model Training</head><p>Each model was trained for two epochs. Since both C4-ORIGINAL and C4-EXACTSUBSTR contain approximately 365M examples, we performed 152K steps with a batch size of 4800 (or approximately 2 epochs). C4-NEARDUP contains approximately 350M examples, we performed 146K steps (or approximately 2 epochs). On a 128-core TPU v3 pod slice, XL models trained on C4-ORIGINAL and C4-EXACTSUBSTR took approximately 131 hours (5.5 days) to train, while the XL model trained on C4-NEARDUP took approximately 126 hours to train. Like T5, models were trained with the Adafactor optimizer <ref type="bibr" target="#b29">(Shazeer and Stern, 2018)</ref>. A constant learning rate of 0.01 was used for the base models and 0.001 for the XL models.</p><p>The 1.5B parameter XL models had 24 layers, each with 32 attention heads. The model embedding size was 2,048, the feed forward layers had a hidden size of 5,120, and the key/value dimension size for the attention heads 64. The 110M parameter base models had 12 layers, each with 12 attention heads. The model embedding size was 768, the feed forward layers had a hidden size of 2,048, and the key/value dimension size for the attention heads 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Energy Consumption</head><p>We trained for approximately 131 hours or 5.5 days on a 128-core TPU v3. The approximate deduplicated dataset is 3.9% smaller than the original dataset and trains in 63 hours/epoch, saving us around 5 hours of compute time for the two epochs. The XL-ORIGINALmodel was trained in North America where the XL-EXACTSUBSTR and XL-NEARDUP were trained in Taiwan. We used data from <ref type="bibr" target="#b26">Patterson et al. (2021)</ref> to estimate amount of energy used in training these models by computing the amount of M W h/hour/core and multiplying by our usage (see Table <ref type="table" target="#tab_12">6</ref> for how we computed these values). For simplicity, we use estimates from Taiwainese datacenters as an estimate. We estimate training 2 epochs of XL-ORIGINAL and XL-EXACTSUBSTR uses 5.86M W h. XL-NEARDUP is trained for fewer steps and we estimate uses 5.63M W h. Training each base model was approximately 3 days on a 64-core TPU v3 pod slice which uses an estimated 1.61M W h.</p><p>In addition to model training, evaluation and inference were performed on 64-core TPU v3 pod slices. Generating 100,000 sequences from the XL models takes approximately 0.64 hours. We generated 100,000 sequences for each of five types of prompts for two checkpoints of the model for a total of 1M sequences per model. This took approximately 19.2 hours. We estimate generating 3M sequences uses 0.43M W h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Results</head><p>Qualitative Examples. Table <ref type="table" target="#tab_15">8</ref> shows several examples of pairs of documents in C4 whose edit distance is close to our chosen edit similarity threshold of 0.8. Table <ref type="table" target="#tab_16">9</ref> shows substrings which were identified by EXACTSUBSTR as being in C4 more than once. Table <ref type="table">10</ref> shows several examples of unprompted generations which were identified as memorized are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of memorization.</head><p>Figure <ref type="figure" target="#fig_1">6</ref> shows the distribution in memorization amount over all generated sequences when using four types of prompting: train example with duplicates in train,   URLs with many duplicates. Table <ref type="table">11</ref> shows the URLs had the largest proportion of examples identified by NEARDUP as near-duplicates. For C4, these tend to be websites that sell many similar products and thus have a large amount of templated text. For RealNews, content aggregators seem especially common.</p><p>NEARDUP cluster sizes. Figure <ref type="figure">8</ref> shows the distribution of cluster sizes from running NEARDUP on RealNews, LM1B, and Wiki-40B (results for C4 are in Figure <ref type="figure" target="#fig_0">1</ref> the main paper).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Sizes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Text</head><p>Freq in C4</p><p>, you'll need to be knowledgeable to make the very best decisions. We will make sure you know what can be expected. We take the surprises from the picture by giving accurate and thorough information. You can start by talking about your task with our client service staff when you dial 888-353-1299. We'll address all of your questions and arrange the initial meeting. We work closely with you through the whole project, and our team can show up promptly and prepared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5,497</head><p>then Waterside Lodge are well equipped for the task. Our fully equipped family sized lodges offer a comfortable luxurious stay for a fantastic price, giving you beautiful views of the lakes and the surrounding countryside. Offering luxurious self-catering holidays in our fully featured Scandinavian holiday lodges. Perfectly located to explore the beaches, coastline. All of our lodges are sized for 6 people and are furnished to the highest standards to ensure you have a stay like no other. At Waterside Lodge the stay itself is only half of the package, Waterside lodge is situated closely to the Heritage Coast which makes our lodges the perfect stay for anyone wanting to get away and have a relaxing countryside break from the city. Whilst you stay with us be sure to take advantage of all the activities Waterside Lodge has to offer. Such as the use of our on-site fishing lakes for the keen fisherman, free internet access, outside relaxation areas, comfortable lounges and much more.</p><p>571 you are only looking to find rent to own homes in your city or are open to exploring all kinds of rent to own home listings, our database does it all. One of the best aspects of iRentToOwn.com is that, besides options to rent to buy a house, it has numerous other categories of home sale options. These include bank foreclosure homes, pre-foreclosure homes, short sales, HUD/government foreclosures, auction homes and owner-financing/FSBO (For Sale By Owner) homes. With help from the convenient search features offered by our site, shoppers are able to find their ideal lease to own home, real estate company, and more in South</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>51</head><p>, IL employs journeyman as licensed to work by themselves, without direct supervision, installing wiring, outlets and fixtures. Our journeyman also does service work, troubleshooting when a breaker fails or a light stops working. Our journeyman does not offer permits that must be issued by our master. Our journeyman follows our master's plans and directions. Our journeyman's responsibilities will vary based on the work that needs to be done. Our journeymen are skilled with residential, commercial and industrial installations and repairs.ust work from six years as an apprentice, under direct supervision of our master, and pass a journeyman test. This person also must have some classroom education on the National Electrical Code and fundamental electricity in a technical school a program affiliated with the National Joint Apprenticeship Training Council. Journeyman training combines hands-on work with education on basic electricity.</p><p>6 combustion process of a petrol engine is never perfect. Dangerous gases, such as nitrogen oxide, carbon monoxide and hydrocarbons will arise and it is the job of the catalytic converter to reduce these to safer emissions. These cat converters can fail by becoming clogged, or if the engine has bad exhaust valves or the plugs fail, causing unburned fuel to overheat the converter. Mettam's Mufflers can resolve these issues with your Karr 5 ,ANDREW Find the ancestral town: Many a researcher is stuck behind records that say, BIRTHPLACE: IRELAND without saying where in Ireland, or whatever other country. Remember that your immigrant ancestor's siblings probably were born in the same ancestral town, so check all o f their records, too. Around 1900, the Roman Catholic churches reported marriages to the churches where the persons were baptised, and before the wedding, they would require a baptismal certificate from that church, without marriage notations, to make sure that the persons were no t already married, ordained, or whatever, and were free to marry. Do check the Catholic records especially for ex loco and the home town. If your ancestor's sister had a daughter who generated a marriage or death record saying, MOTHER'S BIRTHPLACE: and the exact town, then y ou know where to start searching for records that will confirm it is your ancestor's home town. BEWARE: Just because you find a family with the same names does not mean they are the same family, as they could very well be an unrelated family from a different town in the same an cestral country. The webmaster has learned this. One clue was that one family was still having babies in Potenza city, Italy while the other was having babies in Colorado, U.S.A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>will not want to search for Power Washing companies in Wyoming on an extensive basis. The service personnel will be at your doorsteps through online or phone booking. The power wash solutions offered by us are matchless and you can compare with others in Winfield, IL. The power wash services offered by us are very economical. Gutter brightener will be applied which will be followed by cleaning through double scrub. The cleaning will be done by using a soft bristle brush. The bond and contaminants will be released in an effortless manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Z3 Plus are valid in all major cities of India like Delhi, Gurgaon, Noida, Mumbai, Chennai, Bangalore, Hyderabad, Kolkata, Pune, Ahmedabad, Coimbatore, Lucknow, Trichy, Madurai, Trivandrum, Mysore, Jaipur, Chandigarh, Pondicherry, Bhopal, Patna, Bhubaneswar, Amritsar, Cochin, Allahabad, Srinagar, New Delhi, Surat, Ludhiana, Navi Mumbai, Ghaziabad, Bengaluru, Indore, Nagpur, Thane, Agra, Meerut, Ranchi. The delivery feasibility and charges may be varying, hence for them please check with the particular seller or store.</p><p>1 Table <ref type="table">10</ref>: A selection of substrings generated by XL-ORIGINAL with no prompting (and top-k with k=50) that were identified by EXACTSUBSTR as being in C4 multiple times. The number of times each substring was found in C4 is given. We observe that most memorized generations tend to be from advertisements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The distribution of near-duplicate cluster sizes from running NEARDUP on C4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Memorized continuations distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impact of deduplicating the training set on validation perplexity. In (a), we plot the results from T5 base (110M parameters) across three training runs with different random initializations. The black bar represent the lowest perplexity to the highest perplexity, and the colored bar the median perplexity. In (b), we plot the results from T5 XL (1.5B parameters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The fraction of examples identified by NEARDUP as near-duplicates.</figDesc><table><row><cell></cell><cell cols="2">% train tokens with</cell><cell>% valid with</cell></row><row><cell></cell><cell cols="2">dup in train dup in valid</cell><cell>dup in train</cell></row><row><cell>C4</cell><cell>7.18%</cell><cell>0.75 %</cell><cell>1.38 %</cell></row><row><cell>RealNews</cell><cell>19.4 %</cell><cell>2.61 %</cell><cell>3.37 %</cell></row><row><cell>LM1B</cell><cell>0.76%</cell><cell>0.016%</cell><cell>0.019%</cell></row><row><cell>Wiki40B</cell><cell>2.76%</cell><cell>0.52 %</cell><cell>0.67 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The fraction of tokens (note Table2reports the fraction of examples) identified by EXACTSUBSTR as part of an exact duplicate 50-token substring.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>When generating 100k sequences with no prompting, over 1% of the tokens emitted from a model trained on the original dataset are part of a 50-token long sequence copied directly from the training dataset. This drops to 0.1% for the deduplicated datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>in the Appendix. For each model, the perplexity of the official validation set (Orig), valid set examples which were identified by NEARDUP as matches of train set examples (Dups), and valid set examples identified by NEARDUP as unique (Unique). Due to the size of the RealNews validation set, we evaluated on only the first 25k examples meeting each condition.</figDesc><table><row><cell>With prompting. In most real use cases, lan-</cell></row><row><cell>guage model generation is controlled by providing</cell></row><row><cell>a prompt for the model to continue. We experi-</cell></row><row><cell>ment with four possible prompt sources: training</cell></row><row><cell>examples identified by EXACTSUBSTR as having</cell></row><row><cell>near-duplicates in the train set (train dup), train-</cell></row><row><cell>ing examples identified as unique (train unique),</cell></row><row><cell>validation set examples with a near-duplicate in</cell></row><row><cell>the train set (valid in train), and validation set ex-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Estimates of energy usage based on the data in<ref type="bibr" target="#b26">Patterson et al. (2021)</ref>. The first column is<ref type="bibr" target="#b26">Patterson et al. (2021)</ref>'s estimate of the T5 11B encoder-decoder model, which we based our own estimates on. Inference includes all XL models. We generated 100,000 sequences from 3 models, with 5 prompts, and at 2 different checkpoints.).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>XL-ORIGINAL</cell><cell></cell><cell></cell><cell>Base-ORIGINAL</cell></row><row><cell></cell><cell></cell><cell>T5 11B</cell><cell cols="3">XL-EXACTSUBSTR XL-NEARDUP</cell><cell>Base-EXACTSUBSTR Total Inference</cell></row><row><cell cols="2">TPU v3 cores</cell><cell>512</cell><cell cols="2">128</cell><cell>128</cell><cell>64</cell><cell>64</cell></row><row><cell cols="2">Training time (days)</cell><cell>20</cell><cell cols="2">5.47</cell><cell>5.26</cell><cell>3</cell><cell>0.80</cell></row><row><cell>TPU hrs</cell><cell></cell><cell>245760</cell><cell cols="2">16804.70</cell><cell>16149.31</cell><cell>4608</cell><cell>1228.80</cell></row><row><cell cols="2">Energy (MWh)</cell><cell>85.70</cell><cell cols="2">5.86</cell><cell>5.63</cell><cell>1.61</cell><cell>0.43</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell>Example</cell><cell></cell><cell></cell><cell>Near-Duplicate Example</cell></row><row><cell>Wiki-40B</cell><cell cols="3">\n_START_ARTICLE_\nHum</cell><cell>Award</cell><cell cols="2">\n_START_ARTICLE_\nHum Award for Best Actor</cell></row><row><cell></cell><cell>for</cell><cell>Most</cell><cell>Impactful</cell><cell>Character</cell><cell cols="2">in a Negative Role \n_START_SECTION_\nWinners</cell></row><row><cell></cell><cell cols="4">\n_START_SECTION_\nWinners and nom-</cell><cell cols="2">and nominees\n_START_PARAGRAPH_\nIn the list</cell></row><row><cell></cell><cell cols="4">inees\n_START_PARAGRAPH_\nIn the list</cell><cell cols="2">below, winners are listed first in the colored row, fol-</cell></row><row><cell></cell><cell cols="4">below, winners are listed first in the colored row,</cell><cell cols="2">lowed by the other nominees. [...]</cell></row><row><cell></cell><cell cols="3">followed by the other nominees. [...]</cell><cell></cell><cell></cell></row><row><cell>LM1B</cell><cell cols="4">I left for California in 1979 and tracked Cleveland</cell><cell cols="2">I left for California in 1979 , and tracked Cleveland</cell></row><row><cell></cell><cell cols="4">'s changes on trips back to visit my sisters .</cell><cell cols="2">'s changes on trips back to visit my sisters .</cell></row><row><cell cols="5">RealNews KUALA LUMPUR (Reuters) -Roads in South-</cell><cell cols="2">A visitor looks at a Triumph motorcycle on dis-</cell></row><row><cell></cell><cell cols="4">east Asia have been getting a little louder lately</cell><cell cols="2">play at the Indonesian International Motor Show</cell></row><row><cell></cell><cell cols="4">as motorcycle makers, an aspiring middle class and easy bank credit come together to breed a new genus of motorcyclists -the big-bike rider. [...]</cell><cell cols="2">in Jakarta September 19, 2014. REUTERS/Darren Whiteside\nKUALA LUMPUR (Reuters) -Roads in Southeast Asia have been getting a little [...] big-bike rider. [...]</cell></row><row><cell>C4</cell><cell cols="4">Affordable and convenient holiday flights take</cell><cell cols="2">Affordable and convenient holiday flights take off</cell></row><row><cell></cell><cell cols="4">off from your departure country, "Canada". From</cell><cell cols="2">from your departure country, "USA". From April</cell></row><row><cell></cell><cell cols="4">May 2019 to October 2019, Condor flights to your</cell><cell cols="2">2019 to October 2019, Condor flights to your dream</cell></row><row><cell></cell><cell cols="4">dream destination will be roughly 6 a week! Book</cell><cell cols="2">destination will be roughly 7 a week! Book your</cell></row><row><cell></cell><cell cols="4">your Halifax (YHZ) -Basel (BSL) flight now, and</cell><cell cols="2">Maui Kahului (OGG) -Dubrovnik (DBV) flight now,</cell></row><row><cell></cell><cell cols="4">look forward to your "Switzerland" destination!</cell><cell cols="2">and look forward to your "Croatia" destination!</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Qualitative examples of near-duplicates identified by NEARDUP from each dataset. The similarlity between documents is highlighted. Note the small interspersed differences that make exact duplicate matching less effective. Examples ending with "[...]" have been truncated for brevity.</figDesc><table><row><cell></cell><cell>Original</cell><cell>model NearDup</cell><cell>ExactSubstr</cell></row><row><cell>edit sim between generated and groundtruth continuations</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell></row><row><cell></cell><cell>train dup</cell><cell cols="2">train unique Prompt Source valid in train</cell><cell>valid unique</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table13gives the size in BPE tokens and in examples of each dataset before and after deduplication. Because most datasets were Due to high demand, we have yet to critique this request. That said, we assure that the review will be produced in due time by our dilligent and unwavering staff in a professional manner. This site is highly regarded amongst its peers in terms of speed and reliability, so feel free to check us out! Due to a heavy overflow, we have not been able to critique this request. That said, we assure that the review will be produced in due time by our dilligent and unshakable staff in a professional manner. This site is highly regarded amongst its peers in terms of efficiency and reliability, so feel free to visit! Need Pop Tacos parking? You can reserve parking near Pop Tacos with SpotHero. Find low rates without parking coupons by booking a guaranteed spot online. Avoid circling, getting ticketed or running out to feed your meter. Search our parking map, compare parking rates and reserve a discounted parking spot today. Happy parking, and enjoy your meal at Pop Tacos! Il Sole parking. Reserve parking near Il Sole in NYC.\nYou can reserve parking near Il Sole with SpotHero. Find low rates without parking coupons by booking a guaranteed spot online. Avoid circling, getting ticketed or running out to feed your meter. Search our parking map, compare parking rates and reserve a discounted parking spot today. Happy parking, and enjoy your meal at Il Sole! This item was available on Vinyl 7" but is now sold out on all formats, sorry. Take a look at what else we have in by Jumbo, check out some related artists, head over to our new releases or knock yourself out reading our latest music news &amp; album reviews.\n2nd single edn of 550. This item was available on CD but is now sold out on all formats, sorry. Take a look at what else we have in by Sirconical, Misty Dixon, Various, check out some related artists, head over to our new releases or knock yourself out reading our latest music news &amp; album reviews.\nTwisted Nerve comp mini album.</figDesc><table><row><cell>Here is all the information you need about "No One Killed</cell><cell>Here is all the information you need about "A Land Imagined"</cell></row><row><cell>Jessica" on American Netflix. Details include the date it was</cell><cell>on Netflix in the UK. Details include the date it was added to</cell></row><row><cell>added to Netflix in the USA, any known expiry dates and new</cell><cell>UK Netflix, any known expiry dates and new episodes/seasons,</cell></row><row><cell>episodes/seasons, the ratings and cast etc. So scroll down for</cell><cell>the ratings and cast etc. So scroll down for more information</cell></row><row><cell>more information or share the link on social media to let your</cell><cell>or share the link on social media to let your friends know what</cell></row><row><cell>friends know what you're watching.</cell><cell>you're watching.</cell></row><row><cell>8 + 8 = Solve this simple math problem and enter the result.</cell><cell>Math question * 7 + 1 = Solve this simple math problem and</cell></row><row><cell>E.g. for 1+3, enter 4.</cell><cell>enter the result. E.g. for 1+3, enter 4.</cell></row><row><cell>Long Island College Hospital is committed to providing out-</cell><cell>Morristown Memorial Hospital is committed to providing out-</cell></row><row><cell>standing patient care in the Brooklyn, NY area, but before you</cell><cell>standing patient care in the Morristown, NJ area, but before</cell></row><row><cell>commit to Long Island College Hospital for a Endometrial</cell><cell>you commit to Morristown Memorial Hospital for a Breast</cell></row><row><cell>Ablation make sure you compare and shop other medical fa-</cell><cell>Ultrasound make sure you compare and shop other medical</cell></row><row><cell>cilities. It may save you hundreds (in some cases thousands)</cell><cell>facilities. It may save you hundreds (in some cases thousands)</cell></row><row><cell>of dollars. View a Endometrial Ablation cost comparison for</cell><cell>of dollars. View a Breast Ultrasound cost comparison for</cell></row><row><cell>Brooklyn and Request a Free Quote before you make a deci-</cell><cell>Morristown and Request a Free Quote before you make a</cell></row><row><cell>sion.</cell><cell>decision.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Several examples of pairs of documents in C4 that were found by the Approximate Matching algorithm and identified as having edit similarity of almost exactly 0.8. Pairs of documents less similar than 0.8 were not identified as duplicates. For readability, matching subsequences have been highlighted. HD wallpaper. This wallpaper was upload at April 19, 2019 upload by admin in.You can download it in your computer by clicking resolution image in Download by size:. Don't forget to rate and comment if you interest with this wallpaper. 40,340to the address posted below. Include our failure information form,a packing slip with your Company name, contact person, and Email address or phone number. Upon receipt of your repair, we\'ll inspect it and then contact you with a quote or evaluation notice. Normal turn around for repair is 5 to 7 business days, with "Rush Repair" available. Desktop wallpapers were first introduced way back in the 1980s and have gained immense popularity since then. It is possible to come across more than 80 million sites on the web offering some sort of wallpaper. 848 flowers will let them know you're thinking of them and wishing them well. Cheerful yellow flowers bring their own sunshine and will get right to work on lifting spirits, and a colorful vase will bring loads of smiles to friends and visitors! Get Well flower arrangements from 479 our premier 24 hour emergency* plumbing and heating solutions. We realise that when your heating fails or pipes and drains leak it can cause havoc with your routine and even cause damage to your property. When a plumbing problem occurs that requires an immediate response we provide qualified local plumbers throughout 56 is to remove all images that violate copyrights. Please contact us to request that images be removed or to assign proper credit. The images displayed on this site may be used for Free or educational purposes only. If you would like to use any of the images displayed on this site for any other purpose, please obtain permission from the owner. www. 48 list of fishing locations, providing interactive maps that show each location's GPS coordinates, nearby facilities (like restaurants, gas stations, marinas and fishing shops), their current and forecasted weather and, if available, their water conditions.\nFind any of the 8 5 . Dyer, Ph.D., is an internationally renowned author and speaker in the field of self-development. He's the author of 30 books, has created many audio programs and videos, and has appeared on thousands of television and radio shows.</figDesc><table><row><cell>Text</cell><cell>Freq in C4</cell></row><row><cell></cell><cell>5,900</cell></row></table><note>5</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>A selection of substrings identified by EXACTSUBSTR as being in C4 multiple times. The number of times this exact substring occurs in C4 is also given.</figDesc><table><row><cell>already deduplicated of exact matches during their</cell></row><row><cell>creation, EXACTSUBSTRdeduplication does not</cell></row><row><cell>actually remove any examples.</cell></row><row><cell>Perplexity on LM1B. Figure 7 is the same as</cell></row><row><cell>Figure 2 of the main paper, except with perplexity</cell></row><row><cell>on LM1B included. LM1B was omitted from the</cell></row><row><cell>main paper's figure in order to improve readability.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">"by combining fantastic ideas, interesting arrangements, and follow the current trends in the field of that make you more inspired and give artistic touches. We'd be honored if you can apply some or all of these design in your wedding. believe me, brilliant ideas would be perfect if it can be applied in real and make the people around you amazed!"</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">gs://grover-mode s/generation_examp es/ generator=mega~dataset=p0.90.json</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Another interpretation of privacy focuses on the sensitivity of the data involved, when a model is trained on and able to reproduce personal identifiers or other forms of "private data." Our definition is more expansive.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://p ay.aidungeon.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">https://github.com/BurntSushi/suffix</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the many researchers whose technical help, feedback, and discussions shaped this project: Jacob Austin, Samy Bengio, Olivier Bousquet, James Bradbury, Fernando Diaz, Mark Diaz, Noah Fiedel, Jonathan Frankle, David Grangier, Stefanie Karp, David Mimno, Gaurav Mishra, Michael Mozer, Sharan Narang, Alex Passos, Adam Roberts, Hanie Sedghi, Jascha Sohldickstein, David So, Florian Tramer, and Yun William Yu. We are also grateful to the Google</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Pmlr</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jack</forename><surname>Bandy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicholas</forename><surname>Vincent</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2021</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
	<note>Addressing &quot;documentation debt. in machine learning research: A retrospective datasheet for bookcorpus</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data statements for natural language processing: Toward mitigating system bias and enabling better science</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batya</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00041</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="587" to="604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
				<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large scale autoregressive language modeling with meshtensorflow</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</title>
				<meeting>Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemehsadat</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>What does it mean for a language model to preserve privacy? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ulfar Erlingsson, Alina Oprea, and Colin Raffel</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extracting training data from large language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new suffix tree similarity measure for document clustering</title>
		<author>
			<persName><forename type="first">Hung</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242590</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW &apos;07</title>
				<meeting>the 16th International Conference on World Wide Web, WWW &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2016">2016. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Min-hash sketches: A brief survey</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Documenting the english colossal clean crawled corpus</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Agnew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08758</idno>
		<title level="m">Documenting the english colossal clean crawled corpus</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What neural networks memorize and why: Discovering the long tail via influence estimation</title>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying and characterizing highly similar notes in big clinical note datasets</title>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">A</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Ting</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2018.04.009</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="63" to="69" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Datasheets for datasets</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wiki-40b: Multilingual language model dataset</title>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deduplication of scholarly documents using locality sensitive hashing and word embeddings</title>
		<author>
			<persName><forename type="first">Bikash</forename><surname>Gyawali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Anastasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Knoth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
				<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="901" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The distribution of the flora in the alpine zone</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="1912">1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple linear work suffix array construction</title>
		<author>
			<persName><forename type="first">Juha</forename><surname>Kärkkäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International colloquium on automata, languages, and programming</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="943" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Space efficient linear time construction of suffix arrays</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Aluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Combinatorial Pattern Matching</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Suffix arrays: a new method for on-line string searches. siam Journal on Computing</title>
		<author>
			<persName><forename type="first">Udi</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><surname>Myers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="935" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linear suffix array construction by almost pure inducedsorting</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Nong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 data compression conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Carbon emissions and large neural network training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00268</idno>
		<title level="m">Towards controllable biases in language generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the geometry of generalization and memorization in deep neural networks</title>
		<author>
			<persName><forename type="first">Cory</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchismita</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sueyeon</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding invariance via feedforward inversion of discriminatively trained classifiers</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10225" to="10235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Not just bigger: Towards better-quality web corpora</title>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh Web as Corpus Workshop (WAC7)</title>
				<meeting>the seventh Web as Corpus Workshop (WAC7)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Universal adversarial triggers for attacking and analyzing nlp</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07125</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting overfitting of deep generative networks via latent recovery</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01153</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11265" to="11274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Linear pattern matching algorithms</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Annual Symposium on Switching and Automata Theory (swat 1973)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus</title>
		<author>
			<persName><forename type="first">Mikio</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12616</idno>
		<title level="m">Defending against neural fake news</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
