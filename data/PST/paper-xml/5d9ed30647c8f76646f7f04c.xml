<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jia</surname></persName>
							<email>jiawei07@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
							<email>daidai@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<email>xiaoxinyan@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose ARNOR, a novel Attention Regularization based NOise Reduction framework for distant supervision relation classification. ARNOR assumes that a trustable relation label should be explained by the neural attention model. Specifically, our ARNOR framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in training set, we will select it as a trustable instance for further training step. According to the experiments on NYT data, our ARNOR framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation Classification (RC) is a fundamental task in natural language processing (NLP) and is particularly important for knowledge base construction. The goal of RC <ref type="bibr" target="#b19">(Zelenko et al., 2003)</ref> is to identify the relation type of a given entity pair in a sentence. Generally, a relation should be explicitly expressed by some clue words. See the first sentence in Figure <ref type="figure" target="#fig_0">1</ref>. The phrase "was born in" explains the relation type "place of birth" for "Bill Lockyer" and "California". Such indicating words is called patterns <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, 2001)</ref>. The bold words "was born in" in s 1 is the pattern that explains the relation type "place of birth". Hence, this instance is correctly labeled. However, the second instance is noisy due to the lack of corresponding relation pattern. This model is trained using noisy data generated by distant supervision. It mainly pays attention to the input entity pair and ignores other words which might express the real relation. It also happens in Figure <ref type="figure" target="#fig_0">1</ref>. This result comes from the fact that DS method only depends on entities for labeling data.</p><p>In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref> was proposed to automatically generate training data by aligning a knowledge base with an unlabeled corpus. It is built on a weak assumption that if an entity pair have a relationship in a knowledge base, all sentences that contain this pair will express the corresponding relation.</p><p>Unfortunately, DS obviously brings plenty of noisy data, which may significantly reduce the performance of an RC model. There may be no explicit relation pattern for identifying the relation. See the second sentence in Figure <ref type="figure" target="#fig_0">1</ref> for example. <ref type="bibr" target="#b9">Mintz et al. (2009)</ref> reports that distant supervision may lead to more than 30% noisy instances. On the other hand, based on these noisy data, attention-based neural models often only attend to entity words but fail to attend to patterns (See Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>There are mainly three kinds of methods for dealing with such noise problem. First, multiinstance learning <ref type="bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref> relaxes the DS assumption as at-least-one. In a bag of sentences that mention the same entity pair, it assumes that at least one sentence expresses the relation. Multi-instance learning carries out classification on bag-level and often fails to perform well on sentence-level prediction <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Secondly, in order to reduce noise for sentence-level prediction, researchers then resort to reinforcement learning or adversarial training to select trustable data <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b10">Qin et al., 2018a;</ref><ref type="bibr" target="#b4">Han et al., 2018;</ref><ref type="bibr" target="#b18">Xiangrong et al., 2018;</ref><ref type="bibr" target="#b11">Qin et al., 2018b)</ref>. This line of research selects confident relation labels by matching the predicted label of the learned model with DSgenerated label. As the model is also learned from DS data, it might still fail when model predictions and DS-generated labels are both wrong. The third method relies on relation patterns. Pattern-based extraction is widely used in information extraction <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, 2001)</ref>. Among them, the generative model <ref type="bibr" target="#b17">(Takamatsu et al., 2012)</ref> directly models the labeling process of DS and finds noisy patterns that mistakenly label a relation. Data programming <ref type="bibr" target="#b13">(Ratner et al., 2016</ref><ref type="bibr" target="#b12">(Ratner et al., , 2017) )</ref> fuses DS-based labels and manual relation patterns for reducing noise.</p><p>In this paper, we propose ARNOR, a novel attention regularization based framework for noise reduction. ARNOR aims to train a neural model which is able to clearly explain the relation patterns through Attention Regularization (AR), and at the same time reduce noise based on an assumption: the clearer the model explain the relation in an instance, the more trustable this instance is. Specifically, our ARNOR framework iteratively learns the interpretable model and selects trustable instances. We first use attention regularization on the neural model to focus on rela-tion patterns (Section 3.4 will introduce the patterns construction). Then, if the learned model can dicover patterns for candidate instances, we will select these candidates as correct labeled data for further training step. These two steps are mutually reinforced. The more interpretable the model is, the better training data is selected, and vice versa.</p><p>In addition, most previous DS-based RC models are evaluated approximately on the test set which is split from the training set and thus is also full of noisy data. We argue that this might not be the best choice. Instead, we use a recently released sentence-level test set <ref type="bibr" target="#b14">(Ren et al., 2017)</ref> for evaluation. However, there also exist several problems in this test set (see Sec. 4.1). We come up with a revised version that is larger and more precise.</p><p>Overall, the contribution is as follows:</p><p>1. We propose a novel attention regularization method for reducing the noise in DS. Our method forces the model to clearly explain the relation patterns in terms of attention, and selects trustable instances if they can be explained by the model.</p><p>2. Our ARNOR framework achieves significant improvement over state-of-the-art noise reduction methods, in terms of both RC performance and noise reduction effect.</p><p>3. We publish a better manually labeled sentence-level test set<ref type="foot" target="#foot_0">1</ref> for evaluating the performance of RC models. This test set contains 1,024 sentences and 4,543 entity pairs, and is carefully annotated to ensure accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We deal with DS-based RC in this paper. For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks <ref type="bibr" target="#b21">(Zeng et al., 2014</ref><ref type="bibr" target="#b20">(Zeng et al., , 2015) )</ref> and recurrent neural network <ref type="bibr" target="#b22">(Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Zhou et al., 2016)</ref>. To automatically obtain a large training dataset, DS has been proposed <ref type="bibr" target="#b9">(Mintz et al., 2009)</ref>. However, DS also introduces noisy data, making DS-based RC more challenging.</p><p>Previous studies make attempts on kinds of methods to solve the noise problem. The first widely studied method is based on multi-instance  learning <ref type="bibr" target="#b15">(Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Lin et al., 2016;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b20">Zeng et al., 2015)</ref>. However, it models noise problem on a bag of instances and is not suitable for sentence-level prediction. The second kind of approach utilizes RL <ref type="bibr" target="#b2">(Feng et al., 2018b;</ref><ref type="bibr" target="#b18">Xiangrong et al., 2018;</ref><ref type="bibr" target="#b11">Qin et al., 2018b)</ref> or adversarial training <ref type="bibr" target="#b10">(Qin et al., 2018a;</ref><ref type="bibr" target="#b4">Han et al., 2018)</ref> to select trustable instances. The third research line relies on patterns <ref type="bibr" target="#b5">(Hearst, 1992;</ref><ref type="bibr" target="#b3">Hamon and Nazarenko, 2001)</ref>. <ref type="bibr" target="#b17">Takamatsu et al. (2012)</ref> directly models the labeling process of DS to find noisy patterns. <ref type="bibr" target="#b13">Ratner et al. (2016</ref><ref type="bibr" target="#b12">Ratner et al. ( , 2017) )</ref> proposes to fuse DS-based labels and manual relation patterns for reducing noise. <ref type="bibr" target="#b1">Feng et al. (2018a)</ref> presents a pattern extractor based on RL and uses extracted patterns as features for RC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The ARNOR Framework</head><p>In this paper, we reduce DS noise and make the model more interpretable according to the observation that a relation should be expressed by its sentence context. Generally, RC classifier should rely on relation patterns to decide the relation type for a pair of entities. Thus, for a training instance, if such an interpretable model cannot attend to the pattern that expresses the relation type, it is possible that this instance is a noise.</p><p>Our ARNOR Framework consists of two parts: attention regularization training and instance selection. First, we hope the model is capable of locating relation patterns. Thus, attention regularization is applied to guide the training of the model, forcing it to pay attention to given pattern words. Then, we select instances by checking whether the model can give a clear explanation for the relation label generated by DS. These two steps will be repeated in a bootstrap procedure. We illustrate our method in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention-based BiLSTM Encoder</head><p>In order to capture the key feature words for identifying relations, we apply an attention mechanism over a BiLSTM Encoder, which is first introduced in <ref type="bibr" target="#b23">(Zhou et al., 2016)</ref> for RC. The model architecture is illustrated on the left side of Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Input Embeddings. The input embeddings consist of three parts: word embedding, position embedding, and entity type embedding. Position embedding is first proposed by <ref type="bibr" target="#b21">Zeng et al. (2014)</ref> to incorporate position information of input entity pair and has been widely used in the following RC models. We also introduce entity type information by looking up an entity type embedding matrix. The final input embeddings are a concatenation of these embeddings, and are fed to a bidirectional Long Short Term Memory (BiLSTM) with an attention mechanism to generate sentence representation.</p><p>Attention-based BiLSTM. Let H = {h i } denotes the hidden vectors of BiLSTM encoder. The final sentence representation u is a weighted sum of these vectors,</p><formula xml:id="formula_0">M = tanh(H) a = softmax(w T M) u = Ha T (1)</formula><p>where w T is a trained parameter vector. It is demonstrated that attention mechanism is helpful in capturing important features for classification tasks. However, for noisy data generated by distant supervision, it almost only focuses on entities, but neglects relation patterns which are more informative for RC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Attention Regularization</head><p>Attention Regularization (AR) aims to teach the model to attend to the relation patterns for identifying relations. Given a T-word sentence s = {x i } T i=1 , a pair of entities (e 1 , e 2 ) in the s, a relation label y, and a relation patterns m that explains the relation y of e 1 and e 2 . (Section 3.4 will introduce the construction of relation patterns m). We are able to calculate an attention guidance value a m , according to pattern mention significance function q(z|s, e 1 , e 2 , m) conditional on the input m. Here z represents the pattern words in a sentence. We hope that the classifier can approximate its attention distribution a s = p(z|s) to a m , where p represents the classifier network. Intuitively, we apply KL (Kullback-Leibler divergence) as the optimized function, which describes the differences between distributions:</p><formula xml:id="formula_1">KL(a m ||a s ) = a m log a m a s (2)</formula><p>What is more, the Equation 2 can be further reduced as following:</p><formula xml:id="formula_2">loss a = a m log a m a s = (a m log a m − a m log a s )<label>(3)</label></formula><p>where loss a represents the loss of attention regularization. Because a m contains fixed values, the equation is equal to</p><formula xml:id="formula_3">loss a = − a m log a s (4)</formula><p>Therefore, we adapt loss a into classification loss loss c to regularize attention learning. The final loss is</p><formula xml:id="formula_4">loss = loss c + βloss a (5)</formula><p>where β is a weight for loss a , which is generally set as 1 in our experiments.</p><p>In this paper, we implement a fairly simple function to generate a m .</p><formula xml:id="formula_5">b i = 1 x i ∈ {e 1 , e 2 , m} 0 else a m = b k T i=1 b i T k=1<label>(6)</label></formula><p>Here b denotes that whether x i belongs to entity words and relation pattern words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance Selection with Attention from Model</head><p>Based on attention mechanism, a trained RC model can tells us the importance of each word for identifying the relation type. For a training instance, if the relation pattern words that the model focuses on do not match the pattern m which explains the relation type, this instance is probably a false positive. Here we still apply KL to measure the probability that an instance is a false positive.</p><p>Given the attention weights a s from the RC model and a m calculated by Equation <ref type="formula" target="#formula_5">6</ref>, the confidence score c of an instance is normalized by</p><formula xml:id="formula_6">c = 1 1 + KL(a m ||a s ) (7)</formula><p>The higher c is, the more confident an instance is.</p><p>We calculate the confidence score for all instances in the training set and select instances whose score is more than a threshold c t , which is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bootstrap Learning Procedure</head><p>In Relation Pattern Extraction. Another problem is how to build a relation pattern extractor E to extract a pattern from an instance. However, we find it is not quite critical. Even though we use a very simple method, we still achieve considerable improvement. It is certain that a more complicated and well-performed extractor will bring additional improvement. This will be one of our future work. Our pattern extractor E simply takes the words between two entities as a relation pattern. For the building of the initial pattern set M, we extract relation patterns from all instances in original training dataset and count them up. M is initially built by selecting patterns with occurrences. We retain top 10% (maximum 20) patterns for each relation type.</p><p>Data Redistribution. After the trustable pattern set M is built, dataset D will be redistributed using these patterns. All positive instances that are not matched these patterns will be put into the negative set, revising their relation label to 'None'. We will explain the reason for data redistributing in our experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYT</head><p>Training   <ref type="table" target="#tab_2">1 and Table 2</ref>.</p><p>For evaluation, we evaluate our framework on sentence-level (or instance-level). Sentence-level prediction is more friendly with comprehend sentence tasks, like question answering and semantic parsing <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref>. Different from commonly-used bag-level evaluation, a sentencelevel evaluation compute Precision (Prec.), Recall (Rec.) and F1 metric directly on all of the individual instances in the dataset. We think such an evaluation is more intuitive and suitable for a realworld application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our ARNOR framework with several strong baselines for noise reduction as follows: PCNN+SelATT <ref type="bibr" target="#b8">(Lin et al., 2016)</ref> is a bag-level RC model. It adopts an attention mechanism over all sentences in a bag and thus can reduce the weight of noise data. CNN+RL 2 <ref type="bibr" target="#b2">(Feng et al., 2018b</ref>) is a novel reinforcement learning (RL) based model for RC from noisy data. It jointly trains a CNN model for RC as well as an instance selector to remove unconfident samples. CNN+RL 1 <ref type="bibr" target="#b11">(Qin et al., 2018b</ref>) also introduces RL to heuristically recognize false positive instances. Different from <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>, they redis-tribute false positives into negative samples instead of removing them.</p><p>Meanwhile, to demonstrate the effectiveness of RC after denoising, several non-denoising methods are also used for comparison. CNN <ref type="bibr" target="#b21">(Zeng et al., 2014)</ref> is a widely-used architecture for RE. It introduces position embeddings to represent the location of an input entity pair. PCNN <ref type="bibr" target="#b20">(Zeng et al., 2015)</ref> is a revision of CNN which uses piecewise max-pooling to extract more relation features. BiLSTM <ref type="bibr" target="#b22">(Zhang et al., 2015)</ref> is also commonly used for RE with the help of position embeddings. BiLSTM+ATT <ref type="bibr" target="#b23">(Zhou et al., 2016)</ref> adds an attention mechanism into BiLSTM to capture the most important features for identifying relations. It is the base model used in our ARNOR framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For our model and other BiLSTM-based baselines, the word embeddings are randomly initialized with 100 dimensions. The position embeddings and entity type embeddings are randomly initialized with 50 dimensions. The size of BiL-STM hidden vector is set to 500. In attention regularization training, parameter β is set to 1. We set the learning rate as 0.001 and utilize Adam for optimization. To better evaluate our models, we averagely split the test dataset into a development set and a testing set. In instance selection step, an appropriate confidence score threshold is set to 0.5 that should be various in other datasets. And we take max 5 new patterns in a loop for each relation type. In bootstrap procedure, we run 10 epochs in the first loop, and 1 epoch in the rest loops until the classification performance on dev set dose not increase. Generally, the bootstrap procedure end  <ref type="bibr" target="#b2">(Feng et al., 2018b)</ref> starts with a pre-trained CNN model using initial data redistributing (IDR). CNN+IDR is the model trained on initially redistributed data and CNN+IDR+RL 2 applies RL 2 on pre-trained CNN+IDR model. in 5 loops. For CNN-based baselines, we use the same embedding settings. The window size of the convolution layer is set to 3 and the number of the filter is set to 230. All the baselines for noise reduction were implemented with the source codes released by their authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>We compare the results of ARNOR with nondenoising baselines and denoising baselines. As shown in Table <ref type="table" target="#tab_3">3</ref>, ARNOR significantly outperforms all of the baselines in both precision and F1 metric, obtaining about 11% F1 improvement over the state-of-the-art CNN+RL 2 . Note that our model achieves a tremendous improvement on precision without too much decline of recall. This demonstrates the proposed framework can effectively reduce the impact of noisy data. Besides, PCNN+SelATT performs the worst among all of the baselines. We think that it is because PCNN+SelATT is a bag-level method and is not suitable for sentence-level evaluation, which is consistent with <ref type="bibr" target="#b2">Feng et al. (2018b)</ref>. 5 Analysis and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of components</head><p>In order to find which component contributes to our framework, we evaluate our model by adding each of the components. The results are shown in Table <ref type="table">4</ref>. BiLSTM+ATT is the baseline model that is trained by original noisy data. After using the initial redistributed dataset, which is generated by the method described in the above section, the BiLSTM+ATT model achieves about 6% improvement in F1. And the precision sharply increases by about 26%. This demonstrates that the DS dataset contains a large proportion of noise.</p><p>Even such a simple filtering noise method can effectively improve model performance. However, this simple method seriously affects recall. On the one hand, amounts of true positives with long-tail patterns will be mistakenly regarded as false negatives. And we guess some relation patterns in training data are too rare to make the model learn to attend them. Therefore, after we add attention regularization to the model, the recall increases by about 10% with only 2% decline in precision. As a result, our model achieves another 7% F1 improvement. We believe this is the power of guiding the model to understand which words are more crucial for identifying relations. After we obtain an initial model trained by attention regularization, we continue the bootstrap learning procedure and finally achieve 2.4% F1 improvement. In this procedure, ARNOR will collect more confident longtail patterns to improve the recall of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Start with small clean or large noisy data</head><p>In the previous section, we have found that the initial redistributed dataset (with small but clean positive data) helps the model improve a lot.  better choice? In order to figure it out, we use the same initial redistributed dataset to pre-train the CNN which is used in the CNN+RL 2 and then apply RL 2 procedure for noise reduction on the original noisy dataset. We report the results in Table 5. The pre-trained PCNN also achieves a significant improvement, and after further denoising by RL 2 , CNN+RL 2 finally obtain 57.89% in F1, which is still 3% lower than the performance of our model. Therefore, we consider that starting the model with a small but clean dataset might be a choice for noise reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Noise Reduction</head><p>The instance selector in our ARNOR framework calculates a confidence score for each instance in the training set by checking whether the attention weights matches a given pattern. Then we utilize this confidence score to reduce noise. In order to verify the capability of reducing noise, we randomly sample 200 sentences to annotate whether they are noise and use them to evaluate the accuracy of noise reduction. We compare the results with CNN+RL 2 in Table <ref type="table" target="#tab_5">6</ref>. The ARNOR significantly outperforms CNN+RL 2 on percision and obtains a 14.92% F1 improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>Our ARNOR is able to make the RC model more interpretable through attention regularization training. To verify this point, we select some instances from the test set and visualize their attention weights for a case study. As shown in Table <ref type="table" target="#tab_6">7</ref>, BiLSTM+ATT which is trained on original noisy data only focuses on the entity pairs, and makes wrong predictions on these cases. This is probably because the model does not learn the key evidence for RC. While ARNOR can perfectly capture the important features and correctly predict the relation.</p><p>In addition, we also check the confident patterns which are discovered in bootstrap learning. As presented in Table <ref type="table" target="#tab_7">8</ref>, the high-frequency patterns can be easily obtained by initially building of confident pattern set, and after bootstrap learning, we can discover more long-tail patterns, most of which are representative and meaningful. More importantly, some of these additional patterns are not similar in literal terms, demonstrating the model might learn the semantic correlation among related feature words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose ARNOR, an attention regularizationbased noise reduction framework for distant supervision relation classification. We find relation pattern is an important feature but is rarely captured by the previous model trained on noisy data. Thus, we design attention regulation to help the model learn the locating of relation patterns. With a more interpretable model, we then conduct noise reduction by evaluating how well the model explains the relation of an instance. A bootstrap learning procedure is built to iteratively improve the model, training data and trustable pattern set. With a very simple pattern extractor, we outperform several strong RL-based baselines, achieving significant improvements on both relation classification and noise reduction. In addition, we publish a better manually labeled test set for sentence-level evaluation.</p><p>In the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model <ref type="bibr" target="#b7">(Kim et al., 2018)</ref> for jointly modeling instance selector. What is more, we also hope to verify the effectiveness of our method on more tasks, including open information extraction and event extraction, and also overlapping relation extraction models <ref type="bibr" target="#b0">(Dai et al., 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Two relation instances generated by distant supervision. The bold words "was born in" in s 1 is the pattern that explains the relation type "place of birth". Hence, this instance is correctly labeled. However, the second instance is noisy due to the lack of corresponding relation pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average attention weights of BiLSTM+ATT model across five parts in the sentences on our test set.This model is trained using noisy data generated by distant supervision. It mainly pays attention to the input entity pair and ignores other words which might express the real relation. It also happens in Figure1. This result comes from the fact that DS method only depends on entities for labeling data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An overview of our ARNOR framework. It is based on a BiLSTM with attention mechanism and utilizes attention regularization to force the model to attend the corresponding relation patterns. Then, an instance selector calculates a confidence score for each training instance to generate a new redistributed training set and a new trustable pattern set. These two steps are run iteratively to form a bootstrap learning procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>our ARNOR framework, an important problem is how to acquire relation patterns m in model training and instance selecting step. In the model training step, we need more precise patterns in order to guide the model to attend to important evidence for RC. While in the instance selection step, more various patterns are required so as to select more trustable data as well as to discover more confident relation patterns. Here we will simply define the process of the bootstrap learning steps. In model training, given 1) a pattern extractor E which can extract a relation patterns from an instance, 2) an initial trustable pattern set M (which might be manually collected or simply counted up from original training dataset D using E). First, Algorithm 1 The ARNOR Framework Require: DS dataset D, a relation classifier C with parameters θ θ θ 1: Collect high frequency patterns from D into M 2: Redistribute D by M 3c by C for D 7: Update M by high score c from D 8: Redistribute D by new M 9: end loop we redistribute training dataset D based on M (described below). Then, the RC model is trained for epochs only using m in M. Next, instance selection is run on D to select more confident training data. These new trustable instances are fed to E to figure out new trustable patterns and put them into M. We repeat such a bootstrap procedure until the F1 score on dev set does not increase. This bootstrap procedure is detailed in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the dataset in our experiments.</figDesc><table><row><cell>Test</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The 10 relation types we retain and statistics of them in the dataset. The distribution of some relation types are distinct in test set because they are much more noisy.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Dataset and Evaluation</cell></row><row><cell>We evaluate the proposed ARNOR framework on</cell></row><row><cell>a widely-used public dataset: NYT, which is a</cell></row><row><cell>news corpus sampled from 294k 1989-2007 New</cell></row><row><cell>York Times news articles and is first presented in</cell></row><row><cell>(Riedel et al., 2010). Most previous work com-</cell></row><row><cell>monly generates training instances by aligning en-</cell></row><row><cell>tity pairs from Freebase and adopt held-out evalua-</cell></row><row><cell>tion to evaluate without costly human annotation.</cell></row><row><cell>Such an evaluation can only provide an approxi-</cell></row><row><cell>mate measure due to the noisy test set that is also</cell></row><row><cell>generated by distant supervision. In contrast, Ren</cell></row><row><cell>et al. (2017) publishes a training set which is also</cell></row><row><cell>generated by distant supervision, but a manually-</cell></row><row><cell>annotated test set that contains 395 sentences from</cell></row><row><cell>Hoffmann et al. (2011). However, we find that this</cell></row><row><cell>test set was annotated with only one entity pair</cell></row><row><cell>for one sentence. Not all of the triplets in these</cell></row><row><cell>sentences are marked out. In addition, although</cell></row><row><cell>there are enough test instances (3,880 including</cell></row><row><cell>"None" type), the number of positive ones is rela-</cell></row><row><cell>tively small (only 396). Moreover, the test set only</cell></row><row><cell>contains half of the relation types of the training</cell></row><row><cell>set.</cell></row><row><cell>To address these issues and evaluate our</cell></row><row><cell>ARNOR framework more precisely, we annotate</cell></row><row><cell>and publish a new sentence-level test set (the</cell></row><row><cell>source address is in section 1) on the basis of the</cell></row><row><cell>one released by Ren et al. (2017), which also con-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our method and other baselines. The first three methods are normal RC model, and the middle three baselines are models for distant supervision RC.tains annotated named entity types. Firstly, we revise mislabeled instances on the original 395 testing sentences. Then, about 600 sentences are sampled and removed from the original training set. We carefully check their labels and merge them into the test set. We also remove some of the relation types which are overlapping and ambiguous or are too noisy to obtain a non-noise test sample. The details of this dataset and the relation types we used is shown in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of CNN+RL 2</figDesc><table><row><cell>Model</cell><cell>Prec. Rec.</cell><cell>F1</cell></row><row><cell cols="3">BiLSTM+ATT 34.93 65.18 45.48</cell></row><row><cell>+ IDR</cell><cell cols="2">70.95 40.57 51.63</cell></row><row><cell>+ ART</cell><cell cols="2">68.70 50.99 58.52</cell></row><row><cell>+ BLP</cell><cell cols="2">65.23 56.79 60.90</cell></row><row><cell cols="3">Table 4: Evaluation of components in our framework.</cell></row><row><cell cols="3">BiLSTM+ATT is the base model without reducing</cell></row><row><cell cols="3">noise. IDR stands for initial data redistributing using</cell></row><row><cell cols="3">initial confident pattern set. ART denotes attention reg-</cell></row><row><cell cols="3">ularization training for the first loop. BLP stands for</cell></row><row><cell cols="2">bootstrap learning procedure.</cell><cell></cell></row><row><cell>Model</cell><cell>Prec. Rec.</cell><cell>F1</cell></row><row><cell>CNN</cell><cell cols="2">35.75 64.54 46.01</cell></row><row><cell>CNN+RL 2</cell><cell cols="2">40.23 63.78 49.34</cell></row><row><cell>CNN+IDR</cell><cell cols="2">84.87 39.94 54.32</cell></row><row><cell cols="3">CNN+IDR+RL 2 83.63 44.27 57.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of effectiveness on noise reduction. We randomly sample 200 sentences (529 instances) from the training set. After manually checking, 213 of them are not noise. We use these samples to evaluate the capability of reducing noise.</figDesc><table><row><cell cols="2">Noise Reduction Prec. Rec.</cell><cell>F1</cell></row><row><cell>CNN+RL 2</cell><cell cols="2">40.58 96.31 57.10</cell></row><row><cell>ARNOR</cell><cell cols="2">76.37 68.13 72.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Here is attention cases with a heat map. These cases have shown our model's ability to locating relation indicators. Based on attention supervision, our model can concentrate on relation patterns and entities.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">… said Senomyx 's chief executive , Kent Snyder .</cell></row><row><cell></cell><cell></cell><cell>0.36</cell><cell></cell><cell>0.19</cell><cell>0.30</cell><cell></cell><cell>0.03 0.31</cell></row><row><cell>ARNOR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.12 0.13</cell><cell>0.14</cell><cell>0.12</cell><cell>0.15</cell><cell>0.15</cell><cell>0.13</cell><cell>0.13 0.11</cell></row><row><cell cols="3">/people/person/children</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>#Occ</cell><cell>Pattern</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>High</cell><cell>7</cell><cell cols="2">e2 , the son of e1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frequency</cell><cell>4</cell><cell cols="2">e2 , daughter of e1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Long Tail</cell><cell>1</cell><cell cols="3">e1 's youngest son , e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="3">e2 , the son of Secretary General e1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell cols="3">e2 , a daughter of Representative e1</cell><cell></cell><cell></cell></row><row><cell cols="3">/business/person/company</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>#Occ</cell><cell>Pattern</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>High</cell><cell cols="4">74 e2 secretary general , e1</cell><cell></cell><cell></cell></row><row><cell>Frequency</cell><cell cols="4">68 e1 , the chairman of e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">67 e1 , chief executive of e2</cell><cell></cell><cell></cell></row><row><cell>Long Tail</cell><cell>4</cell><cell cols="3">e1 , the secretary general of the e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell cols="3">e1 , the chief executive of the e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell cols="3">e1 , the oil minister of e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="3">e1 , the former chief executive of e2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="3">e1 , the vice chairman of e2</cell><cell></cell><cell></cell></row></table><note>On the contrary, the previous neural network-based model for distant supervision RC, including all baselines in this paper, usually starts with the original dataset which is large but noisy. Which is the Jim Kimsey , a founder of AOL ; Jack Valenti , former head … Entity 1: AOL Entity 2: Jim Kimsey Relation: /business/company/founders BiLSTM+ATT Jim Kimsey , a founder of AOL ; Jack Valenti , former head … … said Senomyx 's chief executive , Kent Snyder . Entity 1: Kent Snyder Entity 2: Senomyx Relation: /business/person/company</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Pattern set cases. This table has shown some high frequency and top long tail patterns discovered by our model in pattern bootstrap.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The dataset used in this paper is on https: //github.com/PaddlePaddle/models/tree/ develop/PaddleNLP/Research/ACL2019-ARNOR</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Natural Science Foundation of China (No. 61533018).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and overlapping relations using positionattentive sequence labeling</title>
		<author>
			<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoqiao</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019)</title>
				<meeting><address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-01-27">2019. January 27. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relation mention extraction from noisy data with hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01237</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detection of synonymy links between terms: experiment and results</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adeline</forename><surname>Nazarenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
	<note>Recent advances in computational terminology</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Denoising distant supervision for relation extraction via instance-level adversarial training</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06834</idno>
		<title level="m">A tutorial on deep latent variable models of natural language</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Long</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dsgan: Generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09929</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09927</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="269" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName><forename type="first">Christopher M De</forename><surname>Alexander J Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Tarek F Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
				<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large scaled relation extraction with reinforcement learning</title>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Xiangrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Shizhu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao Jun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02">2003. Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation</title>
				<meeting>the 29th Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
