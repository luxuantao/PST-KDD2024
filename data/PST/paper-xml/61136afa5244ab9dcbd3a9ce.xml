<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-11">11 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weilun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EEIS Department</orgName>
								<orgName type="laboratory">CAS Key Laboratory of GIPAS</orgName>
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-11">11 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.04547v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning shows great potential in unpaired image-to-image translation, but sometimes the translated results are in poor quality and the contents are not preserved consistently. In this paper, we uncover that the negative examples play a critical role in the performance of contrastive learning for image translation. The negative examples in previous methods are randomly sampled from the patches of different positions in the source image, which are not effective to push the positive examples close to the query examples. To address this issue, we present instance-wise hard Negative Example Generation for Contrastive learning in Unpaired image-to-image Translation (NEGCUT). Specifically, we train a generator to produce negative examples online. The generator is novel from two perspectives: 1) it is instance-wise which means that the generated examples are based on the input image, and 2) it can generate hard negative examples since it is trained with an adversarial loss. With the generator, the performance of unpaired image-to-image translation is significantly improved. Experiments on three benchmark datasets demonstrate that the proposed NEGCUT framework achieves state-of-the-art performance compared to previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-to-image translation aims to transfer images from the source domain to the target domain with the content information preserved, which is of significant importance on various applications such as style transfer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>, domain adaption <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref> and image colorization <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b1">2]</ref>. Due to the inconvenience of collecting paired training data, recent methods are usually based on the unpaired setting. In that case, cycle-consistency loss has been widely used to preserve the consistency between the source images and generated images, for instance, CycleGAN <ref type="bibr" target="#b58">[59]</ref>, StarGAN <ref type="bibr" target="#b6">[7]</ref>, UNIT <ref type="bibr" target="#b32">[33]</ref> and MUNIT <ref type="bibr" target="#b21">[22]</ref>. . We visualize the generated images along with the distribution of cosine similarity between query and negative samples in CUT <ref type="bibr" target="#b39">[40]</ref> and our method. The blue histogram refers to the distribution in CUT while the orange histogram refers to the distribution in our method.</p><p>The recently proposed method CUT <ref type="bibr" target="#b39">[40]</ref> introduces contrastive learning in unpaired image-to-image translation and achieves better performance over methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59</ref>] that use cycle-consistency loss. In this paper, we aim to further improve the performance of contrastive learning for unpaired image-to-image translation. We uncover that the performance of contrastive learning relies heavily on the hardness of negative samples. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the negative samples in the method <ref type="bibr" target="#b39">[40]</ref> are randomly sampled from the patches of different positions in the image, which sometimes leads to the translated results in poor quality and the contents not preserved consistently. Also we calculate the cosine similarities between the query patches and negative patches, and we can find that their cosine similarities are around 0. In other words, these negative patches are not challenging enough to push the positive examples close to the query examples, which will result in the framework not taking full advantage of contrastive learning.</p><p>To address the above issue, we present instance-wise hard Negative Example Generation for Contrastive learning in Unpaired image-to-image Translation (NEGCUT) in this paper. More precisely, we propose a novel negative generator to excavate hard negative examples. For a source image, we first extract its features on different layers of image generator encoder and embed them into feature vectors. Based on the embedded features from source images, the negative generator produces instance-wise negative examples related to the source image. Moreover, the negative samples should be diverse enough to push the query patch closer to the positive patch. To this end, we add the noise as an extra input for the generator. However, the noise input can probably be ignored for the generator, thus the generator can generate similar examples for different input noises. This is also called the mode collapse issue <ref type="bibr" target="#b43">[44]</ref>. Inspired by the mode seeking loss in MSGAN <ref type="bibr" target="#b36">[37]</ref>, we introduce diversity loss to the generator to encourage the generator to produce diverse hard negative samples for different input noise.</p><p>To generate challenging negative samples for contrastive learning, the main idea is to train the negative generator against the encoder network in an adversarial manner. Two components in the framework, i.e., the encoder network and negative generator, are updated alternatively to play a minmax game. On one hand, the encoder network narrows the distance between query and positive samples against hard negative samples to minimize contrastive loss. On the other hand, the negative generator produces hard negative samples close to the positive samples to maximize contrastive loss. Intuitively, the framework will reach an equilibrium where the encoder learns detailed and distinguishing representation to discriminate the positive samples from generated hard negative samples. In Figure <ref type="figure" target="#fig_0">1</ref>, we visualize the generated images along with the distribution of cosine similarity between the query and negative samples in the CUT and NEGCUT. It is observed that the negative samples produced by negative generator are harder than those sampled in the method <ref type="bibr" target="#b39">[40]</ref>, which push the encoder network to learn distinguishing representation and finally results in fine-grained correspondence of structures and textures.</p><p>Our contributions are summarized as follows,</p><p>• We identify that instance-wise negative examples that increase hardness as training process play a critical role in the performance of contrastive learning for unpaired image-to-image translation. • We propose a novel framework NEGCUT to mine instance-wise hard negative examples for contrastive learning in unpaired image-to-image translation. • Extensive experiments on three benchmark datasets demonstrate the superiority of our method, which achieves new state-of-the-art performance. The generated images of our method are of better visual performance with consistent detailed correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly introduce the related topics, including contrastive learning, image-to-image translation and hard negative mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image-to-Image Translation</head><p>Image-to-image translation (I2I) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref> aims to transfer images from source to target domain with the content information preserved. Earlier methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref> apply an adversarial loss <ref type="bibr" target="#b13">[14]</ref>, along with a reconstruction loss to train their model based on the paired training data. However, due to the difficulty of collecting a large amount of paired data, recent methods are usually based on the unpaired setting. In that cases, cycleconsistency loss has been widely used to preserve the consistency between the source images and generated images instead, for instance, CycleGAN <ref type="bibr" target="#b58">[59]</ref>, DiscoGAN <ref type="bibr" target="#b26">[27]</ref>, Du-alGAN <ref type="bibr" target="#b53">[54]</ref> and U-GAT-IT <ref type="bibr" target="#b25">[26]</ref>. Based on the assumption that the generated result should be translated back by an inverse mapping, cycle-consistency learns the mapping from target to source domain and check whether the source images are reconstructed. However, the assumption is overly strict compared to the actual situation, where the images between the two domains are not one-to-one mapping. In view of this, CUT <ref type="bibr" target="#b39">[40]</ref> involves contrastive learning in unpaired image-to-image translation to learn the correspondence between source and generated images, which outperforms previous methods using cycle-consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>Contrastive learning is a framework that learns representation by comparing similar and dissimilar pairs. Recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref> based on the theory of maximizing mutual information have achieved wide success on unsupervised representation learning. These methods take full advantage of noise-contrastive estimation <ref type="bibr" target="#b14">[15]</ref>, mapping the images into an embedding space where associated samples are brought together in contrast with unrelated samples. For a single query sample, the associated samples are referred to as positive samples while the unrelated samples are referred to as negative samples. With similarity measured by dot production, a form of a contrastive loss, called InfoNCE, is proposed as a representative loss function for noise-contrastive estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hard Example Mining</head><p>Hard example mining is a classic method to solve the problem of sample imbalance in several areas, i.e., object detection and unsupervised representation learning. In earlier methods, hard example mining are used to optimize SVMs <ref type="bibr" target="#b10">[11]</ref>, shallow neural networks <ref type="bibr" target="#b41">[42]</ref> and boosted decision trees <ref type="bibr" target="#b9">[10]</ref>. Recent work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>  . The overview of our NEGCUT framework. We perform hard negative example generation for adversarial contrastive learning on multiple layers of the image generator encoder. The black arrows show the forward propagation of our framework while the blue and red arrows show the backward propagation of contrastive loss and adversarial contrastive loss, respectively. On each layer, the representation network randomly samples the source and translated features at the spatial dimension, and produces the query and positive samples. The negative generator produces challenging negative samples by the mean vector of features from the representation network. The query, positive and generated negative samples are involved for contrastive learning in an adversarial manner.</p><p>selects hard examples for training deep networks. In <ref type="bibr" target="#b45">[46]</ref>, an image descriptor is learned to independently select the hard positive and negative samples from a large set. In <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b44">[45]</ref>, online hard examples selection is investigated on image classification and object detection, respectively. Lin et .al design a novel focal loss <ref type="bibr" target="#b31">[32]</ref> to focus training on a sparse set of hard examples, which addresses the imbalance between different classes in object detection. In unsupervised representation learning, a triple loss is used <ref type="bibr" target="#b51">[52]</ref> to mine the hard negative samples from a large set. In <ref type="bibr" target="#b20">[21]</ref>, adversarial learning is involved to generate challenging negative samples for unsupervised representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this paper, we present a novel framework NEGCUT to mine instance-wise hard negative examples for contrastive learning in unpaired image-to-image translation. Different from previous work which randomly samples negative examples from the patches in the image, our method generates instance-wise hard negative examples through adversarial learning. With the produced hard negative examples, our framework can generate images with detailed and finegrained correspondence on structures and textures. The rest of this section is organized as follows: We begin with reviewing the related method in previous work in Sec. 3.1. In Sec. 3.2, we outline the NEGCUT framework and introduce the details of hard negative example generation through adversarial learning. Finally, we discuss the objective function utilized in our framework in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries and Motivation</head><p>We first briefly review the method leveraging contrastive learning in unpaired image-to-image translation developed in CUT <ref type="bibr" target="#b39">[40]</ref>. To generate images of target domain with the content information maintained, the main idea is to learn the correspondence between the source and generated images. Compared with previous methods using cycle-consistency loss, CUT applies contrastive loss to learn the correspondence instead, which directly maximizes the mutual information between the source and generated images. The contrastive loss is formulated as follows,</p><formula xml:id="formula_0">l(q, k + , k − ) = − log[ exp(q • k + /τ ) exp(q • k + /τ ) + N n=1 exp(q • k − n /τ ) ],<label>(1)</label></formula><p>where q is the query samples from the generated image, k + is the positive samples from the corresponding position of the query in the source image, k − n is the negative samples from the other positions in the source images, and τ is the temperature factor.</p><p>CUT develops the contrastive learning in a multi-layer patch-wise manner, which is formulated as follows,</p><formula xml:id="formula_1">L PatchNCE (G, H, X) = E x∼X L l=1 S l s=1 l(q l,s , k + l,s , k − l,s ),<label>(2)</label></formula><p>where q l,s , k + l,s and k − l,s are extracted from the features of source image X and generated image Y at different intermediate layers l of generator encoder. With the contrastive loss, the generator learns to narrow the distance between the query and positive samples against negative samples at different layers, which is equivalent to maximizing the mutual information between the source and generated images.</p><p>By replacing the cycle-consistency loss with the contrastive loss, CUT generates more realistic and corresponding images compared with previous methods. However, the randomly-sampled negative examples in CUT cannot take full advantage of contrastive learning. The approach to estimating the negative examples plays a critical role in the performance of contrastive learning. Negative examples in CUT are not challenging enough to push the encoder network to learn distinguishing representation, which leads to the translated results in poor quality and the contents not preserved consistently. Different from these, we propose a novel framework NEGCUT to mine instance-wise hard negative samples for unpaired image-to-image translation through adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NEGCUT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Framework Architecture</head><p>Figure <ref type="figure" target="#fig_1">2</ref> gives an overview of our framework, which consists of Image Generator, Representation Network and Negative Generator. Image generator G takes the source image X as input and generates the translated image Y. Regarding two variants of a single image, i.e., the source image X and the generated image Y, we conduct multi-layer patchwise contrastive learning to learn the correspondence between these two images. On a certain layer of the image generator encoder, the query and positive samples are produced by the representation network through embedding the spatially sampled feature vectors into high-dimensional representation space.</p><p>To increase the similarity between the query and positive samples, the negative generator mines instance-wise hard negative samples against positive samples. Based on the embedded features of the source image, diverse challenging negative examples are generated by taking various randomly-sampled noise vectors as input. In our framework, the encoder network (i.e., the image generator and representation network) and the negative generator are alternately updated with the adversarial contrastive loss. With more challenging negative examples produced by the negative generator, the encoder network will learn distinguishing representation to discriminate positive samples from the challenging negative samples, which leads to fine-grained and robust correspondence between the source and generated images. Additionally, a discriminator is applied to ensure the domain and realness of the generated image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hard Negative Example Generation</head><p>In this section, we formally present hard negative example generation for contrastive learning in unpaired imageto-image translation. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we perform contrastive learning on multiple layers of the image generator encoder. For a certain layer, we employ a representation network H i (•) to embed the feature of different patches. The representation network is a 2-layer MLP network independently mapping the feature vector at each pixel from the source and translated images to a M -dimension vector. Based on the feature after mapping, we randomly sample S positions in the spatial dimension and take the normed vectors as query and positive samples for contrastive learning, which is formulated as follow,</p><formula xml:id="formula_2">q = H i s (F Y i ) H i s (F Y i ) 2 , k + = H i s (F X i ) H i s (F X i ) 2 ,<label>(3)</label></formula><p>where F X i and F Y i are the source features and the translated features at the i-th layer of image generator encoder, respectively. H i s (F X i ) and H i s (F Y i ) refers to the s-th positive and query examples sampled, respectively.</p><p>To push the positive sample close to the query sample, we generate challenging negative samples with a carefully designed multi-layer negative generator {N 0 , N 1 , • • • , N l }. Base on the spatially-average features from representation network H i (F X i ), the negative generator produces hard negative samples with noise vector z n , which is formulated as follows,</p><formula xml:id="formula_3">k − adv,n = N i (H i (F X i ); z n ) N i (H i (F X i ); z n ) 2 . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>For a positive sample, we generate multiple negative examples through sampling various noise vectors from standard Gaussian distribution.</p><p>To generate challenging negative samples for contrastive learning, the main idea is to train the negative generator against the encoder network in an adversarial manner, which is formulated as follows,</p><formula xml:id="formula_5">min θ H ,θ G max θ N l(q, k + , k − adv ) = − log[ exp(q • k + /τ ) exp(q • k + /τ ) + N n=1 exp(q • k − adv,n /τ ) ].<label>(5)</label></formula><p>From Equation (5), it is observed that the encoder network (i.e., the representation network</p><formula xml:id="formula_6">H = {H 0 , H 1 , • • • , H l } and the image generator G)</formula><p>narrows the distance between the query samples and positive samples against the negative samples to minimize contrastive loss. On the contrary, the negative generator N = {N 0 , N 1 , • • • , N l } produces challenging negative examples to maximize the contrastive loss. Intuitively, the encoder network and the negative generator will reach an equilibrium by alternate training, where the negative generator produces challenging negative samples and the encoder network learns distinguishing representation to discern the positive samples from the negative samples.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, we further illustrate how the negative generator, representation network and image generator are updated. The negative generator is first updated with negative contrastive loss, which is formulated as follows,</p><formula xml:id="formula_7">θ N i ← θ N i + η N ∂l(q, k + , k − adv ) ∂θ N i . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>The backpropagation of the negative contrastive loss is cut off before the representation network and does not affect the weights of the representation network and image generator. After that, the representation network is updated with positive contrastive loss, which is formulated as follow,</p><formula xml:id="formula_9">θ H i ← θ H i − η H ∂l(q, k + , k − adv ) ∂θ H i . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Since the contrastive learning is developed in a multi-layer manner, the total adversarial contrastive loss for the negative generator and representation network is formulated as follows,</p><formula xml:id="formula_11">L AdCont = E x∼X L l=1 S l s=1 l(q l,s , k + l,s , k − adv,l,s ).<label>(8)</label></formula><p>The image generator is trained along with the representation network. Through the back-propagation of adversarial contrastive loss, the image generator receives the gradient at different layers of the encoder. The image generator is updated with the summation of these gradient, which is formulated as follows,</p><formula xml:id="formula_12">θ G ← θ G − η G l i=0 ( ∂l(q, k + , k − adv ) ∂F X i ∂F X i ∂θ G + ∂l(q, k + , k − adv ) ∂F Y i ∂F Y i ∂θ G ),<label>(9)</label></formula><p>where F X i and F Y i are the features of the source and translated images at the i-th layer of encoder, respectively.</p><p>However, when the adversarial contrastive loss is the only function used to update the negative generator, it is observed that the generated negative examples lose diversity and collapse to one negative example. This is because the adversarial contrastive loss focuses on generating hard negative samples rather than diverse negative samples, though diversity is helpful for the performance. To this end, we introduce the diversity loss to generate diverse challenging negative samples with different input noise. The diversity loss encourages the generation of distinctive results when different noise vectors are brought in, which is formulated as follows,</p><formula xml:id="formula_13">L div = − N i (H i (X i ), z 1 ) − N i (H i (X i ), z 2 ) 1 , (10)</formula><p>where z 1 and z 2 are two different input noise randomly sampled from standard Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Objectives</head><p>Besides the adversarial contrastive loss and diversity loss mentioned above, our framework is also optimized by generative adversarial loss. Generative Adversarial Loss. Since the ground-truth images are unavailable in unpaired image-to-image translation, we develop adversarial learning to constrain the realness and the domain of the generated images. For the image generator G(•) and the discriminator D(•), we unitize the LSGAN 100 loss <ref type="bibr" target="#b37">[38]</ref>, which is formulated as follows,</p><formula xml:id="formula_14">L D gan = E xr [(1 − D(x r )) 2 ] + E x f [D(x f ) 2 ], L G gan = E x f [(1 − D(x f ) 2 ],<label>(11)</label></formula><p>where x r and x f indicates the real image distribution and the generated images distribution, respectively. Overall Loss. The overall loss for the negative generator and encoder network is the weighted summation of above losses, which is formulated as follows,</p><formula xml:id="formula_15">L H = L AdCont , L G = L AdCont + λ 1 L G gan , L N = −L AdCont + λ 2 L div ,<label>(12)</label></formula><p>where λ 1 and λ 2 are the trade-off parameters balancing different losses. In our experiments, λ 1 and λ 2 are set to 1 and 1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets. To demonstrate the superiority of our method, we train and test our method on three benchmark datasets, i.e., Cityscapes <ref type="bibr" target="#b8">[9]</ref>, Cat→Dog <ref type="bibr" target="#b7">[8]</ref> and Horse→Zebra <ref type="bibr" target="#b58">[59]</ref> datasets with translation between various different domains. The Cityscapes dataset contains a diverse set of images recorded in the street scenes with high-quality pixel-level annotations. The Cat→Dog dataset is a dataset of 10,000 high-quality cat and dog face images extracted from the AFHQ dataset. The Horse→Zebra dataset consists of about 2,500 images of horse and zebra in different scenes. We learn the translation from semantic masks to real images, from cat images to dog images and from horse images to zebra images on three datasets, respectively. For all the datasets, we resize images to the same resolution of 256 × 256 to train our network. , UNIT <ref type="bibr" target="#b32">[33]</ref> , DRIT <ref type="bibr" target="#b30">[31]</ref>, CUT <ref type="bibr" target="#b39">[40]</ref>, on three benchmark datasets. Compared with previous methods, the generated images of our method show superior performance with correct correspondence between the source and generated image.</p><p>Implementation Details. To make a fair comparison, we set the hyperparameters consistent with previous methods <ref type="bibr" target="#b39">[40]</ref>. We conduct our adversarial contrastive learning on the 1-st, 5-th, 9-th, 13-th, 17-th layers of the generator encoder. The number of negative samples for contrastive learning is set to 256 in our framework. The dimension of the query, positive and negative samples is set to 256. For the whole framework, we utilize Adam optimizer <ref type="bibr" target="#b27">[28]</ref>. The training lasts 400 epochs in total. The learning rate is set to 2e-4 and linearly reduces after 200 epochs. The whole framework is implemented by Pytorch and we perform experiments on NVIDIA RTX 3090Ti.</p><p>Evaluation Metrics. We evaluate the realness of generated images by the FID metric. FID measures the distance between two sets of images. To calculate the FID metric, we first embed the generated images and ground-truth images into the feature space with an Inception model <ref type="bibr" target="#b47">[48]</ref>. The FID metric is computed by the mean value and covariance of the generated image set (µ Y , Σ Y ) and the ground-truth image set (µ Ŷ , Σ Ŷ ):</p><formula xml:id="formula_16">FID(Y, Ŷ) = µ Y −µ Ŷ 2 2 +Tr(Σ Y +Σ Ŷ −2(Σ Y Σ Ŷ )<label>1 2</label></formula><p>). (13) In addition, to evaluate the relevance between source images and generated images, we apply several metrics different from FID on the Cityscapes dataset. With a pretrained segmentation model <ref type="bibr" target="#b54">[55]</ref>, we calculate the mAP, pixel accuracy (pAcc) and class accuracy (cAcc) metrics on the source semantic labels and generated real images. The higher mAP, pAcc and cAcc represent that the generated images are more relevant to source semantic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-art Methods</head><p>We compare our method with several state-of-theart methods of unpaired image-to-image translation, i.e., CUT <ref type="bibr" target="#b39">[40]</ref>, CycleGAN <ref type="bibr" target="#b58">[59]</ref> and DRIT <ref type="bibr" target="#b30">[31]</ref>. The quantitative result on three benchmark datasets is shown in Table <ref type="table" target="#tab_0">1</ref>. From the table, it is observed that our method achieves new state-of-the-art performance on three datasets. Compared with the most challenging method, i.e. CUT, our method outperforms it 14.0%, 26.6% and 13.0% relatively on FID metric on three datasets. Additionally, since only the image generator is used at inference time, NEGCUT does not introduce extra test time consumption compared with CUT.</p><p>Furthermore, we make a qualitative evaluation on three datasets with several competitive methods, i.e. CUT, Cy-cleGAN, UNIT and DRIT. From Figure <ref type="figure" target="#fig_2">3</ref>, it is observed that the images generated by our method have better visual performance compared with previous methods. Especially, our generated images keep a better correspondence with the source images compared with the most challenging method CUT. This benefits from the challenging negative samples generated by the negative generator. The negative examples sampled randomly in CUT help the network learn the correspondence between source images and generated images at the beginning, but become less and less effective as the training process proceeds. In contrast, our negative samples produced by negative generator keep challenging via adversarial learning, which forces the image generator and representation network to learn the fine-grained correspondence. Due to this reason, the images generated by our method have better correspondence with the source images in details, i.e., textures and postures. Table <ref type="table">2</ref>. Ablation study for several different designs, i.e., negative generator, diversity loss and the number of negative samples. H→Z refers to the Horse→Zebra dataset. ↑ indicates the higher the better, while ↓ indicates the lower the better. Without negative generator or the diversity loss, the produced negative examples are not challenging enough, which leads to inferior performance under most of the indicators on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform several ablation experiments to verify the effectiveness of several designs in our framework, i.e. negative generator, diversity loss and number of negative samples. We report the quantitative results in Table <ref type="table">2</ref>.</p><p>To evaluate the necessity of generating instance-wise negative samples with negative generator, we design a variant without negative generator. As an alternative, we directly update the negative samples in the feature vector space in this variant. In such case, the learned negative samples are widely distributed in the feature space and unrelated to the source instance. From Table <ref type="table">2</ref>, it is observed that, without the negative generator, the framework obtains an inferior performance under most of the indicators, which verifies the effectiveness of generating instance-wise negative samples. After that, we conduct an ablation study on the diversity loss by comparing the framework with and without the diversity loss. In Table <ref type="table">2</ref>, it demonstrates that the framework with diversity loss outperforms that without diversity loss. This is because, in the variant without diversity loss, the produced negative samples lose diversity in the early stage of training and maintain less diverse during training. Under this situation, the negative generator fails to produce challenging negative samples, which leads to the poor performance. Additionally, we analyze the visual results under these different settings. In Figure <ref type="figure" target="#fig_3">4</ref>, it can be seen that, without the negative generator or diversity loss, the generated image has a far inferior visual quality on fidelity and correspondence between the source and generated image.</p><p>Based on the framework with negative generator and diversity loss, we further make an ablation study on the number of negative samples. From Table <ref type="table">2</ref>, it can be seen that the performance reaches the top when the number of negative samples equal to 256. When the number of negative samples is more than 256, the main challenging negative samples have been contained in the 256 negative samples. The extra generated negative samples may contain some irrelevant disturbance resulting in inferior performance, and increase the computation cost notably. Nevertheless, too few negative samples may result in the ineffectiveness to push positive samples closer to the query. Comprehensively considering the performance and computational consumption, the best number of negative samples is 256 in our framework. Furthermore, we compare the generated images under different numbers of negative examples. In Figure <ref type="figure" target="#fig_3">4</ref>, it is observed that, when the number of negative samples is set to 256, the generated images have the best visual quality and most correct correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization of Hard Negative Examples.</head><p>To further demonstrate the effect of hard negative examples, we visualize hard negative examples by retrieving regions based on generated features. In Figure <ref type="figure">5</ref> Furthermore, we compare the learned similarity by representation network in CUT and NEGCUT. For each query q, we calculate the similarity maps through computing exp(q • k + /τ ) on all the pixels of the image. From Figure <ref type="figure" target="#fig_6">6</ref>, it is observed that, in the similarity maps of CUT, the corresponding areas are scattered over the entire image and several unrelated areas are also associated. Additionally, when the query point is sampled from a part of the foreground, i.e. head of the horse, the whole foreground is associated in the similarity maps of CUT, which demonstrates that the representation network in CUT has difficulty on discriminating different parts of the foreground. Different from that, the corresponding areas in the similarity maps of NEGCUT are concentrated on the neighborhood of query points or areas with the same semantic, which verifies that the representation network in NEGCUT learns more distinguishing representation and accuracy correspondence under the help of instance-wise hard negative samples.  Compared with the similarity learned by CUT, our similarity maps are more concentrated on the neighbourhood of query points, which verifies that our method learns distinguishing representation with help negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel framework called NEGCUT to mine challenging negative samples for contrastive learning in unpaired image-to-image translation. Specifically, we design a negative generator trained against the encoder network in an adversarial manner. The two components in our framework, i.e., the encoder network and the negative generator, are updated alternately to learn distinguishing representation to discriminate positive samples against generated hard negative samples. Extensive experiments on three benchmark datasets demonstrate the superiority of our method. Our method achieves state-of-the-art performance and shows a better correspondence between source images and generated images compared with previous methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FrequencyFigure 1</head><label>1</label><figDesc>Figure1. We visualize the generated images along with the distribution of cosine similarity between query and negative samples in CUT<ref type="bibr" target="#b39">[40]</ref> and our method. The blue histogram refers to the distribution in CUT while the orange histogram refers to the distribution in our method.</figDesc><graphic url="image-1.png" coords="1,308.90,348.63,235.89,78.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure2. The overview of our NEGCUT framework. We perform hard negative example generation for adversarial contrastive learning on multiple layers of the image generator encoder. The black arrows show the forward propagation of our framework while the blue and red arrows show the backward propagation of contrastive loss and adversarial contrastive loss, respectively. On each layer, the representation network randomly samples the source and translated features at the spatial dimension, and produces the query and positive samples. The negative generator produces challenging negative samples by the mean vector of features from the representation network. The query, positive and generated negative samples are involved for contrastive learning in an adversarial manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Qualitative results with the four challenging methods, i.e., CycleGAN<ref type="bibr" target="#b58">[59]</ref>, UNIT<ref type="bibr" target="#b32">[33]</ref> , DRIT<ref type="bibr" target="#b30">[31]</ref>, CUT<ref type="bibr" target="#b39">[40]</ref>, on three benchmark datasets. Compared with previous methods, the generated images of our method show superior performance with correct correspondence between the source and generated image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative comparison among different designs of negative generator. When the negative generator and diversity loss are employed and the number of negatives is set to 256, the generated images have the best visual quality and most correct correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we first retrieve 8 hard negative examples based on each query feature. After that, we visualize these hard negative examples by retrieving the most related patches in the image. It is observe that the retrieved hard negative examples share similar semantic meanings with the query patch in structure and texture. This indicates that the generated hard examples can encourage the model to generate content consistent results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Positive examples (red) &amp; Hard negative examples (blue) 5. Visualization of negative examples by retrieving regions based on generated features. We visualize 8 hard negative examples by retrieving the most related patches in the image. It is observed that the retrieved patches share similar semantic meanings with the query patch in structure and texture. Learned similarity from two query points to input image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visualization of the learned similarity by representation network in CUT and NEGCUT. Two similarity maps are learned from two query points sampled from the foreground (blue) and background (red). Compared with the similarity learned by CUT, our similarity maps are more concentrated on the neighbourhood of query points, which verifies that our method learns distinguishing representation with help negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods on unpaired image translation, i.e. CycleGAN, UNIT, DRIT, CUT, etc. H→Z refers to the Horse→Zebra dataset. ↑ indicates the higher the better, while ↓ indicates the lower the better. It is notable that our method outperforms previous methods on various metrics.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Cityscapes</cell><cell></cell><cell cols="2">Cat→Dog</cell><cell>H→Z</cell></row><row><cell></cell><cell></cell><cell>mAP↑</cell><cell>pAcc↑</cell><cell>cAcc↑</cell><cell>FID↓</cell><cell></cell><cell>FID↓</cell><cell>FID↓</cell></row><row><cell cols="2">CycleGAN [59]</cell><cell>20.4</cell><cell>55.9</cell><cell>25.4</cell><cell>76.3</cell><cell></cell><cell>85.9</cell><cell>77.2</cell></row><row><cell>UNIT [33]</cell><cell></cell><cell>16.9</cell><cell>56.5</cell><cell>22.5</cell><cell>91.4</cell><cell></cell><cell>104.4</cell><cell>133.8</cell></row><row><cell>DRIT [31]</cell><cell></cell><cell>17.0</cell><cell>58.7</cell><cell>22.2</cell><cell>155.3</cell><cell></cell><cell>123.4</cell><cell>140.0</cell></row><row><cell>Distance [3]</cell><cell></cell><cell>8.4</cell><cell>42.2</cell><cell>12.6</cell><cell>81.8</cell><cell></cell><cell>155.3</cell><cell>72.0</cell></row><row><cell cols="2">SelfDistance [3]</cell><cell>15.3</cell><cell>56.9</cell><cell>20.6</cell><cell>78.8</cell><cell></cell><cell>144.4</cell><cell>80.8</cell></row><row><cell>GCGAN [12]</cell><cell></cell><cell>21.2</cell><cell>63.2</cell><cell>26.6</cell><cell>105.2</cell><cell></cell><cell>96.6</cell><cell>86.7</cell></row><row><cell>CUT [40]</cell><cell></cell><cell>24.7</cell><cell>68.8</cell><cell>30.7</cell><cell>56.4</cell><cell></cell><cell>76.2</cell><cell>45.5</cell></row><row><cell>FastCUT [40]</cell><cell></cell><cell>19.1</cell><cell>59.9</cell><cell>24.3</cell><cell>68.8</cell><cell></cell><cell>94.0</cell><cell>73.4</cell></row><row><cell>NEGCUT</cell><cell></cell><cell>27.6</cell><cell>71.4</cell><cell>35.0</cell><cell>48.5</cell><cell></cell><cell>55.9</cell><cell>39.6</cell></row><row><cell cols="2">Settings</cell><cell></cell><cell></cell><cell cols="2">Cityscapes</cell><cell></cell><cell cols="2">Cat→Dog H→Z</cell></row><row><cell cols="7">Negative Diversity Number mAP↑ pAcc↑ cAcc↑ FID↓ Generator Loss of Neg.</cell><cell>FID↓</cell><cell>FID↓</cell></row><row><cell>×</cell><cell>×</cell><cell>256</cell><cell>27.3</cell><cell>71.9</cell><cell>34.5</cell><cell>49.7</cell><cell>110.9</cell><cell>59.6</cell></row><row><cell></cell><cell>×</cell><cell>256</cell><cell>27.0</cell><cell>71.1</cell><cell>33.7</cell><cell>91.5</cell><cell>83.0</cell><cell>72.1</cell></row><row><cell></cell><cell></cell><cell>64</cell><cell>26.9</cell><cell>71.1</cell><cell>33.7</cell><cell>49.7</cell><cell>59.3</cell><cell>59.3</cell></row><row><cell></cell><cell></cell><cell>128</cell><cell>27.2</cell><cell>71.4</cell><cell>33.9</cell><cell>49.8</cell><cell>86.9</cell><cell>51.8</cell></row><row><cell></cell><cell></cell><cell>512</cell><cell>27.3</cell><cell>71.3</cell><cell>34.3</cell><cell>51.2</cell><cell>62.8</cell><cell>44.0</cell></row><row><cell></cell><cell></cell><cell>256</cell><cell>27.6</cell><cell>71.4</cell><cell>35.0</cell><cell>48.5</cell><cell>55.8</cell><cell>39.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the National Natural Science Foundation of China under Contract 61836011, 61822208, and 62021001, and in part by the Youth Innovation Promotion Association CAS under Grant 2018497. It was also supported by the GPU cluster built by MCC Lab of Information Science and Technology Institution, USTC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Federico</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>González Morín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rodés-Guirao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03400</idno>
		<title level="m">Deep koalarization: Image colorization using cnns and inception-resnet-v2</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-sided unsupervised domain mapping</title>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pietro Perona, and Serge Belongie. Integral channel features</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Pedro F Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometry-Consistent Generative Adversarial Networks for One-Sided Unsupervised Domain Mapping</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries</title>
		<author>
			<persName><forename type="first">Qianjiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08435</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21798" to="21809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">U-GAT-IT: unsupervised generative attentional networks with adaptive layer-instance normalization for imageto-image translation</title>
		<author>
			<persName><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonwoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Style transfer by relaxed optimal transport and self-similarity</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Salavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10051" to="10060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diverse imageto-image translation via disentangled representation</title>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Diverse imageto-image translation via disentangled representations</title>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="701" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06343</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep photo style transfer</title>
		<author>
			<persName><forename type="first">Fujun</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4990" to="4998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">Yk</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">Shumeet</forename><surname>Henry A Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">TPAMI</biblScope>
			<biblScope unit="page" from="23" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Xgan: Unsupervised image-to-image translation for many-to-many mappings</title>
		<author>
			<persName><forename type="first">Amélie</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Moressi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05139</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6537</idno>
		<title level="m">Fracking deep convolutional image descriptors</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Instanceaware image colorization</title>
		<author>
			<persName><forename type="first">Jheng-Wei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7968" to="7977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Attentionguided generative adversarial networks for unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<editor>IJCNN</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transgaga: Geometry-aware unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Harmonic unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Real-time user-guided image colorization with learned deep priors</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
