<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Fusion Network with Multi-scale Multi-path and Cross-modal Interactions for RGB-D Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youfu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical and Biomedical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<addrLine>83 Tat Chee Avenue</addrLine>
									<settlement>Kowloon Tong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Fusion Network with Multi-scale Multi-path and Cross-modal Interactions for RGB-D Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FA04D06696D750C169C37C2A458293B</idno>
					<idno type="DOI">10.1016/j.patcog.2018.08.007</idno>
					<note type="submission">Received date: 4 July 2017 Revised date: 19 April 2018 Accepted date: 12 August 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition RGB-D</term>
					<term>convolutional neural networks</term>
					<term>multi-path</term>
					<term>saliency detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p> Using CNNs to fuse RGB and depth data with only single path is not sufficient.  Both global reasoning and local capturing are important for saliency detection.  Bottom-up cross-modal interactions are also beneficial for learning complements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T multi-path fusion network with cross-modal interactions (MMCI), in which the traditional two-stream fusion architecture with single fusion path is advanced by diversifying the fusion path to a global reasoning one and another local capturing one and meanwhile introducing cross-modal interactions in multiple layers. Compared to traditional two-stream architectures, the MMCI net is able to supply more adaptive and flexible fusion flows, thus easing the optimization and enabling sufficient and efficient fusion. Concurrently, the MMCI net is equipped with multi-scale perception ability (i.e., simultaneously global and local contextual reasoning). We take RGB-D saliency detection as an example task. Extensive experiments on three benchmark datasets show the improvement of the proposed MMCI net over other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed increasing pervasion of RGB-D data in a wide range of computer vision <ref type="bibr" target="#b0">[1]</ref> and robotic vision systems <ref type="bibr" target="#b1">[2]</ref>. Compared to RGB data, which supply detailed appearance and texture, depth data additionally present clear object shapes and spatial layouts. Compared to RGB sensors, depth sensors are more robust to lighting change and color variations. Hence, RGB and depth data are complementary in terms of both data distributions and applicable scenes. Consequently, how to fuse the RGB and depth information in a sufficient manner has been a fundamental problem in dealing with RGB-D data.</p><p>Previous works on dealing with RGB-D data can be generally categorized into two sets: <ref type="bibr" target="#b0">(1)</ref> designing handcrafted features from RGB-D data with domain-specific knowledge <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>; <ref type="bibr" target="#b1">(2)</ref> operating RGB and depth data separately and then fusing the decisions. For the first solution, the handcrafting process highly relies on domainspecific knowledge, making the features hard to be generalized to other tasks readily.</p><p>Moreover, the handcrafted features are deficient in high-level reasoning, which is important for scene understanding. Some unsupervised learning methods including sparse coding <ref type="bibr" target="#b7">[8]</ref> and auto-encoder <ref type="bibr" target="#b8">[9]</ref> are further introduced to address the drawbacks in handcrafted RGB-D features. Nonetheless, these methods, limited by the shallow architectures, are still far from learning high-level representations and satisfactory generalization ability.</p><p>Recently, mounting works resort to Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> to process RGB-D data for the powerful ability of CNNs in excavating high-level representations and modelling complex correlations. Motivated by the advantages, a Even so, the question of how to design the fusion architecture is still under-studied in existing works. Related works mainly involves three veins: 1) RGB and depth data are intergrated as joint inputs to a CNN (noted as "Input fusion" in Fig. <ref type="figure" target="#fig_1">1</ref>. (a)); 2) RGB and depth data are fed into each stream separately and then their low-level or high-level representations are combined as joint representations for further decision (noted as "Early fusion" and "Late fusion" in Fig. <ref type="figure" target="#fig_1">1</ref>. (b), (c), respectively); 3) conducting each stream independently and fusing their decisions. Although some recent works consider the relationships between RGB and depth data (e.g., the independence and consistency) and achieve inspiring performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, a common limitation in these networks is that the fusion path for RGB and depth data is typically onefold, which, in our opinion, is deficient to integrate all the information from RGB and depth. An ideal RGB-D saliency detection system is expected to combine the multi-scale cross-modal complements for joint global contextual reasoning and local spatial capturing. Fulfilling these objectives calls for multiple fusion paths to avoid conflicting optimization between sub-tasks. Otherwise, it is unlikely to achieve collective optimization sufficiently. Therefore, we argue that a well-engineered multi-modal fusion architecture Our argument is supported by recent advances <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> in designing basic CNN architectures, which reveal that the philosophy of designing a CNN has diverted from merely increasing the depth <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> in plain nets to additionally enrich the connection paths <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. To ease the gradient-based optimization, authors in <ref type="bibr" target="#b19">[20]</ref> introduce gating units to allow the flow of information to cross multiple layers unimpededly. He et al. <ref type="bibr" target="#b20">[21]</ref> propose shortcut connections to reformulate learning desired unreferenced layers as approximating residual functions. The proposed shortcut with identity function in the ResNet <ref type="bibr" target="#b20">[21]</ref> can be viewed as a way of facilitating information transmission by promoting the flow path. More recently, Xie et al. <ref type="bibr" target="#b22">[23]</ref> conclude the split-transformmerge strategy in <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> as a new dimension termed "cardinality" accompanying depth and width. The cardinality serves as the size of the set of transformations conjunct with multiple flow paths. In this work, it has been experimentally proven that increasing the cardinality is a more effective and efficient way to boost performance than going deeper or wider.</p><p>Motivated by the philosophy and success inherited in <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, we believe that the multi-modal fusion problem will benefit a lot in terms of optimization efficiency and fusion sufficiency from introducing more paths in the fusion network, while few multimodal fusion networks take this into account. Given this limitation, our further question is how to design a multi-path multi-modal fusion network. We find that human beings perceive and understand scenes in an integrated way <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, i.e., locating target objects with a global perspective and capturing fined details with a local view. Similarly in various robotic vision and computer vision tasks, both global understanding and local</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 7 capturing are typically indispensable. For example, an ideal grasper is expected to identify the target object (e.g., a cup) and meanwhile highlight the specific subpart for grasping (e.g., the handgrip). Also, the salient object detection task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 8 aims at highlighting the object attracting human beings most, also needs global reasoning to judge which object is the most salient and local perspective to obtain precise object boundaries.</p><p>Based on the significance of introducing multi-path and incorporating global and local perspectives jointly, we tailor a multi-path multi-scale multi-modal (MMCI net) fusion network (shown in Fig. <ref type="figure" target="#fig_7">2(b)</ref>), in which the fusion path is diversified to a global contextual reasoning one and a local capturing one. Our proposed MMCI net can ease the joint optimization process and simultaneously endow the multi-modal fusion network with multi-scale perception. Firstly, the network stream for each modality, which embeds a global reasoning branch and a local capturing branch, is trained separately with the same architecture shown in Fig. <ref type="figure" target="#fig_2">2</ref> Another insight drawn from the work is that the complementary information between RGB and depth data is concurrent from low-level representations to high-level contexts.</p><p>To this end, we add cross-modal interactions from the RGB stream to the depth stream in shallower layers to further encourage cross-modal combinations. Without these crossmodal interactions, the RGB and depth streams would have to be learnt separately and their complements could not be fully utilized in the feature extraction process. As a result, the cross-modal complementarity in shallow layers is unlikely to be sufficiently explored. In addition to the main late fusion stage, the cross-modal interactions enable the learning of cross-modal complements in the bottom-up process, allowing explorations on more discriminative multi-modal features in different feature levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Besides, the cross-modal interactions introduce additional back-propagation gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 9 <ref type="bibr" target="#b20">[21]</ref> from the RGB stream to the depth stream, which further encourage the depth stream to learn complementary features.</p><p>In this work, we use the salient object detection task to exemplify and verify our proposed multi-modal fusion strategies. In summary, the contributions of this work are three-fold:</p><p>(1) We propose a multi-path multi-modal fusion network. The diversified fusion paths reduce fusing ambiguity, ease the optimization process and afford better fusion adaptability than previous fusion networks relying on a single straightforward path.</p><p>(2) The MMCI net is endowed with multi-scale contextual reasoning ability, incorporating global reasoning and local capturing simultaneously in an end-to-end architecture. The cross-modal interactions not only empower additional gradients for encouraging the learning of the depth stream, but also allow exploration on cross-modal complements across low-level representations and high-level contexts.</p><p>(3) Extensive evaluations on three public datasets show substantial and consistent improvements of our method over state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Although various models have been devised on RGB saliency detection, the RGB-D saliency detection works are severely limited. Most of previous works on RGB-D saliency detection can be categorized into three modes: input fusion, feature fusion and result fusion. Methods based on input fusion concatenate the RGB-D pair by directly regarding depth image as an undifferentiated channel <ref type="bibr" target="#b28">[29]</ref> or setting constant weights on RGB and depth channels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. The joint inputs are then followed by further feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 10 extraction and saliency inference procedures. Feature fusion methods extract features from RGB and depth modalities separately and then combine their features for saliency detection jointly. In <ref type="bibr" target="#b31">[32]</ref>, hand-crafted features from RGB and depth modalities are concatenated as network input to generate collective features. As the latest CNN-based RGB-D saliency detection model concurrent with ours, this method yields appealing improvements over traditional models based on hand-crafted features. However, the low-level features designed in <ref type="bibr" target="#b31">[32]</ref> are of limited discriminativeness while the crafting process results in unavoidable information loss, limiting the power of CNNs in further feature extraction and saliency inference. Besides, the human prior knowledge guiding the low-level feature extraction may be conflicting with the way of CNNs in perceiving saliency, thus restricting saliency reasoning of the CNNs. Moreover, combining the handcrafted features as the input of a CNN makes it unable to leverage the complementary high-level representations and also unable to leverage existing models trained on large-scale datasets such as the Alex-Net <ref type="bibr" target="#b9">[10]</ref> and VGG-Net <ref type="bibr" target="#b17">[18]</ref> to initialize the network. Due to the random initialization and the small-scale RGB-D training dataset, the multi-modal fusion network is unlikely to be well-trained. As a result, the advantages of CNNs in learning discriminative high-level features are not well explored.</p><p>Result fusion approaches model saliency detection from each modality separately and then combine their predictions by directly summing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, multiplying <ref type="bibr" target="#b32">[33]</ref> or other empirical rules <ref type="bibr" target="#b33">[34]</ref>. However, these result fusion methods cannot leverage the underlying complementary information from the paired modality during the feature extraction process. As a result, the fusion may be insufficient. Another related task is saliency detection in compressed domain or for stereoscopic visual contents. Fang et al. <ref type="bibr" target="#b5">[6]</ref> propose to extract saliency features from discrete cosine transform coefficients in the</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>Put your running title here: Elsevier General template for review 11 JPEG bit-stream. In <ref type="bibr" target="#b26">[27]</ref>, saliency detection in compressed domain is extended to video contents. The motion saliency and static saliency are measured separately and then combined by a weighted result fusion manner. The saliency detection in stereoscopic images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> basically share the similar multi-modal fusion modes for RGB-D image pairs. Recently, <ref type="bibr" target="#b24">[25]</ref> investigates the problem of saliency detection in stereoscopic videos with a classical approach and introduces a computational model to fuse RGB, depth and temporal modalities.</p><p>As for other RGBD-induced tasks conducted with CNNs, Andreas et al. <ref type="bibr" target="#b12">[13]</ref> adopt the traditional two-stream architecture, in which the RGB and depth streams are completely independent and then combined by a single fusion layer in the late stage.</p><p>Wang et al. <ref type="bibr" target="#b10">[11]</ref> and Zhu et al. <ref type="bibr" target="#b14">[15]</ref> extract deep features from each modality separately and then learn the decision weights of different modalities by considering their dependency and consistency in the prediction layer. Actually, both <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b14">[15]</ref> follow the result fusion scheme, which cannot leverage the complementary representations from the counterpart in the feature extraction stage. Besides, the modality relationships formulated in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b14">[15]</ref> are customized for image-wise classification tasks, which are infeasible to be generalized to pixel-wise tasks. Moreover, these methods combine the multi-modal CNN streams by a single fusion layer without any additional paths or connections. However, we argue that it may be ambiguous and insufficient to fuse information flows from multiple modalities only by a single point and path due to the versatility of CNN features. Hence, a multi-path fusion framework with diversified fusion points and more adaptive fusion paths together with additional cross-modal interactions is in demand, which not only reduces fusion ambiguity and eases the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 12 gradient-based optimization process, but also provides a platform for incorporating multi-scale contextual reasoning into the saliency inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>Considering the gap between RGB and depth data in terms of distribution and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RGB-induced saliency detection network (R_SalNet)</head><p>The network architecture of the R_SalNet shown in Fig. <ref type="figure" target="#fig_7">2(a</ref>) is tailored based on the widely-used VGG-Net <ref type="bibr" target="#b17">[18]</ref>. We retain its 13 convolutional (conv.) layers and remove Then the network is bifurcated into a global understanding branch and a local capturing branch at the conv.3_3 layer. As visualized in <ref type="bibr" target="#b35">[36]</ref>, the mid-level layers respond to parts of objects while higher layers are more related to task understanding (i.e., in a global or local view). Hence the shallower features, which are agnostic to task understanding, can be shared between different branches. Therefore, bifurcating the network at an intermediate layer is a reasonable choice to diversify the fusion path with adding less parameters.</p><p>For the global branch, the feature maps after the pooling3 layer are fed to two conv.conv.-conv.-pooling blocks with the output resolution decreasing as 28×28-14×14-7×7 and the effective receptive field increasing gradually. The 7×7 feature maps are followed by a FC layer with 3136 nodes, in which each node infers saliency for each pixel with the entire image as the receptive field. The 3136 saliency probabilities are warped to 56×56 saliency map. The FC layer empowers global contextual reasoning to locate the salient object correctly and avoid highlighting local salient interference.</p><p>However, the sequential pooling layers in the global understanding branch result in loss of resolution and incapability of capturing fined boundaries of the salient object. So another branch for capturing local details is in demand.</p><p>For the local capturing branch, we adopt the dilated convolution (di_conv.) <ref type="bibr" target="#b36">[37]</ref> to aggregate multi-scale context and retain the output resolution. Different from traditional contiguous convolutional filters, the dilated convolutional filter allows spaces between each cell. In this way, the receptive filed can be enlarged while the resolution can be reserved. For example, with the dilation factor k, the size of the receptive field of each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 14 element in the original 3×3 filter will be enlarged to (2 k+1 -1)×(2 k+1 -1). In our network, the feature maps after the conv.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y</head><p>) with the cross-entropy loss:</p><formula xml:id="formula_1"> 1 1 1 ({ , , } | ( , ,<label>, )) 1</label></formula><formula xml:id="formula_2">( , ), ( |{ , };{ , })( , ) ( |{ , };{ , })( , ) ( ),                     GG LL ii GG ii LL ii RD RD RD iN N W H RD RD iG i x y RD RD L L X X F x y X X x y N X X x y r Y Y W<label>(1)</label></formula><p>where ( , ) log( ) (1 )log( <ref type="formula" target="#formula_1">1</ref>), The loss function indicates that the multi-path fusion network equips the optimization process with diversified flow paths. Therefore, the fusion process is more adaptive and flexible. Meanwhile, the MMCI net is endowed with global contextual reasoning and local capturing simultaneously from both modalities.</p><formula xml:id="formula_3">F Y X Y X Y X    <label>and</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Put your running title here: Elsevier General template for review 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the effectiveness of our network, we perform comprehensive experiments on three datasets, NLPR <ref type="bibr" target="#b28">[29]</ref>, NJUD <ref type="bibr" target="#b3">[4]</ref> and STEREO <ref type="bibr" target="#b37">[38]</ref>, which consist of 1000, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>The evaluation metrics adopted in this paper includes the Precision-Recall (PR) curve, AP (average precision) score and F-measure score. In this work, we follow the suggestions in <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref> to vary the threshold from 0 to 255 gradually to generate a series of Precision-Recall pairs. These Precision-Recall pairs consequently form the Precision-</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T Recall (PR) curve.</formula><p>The AP score is computed by averaging the precision values at evenly spaced recall levels. When computing the F-measure score, we adopt an image adaptive threshold as suggested in <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref> to binary the saliency map. More specifically, the adaptive threshold is T = µ +σ, where µ is the mean saliency value and σ is the standard deviation of the saliency map, respectively. With this adaptive threshold, we can get one Precision-Recall pair for each image, generating a single Fmeasure score. The F-measure score balances the precision and recall by</p><formula xml:id="formula_5">  2 2 1 Precision Recall F Precision Recall         </formula><p>, we follow the suggestions in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> to set 2 1   .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>All the experiments are conducted on the caffe <ref type="bibr" target="#b43">[44]</ref> toolbox on a workstation with a GTX 1070 GPU. We adopt the stochastic gradient descent (SGD) <ref type="bibr" target="#b44">[45]</ref> as the optimizer.</p><p>We set the size of mini-batch size as 8 and maximum iteration as 30000. Since the loss is accumulated from 3136 nodes, we set a small learning rate as 10 -7 . The weight decay and momentum are set to 0.0002, and 0.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">On the importance of fusing at a late stage</head><p>The first question we investigate is at which stage to fuse will benefit the two-stream architectures more. Firstly, it is infeasible to directly regard RGB-D as undifferentiated  The comparisons shown in Table <ref type="table">I</ref> indicate that fusing at the late stage offers better performance, which fits with our motivation that the higher layers of a CNN are more task-related and modal-agnostic, thus easier to learn common representations from different modalities towards a specific task (i.e., saliency detection). In contrast, the low-level features are more modal-specific and harder to map into common representations. In this section, we investigate the effectiveness of introducing cross-modal interactions. Fig. <ref type="figure">5</ref> visually shows the improvement after adding cross-modal interactions. Although the multi-path fusion strategy has achieved appealing results, introducing additional cross-modal interactions ("CI") further improves the uniformity and integrity of saliency maps. We attribute the improvement to the benefits from the cross-modal complement in shallow levels. We also compare the multi-path fusion network (noted as "MP") with MMCI variants that add the cross-modal interactions via different directions, i.e., from the depth stream to the RGB stream (noted as "MP+CI-R"), from the RGB stream to the depth stream (noted as "MP+CI-D"), or bi-directional interactions (noted as "MP+CI-Bi"). As shown in Table <ref type="table">II</ref>, the cross-modal interactions allow additional accuracy gains. Besides, the "MP+CI-R" performs slightly better than the "MP+CI-D" and outperforms the "MP+CI-Bi" with a healthy margin. The reason may be that both the "MP+CI-R" and "MP+CI-D" can offer complementary representations from one stream to its counterpart, while the "MP+CI-Bi" may introduce too much parameters and the bi-directional connections may destroy the fragile architecture, making the fusion flow be in chaos, thus hindering the joint optimization and decreasing the performance. We compare our method with nine state-of-the-art methods, including LBE <ref type="bibr" target="#b2">[3]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">On the importance of multi-path fusion</head><p>NLPR <ref type="bibr" target="#b28">[29]</ref>, SRDS <ref type="bibr" target="#b29">[30]</ref> and EGP <ref type="bibr" target="#b45">[46]</ref>, ACSD <ref type="bibr" target="#b3">[4]</ref>, DCMC <ref type="bibr" target="#b34">[35]</ref>, MBP <ref type="bibr" target="#b46">[47]</ref>, MDSF <ref type="bibr" target="#b47">[48]</ref> and one latest CNN-based RGB-D saliency detection model DF <ref type="bibr" target="#b31">[32]</ref>. As shown in Fig. <ref type="figure" target="#fig_14">6</ref>, our model achieves significant outperformance over others in terms of PR curves, AP and F-measure scores. We attribute this advantage to the diversified fusion paths and the integrated multi-scale reasoning capacity as well as the cross-modal interactions across multiple layers. Particularly, the CNN-based DF model relies on additional  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to utilize CNNs for RGB-D saliency detection. We improve the traditional two-stream architecture by diversifying the multi-modal fusion paths and introducing cross-modal interactions in multiple layers. The proposed strategies reduce fusion ambiguity, ease the multi-modal fusion optimization and incorporate global reasoning and local capturing together, thus allowing more adaptive and sufficient </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>S C R I P T Put your running title here: Elsevier General template for review 5 wide range of CNNs based on different architectures have been introduced [11-17].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Traditional two-stream CNN architectures, including (a) input fusion; (b) early fusion; (c) late fusion.</figDesc><graphic coords="6,74.35,79.10,421.68,158.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The frameworks of the proposed R_SalNet and the D_SalNet (a) and the MMCI</figDesc><graphic coords="8,73.65,78.70,424.38,350.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a). Then we connect their local and global branches respectively and the predictions of the combined global and local paths are summed as the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig. 2(a). Finally, the R_SalNet and D_SalNet are combined by multiple paths and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>S C R I P T Put your running title here: Elsevier General template for review 13 all the 3 fully-connected (FC) layers. In the training phase, the fixed 224×224 RGB image is fed into two conv.-conv.-pooling and a conv.-conv.-conv. block sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>3_3 layer are fed to two stacked di_conv.-di_conv.-di_conv. blocks. By customizing the dilation factor of each dilated convolutional layer in the local branch, we are still able to initialize the local branch with the original VGG-Net, although the resolutions of di_conv. layers in the local branch and their counterparts in the original VGG-16 net are different. The resolutions of output feature maps are kept as 56×56 across the local branch without loss, enabling fined details be captured in the saliency prediction layer. Then a 1×1 convolution layer with one channel is appended to transform the 512×56×56 feature maps into 56×56 saliency probabilities. Compared to the input 224×224 image, the inference context is 4 × 4, which is noted as "local reasoning". Hence, the local branch is a fully convolutional network without any pooling or FC layers, preserving the resolution and spatial structure and inferring saliency in a local context. The global and local branches work jointly to equip the R_SalNet with multi-scale contextual reasoning for saliency detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3. 2 (</head><label>2</label><figDesc>Depth-induced saliency detection network (D_SalNet)The basic architecture of the D_SalNet inherits that of the R_SalNet and the implementation differences between the D_SalNet and the R_SalNet mainly lie in data pre-processing and network initialization. More specifically, we adopt the three-channel HHA<ref type="bibr" target="#b13">[14]</ref> representations (i.e., horizontal disparity, height above ground and angle with gravity) encoded from the original depth values as input. Compared to the original onechannel depth map, the HHA representations present additional geometric properties (surface normal and height) and allow the re-use of existing networks (i.e., the VGG-A C C E P T E D M A N U S C R I P T Put your running title here: Elsevier General template for review 15 Net) trained with three-channel RGB images. The mean values for the HHA representations are computed from the training samples. When training the D_SalNet, we choose the parameters of the learnt R_SalNet rather than the VGG-Net as initialization. The underlying consideration is that the R_SalNet is more task-relevant to the D_SalNet than the original VGG-Net, while different tasks promote task-specific parameters. Hence, initializing with the R_SalNet bridges the task gap and makes better use of limited training samples to supply pre-understanding of saliency detection for the D_SalNet. 3.3 Multi-scale multi-path fusion network with cross-modal interactions (MMCI net) As shown in Fig. 2(b), the local and global branches of the R_SalNet and D_SalNet are combined at a late stage separately. Concretely, the FC layers in each global branch are combined by a 1×1 convolution layer. The 1×1 convolution filter combines 2 input pixels at each location into 1 output pixel as the joint prediction. Formally, let { , } RD ii XX denotes the i-th RGB-D input pair, { , } GG RD denotes the parameters (i.e., the weights and biases) of the global branches in the R_SalNet and D_SalNet. Then the merged global prediction for each pixel at location (x,y) can be represented as where  indicates the connection parameters between the 1×1 convolutional layer and preceding FC layers and Ω G is the sigmoid activation function to generate saliency values, respectively. Meanwhile, the di-conv5_3 feature maps in each local branch are stacked as 1024×56×56 features. Then a 1×1 convolutional layer is followed to combine these features and infer saliency collectively. Similarly, let { , } LL RD  be the parameters of the local branches in the R_SalNet and D_SalNet. Then the joint prediction merging the A C C E P T E D M A N U S C R I P T Put your running title here: Elsevier General template for where  denotes the connection parameters of the 1×1 convolutional layer with the 1024×56×56 feature maps and L  is the sigmoid activation function, respectively. Besides, the cross-modal interactions, implemented by element-wise summation, are introduced from the depth stream to the RGB stream in the conv2_2, conv3_3, conv4_3 and di-conv4_3 layers of the shared branch, global branch and local branch respectively. Then the saliency maps predicted by the merged global and local paths are combined by element-wise summation as the final saliency map, which is then compared to the corresponding ground truth saliency mask i Y ( {0,1} i </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>()  r W is a common regularization term encouraging weight sparsity and  is the weight decay factor. N is the number of training samples and W=H=56 are the width and height of the warped ground truth mask and the predicted saliency map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>2003 and 797 paired RGB-D images and corresponding ground truth saliency masks collected from a large range of indoor and outdoor scenes. We sample 650 image pairs from the NLPR dataset and 1400 image pairs from the NJUD dataset randomly and combine them as the training dataset. We also choose 50 image pairs from the NLPR dataset and 100 image pairs from the NJUD dataset as the validation set for tuning parameters (e.g., the learning rate). The remaining samples and the STEREO dataset are used for testing. Due to the small scale of the datasets, we augment the training samples 12 times to overcome the risk of overfitting. The training samples are horizontally flipped or cropped in the top, bottom, leftmost or rightmost or all the boundaries with a small scale (only 0.1 times of the width or height). The RGB-D pair and the corresponding ground truth are augmented in the same way, thus preserving the correspondence between the training RGB-D pair and its ground truth. Besides, the contextual information after horizontal flipping or small-scale cropping in boundaries is well-preserved. Consequently, the ground truth salient object is immune to horizontal flipping and small-scale cropping in boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4 -</head><label>4</label><figDesc>channel inputs and train a specific CNN from scratch due to the unavailability of large-scale labeled RGB-D datasets. Then we use the global fusion path to exemplify the advantages of fusing at a late stage by varying the fusion point from the FC layer to the pooling1 layer. The feature maps at the fusion layers of two streams are A C C E P T E D M A N U S C R I P T Put your running title here: Elsevier General template for review 19 concatenated and followed by an additional 1×1 convolutional layer to calibrate the number of feature maps to match the architecture of the global branch of the R_SalNet or D_SalNet. Hence the following layers after combination can still use the trained R_SalNet or D_SalNet as initialization rather than being trained from scratch. Comparisions of the fusion performance on the NLPR dataset implemented with different fusion points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Quantitative comparison between multi-path fusion and single-path fusion</figDesc><graphic coords="20,74.00,199.32,421.88,285.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Visual comparisons of the proposed "Multi-path" fusion strategy to the</figDesc><graphic coords="21,76.65,78.85,420.08,359.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>23 4. 7</head><label>237</label><figDesc>Put your running title here: Elsevier General template for review Compare to other RGB-D saliency detection models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Quantitative comparisons to other RGB-D saliency detection models. The LBE,</figDesc><graphic coords="24,74.60,79.20,421.78,380.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual comparisons of our model with other RGB-D saliency detection models.</figDesc><graphic coords="25,73.55,79.70,419.48,501.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Put your running title here: Elsevier General template for review 26 fusion and enjoying encouraging accuracy gains. More importantly, the proposed MMCI net can be readily applied to a large range of RGB-D tasks, especially for those pixel-wise prediction tasks such as RGB-D semantic segmentation, grasping and the like. Besides, it is also promising to leverage the proposed strategies for other multimodal data, such as RGB and optical flow, infrared signals or aerial images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="22,75.20,83.50,420.43,193.73" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Research Grants Council of Hong Kong (Project No CityU 11205015 and CityU 11255716).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human action recognition in RGB-D videos using motion sequence information and deep learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ijjina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chalavadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="504" to="516" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion segment decomposition of RGB-D sequences for human behavior understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="222" to="233" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Depth saliency based on anisotropic centersurround difference, Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Hybrid Framework for Automatic Joint Detection of Human Poses in Depth Frames</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Maharjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Saliency-based stereoscopic image retargeting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page" from="347" to="358" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Narwaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2625" to="2636" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning for RGB-D based object recognition, Experimental Robotics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mmss: Multi-modal sharable and specific feature learning for rgb-d object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1125" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RGB-D object recognition and pose estimation based on pre-trained convolutional neural network features, Robotics and Automation (ICRA)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1329" to="1335" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multimodal deep learning for robust rgb-d object recognition, Intelligent Robots and Systems (IROS)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative Multi-Modal Feature Fusion for RGBD Indoor Scene Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Weibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2969" to="2976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-modal deep feature learning for RGB-D object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="300" to="313" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal uniform deep learning for RGB-D person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="446" to="457" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<title level="m">Highway networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stimulus specific responses from beyond the classical receptive field: neurophysiological mechanisms for local-global comparisons in visual neurons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Miezin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="407" to="430" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual attention modeling for stereoscopic video: a benchmark and computational model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4684" to="4696" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A video saliency detection model in compressed domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottom-up saliency detection model based on human visual sensitivity and amplitude spectrum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Salient region detection for stereoscopic images</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014 19. 2014</date>
			<biblScope unit="page" from="454" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An in depth view of saliency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RGBD Salient Object Detection via Deep Fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Internet Multimedia Computing and Service</title>
		<meeting>International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth really Matters: Improving Visual Salient Region Detection with Depth</title>
		<author>
			<persName><forename type="first">K</forename><surname>Desingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multipleinstance learning framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Background prior-based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent tricks, Neural networks: Tricks of the trade</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploiting global priors for RGB-D saliency detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2015 IEEE Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<title level="m">A multilayer backpropagation saliency detection algorithm and its applications, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Depth-Aware Salient Object Detection and Segmentation via Multiscale Discriminative Saliency Fusion and Bootstrap Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
