<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qin</forename><surname>Wang</surname></persName>
							<email>qinwang1@link.</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong(Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyuan</forename><surname>Wang</surname></persName>
							<email>boyuanwang@link.</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenlei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong(Shenzhen)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong(Shenzhen)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<email>shengwwang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution">The Chinese University of Hong Kong(Shenzhen)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protein secondary structure prediction (PSSP) is an essential task in computational biology. To achieve the accurate PSSP, the general and vital feature engineering is to use multiple sequence alignment (MSA) for Position-Specific Scoring Matrix (PSSM) extraction. However, when only low-quality PSSM can be obtained due to poor sequence homology, previous PSSP accuracy (merely around 65%) is far from practical usage for subsequent tasks. In this paper, we propose a novel PSSM-Distil framework for PSSP on low-quality PSSM, which not only enhances the PSSM feature at a lower level but also aligns the feature distribution at a higher level. In practice, the PSSM-Distil first exploits the proteins with high-quality PSSM to achieve a teacher network for PSSP in a full-supervised way. Under the guidance of the teacher network, the low-quality PSSM and corresponding student network with low discriminating capacity are effectively resolved by feature enhancement through EnhanceNet and distribution alignment through knowledge distillation with contrastive learning. Further, our PSSM-Distil supports the input from a pre-trained protein sequence language BERT model to provide auxiliary information, which is designed to address the extremely low-quality PSSM cases, i.e., no homologous sequence. Extensive experiments demonstrate the proposed PSSM-Distil outperforms state-of-the-art models on PSSP by 6% on average and nearly 8% in extremely low-quality cases on public benchmarks, BC40 and CB513.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Protein structure analysis, especially protein tertiary (3D) structure, plays a critical role for practical protein applications, such as the understanding of the protein functions and the design of drugs <ref type="bibr" target="#b17">(Noble, Endicott, and Johnson 2004)</ref>. Currently, there are three mainstream methods for protein tertiary structure (3D) prediction, i.e., X-ray crystallography and nuclear magnetic resonance (NMR) <ref type="bibr" target="#b34">(Wuthrich 1989</ref>), cryo-EM based methods <ref type="bibr" target="#b32">(Wang et al. 2015)</ref> and computeraided ab initio prediction <ref type="bibr" target="#b14">(Mandell and Kortemme 2009)</ref>. Given the extremely time-consuming drawback of X-ray crystallography, the sequence length limitation of nuclear magnetic resonance (NMR) and the expensive equipment requirement for cryo-EM, computer-assisted protein structure  prediction attracts broad attention due to its convenience and superior performance. For ab initio tertiary structure prediction, protein property, such as protein secondary structure, provides crucial information as it represents the local patterns of protein structure. Therefore, enhancing the accuracy of protein secondary structure prediction (PSSP) is fundamental for subsequent protein structure prediction.</p><formula xml:id="formula_0">M G R V T S G T M R F G V T F R E V M S K -H S G T M R F A -T W R D V M -K -S S G S M R F -V T Y R E - M C K -T W -T M L V P -T F K E - M L R V H W G T -L F E L T -K E V -G R I S L -T M -V G -T F S D - -I K -H L G T F K F T L T W S E V M G R V T S G T M R F G V T F R E V M S K -H S G T M R F A -T W R D V M C K -T W -T M L V P -T F K E - -G R I S L -T M -V G -T F S D - -I K -H L G T F K F T L T W S E V</formula><formula xml:id="formula_1">M G R V T S G T M R F G V T F R E V M G R V T S G T M R F G V T F R E V M G R V T S G T M R F G V T F R E V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSSM-Distil</head><p>PSSP is to classify every amino acid on a protein sequence with a secondary structure label (coil, alpha helix, beta-sheet for 3-state secondary structure) indicating the local structure, which is very similar to sequence labeling in natural language processing (NLP). Existing methods usually use the homologs searched from the protein database, which is called multiple sequence alignment (MSA), to generate the Positional-Specific Scoring Matrix (PSSM) for protein sequence. Various sophisticated deep learning models <ref type="bibr" target="#b14">(Li and Yu 2016;</ref><ref type="bibr" target="#b33">Wang et al. 2016;</ref><ref type="bibr" target="#b38">Zhou and Troyanskaya 2014)</ref> achieved satisfactory PSSP performance (around 85% Q3 accuracy) when taking high-quality PSSM along with one-hot amino acid sequence as evolutionary information. Specifically, <ref type="bibr" target="#b38">Zhou and Troyanskaya (2014)</ref> used a deep convolutional network to model the relation between PSSM features and labels. <ref type="bibr" target="#b33">Wang et al. (2016)</ref> proposed an improved model by adding a conditional random field after CNN to better model the sequential relation. <ref type="bibr" target="#b26">Sønderby and Winther (2014)</ref> tackled the problem with a two layers LSTM, while <ref type="bibr" target="#b14">Li and Yu (2016)</ref> added GRU units after convolutional layers to further boost the representation power of the model. <ref type="bibr" target="#b8">Guo et al. (2020)</ref> set the CNN and LSTM networks in parallel to capture both local and long-range information. However, even benefiting from the powerful deep discriminating models, the PSSP performance of protein with low sequence homology and low-quality PSSM is still far from being satisfactory, achieving usually around 65% Q3 accuracy. Such low PSSP would directly affect the subsequent protein folding and tertiary structure prediction. Recent work <ref type="bibr" target="#b8">Guo et al. (2020)</ref> exploited the "Bagging" mechanism to obtain the enhanced PSSM for protein with low-quality PSSM through a fixed ratio MSA down-sampling in an unsupervised manner. However, such "Bagging" method merely conducts PSSM feature enhancement, while ignoring the joint optimization of the enhanced feature and final PSSP on high-level semantic space, leading to inferior robustness and PSSP performance.</p><p>Therefore, in this paper, we will address the practical PSSP problem for protein sequence with low sequence homology (i.e., low-quality PSSM) in a large database. We propose a novel framework called PSSM-Distil, as illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>. The proposed model automatically enhances the lowquality PSSM by aligning its distribution to the high-quality ones. That is to say, PSSM-Distil first exploits proteins with high-quality PSSM to obtain a classifier (any previous general model like BLSTM) for PSSP as a teacher network in a full-supervised way. Under the guidance of teacher network, low-quality PSSM through an EnhanceNet and corresponding student network with low discriminating capacity is effectively resolved by feature enhancement and distribution alignment through knowledge distillation with contrastive learning, which is the core contribution of our proposed PSSM-Distil model. Additionally, our PSSM-Distil model supports the input from the pre-trained BERT <ref type="bibr" target="#b22">(Rao et al. 2019</ref>) model on UniRef90 to provide auxiliary information, which is designed to address the extremely lowquality PSSM cases, i.e., a protein with no homologous sequence. Also, extensive experiments demonstrate the proposed PSSM-Distil outperforms state-of-the-art models on PSSP by a large margin on the validation set of CullPDB, public benchmark CB513 and newly proposed large dataset BC40 (release date is 2020-07-28).</p><p>Our contributions are summarized as follows: 1) We propose a new framework called PSSM-Distil for protein secondary structure prediction (PSSP) on low-quality PSSM, which exploits a teacher-student network to distill knowledge from high-quality PSSM with contrastive learning. 2) Our PSSM-Distil could not only obtain enhanced PSSM in a self-supervised manner through prior knowledge-based down-sampling, but also align the enhanced PSSM distribution with the high-quality one for final PSSP, leading to a largely improved prediction accuracy, i.e., average 6% for protein with low-quality PSSM, and over 8% improvement in extremely low-quality cases. 3) We further release a large scale up-to-date test dataset BC40 (release date is 2020-07-28) to verify the effectiveness of PSSM-Distil. Unlike <ref type="bibr" target="#b22">Rao et al. (2019)</ref> who directly utilized BERT's embedding to facilitate PSSP, we are the first paper to sampling MSAs from pre-trained BERT's output to construct BERT Pseudo PSSM which will input to PSSP as auxiliary information and significantly improve the PSSP performance of protein with no homology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Multiple Sequence Alignment (MSA) MSA is a sequence alignment of multiple homologous protein sequences for a target protein <ref type="bibr" target="#b31">(Wang and Jiang 1994)</ref>. It is a key technique for modeling sequence relationships in computational biology. Given a protein database and a protein sequence, MSA is searched by performing pairwise comparisons <ref type="bibr" target="#b1">(Altschul et al. 1990</ref>), Hidden Markov Model-like probabilistic models <ref type="bibr" target="#b7">(Eddy 1998;</ref><ref type="bibr" target="#b11">Johnson, Eddy, and Portugaly 2010;</ref><ref type="bibr" target="#b23">Remmert et al. 2012)</ref>, or a combination of both <ref type="bibr" target="#b2">(Altschul et al. 1997)</ref> to align the sequence against the given database. Once MSA is conducted, it is usually transferred to the Position-Specific Scoring Matrix (PSSM) for subsequent tasks.</p><p>Low-quality PSSM Enhancement. Since MSA and PSSM are critical for protein property prediction, "Bagging" <ref type="bibr" target="#b8">(Guo et al. 2020)</ref> is the first attempt to enhance the lowquality PSSM. By minimizing the MSE loss between the reconstructed and original PSSM, "Bagging" reconstructs high-quality MSA from down-sampled MSA with lowquality PSSM via an unsupervised method. Even though "Bagging" is the first work to achieve a relatively satisfactory performance, there are still some limitations. First, it exploits a fixed ratio for MSA down-sampling to obtain the low-quality PSSM, which makes the "Bagging" model less robust, especially for sequences with extremely low homology. Second, "Bagging" only conducts PSSM enhancement while ignoring the joint optimization of PSSM and the final PSSP.</p><p>Knowledge Distillation. Knowledge distillation transfers the knowledge from a pre-trained teacher network to a student network through training on the soft targets provided by the teacher network, which is originally proposed by <ref type="bibr" target="#b4">Buciluǎ, Caruana, and Niculescu-Mizil (2006)</ref> and later improved by <ref type="bibr" target="#b10">Hinton, Vinyals, and Dean (2015)</ref>. Over the past years, knowledge distillation has numerous applications <ref type="bibr" target="#b5">(Chen et al. 2017;</ref><ref type="bibr" target="#b36">Yim et al. 2017;</ref><ref type="bibr" target="#b37">Yu et al. 2017;</ref><ref type="bibr" target="#b25">Schmitt et al. 2018)</ref>. Inspired by these works, we propose the first method that exploits the knowledge distillation for PSSP. Same as the motivation for <ref type="bibr" target="#b15">Mirzadeh et al. (2019)</ref>, our approach aims to close the gap between teacher network and student network. However, instead of directly passing the knowledge from teacher network to student network, our enhancement module enhances low-quality PSSM to a high-quality one for student network learning via contrastive learning.</p><p>Contrastive Learning. Contrastive loss was introduced by Hadsell, <ref type="bibr" target="#b8">Chopra, and LeCun (2006)</ref> to learn representation by contrasting positive pairs against negative pairs. Recent work in computer vision <ref type="bibr" target="#b18">(Oord, Li, and Vinyals 2018;</ref><ref type="bibr">He et al. 2020;</ref><ref type="bibr" target="#b16">Misra and Maaten 2020;</ref><ref type="bibr" target="#b29">Tian, Krishnan, and Isola 2019;</ref><ref type="bibr" target="#b38">Zhuang, Zhai, and Yamins 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2020)</ref> presents promising results on unsupervised visual representation learning using approaches related to the contrastive loss. Inspired by the intuition and the results of contrastive learning, we are the first to import contrastive learning into PSSM enhancement. By contrasting high-quality PSSM to low-quality and the corresponding enhanced one, our model learns to generate enhanced PSSM closer to the high-quality PSSM distribution. We notice that Tian, Krishnan, and Isola (2019) also combines the contrastive learning with knowledge distillation, however, our motivations are quite different. While they try to bridge the gap between student and teacher network with contrastive learning, our method instead takes advantage of both methods to improve our EnhanceNet.</p><p>Protein sequence pre-training. Self-supervised learning is a powerful tool for extracting information from unlabeled sequences <ref type="bibr" target="#b7">(Devlin et al. 2018;</ref><ref type="bibr" target="#b20">Peters et al. 2018;</ref><ref type="bibr" target="#b21">Radford et al. 2019;</ref><ref type="bibr" target="#b35">Yang et al. 2019)</ref>. Like language, large unlabeled datasets of protein sequences are expected to contain significant biological information. Recent work in protein sequence pre-training has shown positive results on various downstream tasks including secondary structure prediction <ref type="bibr" target="#b0">(Alley et al. 2019;</ref><ref type="bibr" target="#b3">Bepler and Berger 2019;</ref><ref type="bibr" target="#b9">Heinzinger et al. 2019;</ref><ref type="bibr" target="#b22">Rao et al. 2019;</ref><ref type="bibr" target="#b24">Rives et al. 2019)</ref>. TAPE <ref type="bibr" target="#b22">(Rao et al. 2019)</ref> is the first work proposing systematical evaluation of the protein sequence pre-training model. They assessed the performance of pre-training on three common types of representation models, which are recurrent, convolutional, attention-based models. They also proposed a benchmark dataset for five downstream tasks including secondary structure prediction. We chose the attention-based BERT model based on its downstream performance reported from the TAPE paper. But, we trained our BERT on a larger database-UniRef90 <ref type="bibr" target="#b28">(Suzek et al. 2015)</ref>, since it is the common database choice of MSA search for PSSP. After the pre-training process, the model can then provide auxiliary information as PSSM for protein with no homology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Protein Secondary Structure Prediction (PSSP)</head><p>There are 20 common amino acids that function as the building blocks of a protein sequence. PSSP is a sequence-tosequence task where each amino acid x i in a protein sequence is mapped to a label y i ∈ {alpha-helix (H), betastrand (E), Coil (C)} for 3-state PSSP.</p><p>For PSSP, we adopt the most common choice called Position-Specific Scoring Matrix (PSSM). The PSSM indicates the substitution log-likelihood of all the 20 aminoacid types at each position, based on homologous sequences. PSSM of a protein sequence, denoted by X, is defined as X k,j = log( P k,j B k ), where P is the position probability matrix and B is the background frequency matrix. k is one kind of amino acids and j ∈ (1, .., L) with L denoting the length of the protein sequence. P is defined as</p><formula xml:id="formula_2">P k,j = C k,j +p N +20×p</formula><p>, where p is a scaler called pseudo-count to avoid zero-occurrence issue of some amino-acid types which we set as 1 in practice and N is the number of homologous sequences in the MSA. B is defined as</p><formula xml:id="formula_3">B k = L j=1 C k,j k L j=1 C k,j</formula><p>, which is the frequency of each amino acid occurs in the entire protein MSA. The above C k,j is the occurrence count of amino acid k in position j of an MSA M, which is defined as C k,j = N i=1 I(M i,j = k), where I is an indicator function taking value 1 if M i,j = k and 0 otherwise.</p><p>High-quality PSSM is critical for PSSP, thus the enhancement of low-quality PSSM is the key for high accuracy PSSP. To tackle this issue, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>, we propose a novel teacher-student framework PSSM-Distil with knowledge distillation (KD) and contrastive learning (CL). Details of each component will be disclosed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation for PSSM Enhancement</head><p>A teacher-student framework is exploited to achieve knowledge distillation (KD) on the PSSP task. Specifically, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>, we firstly train a teacher classifier F t with high-quality PSSM X h on PSSP task. Then we downsample the high-quality PSSM X h to obtain the low-quality PSSM X l . Note that here we conduct the down-sampling operation based on prior statistics instead of the fixed downsample ratio used in "Bagging" <ref type="bibr" target="#b8">(Guo et al. 2020</ref>) which details will be given in the experiment section. Based on the low-quality PSSM X l and one-hot encoding of protein sequence S, an EnhanceNet F e is trained to obtain the enhanced PSSM X e . Furthermore, the auxiliary information X b provided by the pre-trained BERT model can also flow into the EnhanceNet as additional input for a better performance, i.e., X e = F e (S, X b , X l ). Successively, the enhanced PSSM X e is fed to a student network F s with its protein sequence S to obtain its classification logits F s (S, X e ). Similarly, we obtain the classification logits F t (S, X h ) through the pre-trained teacher network for highquality PSSM. Finally, we define the KD loss L d as Eq.1.</p><formula xml:id="formula_4">L d = σLce + (1 − σ) L kl (1)</formula><p>where σ is a hyper-parameter weighting the two losses which we set as 0.1 in practice. L ce is the cross entropy loss between the student prediction distribution F s (S, X e ) and the PSSP label Y , i.e., L ce = CE (F s (S, X e ) , Y ). L kl is the Kullback-Leibler divergence between teacher's and student's prediction distribution as shown in Eq. 2.</p><formula xml:id="formula_5">L kl = KL marginal (Fs (S, Xe) , Ft (S, X h ))<label>(2)</label></formula><p>By minimizing L d , both the parameters of student network F s and EnhanceNet F e will be updated. Hence, we achieve better PSSM enhancement and adaptation at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning on PSSM Distribution</head><p>Inspired by the achievement of contrastive learning (CL) on self-supervised vision tasks, we exploit CL loss as additional supervision to optimize the EnhanceNet by further enhancing the low-quality PSSM in a high-level semantic space. Concretely, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>, once the teacher network F t is well-trained, we can use it as a feature extractor, which transforms the original input feature to a high-level semantic Then, the MSE loss L m is used to minimize the difference between the enhanced PSSM X e and the high-quality PSSM X h . Moreover, the high-quality PSSM X h which is the ground truth of the X e is sent into the teacher network F t to extract the classification logits F t (S, X h ) and X e is input to the student network F s to get F s (S, X e ). Then, knowledge distillation is applied between the fixed weights teacher network and the student network by KD loss presented in Eq.1. The high-quality PSSM X h , low-quality PSSM X l and the BERT pseudo-PSSM X b are also fed into the teacher network F t to obtain F t (S, X h ), F t (S, X l ) and F t (S, X b ) respectively. We regard X l and X b as the negative sample and the X h as the positive sample to optimize X e and applied a contrastive loss in Eq. 7 as the additional supervision for networks F e , F s . Finally, a joint loss in Eq. 9 is exploited to train our model in the end-to-end manner. In inference, only a blue arrow path is used. The Algorithm.1 illustrates more details of the above training process.</p><p>space. For CL loss, we regard the high-quality PSSMs X h as positive samples, the low-quality PSSMs X l and BERT Pseudo-PSSM X b as negative samples and the enhanced PSSM X e as enhanced samples. Then we use a conventional contrastive learning inequality defined as Eq. 3 to characterize the shifting of the mapping F X in semantic space for the enhanced sample X e . In particular, the F X will move closer to the mapping F P of positive sample X h and away from the mapping F N of negative samples X l , X b .</p><formula xml:id="formula_6">KL marginal (FX , FP ) ≤ KL marginal (FX , FN )<label>(3)</label></formula><p>More specifically, F X is the mapping in semantic space of the enhanced PSSM X e defined as Eq. 4.</p><formula xml:id="formula_7">Xe = Fe(S, X b , X l ) FX = Ft(S, Xe)<label>(4)</label></formula><p>Similarly, the F P and F N are the mappings in semantic space from positive and negative samples respectively. Here, we choose F P , denoted by Eq. 5, as the classification logits for high-quality PSSM X h extracted from the pre-trained teacher network F t .</p><formula xml:id="formula_8">FP = Ft(S, X h )<label>(5)</label></formula><p>We denote F N as the combination of the classification logits from the teacher network for low-quality PSSM F t (S, X l ) and BERT Pseudo PSSM F t (S, X b ). We defined F N as Eq. 6</p><formula xml:id="formula_9">FN = 1 2 (Ft(S, X l ) + Ft(S, X b ))<label>(6)</label></formula><p>Finally, we adopt a triplet loss to model the inequality in Eq.3 which can be illustrated as Eq. 7, where η is a hyperparameter and we set it equal to 0.6 in practice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Additionally, same as in previous work <ref type="bibr" target="#b8">(Guo et al. 2020)</ref>, we use the mean square error (MSE) loss to directly minimize the difference between the enhanced PSSM X e and the highquality PSSM X h referred in Eq. 8.</p><formula xml:id="formula_10">Lm = Xe − X h 2 (8)</formula><p>By combining with the aforementioned KD and CL loss, the overall loss function of our framework is shown in Eq. 9 where α, β, γ are the weighting hyper-parameters. In practice, we set α = 0.16, β = 0.016, and γ = 0.82.</p><formula xml:id="formula_11">L = αL d + βLm + γLt (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Pseudo PSSM Generation</head><p>To supply auxiliary information for better enhancement of low-quality PSSM, as shown in Fig. <ref type="figure" target="#fig_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>We train the PSSM-Distil framework on the training set of CullPDB <ref type="bibr" target="#b30">(Wang and Dunbrack Jr 2003)</ref>. CullPDB validation set, CB513 <ref type="bibr" target="#b13">(Kryshtafovych et al. 2014)</ref> and a new dataset BC40 constructed by ourselves are used to evaluate the performance of our method and conduct comparisons with previous methods. For the CullPDB dataset, any two proteins share less than 25% sequence identity. Following the same procedure as in <ref type="bibr" target="#b38">(Zhou and Troyanskaya 2014)</ref>, we divide the CullPDB dataset into a training set and validation set with no more than 25% of the training set shared with CB513 and BC40. We conduct an MSA search for all training, validation and test protein sequences from the Uniref90 database <ref type="bibr" target="#b28">(Suzek et al. 2015)</ref>. The protein sequence labels for all training, validation and test proteins are generated by DSSP <ref type="bibr" target="#b12">(Kabsch and Sander 1983)</ref>. The dataset details are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>BC40 Dataset<ref type="foot" target="#foot_0">1</ref> To further validate our proposed approach on real-world PSSP applications, we construct the BC40 dataset (release date is 2020-07-28) in which each entry is publicly available from PDB. Specifically, PDB will cluster all protein chains by MMseq2 (Steinegger and Söding 2017) at 30%, 40%, . . . , 90%, 95%, and 100% sequence identity each week to remove redundancy, and BC40 is the dataset with 40% cutoff such that the proteins share no more than 40% sequence identity. Additionally, we also remove the proteins that share more than 25% sequence identity with our CullPDB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>Like other sequence labeling models in NLP, our teacher network F t and student network F s share a similar design which consists of BiLSTM and linear fully-connected layers, while EnhanceNet F e has additional 1-dimensional convolution layers. More specifically, an embedding layer with dimension 32 in EnhanceNet F e is used to map the original protein sequence to a higher dimensional semantic space with dimension (L×32). Then, embedding features (L×32) is concatenated with the low-quality PSSM X l (L × 20) and BERT Pseudo PSSM X b (L × 20) as an L × 72 dimensional input for the latter part, where L is the sequence length. As shown in the blue part of Fig. <ref type="figure" target="#fig_2">2</ref>, the principal part of F e consists of two branches: BiLSTM and 1D-CNN branches which extract features independently from previous L × 72 input. In the BiLSTM branch, there is a BiLSTM model that contains two hidden layers and each layer has 400 hidden units to extract local features at the token level and global features at the sequence level. In the 1D-CNN branch, three 1D-CNN layers are used to extract local features for each token position along the 72-dimension of input and the hidden number for each CNN is 300. Finally, we concatenate the output of the two branches and use two linear fullyconnected layers to regress out the final enhanced PSSM X e which has the same shape as the low-quality PSSM X l input. The teacher and student networks F s , F t are two PSSP classifiers. Each of them consists of a 2-layer BiLSTM to extract features and 2 linear FC layers with a final softmax layer for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use PyTorch <ref type="bibr" target="#b19">(Paszke et al. 2017</ref>) to implement our work. Three networks F e , F s , F t are trained in an end-to-end manner. Particularly, as depicted in Algorithm 1, we first train the teacher network F t by using high-quality PSSM X h , which learns how to use a good PSSM to predict SS. Then, En-hanceNet F e and F t are jointly optimized by loss function L in Eq. 9. We use Adam optimizer with an initial learning rate that equals 0.01 and conducts learning rate decay every 50 epochs. We employ a drop out layer before the softmax layer in each network with the dropout set to 0.75. Specifically, we train our models on one Tesla V100 GPU. Greed search is utilized for hyper-parameter tuning. Source code<ref type="foot" target="#foot_1">2</ref> </p><p>for our inference phase with pretainined models have been released for demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Distribution based PSSM Down-sampling</head><p>As illustrated by the yellow arrow in Fig. <ref type="figure" target="#fig_2">2</ref>, we down-sample the high-quality PSSM X h to obtain low-quality X l , which is one input for the subsequent module. Since EnhanceNet F e is employed to enhance low-quality PSSMs, in the training phase, the down-sampled low-quality PSSM X l should exist no domain gap to the natural low-quality PSSM. Thus, different from Bagging using a fixed down-sampling ratio, we exploit the prior native distribution based PSSM downsampling strategy. In practice, we first calculate the MSA count distribution X based on native sequences with lowquality PSSM in the training set, i.e., calculate the frequency of MSA count when the MSA count less than 60 <ref type="bibr" target="#b8">(Guo et al. 2020)</ref>. Once the prior distribution of native low-quality X has been achieved, in the training phase, we randomly select a batch of MSAs from original MSAs with a count number which is sampled from distribution X . Benefiting from the low-quality PSSMs X l achieved through domain aligned down-sampling, our EnhanceNet can output more realistic high quality X e , leading to superior and robust performance than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We evaluate our PSSM-Distil framework on low-quality PSSM protein sequences from three public datasets: CullPDB, CB513 and BC40. The comparison experiment with the previous state-of-the-art models confirms the supreme priority of our approach. Particularly, our method is effective on all levels of difficulties with 7%-15% of improvements over the vanilla model denoted as "Real" and surpasses the previous state-of-the-art method "Bagging" <ref type="bibr" target="#b8">(Guo et al. 2020</ref>) by 6% on average and nearly 8% in the extreme low-quality cases. The accuracy is computed on a per-protein basis. Moreover, the performance gained from each component of our model is well examined by the ablation study.</p><p>Comparison Experiment. We compare the PSSP results of PSSM-Distil, the previous state-of-the-art model "Bagging" and the vanilla PSSP model "Real". "Real" is trained on the protein sequences with low-quality PSSMs from CullPDB without any enhancement. To give a more detailed comparison, we split the protein sequences with low-quality PSSM into several divisions of MSA count and MSA meff according to <ref type="bibr" target="#b8">Guo et al. (2020)</ref>. Shown in Table <ref type="table">2</ref> and Table 3, our approach achieves the best performance on protein sequences with low-quality PSSMs under regardless of low MSA count score or low MSA meff score settings. Furthermore, for the extreme low-quality cases, i.e., MSA count equals to 0 or the meff score is less than 5, our method still gains relatively satisfactory results against previous approaches. In particular, in the case of MSA Count equals to 0, our method still has 73.7% accuracy with 7.6% improvement over the previous best method "Bagging" on the BC40 test set, which proves our method is robust and effective under the condition that no MSA is available. Note that for  <ref type="table">2</ref>, due to too few sequences existing for lower threshold levels such as ≤ 30, = 0 to make it representative.</p><p>To specify the effect of MSA count and Meff score on PSSP accuracy in more detail, we also conduct the quantitative comparison between our method and "Bagging" on PSSP accuracy improvement against the vanilla "Real" model over different MSA count and Meff score ranges, which is shown in Fig. <ref type="figure" target="#fig_4">3</ref>. It is worth noting that our PSSM-Distil is increasingly more effective as PSSM quality decreases, demonstrating that our method successfully targets the lower quality cases and outperforms the previous best method "Bagging" by a large margin in those extreme lowquality PSSMs. Particularly, on bin [0, 5] for MSA count and [0, 1] for MSA Meff, our improvements have 8% more than "Bagging". Moreover, our method achieves improvement on all score ranges for both MSA count and MSA meff, while "Bagging" only improves on low MSA count and low MSA meff bins and even worsen the performance for some high MSA count and high MSA meff ranges, which they pointed out as the side-effect of their method in their paper, which may result from the fixed down-sampling strategy.  Ablation Study. We conduct the ablation study to demonstrate the effectiveness of each designed component for the PSSM-Distil framework and evaluate the performance for low-quality PSSMs on the BC40 dataset with the same four divisions of MSA count. In particular, we remove the auxiliary input X b from pre-trained BERT, contrastive learning loss L t and MSE loss L m respectively to show the ablation results on different MSA count ranges in Table <ref type="table" target="#tab_2">4</ref>.</p><p>As shown in Table <ref type="table" target="#tab_2">4</ref>, the results from the "w/o BERT" column entail that the extremely low-quality case suffers the most with 3% of degeneration, which demonstrates the effectiveness of the pre-trained BERT model on these extreme cases. The "w/o MSE" column breaks the direct link of our model to the previous state-of-the-art "Bagging" models, which shows the smallest decreases among the ablation of all the components. These results demonstrated the superior framework choice of PSSM-Distil.</p><p>PSSM Visualization. The visualization results of enhanced PSSMs are given in Fig. <ref type="figure" target="#fig_5">4</ref>, which illustrates the ca- pacity of different amino acids on each residue. The graphs are generated using WebLogo<ref type="foot" target="#foot_2">3</ref> . We can easily observe that the original low-quality PSSM (top row) is very sparse and contains less information comparing with high-quality PSSM (bottom row). However, our enhanced PSSM (middle row) is informative and shares similar patterns with highquality PSSM, which exactly proves the effectiveness of PSSM-Distil on PSSM enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we present the PSSM-Distil, a PSSM enhancement method with knowledge distillation and contrastive learning to tackle the problem of PSSP on lowquality PSSM. We first achieve a discriminative classifier as the teacher network for PSSP by using high-quality PSSMs in the training dataset. Next, we jointly train the En-hanceNet and a student network for PSSM enhancement and PSSP by using the low-quality PSSMs, which are downsampled from high-quality PSSMs. Regardless of the sophisticated architecture design, the novel loss function is elaborated with knowledge distillation, contrastive loss and mean square error loss to jointly optimize the EnhanceNet and the student network for the generation of high-quality PSSMs and accurate PSSP. Additionally, we explicitly utilize the BERT pseudo PSSM for extreme low-quality cases' enhancement, i.e., protein sequences with no homology at all. Our work opens up the possibility for research on newly discovered or unknown protein structure prediction with low-quality PSSMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As mentioned in "Introduction", computer-assisted protein structure prediction is highly time-efficient and economic. Therefore, computer-assisted protein structure prediction is promising for structural biology, which opens up tremendous opportunities for protein function understanding and drug design. However, the current challenge mainly lies in its prediction precision, especially for newly discovered or unknown proteins with no homology. Our work sits at the core of tackling this issue by enhancing PSSM quality to improve the prediction precision of the aforementioned proteins, i.e., with limited or no sequence homology. We not only demonstrate the effectiveness of our PSSM-Distil model by fulfilling significant improvements on PSSP for these hard cases but also confirm that our method can be considered as a general framework for joint optimization of the PSSM enhancement and downstream tasks. The most closely-related task to PSSP is the contact prediction, which requires co-evolutionary information extracted from highquality MSAs. Thus, our work is of great significance for applications such as protein property analysis, contact prediction and protein folding, and serves as the fundamental research for the whole computational biology community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed PSSM-Distil for protein secondary structure prediciton (PSSP) on low-quality PSSM. PSSM-Distil uses a teacher-student network to conduct the knowledge distillation (KD) and contrastive learning (CL) from highquality PSSM, thus leading to the final improved PSSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall pipeline of our framework: a teacher-student model for knowledge distillation and contrastive learning with an EnhanceNet F e for low-quality PSSM enhancement. First, the pure sequence S is concatenated with the pre-trained BERT pseudo-PSSM X b and the low-quality PSSM X l as the input to the EnhanceNet F e to predict an enhanced PSSM X e . Then, the MSE loss L m is used to minimize the difference between the enhanced PSSM X e and the high-quality PSSM X h . Moreover, the high-quality PSSM X h which is the ground truth of the X e is sent into the teacher network F t to extract the classification logits F t (S, X h ) and X e is input to the student network F s to get F s (S, X e ). Then, knowledge distillation is applied between the fixed weights teacher network and the student network by KD loss presented in Eq.1. The high-quality PSSM X h , low-quality PSSM X l and the BERT pseudo-PSSM X b are also fed into the teacher network F t to obtain F t (S, X h ), F t (S, X l ) and F t (S, X b ) respectively. We regard X l and X b as the negative sample and the X h as the positive sample to optimize X e and applied a contrastive loss in Eq. 7 as the additional supervision for networks F e , F s . Finally, a joint loss in Eq. 9 is exploited to train our model in the end-to-end manner. In inference, only a blue arrow path is used. The Algorithm.1 illustrates more details of the above training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lt</head><label></label><figDesc>= max 0, KL(FX , FP ) − KL(FX , FN ) + η = max 0, KL (Ft (S, Xe) , Ft (S, X h )) − KL (Ft (S, Xe) , FN ) + η (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PSSM-Distil PSSP accuracy improvement comparison with "Bagging" over vanilla "Real" model on different count and meff score range. The blue bars are the improvement results of PSSM-Distil, whereas the orange bars are the improvement results of "Bagging". PSSM-Distil significantly improves over "Real" on both low-quality and high-quality cases, while "Bagging" only improves the lowquality cases with the cost of damaging the high-quality prediction accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of the MSA quality by sampling it from low-quality (top row), enhanced (middle row), and high-quality (bottom row) PSSM. Our enhanced PSSM demonstrates higher fidelity reconstruction of the highquality PSSM with more complex MSA patterns, while lowquality PSSM shows only simple and repetitive patterns.</figDesc><graphic url="image-24.png" coords="7,334.38,177.68,223.69,61.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of dataset used in our experiments, including dataset names, types and number of proteins sequence.</figDesc><table><row><cell cols="3">Dataset CullPDB CullPDB</cell><cell cols="2">CB513 BC40</cell></row><row><cell>Type</cell><cell cols="4">Training Validation Testing Testing</cell></row><row><cell>Size</cell><cell>5600</cell><cell>525</cell><cell>514</cell><cell>36976</cell></row><row><cell cols="5">X b , we take advantage of BERT training objective. Since</cell></row><row><cell cols="5">BERT is a masked language model and uses neighboring</cell></row><row><cell cols="5">contexts to recover the masked token, we mask each posi-</cell></row><row><cell cols="5">tion of a protein sequence one at a time to obtain the pre-</cell></row><row><cell cols="5">dicted probability vector of 20 amino acids for the masked</cell></row><row><cell cols="5">position. By repeating the above procedure, we could obtain</cell></row><row><cell cols="5">the BERT sequence probability map for a specific protein</cell></row><row><cell cols="5">sequence. Then we sample 2000 pseudo protein sequences</cell></row><row><cell cols="5">as MSA from the probability map to generate BERT Pseudo</cell></row><row><cell>PSSM X b .</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Algorithm 1: PSSM-Distil for PSSP</cell><cell></cell><cell></cell></row></table><note>, BERT Pseudo PSSM X b derived from pre-trained BERT is concatenated with low-quality PSSM X l and fed to EnhanceNet F e for enhancement. For the generation of the BERT Pseudo PSSM Input: Protein Sequence S; High-quality PSSM X h ; BERT Pseudo PSSM X b ; Low-quality PSSM X l ; Student Fs; Teacher Ft; EnhanceNet Fe; Label Y ; 1 // Training Phase 2 Ft ← P retrain T eacher N etwork Ft by X h ; 3 X l ← Downsample X h ; 4 Xe ← Fe(S, X l , X b ); 5 // Mappings of Enhanced, Positive and Negative Samples 6 FX ← Ft(S, Xe); 7 FP ← Ft(S, X h ); 8 FN ← 1 2 (Ft(S, X l ) + Ft(S, X b )) ; 9 // Using L d , Lm and Lt to Optimize Fs, Fe 10 Fs, Fe ← M inimize αL d + βLm + γLt; 11 // Inference Phase 12 Xe ← Fe(S, X l , X b ); 13 PSSP ← Argmax(Fs(S, Xe)); Output: Parameters of Ft, Fs, Fe</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on the BC40 dataset of our method. The "Our" column is the full pack result of the our method. "w/o BERT" is the result without BERT Pseudo PSSM X b . "w/o CL" is the result without the triplet loss L t from constrastive learning. "w/o MSE" is the result without MSE loss L m between X e and X h . The obvious degeneration from ablating each component from our method implies the important role of these components for our method.</figDesc><table><row><cell>≤ 60</cell><cell>0.778</cell><cell>0.768</cell><cell>0.772</cell><cell>0.776</cell></row><row><cell>≤ 30</cell><cell>0.766</cell><cell>0.754</cell><cell>0.757</cell><cell>0.760</cell></row><row><cell>≤ 10</cell><cell>0.759</cell><cell>0.740</cell><cell>0.743</cell><cell>0.748</cell></row><row><cell>= 0</cell><cell>0.737</cell><cell>0.707</cell><cell>0.714</cell><cell>0.725</cell></row></table><note>MSA Counts Our w/o BERT w/o CL w/o MSE</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://drive.google.com/file/d/1e0gfBDLWp--5txWlOGr0Pju1Wd12cg9D/view?usp=sharing</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">2 https://github.com/qinwang-ai/PSSM-Distil</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://weblogo.berkeley.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported in part by the Key Area RD Program of Guangdong Province with grant No. 2018B030338001, by the National Key RD Program of China with grant No. 2018YFB1800800, by NSFC-Youth 61902335, by Guangdong Regional Joint Fund-Key Projects 2019B1515120039, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No. 2017ZT07X152 and by CCF-Tencent Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Schäffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="j">Profile hidden Markov models. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="755" to="763" />
			<date type="published" when="1998">2018. 1998</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging MSA Learning: Enhancing Low-Quality PSSM with Deep Learning for Accurate Protein Structure Property Prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
				<imprint>
			<date type="published" when="2006">2020. 2006. 2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modeling the Language of Life-Deep Learning Protein Sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">614313</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hidden Markov model speed heuristic and iterative HMM search procedure</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">431</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers: Original Research on Biomolecules</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tramontano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Assessment of the assessment: evaluation of the model quality estimates in CASP10</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="112" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction using cascaded convolutional and recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mandell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kortemme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07176</idno>
	</analytic>
	<monogr>
		<title level="j">Nature chemical biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="797" to="807" />
			<date type="published" when="2009">2016. 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Computer-aided design of functional protein interactions</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03393</idno>
		<title level="m">Improved Knowledge Distillation via Teacher Assistant</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Protein kinase inhibitors: insights into drug design from structure</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Endicott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">5665</biblScope>
			<biblScope unit="page" from="1800" to="1805" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with TAPE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9686" to="9698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>bioRxiv 622803</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zidek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03835</idno>
		<title level="m">Kickstarting deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Protein secondary structure prediction with long short term memory networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7828</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PISCES: a protein sequence culling server</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the complexity of multiple sequence alignment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="348" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">De novo protein structure determination from near-atomicresolution cryo-EM maps</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>.-R.; Kudryashev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Egelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Basler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction using deep convolutional neural fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Protein structure determination in solution by nuclear magnetic resonance spectroscopy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wuthrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="issue">4887</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1974" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep supervised and convolutional generative stochastic network for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1347</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Local aggregation for unsupervised learning of visual embeddings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
