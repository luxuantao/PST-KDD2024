<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast Radial Symmetry Transform for Detecting Points of Interest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gareth</forename><surname>Loy</surname></persName>
							<email>gareth@syseng.anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>0200</postCode>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Zelinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>0200</postCode>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fast Radial Symmetry Transform for Detecting Points of Interest</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">958C5C02E9005A4B44DEF24FFC39324E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new feature detection technique is presented that utilises local radial symmetry to identify regions of interest within a scene. This transform is significantly faster than existing techniques using radial symmetry and offers the possibility of real-time implementation on a standard processor. The new transform is shown to perform well on a wide variety of images and its performance is tested against leading techniques from the literature. Both as a facial feature detector and as a generic region of interest detector the new transform is seen to offer equal or superior performance to contemporary techniques whilst requiring drastically less computational effort.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic detection of points of interest in images is an important topic in computer vision. Point of interest detectors can be used to selectively process images by concentrating effort at key locations in the image, they can identify salient features and compare the prominence of such features, and real-time interest detectors can provide attentional mechanisms for active vision systems <ref type="bibr" target="#b9">[11]</ref>.</p><p>In this paper a new point of interest operator is presented. It is a simple and fast gradient-based interest operator which detects points of high radial symmetry. The approach was inspired by the results of the generalised symmetry transform <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b7">9]</ref>, although the method bares more similarity to the work of Sela and Levine <ref type="bibr" target="#b8">[10]</ref> and the circular Hough transform <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7]</ref>. The approach presented herein determines the contribution each pixel makes to the symmetry of pixels around it, rather than considering the contribution of a local neighbourhood to a central pixel. Unlike previous techniques that have used this approach <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b8">10]</ref> it does not require the gradient to be quantised into angular bins, the contribution of every orientation is computed in a single pass over the image. The new method works well with a general fixed parameter set, however, it can also be tuned to exclusively detect particular kinds of features. Computationally the algorithm is very efficient, being of order O(KN ) when considering local radial symmetry in N × N neighbourhoods across an image of K pixels.</p><p>Section 2 of this paper defines the new radial symmetry transform. Section 3 discusses the application of the transform, including selection of parameters and some additional refinements. Section 4 shows the performance of the new transform on a variety of images, and compares it to existing techniques, and Section 5 presents the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition of the Transform</head><p>The new transform is calculated over a set of one or more ranges N depending on the scale of the features one is trying to detect. The value of the transform at range n ∈ N indicates the contribution to radial symmetry of the gradients a distance n away from each point. Whilst the transform can be calculated for a continuous set of ranges this is generally unnecessary as a small subset of ranges is normally sufficient to obtain a representative result.</p><p>At each range n an orientation projection image O n and a magnitude projection image M n are formed. These images are generated by examining the gradient g at each point p from which a corresponding positively-affected pixel p +ve (p) and negatively-affected pixel p -ve (p) are determined, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The positively-affected pixel is defined as the pixel that the gradient vector g(p) is pointing to, a distance n away from p, and the negatively-affected pixel is the pixel a distance n away that the gradient is pointing directly away from. where 'round' rounds each vector element to the nearest integer. The orientation and projection images are initially zero. For each pair of affected pixels the corresponding point p +ve in the orientation projection image O n and magnitude projection image M n is incremented by 1 and g(p) respectively, while the point corresponding to p -ve is decremented by these same quantities in each image. That is</p><formula xml:id="formula_0">O n (p +ve (p)) = O n (p +ve (p)) + 1 O n (p -ve (p)) = O n (p -ve (p)) -1 M n (p +ve (p)) = M n (p +ve (p)) + ||g(p)|| M n (p -ve (p)) = M n (p -ve (p)) -||g(p)||</formula><p>The radial symmetry contribution at a range n is defined as the convolution</p><formula xml:id="formula_1">S n = F n * A n (1)</formula><p>where</p><formula xml:id="formula_2">F n (p) = || Õn (p)|| (α) Mn (p),<label>(2)</label></formula><formula xml:id="formula_3">Õn (p) = O n max p {||O n (p)||} , Mn (p) = M n max p {||M n (p)||} ,</formula><p>α is the radial strictness parameter, and A n is a two-dimensional Gaussian. These parameters are discussed in more detail in Section 3.</p><p>The full transform is defined as the sum of the symmetry contributions over all the ranges considered,</p><formula xml:id="formula_4">S = n∈N S n (3)</formula><p>If the gradient is calculated so it points from dark to light then the output image S will have positive values corresponding to bright radially symmetric regions and negative values indicating dark symmetric regions (see Figure <ref type="figure" target="#fig_1">2</ref> for example). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applying the Transform</head><p>In order to apply the transform there are a number of parameters that must first be defined, namely, the a set of ranges N = {n 1 , n 2 , ...} at which to calculate S n , the Gaussian kernels A n , and the radial strictness parameter α. Some additional refinements are also considered, including ignoring small gradient elements, and only searching for dark or light radially symmetric regions.</p><p>The traditional approach to local symmetry detection <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b8">10]</ref> is to calculate the symmetry apparent in a local neighbourhood about each point. This can be achieved by calculating S n for a continuous set of ranges N = {1, 2, ..., n max } and combining using equation 3. However, since the symmetry contribution is calculated independently for each range n it is simple to determine the result at a single range, or an arbitrary selection of ranges that need not be continuous. Furthermore, the results obtained by examining a representative subset of ranges give a good approximation of the output obtained by examining a continuous selection of ranges, while saving on computation.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the combined output S calculated for a continuous range of n from 1 to 5 (b) is closely approximated by combining only n = 1, 3 and 5 (c). Also, if the scale of a radially symmetric feature is know a priori then the feature can be efficiently detected by only determining the transform at the appropriate range, this is demonstrated by the effective highlighting of the eyes (that have radius 5 pixels) by S 5 in Figure <ref type="figure" target="#fig_1">2 (d)</ref>.</p><p>The purpose of the Gaussian kernel A n is to spread the influence of the positively-and negatively-affected pixels as a function of the range n. A twodimensional Gaussian is chosen because it is radially symmetric so it will have a consistent effect over all gradient orientations, and it is separable so its convolution can be efficiently determined. Figure <ref type="figure" target="#fig_2">3</ref> shows the contribution for a single gradient element g(p). By scaling the standard deviation linearly with the range n, we define an arc of influence that applies to all affected pixels. The width of the arc is defined by scaling the standard deviation of A n with respect to n.</p><p>The parameter α determines how strictly radial the radial symmetry must be for the transform to return a high interest value. Figure <ref type="figure" target="#fig_3">4</ref> shows the effect of choosing α to be 1, 2 and 3 on S 1 for an image exhibiting strong radial values around the eyes. Note how a higher α eliminates non-radially symmetric features such as lines. A choice of α = 2 is suitable for most applications. Choosing a higher α starts attenuating points of interest, whilst a lower α gives too much emphasis to non-radially symmetric features, however, choosing α as 1 minimises the computation when determining F n in Equation <ref type="formula" target="#formula_2">2</ref>. Gradient elements with small magnitudes have less reliable orientations, are more easily corrupted by noise, and tend to correspond to features that are not immediately apparent to the human eye. Since the purpose of the transform is to pick out points of interest in the image it is logical to ignore such elements in our calculation. A gradient threshold parameter β is introduced for this purpose, and when calculating images O n and M n all gradient elements whose magnitudes are below β are ignored. The effect of a non-zero β is shown in Figure <ref type="figure" target="#fig_4">5</ref>. The main advantage of a non-zero β is an increase in the speed of the algorithm, since there are less gradient elements considered, and hence less affected pixels to be calculated.</p><formula xml:id="formula_5">Input image α = 1 α = 2 α = 3</formula><p>The transform can be tuned to look only for dark or bright regions of symmetry. To look exclusively for dark regions, only the negatively-affected pixels need be considered when determining M n and O n (see Section 2). Likewise, to detect bright symmetry only positive affected pixels need be considered. Examples of dark symmetry are shown in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance of the Transform</head><p>The performance of the new transform was demonstrated on a range of images and compared with several prominent transforms from the literature. Figure <ref type="figure" target="#fig_6">6</ref> demonstrates the performance of the new transform on faces and other images. These figures were generated using the parameter settings presented in Table <ref type="table" target="#tab_0">1</ref>, and show how the transform can provide a useful cue for the location of facial features -especially eyes -in face images, as well as highlighting generic points of interest that are characterised by high contrast and radial symmetry.  All methods were implemented with a local neighbourhood radius of 6 pixels, and where necessary the gradient orientation was quantised into 8 bins.</p><p>Each of the transforms was implemented in Matlab. For the majority of the transforms an estimate of the approximate number of floating point operations involved was obtained from Matlab, however, for Di Gesù et al.'s discrete symmetry transform and Sela and Levine's real-time attention mechanism this was not feasible. These transforms involve optimised low-level processes that were not practical to implement in Matlab, so the number of operations required is not reported here. It suffices to say that the non-optimised implementations used to generate the visual results shown required computation well in excess of the other methods. The estimated computations obtained are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>The new transform effectively highlights the points of interest (eyes) in Figure <ref type="figure" target="#fig_5">7</ref>. Of the existing transforms Reisfeld's generalised (dark and radial) symmetry provide the next best result, and while the other transforms do highlight the eye regions they tend to highlight many other points as well reducing their overall effectiveness.  <ref type="bibr" target="#b7">[9]</ref> 179 Circular Hough <ref type="bibr" target="#b5">[7]</ref> 33.9</p><p>Table <ref type="table" target="#tab_2">3</ref> lists the theoretical order of computation required to compute the transforms on an image of K pixels, where local symmetry is considered in an N × N neighbourhood, and for those methods that require gradient quantisation the gradient is quantized into B bins. The complexity O(KN ) of the new transform is lower than all other transforms considered, with the possible exception of Di Gesu et al.'s Discrete Symmetry Transform that has complexity O(KB). However, as was discussed in Section 3, it is not necessary to calculate the new transform at all ranges 1..N , so the computational order can be further reduced, whereas it is essential to calculate Di Gesu et al.'s Discrete Symmetry Transform across four or more angular bins. Furthermore the results from the Discrete Symmetry Transform do not appear as effective for locating points of interest (see Figure <ref type="figure" target="#fig_5">7</ref>).  The key to the speed of the new transform lies in the use of affected pixels to project the effect of gradient elements. This allows an approximation of the effect of each gradient element on the radial symmetry of the pixels around it, without specifically considering neighbourhoods about each point, as did <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b6">8]</ref>, or requiring multiple calculations for different gradient orientations, as did <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b8">10]</ref>. Unlike other transforms the fast symmetry transform differentiates between dark and bright regions of radial symmetry, while allowing both to be computed simultaneously. Alternatively just dark (or bright) points of symmetry can be considered exclusively with an associated reduction in computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>A novel point of interest detector has been presented that uses the gradient of an image to locate points of high radial symmetry. The method has been demonstrated on a series of face images and other scenes, and compared against a number of contemporary techniques from the literature. As a point of interest operator the new transform provides equal or superior performance on the images tested while offering significant savings in both the computation required and the complexity of the implementation. The efficiency of this transform makes it well suited to real-time vision applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The locations of pixels p+ve(p) and p-ve(p) affected by the gradient element g(p) for a range of n = 2. The dotted circle shows all the pixels which can be affected by the gradient at p for a range n.</figDesc><graphic coords="2,159.44,305.35,114.72,116.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Varying the set of ranges N .</figDesc><graphic coords="4,151.27,47.61,59.40,84.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The contribution of a single gradient element, with An chosen to be a 2D Gaussian of size n × n and standard deviation σ = 0.25n, and n = 10.</figDesc><graphic coords="5,175.76,47.83,85.16,85.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Effect of varying α. Original image from USC-SIPI Image Database [1]</figDesc><graphic coords="5,58.96,273.06,76.80,76.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The effect of different values of β on S. Here β is measured as a percentage of the maximum possible gradient magnitude. Original image from Database of Faces, AT&amp;T Laboratories Cambridge [2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7</head><label>7</label><figDesc>Figure 7 compares the performance of the transform against existing techniques from the literature. Each transform is applied to the image in the centre of the figure (the standard 256 × 256 lena image) for which the intuitive points of interest are the eyes.All methods were implemented with a local neighbourhood radius of 6 pixels, and where necessary the gradient orientation was quantised into 8 bins.Each of the transforms was implemented in Matlab. For the majority of the transforms an estimate of the approximate number of floating point operations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The new transform applied to a variety of images. The parameter settings are indicated beneath each column and refer to the values detailed in Table1. The two top most images are from the Database of Faces<ref type="bibr" target="#b0">[2]</ref>.</figDesc><graphic coords="7,48.28,430.94,82.33,61.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 and</head><label>7</label><figDesc>Figure7and Tables2 and 3demonstrate that the new transform can provide comparable or superior results to existing techniques whilst requiring significantly less computation and complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of new transform (top row) with other available transforms. In order to compare the output of Sela &amp; Levine's real-time attention mechanism with the other transforms, the final step, which involved identifying local maximums in the output as points of interest, has been omitted.</figDesc><graphic coords="9,48.13,302.82,102.60,102.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameter Settings used for Experimentation</figDesc><table><row><cell>Parameter</cell><cell></cell><cell>Setting</cell><cell></cell></row><row><cell></cell><cell>Full</cell><cell>Fast</cell><cell>Fast Dark</cell></row><row><cell>Set of ranges N</cell><cell></cell><cell></cell><cell>1, 3, 5}</cell></row><row><cell>Gaussian kernel</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Volume under kernel</cell><cell>n 2</cell><cell>n 2</cell><cell>n 2</cell></row><row><cell>Size</cell><cell>n</cell><cell>n</cell><cell>n</cell></row><row><cell>Standard deviation</cell><cell>0.5n</cell><cell>0.5n</cell><cell>0.5n</cell></row><row><cell>Radial strictness α</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Small gradients ignored</cell><cell>0</cell><cell>20% ignored</cell><cell>20% ignored</cell></row><row><cell>Dark symmetry</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Bright symmetry</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell></row></table><note><p>{n : n = 1, 2, ..., 6} {n : n = 1, 3, 5} {n : n =</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Estimated Computation Required for Different Transforms to compute the results in Figure7</figDesc><table><row><cell>Transform</cell><cell>Computations</cell></row><row><cell></cell><cell>(Mflop)</cell></row><row><cell>New Transform</cell><cell></cell></row><row><cell>Full</cell><cell>19.7</cell></row><row><cell>Fast</cell><cell>7.93</cell></row><row><cell>Fast Dark</cell><cell>7.02</cell></row><row><cell>Existing Transforms</cell><cell></cell></row><row><cell>Generalised Symmetry</cell><cell></cell></row><row><cell>Radial [8]</cell><cell>259</cell></row><row><cell>Dark</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Computational Order of Different Transforms Transform Order New Radial Symmetry Transform KN Reisfeld's Generalised Symmetry Transform [8] KN 2 Lin and Lin's Gradient-based Inhibitory Mechanism [6] KN 2 Di Gesu et al.'s Discrete Symmetry Transform [3] KB Sela and Levine's Real-Time Attentional Mechanism [10] KBN Circular Hough Transform [7] KBN</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.cam-orl.co.uk/facedatabase.html" />
		<title level="m">Database of Faces. AT&amp;T Laboratories Cambridge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The discrete symmetry transform in computer vision</title>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Gesù</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Valenti</surname></persName>
		</author>
		<idno>DMA 011 95</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Palermo University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extraction of facial features for recognition using neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Intrator</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding circles by an array of accumulators</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kimme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="122" />
			<date type="published" when="1975-02">February 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting facial features by an inhibitory mechanism based on gradient distributions</title>
		<author>
			<persName><forename type="first">Cheng-Chung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2079" to="2101" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detection and segmetaion of blobs in infrared images</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Minor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sklansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tranactions on Systems Man and Cyberneteics</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="1981-03">March 1981</date>
		</imprint>
	</monogr>
	<note>SMC</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context free attentional operators: the generalized symmetry transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision, Special Issue on Qualitative Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Preprocessing of face images: Detection of features and pose normalisation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="430" />
			<date type="published" when="1998-09">September 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time attention for robotic vision</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="173" to="194" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advancing active vision systems by improved design and control</title>
		<author>
			<persName><forename type="first">Orson</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harley</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Rougeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Experimental Robotics (ISER2000)</title>
		<meeting>International Symposium on Experimental Robotics (ISER2000)</meeting>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
