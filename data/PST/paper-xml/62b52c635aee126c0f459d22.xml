<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Offline RL for Natural Language Generation with Implicit Language Q Learning</title>
				<funder>
					<orgName type="full">Amazon-BAIR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-05">5 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
							<email>csnell22@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
							<email>kostrikov@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
							<email>suyi@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Offline RL for Natural Language Generation with Implicit Language Q Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-05">5 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.11871v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL motivated method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility optimization framework of traditional RL algorithms with supervised learning's ability to leverage existing data and its simplicity and stability. Our method, based on dynamic programming, employs a blend of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing utility. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as an example of toxic speech or not.</p><p>Code at https://sea-snell.github.io/ILQL_site/ Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models can acquire a remarkable amount of knowledge from large noisy corpora and can be applied to a wide range of language-based tasks. However, such models are not designed to optimize any user-specified utility, instead requiring considerable trial-anderror to design prompts that "coerce" the models into producing desirable outcomes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. In essence, standard unsupervised language model training only solves part of the problem, being effective at distilling down knowledge in large corpora, but relatively clumsy when applying this knowledge to solve user-specified tasks.</p><p>Reinforcement learning (RL) in principle can provide an effective framework for steering language models toward user specified tasks as long as it could be represented using some utility functions; however, as outlined in Figure <ref type="figure" target="#fig_1">2</ref> contemporary methods suffer from high systems complexity and can require expensive human interaction, we therefore need several conditions to make RL practical: (1) Easy to use: the underlying learning algorithm and workflow should be simple, stable, and scalable; (2) Able to optimize user specified rewards: the algorithm should be able to steer a language model toward maximizing any user-defined reward functions, from high-level task goals (e.g., book a flight) to low-level linguistic subtleties (e.g., avoiding rude or toxic speech); (3) Practical in interactive settings: the system should be able to handle a variety of tasks, from generating text with desired properties to sequential turn-taking in settings such as dialogue tasks; (4) Able to leverage existing data: it is beneficial for such system to directly utilize the large quantities of existing internet data, avoiding expensive and time-consuming online human interactions; <ref type="bibr" target="#b4">(5)</ref> Temporally compositional: finally, the method should be able to attain significant improvement over the average behavior in the data -not merely copying the best behaviors in the dataset, but actually distilling out underlying patterns in the relationship between rewards, task dynamics, and language to produce near optimal generations, even when the dataset demonstrates only mediocre performance on the task. Offline RL provides a learning paradigm (Figure <ref type="figure" target="#fig_0">1</ref>) that combines both supervised learning's ability to leverage existing data (criteria 4) with the general utility optimization power of online reinforcement learning methods (criteria 2, 3, 5) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, prior offline RL approaches for language tasks are either based on dynamic programming, which enjoy the temporal compositionality but suffer from high systems complexity, hyper-parameter instability, and slow training times <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> (meets criteria 5, fails 1), or methods based on conditional imitation or dataset value learning that are simple and stable to train, but do not have the "stitching" benefit of RL (meets criteria 1, fails 5) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Motivated by all these criteria, we design a novel offline RL method based on dynamic programming with an implicit dataset support constraint <ref type="bibr" target="#b4">[5]</ref>, that enjoys greater stability, fewer training time dependencies, and a more flexible decoding process than prior approaches (see Sections 4 and 6.3). Specifically, we fine-tune a transformer language model to predict the state-action Q function and the state value function V at each token. During training we perform iterative policy improvement by fitting value function to an upper-expectile of the Q function. At inference time, we use our learned value functions to perturb the log probabilities of a standard language model, steering the language model towards producing outputs that maximize a specific utility (see Figure <ref type="figure" target="#fig_2">3</ref>). Our main contribution is twofold: 1) a novel offline RL algorithm, ILQL, for language models, that employs a stable optimization process that can flexibly learn high-performing policies from sub-optimal data in arbitrary sequential decision making settings, thus meeting each of the conditions laid out above; and 2) a detailed empirical analysis, not only demonstrating ILQL's ability to flexibly adapt to many different utility functions more consistently and more stably than prior approaches, but also ILQL's unique ability to optimize stochastic or subjective reward functions and its ability to discover optimal behavior in the face of sub-optimal or unusual data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A number of prior works have explored combining online RL methods with language models for natural language tasks such as machine translation or summarization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. These works have demonstrated that RL can be a good tool for steering language models towards satisfying utility functions. However, when it comes to settings that require multiple steps of human interaction, e.g., dialogue, these methods can quickly become impractical <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Offline RL addresses this shortcoming by removing all need for environment interaction or user simulators, instead operating purely on static datasets of prior human interaction. Several prior works have applied offline RL to NLP and more broadly sequence generation problems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. The most closely related to our work are those methods based on approximate dynamic programming <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref>. While all these works present promising offline RL methods for NLP tasks, none of them provide a method that achieves the simplicity, stability, and ease-of-use aspect at the level of supervised learning. For example, Verma et al. and Jang et al. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> define their action space at the "per-utterance" level <ref type="bibr" target="#b10">[11]</ref>, resulting in expensive decoding processes during training <ref type="bibr" target="#b30">[31]</ref>; and while Jaques et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> remove this issue by defining actions at the "per-token" level, the offline RL algorithm proposed requires querying likelihoods from a language model at RL training time. This compounds complexity and its potential sources of error, and leads to increased computational cost during training. Our proposed method instead operates both at the "per-token" level and trains in a fully self-contained way, without the need to simulate generation at training time. This is achieved by combining an implicit dataset support constraint <ref type="bibr" target="#b4">[5]</ref> with a novel policy extraction method that takes advantage of the discrete "per-token" action space. The result of these design considerations is a simple, stable, and effective method that is easy for NLP practitioners to pick up and apply to a variety of language-based tasks; in Section 6.3 we demonstrate our method's effectiveness in meeting these criteria through a series of ablations and comparisons. Much prior work on steering language models towards desired behavior has done so without an explicit utility function, instead either carefully curating finetuning datasets <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, or designing subtle decoding heuristics <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. A more closely related line of work uses classifiers to guide LMs towards generating desired textual attributes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. These methods are closely related to the prior work on offline RL. In RL parlance, such methods could be considered "policy extraction" methods with Monte Carlo value estimates. This can be interpreted as taking a single step of policy improvement which, though often effective <ref type="bibr" target="#b45">[46]</ref>, is known to be suboptimal as compared to full dynamic programming methods (i.e., full Q-learning or actor-critic) <ref type="bibr" target="#b4">[5]</ref>. We will demonstrate empirically in Section 5 that our offline RL method can lead to significant improvements in final performance as compared to such "single step" approaches, particularly when the training data is highly suboptimal for the desired task.</p><p>3 Preliminaries: Language Generation as a Reinforcement Learning Task</p><p>Token-level POMDP. In this work, we formalize language generation tasks as a partially observable Markov decision process (POMDP). We define the POMDP M at the token level with M = (S, A, O, T , Z, ? 0 , R, ?). We define the agent's observation h t ? O as a history of tokens with h t = {t 0 , t 1 , t 2 , t 3 , ...t t-1 }; the action space a t = t t ? A is the set of possible next-tokens in our vocabulary which includes the special end-of-turn token a end (see Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>Value-based offline RL. In offline RL, the goal is to learn the optimal policy ? that achieves highest discounted cumulative reward from a static dataset D that was produced by some potentially suboptimal behavior policy ? ? . In this work, we build on the implicit Q-learning (IQL) algorithm <ref type="bibr" target="#b4">[5]</ref>, which approximates the Bellman optimality equation constrained to in-dataset actions</p><formula xml:id="formula_0">Q * (s, a) = R(s, a) + ? max a ,s.t. ? ? (a |s )&gt;0 Q * (s , a ).</formula><p>Instead of directly implementing the support constraint, IQL approximates the maximization on the right-hand side of the constrained Bellman operator with expectile regression:</p><formula xml:id="formula_1">L V (?) = E (s,a) ?D [L ? 2 (Q ? (s, a) -V ? (s))]<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">L ? 2 (u) = |? -1(u &lt; 0)|u 2 .</formula><p>Increasing the hyperparameter ? , more closely approximates the maximum. Then this approximation can be used to estimate TD-targets for the Q-networks:</p><formula xml:id="formula_3">L Q (?) = E (s,a,s ) ?D [(R(s, a) + ?V ? (s ) -Q ? (s, a)) 2 ].<label>(2)</label></formula><p>IQL was designed for fully observable MDPs. However, in Section 4.1, we discuss how we adapt this formulation to the POMDP setting described above using sequence models. Supervised learning baselines. In line with RL nomenclature, we denote finetuning on curated or filtered data as %BC and finetuning on unfiltered data as BC (also referred to as ? ? ). See Appendix A.3 for filtering details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implicit Language Q-Learning</head><p>Our main technical contribution implicit language Q-learning (ILQL), an offline RL algorithm for NLP tasks. ILQL is specifically designed to enable simple and efficient training of language models with user-specified reward functions, with a workflow that is similar to standard supervised learning. ILQL builds on the IQL algorithm, extending it to the token-level POMDP that defines NLP tasks via the following modifications: (i) it integrates with sequence models to handle partially observable language generation tasks (Section 4.2); (ii) it utilizes a novel policy extraction method that directly  perturbs the behavior policy ? ? with our learned value functions, rather than training a separate actor ?, significantly improving performance and stability on NLP tasks (Section 4.1) and (iii) it adds a conservatism loss term <ref type="bibr" target="#b5">[6]</ref> to the Q-function, fixing a calibration issue in the policy extraction step. Figure <ref type="figure" target="#fig_2">3</ref> provides an overview of our method.</p><formula xml:id="formula_4">Q 0 Q 1 Q 2 V 0 V 1 V 2 ? ? ? 0.97 1.00 1.12 Q 3 Q 4 Q 5 V 3 V 4 V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adapting Implicit Q-Learning to Language Models</head><p>Implicit value function learning. Like IQL, our method learns both a value function and a Qfunction, which bootstrap off each other through Bellman backups with implicit maximization through an expectile loss. This recursive process of fitting Q and V corresponds to iterative policy improvement subject to an implicit dataset support constraint, specified by the expectile used to fit V. Due to parameter sharing, we combine Eqn. 1 and 2 into a single loss function:</p><formula xml:id="formula_5">L Q,V (?) = E ? ?D T i=0 (R(h i , a i ) + ?V ? (h i+1 ) -Q ? (h i , a i )) 2 + L ? 2 (Q ? (h i , a i ) -V ? (h i ))</formula><p>In contrast to IQL, we sample sequences of tokens instead of individual transitions to handle partial observability, such that for each time step, the values are predicted based on a full history.</p><p>Policy extraction. IQL <ref type="bibr" target="#b4">[5]</ref> uses AWR policy extraction <ref type="bibr" target="#b46">[47]</ref>, which distills the Q-function into a policy with a weighted log-likelihood loss, with weights given by e ?( Q-V ) . However, as we discuss in the Section 6, we found this somewhat unstable to train on language models, likely due to the high-variance gradients induced by the advantage weights. Fortunately, the value learning procedure in IQL is independent of policy extraction, so instead of attempting to train a model to represent the optimal policy, we use the learned Q and V values to directly perturb samples from a model finetuned via supervised learning to model ? ? (see Figure <ref type="figure" target="#fig_2">3</ref>). To this end, we compute a modified likelihood for each token by adding its advantage Q(h, a) -V (h) to its logits under the ? ? model, with a multiplier ?. We can then renormalize these pseudo-logits and sample them, resulting in the implicit policy ?(a|h</p><formula xml:id="formula_6">) ? ? ? (a|h)e ?(Q(h,a)-V (h)) = exp(log(? ? (a|h)) + ?(Q(h, a) -V (h))).</formula><p>This does not require training a separate actor, only a behavioral model ? ? , which can be trained with the standard and stable supervised finetuning objective. However, na?vely performing policy extraction in this way can perform poorly due to over-smoothed probabilities in ? ? that may be nonzero for extremely unlikely tokens. In this case, samples from ? ? may be out of distribution for Q and V , and might have erroneous values. To fix this calibration issue, we can either further decrease the probability of low probability actions in ? ? by performing top-p filtering or tuning a temperature parameter, or we can explicitly push down OOD Q-values during training. We implement the latter by adding a small amount of NLL loss to the Q values, which corresponds to the additional loss terms introduced by CQL <ref type="bibr" target="#b5">[6]</ref> with a uniform KL regularizer. Since ILQL actions are discrete tokens, as opposed to the original CQL method <ref type="bibr" target="#b4">[5]</ref>, which operates on continuous action spaces, this CQL loss term is no more expensive than, and in fact equivalent to, a standard cross-entropy loss at the token level. We find that both of these approaches often work in practice, but prefer the latter, finding that it requires less tuning for policy extraction at inference time.   <ref type="bibr">Figure 4:</ref> Left: an abstract depiction of an MDP where SARSA fails to discover the optimal policy. Right: A notional illustrative example where we might expect full "multi-step" RL methods (such as ILQL) to perform significantly better than "single-step" methods such as SARSA or filtered supervised learning methods that simply imitate high-scoring examples. In this example, good utterances tend to start with "The movie was...", while bad utterances start with "The movie wasn't..." However, the very best examples also start with "The movie wasn't...", requiring multi-step planning or multiple steps of policy improvement to derive effective strategies. Due to the temporal structure of this example, methods that implement just a single step of policy improvement will fail to produce maximally positive sentiment outputs. While this example may appear somewhat contrived, we see in our experiments that multi-step RL methods do lead to improvements in a number of more real settings.</p><p>Our full loss function is therefore:</p><formula xml:id="formula_7">L c Q,V (?) = L Q,V (?) -?E ? ?D log e Q ? (si,ai)</formula><p>a ?A e Q ? (si,a ) In early experiments, we found that decoding using the CQL regularized value functions alone, without ? ? , required careful tuning of the CQL weight ?. When the CQL regularized value function is combined with ? ? for policy extraction, it mitigates this issue with hyper parameter sensitivity, and simply setting the CQL weight ? to an arbitrary small value less than 1 typically works well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architectures for Implicit Language Q-Learning</head><p>We use GPT-2 small as the base model for all transformers. Our value function transformer has three MLP heads: two independently initialized and trained Q heads and one V head. Each head has two layers, with a hidden dimension twice that of the transformer's embedding dimension. Our target Q value is parameterized as the minimum prediction of both Polyak averaged target Q heads: <ref type="bibr" target="#b47">[48]</ref>. Just as in standard language modeling, the transformer's auto-regressive causal masking enables us to perform Bellman updates over entire sequences in parallel.</p><formula xml:id="formula_8">Q = min(Q 1 , Q 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proof of Concept: Multi-Step Offline RL on Wordle</head><p>The lack of easily configurable task settings and reliable simulated evaluations has arguably down progress in applying sophisticated RL algorithms to language and dialogue tasks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>To address this, we present the Wordle game <ref type="bibr" target="#b51">[52]</ref> as an easy-to-use but challenging objective benchmark task to test the capabilities of offline RL algorithms. While this task does not require handling compositional natural language, it still retains many of the challenges of RL with sequence models. In this section, we use this task to explore situations where we would expect offline RL to lead to significant improvement over simpler methods based on supervised learning or single-step improvement (e.g., SARSA-style or filtered supervised learning methods). Multi-step RL: a motivating example. General value-based RL methods based on solving the Bellman equation described in Section 3 can be viewed as iteratively improving the policy: each update sets the current value Q(h t , a t ) to be the reward plus the maximum possible next time step value according to the current value function. This is in contrast to "single-step" update methods, which do not recursively update the value function to account for maximization at future time steps, instead only learning to estimate the value of the behavior policy (i.e., the policy that collected the dataset), and then at test-time greedily selecting the action that maximizes this value. Classic examples of such methods use Monte Carlo regression or SARSA <ref type="bibr" target="#b52">[53]</ref> to train the value function, and then greedily choose actions at test-time, though a number of different methods of this sort have been proposed for guided language generation in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. Since ILQL performs multiple steps of policy improvement, it can be strictly better than SARSA when the underlying data is highly sub-optimal. One particular such case corresponds to the notional task in Figure <ref type="figure">4</ref>, in which the optimal sequence of actions requires one to go through a state that is also frequented by sub-optimal examples. In this case, SARSA will learn to take actions that appear safer according to the dataset -such as the transition "The movie" ? "was" in Figure <ref type="figure">4</ref> --whereas full ("multi-step") RL methods would recover the optimal policy. We demonstrate this phenomenon empirically on the Wordle game below.</p><p>Wordle dataset. Our Wordle task is designed to allow us to use data from real humans in a sequential decision-making setting while still enabling objective simulated evaluation and the flexibility to compose datasets with different properties, thus providing an effective benchmark for validating a variety of approaches. Wordle is a word guessing game; the agent gets 6 turns to guess a 5 letter word randomly selected from a vocabulary, and the environment responds with one of three "colors" for each letter in the guessed word: "black" meaning the guessed letter is not in the environment's word, "yellow" meaning the guessed letter is in the word but not in the right location, and "green" meaning the guessed letter is in the right location. We give a reward of -1 for each incorrect guess and a reward of 0 for a correct guess, at which point environment interaction ends; the agent's goal is therefore to guess the correct word in as few turns as possible, a task for which computing optimal behavior has previously been proven to be an intractable NP-Hard problem <ref type="bibr" target="#b51">[52]</ref>. While Wordle may appear distinct from natural language tasks, it shares a number of high-level properties with more complex language domains, making it well suited for a first evaluation of NLP-focused RL methods. Like dialogue, the game has non-deterministic dynamics and a sequential turn-based structure. However, while Wordle does not require any understanding of word semantics or grammatical structure, this property makes it easy to evaluate policies objectively, providing complementary benefits to more realistic (but more subjective and harder to evaluate) tasks, such as dialogue. Additionally, Wordle not only enables training on datasets of naturalistic, human games scraped from Twitter, but also on datasets synthesized by hand-crafted policies of various skill-levels.  Synthetic Wordle task. Having motivated multiple steps of policy improvement in Figure <ref type="figure">4</ref> and introduced the Wordle task, we now describe the synthetic Wordle dataset which we construct to demonstrate how a single step of policy improvement can fail catastrophically in comparison to multiple steps of improvement. We create such a distribution in Wordle, by synthesizing a dataset consisting of a mixture of three distinct policies, each representing one of the paths through the MDP in figure <ref type="figure">4</ref>: <ref type="bibr" target="#b0">(1)</ref>. ? optimal , a high-performing policy which myopically selects the word with the highest information gain, representing the path from S 0 ? Goal. <ref type="bibr" target="#b1">(2)</ref>. ? adversarial , which behaves the same as ? optimal for the first two actions (S 0 ? S 1 ) and then randomly repeats these first two words at every subsequent action (S 0 ? Lava). (3). ? suboptimal , which selects any random word from the game vocabulary 50% of the time, and the other 50% randomly selects a word that meets all known letter constraints, representing the path from S 0 ? S 1 . The relative performance of these policies is ordered according to ? optimal &gt; ? suboptimal &gt; ? adversarial . We construct our dataset with 9% of the data coming from ? optimal , 45.5% from ? suboptimal , and 45.5% from ? adversarial .</p><p>Evaluating ILQL on the synthetic Wordle task. Measuring the predicted Q values from our models, in Figure <ref type="figure" target="#fig_4">6</ref> (right) we observe that ILQL assigns higher values to actions corresponding to the paths towards "misleading states" (i.e. S 2 ) than those to the "goal states" (i.e. S 1 ), whereas SARSA shows the exact opposite preference, confirming both our hypothesis that this type of MDP would be amenable to multiple steps of policy improvement, and that ILQL as an algorithm is able to perform such policy improvement.</p><p>Validating on natural Wordle data. While the synthetic setting explored above was specifically designed to demonstrate a dramatic difference between ILQL and SARSA, the findings still transfer to more realistic settings. In Table <ref type="table">5</ref>, we demonstrate ILQL outperforming SARSA on a natural dataset of Wordle games scraped from Twitter (see Appendix A.4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Natural Language Experiments</head><p>Next, we evaluate ILQL on two realistic language tasks. We first identify scenarios in which one might expect offline RL to be particularly beneficial: <ref type="bibr" target="#b0">(1)</ref> tasks that demand repeated interactions, such as dialogue; (2) data that is highly sub-optimal under its utility function;</p><p>(3) settings with highly stochastic rewards based on subjective human judgement (e.g., avoiding toxic language). Taking into account these three scenarios, we evaluate ILQL on (1) a goal-directed question asking task based on Visual Dialogue <ref type="bibr" target="#b53">[54]</ref>, where achieving high rewards on a diverse set of metrics during repeated interactions is desirable, and (2) a forum comment generation task based on Reddit Comments with highly subjective and noisy reward functions (toxicity ratings or upvotes). For general experiment details see Appendix A.3.  The "standard" agent asks many yes/no question, whereas adding an exact string match penalty for yes/no questions prevents many of such questions from being asked, and adding a more conservative yes/no penalty prevents all of such questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluating Diverse Rewards on Visual Dialogue</head><p>Visual Dialogue dataset. We use the Visual Dialogue dataset <ref type="bibr" target="#b53">[54]</ref> to evaluate our algorithm's ability to optimize many different reward functions in complex dialogue settings. The task involves a question asking agent and a question answering agent, the latter of which is presented with an image and tasked with answering the former's questions about the image. Instead of using this task as a question answering task, we follow Das et al. <ref type="bibr" target="#b54">[55]</ref> and train our agents to ask questions, with rewards based on how well the ground-truth image can be predicted from the resulting dialogue. For evaluation, we use the model from Das et al. <ref type="bibr" target="#b54">[55]</ref> as our environment simulator. To allow our agents to operate entirely in the space of natural language, we treat the image embedding as part of the reward function, using the (separately pre-trained) supervised model proposed by Das et al. <ref type="bibr" target="#b54">[55]</ref> to predict the image embedding from the dialogue. See Figure <ref type="figure" target="#fig_6">7</ref> for example dialogues in this domain. We chose this environment specifically because (1) it has been previously studied in the context of RL <ref type="bibr" target="#b54">[55]</ref>; (2) as a dialogue game, automated evaluation is more reliable than other tasks; and (3) the Q&amp;A structure enables some temporal compositionality (i.e. the answer to one question may prompt new more specific questions).</p><p>Visual Dialogue task. The agent receives a reward of -1 for each turn in which the ground truth image is sufficiently difficult to predict from the dialogue, otherwise the agent receives a reward of 0 and the environment ends interaction. For details on the task setup, see Appendix A.5. Since the Visual-Dialogue dataset was largely designed for supervised learning agents, the data is already near optimal for the original task. However, if we shift the reward function such that the data is no longer optimal, we can observe very large improvements from offline RL. We therefore use this domain to demonstrate offline RL's flexibility to adapt to different reward functions. We consider three reward functions: "standard", "y/n", and "conservative y/n". "Standard" is simply the reward flexibly optimize for many different rewards, even those for which the data is highly sub-optimal (e.g. BC performance). Right: Evaluating each ILQL agent on all other reward functions. Agents generally perform worse on reward functions for which they were not trained.</p><p>described above and detailed in Appendix A.5. "y/n" penalizes the agent for asking questions that produce yes or no answers. It assigns a reward of -2 each time the other speaker says "yes" or "no" in addition to the "standard" reward. This is challenging, because while the data contains many yes/no questions, the goal is not for the agent itself to avoid those words, but rather avoid saying things that will cause the other speaker to use them. Not all simple questions produce literal "yes/no" answers, so our third reward further penalizes all brief responses, such as "I can't tell", "no it isn't", "yes it is", "I don't know". This reward function assigns a -2 reward to a set of low-information responses using a handful of conservative string matching heuristics, detailed in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on optimizing diverse rewards</head><p>We demonstrate that ILQL learns a policy distinct from the dataset behavior policy and can optimize many different rewards in  <ref type="figure" target="#fig_6">7</ref> demonstrates this qualitatively: without the "yes/no" penalty, our policies tend to ask many "yes/no" questions, but under its presence policies tend to ask more color or number based questions instead. Even when the underlying data is highly sub-optimal for a given reward function, ILQL is able to determine the desired behavior. Reddit comments dataset. To evaluate our agents on minimally curated and maximally diverse open-domain text with highly stochastic reward functions based on subjective human judgement, we train our offline RL algorithm on a large dataset of 4 million Reddit comments from<ref type="foot" target="#foot_0">1</ref> ; our agents are given a parent comment or post as context and then trained to produce replies that satisfy one of two rewards: "toxicity" and "upvotes real". Given that this is open internet text, the data contains much toxic language, so for our "toxicity" reward, we train our agents to satisfy a toxicity filter. This filter gives a reward of -10 for toxic comments, -5 for moderately toxic, and 0 for non-toxic. Our second reward function incentivizes generating comments that would receive a positive number of upvotes: giving a reward of +10 for the positive case and 0 for the negative. We automatically evaluate our upvote agents with a finetuned RoBERTa-base model, that predicts whether a comment will receive positive upvotes. We train this model on a held-out split of the data (see Appendix A.6 for more details on our reward models). We train agents on both the ground truth upvotes (denoted "upvotes real") and on this upvote model's predicted reward (denoted "upvotes model").</p><p>Results on optimizing noisy rewards. In natural language tasks, we may need to optimize stochastic, high-variance reward functions based on subjective judgement, such as whether a Reddit comment should be flagged as toxic, or whether it is likely to receive an upvote. Such stochastic settings should be expected when multiple users with different opinions are providing reward labels. Offline RL, by design, is robust to environment stochasticity, and therefore should be able to optimize such noisy reward environments. We use the Reddit toxicity and upvote tasks to study how well ILQL in particular can handle such settings. As we can see in Table <ref type="table">8</ref>, ILQL is surprisingly able to get a perfect or near-perfect score on these more subjective settings, whereas more standard approaches, such as filtered finetuning on only non-toxic or only positive upvote comments (%BC), perform significantly worse (i.e. generates more comments flagged as toxic or predicted to have negative upvotes). We have additional complementary experiments studying this effect in appendix A.8. .  <ref type="bibr" target="#b10">[11]</ref>. We observe that per-token ILQL is generally much faster at inference time than per-utterance methods. All evaluations were performed on a single T4 GPU. All baseline implementations build on the same core code for sampling utterances, with a handful of method specific runtime optimizations in each case. We aim to understand which components of ILQL enable both good results and greater easeof-use than prior offline RL approaches for language tasks. In particular, we validate ILQL's main design decisions: the choice of a per-token action space, our value learning method, and our policy extraction strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ILQL per-token advantages for toxic comments generated by filtered finetuning model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ILQL Ablations</head><p>We evaluate these comparisons on the Visual Dialogue "yes/no" reward because (1) it is important to compare offline RL methods on a challenging and realistic sequential decision problem like dialogue, and (2) since the Visual Dialogue data is already "near-expert" for the "standard" reward, the "yes/no" reward is better able to differentiate between methods.</p><p>Ablations on per-token vs. per-utterance actions. We hypothesize that, since a per-token Q function enables an efficient search through the utterance action space, performing offline RL at the token level, rather than the utterance level, can yield less expensive inference and better performance. We compare ILQL to: (1) ILQL (utterance): a per-utterance adaptation of ILQL that is nearly identical to ILQL, except that it removes the conservatism loss term and performs Bellman backups at the utterance level instead of the token level; (2) SARSA (utterance): a per-utterance version of SARSA, which is identical to (1) with ? = 0.5; and (3) CHAI <ref type="bibr" target="#b10">[11]</ref>: an adaptation of CQL for use on language models at the per-utterance action level. For policy extraction, each of these baselines uses EMAQ <ref type="bibr" target="#b28">[29]</ref>, where we sample N utterances from a learned behavior policy and then re-rank with the Q function.</p><p>We see in Table <ref type="table" target="#tab_9">2</ref> that per-token ILQL outperforms per-utterance ILQL and SARSA, while also running inference ?4x faster on a single T4 GPU. When tuned well, we see that CHAI can slightly, but not significantly, outperform ILQL. However, CHAI is much less stable with respect to hyperparameters than ILQL and is &gt;2x slower at inference time. Additionally, training CHAI involves an expensive preprocessing step of sampling multiple utterances for all actions in the training data, greatly hindering ease of use.</p><p>Ablations on choice of Offline RL algorithm. We compare ILQL to four other RL methods: a per-token version of CQL, an adaptation of the ?-learning as proposed by Jaques et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, decision transformer (DT) <ref type="bibr" target="#b7">[8]</ref>, and SARSA <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. In Table <ref type="table" target="#tab_6">10</ref>, we see that ILQL significantly outperforms baselines, and also has the second lowest hyper-parameter variance, just behind SARSA, confirming our hypothesis that ILQL can provide both high relative performance and training stability.</p><p>Ablations on policy extraction strategies. While ILQL largely follows the design of IQL <ref type="bibr" target="#b4">[5]</ref>, in adapting it to sequence models, we design a novel policy extraction strategy, as described in Section 4.</p><p>In the next experiment, we compare this extraction procedure to the standard AWR-based <ref type="bibr" target="#b46">[47]</ref> policy extraction method used in IQL <ref type="bibr" target="#b4">[5]</ref> and a number of other offline RL algorithms <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. We expect that our approach should generally yield better performance, while also being easier to tune.</p><p>We apply AWR extraction to our best performing ILQL value function, denoting this as ILQL (AWR). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed ILQL, an offline RL method for effectively steering language generation to fulfill a variety of desirable conversational behaviors. Through experiments ranging from word games and goal-directed question asking to optimizing upvotes and minimizing toxic language, ILQL shows that offline RL can serve as a strong alternative to the method landscape of language generation dominated by language model finetuning on manually filtered datasets and classifier guidance. We hope the positive results from ILQL will inspire more work on offline RL for dialogue, and lead to more controllable language models that directly optimize user-specified utility functions for a wide range of tasks in text generation. Additionally, as any utility optimization method can be used to aid or harm, we hope these future works consider ethical uses of offline RL. Our method also has its limitations: for example, ILQL may not prove effective when datasets are extremely suboptimal. Offline RL would also not be ideal in settings that require distributional constraints, such as fairness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Justifications for Offline RL in Dialogue Dialogue tasks are one of the most rich and interactive settings in NLP, and as we will argue, these properties make it an ideal target for applying offline RL. RL in general presents an elegant and highly desirable utility optimization framework for sequential decision making settings, such as dialogue. However, solving realistic interactive tasks with online RL requires either repeated realworld interaction or building a realistic simulator of the environment. In the case of dialogue, such online interaction means communicating with real humans, which may be impractically expensive and time-consuming with contemporary sample-inefficient online RL methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>, and building a realistic simulator of human responses may be largely intractable in sufficiently rich or complex dialogue settings. Offline RL, on the other hand, avoids both of these heavy requirements, by, just as many recent breakthroughs in the field of NLP <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, operating purely on previously collected data, which is wildly available on the internet in general. Offline-RL therefore presents an ideal approach for flexibly steering language models towards the successful completion of dialogue tasks in a way that effectively leverages existing data, just as supervised learning does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Full Token-Level POMDP Formulation</head><p>We expand on the POMDP definition presented in Section 3. In order to apply RL to interactive language settings, we need to formalize dialogue generation and other NLP tasks as a partially observable Markov decision processes (POMDP). We define the POMDP M at the token level with M = (S, A, O, T , Z, ? 0 , R, ?). We define the agent's observation h t ? O to be a history of tokens with h t = {t 0 , t 1 , t 2 , t 3 , ...t t-1 }; the action space a t = t t ? A is defined to be the set of possible next-tokens in our vocabulary, which includes the special end-of-turn token a end . The agent's policy then corresponds to a mapping ? : O ? P(A). Many tasks such as dialogue have an underlying state s t that goes beyond just the sequence history, which can encompass things like the speaker's mental state. The environment transitions T (?|s t , a t ) are defined as a function of this s t . In particular, in domains, such as dialogue, the dynamics are trivial within the agent's utterance (the selected token is deterministically appended to the history), but when the policy outputs a special "end of turn" token, the other speaker gets a turn, which is subsequently appended to the history. In other tasks, where the goal is to generate a single utterance, such as generating a summary or a single Reddit comment, the episode ends when the policy produces the end token. The agent receives a reward, defined by R(s t , a t ) ? R, after each action taken. However, in all the settings we consider, the agent receives non-zero reward r t only after producing an "end of turn" token, rather than densely at every token in the agent's utterances. While some prior works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref> have considered actions at the utterance level, defining decision processes at the token level can yield a more effective search over the exponentially large utterance action space, simply by selecting tokens with high estimated values. Typically, searching over a per-utterance action space requires a Monte Carlo process of sampling multiple full utterances and then re-ranking with estimated values, which can generally bring additional computational complexity at both training and inference time. In Section 6.3, we demonstrated the effectiveness of learning at the token level through an ablation study that compares with learning at the utterance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 General Experiment Details</head><p>Here we outline architecture and hyper-parameter details of all our models and baselines. ILQL experiment details. We run all of our experiments on GPT-2 small transformer architectures, with the supervised learning policy on one transformer and Q and V heads on a separate one. The target Q network is also on a separate transformer. In all our experiments we initialize with GPT-2 pre-trained weights, except in the case of Wordle, where we initialize randomly. Additionally, Wordle uses a different token set: the set of 26 characters, plus an additional token for each "color". We train all RL baselines with double-Q learning, using two separate heads on the same transformer model as the two Q-functions. Our target Q networks are Polyak-averaged with decay factor 0.005 for both the transformer and the Q function head. We use ? = 0.99 for all offline-RL experiments. All value function heads are two layer MLPs with hidden dimension twice that of the transformer's embedding dimension. Our MLPs used ReLU non-linearities and no dropout. We used a learning rate of 1e-4 on the Reddit and Visual Dialogue tasks and 1e-5 on the Wordle task. We used no weight decay in training any of our models, and we used a dropout of 0.1 inside the transformer (i.e. the same dropout setting that was used for pretraining GPT2). We trained all Wordle models with a batch size of 1024, all Visual Dialogue models with a batch size of 64, and all Reddit models with a batch size of 32.</p><p>We always truncate token sequences to length 1024, except on Reddit tasks, in which we truncate to length 512. Except on the Reddit comment task, we train ILQL on each of ? = {0.7, 0.8, 0.9}, and we also evaluate each on ? = {4, 8, 16}. On the Reddit comment tasks, we only train with ? = 0.6 and evaluate on ? = {1, 2, 4, 8, 16, 32}. On all tasks, we report the setting with the greatest performance.</p><p>For the NLL (CQL) loss term applied to ILQL, we used a weight ? of 1.0 on all VisualDialogue experiments, 0.25 on all Reddit Comment experiments, and 0.0001 on Wordle. These values were tuned by hand. Generally, we find this loss parameter to not be too critical to performance; we tune it a little at first for each task and then don't worry about it.</p><p>All ILQL models and all baselines were trained on a single GPU until convergence. Training never exceeded three days.</p><p>Evaluation details. During evaluation, we use greedy decoding to generate utterances on all tasks and baselines, except the Reddit Comments tasks, where we sample instead. All experiments are evaluated on 1024 task-instances from an unseen evaluation set. We use our BC baseline model as ? ? for guiding ILQL's perturbation-based policy extraction.</p><p>BC baselines. We train BC baselines with the same optimization parameters (i.e., weight decay, dropout, learning rate, batch size) and initialization as ILQL. We use early stopping: when the validation loss exceeds the training loss, we stop training. Unlike our ILQL value function models, we use a linear head on top of the transformer to parameterize our BC policy, as is standard for language model finetuning. The only difference between our BC and standard language model training is that instead of finetuning the model to predict the whole sequence of states and actions, we only finetune the model to predict the agent's own actions or utterances.</p><p>%BC baselines. For %BC baselines we report the performance of the best model out of several different percentages: {10%, 30%, 50%} for Wordle, {10%, 20%, 30%} for Visual Dialogue, and for Reddit, since our rewards are discrete, we define %BC to mean just training on the data-points with the maximum reward label. We use the same hyper-parameters as our BC baselines for training these models.</p><p>Decision transformer baseline. Our decision transformer baseline follows from Chen et al. <ref type="bibr" target="#b7">[8]</ref>, except we initialize with pretrained GPT2 weights. All hyperparameters are identical to those used to train our BC baselines. To evaluate decision transformer on our Visual Dialogue "y/n" reward, we swept over a broad range of conditional reward-to-go values: {-11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0}. We report the setting with the best performance.</p><p>SARSA baselines. SARSA baselines are implemented as ILQL with ? = 0.5 and all other hyperparameters are identical to those used with ILQL as described above. Except on the Reddit comment tasks, we evaluate all SARSA models on ? = {4, 8, 16}, and report the setting with the greatest performance. On the Reddit comment tasks, we show the best performance from ? = {1, 2, 4, 8, 16, 32}.</p><p>Per-utterance ILQL. For "ILQL (utterance)", we train models with ? = {0.7, 0.8, 0.9}. We also evaluate each model with number of EMAQ-style <ref type="bibr" target="#b28">[29]</ref> samples chosen from N={4, 8, 16}.</p><p>We report the setting with the best task performance. "SARSA (utterance)" is a special case of "ILQL (utterance)" with ? = 0.5, which we evaluate on each of N={4, 8, 16} and report the setting with the best performance. The architecture for "ILQL (utterance)" and "SARSA (utterance)" is largely identical to that of per-token ILQL, with the main difference being that Bellman backups are performed at the utterance level instead of the token-level. As a result of this difference, Q-heads map to a scalar at the end of an utterance instead of a vector at every token with length equal to the size of the vocabulary.</p><p>CHAI baseline. Our CHAI baseline is adopted from Verma et al. <ref type="bibr" target="#b10">[11]</ref>. The tasks we consider only require utterance actions; no axuiliary actions, like the price proposal action required by that of Verma et al. <ref type="bibr" target="#b10">[11]</ref>'s bargaining task. We therefore only adopt the components from CHAI relevant to utterance level actions. In order to compute CHAI's CQL loss at the utterance level, we need to sample counterfactual utterances for each action in the training data, which can be highly expensive and can greatly slow training. Following Verma et al. <ref type="bibr" target="#b10">[11]</ref>, we amortize this cost at the risk of inducing some bias by caching 5 counterfactual samples for each action in the training data as a preprocessing step. In our case, this preprocessing step took over 30 hours to execute on a V-100 GPU for the full the Visual Dialogue training set. As in all our other experiments, we train two Q networks <ref type="bibr" target="#b47">[48]</ref>, where our target Q value is parameterized as the minimum of both Polyak averaged target Q heads. We train our CHAI models with a batch size of 16 and otherwise all other hyperparameters are identical to those used with ILQL. We train models with CQL ? = {0.1, 1.0, 10.0}, and we evaluate each model with the number of EMAQ-style <ref type="bibr" target="#b28">[29]</ref> samples chosen from N={4, 8, 16}. We report the setting with the best performance.</p><p>Per-token dynamic programming baselines. For our per-token CQL and ?-learning baselines in Table <ref type="table" target="#tab_6">10</ref>, we tuned the CQL loss weight with ? = {0.1, 1.0, 10.0}, and the ?-learning reward scale with c = {0.1, 1.0, 10.0}. For each baseline agent, we evaluated using ILQL's policy extraction with ? = {4, 8, 16} and also evaluated by greedily selecting tokens with the Q function by itself. We report the setting with the best performance for each baseline.</p><p>Our implementation of per-token CQL is identical to ILQL with the only exception being that for per-token CQL the loss function is defined as:</p><formula xml:id="formula_9">L Q,V (?) = E ? ?D T i=0 (R(h i , a i ) + ? max at+1?A Q ? (h i+1 , a t+1 ) -Q ? (h i , a i )) 2</formula><p>Our implementation of ?-learning is adapted from Jaques et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> for use on transformer language models <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b59">60]</ref> instead of RNNs <ref type="bibr" target="#b62">[63]</ref>. The architecture is identical to that of ILQL, the main difference is in the loss function:</p><formula xml:id="formula_10">L Q,V (?) = E ? ?D ? ? T i=0 L ? ( R(h i , a i ) c + log(? ? (h i , a i )) + ? log( at+1?A exp Q ? (h i+1 , a t+1 )) -Q ? (h i , a i )) ? ?</formula><p>Where ? ? is our BC baseline model: a transformer language model trained with supervised learning. And L ? defines the Huber loss; we use ? = 1 in our experiments.</p><p>In both baselines, we also fit a value function head to the mean of the Q functions, as in ILQL with ? = 0.5. Additionally, for both baselines, aside from the parameters mentioned, all other parameters are identical to those used with ILQL, as described above. The only exception being that for ?-learning, we used a learning rate of 1e-5 instead of 1e-4 due to training instability with the higher learning rate. We generally found ?-learning to be highly unstable to train in our experiments, often producing incomprehensible outputs. It is possible that the baseline could work better with even more careful tuning.</p><p>AWR extraction abalation details. For our "ILQL (AWR)" ablation, we extracted a policy with AWR extraction using the best performing ILQL value function out of those trained with ? = {0.7, 0.8, 0.9}. We performed AWR extraction from this value function using 3 different settings for beta: ? = {4, 8, 16}. Using the same value function, we performed ILQL extraction with ? = {4, 8, 16}. For each, we reported the setting with the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Wordle Task Details</head><p>Environment details. Our agents observe the game-state as a history of alternating sequences of 5 letter tokens followed by 5 color tokens. Unlike the actual Wordle game, we do not prevent the agent from generating words that aren't in the vocabulary (i.e., the agent is free to produce any sequence of 5 letters). For our synthetic experiments, we chose to use the full word list given at https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7bdue to its relatively large size and its use in the actual Wordle game. However, the environment can be configured with any provided list of 5 letter words.</p><p>Wordle Twitter dataset details. We outline the details of our natural Wordle dataset scraped from Twitter, introduced in Section 5. Due to Wordle's popularity, we have access to a large amount of natural human data for Wordle (specifically 214,930 games), scraped from tweets <ref type="foot" target="#foot_1">2</ref> . Existing offline RL benchmarks are composed of purely synthetic data <ref type="bibr" target="#b63">[64]</ref>, and as a result, it may be unclear how well offline RL algorithms work on more natural data distributions. We therefore present this human Wordle dataset as a more naturalistic offline RL task. While the scraped Tweets don't display the actual words used by human players, only the sequence of transition colors given by the environment, we can retrofit valid words onto these tweets to produce a dataset of full trajectories.</p><p>Note that the words we retrofit may not necessarily be the natural words human players would have used, but the dataset still represents the average performance of human players, since the number of turns remains unchanged in this retrofitting process. Additionally, this retrofitting allows us to further multiply the size of the dataset, since typically several different sequences of words can be valid for a given tweet. We can also partially control the difficulty level of the task and dataset by specifying the size and composition of the vocabulary used to retrofit words onto Tweets. In our Wordle human experiments in Table <ref type="table">5</ref>, we use a random subset of 200 words from the word list given at https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Visual Dialogue Task Details</head><p>Here we detail the task setup and reward functions used in our Visual Dialogue experiments in Section 6.1. We use Das et al.'s code <ref type="foot" target="#foot_2">3</ref> to produce generations and to predict image embeddings from the provided supervised learning answer and question bots, respectively. We integrate these components of Das et al.'s codebase into ours by wrapping the relevant functionality of Das et al.'s codebase in a flask webserver interface that is then queried by our system. As described in Section 6.1, our "standard" reward function gives a reward of -1 for each turn in which the true image is sufficiently difficult to predict from the dialogue, otherwise the agent receives a reward of 0 and the environment interaction ends. We firstly formalize this notion of "sufficiently difficult to predict". The standard reward is based on the relative percentile ranking of the ground truth image's distance from the predicted embedding among a set of images taken from the evaluation set. We give a -1 reward to our agent for every turn in which (1 -p t ) &lt; (1 -p 0 ) * 0.5, where p t is the ground truth image's percentile rank at dialogue turn t and p 0 is the ground truth image's percentile rank at the beginning of the dialogue, when only the image caption is observed. Otherwise, the agent gets a reward of 0 and the episode ends. This condition effectively rewards the agent once the ground truth image is preferred over 50% of the images that were preferred over it at initialization. The agent should learn to ask as many good questions as possible to get the episode to successfully end as early as possible. In initial experiments, we found it took a very long time to train good value functions for rewards based on the absolute Euclidean distance alone, as used by Das et al. <ref type="bibr" target="#b54">[55]</ref>, so to make it faster to iterate, we used the relative distance formulation described above.</p><p>Our "y/n" reward adds, on top of the "standard" reward, a reward of -2 for every question that results in a response that exactly matches the strings "yes" or "no".</p><p>Our "conservative y/n" reward instead aims to provide a more conservative, higher-recall lowerprecision penalty to any question which might be a yes/no question. This accounts for the fact that people often answer yes/no questions with longer phrases (e.g., "It appears so"). This reward function provides a reward of -2 if any of the following words are sub-strings of the response to the agent's question: "not", "don't", "can't", "cannot", "fairly", "could", "think so", "okay", "maybe", "yes", "no", "looks", "appears", "tell", "mostly just". All of these were determined by hand to be words/phrases that occur often in answers to questions that are effectively yes/no questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Reddit Reward Model Details</head><p>We outline the details of our reward functions for the Reddit tasks presented in Section 6.2. Toxicity Reward. Our toxicity filter reward uses OpenAI's API <ref type="foot" target="#foot_3">4</ref> , which provides a free toxicity filter, meant for developers building applications off the GPT3 API to use to block toxic inputs or generations. We assign a reward of -10 for comments labeled as toxic (scored as 2), -5 for comments labeled as moderately toxic (scored as 1), and 0 for comments labeled as non-toxic (scored as 0). Upvote Model Reward. Our upvote reward function is finetuned from RoBERTa-base <ref type="bibr" target="#b64">[65]</ref> with a learning rate of 1e-5 and a batch size of 64 256. At inference time, we predict a reward of +10 if the model's reward logit is ? 0 and a reward of 0 otherwise. We used binary (positive or negative) rewards for upvotes instead of the more natural cardinal numeric representation, because different sub-reddits can have drastically different upvote counts depending on the sub-reddit's population, and our binarization (positive or negative upvotes) is invariant to these differences in scale. However, this binarization is not the only normalization that we could have used to overcome this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Evaluating Reddit Agents on Different Rewards</head><p>To complement Table <ref type="table" target="#tab_6">1</ref> right, comparing Visual Dialogue agents on all Visual Dialogue reward functions, we also present Table <ref type="table" target="#tab_12">3</ref>, showing how different Reddit agents perform when evaluated on different rewards. Here we find the same result, that agents trained on one reward function are generally less optimal for others, further confirming ILQL as an effective utility optimizer on natural language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Noisy Rewards</head><p>In figure <ref type="figure" target="#fig_0">11</ref>, we present a more detailed visual explanation for why offline RL outperforms filtered finetuning on our Reddit tasks in section 6.2. As our results in Table <ref type="table">8</ref> show, offline RL consistently outperforms finetuning on filtered data on this task. We hypothesize that this is due to offline RL's ability to effectively reason about the inherently stochastic and subjective rewards functions present in these tasks. In these high-variance reward settings, simply filtering or curating datasets for exclusively high-reward examples can fail to produce desirable outputs, since such filtered finetuning approaches do not make the model aware of the reward uncertainty. Put another way, training on curated datasets that exclude low-reward examples doesn't teach the model about what not to generate, whereas models that are aware of the reward, such as Q-learning, directly learn to relate actions to their expected reward values, averaging out uncertainty and stochasticity. This can enable models to avoid outputs that have low but non-trivial probability of undesirable outcomes (e.g., toxicity). Offline RL in this sense is able to find the safest outputs, whereas fine-tuning on filtered data does not explicitly express such a preference. To further test this hypothesis, we artificially add further noise to the toxicity reward function. The standard toxicity reward assigns all comments one of three reward values: 0, indicating the comment is non-toxic; -5, indicating the comment is moderately toxic; -10, indicating the comment is highly toxic. We now relabel the reward for all comments originally given a -5 reward, randomly to either 0 or 10 with equal probability. Since some of the moderately toxic comments get relabeled with reward=0, they would be included in the %BC training set, whereas offline RL should learn to represent uncertainty about such comments and thus push away from the stochastic "middle ground" of this reward function. In Table <ref type="table">A</ref>.8 in the "noised toxicity" column, we see that our offline RL agents learn to never generate toxic outputs despite the additional noise, and in Figure <ref type="figure">9</ref> we can see qualitatively that offline RL assigns low advantages to potentially negative or toxic words/phrases that were incidentally generated by the %BC model. All of this goes to support our hypothesis that the advantage of offline RL over filtered supervised learning in these settings lies in its improved ability to handle reward uncertainty. Figure <ref type="figure" target="#fig_0">11</ref>: A visual explanation of offline RL's ability to optimize high variance reward functions based on subjective judgement, such as whether to label a comment as an example of toxic speech or not. Finetuning on filtered data accidentally generalizes into producing undesirable outputs, whereas offline RL is able to find the "safe" outputs. Top: In the case of stochastic rewards, offline RL learns to avoid the highly stochastic regions of the action space, whereas filtered finetuning will explicitly learn to imitate undesirable outputs that were stochastically given a positive reward in the training data, thus leading to suboptimal behavior. Right: In the case of non-stochastic but sharp-boundary reward functions, ILQL is still able to integrate into its Q values uncertainty about actions near the sharper parts of the reward function's decision boundary, thus avoiding these regions. Finetuning on filtered data expresses no such preference and thus risks generalizing into occasionally producing undesirable outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Trading Off Output Diversity for Optimization</head><p>An advantage of our novel policy extraction mechanism is that we can flexibly tune the parameter ? at inference time, directly trading off between random generation and optimality. As discussed in Section 12, this parameter controls a constraint on our policy's deviation from the data distribution.</p><p>As we increase ?, the resulting policy will be more strongly influenced by the Q-function, and as we decrease ?, it should approach the data distribution. Beyond the risk in diverging too far from the data, another potential downside to increasing ? is that the resulting policy distribution will become more deterministic. In some settings, such as chit-chat dialogue, we may desire policies capable of producing diverse and interesting outputs, so such a deterministic, highly-optimized agent would be undesirable. We demonstrate in Figure <ref type="figure" target="#fig_1">12</ref>, using the Reddit Toxicity task, how varying ? can modulate the diversity of the generations produced by our policy, as measured by its entropy, at the cost of a small decrease in performance. We show that as we increase ?, while the policy's performance generally increases, the entropy decreases and subsequently so does the interestingness and diversity of the language model's outputs. At inference time, we can tune this parameter to trade-off optimization for output diversity as we desire.   Here yes/no questions are determined by the same exact match heuristic used by the "y/n" reward. ILQL agents ask fewer questions triggered as being yes/no than baselines. The best performing agent is used for all methods. Bottom: Histogram of the fraction of yes/no questions asked per dialogue by Visual Dialogue agents trained on the "conservative y/n" reward. Here yes/no questions are determined by the same exact match heuristic used by the "conservative y/n" reward. ILQL agents ask fewer questions triggered as being yes/no than baselines. The best performing agent is used for all methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Offline RL differs from supervised learning in that it learns to maximize user-specified rewards from suboptimal data with reward labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ILQL meets each of the five criteria for practical NLP RL methods we outline in Section 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A diagram of our Implicit Language Q Learning algorithm. Left: ILQL training involves three transformers, each of which is finetuned from a standard pretrained model: (1) A ? ? model, finetuned with standard supervised learning. (2) A value function model, with Q and V on two separate heads; the value functions are trained with Bellman backups using a combination of conservatism and and an implicit dataset support constraint. (3) A target value network, which is a Polyak moving average of (2). Right: At inference time, we use our learned value functions to perturb the log probabilities of ? ? towards utility maximizing behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: We empirically validate the setting depicted in Figure 4 on a Wordle task. Left: a visualization of the synthetic dataset distribution we constructed to demonstrate the benefits of ILQL's multiple steps of policy improvement over "single step" methods, such as SARSA. Right: a plot showing that ILQL's Q function learns to more often assign higher Q values to optimal actions than SARSA.</figDesc><graphic url="image-26.png" coords="6,366.18,113.64,116.93,70.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing ILQL to baselines on Wordle human data. Even on realistic human data, ILQL outperforms "single step" SARSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Dialogues from agents optimized fordifferent reward functions on the Visual Dialogue task. The questions are qualitatively different depending on the reward function. The "standard" agent asks many yes/no question, whereas adding an exact string match penalty for yes/no questions prevents many of such questions from being asked, and adding a more conservative yes/no penalty prevents all of such questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )</head><label>a</label><figDesc>Did you state the full set of assumptions of all theoretical results? [N/A] We have no theoretical results. (b) Did you include complete proofs of all theoretical results? [N/A] We have no proofs. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] see supplemental (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] see Appendix (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] see Section 6 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] see Appendix 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We cite all datasets and models used in Section 6. (b) Did you mention the license of the assets? [No] They are open sourced. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See supplemental for code. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] All assets used are open source. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We use potentially toxic internet text for some experiments; we discuss this in Section 6. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] We do not conduct experiments with human subjects. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] We do not conduct experiments with human subjects. (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] We do not conduct experiments with human subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>example standard reward Visual DialoguesFigure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: Example dialogues produced by the best performing ILQL agent on the Visual Dialogue "standard" reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: Top: Histogram of the fraction of yes/no questions asked per dialogue by Visual Dialogue agents trained on the "y/n" reward. Here yes/no questions are determined by the same exact match heuristic used by the "y/n" reward. ILQL agents ask fewer questions triggered as being yes/no than baselines. The best performing agent is used for all methods. Bottom: Histogram of the fraction of yes/no questions asked per dialogue by Visual Dialogue agents trained on the "conservative y/n" reward. Here yes/no questions are determined by the same exact match heuristic used by the "conservative y/n" reward. ILQL agents ask fewer questions triggered as being yes/no than baselines. The best performing agent is used for all methods.</figDesc><graphic url="image-28.png" coords="25,108.00,223.38,414.72,248.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>The movie wasn't good. It was utterly mind blowing, creative, and inspiring! The movie wasn't good. It was</head><label></label><figDesc>a masterpiece, an experience, a lifetime!</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>The movie wasn't good. I wanted to fall asleep by the end. The movie wasn't good. All</head><label></label><figDesc></figDesc><table><row><cell>the</cell></row><row><cell>characters were one dimensional, and</cell></row><row><cell>it lacked creativity.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>The movie was good. The movie was solid. It entertained. 10% of data +1.0 reward each 45% of data -1.0 reward each 45% of data 0.5 reward each Dataset Composition SARSA</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Reward Function: Yes/No Penalty: Conservative Yes/No Penalty:</head><label></label><figDesc></figDesc><table /><note><p>Image Caption: A man, with other people in the background, is sitting at a picnic table with a cell phone in front of him while he uses a fork and knife to cut a dessert on a plate. Questioner: is man young? Answerer: yes Questioner: is he wearing glasses? Answerer: no Questioner: is there food on table? Answerer: yes Questioner: is there food on table? Answerer: yes Image Caption: A person flying a kite over a sand dune. Questioner: what color is kite? Answerer: it's black and white Questioner: is person male or female? Answerer: can't tell Questioner: what is person wearing? Answerer: i can't tell Questioner: what color is sand? Answerer: dark brown Questioner: is person flying kite flying in air? Answerer: yes Questioner: what color is kite? Answerer: black Image Caption: Tour buses are lined up on the street waiting for people. Questioner: how many buses? Answerer: 2 Questioner: what color are buses? Answerer: white and red Questioner: how many people? Answerer: 2 Questioner: what gender are people? Answerer: 1 is male and 1 is female Questioner: what are they wearing? Answerer: 1 is wearing shorts and other is wearing shorts and shirt Questioner: what color is their hair? Answerer: dark brown Standard</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Left: Comparing ILQL to baselines on various Visual Dialogue reward settings. ILQL is able to</figDesc><table><row><cell>method ILQL SARSA %BC BC</cell><cell>standard -5.22 ? 0.13 -5.14 ? 0.13 -5.07 ? 0.13 -5.25 ? 0.13</cell><cell>y/n -5.69?0.13 -6.19?0.15 -7.48 ? 0.21 -10.85 ? 0.27</cell><cell>conservative y/n -6.57 ? 0.18 -7.77 ? 0.20 -9.13 ? 0.22 -15.16 ? 0.35</cell><cell>train/eval standard y/n conservative y/n</cell><cell>standard -5.22 ? 0.13 -5.41 ? 0.12 -5.29 ? 0.13</cell><cell>y/n -11.12 ? 0.30 -5.69 ? 0.13 -5.42 ? 0.13</cell><cell>conservative y/n -14.97 ? 0.36 -8.24 ? 0.22 -6.57 ? 0.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 1 left, where we can see that ILQL is able to outperform baselines on most of our Visual Dialogue reward functions. Agents optimized for each reward function are quantitatively different as well: in Table 1 right, we see that offline RL agents trained on one reward function are generally suboptimal on others. Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 :</head><label>2</label><figDesc>On the VisualDialogue "y/n" reward, we compare per-token ILQL to per-utterance ILQL, per-utterance SARSA, and CHAI</figDesc><table><row><cell>advantage:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>token:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>advantage:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>token:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Figure 9: Two toxic comments incidentally generated by the filtered fine-tuning model. ILQL assigns negative</cell></row><row><cell cols="4">advantages to many tokens, thus demonstrating how ILQL is more effectively able to avoid such generations.</cell></row><row><cell>method</cell><cell>max score</cell><cell cols="2">? w.r.t hparams inference time per-dialogue (sec)</cell></row><row><cell>ILQL</cell><cell>-5.69?0.13</cell><cell>0.42</cell><cell>5.10?0.12</cell></row><row><cell>ILQL (utterance)</cell><cell>-5.89?0.14</cell><cell>0.51</cell><cell>22.1?0.47</cell></row><row><cell cols="2">SARSA (utterance) -7.35 ? 0.17</cell><cell>0.21</cell><cell>20.38?0.41</cell></row><row><cell>CHAI</cell><cell>-5.57?0.13</cell><cell>1.11</cell><cell>12.13 ? 0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table 10 demonstrates that ILQL is both more stable and better at extracting good performance from a given value function than the well established AWR-style extraction. Additionally, AWR extraction requires tuning ? at training time rather than at inference time, generally decreasing flexibility and increasing the time and effort spent tuning parameters and re-training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 :</head><label>3</label><figDesc>. Since our reward functions are binary, we train with binary cross entropy loss. Like our value function heads in ILQL, we predict the reward as a scalar from a 2-layer MLP on top of the RoBERTa transformer, with hidden dimension twice that of the transformer, ReLU non-linearity, and no dropout. We truncate token sequences to maximum length Evaluating each Reddit ILQL agent on all other reward functions. Agents trained on one reward function are less optimal on rewards for which they were not trained.</figDesc><table><row><cell></cell><cell>train/eval</cell><cell></cell><cell>toxicity</cell><cell cols="2">upvotes model</cell></row><row><cell></cell><cell>toxicity</cell><cell></cell><cell>0.0?0.0</cell><cell>9.07?0.09</cell></row><row><cell></cell><cell cols="2">upvotes gold</cell><cell>-5.00?0.00</cell><cell>9.83?0.04</cell></row><row><cell></cell><cell cols="3">upvotes model -5.00?0.00</cell><cell>10.0?0.0</cell></row><row><cell>method</cell><cell>toxicity</cell><cell cols="4">noised toxicity upvotes real upvotes model</cell></row><row><cell>ILQL</cell><cell>0.0?0.0</cell><cell></cell><cell>0.0?0.0</cell><cell>9.83?0.04</cell><cell>10.0?0.0</cell></row><row><cell>SARSA</cell><cell>0.0?0.0</cell><cell></cell><cell>0.0?0.0</cell><cell>6.23?0.15</cell><cell>10.0?0.0</cell></row><row><cell>%BC</cell><cell>-0.74?0.07</cell><cell></cell><cell>-1.61?0.11</cell><cell>7.06?0.14</cell><cell>7.86?0.13</cell></row><row><cell>BC</cell><cell>-3.51?0.13</cell><cell></cell><cell>-3.48?0.15</cell><cell>4.87?0.16</cell><cell>4.87?0.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4 :</head><label>4</label><figDesc>A comparison of ILQL against baselines on the various Reddit comments reward functions. ILQL manages to never generate undesirable comments on 3 out of 4 reward functions, whereas fine-tuning on filtered data occasionally does.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Image Caption: Tour buses are lined up on the street waiting for people. Pictures of living room, dining room, kitchen and bedroom.</figDesc><table><row><cell>Questioner: how many buses?</cell></row><row><cell>Answerer: 2</cell></row><row><cell>Questioner: what color are buses?</cell></row><row><cell>Answerer: white and red</cell></row><row><cell>Questioner: how many people?</cell></row><row><cell>Answerer: 2</cell></row><row><cell>Questioner: what gender are people?</cell></row><row><cell>Answerer: 1 is male and 1 is female</cell></row><row><cell>Questioner: what are they wearing?</cell></row><row><cell>Answerer: 1 is wearing shorts and other is wearing shorts and shirt</cell></row><row><cell>Questioner: what color is their hair?</cell></row><row><cell>Answerer: dark brown</cell></row><row><cell>Questioner: what color is their shirt?</cell></row><row><cell>Answerer: black</cell></row><row><cell>Questioner: what color is their pants?</cell></row><row><cell>Answerer: black</cell></row><row><cell>Questioner: what color is their shirt?</cell></row><row><cell>Answerer: black</cell></row><row><cell>Questioner: what color is their pants?</cell></row><row><cell>Answerer: black</cell></row><row><cell>Questioner: how many people are there?</cell></row><row><cell>Answerer: 2</cell></row><row><cell>Questioner: what are they doing?</cell></row><row><cell>Answerer: 1 is on top of other 1 is on other side of fridge</cell></row><row><cell>Questioner: what color is fridge?</cell></row><row><cell>Answerer: silver</cell></row></table><note><p>Image Caption: a living room with a table couch guitar and some pictures Questioner: what color is couch? Answerer: brown Questioner: what kind of pictures are on couch? Answerer: 1 is UNK other is not sure Questioner: what color is guitar? Answerer: black Questioner: what color is table? Answerer: brown Questioner: what color is table? Answerer: brown Questioner: what color is guitar? Answerer: black Questioner: what color are walls? Answerer: white Image Caption: Little girl reaching through fence to pet a giraffe. Questioner: what is little girl wearing? Answerer: she is wearing long sleeved shirt and jeans Questioner: what color is fence? Answerer: it's metal fence with black metal fence Questioner: what is giraffe doing? Answerer: it is just standing there Image Caption: example conservative y/n reward Visual Dialogues Figure 15: Example dialogues produced by the best performing ILQL agent on the Visual Dialogue "conservative y/n" reward.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.kaggle.com/code/danofer/reddit-comments-scores-nlp/notebook</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>we use the wordle tweet dataset provided here: https://www.kaggle.com/code/benhamner/wordle-1-6/notebook</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/batra-mlp-lab/visdial-rl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://openai.com/api/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported by an <rs type="funder">Amazon-BAIR</rs> Commons project, as well as the <rs type="institution">Office of Naval Research</rs>. We thank <rs type="person">Dilek Hakkani-tur</rs>, <rs type="person">Alexandros Papangelis</rs>, and <rs type="person">Ruiqi Zhong</rs> for advice and feedback.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 Visual Dialogue Example Dialogues and Histogram</head><p>In Figures <ref type="figure">13,</ref><ref type="figure">14</ref>,and 15 we present a set of selected representative example dialogues produced by our best performing ILQL agent on each reward. In Figure <ref type="figure">16</ref> we present a histogram showing often different agents generate yes/no questions as judged by their respective "y/n" or "y/n conservative" reward functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 Comprehensive Result Tables</head><p>In Tables <ref type="table">5,</ref><ref type="table">6</ref>,and 7 we present comprehensive results for all hyper-parameter settings for all baselines on all tasks.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Noisy channel language model prompting for few-shot text classification</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Offline reinforcement learning with implicit q-learning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conservative q-learning for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline reinforcement learning as one big sequence modeling problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Mopo: Model-based offline policy optimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Morel : Model-based offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Kidambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Human-centric dialog training via offline reinforcement learning</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">Hanwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Shane Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Context-aware language modeling for goal-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FUDGE: Controlled text generation with future discriminators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><surname>Gedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="4929" to="4952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to extract coherent summary via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Controllable neural story plot generation via reward shaping</title>
		<author>
			<persName><forename type="first">Pradyumna</forename><surname>Tambwekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lara</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2019-08">aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Trainable greedy decoding for neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to summarize from human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fine-tuning language models from human preferences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Florian</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Shapira</surname></persName>
		</author>
		<title level="m">Ido Dagan, and Iryna Gurevych. Better rewards yield better summaries: Learning to summarise without references</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Emaq: Expected-max q-learning operator for simple yet effective offline and online rl</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Kamyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><forename type="middle">Shane</forename><surname>Gu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GPT-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Youngsoo</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">If beam search is the answer, what was the question?</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beam decoding with controlled patience</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Typical decoding for natural language generation</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gian</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mirostat: A neural text decoding algorithm that directly controls perplexity</title>
		<author>
			<persName><forename type="first">Sourya</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Govardana</forename><surname>Sachitanandam Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Discriminative adversarial search for abstractive summarization</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul-Alexis</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hafez: an interactive poetry generation system</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Machine translation decoding beyond beam search</title>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna</forename><surname>Pislar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lespiau</forename><surname>Jean-Baptiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="8410" to="8434" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Generating more interesting responses in neural conversation models with distributional constraints</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Offline rl without off-policy evaluation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Advantage-weighted regression: Simple and scalable off-policy reinforcement learning</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards automatic evaluation of dialog systems: A model-free off-policy evaluation approach</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10242</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Survey on evaluation methods for dialogue systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Echegoyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Cieliebak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="755" to="810" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A review of evaluation techniques for social dialogue systems</title>
		<author>
			<persName><forename type="first">Amanda Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents</title>
		<meeting>the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Wordle is np-hard</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lokshtanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Subercaseaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual dialog</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Awac: Accelerating online reinforcement learning with offline datasets</title>
		<author>
			<persName><forename type="first">Ashvin</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtaza</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Nicolas Heess, and Nando de Freitas. Critic regularized regression</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">10 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">D4rl: Datasets for deep data-driven reinforcement learning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image Caption: Yellow and blue passenger train rounding a curve</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Questioner: how many cars on train? Answerer: i can only see 1 Questioner: is it day or night? Answerer: day Questioner: is it sunny or cloudy? Answerer: sunny Questioner: is there anything else on tracks? Answerer: no, just birds Questioner: how many birds are there? Answerer: 2 Questioner: what color are birds? Answerer: black Questioner: what color is sky? Answerer: blue Questioner: what time of day is it? Answerer: it is daytime Questioner: what time of year is it? Answerer: i can&apos;t tell Questioner: what else can you see? Answerer: trees, sky, sky, sky Image Caption: A person flying a kite over a sand dune. Questioner: what color is kite? Answerer: it&apos;s black and white Questioner: is person male or female? Answerer: can&apos;t tell Questioner: what is person wearing? Answerer: i can&apos;t tell Questioner: what color is sand? Answerer: dark brown Questioner: is person flying kite flying in air? Answerer: yes Questioner: what color is kite? Answerer: black Image Caption: Little girl reaching through fence to pet a giraffe. Questioner: how old is girl? Answerer: maybe 3 Questioner: what color is her hair? Answerer: dark brown Questioner: what is she wearing? Answerer: i can only see her from waist up Questioner: what color is her hair? Answerer: dark brown Questioner: what is her eye color? Answerer: i can&apos;t tell, it&apos;s covered with white and black and red Questioner: what is fence made of? Answerer: wood Questioner: what color is fence? Answerer: it&apos;s metal fence with black metal fence Questioner: is giraffe full grown? Answerer: yes Image Caption</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>A robustly optimized bert pretraining approach. with a kite in the air Questioner: how many people are there? Answerer: i see 4 Questioner: what color is kite? Answerer: it&apos;s hard to tell, but it looks black Questioner: are they male or female? Answerer: both are male Questioner: what are they wearing? Answerer: they are wearing shorts and t shirts Questioner: what color are shorts? Answerer: black</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
