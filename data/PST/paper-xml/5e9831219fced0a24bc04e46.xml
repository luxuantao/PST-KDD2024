<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
				<funder ref="#_KxccvC5">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_EUE6paV">
					<orgName type="full">Project of Youth Innovation Promotion Association CAS</orgName>
				</funder>
				<funder ref="#_TC5JCb3 #_bJDTRh8 #_e56AZst #_sc66n5K #_qNh3vys">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_hqHM9w9">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deqing</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenwei</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0000-0001-7650-3657</idno>
						</author>
						<author>
							<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Du</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<email>zhuangfuzhen@ict.ac.cn</email>
							<idno type="ORCID">0000-0002-0520-2619</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Economics and Management</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sci-ences</orgName>
								<address>
									<postCode>100864</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Chinese Acad-emy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNNLS.2020.2979225</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Coarse alignment</term>
					<term>cross-lingual sentiment classification (CLSC)</term>
					<term>topic model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-lingual sentiment classification (CLSC) aims to leverage rich-labeled resources in the source language to improve prediction models of a resource-scarce domain in the target language. Existing feature representation learning-based approaches try to minimize the difference of latent features between different domains by exact alignment, which is achieved by either one-to-one topic alignment or matrix projection. Exact alignment, however, restricts the representation flexibility and further degrades the model performances on CLSC tasks if the distribution difference between two language domains is large. On the other hand, most previous studies proposed documentlevel models or ignored sentiment polarities of topics that might lead to insufficient learning of latent features. To solve the abovementioned problems, we propose a coarse alignment mechanism to enhance the model's representation by a groupto-group topic alignment into an aspect-level fine-grained model. First, we propose an unsupervised aspect, opinion, and sentiment unification model (AOS), which trimodels aspects, opinions, and sentiments of reviews from different domains and helps capture more accurate latent feature representation by a coarse alignment mechanism. To further boost AOS, we propose ps-AOS, a partial supervised AOS model, in which labeled source language data help minimize the difference of feature representations between two language domains with the help of logistics regression. Finally, an expectation-maximization framework with Gibbs sampling is then proposed to optimize our model. Extensive experiments on various multilingual product review data sets</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ONSUMERS from different countries often write online reviews in different languages to express their opinions after buying products from Amazon or Alibaba, which are deemed valuable to producers, service providers, and consumers themselves. Generally, high-quality and annotated English review corpora are often available, whereas non-English corpora (e.g., Chinese and Japanese) are more difficult to obtain. Cross-lingual sentiment classification (CLSC) thus emerges as an important learning task and has attracted much attention from both academia and industries <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. The key idea behind CLSC is to bridge the gap of vocabularies and/or semantics between the source and target language domains such that the resources in the source language domain can be adapted for target language domain. Along this line, machinetranslation-based methods <ref type="bibr" target="#b3">[4]</ref>, transfer learning <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, and feature representation learning-based methods <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> have been proposed to solve cross-lingual problems.</p><p>Feature representation learning-based methods aim to induce a reasonable feature representation between the source and target language domains so as to reduce distributional differences. Many variants of topic models have been proposed to solve cross-lingual classification problems. For example, Lin et al. <ref type="bibr" target="#b8">[9]</ref>, Paul and Girju <ref type="bibr" target="#b11">[12]</ref>, Bao et al. <ref type="bibr" target="#b12">[13]</ref>, and Zhuang et al. <ref type="bibr" target="#b13">[14]</ref> proposed to encode exact alignment by forcing domains to share the same common topics. Li et al. <ref type="bibr" target="#b14">[15]</ref> utilized common topics to learn a projection matrix between different domains. However, the abovementioned methods have an intrinsic drawback, i.e., exact alignment of topics across domains. Specifically, the exact alignment is achieved by either one-to-one topic alignment or matrix projection. The abovementioned exact alignment restricts the representation flexibility and further depresses model performances when the distributional differences between the source and target language domains are large <ref type="bibr" target="#b2">[3]</ref>, that is, the assumption of exact alignment is often violated since different language domains usually differ in their underlying distributions. This motivates us to reduce the restrictions of exact alignments in modeling cross-lingual classification problems.</p><p>The second problem refers to the model granularity problem, i.e., which is more suitable for high-level representation learning, coarse-grained model, or fine-grained one? Generally, coarse-grained models only learn document-level feature representations, which often fail to capture various aspects (e.g., the screen, battery, and camera of an iPhone) in one real-life product review. As a matter of fact, fine-grained models <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> outperform coarse-grained ones <ref type="bibr" target="#b17">[18]</ref> for monolingual sentiment classification because the former can capture more accurate latent feature representations. However, for CLSC tasks, even though fine-grained model performs better than coarse-grained one in <ref type="bibr" target="#b8">[9]</ref>, it only achieves comparable performance to support vector machine (SVM) <ref type="bibr" target="#b9">[10]</ref>. Similar to the model in <ref type="bibr" target="#b8">[9]</ref>, the abovementioned methods sample topics from source and target language domains by word-level translation, which might cause semantic drift and result in inaccurate topic-word distributions because of synonym and polysemy.</p><p>To address the abovementioned problems, we follow the theoretical work of <ref type="bibr" target="#b18">[19]</ref> and try to learn a more accurate and flexible feature representation for CLSC. In this regard, we introduce a coarse alignment mechanism of topic and sentiment and then propose an aspects, opinions, and sentiments unification model named AOS, which not only distinguishes the common and specific topics across different language domains but also identifies sentiment polarity of topics and further coarsely aligns specific topics across different domains. AOS model helps learn a fine-grained representation for CLSC tasks. To further boost AOS, we propose an improved model named ps-AOS so that the labeled data in the source domain are used as partial supervision information to help minimize the distribution difference between the source and target domains, as well as the empirical loss in the source language domain. Finally, we present an EM framework with Gibbs sampling to infer the parameters of our model.</p><p>Our main research contributions are summarized as follows.</p><p>1) We propose an aspect-level unification model (AOS), which trimodels aspects, opinions, and sentiments of reviews from different domains and helps learn more accurate latent feature representation. 2) In unsupervised AOS, we introduce a coarse alignment mechanism to align specific topics with the same sentiment label of different domains, which overcomes the drawbacks of exact alignment in previous models. 3) To make full use of labeled instances in source language training data, we propose an improved model named ps-AOS with partial supervision, in which labeled source language data help minimize the difference of feature representations between the source and target language domains with the help of logistics regression. 4) We present an EM framework with Gibbs sampling to infer the parameters of our models and conduct extensive experiments to demonstrate the significant improvement of our models for cross-lingual and/or cross-domain sentiment classification tasks. The remainder of this article is organized as follows. Section II reviews some related work on CLSC. Then, we formally present our model in Section III and give the experimental setup and results in Sections IV. Finally, Section V summarizes this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-Domain Adaptation</head><p>Cross-domain adaptation aims to extract the knowledge from the label-rich source domain to enhance the predictive model of the target domain. Existing methods often achieve knowledge transfer by detecting a shared low-dimensional feature representation from source domain to target domain. For example, Dai et al. <ref type="bibr" target="#b19">[20]</ref> proposed a coclustering-based method, which identified the word clusters across different domains by propagating the class information and knowledge from source domain to target domain. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed to share the same word clusters between the source and target domains to transfer label information. However, the word clusters between the source and target domains are only related, rather than the exactly same. Zhuang et al. <ref type="bibr" target="#b21">[22]</ref> exploited the association between the word features concepts and the example classes as the bridge across domains.</p><p>Moreover, another recent studies argued that the high-level concepts help to model the difference of data distribution, and they are more appropriate for classification. Specifically, these methods assume that all the data domains have the same set of shared concepts or identical concepts, alike concepts, distinct concepts, which are used as the bridge for knowledge transfer. For instance, Wang et al. <ref type="bibr" target="#b22">[23]</ref> first attempted to discover the alike concepts and used them for knowledge transfer. Then, Long et al. <ref type="bibr" target="#b4">[5]</ref> divided shared concepts into identical and alike ones as cross-domain knowledge. Zhuang et al. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref> exploited the identical, alike and distinct concepts for distinguishing knowledge. Hu et al. <ref type="bibr" target="#b24">[25]</ref> proposed multiknowledge transfer from multisource domains to target domain.</p><p>To sum up, the abovementioned transfer learning approaches achieved a better performance than nontransferred methods for cross-domain sentiment classification. However, even though some approaches have noted the difference of knowledge or model transferred from source domain to target domain, they are two-stage transfer learning and could not consider transfer learning as a whole framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Lingual Adaptation</head><p>Compared with cross-domain adaptation, cross-lingual adaptation needs to address the nonoverlapped feature space problem. Thus, the key idea behind CLSC is to bridge the gap of vocabularies and/or semantics between the source and target language domains. Alone this line, many methods have been studied extensively and deeply <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref>. For example, Banea et al. <ref type="bibr" target="#b30">[31]</ref> leveraged a machine translation technique to improve the model performance of target language. Bilingual parallel corpora and dictionaries are also ideal resources for cross-language sentiment classification tasks <ref type="bibr" target="#b3">[4]</ref>. For instance, Wan <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> proposed a cotraining method, which applied bilingual reviews to improve the performance of classifier.</p><p>Topic model <ref type="bibr" target="#b33">[34]</ref> is a widely used model for learning latent feature representation across different domains. Paul and Girju <ref type="bibr" target="#b11">[12]</ref> proposed a cross-domain Latent Dirichlet Allocation model (ccLDA) and Bao et al. <ref type="bibr" target="#b12">[13]</ref> combined partial supervision into ccLDA; these two models both encoded exact alignment by forcing specific topics between two domains to share the same topic indexes with common topics. Li et al. <ref type="bibr" target="#b14">[15]</ref> utilized common topics to learn a projection matrix between specific topics of different domains. The disadvantage of the abovementioned methods lies in that exact alignment restricts the representation flexibility and results in a significant decline in accuracy when the distribution difference between the source and target language domains is large <ref type="bibr" target="#b2">[3]</ref>, that is, the assumption of exact alignment is often violated since different language domains usually differ in their underlying distributions. Moreover, the abovementioned models are coarse-grained ones because they only learn document-level feature representations, which often fail to capture various aspects in one real-life product review, e.g., the screen, battery, and camera of an iPhone. For fine-grained models, Lin et al. <ref type="bibr" target="#b8">[9]</ref> sampled topics in word-level translation, but it might cause semantic drift and result in inaccurate topicword distributions because of synonym and polysemy. Another challenge in a topic model is that exact inference is often intractable in topic models. Thus, some studies proposed to employ a stochastic EM framework <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, which incorporated the functional optimization problem with Gibbs sampling <ref type="bibr" target="#b36">[37]</ref>.</p><p>Recently, deep neural networks (DNNs) have been applied to learn shared feature representations for cross-lingual sentiment analysis. For example, Chandar et al. <ref type="bibr" target="#b25">[26]</ref> proposed a predicative autoencoder for learning shared representation. A compositional distributed semantics was learned in <ref type="bibr" target="#b37">[38]</ref>. Jain and Batra <ref type="bibr" target="#b38">[39]</ref> developed a cross-lingual sentiment analysis tool based on a bilingually constrained recursive autoencoder. Zhou et al. <ref type="bibr" target="#b26">[27]</ref> proposed to learn bilingual word embedding for cross-lingual sentiment analysis. Generally, DNNs-based approaches need paired sentences from parallel corpora. Moreover, the learned feature representations are difficult to interpret and the algorithms own higher time complexities.</p><p>In this article, we first propose an aspect-level unification model (AOS), which trimodels aspects, opinions, and sentiments of reviews from different domains and helps learn more accurate latent feature representation by a coarse alignment mechanism. Then, to further boost AOS, we propose an improved model named ps-AOS, in which labeled source language data help minimize the difference of feature representations between the source and target language domains by applying logistics regression. Our proposed two methods achieve better performance than various state-of-the-art baselines on CLSC tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL AND INFERENCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generative Process of AOS and Ps-AOS</head><p>Given a source language domain</p><formula xml:id="formula_0">D s = {(x s 1 , p s 1 ), . . . , (x s N s , p s N s )} and a target language domain D t = {x t 1 , . . . , x t N t },</formula><p>where N s and N t are the number of documents in source and target domains, respectively. p ? {+1, -1} is the class label. First, we eliminate the gap of vocabularies between two language domains by machine translation, i.e., translating D t into source language. For convenience, we use subscript to denote the translated domain, i.e., D t trans = {x t trans 1 , . . . , x t trans N t }. Thus, predicting the sentiment label of each x t j in D t becomes predicting the label of each x t trans j by employing D s . Following the abovementioned models, our models have the following assumptions: 1) each sentence in reviews only belongs to one topic and has one sentiment polarity and 2) there are some domain-independent (common) and some domain-dependent (specific) topics across different domains.</p><p>In this article, we proposed an unsupervised model AOS and its improved model ps-AOS with partial supervision. Their graphical representations are shown in Fig. <ref type="figure" target="#fig_1">1</ref> together. It is worth noting that AOS is the plate without considering the orange shaded region, and its generative process is similar to ps-AOS without Step 3(e), that is, we do not draw a class label for document d in the source language domain by logistic regression. In order to present the generative process of our two models succinctly, we only show the generative process of ps-AOS as follows.</p><p>1) For each common topic z, the following steps hold. a) Choose ? A l,z ?Dir (?) for each sentiment l. </p><formula xml:id="formula_1">. i) Choose a sentiment l d,s ?Bern(? d ). ii) Choose r d,s ?Bern(? d,l d,s ). iii) if r d,s = 0 choose z d,s ?Multi (? I d,l d,s ) if r d,s = 1 choose z d,s ?Multi (? S d,l d,s ). iv) For each word n, the following steps hold. A) Choose y d,s,n ?Bern(? d,s,n ). B) If r d,s = 0 and y d,s,n = 0: w d,s,n ?Multi (? A l d,s ,z d,s ) if r d,s = 0 and y d,s,n = 1: w d,s,n ?Multi (? O c,l d,s ,z d,s ) if r d,s = 1 and y d,s,n = 0: w d,s,n ?Multi (? A c,l d,s ,z d,s ) if r d,s = 1 and y d,s,n = 1: w d,s,n ?Multi (? O c,l d,s ,z d,s</formula><p>). The math notations are provided in Table <ref type="table">I</ref>. Our ps-AOS model has three key components: 1) the fine-grained model helps capture fine-grained semantic information across two language domains; 2) sentiment variable l, which is crucial Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 15:42:32 UTC from IEEE Xplore. Restrictions apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I MATH NOTATIONS</head><p>to both coarse alignment and fine graininess, helps reduce the distributional differences; and 3) partial supervision by embedded logistic regression enhances feature representation learning by explicitly minimizing the training error in the source language domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Fine Grained Topics:</head><p>In our ps-AOS model, topics are first divided into domain-independent common topics ? and domain-dependent specific topics ?. The switcher r is used to distinguish common and specific topics. Topics are then further fine-grained through sentiment variable l and aspect/opinions switcher y. Specifically, each common/specific topic ? z /? z is first divided into sentiment-associated subtopics ? l,z /? l,z . Then, ? l,z /? l,z is subdivided into aspect ? A l,z /? A l,z and opinion ? O l,z /? O l,z parts. Finally, we assume that specific topics of one domain are independent of the others, and we have ? A c,l,z , ? O c,l,z for each domain. For common topics, we assume that aspect parts are domain-independent, while we allow different domains to have different opinion words toward the same aspects; thus, we have ? A l,z and ? O c,l,z . 2) Functionalities of Sentiment Variable: Sentiment variable has two roles, i.e., identifying sentiment of topics for fine graininess and aligning specific topics coarsely. For fine graininess, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, we insert a sentiment variable l as topic z's parent to identify the sentiment polarity of z. Suppose that we have two reviews: Review_A is "I like the big screen of iPhone. But its price is too high," and Review_B is "I dislike the screen of iPhone, it is too huge for me." The first benefit of inserting sentiment variable l is that it allows aspects in the same topic to be associated with different sentiments. We can associate the aspect SCREEN with POSITIVE polarity from Review_A and the same aspect with NEGATIVE from Review_B. The second benefit is that subdividing one topic into different sentiment polarities will strengthen model's representation ability for classification tasks. In our model, &lt;SCREEN, POSITIVE&gt; and &lt;SCREEN, NEGATIVE&gt; are considered as two different features, whereas they are represented as one feature &lt;SCREEN&gt; in previous models. We validate the effectiveness of this fine graininess through extensive experiments. 3) Coarse Alignment: The major issue for existing crosslingual topic models is that they try to learn an exact alignment for topics in different domains, either through one-to-one alignment (CLJAS <ref type="bibr" target="#b8">[9]</ref>, ccLDA <ref type="bibr" target="#b11">[12]</ref>, and PSCCLDA <ref type="bibr" target="#b12">[13]</ref>) or projection matrix (TCA <ref type="bibr" target="#b14">[15]</ref>). To address this problem, we introduce a coarse alignment mechanism, which is an alignment between topic groups. For briefness, we define a topic group as a set of topics sharing the same sentiment. The coarse alignment is encoded in the generative process by assuming that a topic z d,s and its common/specific switcher r d,s are generated after sentiment l d,s has been chosen. Under such a generative process, specific topics with the POSITIVE label will forcibly be aligned with common topics with the POSITIVE label. Thus, specific topics with POSITIVE labels in different domains will be aligned via common topics with POSITIVE labels. The topics with NEGATIVE labels are similarly aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Partial Supervision:</head><p>To further improve AOS, we introduce partial supervision into AOS and propose its variant ps-AOS. Here, "partial supervision" means that the model only observes the class labels in the source language domain. To reduce the training error, we adopt class labels in the source domain to guide the sampling of topics by partial supervision in ps-AOS, which will enhance learned feature representations. Specifically, we draw a class label p d for each review in source domain from Bern(logi sti c(-p d ? T zd )) in Step 3(e) while doing nothing to reviews in the target domain. Then, the topic z d,s of source domain is sampled according to 1. We can observe that the sampling of z d,s is related to p d and ?. Actually, our model considers class labels of examples in the source domain into the generative process of topics by parameters p d and ?. Under the partial supervision, for a sentence s in d from source domain, if the label of d is NEGATIVE, then the probability of z d,s belonging to NEGATIVE sentiment increases. With the supervision for generating topics, feature representations will be enhanced by reducing training error.</p><p>Therefore, according to the theoretical work in <ref type="bibr" target="#b18">[19]</ref>, a good representation enables achieving a low error rate in the source domain as well as minimizing a distance between the induced marginal distributions of the two domains. In our model, the embedded logistic regression explicitly models class labels of examples in the source domain, which could help minimize the empirical training error in the source domain and 2) new representation learned by aspect-level fine-grained model and coarse alignment can achieve lower proxy A-distance (PAD) value, which indicates that the new representations of both source and target domains are as indistinguishable as possible <ref type="bibr" target="#b39">[40]</ref>. We will provide experimental verification of PAD in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference</head><p>Exact posterior inference is intractable in AOS and ps-AOS models, so we employ a collapsed Gibbs sampling algorithm <ref type="bibr" target="#b36">[37]</ref> for approximate inference. To solve the optimization problem of our proposed AOS and ps-AOS models, we propose an EM framework. In E-steps, we fix ? and use collapsed Gibbs sampling to infer hidden variables (z, r , l, and y) and then update the empirical topic representation zd of document d. In M-steps, we update the logistic regression coefficient ? by maximizing the joint likelihood of training examples in the source domain.</p><p>Note that the maximum entropy (MaxEnt) component is trained before performing Gibbs sampling, which means {? O , ? A } 1 are fixed during Gibbs sampling. Thus, in E-step, we have four sets of latent variables for ps-AOS model: z, r , l, and y and can jointly sample <ref type="figure">(z d,</ref><ref type="figure">s ,</ref><ref type="figure">r d,</ref><ref type="figure">s ,</ref><ref type="figure">l d,</ref><ref type="figure">s</ref> ) as a block in 1, given the assignments of all other hidden variables, that is</p><formula xml:id="formula_2">P(z d,s = k, r d,s = j, l d,s = t|z ?(d,s) , r ?(d,s) , l ?(d,s) , y, w, x) ? C d (t ) + ? C d (?) + L? ? C d,t ( j ) + ? C d (t ) + 2? ? C d,t, j (k) + ? C d,t ( j ) + K j ? ? 1 + e -p d ? T zd ? e p d ? k /C d (?)</formula><p>1 + e -p d ? T zd</p><p>1 Similar to MaxEnt-LDA <ref type="bibr" target="#b40">[41]</ref>, we adopt a maximum entropy model trained with part-of-speech (POS) tags of words to compute hyperparameter ? for ? .</p><formula xml:id="formula_3">? ? ? C A,t, j,k (?) + V? C A,t, j,k (?) + N A,t, j,k (?) + V? ? V v=1 C A,t, j,k (v) + N A,t, j,k (v) + ? C A,t, j,k (v) + ? ? ? ? ? ? C O,t, j,k (?) + V? C O,t, j,k (?) + N O,t, j,k (?) + V? ? V v=1 C O,t, j,k (v) + N O,t, j,k (v) + ? C O,t, j,k (v) + ? ? ?<label>(1)</label></formula><p>where are the number of times word v is assigned as an aspect and opinion word to topic k of type j with sentiment t, respectively.</p><formula xml:id="formula_4">N A,t, j,k (v) and N O,t, j,k (v)</formula><p>are the number of times word v is assigned as an aspect and opinion word to topic k of type j with sentiment t in sentence s of document d, respectively, C x (?) is the total number of sentences assigned with x, and N x (?) is the total number of words assigned with x in sentence s of document d. Note that all these counts represented by symbol C(?) exclude sentence s of document d.</p><p>For AOS model, in E-step, we only have three sets of latent variables: z, r , and l, i.e., without considering y. Thus, the inference for triplet (z, r, l) in AOS model is as follows:</p><formula xml:id="formula_5">P(z d,s = k, r d,s = j, l d,s = t|z ?(d,s) , r ?(d,s) , l ?(d,s) , y, w, x) ? C d (t ) + ? C d (?) + L? ? C d,t ( j ) + ? C d (t ) + 2? ? C d,t, j (k) + ? C d,t ( j ) + K j ? ? ? ? C A,t, j,k (?) + V? C A,t, j,k (?) + N A,t, j,k (?) + V? ? V v=1 C A,t, j,k (v) + N A,t, j,k (v) + ? C A,t, j,k (v) + ? ? ? ? ? ? C O,t, j,k (?) + V? C O,t, j,k (?) + N O,t, j,k (?) + V? ? V v=1 C O,t, j,k (v) + N O,t, j,k (v) + ? C O,t, j,k (v) + ? ? ? . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>With assignments of z, r, and l, we can sample y (d,s,n) for With the abovementioned assignments, the approximate probability (?) of sentiment l in d, the approximate probability (? ) of topic k of type j with sentiment polarity l in d, and the approximate probabilities of word w in topic k of type j = 0 (?) and j = 1 (?) with sentiment l can be estimated as follows:</p><formula xml:id="formula_7">y (d,s,n) = 0 p(y (d,s,n) = 0|z, r, l, y ?(d,s,n) , w, x) ? exp(? A ?x d,s,n ) i?{O,A} exp(? i ?x d,s,n ) ? C A,l d,s ,</formula><formula xml:id="formula_8">? d (t ) = C d (t ) + ? C d (?) + L? , ? d,t, j (k) = C d,t, j (k) + ? C d,t, j (?) + K j ? ? t,k (v) = C t, j =0,k (v) + ? C t, j =0,k (?) + K I ? , ? t,k (v) = C t, j =1,k (v) + ? C t, j =1,k (?) + K S ? .</formula><p>After obtaining the new empirical topic representation zd of each document d in the source training domain, in M-step, we update the logistic regression coefficient ? through zd . Then, the updated ? is used in the E-step of the next iteration.</p><p>Note that the computational complexity of our proposed AOS and ps-AOS models is similar to the traditional topic model. Concretely, for each sentence, we need to sample z, r , and l, and the time complexity of the abovementioned sampling steps can be seen as O <ref type="bibr" target="#b0">(1)</ref>. Therefore, the computational complexity of each iteration in our proposed models is O((K + L + 4)N), where K is the number of topics, N is the number of sentences, and L is the number of sentiment labels (e.g., L = 2 for binary sentiment classification). For traditional LDA, its computational complexity of each iteration is O(K N) <ref type="bibr" target="#b41">[42]</ref>. The other variants based on LDA have the similar computational complexity, such as TSU <ref type="bibr" target="#b15">[16]</ref> and PSCCLDA <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-Lingual Sentiment Classification</head><p>After applying the EM algorithm with Gibbs sampling described in Section III-B, we obtain the topic distribution of each review and use them to represent reviews in high-level latent feature space. Specifically, zd is an L * (K I + K S )dimensional vector, in which the coarsely aligned specific topics in different domains have the same topic indexes. In our product review classification, the review is labeled as positive or negative, so we set L = 2 in our experiments; and we predict their sentiment labels according to the equation p( p d |z d ) = 1/(1 + e -p d ? zd ). If p( p d = 1|z d ) ? 0.5, the predicted label of document d is positive and negative otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Multilingual Amazon product reviews data set<ref type="foot" target="#foot_1">2</ref> comprises of three product categories: BOOKS (B), DVD (D), and MUSIC (M). It has reviews in four languages: English (E), German (G), French (F), and Japanese (J). For each category in each language, the numbers of both training data and test data are 2000, i.e., 1000 positive and 1000 negative examples. The second one is the balanced English-Chinese NLP&amp;CC cross-language data set. <ref type="foot" target="#foot_2">3</ref> It also contains the abovementioned three categories. However, there are 2000 positive and 2000 negative reviews. In our experiments, we use English as the source language and each of the other four languages as the target language. Note that the two data sets also provide the corresponding English test reviews of each target language test reviews, which are translated by Google Translate. Therefore, we directly use the translated reviews to predict their categories in AOS and ps-AOS models. Finally, we obtain 12 CLSC tasks and 18 cross-lingual/domain tasks, as shown in Table <ref type="table" target="#tab_2">II</ref>. We perform the same data preprocessing for all of the models: remove punctuations, stop words and nonalpha characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Methods and Setup</head><p>We compare our proposed method with various kinds of state-of-the-art baseline algorithms. They are given in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Machine-Translation-Based Methods (MT-SVM with linear kernel and MT-LR) [4]):</head><p>We obtained the best values of their parameters by fine-tuning on EFB task. 2) Transfer Learning Methods (DTL <ref type="bibr" target="#b4">[5]</ref> and TRiTL <ref type="bibr" target="#b5">[6]</ref>):</p><p>For DTL and TRiTL, we first fine-tuned their parameters on the EFB task and then adopted the fine-tuned values as the default parameters of the rest tasks in order to raise efficiency of our experiments. Finally, the parameters of DTL and TRiTL are as follows. The numbers of iterations of both DTL and TRiTL are set as 180, the numbers of common/specific topics in DTL are 25/25, and the numbers of identical concepts, alike concepts, and distinct concepts in TRiTL are set as 20, 20, and 10, respectively. 3) Semi-Supervised Methods (CL-SCL <ref type="bibr" target="#b42">[43]</ref> and Co-training <ref type="bibr" target="#b32">[33]</ref>): Note that CL-SCL and cotraining need auxiliary data. After fine-tuning on the EFB task, for CL-SCL, we set m = 600 and k = 120 for a better performance, while the rest parameters are the same with CL-SCL (i.e., ? = 30 and ? = 10 -5 ), which achieves the best performance. For cotraining, we adopted two monolingual SVM to select ten most confidently predicted instance (five positive instances and five negative ones). 4) Topic Models on Singular Domain (TSU <ref type="bibr" target="#b15">[16]</ref>): After tuning on the EFB task, the parameters for TSU are: #topics = 15, #iterations = 1000, ? = ? = ? = 0.01, and ? = 1. JST and ASUM are excluded because TSU outperforms them significantly. 5) Cross-Domain Topic Models (TCA <ref type="bibr" target="#b14">[15]</ref> and PSC-CLDA <ref type="bibr" target="#b12">[13]</ref>): For the two models, we also fine-tuned them on the EFB task and then set their parameters as follows: #iterations = 1000 and #common/specific topics is 50/50. Besides, for ? = ? = 0.01. 6) Our Baseline (AOS(exact)) <ref type="foot" target="#foot_3">4</ref> : propose a baseline to compare the effectiveness of coarse alignment mechanism. AOS(exact) implements an one-to-one alignment of topics between two domains by assuming that topic type switcher r and sentiment variable l are generated after topic z has been selected. The hyperparameters of our proposed models are set as follows: #iterations = 300, K I = K S = 10, ? = ? = ? = 0.01, and ? = 1e -4 . For the initialization of models containing sentiment variables, we use MPQA subjectivity lexicon <ref type="foot" target="#foot_4">5</ref> to determine the initial sentiment assignments for words/sentences, as done in <ref type="bibr" target="#b8">[9]</ref>. For a word, its sentiment label is initialized by its sentiment polarity. For a sentence, we first find all of sentiment words in it and then use simple majority voting to determine its initial sentiment assignment.</p><p>For the baselines, LR, SVM, TRiTL, and DTL belong to supervised models because they use the labeled data in the source domain. CL-SCL and cotraining are semisupervised models because they employ auxiliary data except for training data. TSU, TCA, AOS(exact), and AOS are unsupervised models because they only use training and test data to learn high-level features without using the label information of source language data, and thus, a logistic regression classifier is trained on high-level features of reviews in the source domain and is employed to predict the labels of reviews in the target domain for all the unsupervised models. Compared with the abovementioned unsupervised topic models, PSCCLDA and ps-AOS are partially supervised models because they consider the label information of source language data when learning high-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Performance</head><p>It is obvious that ps-AOS is not only suitable for CLSC tasks but also suitable for cross-domain sentiment classifications. In this section, we show both cross-lingual and cross-domain sentiment classification experimental results. The classification accuracies are presented in Table <ref type="table" target="#tab_2">II</ref>. According to the Wilcoxon signed-rank test <ref type="bibr" target="#b43">[44]</ref> (z-score = -4.782 l), our ps-AOS significantly outperforms the second best method (AOS) on 30 kinds of cross-lingual and crossdomain tasks.</p><p>On average, for CLSC tasks, the accuracies of AOS and ps-AOS are improved up to 4% and 5.2%, respectively, compared with the second best cotraining. For cross-domain tasks, the improvements are 2.8% and 4.8%, respectively.</p><p>The ps-AOS model improves the averaged accuracy over traditional LR and SVM by at least 7%. Meanwhile, transfer learning methods (DTL and TRiTL) obtain better performance than LR and SVM, but their accuracies are lower about 6% than our model. CL-SCL utilizes many auxiliary unlabeled target language examples, so its accuracy is higher than machine-translation methods and transfer learning. However, CL-SCL is lower than cotraining and our method in terms of accuracy. Our ps-AOS model only takes as input the source language examples and the translated target language ones. We can observe that comodeling two domains helps improve classification accuracy significantly from the results shown in Table <ref type="table" target="#tab_2">II</ref>. Compared to cotraining, our ps-AOS model improves about 5% in terms of accuracy because it provides an informative high-level latent feature representation for reviews, rather than noisy raw word representation.</p><p>For unsupervised topic models, TSU performs much better than TCA and PSCCLDA because TSU is a fine-grained model and it can capture more accurate latent features. It also shows that fine-grained model performs better for sentiment classification task. Therefore, we also adopt fine-grained model for cross-domain situations, where topics are divided into common and specific topics and each topic is further subdivided by sentiment, aspect, and opinion. For classification, each topic with different sentiment polarities is considered as different latent feature representations of reviews. Due to the representation flexibility introduced by coarse alignment and partial supervision introduced by logistic regression, the accuracy is improved about 11% by ps-AOS, compared with TSU. For cross-domain models, TCA and PSCCLDA achieve the worst performance in general. This is because TCA and PSCCLDA belong to a coarse-grained document-level model and thus fail to capture the aspects and sentiment details of each review and the exact alignment restricts the representation ability of the models and results in worse accuracy when the source and target domains have different distributions.</p><p>To verify the effectiveness of the coarse alignment mechanism and partial supervision, we compare ps-AOS with our proposed baseline models [AOS and AOS(exact)] under the same parameter settings. The only difference between AOS and AOS(exact) is the topic alignment mechanism: coarse alignment versus exact alignment. As shown in Table <ref type="table" target="#tab_2">II</ref>, we can observe that coarse alignment improves the averaged accuracy over AOS(exact) by 7%. Compared with AOS, the partial supervision (ps-AOS) further improves the classification accuracy by 1.2% and 2% for cross-lingual and crossdomain tasks, respectively, From the abovementioned experiments, we can observe that our ps-AOS not only constructs a fine-grained model to help capture more accurate feature representations across different domains but also employs coarse alignment and partial supervision to help minimize the differences of latent feature representations across domains for CLSC tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mining Aspects, Opinions, and Sentiments</head><p>Besides classification, ps-AOS can mine aspects, opinions, and sentiment polarities, simultaneously. Table <ref type="table" target="#tab_3">III</ref> shows the top five words of randomly selected one common topic and two specific topics with different sentiment polarity from the English and Chinese DVD reviews data sets. Note that the indexes of two specific topics are the same.</p><p>First, we can observe that topics with different polarities focus on different aspects. For example, the common aspect with negative sentiment focuses on "quality" and "picture" of DVD products, whereas the common aspect with positive sentiment is about "people" and "story." Second, for common topics in different language domains, we find that the opinion words of the same aspect are different. For example, Chinese consumers express their negative opinions by using "stuck" and "garbage," which do not occur in English consumers' reviews. Third, the content differences are more obvious for specific topics. For instance, English customers mainly complain "people" or "character" of DVD products in the specific aspect with negative sentiment, whereas Chinese consumers mainly express their dissatisfaction about "quality" or "price." For example, we retrieve many sentences, such as "the dvd is broken" or "the price is expensive" from Chinese reviews. For positive sentiment, English customers praise for "story" or "books" of DVD, whereas Chinese consumers focus more on "picture" or "disc." Through the earlier observations, we can find the mined aspects and opinions with different sentiment polarities indeed reflect customers' different focuses on the same products and their opinion expression habits. The aspects, opinions, and sentiments mined by our model not only reveal the similarity and difference between different language consumers' reviews on the same products but also distinguish aspects with different sentiment as different latent features to strengthen the feature representation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proxy Distance of Feature Representation</head><p>Ben-David's theoretical work shows that PAD is a metric estimating the similarity of the source and target representations <ref type="bibr" target="#b18">[19]</ref>. Generally, a lower PAD value indicates a better representation between two domains. We obtain the PAD value as done in <ref type="bibr" target="#b39">[40]</ref> and verify that feature representations learned by ps-AOS in the source and target domains are difficult to distinguish. We calculate the averaged PAD values of 18 crosslingual and cross-domain tasks and compare ps-AOS with other cross-domain models (TCA <ref type="bibr" target="#b14">[15]</ref> and PSCCLDA <ref type="bibr" target="#b12">[13]</ref>).</p><p>According to the Wilcoxon signed-rank test <ref type="bibr" target="#b43">[44]</ref>, our ps-AOS significantly outperforms TCA and PSCCLDA in terms of PAD value on the 18 tasks at a significance level of 0.01 ( p-values are 0.0004 and 0.0002, respectively). As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, the averaged PAD values (three horizontal lines) of ps-AOS, PSCCLDA, and TCA are 0.215, 0.433, and 1.693, respectively. To illustrate it clearly, Fig. <ref type="figure" target="#fig_2">2 also</ref> shows the PAD values of different algorithms on randomly selected three tasks (EBFD, EDJB, and EMGB). We can observe that TCA, PSCCLDA, and ps-AOS all achieve lower PAD values than bag-of-words (BoW) representation. It means that the learned high-level latent feature representation helps minimize the semantic gap between the source and target domains. Compared with TCA and PSCCLDA, ps-AOS further achieves lower PAD values. These observations also explain that sentiment variable and partial supervision in ps-AOS both help learn a better feature representation between two language domains, which plays a critical role on the improvement of classification accuracy in the target language domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameters Analysis</head><p>In ps-AOS, there are four hyperparameters. Fig. <ref type="figure" target="#fig_3">3</ref>(a) and (b) shows the influence of hyperparameters on for EFB and EDGM tasks. First, we can observe that the classification accuracy increases as the values of ? and ? increase. This cates, to some degree, that larger ? and ? better approximate the true distributions of the data sets. Actually, larger ? and ? imply that the latent semantic of a review comprises more topics, and words in each topic have more co-occurrences <ref type="bibr" target="#b44">[45]</ref>. This is because larger ? and ?, hyperparameters for symmetric Dirichlet distributions, encode stronger assumption that ? and ?/? are uniformly distributed. From Fig. <ref type="figure" target="#fig_3">3</ref>(a) and (b), we observe that ? = 0.01 can achieve the best accuracy for both EFB and EDGM tasks. However, the best values of ? for both EFB and EDGM tasks are 0.001 and 0.01, respectively.</p><p>In addition, ? is the parameter of symmetric Beta prior distribution for ?, which is used to approximate the true sentimental distribution of sentences in each review, i.e., the proportions of POSITIVE/NEGATIVE sentiments in each review. Similarly, ? is the parameter of symmetric Beta prior distribution for ? , which is used to approximate the true topic distribution of common/specific topics in each review, i.e., the proportions of common/specific topics in each review. Generally, in each review d, ? d is review-dependent and is determined by the proportions of POSITIVE/NEGATIVE sentences in d, and ? d is also review-dependent and is determined by the proportions of common/specific topics in d. For example, for a review with a POSITIVE label, the proportion of positive sentences is larger than that of negative ones, and thus, smaller ? could fit the abovementioned situation better. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>(c), we can observe that the model achieves the best performance when the ratio of common topics is set to 0.9 in the same domain task (EFB) and 0.1 in cross-domain task (EDGM). The earlier observation indicates that the proportions of common/specific topics in each review should be uneven, and thus, larger ? in the EFB task [Fig. <ref type="figure" target="#fig_3">3(a)</ref>] and smaller ? in the EDGM task [Fig. <ref type="figure" target="#fig_3">3(b)</ref>] could fit the distribution of common/specific topics better. Therefore, for EFB task, the best values of ? and ? are 1e -4 and 1, whereas the best values for EDGM task are 0.01 and 1e -5 .</p><p>In addition to the four hyperparameters, the ratio of common topics and the total number of topics will also influence the performance of the model. Fig. <ref type="figure" target="#fig_3">3(c</ref>) shows the trend of accuracy when the ratio is changed from 0.1 to 0.9. In this experiment, the total number of topics is fixed as 20. It can be observed that our model performs best when the ratio of common topics falls in [0.6, 0.9] for EFB task. The potential reason behind this might be that the source and target domains are both about BOOK despite the language differences, and thus, it is reasonable that two domains share many common topics. For the EDGM task, the model achieves the best accuracy when the ratio of common topics is 0.1. We believe that this is due to the fact that source and target domains of EDGM task come from DVD and MUSIC, respectively, and thus, it is reasonable for these two domains to share only a small number of common topics. Fig. <ref type="bibr">3(d)</ref> shows the trend of accuracy when the total number of topics is changed from 4 to 40. Generally, the accuracy increases as the total number of topics increases. The potential reason is that the model may capture more semantic details about the reviews as the number of topics increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this article, we jointly model aspects, opinions, and sentiments through a coarse alignment in a partially supervised way for CLSC tasks. Through the proposed ps-AOS model, we can mine polarized cross-lingual topics and their corresponding opinions in a coarse alignment manner. Moreover, we adopt logistic regression to make full use of labeled data in the source domain to minimize the difference of latent feature representations between two domains. Experimental results demonstrate the effectiveness of our model over various kinds of baselines on CLSC tasks. However, our proposed AOS and ps-AOS models still have some limitations to be improved. For example, the number of common/specific topics across domains is predefined and data-dependent. In the future, we need some algorithms to help determine the number of common/specific from the training data automatically. Another improved direction is how to extend our model to solve multiclass cross-domain text classification problems, in which we need to replace the Bernoulli distribution of class labels with multinomial distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX THE MODEL OF AOS(EXACT)</head><p>In the appendix, we show the plate notation of AOS(exact) model and its corresponding inference results for triplet (z, r, l). Similar to the ps-AOS model, the math notations used are provided in Table <ref type="table">I</ref>, Fig. <ref type="figure" target="#fig_5">4</ref> shows the plate notation of AOS(exact), and the blue dashed arrows show the differences between AOS and AOS(exact). AOS(exact) assumes that topic z is generated first, and then, common/specific topic switcher r and sentiment variable l are generated. Thus, it forces the common and specific topics that share the same topic index to be aligned.</p><p>The inference result for triplet (z, r, l) in the AOS(exact) model is as follows:    is the number of sentences assigned to sentiment t of type j 's topic k. The rest of math symbols are the same as those in 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 )</head><label>2</label><figDesc>For each domain c, the following steps hold. a) Choose ? O c,l,z ?Dir (?) for common topic z and l. b) Choose ? A/O c,l,z ?Dir (?) for specific topic z and l. 3) For each document d, the following steps hold. a) Choose a domain indicator c (not shown). b) Choose ? d ?Beta(?). c) Choose ? d,l ?Dir (?), for each sentiment l. d) Choose ? d,l ?Beta(? ), for each sentiment l. e) If d is from the source language domain, draw a class label p d ? {-1, 1}?Bern(logi sti c (p d ? T zd )) //ignore this step for AOS model. f) For each sentence s, the following steps hold</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Plate notation of the proposed AOS and ps-AOS model, where AOS is the plate without the orange shaded region. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. PAD values on different feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Parameters analysis. (a) EFB task. (b) EDGM task. (c) Ratio comparison. (d) Total topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>P</head><label></label><figDesc>(z d,s = k, r d,s = j, l d,s = t|z ?(d,s) , r ?(d,s) , l ?(d,s) , y, w, x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Plate notation for AOS(exact).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>( j ) is the number of sentences assigned with type j (common/specific) topic in document d, C d, j (k) is the number of sentences assigned to topic k of type j in document d, and C d, j,k (t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>C d (t ) is the number of sentences assigned with label t in document d, C d,t ( j ) is the number of sentences assigned to type j (common/specific) topics with sentiment label t in document</figDesc><table><row><cell>d, C j with sentiment label t, C d,t, j (k) is the number of sentences assigned to topic k of type A,t, j,k (v) O,t, j,k and C (v)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>ACCURACIES (%) FOR CROSS-LINGUAL AND CROSS-DOMAIN SENTIMENT CLASSIFICATION TASKS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III TOP</head><label>III</label><figDesc>FIVE WORDS OF ASPECTS WITH CORRESPONDING OPINIONS AND SENTIMENTS ON ENGLISH AND CHINESE DVD REVIEWS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 15:42:32 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.uni-weimar.de/medien/webis/corpora/corpus-webis-cls-10/cls-acl10-processed.tar.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip Authorized licensed use limited to: Auckland University of Technology. Downloaded on June 07,2020 at 15:42:32 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Model and inference can be found in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://mpqa.cs.pitt.edu/lexicons</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">National Key Research and Development Program of China</rs> under Grant <rs type="grantNumber">2018YFB1402800</rs> and in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">71501003</rs>. The work of <rs type="person">Junjie Wu</rs> was supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2019YFB2101804</rs> and in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">71531001</rs>, Grant <rs type="grantNumber">71725002</rs>, and Grant <rs type="grantNumber">U1636210</rs>. The work of <rs type="person">Fuzhen Zhuang</rs> was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">U1836206</rs> and in part by the <rs type="funder">Project of Youth Innovation Promotion Association CAS</rs> under Grant <rs type="grantNumber">2017146</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KxccvC5">
					<idno type="grant-number">2018YFB1402800</idno>
				</org>
				<org type="funding" xml:id="_TC5JCb3">
					<idno type="grant-number">71501003</idno>
				</org>
				<org type="funding" xml:id="_hqHM9w9">
					<idno type="grant-number">2019YFB2101804</idno>
				</org>
				<org type="funding" xml:id="_bJDTRh8">
					<idno type="grant-number">71531001</idno>
				</org>
				<org type="funding" xml:id="_e56AZst">
					<idno type="grant-number">71725002</idno>
				</org>
				<org type="funding" xml:id="_sc66n5K">
					<idno type="grant-number">U1636210</idno>
				</org>
				<org type="funding" xml:id="_qNh3vys">
					<idno type="grant-number">U1836206</idno>
				</org>
				<org type="funding" xml:id="_EUE6paV">
					<idno type="grant-number">2017146</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chenguang Du is currently pursuing the master's degree in computer science with Beihang University, Beijing, China.</p><p>His main research interests include transfer learning and sentiment analysis. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crosslingual mixture model for sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-lingual opinion analysis via negative transfer detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="860" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A subspace learning framework for cross-lingual sentiment classification with partial parallel data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1426" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="540" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Triplex transfer learning: Exploiting both shared and distinct concepts for text classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction reweighting for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1682" to="1695" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A cross-lingual joint aspect/sentiment model for sentiment analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM Int. Conf. Conf. Inf. Knowl. Manage. (CIKM)</title>
		<meeting>23rd ACM Int. Conf. Conf. Inf. Knowl. Manage. (CIKM)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1089" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An unsupervised cross-lingual topic model framework for sentiment classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="444" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-cultural analysis of blogs and forums with mixed-collection topic models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1408" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A partially supervised cross-collection topic model for cross-domain text classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Conf. Inf. Knowl. Manage. (CIKM)</title>
		<meeting>22nd ACM Int. Conf. Conf. Inf. Knowl. Manage. (CIKM)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative dual-PLSA: Mining distinction and commonality across multiple domains for text classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM Int. Conf. Inf. Knowl. Manage</title>
		<meeting>19th ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic correlation analysis for cross-domain text classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topic and sentiment unification maximum entropy model for online review analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. World Wide Web</title>
		<meeting>24th Int. Conf. World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="998" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Conf. Web Search Data Mining (WSDM)</title>
		<meeting>4th ACM Int. Conf. Web Search Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Proc. 18th ACM Conf. Inf. Knowl. Manage. (CIKM)</title>
		<meeting>18th ACM Conf. Inf. Knowl. Manage. (CIKM)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for out-of-domain documents</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD)</title>
		<meeting>13th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge transformation for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Int</title>
		<meeting>32nd Int</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="716" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting associations between word clusters and document classes for crossdomain text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. Data Mining</title>
		<meeting>SIAM Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-language Web page classification via dual knowledge transfer using nonnegative matrix trifactorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. ACM SIGIR Conf. Res. Develop. Inf. (SIGIR)</title>
		<meeting>34th Int. ACM SIGIR Conf. Res. Develop. Inf. (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concept learning for cross-domain text classification: A general probabilistic framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1960" to="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-bridge transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>53rd Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-based LSTM network for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributional correspondence indexing for cross-lingual and cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual distillation for text classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>55th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity: Are more languages better</title>
		<author>
			<persName><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Comput. Linguistics</title>
		<meeting>23rd Int. Conf. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang. Process</title>
		<meeting>Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accounting for burstiness in topic models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn</title>
		<meeting>26th Annu. Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A timedependent topic model for multiple text streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD)</title>
		<meeting>17th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics</title>
		<meeting>52nd Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross lingual sentiment analysis using modified BRAE</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
		<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2030" to="2096" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects and opinions with a maxent-lda hybrid</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast collapsed Gibbs sampling for latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
		<meeting>14th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="569" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-lingual adaptation using structural correspondence learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dem?ar</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1248547.1248548" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">His research focuses on text categorization and data mining for software engineering and machine learning. Baoyu Jing received the bachelor&apos;s degree from Beihang University</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heinrich ; Fraunhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igd</forename><surname>Darmstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tech</forename><surname>Germany</surname></persName>
		</author>
		<author>
			<persName><surname>Rep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016, and the master&apos;s degree from School of Science</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China; Pittsburgh, PA, USA; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2013. 2018. 2018</date>
		</imprint>
		<respStmt>
			<orgName>National Engineering Research Center for Science Technology Resources Sharing and Service, Beihang University ; Carnegie University</orgName>
		</respStmt>
	</monogr>
	<note>His main research interests include topic model and cross-domain transfer learning. Chenwei Lu received the master&apos;s degree in computer science from Beihang University. His main research interests include transfer learning sentiment analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
