<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
							<email>zhengliu1026@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Ltd. Co</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
							<email>shaoyx@bupt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained models have demonstrated superior power on many important tasks. However, it is still an open problem of designing effective pre-training strategies so as to promote the models' usability on dense retrieval. In this paper, we propose a novel pre-training framework for dense retrieval based on the Masked Auto-Encoder, known as RetroMAE. Our proposed framework is highlighted for the following critical designs: 1) a MAE based pretraining workflow, where the input sentence is polluted on both encoder and decoder side with different masks, and original sentence is reconstructed based on both sentence embedding and masked sentence; 2) asymmetric model architectures, with a large-scale expressive transformer for sentence encoding and a extremely simplified transformer for sentence reconstruction; 3) asymmetric masking ratios, with a moderate masking on the encoder side (15%) and an aggressivev masking ratio on the decoder side (50?90%). We pre-train a BERT like encoder on English Wikipedia and Book-Corpus, where it notably outperforms the existing pre-trained models on a wide range of dense retrieval benchmarks, like MS MARCO, Open-domain Question Answering, and BEIR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dense retrieval plays the fundamental role in many important web applications, like search engines and recommender systems <ref type="bibr" target="#b33">(Xiong et al., 2020;</ref><ref type="bibr" target="#b26">Qu et al., 2020)</ref>. By having semantically correlated query and document represented as spatially close embeddings with dual-encoders, dense retrieval can be efficiently conducted via approximate nearest neighbour search, e.g., product quantization <ref type="bibr" target="#b12">(Jegou et al., 2010)</ref> and HNSW <ref type="bibr" target="#b23">(Malkov and Yashunin, 2018)</ref>. In recent years, the pre-trained language models have been widely utilized as ?. Part of the work was done when Zheng Liu was affiliated with Microsoft Research Asia.</p><p>the backbone of dual-encoders <ref type="bibr" target="#b14">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b33">Xiong et al., 2020;</ref><ref type="bibr" target="#b22">Luan et al., 2021)</ref>. However, the mainstream pre-trained models, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, XLNET <ref type="bibr" target="#b35">(Yang et al., 2019)</ref>, and T5 <ref type="bibr" target="#b28">(Raffel et al., 2019)</ref>, are typically pre-trained with tokenlevel tasks, like masked language modeling and sequence-to-sequence. As a result, the sentencelevel representation capability is not be properly developed, which will probably restrict the quality of dense retrieval. Aware of the above defect, continuous effort has been made so as to better prepare pre-trained models for dense retrieval tasks. One popular strategy is to take advantage of self contrastive learning (SCL) <ref type="bibr" target="#b0">(Chang et al., 2020;</ref><ref type="bibr" target="#b11">Izacard et al., 2021;</ref><ref type="bibr" target="#b25">Ni et al., 2021;</ref><ref type="bibr" target="#b34">Xu et al., 2022)</ref>, where the models are pre-trained to recognize manually created positive samples from data augmentation. However, the SCL based methods can be limited by the data augmentation quality, and usually call for massive amounts of negative samples <ref type="bibr" target="#b34">(Xu et al., 2022;</ref><ref type="bibr" target="#b9">He et al., 2020)</ref>. Another popular strategy leverages auto-encoding (AE), which is freed from the requirements on data augmentation and negative sampling. The AE based strategy is differentiated in terms of reconstruction tasks: the existing methods leverage masked language modeling (MLM) <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref>, replaced token detection (RTD) <ref type="bibr" target="#b2">(Chuang et al., 2022)</ref>, and autoregression ( <ref type="bibr" target="#b21">(Lu et al., 2021;</ref><ref type="bibr" target="#b32">Wang et al., 2021)</ref>), whose impacts on reconstruction difficulty and data efficiency are highly different. So far, it is still an open problem of exploring more effective AE based pre-training algorithms.</p><p>In this paper, we propose a novel masked autoencoding (MAE) framework to pre-train retrievaloriented language models, known as RetroMAE (Figure <ref type="figure">1</ref>). The proposed pre-training framework not only simplifies the existing AE based methods, but also gives rise to surprisingly competitive per- formances on downstream dense retrieval tasks. In particular, RetroMAE is featured for the following critical components and strategies.</p><p>? Masked auto-encoding. The pre-training follows a novel masked auto-encoding process. The input sentence is polluted twice with two different masks. One masked input is used by the encoder, where the sentence embedding is generated. The other masked input is used by the decoder; and together with the sentence embedding, the original sentence is reconstructed.</p><p>? Asymmetric structure. RetroMAE adopts an asymmetric model structure. The encoder is a large-scale transformer, e.g., BERT, which is learned to generate a discriminative embedding for the input sentence. In contrast, the decoder follows an extremely simplified structure, e.g., one single layer of transformer, which learns to reconstruct an masked sentence.</p><p>? Asymmetric masking ratios. The encoder's input sentence is masked at a moderate ratio: 15%, which is the same as the typical MLM strategies. However, the decoder's input sentence is masked with a much more aggressive ratio: 50?90%.</p><p>The above designs of RetroMAE is favorable to pre-training effectiveness thanks to the following reasons. Firstly, the auto-encoding task becomes much more challenging compared with the existing methods. The auto-regression may attend to the prefix during the decoding process; and the conventional MLM merely has a small portion (15%) of the input tokens masked. By comparison, RetroMAE aggressively masks most of the input during the decoding process, forcing the in-depth semantics to be encoded within the sentence embedding so as to ensure the reconstruction quality. Besides, the decoder is merely an one-layer transformer; the extremely simplified network further increases the difficulty of autoencoding. Secondly, it ensures training signals to be fully generated from each pre-training sentence. For typical MLM style methods, the training signals may only be derived from 15% of the input tokens. Whereas for RetroMAE, the training signals can be derived from the majority of the tokens. Besides, knowing that the decoder only contains one-single layer, we propose the enhanced decoding with two-stream attention <ref type="bibr" target="#b35">(Yang et al., 2019)</ref> and position-specific attention mask <ref type="bibr" target="#b5">(Dong et al., 2019)</ref>, where the training signals can be derived from the entire input tokens.</p><p>We perform comprehensive experimental studies with popular dense retrieval benchmarks, such as MS MARCO <ref type="bibr" target="#b24">(Nguyen et al., 2016)</ref> and BEIR <ref type="bibr" target="#b31">(Thakur et al., 2021)</ref>. According to the evaluation results, RetroMAE notably improves both indomain and out-of-domain performance in comparison with the existing generic and retrievaloriented pretrained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>The related works are reviewed from two aspects: dense retrieval and pretrained language models.</p><p>Dense retrieval becomes increasingly popular in recent years. It represents query and document as embeddings within the same latent space, where the semantic relationship between query and document can be measured based on the embedding similarity. As a result, dense retrieval can be efficiently conducted leveraging approximate nearest neighbour search, like HNSW (Malkov and Yashunin, 2018) and Product Quantization <ref type="bibr" target="#b12">(Jegou et al., 2010</ref>). The encoding model, i.e., the dual encoder, is fundamental to the retrieval quality. With the progress of deep learning, the model architecture is being continuously evolving, from simple linear transformations <ref type="bibr" target="#b10">(Huang et al., 2013)</ref> to CNNs <ref type="bibr" target="#b29">(Shen et al., 2014)</ref>, RNNs <ref type="bibr" target="#b15">(Kiros et al., 2015)</ref>, etc. The adventure of large-scale pre-trained language models, e.g., BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, T5 <ref type="bibr" target="#b28">(Raffel et al., 2019)</ref>, brings about a major leap-forward for dense retrieval. Thanks to the equipment of such big expressive models, the retrieval quality has been substantially enhanced <ref type="bibr" target="#b14">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b22">Luan et al., 2021;</ref><ref type="bibr" target="#b19">Lin et al., 2021)</ref>.</p><p>One important feature about the pre-trained language models is that the pre-training tasks are highly differentiated. One common practice is the masked language modeling (MLM) as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, where the masked tokens are predicted based on the rest of context. The basic MLM is extended by tasks like entity masking, phrase masking and span masking <ref type="bibr" target="#b30">(Sun et al., 2019;</ref><ref type="bibr" target="#b13">Joshi et al., 2020)</ref>, which substantially contribute to the sequence labeling applications, such as entity resolution and question answering. Besides, tasks like auto-regression <ref type="bibr" target="#b27">(Radford et al., 2018;</ref><ref type="bibr" target="#b35">Yang et al., 2019)</ref> and sequence-to-sequence <ref type="bibr" target="#b28">(Raffel et al., 2019;</ref><ref type="bibr" target="#b17">Lewis et al., 2019)</ref> are also utilized, where the pre-trained models can be better fit into NLG related applications. However, all these generic approaches focus on token-level language modeling, where the sentence representation capability is not effectively developed <ref type="bibr" target="#b0">(Chang et al., 2020)</ref>. As a result, it usually calls for a large scale of labeled data <ref type="bibr" target="#b24">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b16">Kwiatkowski et al., 2019)</ref> and sophisticated finetuning strategies <ref type="bibr" target="#b33">(Xiong et al., 2020;</ref><ref type="bibr" target="#b26">Qu et al., 2020)</ref> to guarantee the pre-trained models' performance on dense retrieval.</p><p>To mitigate the above problem, recent works target on pre-training retrieval oriented language models. The mainstream approaches are based on two different strategies: self-contrastive learning (SCL) and auto-encoding (AE). The SCL based methods <ref type="bibr" target="#b0">(Chang et al., 2020;</ref><ref type="bibr" target="#b8">Guu et al., 2020;</ref><ref type="bibr" target="#b34">Xu et al., 2022)</ref> require specific data augmentation, e.g., inverse cloze task (ICT), where positive samples are generated for each anchor sentence; then, the language model is trained to discriminate the positive samples from the negative ones via contrastive learning. However, the self-contrastive learning usually calls for huge amounts of negative samples, which is computationally challenging; besides, the pre-training effect can be severely restricted by the quality of data augmentation. The AE based methods are free from these restrictions, where the language models are learned to reconstruct the input sentence based on the sentence embedding. The existing methods utilize various reconstruction tasks, such as MLM <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref>, auto-regression <ref type="bibr" target="#b21">(Lu et al., 2021;</ref><ref type="bibr" target="#b32">Wang et al., 2021)</ref>, and token replace detection <ref type="bibr" target="#b2">(Chuang et al., 2022)</ref>, etc, which are quite differentiated in terms of data efficiency and reconstruction difficulty. For example, MLM learns from the masked tokens, which only consist 15% of each sentence; in contrast, the training signals can be derived from all the input tokens with auto-regression. Besides, the conventional auto-encoding leverages a large-scale decoder <ref type="bibr" target="#b18">(Li et al., 2020;</ref><ref type="bibr" target="#b32">Wang et al., 2021)</ref>; while in <ref type="bibr" target="#b21">(Lu et al., 2021)</ref>, a smaller decoder is used, which increases the reconstruction difficulty. Ideally, the auto-encoding should be data efficient, which ensures the pre-training corpus to be fully leveraged; meanwhile, it should also be made sufficiently challenging, which forces sentence embeddings to capture the in-depth semantics about the input sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose the following masked auto-encoding workflow for the retrieval-oriented pre-training. The model consists two components: a BERTlike transformer ? enc (?) for sentence representation, and a light-weight transformer ? dec (?) for sentence reconstruction. An input sentence X is masked as Xenc and encoded for the sentence embedding h X ; the sentence is masked once again as Xdec , and together with h X , the original sentence X is decoded. Detailed elaborations about the above workflow is illustrated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding</head><p>The input sentence X is polluted as Xenc for the encoding stage, where a small fraction of its tokens are randomly replaced by the special token [M] (Figure <ref type="figure">1</ref>. A). We apply a moderate masking ratio, 15% by default, where the majority of information about the original sentence can be preserved. Then, the encoder ? enc (?) is used to map the polluted input as the sentence embedding h X :</p><formula xml:id="formula_0">h X ? ? enc ( Xenc ).</formula><p>(1)</p><p>We leverage a BERT-like transformer for the encoding operation, i.e., 12 layers and 768 hiddendimension, where the in-depth semantic about the input may get effectively captured. Following the common practice, we select the [CLS] token's last-layer hidden state as the sentence embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding</head><p>The input sentence X is polluted once again as Xdec for the decoding stage (Figure <ref type="figure" target="#fig_0">2</ref>. B). The</p><formula xml:id="formula_1">? ! " + ? ? ! ? " ? # ? $ ? P # P $ P % P &amp; ? !</formula><p>? "</p><formula xml:id="formula_2">P # P $ P % P &amp; ? # ? $ ? " + ? mask 1 2 3 4 1 2 3 4 Encoder CLS ? ! [M] ? $ ? &amp; ? , ? '() ? " ? ! [M] [M]</formula><p>? " masking ratio is much more aggressive compared with the one used by the encoder, where up to 50?90% of the input tokens will be masked. The masked input is joined with the sentence embedding, based on which the original input sentence can be reconstructed by the decoder. Particularly, the sentence embedding and the masked input are combined into the following sequence:</p><formula xml:id="formula_3">P * P # P $ P % P &amp; ? # ? $ , ? +') (A) Encoding (B) Decoding (C) Enhanced decoding</formula><formula xml:id="formula_4">h X ? E Xdec = [h X , e x 1 , ..., e x N ].<label>(2)</label></formula><p>In the above equation, e * denotes the word embedding; x 1 equals to the original token value if it is not masked, otherwise <ref type="bibr">[M]</ref>. We add extra position embeddings P to the above sequence, which form the decoder's input. Finally, the decoder ? dec is learned to reconstruct the original sentence X by optimizing the following objective:</p><p>min .</p><formula xml:id="formula_5">x i ?masked CE(x i |h X ? E Xdec + P),<label>(3)</label></formula><p>where CE indicates the cross-entropy loss. We adopt a light-weight network, i.e., an one-layer transformer by default, for the decoding operation.</p><p>Given the aggressively masked input and the extremely simplified decoding network, it will force the generation of high-quality sentence embedding such that the original input can be reconstructed with good fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhanced Decoding</head><p>One shortcoming about the above decoding process is that the training signals, i.e., the crossentropy loss, may only be derived from the masked tokens, instead of the entire input tokens. Besides, each masked token is reconstructed based on the same context, i.e., h X ? E Xdec + P. We argue that the pre-training effect can be further enhanced if 1) more training signals can be derived from the input, and 2) the reconstruction task can be performed based on different contexts. To this end, we propose the following enhanced decoding inspired by two-stream attention <ref type="bibr" target="#b35">(Yang et al., 2019)</ref> and position-specific attention mask <ref type="bibr" target="#b5">(Dong et al., 2019)</ref>. Particularly, we will have the following two input sequences: H 1 and H 2 , for decoding (Figure <ref type="figure" target="#fig_0">2</ref>. C):</p><formula xml:id="formula_6">H 1 ? h X + P, H 2 ? E X + P,<label>(4)</label></formula><p>where h X is the sentence embedding, E X is the word embedding for the original tokens, and P is the position embedding. Then, we introduce the position-specific attention mask M ? L?L, based on which the self-attention is performed as:</p><formula xml:id="formula_7">Q = H 1 W Q , K = H 2 W K , V = H 2 W V ; M ij = 0, can be attended, -?, masked; A = softmax( Q T K ? d + M)V.</formula><p>(5)</p><p>The output A, together with h X (because of the residual connection) are used to reconstruct the original input (other operations in transformers, like layer-norm and FFN, are omitted for simplicity.) Finally, the objective will be optimized:</p><p>min .</p><formula xml:id="formula_8">x i ?X CE(x i |A, h X )<label>(6)</label></formula><p>Knowing that the decoder only consists of one single transformer layer, each token x i can only be reconstructed based on the information which are visible to the i-th row of matrix M. In this place, the following rules are applied to generate the attention mask matrix M:</p><formula xml:id="formula_9">M ij = 0, x j ? s(X =i ), -?, otherwise.<label>(7)</label></formula><p>In the above equation, s(X =i ) represents the random sampling of the input tokens. The sampled tokens will be visible when reconstructing x i . The diagonal elements, i.e., x i for the i-th row, will always be excluded, which means they will always be masked; therefore, each token cannot attend to itself during the reconstruction stage. We summarize the pre-training workflow of the encoding and enhanced decoding as Algorithm 1, where the following features need to be emphasized. Firstly, the reconstruction task is challenging given the aggressive masking ratio and the lightweight network about the decoder. Secondly, we may derive abundant pre-training signals from the unsupervised corpus since all tokens within each input sentence can be used for the reconstruction task. Finally, the pre-training is made simple and efficient: 1) there are no requirements on sophisticated data augmentation and negative sampling, and 2) the training cost is maintained to be comparable to the conventional BERT/RoBERTa style pre-training due to the simplicity of decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We explore the following issues in our experimental studies. 1) RetroMAE's impact on dense retrieval, in comparison with the generic pre-trained language models and the retrieval-oriented pretrained models. 2) The impact resulted from the four key factors in RetroMAE, the enhanced decoding, the size of decoder, the decoder's masking ratio, and the encoder's masking ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>The following datasets are utilized for the pretraining and evaluation of RetroMAE.</p><p>? Pre-training. We reuse the same pre-training corpus as the ones utilized by BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, i.e., the English Wikipedia and Book-Corpus. Both datasets are also frequently leveraged by previous works on retrieval-oriented pretraining, such as SEED <ref type="bibr" target="#b21">(Lu et al., 2021)</ref> and Condenser <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref>.</p><p>? Supervised evaluation. We make use of the MS MARCO passage retrieval dataset <ref type="bibr" target="#b24">(Nguyen et al., 2016)</ref> to evaluate RetroMAE's performance after supervision. It is one of the large-scale datasets for dense retrieval evaluation, which consists of real-world questions from Bing search. The questions are paired with their corresponding passages from web documents, where human annotated ground-truth answers to the questions are included. RetroMAE is fine-tuned on its training set and evaluated on its dev set.</p><p>? Zero-shot evaluation. We evaluate Retro-MAE's zero-shot retrieval performance on top of the recently released BEIR benchmark <ref type="bibr" target="#b31">(Thakur et al., 2021)</ref>. It contains a total of 18 datasets, covering dense retrieval tasks across different domains, such as question answering, fact checking, bio-medical retrieval, news retrieval, and social media retrieval, etc.</p><p>We consider three categories of baseline methods 1 in our experimental studies.</p><p>? Generic models. The following generic pretrained language models are included in our experiments. 1) BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, which is the most popular pre-trained language model in practice. It is also widely used as the backbone for the fine-tuning of dense retrievers <ref type="bibr" target="#b14">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b33">Xiong et al., 2020)</ref>. 2) RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref>, which is an enhanced replication of BERT with improved training settings and substantially augmented training data. 3) ELECTRA <ref type="bibr" target="#b3">(Clark et al., 2020)</ref>, introduces the generator-discriminator framework and the token replacement prediction task to further improve the pre-training effect.</p><p>? Constrastive learning. The following contrastive learning based methods are considered. 1) SimCSE <ref type="bibr">(Gao et al., 2021)</ref>, where the language model is learned to discriminate different views of the anchor sentence augmented by drop-1. We use the original checkpoints released by the authors.  <ref type="bibr" target="#b8">(Guu et al., 2020;</ref><ref type="bibr" target="#b0">Chang et al., 2020)</ref> with the alternative training of query and document encoder, where the scale of negative samples can be greatly increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>? Auto-encoding. We also make comparison with three pre-trained models following the auto-encoding framework. 1) Condenser <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref>, where the sentence embedding is joined with the intermediate hidden-states from encoder for the masking language modeling task. 2) SEED, in which the sentence embedding is used to reconstruct the original sentence via auto-regression. 3) DiffCSE <ref type="bibr" target="#b2">(Chuang et al., 2022)</ref>, which is a combination of SimCSE and auto-encoding based pre-training: for one thing, the sentence embedding is learned by contrastive learning as SimCSE; for another thing, the sentence embedding is applied to the ELETRA-style pre-training, i.e., the prediction of replaced tokens generated by a generator.</p><p>The implementation settings are specified as follows. The encoder backbone of RetroMAE is the same as BERT base, i.e., with 12 layers, 768 hidden-dimensions, and a vocabulary of 30522 tokens. The decoder is a one-layer transformer. The encoder masking ratio is 0.5, and the decoder masking ratio is 0.15. The model is trained for 8 epochs, with the AdamW optimizer, a batch-size of 32 for each device, and a learning rate of 1e-2. The original LaPraDoR is an ensemble of dense retriever and BM25. We preserve its released dense retriever for our experimental studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The training is performed on a cluster of 8? Nvidia A100 (40GB) GPUs. The model is implemented with PyTorch 1.8 and HuggingFace transformers 4.16. The evaluation settings are specified as follows. Both RetroMAE and the baselines are fine-tuned via DPR <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref> for their supervised performance on MS MARCO.</p><p>The fine-tuned models on MS MARCO are directly applied to the BEIR benchmark for their zero-shot retrieval performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Main Results</head><p>The supervised dense retrieval performances on MS MARCO are measured by MRR and Recall, whose results are shown as Table <ref type="table" target="#tab_2">1</ref>. The baselines methods are partitioned into three groups according to whether they are generic pre-trained models, contrastive-learning based models, or autoencoding based models. It can be observed that RetroMAE achieves much better retrieval performance than the rest of baseline methods; for example, RetroMAE outperforms the strongest baseline performance by +1.44% on MRR@10, and by +2.24% on Recall@10. The zero-shot dense retrieval performances on BEIR benchmark are are measured by NDCG@10, whose results are shown as Table <ref type="table" target="#tab_4">2</ref>. Similar as what we find in denser, are generally better than other methods. Such an observation indicates that auto-encoding is probably a more suitable paradigm for the pretraining of retrieval-oriented language models. We would also attribute RetroMAE's advantage over other auto-encoding based methods to its higher data-efficiency and reconstruction difficulty. More analysis about this point will be made in the following discussions. Secondly, the contrastive learning based methods merely bring very limited improvements over the generic pre-trained models, as can be observed from the comparison between SimCSE, LaPraDoR and BERT in Table <ref type="table" target="#tab_3">1</ref> and<ref type="table" target="#tab_4">2</ref>. In fact, similar observations are also made by previous study <ref type="bibr" target="#b6">(Gao and Callan, 2021)</ref>: although contrastive learning may equip the pre-trained models with preliminary capability on dense retrieval, the advantage is almost wiped out when the models are fine-tuned with labeled data.</p><p>Thirdly, despite that RoBERTa and ELECTRA are proved to be more effective than BERT on generic NLU tasks, like GLUE and MRC, they are no better than BERT on dense retrieval scenarios. Such an observation validates once again that the conventional token-level pre-training contributes little to the models' dense retrieval capability; thus, retrieval-oriented pre-training is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Studies</head><p>We ablate RetroMAE based on MS MARCO, where the following factors are analyzed: 1) decoding method, 2) decoder's size, 3) decoder's masking ratio, 4) encoder's masking ratio. The experiment results are shown as Table <ref type="table" target="#tab_6">3</ref>, where the following observations can be made.</p><p>Firstly of all, we analyze the impact from the decoding method. It can be observed that the enhanced decoding outperforms the basic decoding with notable advantages. Such an empirical advantage can be explained by the higher data efficiency of the enhanced decoding. Particularly, the basic decoding (Section 3.2) samples 50% of tokens (the default masking ratio for decoder) for reconstruction, and all of the masked tokens are predicted based on the same context. In contrast, the enhanced decoding (Section 3.3) may use all of the input tokens for reconstruction, and each of the masked tokens is predicted based on a unique context sampled as Eq. 7. Therefore, the enhanced decoding may obtain augmented and diverse training signals from the input data.  Secondly, we analyze the impact from the size of decoder. In this place, we use two different decoders for comparison: 1) the decoder with onesingle transformer layer (H de = 1), and 2) the decoder with two transformer layers (H de = 2). It can be found that the smaller decoder, which increases the difficulty of input reconstruction, gives rise to better empirical performances.</p><p>Thirdly, we further analyze the impact from different masking ratios of the decoder (? de ), which is increased from 0.3 to 0.9. It can be observed that the decoder with an aggressive masking ratio, i.e., 0.5?0.7, results in a relatively better empirical performance. However, further increasing of the masking ratio, i.e., ? de = 0.9, does not bring in additional improvements. It is probably because an over aggressive masking ratio will discard too much necessary information to reconstruct the original sentence.</p><p>Lastly, we also make analysis for the encoder's masking ratio (? en ). It is quite interesting that a slightly improved masking ratio, i.e., ? en = 0.3, achieves better performances than the default one ? en = 0.15. Similar as the decoder's situation, an increased masking ratio on the encoder side will also increase the reconstruction difficulty. However, the empirical performance will not benefit from an even larger masking ratio; and the ideal value of ? en is smaller than ? de . This is because a too large ? en will prevent the generation of highquality sentence embedding, considering that too much useful information about the input sentence will be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discussion</head><p>We draw the following conclusions based on our experimental findings in this paper. Firstly, the auto-encoding framework demonstrates strong potential in pre-training retrieval-oriented language models, and RetroMAE brings in substantially improvements over the existing auto-encoding based methods. Secondly, RetroMAE's performance is optimized by the enhanced decoding strategy, the simplified network of decoder, and the proper setting of the masking ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose RetroMAE, which pretrains retrieval-oriented language models based on masked auto-encoding: the input sentence is randomly masked twice for encoder and decoder, respectively; the sentence embedding from encoder is joined with the masked input of decoder to reconstruct the original sentence. We take advantage of asymmetric model structure (full-scale encoder and simplified decoder) and asymmetric masking ratio (a moderate one for encoder and an aggressive one for decoder), which improves the difficulty of reconstruction. We also introduce the enhanced decoding with two-stream attention and position-specific attention mask, which increases the data efficiency of pre-training. The experimental studies on MS MARCO and BEIR benchmark validate RetroMAE's effectiveness, as significant improvements on retrieval quality can be achieved against the existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Masked auto-encoding workflow. (A) Encoding: the input is moderately masked and encoded as the sentence embedding (the green rectangle). (B) Decoding: the input is aggressively masked, and joined with the sentence embedding to reconstruct the masked tokens (the shadowed tokens). (C) Enhanced encoding: all input tokens are reconstructed based on the sentence embedding and the visible context in each row (defined in Eq. 7); the main diagonal positions are filled with -? (grey), and positions for the visible context are filled with 0 (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Supervised dense retrieval performance on MS MARCO.</figDesc><table><row><cell cols="2">MRR@10</cell><cell>MRR@100</cell><cell>Recall@10</cell><cell>Recall@100</cell><cell>Recall@1000</cell></row><row><cell>BERT</cell><cell>0.3170</cell><cell>0.3278</cell><cell>0.5801</cell><cell>0.8570</cell><cell>0.9598</cell></row><row><cell>RoBERTa</cell><cell>0.3139</cell><cell>0.3245</cell><cell>0.5595</cell><cell>0.8155</cell><cell>0.9351</cell></row><row><cell>ELECTRA</cell><cell>0.3136</cell><cell>0.3258</cell><cell>0.5638</cell><cell>0.8478</cell><cell>0.9579</cell></row><row><cell>SimCSE</cell><cell>0.3191</cell><cell>0.3307</cell><cell>0.5833</cell><cell>0.8537</cell><cell>0.9602</cell></row><row><cell>LaPraDoR</cell><cell>0.3193</cell><cell>0.3307</cell><cell>0.5907</cell><cell>0.8653</cell><cell>0.9699</cell></row><row><cell>SEED</cell><cell>0.3264</cell><cell>0.3374</cell><cell>0.5913</cell><cell>0.8535</cell><cell>0.9539</cell></row><row><cell>DiffCSE</cell><cell>0.3202</cell><cell>0.3311</cell><cell>0.5832</cell><cell>0.8561</cell><cell>0.9607</cell></row><row><cell>Condenser</cell><cell>0.3357</cell><cell>0.3471</cell><cell>0.6082</cell><cell>0.8770</cell><cell>0.9683</cell></row><row><cell>RetroMAE</cell><cell>0.3501</cell><cell>0.3606</cell><cell>0.6306</cell><cell>0.8890</cell><cell>0.9757</cell></row><row><cell cols="3">out. Despite its simplicity, SimCSE achieves quite</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">promising results on semantic textual similarity</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tasks. 2) LaPraDoR 2 (Xu et al., 2022), which is a</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">recently proposed unsupervised retriever on top of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">contrastive learning. It notably enhances the pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vious ICT based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, RetroMAE maintains notable advantages</cell></row><row><cell>over the baseline methods on almost every evalu-</cell></row><row><cell>ation dataset. All these observations verify Retro-</cell></row><row><cell>MAE's effectiveness as a retrieval-oriented pre-</cell></row><row><cell>training framework.</cell></row><row><cell>Besides the overall observations, more detailed</cell></row><row><cell>analysis about Table 1 and 2 are made as follows.</cell></row><row><cell>Firstly, the auto-encoding based methods, both</cell></row><row><cell>RetroMAE and baselines like SEED and Con-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot dense retrieval performances on BEIR benchmark (measured by NDCG@10).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of RetroMAE based on MS MARCO.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pretraining tasks for embedding-based large-scale retrieval</title>
		<idno type="arXiv">arXiv:2002.03932</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Diffcse: Difference-based contrastive learning for sentence embeddings</title>
		<author>
			<persName><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumen</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marin</forename><surname>Soljacic</surname></persName>
		</author>
		<author>
			<persName><surname>Shang-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.10298</idno>
		<idno>CoRR, abs/2204.10298</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Condenser: a pre-training architecture for dense retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08253</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09118</idno>
		<title level="m">Towards unsupervised dense information retrieval with contrastive learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimus: Organizing sentences via pre-trained modeling of a latent space</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04092</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pretrained transformers for text ranking: Bert and beyond</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="325" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09206</idno>
		<title level="m">Less is more: Pre-training a strong siamese encoder using a weak decoder</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sparse, dense, and attentional representations for text retrieval</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="329" to="345" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hern?ndez ?brego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07899</idno>
		<title level="m">Large dual encoders are generalizable retrievers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08191</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on conference on information and knowledge management</title>
		<meeting>the 23rd ACM international conference on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Abhishek Srivastava, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tsdae: Using transformer-based sequential denoising auto-encoder for unsupervised sentence embedding learning</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06979</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00808</idno>
		<title level="m">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06169</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
