<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Activity Recognition on an Accelerometer Embedded Mobile Phone with Varying Positions and Orientations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Handicom Lab</orgName>
								<orgName type="institution">TELECOM SudParis</orgName>
								<address>
									<addrLine>9, Rue Charles Fourier</addrLine>
									<postCode>91011</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Handicom Lab</orgName>
								<orgName type="institution">TELECOM SudParis</orgName>
								<address>
									<addrLine>9, Rue Charles Fourier</addrLine>
									<postCode>91011</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Handicom Lab</orgName>
								<orgName type="institution">TELECOM SudParis</orgName>
								<address>
									<addrLine>9, Rue Charles Fourier</addrLine>
									<postCode>91011</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shijian</forename><surname>Li</surname></persName>
							<email>shijianli@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Handicom Lab</orgName>
								<orgName type="institution">TELECOM SudParis</orgName>
								<address>
									<addrLine>9, Rue Charles Fourier</addrLine>
									<postCode>91011</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Activity Recognition on an Accelerometer Embedded Mobile Phone with Varying Positions and Orientations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F04725B12E34FE2106C0A320FD8C1A2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Activity recognition</term>
					<term>SVM</term>
					<term>mobile phone</term>
					<term>accelerometer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper uses accelerometer-embedded mobile phones to monitor one's daily physical activities for sake of changing people's sedentary lifestyle. In contrast to the previous work of recognizing user's physical activities by using a single accelerometer-embedded device and placing it in a known position or fixed orientation, this paper intends to recognize the physical activities in the natural setting where the mobile phone's position and orientation are varying, depending on the position, material and size of the hosting pocket. By specifying 6 pocket positions, this paper develops a SVM based classifier to recognize 7 common physical activities. Based on 10-folder cross validation result on a 48.2 hour data set collected from 7 subjects, our solution outperforms Yang's solution and SHPF solution by 5âˆ¼6%. By introducing an orientation insensitive sensor reading dimension, we boost the overall F-score from 91.5% to 93.1%. With known pocket position, the overall F-score increases to 94.8%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prevailing sedentary lifestyle in modern society has lead to various physical and mental diseases, such as obesity, coronary heart diseases, type II diabetes and depression, which request enormous medical cost. According to World Health Organization, there are at least 1.9 million people die as a result of physical inactivity annually <ref type="bibr" target="#b0">[1]</ref>. In U.S. alone, it leads to about 300, 000 preventable deaths and more than 90$ billion direct health cost annually <ref type="bibr" target="#b1">[2]</ref>. Even though people are aware of the benefits of exercises, there is a lack of external intervention which can properly bring the busy people out of the sedentary routine, thus an automatic and personal reminder will be very helpful if it can monitor one's physical activities and persuade people to participate in physical activities regularly at the right time and place.</p><p>Activity recognition technology is a key enabling technology to tackle this problem as it's able to monitor individual's physical daily activities and the lasting duration so as to estimate the calories consumed each day. Based on the consumed calorie, the system can provide recommendation and advices when they fail to complete enough exercise and also build systems to encourage people to conduct more activities <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref>. There are several ways to recognize people's daily activities. One way is using cameras to visually detect people's motion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>. The drawback of this solution is that to monitor a moving person, large number of cameras need be deployed with high cost. And also the system should be designed to aggregate the information from each camera and deal with the influential factors such as lighting condition, mounting distance and angel, which make the system very complicate and impractical. Another way is using personal companion devices such as mobile phones or watches with sensing and computing power embedded to detect physical activities. The merit of this solution is that we don't need to deploy additional devices and the system is simple and easy to use. Since people carry their personal companion devices all the time and have the full control of their own devices, thus those devices won't make the users feel intrusive or cause extra money burden. Out of the two companion devices, the watches are normally placed on the wrist. Since the casual moving of arms doesn't have a direct and obvious relationship with ongoing activities, also modern watches are still not powerful enough to do data processing, therefore personal watches have a lot of constraints in detecting one's physical activities. On the contrary, mobile phones are becoming increasingly intelligent and powerful. When they are carried by people in pockets or bags, they are moving with the pace of the human body, thus they appear to be the ideal platforms for detecting people's physical activities such as sitting, walking, running and etc. Modern mobile phones like iPhone or Nokia N97 are embedded with various sensors such as the accelerometer, approximity sensor, magnetometer, GPS and etc. Of all these embedded sensors, the accelerometer is commonly used for activity recognition. Although GPS could detect one's movement in terms of location and speed, it cannot tell the user moves in an accurate manner. In particular, GPS doesn't work inside buildings where people spend most of their time in. Therefore, using the accelerometer-embedded mobile phones to recognize people's physical activities becomes the primary choice among all the solutions.</p><p>With the accelerometer-embedded mobile phone, there are two possibilities to monitor people's physical activities. One is turning the mobile phone as a pedometer, measuring the step counts and calorie consumption <ref type="bibr" target="#b8">[9]</ref> for each user. The other is recognizing precise physical activities such as walking, running, bicycling, driving and etc. Apparently the pedometer solution is quite simple, it provides good indication for each user's calorie consumed. While it works well for the cases of walking, running, taking staircases, etc., it fails to estimate the calorie consumption correctly in the case of bicycling (helpful to the health but cannot be measured by pedometer). On the contrary, recognizing one's physical activities and the lasting duration can infer more accurate and comprehensive information about people's life style. Besides informing the calorie consumed more accurately, the activity patterns can inform users' preferences and habits, which can serve as the basis for further exercise recommendation. <ref type="bibr" target="#b9">[10]</ref> shows that 60% of men put their mobile phones in their pockets. With different clothes dressed each day, people are used to putting the mobile phone in different pocket (often the most convenient one). Depending on the position, material and size of the pocket, the mobile phones often have varying orientation, especially when the very pocket swings with human body. Till now, the prior work on activity recognition with accelerometer-equipped mobile devices assumes a fixed mobile phone position or certain orientation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17]</ref>, this assumption usually doesn't hold for the usual case of carrying the phone in the pocket. In this work we choose to recognize seven most representative daily activities that are strongly linked to physical exercises, and we intend to investigate the activity recognition issue assuming that the mobile phone is freely placed in one of the pockets. Under this assumption, the accelerometer sensor inside the phone will take the position and orientation associated with the moving pocket.</p><p>With the varying orientation of the mobile phone, the experienced force will cause varying effect on the three components of the acceleration signal <ref type="bibr" target="#b18">[17]</ref>. This paper attempts to propose an orientation independent sensor reading dimension which can relieve the effect of the varying orientation on the performance of the activity recognition. For the position variation of the mobile phone, besides training a single optimal SVM classifier for all seven physical activities in all the pocket positions, we would like to train an optimal classifier for each pocket location and hopefully can select the right classifier according to the mobile phone position detected in the future.</p><p>The rest of the paper is organized as follows: in Section 2, the related work about activity recognition using mobile or wearable devices is summarized. Then in Section 3, our design hypothesis is elaborated to set-up the stage for the research work. Section 4 presents the detailed design process for feature extraction and classification, aiming at developing an orientation insensitive algorithm. Section 5 describes the experimentation strategy to select the optimal size of the window as well as the optimal set of SVM parameters corresponding to different pocket position. In Section 6, the experimental results and analysis are provided to demonstrate the effectiveness of the proposed approaches for tackling the varying orientation and position issue. Finally, Section 7 gives the conclusions about the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Activity recognition with wearable sensors has been a hot research field in the last decade. Much research work has been done to recognize physical activities such as sitting, standing, running and so on for wellbeing management. In order to differentiate diverse activities or gestures, sensors are best placed at locations where the intrinsic characteristics of the target activities can be well captured. For example, an accelerometer placed in the ankle can measure the leg motion properly, and a barometers fixed on human body can detect the altitude change quite well. In a well cited paper by Bao et al <ref type="bibr" target="#b10">[11]</ref>, five biaxial accelerometers are placed simultaneously on the right ankle, the left thigh, the waist, the left upper arm and right wrist respectively. The work could distinguish not only the whole body movement like walking or running, but also those activities involving partial body movement such as standing still, folding laundry and brushing teeth, watching TV or reading. In <ref type="bibr" target="#b12">[12]</ref>, dozens of heterogeneous sensors are placed in various parts of the body to measure 18 different quantities, such as the acceleration of the chest and wrist, heart rate from finger, forehead and chest, temperature of the environment and skin. With the large number of sensing sources, even similar activities like Nordic walk and walk can be accurately distinguished. However, the drawbacks of all the multiple-sensor multiple-position solutions for activity recognition are high deployment cost and large deployment constraints, which lead to difficulty for real usage.</p><p>Compared to the multiple-sensor multiple-position solutions, putting multiple sensors in a single platform and one part of the human body is a preferable way for physical activity recognition. Apparently, all the sensors can only sense the information specific to the target part of the body, the ability to distinguish diverse activities decreases because the feature characteristics reflected in the attached position might be very similar. <ref type="bibr">Lester [13]</ref> mounts one board embedded with eight sensors on the shoulder to classify physical activities such as sitting, standing, walking (stairs) and etc. Ravi et al <ref type="bibr" target="#b14">[14]</ref> places an accelerometer embedded hoarder board near the pelvic region to detect similar activities, demonstrating the ability of distinguishing daily physical activities with a single accelerometer. To investigate the effect of the sensor position on the activity recognition performance, Maurer <ref type="bibr" target="#b15">[15]</ref> deploys a multi-sensor device eWatch in locations like the left wrist, belt, necklace, right trousers pocket, shirt pocket and bag, and compares the recognition performance in those cases. The eWatch contains a dual axes accelerometer, a light sensor, a temperature sensor and a microphone, the experimental results show that with the eWatch fixed in all the locations, activities like walking, standing, sitting and running can be well detected, but ascending stairs and descending stairs are difficult to be distinguished from walking. In all the above-mentioned cases, the position of the sensor device is known and predefined, the case of unknown device position and varying orientation is not considered.</p><p>To investigate the effect of varying sensor placement on activity recognition, Lester <ref type="bibr" target="#b17">[16]</ref> places a multi-sensor board on three representative locations, including the wrist, the waist and the shoulder. A general HMM model is built to recognize the physical activities for all three locations, in contrast to a separate HMM model for each location. The result shows that the general model performs slightly worse than separate models. However, as we can see, additional straps are usually needed to fix the device in the target location, causing fixed orientations of the platform, which is different from the common case of putting the phone in a normal pocket. To tackle the issue of varying orientations, Yang <ref type="bibr" target="#b18">[17]</ref> proposes to compute the vertical and horizontal component of each accelerometer sensor reading for compensating the effect of gravity, based on the gravity-related estimation work of Mizell <ref type="bibr" target="#b19">[18]</ref>. However, Yang's work doesn't show the performance improvement of using his approach, compared to the case without using the orientation-independent feature. In addition, his work doesn't consider influence of the varying positions of the sensor platforms. Baek <ref type="bibr" target="#b22">[21]</ref> proposes to eliminate the gravity component with a second-order Butterworth highpass filter(SHPF) and extract the motion acceleration component.</p><p>Different from the prior work, we intend to address the varying position and orientation issues simultaneously in accelerometer-based physical activity recognition. The research challenges are inspired by the commonly observed phenomena that most of the people put their mobile phones in one of their pockets, and the mobile phone's position and orientation are varying, depending on the position, material and size of the hosting pocket. For the varying orientation issue, we plan to extract features that are independent or insensitive to orientation change, for the activity classification. While for the pocket position change issue, we would like to develop two solutions: One is to build a single and robust SVM classifier for all physical activities in all the pocket positions, handling the unknown phone hosting pocket problem; the other is to train a separate SVM classifier for each pocket location and hopefully the system can select the right classifier according to the mobile phone position detected using certain techniques. Experiment result shows that our proposed method outperforms Yang's work and Baek's work for over 4%. By introducing an orientation insensitive sensor reading dimension, we boost the overall F-score from 91.5% to 93.1%. With known pocket position, the overall F-score slightly increases to 94.8%, which is in consistent with the result shown in <ref type="bibr" target="#b17">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Hypothesis</head><p>In this section, we set up the hypothesis for the considered problem. We define our hypothesis space in three dimensions, i.e., pocket locations, mobile phone orientations, and physical activities. The proposed method and the experiments in the following sections are limited in the defined hypothesis space. Generally speaking, there are six common pocket locations in people's daily costume that are frequently used to place mobile phones, including the two front pocket and two rear pockets on the trousers, and two front pockets on the coat, as shown in Fig <ref type="figure" target="#fig_0">1(a)</ref>. The size, shape and orientation of the pocket at each location may vary for different clothes such that the motion patterns of the mobile phone can be very different. For example, front pockets on casual trousers are usually looser and deeper than those on jeans. This will lead to higher vibration magnitude when walking or running. In this paper, we will investigate the influence of the six pockets locations and use location-specific classifiers to relieve such kinds of influences.</p><p>Based on the observation to a large number of people, we find that there are only a very limited number of orientations for a mobile phone to be placed into a pocket, since most people's habits and most pocket styles only fall into a few numbers of most common ones. As shown in Fig <ref type="figure" target="#fig_0">1(b)</ref>, people normally put the mobile phone into the front pocket of the trousers vertically, which can result in 4 possible orientations: upward facing out, downward facing out, upward facing in, and downward facing in. Noting that, after the mobile phone is placed inside the pocket, it may slip or rotate when the user is moving. We don't consider these uncertain cases in this paper. In the following sections, we will investigate the influences of the four mobile phone orientations and use an additional feature to relieve such kind of influences. In our real life, we observed that seven physical activities are conducted by people every day, including stationary, walking, running, bicycling, ascending stairs, descending stairs and driving. In this paper we aim to distinguish these seven physical activities with an accelerometer-embedded mobile phone. Noting that in our experimentation, some activities such as bicycling and driving require people to sit down upon something. In these cases, the mobile phone cannot be put in their rear pockets on the trousers and the experimental results for these cases are absent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Activity Recognition Methodology</head><p>The proposed activity recognition method based on an accelerometer-embedded mobile phone comprises the following three steps: 1) collecting and pre-processing the sensor data from mobile phones, 2) extracting features, and 3) training classifiers. It is worth noting that in the first step, we add an additional sensor reading dimension, named acceleration magnitude, to enhance the insensitivity to the influences of the phone orientation. And in the third step, we train location-specific SVM classifiers to adapt the different pocket locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sensor Data Preprocessing</head><p>The embedded triaxial accelerometer inside a mobile phone can continuously sample the experienced accelerations at each sampling interval and produce 3-D acceleration readings A = (a x , a y , a z ), which are measures of the acceleration experienced in the three orthogonal axes: X-axis, Y-axis and Z-axis. Taking the Nokia mobile phone for example, the coordinate system with respect to the phone body is shown in Fig <ref type="figure" target="#fig_0">1(c</ref>). When the orientation of the phone body changes, the coordinate system will rotate accordingly and the readings at the three axes will change. Since the acceleration magnitude is a measure for the quantity of acceleration and has no directions, it is insensitive for the orientations of the mobile phones. Furthermore, we will show it is also a discriminative feature for the considered physical activities in the experimental results. As the exact orientation of the acceleration is unknown, to relieve the influences of the phone orientations, we add an additional orientation insensitive feature, i.e., the magnitude of A to the sensor readings. The sensor readings at each time slice thus becomes to be a 4-D vector A = (A, A ) = (a x , a y , a z , a x , a y , a z ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Extraction</head><p>We use a half overlapping sliding window to separate the collected sensing data into a number of windows. Then, each window is further divided into multiple frames as shown in Fig 2 . Instead of fixing the number of frames inside a window, we choose the frame size one second, as the collected sensing data shows that one second is long enough to comprise more than one footstep. When a window contains more footsteps, it takes information from longer time to make a classification decision, which is intuitively capable of eliminating short time noise. Various kinds of features of the accelerometer sensing data have been investigated in previous activity recognition work, including Mean, Variance, Correlation, Energy, Frequency-Domain Entropy, Cepstral Coefficients, Log FFT Frequency Bands and etc <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b20">19]</ref>. When the application scenarios change, the contributions of these features may change accordingly. To the best of our knowledge, little general analysis of the contributions has been reported. Applying more features may bring benefits to the recognition accuracy in the case of computing on the powerful computers. However, when we are trying to implement these features inside the resource and power limited mobile phones, we should try to avoid the features that need complex computing workload, since it consumes much of computing resources and energy, which is critical to the user experience and acceptance of such application.</p><p>Five types of features are employed in this work, including Mean, Variance, Correlation, FFT Energy and Frequency-Domain Entropy, as they have shown good performance in <ref type="bibr" target="#b20">[19]</ref>. For each frame, there are 22 features (4 features for   <ref type="table" target="#tab_0">1</ref>, where m is the number of the windows and n is the number of elements in a feature vector for a window. For the cth column -â†’ t c (c = 1, 2, . . . , n) in T , the maximum value M ax( -â†’ t c ) and minimum value M in( -â†’ t c ) are selected to scale the column to [0, 1] with equation <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">-â†’ t ci = -â†’ t ci -M in( -â†’ t c ) M ax( -â†’ t c ) -M in( -â†’ t c ) , i = 1 . . . m (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SVM Classification and Evaluation Metrics</head><p>In this paper, we adopt LibSVM <ref type="bibr" target="#b21">[20]</ref> to perform SVM training and classification within our experimentation. We use the RBF (Radial Basis Function) kernel and choose the optimal tradeoff parameter C and the bandwidth Gamma in RFB kernel by conducting a grid search with cross validation using the gird.py python script provided with LibSVM. We use the balanced F-score as the performance index to evaluate the experiment results. The definition is as following:</p><formula xml:id="formula_1">F-score = 2 * precision * recall precision + recall . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>For the test result, the F-score is calculated for each activity and the overall F-score for the classification model is computed by averaging the F-scores for all the activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimentation Methodology</head><p>We program a python application to collect accelerometer information from two Nokia N97s to conduct the experimentation. The data sampling rate is reduced to 10Hz by getting the mean of the data from 0.1 seconds. In order to ease the data labeling work, we build a simple touch screen user interface to label the data when launching the application as shown in Fig 3 . Before recording data, the application lables the test with user's selection of the activities, pocket locations and phone orientations. After successfully launching the application, testers put the mobile phone into the right pocket with the chosen orientation and start to conduct the selected activity.</p><p>In order to fully investigate the influences of the orientation, we asked the test subjects to test the aforementioned 4 possible orientations for each pocket and each activity. Noting that when dealing with front pockets of the coat, the mobile phone is horizontally facing the body instead of vertically facing the body, which is different with the trousers scenario. One female and six males test subjects aged 25âˆ¼46 were volunteered to conduct the experiment from Institute TELECOM SudParis during a period of three weeks. Before conducting the experiments, an introduction of how to use the application was given to the test subjects. Before each test, they were only given instructions about in which pocket and in which orientation to put the mobile phone. There were no limitations for the clothes, such as whether to wear tight or loose clothes, or whether to wear a jeans or a pant. Each time the test subject carried two mobile phones. They launched the application in the mobile phone, selected the setups, put them into the target pockets and started to do the target application. When the test was finished, the test subject took out the mobile phone and stopped the application. A log file whose name contains information about the activity type, pocket locations, phone orientation and the starting time was produced with the contents of time stamps and accelerometer readings and stored in the mobile phone memory. During the experimentation, no concerns were given to the mobile phones.</p><p>The data at the beginning and the end of the log file was cut off since people need time to put the mobile phones inside the pockets and also take it out. Observations from the experiment suggest that the dirty data is about less than 10 seconds. So we exclude 10 seconds data from the beginning and the end of the log file. Totally about 48.2 hours sampling data is collected (Table <ref type="table" target="#tab_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result Analysis</head><p>We collect a data set of about 48.2 hours from the experimentation. The window size is chosen as 1 second as it's normally sufficient for more than 1 steps. In order to optimize the SVM model, a grid search is performed to choose the best Cost and Gamma paramters. The 10-folder cross-validation is used to evaluate the SVM models. We put all the test cases in one data set and then randomly divide it into 10 equal-sized folders. Each time we choose one folder as the test data set and the rest as the training data set. We train the SVM model with the training data set, evaluate it with the test data set and get the precision, recall and the F-score for each activity. After each folder is tested, we compute the average F-score of all the folders as the overall results for the activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Generic SVM Model Analysis</head><p>We train a generic SVM model to recognize these activities without the consideration of the exact pocket where the phone is in. The window length represents the time duration of the accelerometer data that is needed to distinguish an activity. When the frame size is fixed, with small window size, the decision is made with the information within short time duration and the features may be insufficient to describe an activity. On the contrary, with large length, the decision is made with large amount of features over long time duration and with limited training data, it may cause over-fitting problems. So there should be a suitable window length that could achieve the best tradeoff. To choose the best window length for the generic SVM model, we evaluate the window length from 1 to 6 seconds. The mean F-scores produced by the cross-validation for each window length are shown as the blue line in Fig 4 . One can see that the F-score achieves the peak 93.1% when the window length is 4 seconds. When the window length is smaller or larger, the F-score decreases.</p><p>In order to verify the classification contributions of the acceleration magnitude, we build generic SVM models without the acceleration magnitude as one element of the sensor readings for all the window lengths. The overall F-score  with respect to the window length is shown as the red line in Fig 4 . One can see that the overall F-scores with acceleration magnitude outperform those without it for all the frame sizes. It is also interesting to note that the optimal window length shifts to 5 seconds, which suggests that, without the orientation insensitive features, we need more time to achieve the best performance. The confusion matrix for the generic SVM model is shown in Table <ref type="table">4</ref> and the precision, recall and F-score of each acitivity are shown in Table <ref type="table">5</ref>.One can see that both the precision and recall of "running" is very high compared with other activities, meaning that it is quite distinguishable than other activities.</p><p>To compare with Yang's solution, we estimate the gravity acceleration by averaging the signal in each axis during a sampling interval of 10 seconds as proposed in Yang's work and use the result to compute the vertical and horizontal accelerations. To compare with Baek's work, we also use SHPF to filter the sampling data. then use the same feature extraction and parameter optimization process for SVM to conduct the activity recogntion work. The result is shown in Table <ref type="table" target="#tab_2">3</ref>. We can see that our method outperforms their method about 5âˆ¼6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Location Adaptation Analysis</head><p>We train location-specific SVM models with the experiment data from each pocket and compare the classification F-score with the generic SVM model for each activity. Fig <ref type="figure">5</ref> shows that the individual SVM model outperforms the generic SVM models for all the activities, meaning that with the location context of the mobile phone, we could train location-specific SVM classifiers to achieve orientation insensitive feature, the overall F-score is boosted to 93.1%. With known pocket position, the overall F-score increases to 94.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we investigated the physical activity recognition issue on an accelerometer-embedded mobile phone, considering most of people's habit of putting the mobile in one of the pockets. In contrast to the previous work of assuming that the phone is placed in a known position or fixed orientation, this paper intends to recognize the physical activities in the natural setting where the mobile phone's position and orientation are varying, depending on the position, material and size of the hosting pocket.</p><p>For the varying orientation issue, we add the acceleration magnitude as a new sensor reading dimension, which can relieve the effect of the orientation change on the performance of activity classification. While for the pocket position change issue, we develop two solutions: One is building a single and robust SVM classifier for all physical activities in all the pocket positions, handling the unknown phone hosting pocket problem; the other is training a separate SVM classifier for each pocket location so that the system can select the right classifier according to the mobile phone position detected. Five features including mean, variance, correlation, energy and entropy are extracted to build the SVM classifiers with optimized cost and gamma parameters.</p><p>By specifying six pocket positions for hosting the mobile phone, this paper targets to recognize seven common physical activities, including stationary, walking, running, bicycling, ascending stairs, descending stairs and driving. Based on 48.2 hours data collected from seven subjects, our method is shown to have better perfomance than the works in <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b22">21]</ref>. By adding the magnitude of the acceleration as the 4th data dimension for feature extraction, we manage to boost the overall F-score of SVM classifier to 93.1%. With the magnitude of the acceleration as the 4th data dimension for feature extraction and selecting a separate SVM classifier for each pocket position, the overall F-score of the classifiers for the seven activities can increase to 94.8%.</p><p>For the future work, we plan to implement other classification algorithms and compare their performance with that of the SVM-based classifiers presented in this paper. We also plan to build the exercise reminder application on top of the activity recognition algorithms, aiming to achieve the goal of prompting the mobile user at the right time, in the right place and the right manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Pocket locations. For each pocket shown, there is a corresponding one in the left side of the body. (b) Four phone orientations when users put the mobile phone into the right front jean's pocekt. (c) Coordinate system of the accelerometer in Nokia phones.</figDesc><graphic coords="6,41.82,131.88,345.35,84.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Half overlapping windows and frame definitions within a window</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Experiment interface on Nokia N97</figDesc><graphic coords="9,93.66,178.97,241.71,106.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The overall F-score for the generic SVM models w/ and w/o the acceleration magnitude</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The extracted feature matrix for the collected sensing data All the K frames inside a sliding window will produce a feature vector of 22 * K elements.Normalization is performed on the extracted feature vectors before training. All the extracted feature vectors form an m * n matrix T as shown in Table</figDesc><table><row><cell></cell><cell cols="4">feature vector</cell></row><row><cell></cell><cell cols="4">t11 t12 . . . t1n</cell></row><row><cell></cell><cell cols="4">t21 t22 . . . t2n</cell></row><row><cell>test cases</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell></cell><cell cols="4">tm1 tm2 . . . tmn</cell></row><row><cell cols="5">Mean, Variance, Energy, Frequency-Domain Entropy, respectively, and 6 features</cell></row><row><cell>for Correlation).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The sampling time of the each activity during the experimentation</figDesc><table><row><cell>Activity</cell><cell>Time(Hour)</cell></row><row><cell>Stationary</cell><cell>10.4</cell></row><row><cell>Walking</cell><cell>9.8</cell></row><row><cell>Running</cell><cell>6.3</cell></row><row><cell>Bicycling</cell><cell>6.6</cell></row><row><cell>Ascending stairs</cell><cell>4.6</cell></row><row><cell>Descending stairs</cell><cell>4.0</cell></row><row><cell>Driving</cell><cell>6.5</cell></row><row><cell>Total</cell><cell>48.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Recognition result comparison</figDesc><table><row><cell>Solution</cell><cell>Precision Recall F-score</cell></row><row><cell>Our solution</cell><cell>93.2% 93.0% 93.1%</cell></row><row><cell cols="2">Yang's solution 87.5% 87.0% 87.2%</cell></row><row><cell>SHPF</cell><cell>88.5% 88.0% 88.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Precision, recall and F-score comparisons</figDesc><table><row><cell></cell><cell cols="3">Overall Precision Overall Recall Overall F-score</cell></row><row><cell>Generic SVM without Magnitude</cell><cell>91.6%</cell><cell>91.4%</cell><cell>91.5%</cell></row><row><cell>Generic SVM with Magnitude</cell><cell>93.2%</cell><cell>93.0%</cell><cell>93.1%</cell></row><row><cell>Individual SVM with Magnitude</cell><cell>94.8%</cell><cell>94.8%</cell><cell>94.8%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledegement</head><p>The authors would like to thank Chanaphan Prasomwong and Wei Wang for building the data collection program with python in Nokia N97 and collecting experiment data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>higher accuracy. This result also implies that, compared with the generic SVM model, location-specific models only reflect the acceleration patterns from one fixed location and should have less uncertainty.</p><p>Table <ref type="table">6</ref> shows the overall Precision, Recall and F-score of the generic SVM model without acceleration magnitude, generic SVM model with acceleration magnitude and the individual SVM models. We can see that an overall F-score of 91.5% has been achieved for unknown pocket positions. By introducing an</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://www.who.int/moveforhealth/en/" />
	</analytic>
	<monogr>
		<title level="j">World Health Organization: Move for Health</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Escalating Pandemics of Obesity and Sedentary Lifestyle: A Call to Action for Clinicians</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Manson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Skerrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Vanitallie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activity Sensing in the Wild: A Field Trial of UbiFit Garden</title>
		<author>
			<persName><forename type="first">S</forename><surname>Consolvo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CHI</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fish&apos;n&apos;Steps: Encouraging Activitiy with an Interactive Computer Game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mamykina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lindtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Delajoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp 2006</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Dourish</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Friday</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4206</biblScope>
			<biblScope unit="page" from="261" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maitlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barkhuus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<title level="m">Shakra: Tracking and Sharing Daily Activity Levels with Unaugmented Mobile Phones. Mobile Networks and Applications</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="185" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Increasing the Awareness of Daily Activity Levels with Pervasive Computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maitland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barkhuus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Pervasive Health</title>
		<meeting>of Pervasive Health</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine Recognition of Human Activities: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on Visual Surveillance of Object Motion and Behaviors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics. Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">iPhone as a Physical Activity Measurement Platform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI 2010 USA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Where is the Phone? A Study of Mobile Phone Location in Public Spaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ichikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chipchase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second International Conference on Mobile Technology, Application and Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="797" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activity Recognition from User-Annotated Acceleration Data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PERVASIVE 2004</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Ferscha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Mattern</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3001</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activity Classification Using Realistic Data From Wearable Sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Parkka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Korpipaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mantyjarvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peltolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hybrid discriminative /generative approach for modeling human activities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hannaford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>of the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="776" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Activity Recognition from Accelerometer Data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1541" to="1546" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activity Recognition and Monitoring Using Multiple Sensors on Different Body Positions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smailagic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Of the International Workshop on Wearable and Implantable Body Sensor Netowrks (BSN 2006)</title>
		<meeting>Of the International Workshop on Wearable and Implantable Body Sensor Netowrks (BSN 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Practical Approach to Recognize Physical Activities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Borriello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PERVASIVE 2006</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nixon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Quigley</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3968</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward Physical Activity Diary: Motion Recognition Using Simple Acceleration Features with Mobile Phones</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMCE</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<pubPlace>Beijing; China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using gravity to estimate accelerometer orientation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mizell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC 2003, Proc. Of the 7th IEEE International Symposium on Wearable Computers</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gesture Recognition with a 3-D Accelerometer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIC 2009</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Portmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A.-H</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Indulska</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5585</biblScope>
			<biblScope unit="page" from="25" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<title level="m">LIBSVM: a library for support vector machines (2001) Software available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition of User Activity for User Interface on a Mobile Device</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th South East Asia Regional Computer Conference</title>
		<meeting>of the 24th South East Asia Regional Computer Conference<address><addrLine>Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
