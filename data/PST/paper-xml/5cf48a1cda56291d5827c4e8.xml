<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting Data with Mixup for Sentence Classification: An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-22">22 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
							<email>hongyu.guo@nrc-cnrc.gc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<addrLine>1200 Montreal Road</addrLine>
									<settlement>Ottawa</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
							<email>yymao@eecs.uottawa.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
							<email>zhangrc@act.buaa.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">BDBC</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting Data with Mixup for Sentence Classification: An Empirical Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-22">22 May 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1905.08941v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup <ref type="bibr" target="#b23">(Zhang et al., 2017)</ref>, a recent proposed data augmentation method through linearly interpolating inputs and modeling targets of random samples, has demonstrated its capability of significantly improving the predictive accuracy of the state-of-the-art networks for image classification. However, how this technique can be applied to and what is its effectiveness on natural language processing (NLP) tasks have not been investigated. In this paper, we propose two strategies for the adaption of Mixup on sentence classification: one performs interpolation on word embeddings and another on sentence embeddings. We conduct experiments to evaluate our methods using several benchmark datasets. Our studies show that such interpolation strategies serve as an effective, domain independent data augmentation approach for sentence classification, and can result in significant accuracy improvement for both CNN (Kim, 2014b)  and LSTM (Hochreiter and Schmidhuber, 1997) models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models have achieved state-of-theart performance on many NLP applications, including parsing <ref type="bibr" target="#b17">(Socher et al., 2011)</ref>, text classification <ref type="bibr" target="#b6">(Kim, 2014b;</ref><ref type="bibr" target="#b20">Tai et al., 2015)</ref>, and machine translation <ref type="bibr" target="#b19">(Sutskever et al., 2014)</ref>. These models typically have millions of parameters, thus require large amounts of data for training in order for over-fit avoidance and better model generalization. However, collecting a large annotated data samples is time-consuming and expensive.</p><p>One technique aiming to address such a data hungry problem is automatic data augmentation. That is, synthetic data samples are generated as additional training data for regularizing the learning models. Data augmentation has been actively and successfully used in computer vision <ref type="bibr" target="#b16">(Simard et al., 1998;</ref><ref type="bibr" target="#b10">Krizhevsky et al., 2017;</ref><ref type="bibr" target="#b23">Zhang et al., 2017)</ref> and speech recognition <ref type="bibr" target="#b4">(Jaitly and Hinton, 2015;</ref><ref type="bibr" target="#b8">Ko et al., 2015)</ref>. Most of these methods, however, rely on human knowledge for label-invariant data transformation, such as image scaling, flipping and rotation. Unlike in image, there is, however, no simple rule for label-invariant transformation in natural languages. Often, slight change of a word in a sentence can dramatically alter the meaning of the sentence. To this end, popular data augmentation approaches in NLP aim to transform the text with word replacements with either synonyms from handcrafted ontology (e.g., WordNet <ref type="bibr" target="#b24">(Zhang et al., 2015)</ref>) or word similarity (Wang and <ref type="bibr" target="#b22">Yang, 2015;</ref><ref type="bibr" target="#b9">Kobayashi, 2018)</ref>. Such synonym-based transformation, however, can be applied to only a portion of the vocabulary due to the fact that words having exactly or nearly the same meanings are rare. Some other NLP data augmentation methods are often devised for specific domains thus makes them difficult to be applied to other domains <ref type="bibr" target="#b15">(Sahin and Steedman, 2018)</ref>.</p><p>Recently, a simple yet extremely effective augmentation method Mixup <ref type="bibr" target="#b23">(Zhang et al., 2017)</ref>  How Mixup can be applied to and what is its effectiveness on NLP tasks? We here aim to answer these questions in this paper. In specific, we propose two strategies for the application of Mixup on sentence classification: one performs interpolation on word embedding and another on sentence embedding. We empirically show that such interpolation strategies serve as a simple, yet effective data augmentation method for sentence classification, and can result in significant accuracy improvement for both CNN <ref type="bibr" target="#b6">(Kim, 2014b)</ref> and LSTM <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997)</ref> models. Promisingly, unlike traditional data augmentation in NLP, these interpolation based augmentation strategies are domain independent, exclusive of human knowledge for data transformation, and of low additional computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Augmentation through Sentence Interpolation 2.1 MixUp for Image Classification</head><p>Zhang et al. <ref type="bibr" target="#b23">(Zhang et al., 2017)</ref> proposed the Mixup method for image classification. The idea is to generate a synthetic sample by linearly interpolating a pair of training samples as well as their modeling targets. In detail, consider a pair of samples (x i ; y i ) and (x j ; y j ), where x denotes the input and y the one-hot encoding of the corresponding class of the sample. The synthetic sample is generated as follows.</p><formula xml:id="formula_0">x ij = λx i + (1 − λ)x j<label>(1)</label></formula><formula xml:id="formula_1">y ij = λy i + (1 − λ)y j (2)</formula><p>where λ is the mixing policy or mixing-ratio for the sample pair. λ is sampled from a Beta(α, α) distribution with a hyper-parameter α. It is worth noting that, when α equals to one, then the Beta distribution is equivalent to an uniform distribution. The generated synthetic data are then fed into the model for training to minimize the loss function such as the cross-entropy function for the supervised classification. For an efficient computation, the mixing happens by randomly pick one sample and then pairs it up with another sample drawn from the same mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptation of Mixup for Sentence Classification</head><p>Unlike image which is consist of pixels, sentence is composed of a sequence of words. Therefore, a sentence representation is often constructed to aggregate information from a sequence of words. Specifically, in a standard CNN or LSTM model, a sentence is first represented by a sequence of word embeddings, and then fed into a sentence encoder. The most popular such encoders are CNN and LSTM. The sentence embedding generated by CNN or LSTM are then passed through a soft-max layer to generate the predictive distribution over the possible target classes for predictions.</p><p>To this end, we propose two variants of Mixup for sentence classification. The first one conducts sample interpolation in the word embedding space (denoted as wordMixup), and the second on the final hidden layer of the network before it is passed to a standard soft-max layer to generate the predictive distribution over classes (denoted as senMixup). The two models are illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, where the standard CNN <ref type="bibr" target="#b6">(Kim, 2014b)</ref> or LSTM <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997)</ref> model for sentence classification corresponds to the one without the red rectangle.</p><p>In specific, in the wordMixup, all sentences are zero padded to the same length and then interpolation is conducted for each dimension of each of the words in a sentence. Given a piece of text, such as a sentence with N words, it can be represented as a matrix B ∈ R N ×d . Each row t of the matrix corresponds to one word (denoted B t ), which is represented by a d-dimensional vector as provided either by a learned word embedding table or being randomly generated. Formally, consider a pair of samples (B i ; y i ) and (B j ; y j ), where B i and B j denotes the embedding vectors of the input sentence pairs and y i and y j denote the corresponding class labels of the samples using one-hot representation. Then for the t th word in the sentence, linear interpolation process can be formulated as:</p><formula xml:id="formula_2">B ij t = λB i t + (1 − λ)B j t</formula><p>(3)</p><formula xml:id="formula_3">y ij = λy i + (1 − λ)y j (4)</formula><p>The resulting new sample ( B ij ; y ij ) is then used for training.</p><p>In senMixup, the hidden embeddings (with the same dimension) for the two sentences are first generated by an encoder such as CNN or LSTM. Next, the pair of sentence embeddings are interpolated linearly. In specific, let f denote the sentence encoder, then a pair of sentences B i and B j will be first encoded into a pair of sentence embeddings f (B i ) and f (B j ), respectively. In this case, the mixing is conducted for each k th dimension of the sentence embedding, as follows.</p><formula xml:id="formula_4">B ij {k} = f (B i ) {k} + (1 − λ)f (B j ) {k} (5)</formula><formula xml:id="formula_5">y ij = λy i + (1 − λ)y j (6)</formula><p>Finally, the embedding vector B ij will be passed to a softmax layer to produce a distribution over the possible target classes. For training, we use multi-class cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We evaluate the proposed methods with five benchmark tasks for sentence classifications.</p><p>• TREC is a question dataset to categorize a question into six question types <ref type="bibr" target="#b11">(Li and Roth, 2002)</ref>.</p><p>• MR is a movie review dataset for detecting positive/negative reviews <ref type="bibr" target="#b13">(Pang and Lee, 2005)</ref>.</p><p>• SST-1 is the Stanford Sentiment Treebank with five categories labels <ref type="bibr" target="#b18">(Socher et al., 2013)</ref>.</p><p>• SST-2 dataset is the same as SST-1 but with neutral reviews removed and binary labels.</p><p>• Subj is a subjectivity detection dataset for classifying a sentence as being subjective or objective <ref type="bibr" target="#b12">(Pang and Lee, 2004)</ref>.</p><p>The summary of the data sets is presented in Table <ref type="table" target="#tab_0">1</ref>. Note that, for comparison purposes on the SST-1 and SST-2 datasets, following <ref type="bibr" target="#b6">(Kim, 2014b;</ref><ref type="bibr" target="#b20">Tai et al., 2015)</ref>, we trained the models using both phrases and sentences, but only evaluated sentences at test time.</p><p>We evaluate our wordMixup and senMixup using both CNN and LSTM for sentence classification. We implement the CNN model exactly as reported in <ref type="bibr">(Kim, 2014b,a)</ref>. For LSTM, we just simply replace the convolution/pooling components in CNN with standard LSTM units as implemented in <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. The final feature map of CNN and the final state of LSTM are passed to a logistic regression classifier for label prediction.</p><p>To evaluate our models in terms of their regularization effects on the training, we present four word embedding settings: random and trainable word embedding (denoted RandomTune), random and fix word embedding (denoted Random-Fix), pre-trained and tunable word embedding (denoted PretrainTune), and pre-trained fix word embedding (denoted PretrainFix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>c In our experiments, following the exact implementation and settings in <ref type="bibr" target="#b5">(Kim, 2014a)</ref> we use filter sizes of 3, 4, and 5, each with 100 feature maps; dropout rate of 0.5 and L2 regularization of 0.2 for the baseline CNN and LSTM. For datasets without a standard dev set we randomly select 10% of training data as dev set. Training is done through Adam (Kingma and Ba, 2014) over mini-batches of size 50. The pre-trained word embeddings are 300 dimensional GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>. The hidden state dimension for LSTM is 100.</p><p>For senMixup and wordMixup, the mixing policy α is set to the default value of one. Also, following the original Mixup <ref type="bibr" target="#b23">(Zhang et al., 2017)</ref>, we did not use dropout or L2 constraint for the word-Mixup and senMixup models.</p><p>We train each model 10 times each with 20000 steps, and compute their mean test errors and standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Results</head><p>RandomTune has the largest number of parameters, compared to RandomFix, PretrainTune, and PretrainFix, and thus requires a strong regularization method to avoid over-fit the training data. We, therefore, focus our experiments on the Random-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandomTune</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trec</head><p>SST-1 SST-2 Subj MR CNN-KIM Impl. <ref type="bibr" target="#b6">(Kim, 2014b)</ref> 91 Tune setting. The results on the RandomTune setting are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>The results in Table <ref type="table" target="#tab_1">2</ref> show that wordMixup and senMixup provide good regularization to CNN, resulting in accuracy improvement on all the five testing datasets. For example, in SST-1 and MR, the relative improvement was over 3.3%. Interestingly, both wordMixup and senMixup failed to significantly improved over the baseline against the SST-2 dataset; with senMixup slightly outperformed the baseline with only 0.7%. Also, results in Table <ref type="table" target="#tab_1">2</ref> suggest that senMixup and word-Mixup were quite competitively, in terms of predicticve performance obtained, against the five testing datasets. For example, on the Trec dataset, senMixup outperformed senMixup with 1.2%, but for the other four datasets, the two methods obtained very similiar predictive accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization Effect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM Networks as Sentence Encoder</head><p>We also evaluate the effect of using LSTM as the sentence encoder. Results in Table <ref type="table" target="#tab_2">3</ref> show that, similar to the case of using CNN as sentence encoder, word-Mixup and senMixup with LSTM as encoder also improved the predictive performance of the baseline models. For example, the largest improvements came from the Trec and SST-1 cases (with relative improvement of 4.62% and 5.22%), which have six and five classes, respectively. Results in the table also suggest that, on the Subj dataset, wordMixup outperformed senMixup with 1.2%, but for the other four datasets, the two methods performed comparably well.</p><p>One notable fact when compared with the CNNbased models as presented in Table <ref type="table" target="#tab_1">2</ref> is that, against the SST-2 data sets, both wordMixup and senMixup with LSTM here were able to improve over the baseline with about 2%.</p><p>Results for RandomFix, PretrainTune, and PretrainFix Results for the settings of Random-Fix, PretrainTune, and PretrainFix are presented in Tables 4, 5, and 6, respectively. Results in these three tables further confirm that data augmentation through wordMixup and senMixup can improve the predictive performance of the base mod-  els, except on the SST-2 dataset. On the SST-2 data set, both wordMixup and senMixup degraded the predictive accuracy of the baseline when the word embeddings were not allowed to be tuned during training. With learnable word embeddings, although both wordMixup and senMixup failed to significantly improve over the baseline on this dataset, but they did obtain similar predictive performance as the baseline. In short, our experiments here suggest that when the word embeddings are tuned, both wordMixup and senMixup are able to improve the predictive accuracy of the base models.</p><p>Inspired by the success of Mixup, a simple and effective data augmentation method through sample interpolation for image recognition, we investigated two variants of Mixup for sentence classification. We empirically show that they can improve the accuracy upon both CNN and LSTM sentence classification models. Interestingly, our studies here show that such interpolation strategies can serve as an effective, domain independent regularizer for overfitting avoidance for sentence classification.</p><p>We plan to investigate some lately proposed variants of Mixup, such as Manifold Mixup <ref type="bibr">(Verma et al.)</ref>, where interpolation is performed in a randomly selected layer of the networks, and AdaMixup <ref type="bibr" target="#b2">(Guo et al., 2018b)</ref>, which addresses the manifold intrusion issues in Mixup. We are also interested in questions such as what the mixed sentences look like and why interpolation works for sentence classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>has been proposed and shown superior performance on enhancing the accuracy of image classification models. Through linearly interpolating pixels of random image pairs and their training targets, Mixup generates synthetic examples for training. Such training has been shown to act as an effective model regularization strategy for image classification networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of wordMixup (left) and sen-Mixup (right), where the added part to the standard sentence classification model is in red rectangle.</figDesc><graphic url="image-1.png" coords="2,307.52,109.05,222.66,147.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We plot the training and testing cross-entropy loss across the first 12K training steps on the MR dataset in Figure 1. Figure 1 shows that with (top-left subfigure) or without (top-right subfigure) dropout, the training loss of CNN drops to zero quickly and provides no training signal for further tuning the networks. On the otherhand, the training loss of wordMixup (bottom-right subfigure) keeps above zero during the training, continuously provide training signal for the network learning. Also, the training loss curve of senMixup (bottom-left subfigure) maintains a relatively high level, allowing the model to keep tuning. The relatively higher training loss of both wordMixup and senMixup is due to the much larger space of the mixed samples, thus preventing the model from being over-fitted by limited number of individual examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training and testing entropy loss obtained by the baseline CNN without dropout (top-left), baseline CNN with dropout and L2 (top-right), wordMixup (bottom-right), and senMixup (bottom-left).</figDesc><graphic url="image-2.png" coords="5,96.49,63.28,401.91,237.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary for the datasets after tokenization.</figDesc><table><row><cell></cell><cell>l</cell><cell>N</cell><cell>V</cell><cell>Test</cell></row><row><cell cols="3">TREC 6 10 5952</cell><cell>9592</cell><cell>500</cell></row><row><cell cols="5">SST-1 5 18 11855 17836 2210</cell></row><row><cell cols="5">SST-2 2 19 9613 16185 1821</cell></row><row><cell>Subj</cell><cell cols="4">2 23 10000 21323 CV</cell></row><row><cell>MR</cell><cell cols="4">2 20 10662 18765 CV</cell></row></table><note>c: number of target labels. l: average sentence length. N: number of samples. V: vocabulary size. Test: test set size (CV means no standard train/test split was provided and thus 10-fold CV was used).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) of the testing methods using CNN (with randomly initialized, trainable embeddings). We report mean scores over 10 runs with standard deviations (denoted ±). Best results highlighted in Bold.</figDesc><table><row><cell></cell><cell>.2</cell><cell>45.0</cell><cell></cell><cell>82.7</cell><cell>89.6</cell><cell>76.1</cell></row><row><cell>CNN-HarvardNLP Impl. 1</cell><cell>88.2</cell><cell>42.2</cell><cell></cell><cell>83.5</cell><cell>89.2</cell><cell>75.9</cell></row><row><cell>CNN -Our Impl.</cell><cell cols="6">90.2±0.20 43.6±0.19 82.3±0.47 90.6±0.45 75.5±0.36</cell></row><row><cell>CNN+wordMixup</cell><cell cols="6">90.9±0.42 45.2±0.90 82.8±0.45 92.9±0.41 78.0±0.39</cell></row><row><cell>CNN+senMixup</cell><cell cols="6">92.1±0.31 45.2±0.22 83.0±0.35 92.7±0.38 77.9±0.76</cell></row><row><cell>RandomTune</cell><cell></cell><cell>Trec</cell><cell>SST-1</cell><cell>SST-2</cell><cell>Subj</cell><cell>MR</cell></row><row><cell cols="2">LSTM-StanfordNLP Impl. (Tai et al., 2015)</cell><cell>N/A</cell><cell>46.4</cell><cell>84.9</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">LSTM-AgrLearn Impl. (Guo et al., 2018a)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>90.2</cell><cell>76.2</cell></row><row><cell>LSTM -Our Impl.</cell><cell cols="6">86.5±0.61 45.9±0.58 84.4±0.35 90.9±0.42 77.2±0.75</cell></row><row><cell>LSTM + wordMixup</cell><cell cols="6">90.5±0.50 48.2±0.18 86.3±0.35 93.1±0.49 78.0±0.33</cell></row><row><cell>LSTM + senMixup</cell><cell cols="6">89.4±0.40 48.3±0.77 86.7±0.33 91.9±0.34 77.9±0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) obtained by the testing methods using LSTM (with randomly initialized, trainable embeddings). We report mean scores over 10 runs with standard deviations (denoted ±). Best results highlighted in Bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>4±0.52 40.3±0.77 80.4±0.17 88.2±0.50 72.9±0.74 CNN + wordMixup 90.9±0.58 40.5±1.17 77.5±0.33 89.3±0.47 74.2±1.15 CNN + senMixup 88.8±1.10 41.0±0.64 77.6±0.76 90.5±0.36 72.6±0.67 Accuracy (%) obtained by the testing methods using CNN with randomly initialized and fixed embeddings. Best results highlighted in Bold.</figDesc><table><row><cell>RandomFix</cell><cell>Trec</cell><cell>SST-1</cell><cell>SST-2</cell><cell>Subj</cell><cell>MR</cell></row><row><cell cols="2">CNN 88.PretrainTune Trec</cell><cell>SST-1</cell><cell>SST-2</cell><cell>Subj</cell><cell>MR</cell></row><row><cell>CNN</cell><cell cols="5">92.1±0.12 46.3±0.35 86.9±0.49 94.4±0.36 79.8±0.60</cell></row><row><cell cols="6">CNN + wordMixup 93.7±0.80 48.2±0.91 87.1±0.26 94.7±0.45 81.3±0.28</cell></row><row><cell>CNN + senMixup</cell><cell cols="5">93.3±0.23 48.6±0.23 87.2±0.35 94.9±0.34 80.6±0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) of the testing methods using CNN with pre-trained GloVe and trainable embeddings. Best results highlighted in Bold.</figDesc><table><row><cell>PretrainFix</cell><cell>Trec</cell><cell>SST-1</cell><cell>SST-2</cell><cell>Subj</cell><cell>MR</cell></row><row><cell>CNN</cell><cell cols="5">92.0±0.2 44.6±0.56 85.7±0.33 94.5±0.36 79.7±0.68</cell></row><row><cell cols="6">CNN + wordMixup 94.2±0.52 46.6±0.85 84.5±0.54 94.3±0.23 79.7±0.52</cell></row><row><cell>CNN + senMixup</cell><cell cols="5">94.8±0.35 46.5±0.23 84.7±0.48 95.0±0.22 80.3±0.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Accuracy (%) obtained by the testing methods using CNN with pretrained GloVe and fixed embeddings. Best results highlighted in Bold.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Aggregated learning: A vector quantization approach to learning with neural networks</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1807.10251</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1809.02499</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vocal tract length perturbation (vtlp) improves speech recognition</title>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<ptr target="https://github.com/yoonkim/cnnsentence" />
		<imprint>
			<date type="published" when="2014">2014a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP2014</title>
				<imprint>
			<date type="published" when="2014">2014b</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2015, 16th Annual Conference of the International Speech Communication Association</title>
				<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-06">2015. September 6-10, 2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
				<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-26">2004. 21-26 July, 2004</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL &apos;05</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data augmentation via dependency tree morphing for lowresource languages</title>
		<author>
			<persName><forename type="first">Gözde</forename><surname>Gül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5004" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
				<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Ioannis Mitliagkas, and Yoshua Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer</title>
				<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
				<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
