<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-based rendering using image-based priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Engineering Science</orgName>
								<orgName type="institution">The University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
							<email>wexler@wisdom.weizmann.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Applied Math The Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Engineering Science</orgName>
								<orgName type="institution">The University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image-based rendering using image-based priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12B5860D64D0202B1F0B55DBBE15E03E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a set of images acquired from known viewpoints, we describe a method for synthesizing the image which would be seen from a new viewpoint. In contrast to existing techniques, which explicitly reconstruct the 3D geometry of the scene, we transform the problem to the reconstruction of colour rather than depth. This retains the benefits of geometric constraints, but projects out the ambiguities in depth estimation which occur in textureless regions.</p><p>On the other hand, regularization is still needed in order to generate high-quality images. The paper's second contribution is to constrain the generated views to lie in the space of images whose texture statistics are those of the input images. This amounts to a image-based prior on the reconstruction which regularizes the solution, yielding realistic synthetic views. Examples are given of new view generation for cameras interpolated between the acquisition viewpoints-which enables synthetic steadicam stabilization of a sequence with a high level of realism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a small number of photographs of the same scene from several viewing positions, we want to synthesize the image which would be seen from a new viewpoint. This "view synthesis" problem has been widely researched in recent years. However, even the best methods do not yet produce images which look truly real. The primary source of error is in the trade-off between the inherent ambiguity of the problem, and the loss of high-frequency detail due to the regularizations which must be applied to alleviate that ambiguity. In this paper, we show how to constrain the generated images to have the same local statistics as natural images, effectively projecting the new view onto the space of real-world images. As this space is a small subspace of the space of all images, the result is strongly regularized synthetic views which preserve high-frequency details.</p><p>Strategies for view synthesis are divided into those which explicitly compute a 3D representation of the scene, and those in which the computation of scene geometry is implicit. The first class includes texture-mapped rendering of stereo reconstructions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, volumetric techniques such as space carving <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, and other volumetric approaches <ref type="bibr" target="#b23">[24]</ref>. Implicit-geometry techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> assemble the pixels of the synthesized view from the rays sampled by the pixels of the input images. In a newly emergent class of technique, to which this paper is most closely related, view-dependent geometry <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref> is used to guide the selection of the colour at each pixel. What all these techniques have in common, whether based on lightfields or explicit 3D models, is that there is no free lunch: in order to generate a new ray which is not in the bundle one is given, one must solve a form of the stereo correspondence problem. This is a difficult inverse problem, which is poorly conditioned: for a given set of images, many different solutions will model the image data equally well. Thus, in order to select between the nearly equivalent solutions the problem must be regularized by incorporating prior knowledge about the likely form of the solution. Previous work on new-view synthesis or stereo reconstruction has typically included such prior knowledge as a priori constraints on the (piecewise) smoothness of the 3D geometry, which results in artifacts at depth boundaries. In this paper, because the problem is expressed in terms of the reconstructed image rather than the reconstructed depth map, we can impose image-based priors, which can be learnt from natural images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>The most relevant previous work is primarily in two areas: view-dependent geometry, and natural image statistics. Irani et al <ref type="bibr" target="#b9">[10]</ref> expressed new view generation as the estimation of the colour at each generated pixel. Their representation implies, as does ours, a 3D geometry for the scene which is different for each synthetic viewpoint, and is thus related to view-dependent visual hull computation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>. As they note, this greatly improves the fidelity of the reconstructed image. However, it does not remove the fundamental ambiguity in the problem, which this paper directly addresses. In addition, their technique depends on the presence of a dominant plane in the scene, where this paper deals with the case of a general 3D scene with general camera motion.</p><p>The use of image-based priors to regularize hard inverse problems is inspired by Freeman and Pasztor's work <ref type="bibr" target="#b5">[6]</ref> on learning priors for Bayesian image reconstruction. Our texture representation, as a library of exemplar image patches, derives from this and from the recent tecture synthesis literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. In this paper we extend these ideas to deal with the strongly multimodal data likelihoods present in the image-based rendering task, allowing the generation of new views which are locally similar to the input images, but globally consistent with the new viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem statement</head><p>We are given a collection of n 2D images I 1 to I n , in which I i (x, y) is the color at pixel (x, y) of the i th image. 1 Color is expressed as a 3-vector in an appropriate colorspace. The images are taken by cameras in different positions represented by 3 × 4 projection matrices P 1 to P n , which are supplied. Figure <ref type="figure" target="#fig_0">1</ref> summarizes the situation. The projection matrix P projects homogeneous 3D points X to homogeneous 2D points x = λ(x, y, 1) linearly: x = PX where the equality is up to scale. We denote by I i (X) the pixel in image i to which 3D point X projects, so</p><formula xml:id="formula_0">I i (X) = I i (π(P i X)), π(x, y, w) = (x/w, y/w) (1)</formula><p>1 Notation guide: calligraphic letters L are images or windows from images. Uppercase roman letters L are RGB (or other colourspace) vectors. Bold roman lowercase x denotes 2D points, also written (x, y), and bold roman uppercase are 3D points X. Matrices are in fixed-width font, viz M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epipolar lines: Projections of ray X(z)</head><p>The stack of epipolar lines is C(i,z)</p><formula xml:id="formula_1">Input images I1 I2 I3 3D Object</formula><p>View to be synthesized Pixel (x,y) to be generated, with colour V(x,y) 3 D R a y X (z )</p><p>Figure <ref type="figure" target="#fig_2">2</ref>: Geometric configuration. The supplied information is a set of 2D images I1..n and their camera positions P1..n. At each pixel in the view to be synthesized, we wish to discover the colour which is most likely to be a reprojection of a 3D object point, based on the implied projection into the source images.</p><p>The task of virtual view synthesis is to generate the image which would be seen by a virtual camera in a position not in the original set. Specifically, we wish to compute, for each pixel V (x, y) in a virtual image V the color which that pixel would observe if a real camera were placed at the new location. We assume we are dealing with diffuse, opaque objects, and that any deviations from this assumption may be considered part of imaging noise. The extensions to more general lighting assumptions are exactly those in space carving <ref type="bibr" target="#b12">[13]</ref>, and will not be dealt with here. The objective of this work is to infer the most likely rendered view V given the set of input images I 1 , .., I n . In a Bayesian framework, we wish to choose the synthesised view V which maximizes the posterior p(V | I 1 , .., I n ). Bayes' rule allows us to write this as</p><formula xml:id="formula_2">p(V | I 1 , .., I n ) = p(I 1 , .., I n | V)p(V) p(I 1 , .., I n )<label>(2)</label></formula><p>where p(V) is the prior on V, and the data term p(I 1 , .., I n | V) measures the likelihood that the observed images could have been observed if V were the true colours at the novel viewpoint. Because we shall maximize this posterior over V, we need not compute the denominator p(I 1 , .., I n ), and will instead optimize the function</p><formula xml:id="formula_3">q(V) = p(I 1 , .., I n | V)p(V)<label>(3)</label></formula><p>This quasi-likelihood has two parts: the photoconsistency likelihood p(I 1 , .., I n | V) and the prior p(V) which we shall call p texture (V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Photoconsistency constraint</head><p>The color consistency constraint we employ is standard in the stereo and space-carving literature. We consider each pixel V (x, y) in the synthesised view separately, so the likelihood is written as the product of per-pixel likelihoods</p><formula xml:id="formula_4">p(I 1 , .., I n | V) = (x,y) p(I 1 , .., I n | V (x, y))<label>(4)</label></formula><p>Consider the generation of new-view pixel V (x, y). This is a sample from along the ray emanating from the camera centre, which we may assume to be the origin. Let the direction of this ray be denoted d(x, y). It can be computed easily given the calibration parameters of the virtual camera. Let a 3D point along the ray be given by the function X(z) = zd(x, y) where z ranges between preset values z min and z max . For a given depth z, we can compute using (1) the set of pixels to which X(z) projects in the images I 1..n . Denote the colors of those pixels by the function</p><formula xml:id="formula_5">C(i, z) = I i (X(z)).<label>(5)</label></formula><p>Let the set of all colours at a given z value be written</p><formula xml:id="formula_6">C(:, z) = {C(i, z)} n i=1 ,<label>(6)</label></formula><p>and the set, C, of all samples-at location (x, y)-be</p><formula xml:id="formula_7">C = {C(i, z) | 1 ≤ i ≤ n, z min &lt; z &lt; z max }.<label>(7)</label></formula><p>Figure <ref type="figure">3</ref> shows an example of C at one pixel in a real sequence. Because the input-image pixels whose colours form C are the only pixels which influence new-view pixel (x, y), the photoconsistency likelihood further simplifies to (writing V for V (x, y)) p(I 1 , ..,</p><formula xml:id="formula_8">I n | V ) = p(C | V )<label>(8)</label></formula><p>Now, by making explicit the dependence on the depth z and marginalizing, we obtain</p><formula xml:id="formula_9">p(C|V ) = p(C | V, z)dz = p(C(:, z) | V, z)dz<label>(9)</label></formula><p>The noise on the input image colours C(i, z) will be modelled as being drawn from distributions with density functions of the form exp(-βρ(t)), centred at V , where β is a constant specifying the width of the distribution. Thus the likelihood is of the form</p><formula xml:id="formula_10">p(C(:, z) | V, z) = n i=1 exp -βρ( V -C(i, z) ) (10)</formula><p>The function ρ is a robust kernel, and in this work is generally the absolute distance ρ(x) = |x|, corresponding to an exponential distribution on the pixel intensities. In situations (discussed later) where a Gaussian distribution is more appropriate, the kernel becomes ρ(x) = x 2 . In order to choose the colour V , we shall be computing (in §3.1) the modes of the function p(C(:, z) | V (x, y)). As defined above, this requires the computation of the integral <ref type="bibr" target="#b8">(9)</ref>, which is computationally undemanding. However, because the value of β is difficult to know, and because the function is sensitive to its value, the integral must also be over a hyperprior on β, rendering it much more challenging. Approximating the marginal by the maximum gives us an approximation, denoted p photo ,</p><formula xml:id="formula_11">p photo (V (x, y)) ≈ max z p(C(:, z) | V, z)<label>(11)</label></formula><p>which avoids both of these problems. In the implementation, the maximum over z is computed by explicitly sampling, typically using 500 values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Incorporating the texture prior</head><p>The function p photo (V ) will generally be multimodal, due firstly to physical factors such as occlusion and partial pixel effects and secondly to deficiencies in the image-formation model, such as not modelling specular reflections or having an inaccurate model of imaging noise. Thus the data likelihood at the true colour may often be lower than the likelihood at other, spurious values. Consequently, selecting the maximum-likelihood V at each pixel yields images with significant artefacts, such as those shown in figure <ref type="figure" target="#fig_0">1c</ref>.</p><p>We would like to constrain the generated views to lie in the space of real images by imposing a prior on the possible generated images. Defining such a prior is in the domain of the analysis of natural image statistics, an active area of recent neurophysiological and machine learning research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. Because it has been observed that correlation between pixels falls off quickly as a function of distance, we can make the assumption that the probability density can be written as a product of functions operating on small neighborhoods. Let the generated image V have pixels V (x, y). Then the prior has the form</p><formula xml:id="formula_12">p texture (V) = x,y p texture (N (x, y))<label>(12)</label></formula><p>where the function N (x, y) is the set of colours of neighbours of (x, y). Here we use 5 × 5 neighbourhoods, so</p><formula xml:id="formula_13">N (x, y) = {V (x + i, y + j) | -2 ≤ i, j ≤ 2} . (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>As the form of p texture is typically very difficult to represent analytically <ref type="bibr" target="#b8">[9]</ref>, we follow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and represent our texture prior as a library of texture patches. The likelihood of a particular neighbourhood is measured by computing its distance to the closest database patch. Thus, we are given a texture database of 5 × 5 image patches, denoted T = {T 1 , ..., T N } where N is typically extremely large. The definition of p texture is then where λ is a tuning parameter. This is a closest-point problem in the set of 75-d points (75 = 5 × 5 × 3) in T and may be efficiently solved using a variety of algorithms, for example vector quantization and BSP tree indexing <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_15">p texture (N (x, y)) = exp -λ min T ∈T T -N (x, y) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Combining photoconsistency and texture</head><p>Finally, combining the data and prior terms, we have the expression for the quasi-likelihood q(V) =</p><p>x,y p photo (V (x, y)) p texture (N (x, y)).</p><p>In the implementation, we minimize the negative log of q, yielding the energy formulation</p><formula xml:id="formula_16">E(V) = x,y E photo (V (x, y)) + x,y E texture (N (x, y))<label>(14</label></formula><p>) where E photo measures the deviation from photoconsistency at pixel (x, y) and E texture measures the a-priori likelihood of the texture patch surrounding (x, y). From ( <ref type="formula" target="#formula_11">11</ref>), the definition of E photo at a pixel (x, y) with 3D ray X(z) is</p><formula xml:id="formula_17">E photo (V ) = min zmin&lt;z&lt;zmax n i=1 ρ ( V -I i (X(z)) ) (15)</formula><p>The texture energy is the negative log of p texture , giving</p><formula xml:id="formula_18">E texture (N (x, y)) = λ min T ∈T T -N (x, y) 2<label>(16)</label></formula><p>The view synthesis problem is now one of minimization of E over the space of images. This is a difficult global optimization problem, and making it tractable is the subject of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head><p>The optimization of the energy defined above could be directly attempted using a global optimization strategy such as simulated annealing. However, both the prior and the data term E photo are expensive to evaluate, with multiple local minima at each pixel, meaning that attaining a global optimum will be difficult, and certainly time consuming.</p><p>To render the optimization tractable, we exploit the simplification of the energy function conferred by estimating colour rather than depth. That is, we compute the set of modes of the photo-consistency term for each pixel, and restrict the solution for that pixel to this set. Then the texture prior is used to select the values from this set. This reduces the problem from a search over a high-dimensional space to an enumeration of the possible combinations. Although the data likelihood p(C|V ) is multimodal, there are typically many fewer modes than there are maxima of p(C(:, z) | V, z) over depth, so we can hope to explicitly compute the modes of p(C|V ) as the first step. This means that the optimization becomes a discrete labelling problem, which although still complex, can be analysed much more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Enumerating the minima of E photo (V )</head><p>The goal then is to generate a list of plausible colours for each rendered pixel V (x, y). One option would be to sample from p photo (V ) using MCMC, but this is computationally unattractive. A more practical alternative is to find all local minima of the energy function E photo (V ). On the face of it, this seems a tall order, but as figure <ref type="figure">3</ref>.1 indicates, there are typically few minima in a generally well-behaved space.</p><p>Inspection of several such plots on a number of scenes suggests that this behaviour is typical. Finding all local minima of such functions is task for which several strategies have emerged from the computational chemistry community, and have been introduced to computer vision by Sminchisescu and Triggs <ref type="bibr" target="#b21">[22]</ref>. The most expensive is to densely sample the space of V (here 3D RGB space), and this is the strategy used to obtain the isosurface plot shown in figure <ref type="figure">3</ref>.1. A more efficient strategy to isolate the minima is to start gradient descent from several randomly chosen starting points, and iterate until local minima are found. Finally clustering on the locations of the minima produces a set of distinct colours which are likely at that pixel. On the images we have tested, 12 steps of gradient descent on each of 20 random starting colours V takes a total of about 0.1 seconds in Matlab, and produces between four and six colour hypotheses at each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Texture reference and rectification</head><p>The second implementation issue is the source of reference textures. To build a general tool for projection of images onto natural images, a large database of images of natural scenes would be the ideal choice. In this case, however, we are operating in a limited problem domain. We expect that the newly synthesized views will be similar locally to the input views with which the algorithm is provided. Therefore, the texture library is built of patches from the input images. This provides excellent performance with a small library, and the photoconsistency term means that the system cannot "overlearn" by simply copying large patches from the nearest source image to the newly rendered view. For speed, we can also use the known z range to limit the search for matching texture windows in source image I i to the bounding box of {P i X(z) | z min &lt; z &lt; z max }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>Given the modes of the photoconsistency distribution at each pixel, the optimization of ( <ref type="formula" target="#formula_16">14</ref>) becomes a labelling problem. Each pixel is associated with an integer label l(x, y), which indicates which mode of the distribution will be used to colour that pixel, with a corresponding photoconsistency cost which is precomputed. This significantly reduces the cost of function evaluations, but the optimization is still a computationally challenging problem. For this work, we have implemented a variant of the iterated conditional modes (ICM) algorithm <ref type="bibr" target="#b1">[2]</ref>, alternately optimizing the photoconsistency and texture priors. The algorithm begins by selecting, for each pixel, the most likely mode of the photoconsistency function, yielding an initial estimate V 0 . Then, at each ICM iteration, each pixel is varied until the 5 × 5 window surrounding it minimizes the sum E photo + E texture at that pixel. This optimization is potentially extremely expensive, implying the evaluation of E photo (V ) for the value V in the centre of each texture patch T . However, because the minima of E photo are available, a fast approximation is obtained simply by writing</p><formula xml:id="formula_19">E photo (V ) ≈ V -V r-1 2</formula><p>, where V r-1 is the colour obtained at the previous iteration. It can be shown that this amounts to setting the centre pixel to a linear combination of (a) the photoconsistency mode, and (b) the value that would be predicted by sampling-based texture synthesis. If V r-1 is the value predicted by photoconsistency at the previous iteration, and T is the value at the centre pixel of the best matching texture patch, then the pixel should be replaced by</p><formula xml:id="formula_20">V r = V r-1 + λT 1 + λ<label>(17)</label></formula><p>Finally, replacing V r by the closest mode at each iteration ensures that the synthesized colour is always a subset of the photoconsistency minima. Note that this does not undo the good work of the robust kernel in computing the modes of E photo , but allows the texture prior to efficiently select between the robustly-computed colour candidates at each pixel. This also prevents the algorithm from copying large sections of the texture source. Figure <ref type="figure">3</ref>.3 summarizes the steps in the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Choice of robust kernels</head><p>In the preceding, the choice of robust kernels for the photoconsistency likelihood has been mentioned several times. In practice, there is a significant tradeoff between speed and accuracy implied by choosing other than the squarederror kernel ρ(x) = x 2 kernel, as the mode computation can be significantly optimized for the squared-error case. The problem arises when there is significant occlusion in the sequence, as on the example pixel in figure <ref type="figure">3</ref>, and it becomes necessary to produce a view which looks "behind" the foreground pixel. Using the squared-error kernel, the true colour (in this case, black) is not a minimum of E photo , because the column C(:, z) at the depth corresponding to the background contains some white pixels which are significant outliers to the Gaussian distribution exp(-ρ(•)).</p><p>The true colour is a minimum using the absolute distance ρ(x) = |x| or Huber kernels, which are less sensitive to such outliers. To provide a rule of thumb, the squared-error kernel is fast, and works well for interpolation, but the absolute distance kernel is needed for extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Examples</head><p>Image sequences were captured using a hand-held camera, and the sequences were calibrated using commer- for each pixel (x, y)</p><formula xml:id="formula_21">Extract window N = {V r-1 (x + i, y + j)| -2 ≤ i, j ≤ 2}. Find closest texture patch T = argmin T ∈T M(N -T ) 2</formula><p>M is a mask which ignores the centre pixel. Set V r (x, y) to the mode V k (x, y) nearest the value computed by <ref type="bibr" target="#b16">(17)</ref>.  Single still frames are reproduced here, and complete MPEG sequences may be found at http://www.robots.ox.ac.uk/∼awf/ibr.</p><p>The first experiment is a leave-one-out test, so that the recovered images can be compared against ground truth. Each frame of the 27-frame "monkey" sequence was reconstructed based on the other 26 frames. Figure <ref type="figure" target="#fig_3">4</ref> shows the results for a typical frame, comparing the ground truth image first to the synthesized view using photoconsistency alone, and then to the result guided by the texture prior. Visually, the fidelity is high, and the image is free of the highfrequency artifacts which the photoconsistency-maximizing view exhibits. Artifacts do occur in the background visible under the monkey's arm, where few of the source views have observed the background, meaning it does not appear as a mode of the photoconsistency distribution. The difference image in figure <ref type="figure" target="#fig_3">4d</ref> is simply the length of the RGB difference vector at each pixel, but shows that the texture prior does not bias the generated view, for example by copying one of the texture sources.</p><p>The second example shows performance on a "steadicam" task, where the scene is re-rendered at a set of viewpoints which smoothly interpolate the first and last camera position and orientation. The reader is encouraged to consult the videos on the webpage above to confirm the absence of artifacts, and the subtle movements of the partial occlusions at the boundaries.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows example images from one sequence and illustrates the improvement obtained. The erroneous areas surrounding the ear are removed, while the remainder of the image retains its (correct) solution. At high magnification, it is in fact possible to see that the optimized solution has added back some high-frequency detail in the image. This is because the local statistics of the texture library are being applied to the rendered view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has shown that view synthesis problems can be regularized using texture priors. This is in contrast to the depth-based priors that previous algorithms have used. Image-based priors have several advantages over the depthbased ones. First, depth priors are difficult to learn from real images, so artificial approximations are used. These approximations are equivalent to assuming very simple models of the world-for example, that it is piecewise planarand thus introduce artifacts into the generated views. In contrast, image-based priors are easy to obtain from the world. If the problem domain is restricted, as it is here, a small number of images can be used to regularize the solution to a complex inverse problem.</p><p>There are many areas for further work: (1) image-based priors as implemented here are expensive to evaluate. For a typical depth prior, evaluation of the prior in a pixel neighbourhood requires computation of the order of a few machine instructions. As image-based priors are stored in large lookup tables, the cost of evaluating them is many times higher. <ref type="bibr" target="#b1">(2)</ref> In this paper, only one optimization strategy was investigated. It is hoped that examination of other strategies will lead to significantly quicker solutions. (3) Occlusion is handled here by the robust kernel ρ. More geometric handling of occlusion, analogous to space carving's improvement over voxel colouring, ought to yield better results. (4) When rendering sequences of images, it is valuable to impose temporal continuity from frame to frame. This paper has not addressed this issue, so the rendered sequences show some flicker. On the other hand this does allow the stability of the per-frame solutions to be evaluated.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: View synthesis. (a,b): Two from a set of 39 images taken by a hand-held camera. (c): Detail from a new view generated using state-of-the-art view synthesis. The new view is about 20 • displaced from the closest view in the original sequence. Note the spurious echo of the ear. (d): The same detail, but constrained to only generate views which have similar local statistics to the input images.</figDesc><graphic coords="1,325.33,304.14,106.31,93.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Frame number, i →Figure 3 :</head><label>i3</label><figDesc>Figure 3: Photoconsistency. One image is shown from a sequence of 27 captured by a hand-held camera. The circled pixel x's photoconsistency with respect to the other 26 images is illustrated on the right. The upper right image shows the reprojected colours C(:, z) as columns of 26 colour samples, at each of 500 depth samples. The colours are the samples C(i, z) where the frame number i varies along the vertical axis, and the depth samples z vary along the horizontal. Equivalently, row i of this image is the intensity along the epipolar line generated by x in image i. Below are shown photoconsistency likelihoods p(C | V, z) for two values of the colour V (backgrounds to the plots). As this pixel is a co-location of background and foreground, these two colours form modes of p(C | V ) when z is maximized. This multi-modality is the essence of the ambiguity in new-view synthesis, which prior knowledge must remove.</figDesc><graphic coords="3,58.50,74.84,223.22,167.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>2 shows a plot of p(C(:, z) | V, z) for grayscale C at a typical pixel. Figure 3.1 shows isosurface plots of p photo (V ) in RGB space for the same pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The function p(C(:, z)|V, z) plotted for the pixel studied in figure3, with grayscale images, so V is a scalar, and ρ(x) = |x|. The projected graphs show the marginals (blue) and the maxima (red). The marginalization over colour (V ) has fewer minima than that over z, and the two modes corresponding to foreground and background are clearly seen.</figDesc><graphic coords="4,317.25,72.00,236.25,132.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Minima of E photo . (a) Isosurfaces in RGB space of the photoconsistency function E photo (V ) at the pixel studied in figure 3. Minima are computed by gradient descent from random starting positions, of which twelve are shown (black circles), with the gradient descent trajectories plotted in black. Four modes were retained after clustering; their locations are marked by white 3D "axes" lines in (a), and their RGB colours are shown in (b).</figDesc><graphic coords="5,323.23,72.00,157.89,165.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Input: Images I1 to In, Camera positions P1 to Pn Texture library T ⊂ R 75 Output: New view V Preprocessing: for each pixel (x, y) Compute ray direction d(x, y). Choose m depths to sample {zj = zmin + j∆z} m j=1Compute n × m × 3 array of pixel colours Cij = Ii (Pi * zjd(x, y))Compute K local minima, denoted V1..K (x, y), of Ephoto(V ) = minj i ρ( Cij -V ) Sort so that Ephoto(V k ) &lt; Ephoto(V k+1 )∀kSet initial estimate of new view V 0 (x, y) = V1(x, y) end Update at iteration r:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Pseudocode for iterative computation of new viewV. The preprocessing is expensive (about 0.1sec/pixel), the iterations cost as much as patch-based texture synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Leave-one-out test. Using 26 views to render a missing view allows comparison to be made between the rendered view and ground truth. (a) Maximum-likelihood view, in which each pixel is coloured according to the highest mode of the photoconsistency function. High-frequency artifacts are visible throughout the scene. (b) View synthesized using texture prior. The artifacts are significantly reduced. (c) Ground-truth view. (d) Difference image between (b) and (c).</figDesc><graphic coords="8,58.50,86.89,494.98,90.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Steadicam test. Three novel views of the monkey scene from viewpoints not in the original sequence. The complete sequence may be found at http://www.robots.ox.ac.uk/∼awf/ibr.</figDesc><graphic coords="8,65.90,281.08,158.40,156.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (a) 3D composite from 2D images. The camera motion from the live-action background plate is applied to the head sequence, rendering new views of the face. (b) Tsukuba. Fine details such as the lamp arm are retained, but some ghosting is evident around the top of the lamp.</figDesc><graphic coords="8,64.48,519.91,144.84,108.63" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Funding for this work was provided by the DTI/EPSRC Link grant V2I. AWF thanks the Royal Society for its generous support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.2d3.com" />
		<title level="m">Ltd</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Royal Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical consistency check for the space carving algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broadhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1039" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1182" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The lumigraph</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probability models for clutter in natural images</title>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="424" to="429" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistics of natural images and models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="541" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What does the scene look like from a scene point?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D surface reconstruction from stereoscopic image sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-based rendering from uncalibrated lightfields with scalable geometry</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heigl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-Image Analysis, Springer LNCS 2032</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-based visual hulls</title>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gortler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="369" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-based 3d photography using opacity hulls</title>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Plenoptic modeling: An image-based rendering system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH95</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View-dependent geometry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rademacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View Synthesis Using Stereo Vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">1583</biblScope>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photorealistic scene reconstruction by voxel coloring</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1067" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building roadmaps of local minima of visual models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="566" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On advances in statistical modeling of natural images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stereo matching with transparency and matting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">View synthesis using convex and visual hulls</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
