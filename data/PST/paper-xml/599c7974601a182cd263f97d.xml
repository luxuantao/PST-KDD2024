<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach</title>
				<funder ref="#_zzRqW6v">
					<orgName type="full">JSPS Kakenhi</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Takuo</forename><surname>Hamaguchi</surname></persName>
							<email>takuo-h@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<settlement>Ikoma, Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hidekazu</forename><surname>Oiwa</surname></persName>
							<email>oiwa@recruit.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Recruit Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
							<email>shimbo@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<settlement>Ikoma, Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<settlement>Ikoma, Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge base completion (KBC) aims to predict missing information in a knowledge base. In this paper, we address the out-of-knowledge-base (OOKB) entity problem in KBC: how to answer queries concerning test entities not observed at training time. Existing embedding-based KBC models assume that all test entities are available at training time, making it unclear how to obtain embeddings for new entities without costly retraining. To solve the OOKB entity problem without retraining, we use graph neural networks (Graph-NNs) to compute the embeddings of OOKB entities, exploiting the limited auxiliary knowledge provided at test time. The experimental results show the effectiveness of our proposed model in the OOKB setting. Additionally, in the standard KBC setting in which OOKB entities are not involved, our model achieves state-of-the-art performance on the WordNet dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases such as WordNet <ref type="bibr">[Miller, 1995]</ref> and Freebase <ref type="bibr" target="#b0">[Bollacker et al., 2008]</ref> are used for many applications including information extraction, question answering, and text understanding. These knowledge bases can be viewed as a set of relation triplets, i.e., triplets of the form (h, r, t) with an entity h called the head entity, a relation r, and an entity t called the tail entity <ref type="bibr" target="#b1">[Bordes et al., 2013;</ref><ref type="bibr" target="#b5">Nguyen et al., 2016]</ref>. Some examples of relation triplets are (Philip-K.-Dick, write, Do-Androids-Dream-of-Electric-Sheep?) and (Do-Androids-Dream-of-Electric-Sheep?, is-a, Science-fiction). Although a knowledge base contains millions of such triplets, it is known to suffer from incompleteness <ref type="bibr" target="#b6">[Nickel et al., 2016]</ref>. Knowledge base completion (KBC) thus aims to predict the information missing in knowledge bases.</p><p>In recent years, embedding-based KBC models have been successfully applied to large-scale knowledge bases <ref type="bibr">[Wang et al., 2014b;</ref><ref type="bibr" target="#b5">Lin et al., 2015;</ref><ref type="bibr">Ji et al., 2015;</ref><ref type="bibr">Xiao et al., 2016b;</ref><ref type="bibr" target="#b5">Nguyen et al., 2016;</ref><ref type="bibr" target="#b3">Guu et al., 2015]</ref>. These models build the distributed representations (or, vector embeddings) of entities and relations observed in the training data, and use vari-  <ref type="formula">1</ref>) depicted as a red double arrow, which contains an entity "Blade Runner" that is not observed in the knowledge graph (shown as the shaded box). The task is to tell whether any other relations hold between Blade Runner and the entities in the knowledge graph, by using (1) the new triplet and (2) the existing triplet, depicted by a black arrow. For example, we want to answer the question, "Is Blade Runner science fiction?", i.e., whether (3) the green dashed arrow in the figure should be drawn.</p><p>ous vector operations over the embeddings to predict missing relation triplets.</p><p>In this paper, we address the out-of-knowledge-base (OOKB) entity problem in embedding-based KBC. This problem arises when new entities (OOKB entities) occur in the relation triplets that are given to the system after training. As these entities were unknown to the system at training time, the system does not have their embeddings, and hence does not have a means to predict the relations for these entities. The OOKB entity problem thus asks how to perform KBC involving such OOKB entities. Although it can be solved by retraining the embeddings using the added relation triplets containing the OOKB entities, a solution avoiding costly retraining is desirable.</p><p>This problem is of practical importance because OOKB entities crop up whenever new entities, such as events and products, are produced, which happens everyday. For example, suppose we find an OOKB entity "Blade-Runner" in a new triplet (Blade-Runner, based-on, Do-Androids-Dream-of-Electric-Sheep?). We want to infer more facts (viz. triplets) about Blade-Runner from the knowledge we already have, and answer questions such as "Is Blade Runner science fiction?" If the knowledge base contains a triplet (Do-Androids-Dream-of-Electric-Sheep?, is-a, Science-fiction), it should help us estimate that the answer is yes. Figure <ref type="figure" target="#fig_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>illustrates this example schematically.</head><p>There have been some attempts to obtain the embeddings for OOKB entities using external resources <ref type="bibr">[Wang et al., 2014a;</ref><ref type="bibr" target="#b2">Fang et al., 2016;</ref><ref type="bibr" target="#b14">Zhong et al., 2015]</ref>. Although these approaches may be useful, they require additional computation over large resources, which may not be always feasible. By contrast, we pursue a KBC model that exploits existing triplets in the knowledge base, without relying on external resources. Indeed, the Blade Runner example above suggests the possibility of inferring new facts about OOKB entities without the help of external resources.</p><p>To solve the OOKB entity problem, we apply graph neural networks (Graph-NNs) <ref type="bibr" target="#b7">[Scarselli et al., 2009;</ref><ref type="bibr" target="#b5">Li et al., 2015]</ref> to a knowledge graph, which is a graph obtained by regarding entities as nodes and triplets as edges. A Graph-NN is a neural network architecture defined on a graph structure and composed of two models called the propagation model and output model. The propagation model manages how information propagates between nodes in the graph. In the propagation model, we first obtain the embedding vectors of the neighborhood of a given node (entity) e and then convert these vectors into a representation vector of e using a pooling function such as the average. In other words, each node is embedded as a vector in continuous space, which is also used to calculate the vectors of the neighborhood nodes. This mechanism enables the vector for an OOKB entity to be composed from its neighborhood vectors at test time. The output model defines the task-oriented objective function over node vectors. This allows us to use an existing embedding-based KBC model as the output model. In this paper, we use TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> as the output model, but we can adopt other embedding-based KBC methods.</p><p>Our main contributions can be summarized as follows:</p><p>? We propose a new formulation of the problem of OOKB entities in KBC. ? We propose a Graph-NN suitable for the KBC task with OOKB entities. ? We verify the effectiveness of our models in both the standard and the "OOKB" entity settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OOKB Entity Problem in Knowledge Base Completion 2.1 Knowledge Graph</head><p>Let E be a set of entities and R be a set of relations. Define the fact, or relation triplet, to be a triplet of form (h, r, t) where h, t ? E and r ? R. Let G gold ? E ? R ? E be the set of gold facts, i.e., the set of all relation triplets that hold for pairs of entities in E and relations in R. If a triplet is in G gold , we say it is a positive triplet; otherwise, it is a negative triplet.</p><p>The goal of knowledge completion is to identify G gold , when only its proper subset, or an "incomplete" knowledge base, G ? G gold is accessible.</p><p>A knowledge base G is often called knowledge graph because each triplet in G can be regarded as a (labeled) edge in a graph; i.e., the entities in a triplet correspond to the end nodes and the relation gives the label of the edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KBC: Triplet Classification</head><p>Triplet classification is a typical KBC task introduced by <ref type="bibr" target="#b8">[Socher et al., 2013]</ref> <ref type="foot" target="#foot_1">1</ref> and has since been a standard benchmark for KBC methods <ref type="bibr">[Wang et al., 2014b;</ref><ref type="bibr">Ji et al., 2015;</ref><ref type="bibr">Xiao et al., 2016a;</ref><ref type="bibr" target="#b13">Yoon et al., 2016;</ref><ref type="bibr" target="#b5">Nguyen et al., 2016]</ref>.</p><p>In this task, existing knowledge base G is assumed to be incomplete, in the sense that some of triplets that must be present in G are missing; i.e., G = G gold .</p><p>Let H = (E ? R ? E)\G be the set of triplets not present in G. Because G is incomplete, two cases are possible for each triplet x ? H; either x is a positive triplet (i.e., x ? G gold ), or x is a negative triple (i.e., x ? G gold ). For the former case, x is not in G only because of the incompleteness, and the knowledge base must be updated to contain x. We thus encounter the problem of determining which of the above two possible cases each triplet not present in G falls into. This problem is called triplet classification.</p><p>Viewed as a machine learning problem, triplet classification is a classifier induction task in which E and R are given In the standard triplet classification, E and R are limited to the entities and relations that appear in</p><formula xml:id="formula_0">G. That is, E = E(G) and R = R(G), where E(G) = {h | (h, r, t) ? G} ? {t | (h, r, t) ? G} and R(G) = {r | (h,</formula><p>r, t) ? G} denote the entities and relations appearing in G, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">OOKB Entity Problem</head><p>We now introduce a new task in KBC, called the OOKB entity problem.</p><p>In addition to the knowledge base G observed at training time, new triplets G aux are given at test time, with E(G aux ) ? E(G) and R(G aux ) ? R(G). Thus, G aux contains new entities E OOKB = E(G aux )\E(G), but no new relations are involved.</p><p>We call E OOKB OOKB entities. It is assumed that every triplet in G aux contains exactly one OOKB entity from E OOKB and one entity from E(G); that is, the additional triplets G aux represent edges bridging E(G) and E OOKB in the combined knowledge graph G ? G aux . In this setting, E = E(G) ? E OOKB = E(G), and the task is to correctly identify missing relation triplets that involve the OOKB entities E OOKB . Because the embeddings for these entities are missing, they must be computed from those for entities in G. In other words, we want to design a model by which the information we already have in G can be transferred to OOKB entities E OOKB , with the help of the added knowledge G aux .</p><p>3 Proposed Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph-NNs</head><p>Graph-NNs are neural networks defined on a graph structure. Although there exist graph-NNs that encode an entire graph into a vector <ref type="bibr" target="#b1">[Cao et al., 2016;</ref><ref type="bibr">Defferrard et al., 2016]</ref>, here we focus on the one that provides the means to encode nodes and edges into vectors, as this is more suitable for KBC.</p><p>According to <ref type="bibr" target="#b7">[Scarselli et al., 2009;</ref><ref type="bibr" target="#b5">Li et al., 2015]</ref>, a graph-NN consists of two models, the propagation model and the output model. The propagation model determines how to propagate information between nodes in a graph. The output model defines an objective function according to given tasks using vector-represented nodes and edges. In this paper, we modify the propagation model to be suitable for knowledge graphs. For the output model, we use the embedding-based KBC model TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Propagation Model on a Knowledge Graph</head><p>Let G be a knowledge graph, e ? E(G) be an entity, and v e ? R d be the d-dimensional representation vector of e. Li et al. define the propagation model by the following equation <ref type="bibr" target="#b5">[Li et al., 2015]</ref>:</p><formula xml:id="formula_1">v e = (h,r,e)?Nhead(e) T head (v h ; h, r, e) + (e,r,t)?Ntail(e)</formula><p>T tail (v t ; e, r, t), (1) where head neighborhood N head and tail neighborhood N tail are N head (e) = {(h, r, e) | (h, r, e) ? G} and N tail (e) = {(e, r, t) | (e, r, t) ? G} in a knowledge graph G, respectively. In addition, T head , T tail : <ref type="bibr">et al., 2015]</ref> and used to transform the vector of a neighbor node for its incorporation in the current vector v e , depending on the property of the edge between them.</p><formula xml:id="formula_2">R d ? E(G) ? R(G) ? E(G) ? R d are called transition functions [Li</formula><p>We generalize the propagation function using the following pooling function P : S head (e) = {T head (v h ; h, r, e) | (h, r, e) ? N h (e)}, (2) S tail (e) = {T tail (v t ; e, r, t) | (e, r, t) ? N t (e)},</p><p>(3) v e = P (S head (e) ? S tail (e)),</p><p>Here, S head (e) contains the representation vectors of neighborhood N head (e), and S tail (e) contains those for N tail (e). The difference between Eq. (1) and ours (Eqs. ( <ref type="formula">2</ref>)-( <ref type="formula" target="#formula_3">4</ref>)) is the use of the pooling function in place of the summation. The candidates for functions T head , T tail , and P are described below.</p><p>Transition Function The aim of transition function T (including both T head and T tail ) is to modify the vector of a neighbor node to reflect the relations between the current node and the neighbor. The examples of the transition function are listed here: We can also make the transition function dependent on the relation between the current node (entity) and the neighbor, such as in the following:</p><formula xml:id="formula_4">T (v) = v, (identity) T (v) = tanh(Av), (single tanh layer) T (v) = ReLU(</formula><formula xml:id="formula_5">T head (v h ; h, r, e) = tanh(A head (h,r,e) v h ), T tail (v t ; e, r, t) = tanh(A tail (e,r,t) v t ).</formula><p>Note that the parameter matrices are now defined individually for each combination of node e, the current neighbors (h or t), and the relation r between them.</p><p>In the experiments of Section 4, we use the following transition functions:</p><formula xml:id="formula_6">T head (v h ; h, r, e) = ReLU(BN(A head r v h )),<label>(5)</label></formula><formula xml:id="formula_7">T tail (v t ; e, r, t) = ReLU(BN(A tail r v t )),<label>(6)</label></formula><p>where BN indicates batch normalization <ref type="bibr">[Ioffe and Szegedy, 2015]</ref>.</p><p>Pooling Function Pooling function P is a function that maps a set of vectors to a vector, i.e., P : 2 R d ? R d . Its objective is to extract shared aspects from a set of vectors.</p><p>For S = {x i ? R d } N i=1 , some simple pooling functions are as follows:</p><formula xml:id="formula_8">P (S) = N i=1 x i , (<label>sum pooling)</label></formula><formula xml:id="formula_9">P (S) = 1 N N i=1</formula><p>x i , (average pooling)</p><formula xml:id="formula_10">P (S) = max({x i } N i=1 ), (max pooling)</formula><p>where max is the elementwise max function. Sum pooling was used in <ref type="bibr" target="#b7">[Scarselli et al., 2009;</ref><ref type="bibr" target="#b5">Li et al., 2015]</ref>; see also Eq. ( <ref type="formula">1</ref>).</p><p>Stacking and Unrolling Graph Neural Networks As explained above, the propagation models decide how to propagate information from a node to its neighborhood. Applying this propagation model repeatedly, we can broadcast information of a node to farther nodes, i.e., each node can receive further information. Broadcasting can be implemented in one of two ways: stacking or unrolling. The unrolled Graph-NN is discussed in <ref type="bibr" target="#b7">[Scarselli et al., 2009;</ref><ref type="bibr" target="#b5">Li et al., 2015]</ref>. In the unrolled Graph-NN, the propagation model uses the same model parameters in every propagation. The propagation procedures are the same as described in Eqs. ( <ref type="formula">2</ref>)-( <ref type="formula" target="#formula_3">4</ref>).</p><p>The stacked Graph-NN is constructed in a similar manner to the well-known stacking technique <ref type="bibr" target="#b9">[Vincent et al., 2010]</ref>. In particular, the propagation process in the stacked Graph-NN uses different model parameters depending on time step n. The transition function at each time step n, indicated by superscript n, is as follows.</p><formula xml:id="formula_11">v (n) e = v e , if n = 0, P (S (n-1) head (e) ? S (n-1) tail (e)), otherwise,</formula><p>where</p><formula xml:id="formula_12">S (n) head (e) = {T (n) head (v (n) h ; h, r, e) | (h, r, e) ? N head (e)} S (n) tail (e) = {T (n) tail (v (n) t ; e, r, t) | (e, r, t) ? N tail (e)}.</formula><p>where</p><formula xml:id="formula_13">T (n) head and T (n)</formula><p>tail are transition functions depending on head/tail and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Model: Score and Objective Functions</head><p>We use a TransE-based objective function as the output model. TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> is one of the basic embedding-based models for KBC, and we use it for its simplicity and ease of training. Notice however that our architecture is not limited to TransE, and adopting other embeddingbased models for the output model is equally straightforward.</p><p>Below, we explain the score function of TransE and its commonly used pairwise-margin objective functions. We then describe the modified objective function we used in our experiments, called the absolute-margin objective.</p><p>Score Function The (implausibility) score function f evaluates the implausibility of a triplet (h, r, t); smaller scores indicate that the triplet is more likely to hold. In TransE, the score function is defined by f (h, r, t) = v h + v r -v t , where v h , v r , and v t are the embedding vectors of the head, relation, and tail, respectively. This score function states that for a positive triplet (h, r, t), the sum of the head and relation vectors v h + v r must be close to the tail vector v t , i.e., v h + v r ? v t . This score function is modified and extended in <ref type="bibr">[Wang et al., 2014b;</ref><ref type="bibr" target="#b5">Lin et al., 2015;</ref><ref type="bibr">Ji et al., 2015;</ref><ref type="bibr">Xiao et al., 2016b]</ref>. As mentioned earlier, all these models can be used as our output model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise-Margin Objective Function</head><p>The objective (loss) function defines the quantity to be minimized through optimization. The following pairwise-margin objective function is commonly used with KBC methods including TransE <ref type="bibr" target="#b1">[Bordes et al., 2013;</ref><ref type="bibr">Xiao et al., 2016b]</ref>:</p><formula xml:id="formula_14">L = N i=1 [? + f (h i , r i , t i ) -f (h i , r i , t i )] +<label>(7)</label></formula><p>where [x] + is the hinge function [x] + = max(0, x) and scalar ? ? R is a threshold (called margin), with (h i , r i , t i ) denoting a positive triplet and (h i , r i , t i ) denoting a negative triplet. This objective function requires score f (h i , r i , t i ) to be greater than score f (h i , r i , t i ) by at least ? . If the difference is smaller than ? , then the optimization changes the parameters to meet the requirement. In contrast, if the difference is greater than ? , the parameters are not updated. The pairwise-margin objective thus pays attention to the difference in scores between positive-negative triplet pairs. Absolute-Margin Objective Function Instead of the pairwise-margin objective, in this paper, we employ the following objective function, which we call the absolute-margin objective. </p><formula xml:id="formula_15">L = N i=1 f (h i , r i , t i ) + [? -f (h i , r i , t i )] + (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method WordNet11 Freebase13</head><p>NTN <ref type="bibr" target="#b8">[Socher et al., 2013]</ref> 70.4 87.1 TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> 75.9 81.5 TransH <ref type="bibr">[Wang et al., 2014b]</ref> 78.8 83.3 TransR <ref type="bibr" target="#b5">[Lin et al., 2015]</ref> 85.9 82.5 TransD <ref type="bibr">[Ji et al., 2015]</ref> 86.4 89.1 TransE-COMP <ref type="bibr" target="#b3">[Guu et al., 2015]</ref> 80.3 87.6 TranSparse <ref type="bibr" target="#b5">[Ji et al., 2016]</ref> 86.8 88.2 ManifoldE <ref type="bibr">[Xiao et al., 2016a]</ref> 87.5 87.3 TransG <ref type="bibr">[Xiao et al., 2016b]</ref> 87.4 87.3 lppTransD <ref type="bibr" target="#b13">[Yoon et al., 2016]</ref> 86.2 88.6 NMM <ref type="bibr" target="#b5">[Nguyen et al., 2016]</ref> 86.8 88.6 Proposed method 87.8 81.6</p><p>where ? is a hyperparameter, again called the margin. This objective function considers positive and negative triplets separately in the first and the second terms, not jointly as in the pairwise-margin objective. The scores for the positive triplets will be optimized towards zero, whereas the scores of the negative triplets are going to be at least ? . This objective function not only is easy to optimize, but also obtained good results in our preliminary experiments. We thus used this objective function for the experiments in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Hyperparameters</head><p>We implemented our models using the neural network library Chainer (http://chainer.org/). The code and dataset are available at https://github.com/takuo-h/GNN-for-OOKB. All networks were trained by stochastic gradient descent with backpropagation; specifically, we used the Adam optimization method <ref type="bibr" target="#b5">[Kingma and Ba, 2014]</ref>. The step size of Adam was</p><formula xml:id="formula_16">? 1 /(? 2 ? k + 1.0),</formula><p>where k indicates the number of epochs performed, ? 1 = 0.01, and ? 2 = 0.0001. The mini-batch size was 5, 000 and the number of training epochs was 300 in every experiment. Moreover, the dimension of the embedding space was 200 in the standard triplet classification and 100 in other settings.</p><p>In the preliminary experiments, we tried several activation functions and pooling functions, and found the following hyperparameter settings on account of both computational time and performance. We used Eqs. ( <ref type="formula" target="#formula_6">5</ref>)-( <ref type="formula" target="#formula_7">6</ref>) as transision functions in both the standard and OOKB settings. As the pooling  <ref type="bibr">8,</ref><ref type="bibr">191 16,</ref><ref type="bibr">193 20,</ref><ref type="bibr">345 9,</ref><ref type="bibr">899 19,</ref><ref type="bibr">218 23,</ref><ref type="bibr">792 Auxiliary triplets 4,</ref><ref type="bibr">352 12,</ref><ref type="bibr">376 19,</ref><ref type="bibr">625 15,</ref><ref type="bibr">277 31,</ref><ref type="bibr">770 40,</ref><ref type="bibr">584 18,</ref><ref type="bibr">638 38,</ref><ref type="bibr">285 48,</ref><ref type="bibr">425</ref> Table <ref type="table">4</ref>: Results of the OOKB experiment: accuracy of the simple baseline and proposed models. Bold and underlined figures are respectively the best and second best scores for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Tail Both</head><p>Method Pooling 1,000 3,000 5,000 1,000 3,000 5,000 1,000 3,000 5,000 function, we used the max pooling function in the standard triplet classification, and tried three pooling functions, max, sum, average, in the OOKB setting. The results of the preliminary experiments were reflected in our selection of the absolute-margin objective function over the pairwise-margin objective function as well as the margin value ? = 300 in the absolute-margin objective function (Eq. ( <ref type="formula">8</ref>)). The absolutemargin objective function converged faster than the pairwisemargin objective function. Because the task is a binary classification of triplets into positive (i.e., the relations that must be present in the knowledge base) and negative triplets (those that must not), we determined the threshold value for output scores between these classes using the validation data.</p><p>To deal with the limited available computational resources (e.g., GPU memory), we sampled the neighbor entities randomly when an entity has too many of them. Indeed, there were some entities that appeared in a large number of the triplets, and thus had many neighbors; when the neighborhood size exceeded 64, we randomly chose 64 entities from the neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Standard Triplet Classification</head><p>We compared our model with the previous KBC models in the standard setting, in which no OOKB entities are involved.</p><p>Datasets We used WordNet11 and Freebase13 <ref type="bibr" target="#b8">[Socher et al., 2013]</ref> for evaluation. The data files were downloaded from http://cs.stanford.edu/people/danqi/. These datasets are subsets of two popular knowledge graphs, WordNet <ref type="bibr">[Miller, 1995]</ref> and Freebase <ref type="bibr" target="#b0">[Bollacker et al., 2008]</ref>. The specifications on these datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. Both datasets contain training, validation, and test sets. The validation and test sets include positive and negative triplets. In contrast, the training set does not contain negative triplets. As usual with the case in which negative triplets are not available, corrupted triplets are generated from positive triplets are used as a substitute for negative triplets. From a positive triplet (h, r, t) in knowledge base G, a corrupted triplet is generated by substituting a random entity sampled from E(G) for h or t. Specifically, to generate corrupted triplets, we used the "Bernoulli" trick, a technique also used in <ref type="bibr">[Wang et al., 2014b;</ref><ref type="bibr" target="#b5">Lin et al., 2015;</ref><ref type="bibr">Ji et al., 2015;</ref><ref type="bibr" target="#b5">Ji et al., 2016]</ref>.</p><p>Result The results are shown in Table <ref type="table" target="#tab_2">2</ref>. Our model showed state-of-the-art performance on the WordNet11 dataset. On the Freebase13 dataset, it did not perform as well as the stateof-the-art KBC methods, although it was slightly better than TransE on which our model was built on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OOKB Entity Experiment</head><p>Datasets We processed the WordNet11 dataset to construct several datasets for our OOKB entity experiment.</p><p>In total, nine datasets were constructed with different numbers and positions of OOKB entities sampled from the test set. The process consists of two steps: choosing OOKB entities and filtering and splitting triplets.</p><p>1. Choosing OOKB entities. To choose the OOKB entities, we first selected N = 1, 000, 3, 000, and 5, 000 triplets from the WordNet11 test file. For each of these three sets, we chose the initial candidates for the OOKB entities (denoted by I) in three different ways (thereby yielding nine datasets in total); these settings are called Head, Tail, and Both. In the Head setting, all head entities in the N triplets are regarded as candidate OOKB entities. The Tail setting is similar, but with the tail entities regarded as candidates.</p><p>In the Both setting, all entities appearing as either a head or tail are the candidates.</p><p>The final OOKB entities are the entities e ? I that appear in a triplet (e, r, e ) or (e , r, e) in the WordNet11 training set, with e ? I. Note that the entities h, t ? I are contained in the knowledge bases we already have and not in the OOKB entities. This last process filters out candidate OOKB entities that do not have any connection with the training entities.</p><p>2. Filtering and splitting triplets. Using the selected OOKB entities, the original training dataset was split into the training dataset and the auxiliary datasets for the OOKB entity problem. That is, triplets that did not contain the OOKB entities were placed in the OOKB training set, and triplets containing one OOKB entity and one non-OOKB entity were placed in the auxiliary set. Triplets that contained two OOKB entities were discarded.</p><p>For the test triplets, we used the same first N triplets in the WordNet11 test file that we used in Step 1, with the exception that the triplets that did not contain any OOKB entities were removed. For the validation triplets, we simply removed the triplets containing OOKB entities from the WordNet11 validation set.</p><p>The details of the generated datasets are shown in Table <ref type="table" target="#tab_3">3</ref>. We denote each of the nine datasets by {Head, Tail, Both}-{1,000, 3,000, 5,000}, respectively, where the first part represents the position of OOKB entities and the second part represents the number of triplets used for generating the OOKB entities.</p><p>Result Using the nine datasets generated from WordNet11, we verified the effectiveness of our proposed model.</p><p>We used the following simple method as the baseline in this experiment. Given an OOKB entity u, we first obtained the embedding vectors of the neighborhood (determined by the triplets in the auxiliary knowledge) using TransE, and then converted these vectors into the representation vector of u using a pooling function: sum, max, or average. Note that because all the neighborhood entities of u are in the training knowledge base, their vectors can be computed using standard KBC methods. We followed the original paper <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> of TransE for the hyperparameter and other settings.</p><p>The results are shown in Table <ref type="table">4</ref>. The column labeled "pooling" indicates which pooling function was used. As the table shows, our model outperforms the baselines considerably. In particular, Graph-NN with average pooling outperforms the other methods on all datasets. Graph-NN with max pooling also shows good accuracy, but in several settings, such as Tail-3000, it was outperformed by the baseline model with average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stacking and Unrolling Graph-NNs</head><p>Stacking and unrolling are important techniques because they enable us to broadcast node information to distant nodes. In this experiment, we illustrate the effect of stacking and unrolling in the standard triplet classification task. The dataset is WordNet11, used for standard triplet classification. Accuracy Comparison Table <ref type="table" target="#tab_5">5</ref> shows the performance of the stacked and unrolled Graph-NNs. The parameter "depth" indicates how many times the propagation model is iteratively applied. Note that when depth = 1, the two models reduce to the vanilla Graph-NN, for which the result was 87.8%, as also shown in Table <ref type="table" target="#tab_2">2</ref>. These results imply that the stacking and unrolling techniques do not improve performance. We believe this is because of the power of the embedding models, i.e., we can embed information about distant nodes into a continuous space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a new KBC task in which entities unobserved at training time are involved. For this task, we proposed a Graph-NN tailored to KBC with OOKB entities. We conducted two triplet classification tasks to verify the effectiveness of our proposed model. In the OOKB entity problem, our model outperformed the baselines considerably. Our model also showed state-of-the-art performance on WordNet11 in the standard KBC setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: OOKB entity problem. At test time, we receive a new triplet (1) depicted as a red double arrow, which contains an entity "Blade Runner" that is not observed in the knowledge graph (shown as the shaded box). The task is to tell whether any other relations hold between Blade Runner and the entities in the knowledge graph, by using (1) the new triplet and (2) the existing triplet, depicted by a black arrow. For example, we want to answer the question, "Is Blade Runner science fiction?", i.e., whether (3) the green dashed arrow in the figure should be drawn.</figDesc><graphic url="image-1.png" coords="1,345.38,216.00,182.25,98.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, and knowledge base G forms the training set (with only positive examples), with H being the test set. The set H can be divided into the set of positive test examples H ? G gold = G gold \G and the set of negative test examples H\G gold .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Specifications of the triplet classification datasets. half of the validation and test sets are negative triplets, and these are included in the numbers of validation triplets and test triplets.</figDesc><table><row><cell></cell><cell cols="2">WordNet11 Freebase13</cell></row><row><cell>Relations</cell><cell>11</cell><cell>13</cell></row><row><cell>Entities</cell><cell>38,696</cell><cell>75,043</cell></row><row><cell>Training triplets</cell><cell>112,581</cell><cell>316,232</cell></row><row><cell>Validation triplets</cell><cell>5,218</cell><cell>11,816</cell></row><row><cell>Test triplets</cell><cell>21,088</cell><cell>47,466</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Result of the standard KBC experiment (without OOKB entities). The figures represent accuracy. Except for the proposed method, they are obtained from the respective papers. Bold and underlined figures are the best and second best scores for each dataset, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Number of entities and triplets in the the OOKB datasets. The numbers of triplets include negative triplets.</figDesc><table><row><cell></cell><cell></cell><cell>Head</cell><cell></cell><cell></cell><cell>Tail</cell><cell></cell><cell></cell><cell>Both</cell><cell></cell></row><row><cell></cell><cell>1,000</cell><cell>3,000</cell><cell>5,000</cell><cell>1,000</cell><cell>3,000</cell><cell>5,000</cell><cell>1,000</cell><cell>3,000</cell><cell>5,000</cell></row><row><cell>Training triplets</cell><cell cols="9">108,197 99,963 92,309 96,968 78,763 67,774 93,364 71,097 57,601</cell></row><row><cell>Validation triplets</cell><cell>4,613</cell><cell>4,184</cell><cell>3,845</cell><cell>3,999</cell><cell>3,122</cell><cell>2,601</cell><cell>3,799</cell><cell>2,759</cell><cell>2,166</cell></row><row><cell>OOKB entities</cell><cell>348</cell><cell>1,034</cell><cell>1,744</cell><cell>942</cell><cell>2,627</cell><cell>4,011</cell><cell>1,238</cell><cell>3,319</cell><cell>4,963</cell></row><row><cell>Test triplets</cell><cell>994</cell><cell>2,969</cell><cell>4,919</cell><cell>986</cell><cell>2,880</cell><cell>4,603</cell><cell>960</cell><cell>2,708</cell><cell>4,196</cell></row><row><cell>Auxiliary entities</cell><cell>2,474</cell><cell cols="2">6,791 10,784</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of stacked and unrolled Graph-NNs. The depth indicates the number of times the propagation model is iteratively applied.</figDesc><table><row><cell cols="3">Depth Stacking Unrolling</cell></row><row><cell>1</cell><cell>87.8</cell><cell>87.8</cell></row><row><cell>2</cell><cell>87.5</cell><cell>87.2</cell></row><row><cell>3</cell><cell>87.1</cell><cell>86.7</cell></row><row><cell>4</cell><cell>87.0</cell><cell>87.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>A similar task had been around for general graphs under the name of link prediction.Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Proceedings of the International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the anonymous reviewers for valuable comments. This work was partially supported by <rs type="funder">JSPS Kakenhi</rs> Grant <rs type="grantNumber">15H02749</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zzRqW6v">
					<idno type="grant-number">15H02749</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Micha?l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2016. 2016. 2016</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity disambiguation by knowledge and text jointly embedding</title>
		<author>
			<persName><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<author>
			<persName><surname>Guu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015. 2016. 2016. 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>Proceedings of the 32nd International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<idno>CoRR, abs/1511.05493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Kairit Sirts, Lizhen Qu, and Mark Johnson</publisher>
			<date type="published" when="1995">2016. 2016. 2014. 2014. 2015. 2015. 2015. 2015. 1995. 2016. 2016</date>
			<biblScope unit="volume">6980</biblScope>
			<biblScope unit="page" from="40" to="50" />
		</imprint>
	</monogr>
	<note>Proceedings of the 30th AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Knowledge graph embedding for precise link prediction</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2016. 2016</date>
			<biblScope unit="page" from="1315" to="1321" />
		</imprint>
	</monogr>
	<note>Proceedings of the 28th AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TransG: A generative model for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A translation-based knowledge graph embedding preserving logical property of relations</title>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aligning knowledge and text embeddings by entity descriptions</title>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
