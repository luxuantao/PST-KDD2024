<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manuscript received May 29, 2001; revised April 22, 2003. The associate editor coordinating the review of this manuscript and approving it for publication was</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">D</forename><surname>Trac</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">A M</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Bruckstein</surname></persName>
						</author>
						<author>
							<persName><surname>Kimmel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The Technion-Israel Institute of Technology</orgName>
								<address>
									<postCode>32000</postCode>
									<settlement>Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department-SCCM Program</orgName>
								<orgName type="institution">Stan-ford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manuscript received May 29, 2001; revised April 22, 2003. The associate editor coordinating the review of this manuscript and approving it for publication was</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">520D747301A8728A03F933E1DA2A37B8</idno>
					<idno type="DOI">10.1109/TIP.2003.816023</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Down-Scaling for Better Transform Compression</head><p>Alfred M. Bruckstein, Michael Elad, and Ron Kimmel Abstract-The most popular lossy image compression method used on the Internet is the JPEG standard. JPEG's good compression performance and low computational and memory complexity make it an attractive method for natural image compression. Nevertheless, as we go to low bit rates that imply lower quality, JPEG introduces disturbing artifacts. It is known that at low bit rates a down-sampled image when JPEG compressed visually beats the high resolution image compressed via JPEG to be represented with the same number of bits. Motivated by this idea, we show how down-sampling an image to a low resolution, then using JPEG at the lower resolution, and subsequently interpolating the result to the original resolution can improve the overall PSNR performance of the compression process. We give an analytical model and a numerical analysis of the down-sampling, compression and up-sampling process, that makes explicit the possible quality/compression trade-offs. We show that the image auto-correlation can provide good estimate for establishing the down-sampling factor that achieves optimal performance. Given a specific budget of bits, we determine the down sampling factor necessary to get the best possible recovered image in terms of PSNR.</p><p>Index Terms-Bit allocation, image down-sampling, JPEG compression, quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE most popular lossy image compression method used on the Internet is the JPEG standard <ref type="bibr" target="#b0">[1]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> presents a basic block diagram of the JPEG encoder. JPEG uses the Discrete Cosine Transform (DCT) on image blocks of size 8 8 pixels. The fact that JPEG operates on small blocks is motivated by both computational/memory considerations and the need to account for the nonstationarity of the image. A quality measure determines the (uniform) quantization steps for each of the 64 DCT coefficients. The quantized coefficients of each block are then zigzag-scanned into one vector that goes through a run-length coding of the zero sequences, thereby clustering long insignificant low energy coefficients into short and compact descriptors. Finally, the run-length sequence is fed to an entropy coder, that can be a Huffman coding algorithm with either a known dictionary or a dictionary extracted from the specific statistics of the given image. A different alternative supported by the standard is arithmetic coding.</p><p>JPEG's good middle and high rate compression performance and low computational and memory complexity make it an at-tractive method for natural image compression. Nevertheless, as we go to low bit rates that imply lower quality, the JPEG compression algorithm introduces disturbing blocking artifacts. It appears that at low bit rates a down-sampled image when JPEG compressed and later interpolated, visually beats the high resolution image compressed directly via JPEG using the same number of bits. Whereas this property is known to some in the industry (see, for example, <ref type="bibr" target="#b8">[9]</ref>), it was never explicitly proposed nor treated in the scientific literature. One might argue however, that the hierarchical JPEG algorithm implicitly uses this idea when low bit-rate compression is considered <ref type="bibr" target="#b0">[1]</ref>.</p><p>Let us first establish this interesting property though a simple experiment, testing the compression-decompression performance both by visual inspection and quantitative mean-square-error comparisons. An experimental result displayed in Fig. <ref type="figure" target="#fig_1">2</ref> shows indeed that both visually and in terms of the Mean Square Error (or PSNR), one obtains better results using down-sampling, compression, and interpolation after the decompression. Two comments are in order at this stage: i) throughout this paper, all experiments are done using Matlab v.6.1. Thus, simple IJG-JPEG is used with fixed quantization tables, and control over the compression is achieved via the Quality parameter and ii) throughout this paper, all experiments applying down-sampling use an anti-aliasing pre-filter, as Matlab 6.1 suggests, through its standard image resizing function.</p><p>Let us explain this behavior from an intuitive perspective. Assume that for a given image we use blocks of 8 8 pixels in the coding procedure. As we allocate too few bits (say 4 bits per block on average), only the DC coefficients are coded and the resulting image after decompression consists of essentially constant valued blocks. Such an image will clearly exhibit strong blocking artifacts. If instead the image is down-sampled by a factor of 2, the coder is now effectively working with blocks of 16 16 and has an average budget of bits to code the coefficients. Thus, some bits will be allocated to higher order DCT coefficients as well, and the blocks will exhibit more detail. Moreover, as we up-scale the image at the decoding stage we add another improving ingredient to the process, since interpolation further blurs the blocking effects. Thus, the down-sampling approach is expected to result in better both visually and qualitatively outcomes.</p><p>In this paper we propose an analytical explanation to the above phenomenon, along with a practical algorithm to automatically choose the optimal down-sampling factor for best PSNR. Following the method outlined in <ref type="bibr" target="#b3">[4]</ref>, we derive an analytical model of the compression-decompression reconstruction error as a function of the memory budget, (i.e., the total number of bits) the (statistical) characteristics of the image, and the down-sampling factor. We show that a simplistic  second order statistical model provides good estimates for the down-sampling factors that achieves optimal performance. This paper is organized as follows.Sections II-IV present the analytic model and explore its theoretical implications. In Section II we start the analysis by developing a model that describes the compression-decompression error based on the quantization error and the assumption that the image is a realization of a Markov random field. Section III then introduces the impact of bit-allocation so as to relate the expected error to the given bit-budget. In Section IV we first establish several important parameters used by the model, and then use the obtained formulation in order to graphically describe the trade-offs between the total bit-budget, the expected error, and the coding block-size. Section V describes an experimental setup that validates the proposed model and its applicability for choosing best down-sampling factor for a given image with a given bits budget. Finally, Section VI ends the paper with some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ANALYSIS OF A CONTINUOUS "JPEG-STYLE" IMAGE REPRESENTATION MODEL</head><p>In this section we start building a theoretical model for analyzing the expected reconstruction error when doing compression-decompression as a function of the total bits budget, the characteristics of the image, and the down-sampling factor. Our model considers the image over a continuous domain rather then a discrete one, in order to simplify the derivation. The steps we follow are as follows.</p><p>1) We derive the expected compression-decompression mean-square-error for a general image representation. Slicing of the image domain into by blocks is assumed.</p><p>2) We use the fact that the coding is done in the transform domain using an orthonormal basis, to derive to error induced due to truncation only. 3) We extend the calculation to account for quantization error of the nontruncated coefficients. 4) We specialize the image transform to the DCT basis. 5) We introduce an approximation for the quantization error, as a function of the allocated bits. 6) We explore several possible bit-allocation policies and introduce the overall bit-budget as a parameter into our model. At the end of this process we obtain an expression for the expected error as a function of the bit budget, scaling factor, and the image characteristics. This function eventually allows us to determine the optimal down-sampling factor in JPEG-like image coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Compression-Decompression Expected Error</head><p>Assume we are given images on the unit square , , realizations of a 2-D random process , with second order statistics given by <ref type="bibr" target="#b0">(1)</ref> Note that here we assume that the image is stationary. This is a marked deviation from the real-life scenario, and this assumption is done mainly to simplify our analysis. Nevertheless, as we shall hereafter see, the obtained model succeeds in predicting the down-sampling effect on compression-decompression performance.</p><p>We assume that the image domain is sliced into regions of the form Assume that due to our coding of the original image we obtain the compressed-decompressed result , which is an approximation of the original image. We can measure the error in approximating by as follows:</p><p>(</p><formula xml:id="formula_0">)<label>2</label></formula><p>where we define</p><p>(3)</p><p>We shall, of course, be interested in the expected mean square error of the digitization, i.e.,</p><p>Note that the assumed wide-sense stationarity of the image process results in the fact that the expression is independent of , i.e., we have the same expected mean square error over each slice of the image. Thus, we can write <ref type="bibr" target="#b4">(5)</ref> Up to now we considered the quality measure to evaluate the approximation of in the digitization process. We shall next consider the set of basis functions needed for representing over each slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bases for Representing Over Slices</head><p>In order to represent the image over each slice , we have to choose an orthonormal basis of functions. Denote this basis by . We must have if otherwise.</p><p>If is indeed an orthonormal basis then we can write <ref type="bibr" target="#b5">(6)</ref> as a representation of over in terms of an infinite set of coefficients <ref type="bibr" target="#b6">(7)</ref> Suppose now that we approximate over by using only a finite set of the orthonormal functions , i.e., consider <ref type="bibr" target="#b7">(8)</ref> The optimal coefficients in the approximation above turn out to be the corresponding 's from the infinite representation. The mean square error of this approximation, over say, will be where . Some algebraic steps leads to the following result for the expected . The expected is therefore given by (15) Hence, in order to evaluate in a particular representation when the image is sliced into pieces and over each piece we use a subset of the possible basis functions (i.e.,</p><p>) and we quantize the coefficients with -bits we have to evaluate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. An Important Particular Case: Markov Process With Separable Cosine Bases</head><p>We now return to the assumption that the statistics of is given by ( <ref type="formula">1</ref>), namely, and we choose a separable cosine basis for the slices, i.e., over , , where</p><p>This choice of using the DCT basis is motivated by our desire to model the JPEG behavior. As is well known <ref type="bibr" target="#b5">[6]</ref>, the DCT offers a good approximation of the KLT if the image is modeled as a 2-D random Markov field with very high correlation factor.</p><p>To compute for this case we need to evaluate the variances of defined as (16)</p><p>We have</p><formula xml:id="formula_2">(17)</formula><p>Therefore, by separating the integrations we obtain (18)</p><p>Changing variables of integration to yields</p><p>Let us define, for compactness, the following integral:</p><p>(20) Then we see that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Incorporating the Effect of Coefficient Quantization</head><p>According to rate-distortion theory, if we either assume uniform or Gaussian random variables, there is a formula for evaluating the Mean-Square-Error due to quantization. This formula, known to be accurate at high rates, is given by <ref type="bibr" target="#b2">[3]</ref> (24</p><formula xml:id="formula_4">)</formula><p>where is a constant in the range and represents the number of bits allocated for representing . Putting the above results together, we get that the expected mean square error in representing images from the process with Markov statistics, by slicing the image plane into slices and using, over each slice, a cosine basis is given by (25) This expression gives in terms of and -the bits allocated to the coefficients where the subset of the coefficient is given via .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SLICING AND BIT-ALLOCATION OPTIMIZATION PROBLEMS</head><p>Suppose we consider (26) as a function of . We have that the total bit usage in representing the image is Now we can solve a variety of bit-allocation and slicing optimization problems <ref type="bibr" target="#b2">[3]</ref>.</p><p>It is important to note that in adopting a bit-allocation procedure we effectively deviate from the way the JPEG assigns bits to the different DCT coefficients. In the JPEG algorithm, a uniform quantizer is used, which at a first glance may appear to be inferior to any reasonable bit-allocation method. However, due to the entropy coding (Huffman, and RLE) following the quantization stage, the overall bit assignment effect seems to be similar to a global procedure of bit-allocation based on variances, and for that matter, approximates well the rate-distortion behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimal Local Bit Allocation and Slicing Given Total Bit Usage</head><p>The problem here is: Given the constraint , find that minimize the . Thus, the following expression is to be minimized with respect to <ref type="bibr">(27)</ref> This is a classical bit allocation process and we have that the optimal bit allocation yields (theoretically) the same error for all terms in (28) where we defined as the number of quantization levels, see <ref type="bibr" target="#b5">[6]</ref>. Hence, we need </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Slicing With Rigid Relative Bit Allocation</head><p>An alternative bit allocation strategy, perhaps more in the spirit of the classical JPEG standard, can also be thought of. Consider that is chosen and the 's are also chosen a priori for all</p><p>. Then, we have (39) as a function of and . This function clearly decreases with increasing and since more and more bits are allocated to the image, and here . Suppose now that for , we choose a certain bit allocation for a given (say ), i.e., we chose but now as we increase the number of slices (i.e., increase and ) we shall modify the 's to keep a constant by choosing . Here remains a constant and we can again analyze the behavior of as and vary. In our experiments we assumed that the above limit was set to 7 (i.e., coefficients with are assigned with bits). As to the choice of the per each of these coefficients, we used a fixed template based on the following JPEG quantization table (see <ref type="bibr" target="#b0">[1]</ref>) that specifies the quantization step per each coefficient Values of were chosen to be inversely proportional to these values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Soft Bit Allocation With Cost Functions For Error and Bit Usage</head><p>We could also consider cost functions of the form where and are cost functions chosen according to the task in hand, and ask for the bit allocation that minimize the joint functionals, in the spirit of <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL PREDICTIONS OF THE MODEL</head><p>In Sections II and III, we proposed a model for the compression error as a function of the image statistics , the given total bits budget , and the number of slicings and . Here, we fix these parameters according to the behavior of natural images and typical compression setups and study the behavior of the theoretical model.</p><p>Assume we have a gray scale image of size 512 512 with 8 bits/pixel as our original image. JPEG considers 8 8 slices of this image and produces, by digitizing the DCT transform coefficients with a predetermined quantization table, approximate representation of these 8 8 slices. We would like to explain the observation that down-sampling the original image, prior to applying JPEG compression to a smaller image, produces with the same bit usage, a better representation of original image.</p><p>Suppose the original image is regarded as the "continuous" image defined over the unit square , as we have done in the theoretical analysis. Then, the pixel width of a 512 512 image will be . We shall assume that the original image is a realization of a zero mean 2-D stationary random process with autocorrelation of the form , with , and in the range of , as is usually done (see <ref type="bibr" target="#b5">[6]</ref>). From a single image, can be estimated via the expression assuming an equalized histogram. If we consider that we can obtain an estimate for using , This provides The total number of bits for the image representation will range from 0.05 bpp to about 2.0 bpp, hence, will be between 512 512 0.05 13 107 to 512 512 2 524, 288 bits for 512 512 original images. Therefore, in the theoretical evaluations we shall take , for 256 gray level images, with total bit usage between 10 000 and 20 000.</p><p>The symmetric and axis slicings considered will be , where we assume that and . Then we we shall evaluate [see ( <ref type="formula">26</ref>)] with -s provided by the optimal level allocation Practically, the optimal level allocation should be given by , a measure that automatically prevents the allocation of negative numbers of bits. Obviously this step must be followed by re-normalization of the bit allocation in order to comply with the bits budget constraint. can be taken from 1 to 3, whereas will be , , simulating the standard JPEG approach which is coding of 8 8 transform coefficients, emphasizing the low frequency range via the precise encoding of only about coefficients. Using the above described parameter ranges, we plot the predictions of the analytical model for the expected mean square error as a function of the slicings with bit usage as a parameter.</p><p>Figs. <ref type="figure">3</ref> and<ref type="figure" target="#fig_5">4</ref> demonstrate the approximated error as a function of the number of slicings for various total number of bits. Fig. <ref type="figure">3</ref> displays the predictions of the theoretical model in conjunction with optimal level allocation while Fig. <ref type="figure" target="#fig_5">4</ref> uses the JPEG style rigid relative bit allocation. In both figures the left side shows the results of restricting the number of bits or quantization levels to integers, while the right side shows the results allowing fractional bit and level allocation.</p><p>These figures show that for every given total number of bits there is an optimal slicing parameter indicating the optimal down-sampling factor. For example, if we focus on the bottom right graph (rigid bit allocation with possible fractions), if 50 Kbits are used, the optimal is found to be 32. This implies that an image of size 512 512 should be sliced to blocks of size . As we move to a budget of only 10 Kbits, optimal is found to be 18 and the block size to work with becomes . Since JPEG works with fixed block size of 8 8, the first case of 50 Kbits Fig. <ref type="figure">3</ref>. Theoretical prediction of MSE based on optimal bit allocation versus number of slicings M with total bits usage as a parameter. Here, we used the typical values = 150, and K = 3. calls for a down-sampling by a factor of 2, and the second case with 10 Kbits requires a down-scaling factor of 3.5.</p><p>Note that integer bit allocation causes in both cases nonsmooth behavior. Also in Fig. <ref type="figure">3</ref> it appears that the minimum points are local ones and the error tends to decrease again as increases. This phenomenon can be explained by the fact that we used an approximation of the quantization error which fails to predict the true error for a small number of bits at large down-scaling factors. Finally, we should note that the parameter was chosen differently between the two allocation policies. Within the range , we empirically set a value that matched the true JPEG behavior. The overall qualitative behavior however was quite similar for all the range of 's.</p><p>Fig. <ref type="figure">5</ref> shows the theoretical prediction of PSNR versus bits per pixel curves for typical 512 512 images with different down-sampling factors (different values of , where the downsampling factor is ). One may observe that the curve intersections occur at similar locations as those of the experiments with real images shown in Section V. Also, it appears that even though the allocation policies are different, the results are very similar. An interesting phenomenon is observed in these graphs: For down-sampling factors smaller than 1 an almost flat saturation of the PSNR versus bit-rate is seen at sufficiently large bit-rate. This phenomenon is quite expected, since the down-sampling of the image introduces un-recoverable loss. Thus, even an infinite amount of bits can not recover this induced error, and this is the obtained saturation height. The reason this flattering happens at lower bit-rates for smaller factors is that the smaller the image, the smaller the amount that will be considered as leading to near-perfect transmission. In terms of quality-scalable coder, this effect is parallel to the attempt to work with an image pyramid representation and allocating too many bits to a specific resolution layer, instead of also allocating bits to the next resolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. COMPRESSION RESULTS OF NATURAL AND SYNTHETIC IMAGES</head><p>To verify the validity of the analytic model and design a system for image trans-coding we can generate synthetic images for which the autocorrelation is similar to that of a Fig. <ref type="figure">5</ref>. Optimal (top) and rigid relative (bottom) bit allocation based prediction of PSNR versus bits per pixel with image down-sampling as a parameter. Again, we used the typical values = 150, and K = 3 for the optimal bit allocation case, and K = 1 for the JPEG-style case.</p><p>given image. Then we can plot the PSNR/bpp JPEG graphs for all JPEG qualities, one graph for each given down-sampling ratio. The statistical model is considered valid if the behavior is similar for the natural image and the synthesized one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Synthesis</head><p>Assume that for an image the autocorrelation function is that of a sample of an ergodic homogeneous random field of the form we assumed, hence Define the Fourier transform . Then, the power spectrum of the real signal is given by Now, considering a 1D signal with the above statistics, we have Thus, we have that . The solution is chosen so as to satisfy for . Therefore</p><p>To generate synthetic images, we can "color" a uniform random (white) noise as follows. Let be an matrix in which each entry is a uniformly distributed random number. Next, let be an matrix with elements otherwise and similarly, is an matrix such that otherwise. A synthetic sample image with desired autocorrelation is then generated by the process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimating the Image Statistics and</head><p>In order to generate a synthetic image with the same statistics as that of the natural one, we have to first estimate the properties of the given image. Let us present a simple method for estimating the image statistics. We already used the relation Explicitly, for our statistical image model we have that the power spectrum is given by and the autocorrelation is Thus, all we need to do is to estimate the slopes of the plane given by This was the estimation procedure implemented in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>A JPEG compression performance comparison for a natural image and its random synthesized version is shown in Fig. <ref type="figure" target="#fig_6">6</ref> for a 256 256 image. The examples in Figs. <ref type="figure" target="#fig_7">7</ref><ref type="figure" target="#fig_8">8</ref><ref type="figure" target="#fig_9">9</ref>, are of 512 512 images. The figures show the compression results of synthetic versus natural images with similar statistics. Synthetic and original images and their corresponding autocorrelations are presented with their corresponding JPEG PSNR/bpp compression curves for 4 down-sampling factors. Fig. <ref type="figure" target="#fig_9">9</ref> presents an image for which the statistical model is highly inaccurate due to the diagonal texture that characterizes a relatively large part of the "Barbara" image. In all these examples we specify the estimated values of and . Note that since these images are of size 256 256, the values are to be multiplied by 2 if they are to be compared to the range of suggested in Section V.</p><p>The above experiments indicate that the crossing locations between down-sampling factors in the synthetic images appear to be a good approximation of the crossings in the natural images. Thus, based on the second order statistics of the image we can predict the optimal down-sampling factor. Moreover, the nonstationarity nature of images has a relatively minor impact on the optimal down-sampling factor. This is evident from the alignment of the results of the natural and the synthetic images. There appears to be a vertical gap (in PSNR) between the syn-thetic and the natural images. However, similar PSNR gaps also appear between different synthetic images. Based on the above two observations (ability to use stationary model, and ability to refer to the second moment only), we assert that one can predict the best down-sampling factor based on the model proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This paper started with the observation that a the use of downsampling prior to JPEG coding can improve the overall coding performance both objectively and subjectively. We then presented an analytical model to explain this phenomenon. A set of experiments are shown to verify this model and support the idea of down-sampling before transform coding for optimal compression. Based on the theoretical developments above, a simple algorithm can be developed to set the optimal down-sampling for a given image, based on the image statistics, size, and bit budget available. Further work is required in order explore extensions and implementation issues, such as an efficient estimation for the image statistics, extraction of second order statistics locally and using an hierarchical slicing of the image to various block sizes, and more.</p><p>The emerging new standard for image compression, JPEG-2000, is based on coding with a wavelet transform (see <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b6">[7]</ref>). This new algorithm is known to outperform the regular JPEG. Among the many reasons to this performance improvement, is the fact that wavelet-based coding applies a multiresolution analysis of the underlying image. In this sense, the work presented here proposes the introduction of a simple multiscale feature into the JPEG standard, thereby gaining compression ratio. Further work is required to replace the approach presented here to a locally adaptive one, as is done naturally by the wavelet coders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>To compute the second order statistics of the coefficients we need to carry out the following integral (40)</p><p>We shall separate the integration into two parts i.e., . Note however that , hence, . Hence, after some algebraic steps we obtain and indeed, This agrees with the general expression we got above.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. JPEG encoder block diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Original image (on the left), JPEG compressed-decompressed image (middle), and down-sampled-JPEG compressed-decompressed and up sampled image (right). The down-sampling factor was 0.5. The compressed 256 2 256 "Lena" image in both cases used 0.25 bpp inducing MSEs of 219.5 and 193.12, respectively. The compressed 512 2 512 "Barbara" image in both cases used 0.21 bpp inducing MSEs of 256.04 and 248.42, respectively.</figDesc><graphic coords="2,88.50,258.17,413.35,305.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>C. Effect of Quantization of the Expansion CoefficientSuppose that in the approximation (13) we can only use a finite number of bits in representing the coefficients that take values in R. If is represented/encoded with -bits we shall be able to describe it via that takes on values only, i.e., set of representation levels. The error in representing in this way is Let us now see how the quantization errors affect the . We have (14)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Rigid relative bit allocation based prediction of MSE versus number of slicings M with total bits usage as a parameter. Here, we used the typical values = 150, and K = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison between a natural image, and a synthesized one with similar autocorrelation ( = 27, = 11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison between a natural image and a synthesized one with similar autocorrelation ( = 50, = 100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison between a natural and a synthesized image with similar autocorrelation ( = 39, = 91).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison between a natural and a synthesized image with similar autocorrelation ( = 42, = 26).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors thank Dr. M. Rudzsky and A. Spira for intriguing discussions. They also thank Y. Katz for translating some of their never-ending equations into LaTex. Also, they thank the reviewers of this paper and the associate editor for their valuable and detailed comments and suggestions for improvements of the presentation of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">JPEG Still Image Compression Standard</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High performance scalable image compression with EBCOT</title>
		<author>
			<persName><forename type="first">D</forename><surname>Taubman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1158" to="1170" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vector Quantization and Signal Compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On optimal image digitization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="553" to="555" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On soft bit allocation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="614" to="617" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Fundamentals of Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">JPEG 2000 standard: Still image compression scheme of 21st century</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JPEG2000 Tutorial, the European Signal Processing Conference (EUSIPCO 2000)</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05-08">Sept. 5-8, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">JPEG-2000 Image Coding System WG1N390 REV</title>
		<ptr target="ISO/IECJTC1/SC29/WG1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://public.migrator2000.org/J2Kdemonstrator" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Since 1985, he has been a Faculty Member of the Computer Science Department at The Technion-Israel Institute of Technology, where he currently a Full Professor, holding the Ollendorff Chair. During the summers from 1986 to 1995 and from 1998 to 2000 he was a Visiting Scientist at Bell Laboratories. He is in the editorial boards of Pattern Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alfred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bruckstein received the B.Sc. degree (with honors) and the M.Sc. degree in electrical engineering from The Technion-Israel Institute of Technology, Haifa, and the Ph.D. degree in electrical engineering from Stanford University</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="1977">1977. 1980. 1984</date>
		</imprint>
	</monogr>
	<note>He also served as a member of program committees of 20 conferences. His research interests are in Image and signal processing, computer vision, computer graphics, pattern recognition, robotics, especially ant robotics, applied geometry, estimation theory and inverse scattering, and neuronal encoding process modeling. and MM. He was awarded the Rothschild Fellowship for Ph.D. Studies at Stanford, Taub Award, Theeman Grant for a scientific tour of Australian Universities, Hershel Rich Technion Innovation Award, and Hershel Rich Innovation Award</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Elad received the Bs.C. (with honors), Ms.C., and Ds.C. degrees from the Electrical Engineering Department of The Technion-Israel Institute of Technology</title>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986. 1988. 1996. 1988 to 1993</date>
			<pubPlace>Haifa, Israel</pubPlace>
		</imprint>
	</monogr>
	<note>he served as an R&amp;D Officer in the Israeli Air-Force</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">His research interests include inverse problems, signal representations, and numerical algorithms in the areas of signal processing, image processing, and computer vision. He worked on super-resolution reconstruction of images, motion estimation, nonlinear filtering, sparse representations for signals, target detection in images, polar Fourier transform, and image compression</title>
		<author>
			<persName><surname>Hpl-I)</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hewlett-Packard Laboratories-Israel</title>
		<meeting><address><addrLine>Stanford, CA, as a Research Associate</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999. 2000. 2001</date>
		</imprint>
		<respStmt>
			<orgName>SCCM program) at Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>During 2000-2001 he led the research division in Jigami Corporation, Israel. In parallel, during 1997-2001 he was a Lecturer in the Electrical Engineering Department at The Technion. He is now with the Computer Science Department. Dr. Elad was awarded the Wolf, Gutwirth, and Ollendorff Fellowships. He was awarded Best Lecturer in</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">He was a Consultant with the HP Research Lab in image processing and analysis during 1998-2000, and to Net2Wireless/Jigami Research Group during 2000-2001. He has been is on the advisory board of MediGuide (biomedical imaging) since 2001, and has been on various program and organizing committees of conferences, workshops, and journal editorial boards, in the fields of image synthesizing, processing, and analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Israel Institute of Technology, Haifa ; University of California</orgName>
		</respStmt>
	</monogr>
	<note>Since 1998, he has been a Faculty Member of the Computer Science Department at The Technion, where he is currently an Associate Professor. His research interests are in computational methods and their applications in differential geometry, numerical analysis, image processing and analysis, computer aided design, robotic navigation, and computer graphics. Dr. Kimmel was awarded the Hershel Rich Technion Innovation Award (twice), the Henry Taub Prize for excellence in research, Alon Fellowship, the HTI Postdoctoral Fellowship, and the Wolf, Gutwirth, Ollendorff, and Jury Fellowships</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
