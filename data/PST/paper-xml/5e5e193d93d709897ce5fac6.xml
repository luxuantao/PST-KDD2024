<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SG-Net: Syntax-Guided Machine Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
							<email>zhangzs@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">College of Zhiyuan</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junru</forename><surname>Zhou</surname></persName>
							<email>zhoujunru@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sufeng</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<email>zhaohai@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>wangrui@nict.go.jp</email>
							<affiliation key="aff4">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SG-Net: Syntax-Guided Machine Reading Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For machine reading comprehension, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy passages and getting ride of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling of both passages and questions by incorporating explicit syntactic constraints into attention mechanism for better linguistically motivated word representations. To serve such a purpose, we propose a novel dual contextual architecture called syntax-guided network (SG-Net), which consists of a BERT context vector and a syntax-guided context vector, to provide more fine-grained representation. Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed approach achieves a substantial and significant improvement over the fine-tuned BERT baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, much progress has been made in generalpurpose language modeling that can be used across a wide range of tasks <ref type="bibr" target="#b22">(Peters et al., 2018;</ref><ref type="bibr" target="#b23">Radford et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2018)</ref>. Understanding the meaning of a sentence is a prerequisite to solve many natural language understanding (NLU) problems, such as machine reading comprehension (MRC) based question answering <ref type="bibr" target="#b24">(Rajpurkar et al., 2018)</ref>. Obviously, it requires a good representation of the meaning of a sentence.</p><p>A person reads most words superficially and pays more attention to the key ones during reading and un- derstanding sentences <ref type="bibr" target="#b32">(Wang et al., 2017a)</ref>. Although a variety of attentive models have been proposed to imitate human learning, most of them, especially global attention methods <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> equally tackle each word and attend to all words in a sentence without explicit pruning and prior focus, which would result in inaccurate concentration on some dispensable words <ref type="bibr" target="#b20">(Mudrakarta et al., 2018)</ref>.</p><p>We observe that the accuracy of MRC models decreases when answering long questions (shown in Section 5.1). Generally, if the text is particularly lengthy and detailed-riddled, it would be quite difficult for deep learning model to understand as it suffers from noise and pays vague attention on the text components, let alone accurately answering questions. In contrast, extensive studies have verified that human reads sentences efficiently by taking a sequence of fixation and saccades after a quick first glance <ref type="bibr" target="#b34">(Yu et al., 2017)</ref>.</p><p>Besides, for passage involved reading comprehen-arXiv:1908.05147v2 [cs.CL] 3 Sep 2019 sion, the input sequence always consists of multiple sentences. Nearly all of the current attentive methods and language models, e.g., BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, regard the input sequence as a whole, e.g., a passage, with no consideration of the inner linguistic structure inside each sentence. This would result in process bias caused by much noise and lack of associated spans for each concerned word. All these factors motivate us to seek for an informative method that can selectively pick out important words by only considering the related subset of words of syntactic importance inside each input sentence explicitly with a guidance of syntactic structure clues to give more accurate attentive signals and reduce the impact of noise brought about by lengthy sentences.</p><p>In this paper, we extend the self attention mechanism with syntax-guided constraint, to capture syntax related parts with each concerned word. Specifically, we adopt pre-trained dependency syntactic parse tree structure to produce the related nodes for each word in a sentence, namely dependency of interest (DOI), by regarding each word as a child node and the DOI consists all its ancestor nodes and itself in the dependency parsing tree. An example is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To effectively accommodate such DOI information, we propose a novel dual contextual architecture called syntax-guided network (SG-Net), which incorporates a BERT context vector and a syntax-guided context vector, to provide more fine-grained representation for challenging reading comprehension tasks. Our evaluations are based on two widely used challenging MRC tasks, span-based SQuAD 2.0 and multi-choice style RACE.</p><p>To our best knowledge, we are the first to integrate syntactic relationship as attentive guidance for machine reading comprehension and propose a general syntaxguided structure for deep aggregation. A series of experiments and analysis show the proposed method is effective and boosts the strong BERT baseline substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Reading Comprehension</head><p>In the last decade, the MRC tasks have evolved from the early cloze-style test <ref type="bibr" target="#b11">(Hill et al., 2015;</ref><ref type="bibr" target="#b10">Hermann et al., 2015;</ref><ref type="bibr" target="#b36">Zhang et al., 2018)</ref> to span-based answer extraction from passage <ref type="bibr" target="#b25">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b21">Nguyen et al., 2016;</ref><ref type="bibr" target="#b12">Joshi et al., 2017;</ref><ref type="bibr" target="#b24">Rajpurkar et al., 2018)</ref> and multi-choice style ones <ref type="bibr" target="#b16">(Lai et al., 2017)</ref> where the two latter ones are our focus in this work. A wide range of attentive models have been employed, including Attention Sum Reader <ref type="bibr" target="#b13">(Kadlec et al., 2016)</ref>, Gated attention Reader <ref type="bibr" target="#b7">(Dhingra et al., 2017)</ref>, Selfmatching Network <ref type="bibr" target="#b33">(Wang et al., 2017b)</ref>, Attention over Attention Reader <ref type="bibr" target="#b5">(Cui et al., 2017)</ref> and Bi-attention Network <ref type="bibr" target="#b28">(Seo et al., 2016)</ref>.</p><p>Recently, deep contextual language model has been shown effective for learning universal language rep-resentations by leveraging large amounts of unlabeled data, achieving various state-of-the-art results in a series of NLU benchmarks. Some prominent examples are Embedding from Language models (ELMo) <ref type="bibr" target="#b22">(Peters et al., 2018)</ref>, Generative Pre-trained Transformer (OpenAI GPT) <ref type="bibr" target="#b23">(Radford et al., 2018)</ref> and Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> among which BERT uses a different pre-training objective, masked language model, which allows capturing both sides of context, left and right. Besides, BERT also introduces a next sentence prediction task that jointly pre-trains text-pair representations. The latest evaluation shows that BERT is powerful and convenient for downstream tasks, which could be either easily applied to downstream models as the encoder or directly used for fine-tuning. Following this line, we extract context-sensitive syntactic features and take pre-trained BERT as our backbone encoder for empowering explicit syntactic dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactic Structures</head><p>Recently, dependency syntactic parsing have been further developed with neural network and attained new state-of-the-art results <ref type="bibr" target="#b3">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b8">Dozat and Manning, 2016;</ref><ref type="bibr" target="#b17">Ma et al., 2018)</ref>. Benefiting from the highly accurate parser, neural network models could enjoy even higher accuracy gains by leveraging syntactic information rather than ignoring it <ref type="bibr" target="#b27">(Roth and Lapata, 2016;</ref><ref type="bibr" target="#b8">He et al., 2017;</ref><ref type="bibr" target="#b18">Marcheggiani and Titov, 2017;</ref><ref type="bibr" target="#b9">He et al., 2019)</ref>.</p><p>Syntactic dependency parse tree provides a form that is capable of indicating the existence and type of linguistic dependency relation among words, which has been shown generally beneficial in various natural language understanding tasks <ref type="bibr" target="#b2">(Bowman et al., 2016)</ref>. To effectively exploit syntactic clue, most of previous works <ref type="bibr" target="#b14">(Kasai et al., 2019)</ref> absorb parse tree information by transforming dependency labels into vectors and simply concatenate the label embedding with word representation. However, such simplified and straightforward processing would result in higher dimension of joint word and label embeddings and is too coarse to capture contextual interactions between the associated labels and the mutual connections between labels and words. This inspires us to seek for an attentive way to enrich the contextual representation from the syntactic source. A related work is from <ref type="bibr" target="#b29">(Strubell et al., 2018)</ref>, which proposed to incorporate syntax with multi-task learning for semantic role labeling. However, their syntax is incorporated by training one extra attention head to attend to syntactic ancestors for each token while we use all the existing heads rather than add an extra one. Besides, this work is based on the remarkable representation capacity of recent language models such as BERT, which have been suggested to be endowed with some syntax to an extent <ref type="bibr" target="#b4">(Clark et al., 2019)</ref>. Therefore, we are motivated to apply syntactic constraints through syntax guided method to prune the self attention instead In this work, we form a general approach to benefit from syntax-guided representations, which is the first attempt for machine reading comprehension to our best knowledge. The idea of updating the representation of a word with information from its neighbors in the dependency tree is similar to that of the graph convolutional networks <ref type="bibr" target="#b1">(Bastings et al., 2017;</ref><ref type="bibr" target="#b18">Marcheggiani and Titov, 2017)</ref> and graph attention network <ref type="bibr" target="#b31">(Veličković et al., 2017)</ref> to some extent. However, the methodology and implementation are quite different since our proposed syntax-guided attention benefits from explicit syntactic constraints, which is well linguistically motivated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Syntax-Guided Network</head><p>Our goal is to design an effective neural network model which makes use of linguistic information as effectively as possible in order to perform end-to-end MRC. We first present the general syntax-guided attentive architecture, building upon the recent advanced BERT<ref type="foot" target="#foot_0">1</ref> and then fit with task-specific layers for machine reading comprehension tasks.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts the whole architecture of our model. The basis for our model is the fine-tuned BERT encoder introduced from <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>. We transform BERT embeddings into conditionally attentive representation using our proposed syntax-guided self attention layer. Then we integrate the outputs from the BERT and conditional attention learning. At last, the resulting syntax-enhanced representation is passed to task-specific layers for final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT Encoder</head><p>Following the implementation of BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, the first token of every sequence is the special token [CLS] and the sequences are separated by the [SEP] token. The output of BERT H is then fed to our proposed syntax-guided attention layers to obtain the The increase reflects lower credit losses</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root</head><p>The increase reflects lower credit losses</p><formula xml:id="formula_0">DT NN VBZ JJR NN NNS 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 Figure 3: An example of dependency of interest (DOI) mask.</formula><p>syntax-enhanced representation. We omit rather extensive formulations of BERT and recommend readers to get the details from <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>. The output of BERT model is then passed to our proposed syntaxguided self attention layer and the vanilla self attention layer (the same as that used in BERT), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntax-Guided Network</head><p>Our syntax-guided representation is obtained by two steps. Firstly, we pass the encoded representation to a syntax-guided self attention layer and the corresponding output is then aggregated with the original BERT encoder output to form a syntax-enhanced representation. The proposed model can be seen as an expansion of the attentive language models such as BERT, with the addition of a syntax-guided layer to incorporate syntactic dependency. It is designed to incorporate the syntactic tree structure information inside a multihead attention mechanism to indicate the token relationships of each sentence which will be demonstrated as follows.</p><p>Syntax-Guided Self Attention Layer In this work, we first pre-train a syntactic dependency parser to annotate the dependency structures for each sentence which are then fed to SG-Net as guidance of tokenaware attention. Details of the pre-training process of the parser are reported in Section 4.2.</p><p>To use the relationship between head word and de-pendent words provided by the syntactic dependency tree of sentence, we restrain the scope of attention only between word and all of its ancestor head words<ref type="foot" target="#foot_1">2</ref> . In other word, we would like to have each word only attend to words of syntactic importance in a sentence, the ancestor head words in the view of the child word. As shown in Figure <ref type="figure">3</ref>, instead of taking attention with each word in whole passage, the word credit only makes attention with its ancestor head words reflects and losses and itself in this sentence, which means that the DOI of credit contains reflects, losses along with itself<ref type="foot" target="#foot_2">3</ref> . Specifically, given input token sequence S = {s 1 , s 2 , ..., s n } where n denotes the sequence length, we first use syntactic parser to generate a dependency tree. Then, we derive the ancestor node set P i for each word s i according to the dependency tree. Finally, we learn a sequence of DOI mask M, organized as n * n matrix where n denotes the length of sentence, and elements in each row denote the dependency mask of all words to the row-index word.</p><formula xml:id="formula_1">M[i, j] = 1, if j ∈ P i or j = i 0, otherwise.<label>(1)</label></formula><p>Obviously, if M[i, j] = 1, it means that token s i is the ancestor node of token s j . As example shown in Figure <ref type="figure">3</ref>, the ancestors of credit (i=4) are reflects (j=2), losses (j=5) along with itself (j=4), so M[4, (2, 4, 5)] = 1 and M[4, (0, 1, 3)] = 0.</p><p>We then project the last layer output H from the vanilla BERT into the distinct key, value, and query representations of dimensions L × d k , L × d q , and L × d v , respectively, denoted K i , Q i , V i for each head i. Then we perform a dot product to score key-query pairs with the dependency of interest mask to obtain attention weights of dimension L × L, denoted A i :</p><formula xml:id="formula_2">A i = Softmax M • Q i K i T √ d k .<label>(2)</label></formula><p>We then multiply attention weight A i by V i to obtain the syntax-guided token representations:</p><formula xml:id="formula_3">W i = A i V i .<label>(3)</label></formula><p>Then W i for all heads are concatenated and passed through a feed-forward layer followed by GeLU activations (Hendrycks and Gimpel, 2016). After passing through another feed-forward layer, we apply a layer normalization to the sum of output and initial representation to obtain the final representation, denoted as</p><formula xml:id="formula_4">H = {h 1 , h 2 , ..., h n }.</formula><p>Dual Context Aggregation Compared with the multi-head attention in language models such as BERT, we integrate two contextual vectors for answer prediction: a vanilla BERT context vector directly from the last layer of the BERT encoder which always attends to all words and a syntax-guided context vector from our proposed syntax-guided layer. To that end, in addition to the traditional context vector H = {h 1 , h 2 , ..., h n } from BERT encoder, we learn a context vector H = {h 1 , h 2 , ..., h n } from syntax-guided layer from the above part. Formally, the final model output H = { h1 , h2 , ..., hn } is computed by:</p><formula xml:id="formula_5">hi = αh i + (1 − α)h i .</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-specific Adaptation</head><p>We focus on two types of reading comprehension tasks, span-based and multi-choice style which can be described as a tuple &lt; P, Q, A &gt; or &lt; P, Q, C, A &gt; respectively, where P is a passage (context) and Q is a query over the contents of P , in which a span or choice C is the right answer A. For the span-based one, we implemented our model on SQuAD 2.0 task that contains unanswerable questions. Our system is supposed to not only predict the start and end position in the passage P and extract span as answer A but also return a null string when the question is unanswerable. For the multi-choice style, the model is implemented on RACE dataset which is requested to choose the right answer from a set of candidate ones according to given passage and question.</p><p>Here, we formulate our model for both of the two tasks and feed the output from the syntax-guided network to task layers according to specific task. Given the passage P , the question Q and the choice C specially for RACE, we organize the input X for BERT as the following two sequences.</p><p>[CLS] P</p><formula xml:id="formula_6">[SEP] Q [SEP] [CLS] P || Q [SEP] C [SEP]</formula><p>Span:</p><p>Choice:</p><p>where || denotes concatenation. The sequence is fed to BERT encoder mentioned above to obtain the contextualized representation H which is then passed to our proposed syntax-guided self attention layer and aggregation layer to obtain the final syntax-enhanced representation H. To keep simplicity, the downstream task-specific layer basically follows the implementation of BERT. We outline below to keep the integrity of our model architecture. For spanbased task, we feed H to a linear layer and obtain the probability distributions over the start and end positions through a softmax. For multi-choice task, we feed it into the classifier to predict the choice label for the multi-choice model. SQuAD 2.0 For SQuAD 2.0, our aim is a span of answer text, thus we employ a linear layer with SoftMax operation and feed H as the input to obtain the start and end probabilities, s and e:</p><p>s, e = SoftMax(Linear( H)).</p><p>(5)</p><p>The training objective of our SQuAD model is defined as cross entropy loss for the start and end predictions, L has = y s log s + y e log e.</p><p>(6)</p><p>For prediction, given output start and end probabilities s and e, we calculate the has-answer score score has and the no-answer score score na :</p><formula xml:id="formula_7">score has = max(s k + e l ), 0 ≤ k ≤ l ≤ n, score na = s 0 + e 0 . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>We obtain a difference score between has-answer score and the no-answer score as final score. A threshold δ is set to determine whether the question is answerable, which is heuristically computed in linear time with dynamic programming according to the development set. The model predicts the answer span that gives the has-answer score if the final score is above the threshold, and null string otherwise. <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, the pooled representation explicitly includes classification information during the pre-training stage of BERT. We expect the pooled to be overall representation of the input. Thus, the first token representation h0 in H is picked out and is passed to a feed-forward layer to give the prediction p. For each instance with n choice candidates, we update model parameters according to crossentropy loss during training and choose the one with highest probability as the prediction when testing. The training objectives of our RACE model is defined as,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RACE As discussed in</head><formula xml:id="formula_9">L(θ) = − 1 N i y i log p i (8)</formula><p>where p i denotes the prediction, y i is the target, and i denotes the data index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Setup</head><p>Our experiments and analysis are carried on two data sets, involving span-based and multi-choice MRC and we use the fine-tuned cased BERT (whole word masking) as the baseline.</p><p>Span-based MRC As a widely used MRC benchmark dataset, SQuAD 2.0 <ref type="bibr" target="#b24">(Rajpurkar et al., 2018)</ref> combines the 100,000 questions in SQuAD 1.1 <ref type="bibr" target="#b25">(Rajpurkar et al., 2016)</ref> with over 50,000 new, unanswerable questions that are written adversarially by crowdworkers to look similar to answerable ones. For the SQuAD 2.0 challenge, systems must not only answer questions when possible, but also abstain from answering when no answer is supported by the paragraph. Two official metrics are selected to evaluate the model performance: Exact Match (EM) and a softer metric F1 score, which measure the weighted average of the precision and recall rate at a character level.</p><p>Multi-choice MRC Our multi-choice MRC is evaluated on Large-scale ReAding Comprehension Dataset From Examinations (RACE) dataset <ref type="bibr" target="#b16">(Lai et al., 2017)</ref>, which consists of two subsets: RACE-M and RACE-H corresponding to middle school and high school difficulty levels. RACE contains 27,933 passages and 97,687 questions in total, which is recognized as one of the largest and most difficult datasets in multi-choice MRC. The official evaluation metric is accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>For the syntactic parser, we follow the deep biaffine attention based dependency parser from <ref type="bibr" target="#b8">(Dozat and Manning, 2016)</ref> using English dataset Penn Treebank (PTB) <ref type="bibr" target="#b19">(Marcus et al., 1993)</ref> to annotate our task datasets. We re-train the dependency parser by joint learning of constituent parsing <ref type="bibr" target="#b15">(Kitaev and Klein, 2018)</ref> using BERT as sole input which achieves very high accuracy: 97.00% UAS and 95.43% LAS on PTB test set<ref type="foot" target="#foot_3">4</ref> . Note this work is done in data preprocessing and our parser is not updated with the following MRC models.</p><p>For MRC model implementation, We follow the same fine-tuning procedure as BERT to avoid extra in-  fluence and focus on the intrinsic performance of our newly proposed method. We use Adam as our optimizer with an initial learning rate in {8e-6, 1e-5, 2e-5, 3e-5} with warm-up rate of 0.1 and L2 weight decay of 0.01. The batch size is selected in {16, 20, 32}. The maximum number of epochs is set to 3 or 10 depending on tasks. The weight α in the dual context aggregation is 0.5. All the texts are tokenized using wordpieces, and the maximum input length is set to 384 for both of SQuAD and RACE. The configuration for multi-head self attention is the same as that for BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>To focus on the evaluation of syntactic advance and keep simplicity, we only compare with single models instead of ensemble ones.</p><p>SQuAD 2.0 Table <ref type="table" target="#tab_0">1</ref> shows the result on SQuAD 2.0. Various state of the art models from the official leaderboard are also listed for reference. We can see that the performance of BERT is very strong. However, our model is more powerful, boosting the BERT baseline essentially. It also outperforms all the pub-lished works and achieves the 2nd place on the leaderboard when submitting SG-NET, with competitive performance compared with more sophisticated mechanisms and pipeline systems.</p><p>RACE For RACE, we compare our model with the following latest baselines: Dual Co-Matching Network (DCMN) <ref type="bibr" target="#b35">(Zhang et al., 2019)</ref>, Option Comparison Network (OCN) <ref type="bibr" target="#b26">(Ran et al., 2019)</ref> , Reading Strategies Model (RSM) <ref type="bibr" target="#b30">(Sun et al., 2018)</ref>, and Generative Pre-Training (GPT) <ref type="bibr" target="#b23">(Radford et al., 2018)</ref>. Table <ref type="table">2</ref> shows the result<ref type="foot" target="#foot_4">5</ref> . Turkers is the performance of Amazon Turkers on a random subset of the RACE test set. Ceiling is the percentage of unambiguous questions in the test set. From the comparison, we can observe that our model outperforms all baselines, which verifies the effectiveness of our proposed syntax enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Answering Long Questions</head><p>We sort the questions from SQuAD dev set according to the length and group them into subsets split by equal range of question length and equal amount of questions<ref type="foot" target="#foot_5">6</ref> . Then we calculate the exact match accuracy of the baseline and SG-Net per group, as shown in Fig- <ref type="figure" target="#fig_2">ure 4</ref>. We observe that the performance of the baseline drops heavily when encountered with long questions, especially for those longer than 20 words while our proposed SG-Net works robustly, even showing positive correlation between accuracy and length. This shows that with syntax-enhanced representation, our  model is better at dealing with lengthy questions compared with baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization</head><p>To have an insight that how syntax-guided attention works, we draw attention distributions of the vanilla attention of the last layer of BERT and our proposed syntax-guided self attention<ref type="foot" target="#foot_6">7</ref> , as shown in Figure <ref type="figure" target="#fig_3">5</ref>.</p><p>With the guidance of syntax, the keywords name, legislation and 1850 in the question are highlighted, and (the) Missouri, and Compromise in the passage are also paid great attention, which is exactly the right answer. The visualization verifies that benefiting from syntaxguided attention layer, our model is effective at selecting the vital parts, guiding the downstream layer to collect more relevant pieces to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dual Context Mechanism Evaluation</head><p>In SG-Net, we integrate the representations from syntax-guided attention layer and the vanilla self attention layer in dual context layer. To unveil the contribution of each potential component, we conduct comparisons on the baseline with:</p><p>1. Vanilla attention only that adds an extra vanilla BERT attention layer after the BERT output.</p><p>2. Syntax-guided attention only that adds an extra syntax-guided layer after the BERT output.</p><p>3. Dual contextual attention that is finally adopted in SG-Net as described in Section 3.2.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the results. We observe that dual contextual attention yields the best performance. Adding extra vanilla attention gives no advance, indicating that introducing more parameters would not promote the strong baseline. It is reasonable that syntax-guided attention only is also trivial since it only considers the syntax related parts when calculating the attention, which is complementary to traditional attention mechanism with noisy but more diverse information and finally motivates the design of dual contextual layer.</p><p>Actually, there are other operations for merging representations in dual context layer besides the dual aggregation, such as concatenation and Bi-attention <ref type="bibr" target="#b28">(Seo et al., 2016)</ref>, which are also involved in our comparison, and our experiments show that using dual contextual attention produces the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a novel syntax-guided framework for machine reading comprehension. We explore to adopt syntax to guide the text modeling by directly incorporating syntactic constraints into attention mechanism for better linguistically motivated word representations in machine reading comprehension task. Thus, we propose a novel dual contextual architecture called syntax-guided network (SG-Net), which consists of a vanilla BERT context vector and a syntax-guided context vector. Experiments on two major machine reading comprehension benchmarks involving span-based answer extraction (SQuAD 2.0) and multi-choice inference (RACE) show that our model can yield new stateof-the-art or comparative results in both extremely challenging tasks. This work empirically discloses the effectiveness of syntactic structural information for text modeling in machine reading comprehension tasks. The proposed attention mechanism also verifies the practicability of using linguistic information to guide attention learning and can be easily adapted with other tree-structured annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Example of syntax-guided span-based QA. The DOI of each word consists of all its ancestor words and itself marked with same color. (b-c) The dependency parsing tree of the given passage sentence and question.</figDesc><graphic url="image-1.png" coords="1,307.56,242.81,217.70,193.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Syntax-Guided Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy for different question length. Each data point means the accuracy for the questions in the same length range (a) or of the same number (b) and the horizontal axis in (b) shows that most of questions are of length 7-8 and 9-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of the vanilla BERT attention (left) and syntax-guided self attention (right). Weights of attention are selected from first head of the last attention layer. For the syntax-guided self attention, the columns with weights represent the DOI for each word in the row. For example, the DOI of passed contains {name, of, legislation, passed}. Weights are normalized by SoftMax for each row.</figDesc><graphic url="image-2.png" coords="7,94.68,62.81,408.18,167.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Exact Match (EM) and F1 scores (%) on SQuAD 2.0 dataset for single models. Our model is in boldface. † refers to unpublished work. Besides published works, we also list competing systems on the SQuAD leaderboard at the time of submitting SG-Net (May 14, 2019).</figDesc><table><row><cell>Model</cell><cell>Dev EM</cell><cell>F1</cell><cell>Test EM</cell><cell>F1</cell></row><row><cell cols="2">In literature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BiDAF-No-Answer</cell><cell cols="4">59.8 62.6 59.2 62.1</cell></row><row><cell>DocQA</cell><cell cols="4">61.9 64.8 59.3 62.3</cell></row><row><cell>DocQA + ELMo</cell><cell cols="4">65.1 67.6 63.4 66.3</cell></row><row><cell>Joint SAN</cell><cell cols="4">69.3 72.2 68.7 71.4</cell></row><row><cell>U-Net</cell><cell cols="4">70.3 74.0 69.2 72.6</cell></row><row><cell>RMR + ELMo + Verifier</cell><cell cols="4">72.3 74.8 71.7 74.2</cell></row><row><cell>SLQA+ †</cell><cell>-</cell><cell>-</cell><cell cols="2">71.5 74.4</cell></row><row><cell cols="2">Leaderboard</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell cols="2">86.8 89.5</cell></row><row><cell>BERT + DAE + AoA †</cell><cell>-</cell><cell>-</cell><cell cols="2">85.9 88.6</cell></row><row><cell>BERT + NGM + SST †</cell><cell>-</cell><cell>-</cell><cell cols="2">85.2 87.7</cell></row><row><cell>BERT + CLSTM + MTL + V †</cell><cell>-</cell><cell>-</cell><cell cols="2">84.9 88.2</cell></row><row><cell>SemBERT †</cell><cell>-</cell><cell>-</cell><cell cols="2">84.8 87.9</cell></row><row><cell>Insight-baseline-BERT †</cell><cell>-</cell><cell>-</cell><cell cols="2">84.8 87.6</cell></row><row><cell>BERT + MMFT + ADA †</cell><cell>-</cell><cell>-</cell><cell cols="2">83.0 85.9</cell></row><row><cell>BERT LARGE</cell><cell>-</cell><cell>-</cell><cell cols="2">82.1 84.8</cell></row><row><cell>Baseline (BERT WWM )</cell><cell cols="2">84.1 86.8</cell><cell>-</cell><cell>-</cell></row><row><cell>SG-Net</cell><cell cols="4">85.1 87.9 85.2 87.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on potential components and aggregation methods on SQuAD 2.0 dev set.</figDesc><table><row><cell>Model</cell><cell>EM</cell><cell>F1</cell></row><row><cell>baseline</cell><cell cols="2">84.1 86.8</cell></row><row><cell>+ Vanilla attention only</cell><cell cols="2">84.2 86.9</cell></row><row><cell cols="3">+ Syntax-guided attention only 84.4 87.2</cell></row><row><cell>+ Dual contextual attention</cell><cell cols="2">85.1 87.9</cell></row><row><cell>Concatenation</cell><cell cols="2">84.5 87.6</cell></row><row><cell>Bi-attention</cell><cell cols="2">84.9 87.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that our method is not limited to cooperate with BERT. We use it as the backbone because of the superior representation capacity and the strong baseline it provides.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We extend the idea of using parent in<ref type="bibr" target="#b29">(Strubell et al., 2018)</ref> to ancestor for a wider receptive range.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that for special tokens such as [CLS], [SEP] and [PAD], the DOI of these tokens is themselves alone in our implementation, which means these tokens will only attend to themselves in syntax-guided self attention layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We report the results without punctuation of the labeled and unlabeled attachment scores (LAS, UAS).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Our concatenation order of P and Q is slightly different from the original BERT. Therefore, the result of our BERT baseline is higher than the public one on the leaderboard, thus our improved BERT implementation is used as the stronger baseline for our evaluation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Since the question length is at variance, we depict the two aspects to show the discovery comprehensively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Since special symbols such as[PAD]  and[CLS]  are not considered in the dependency parsing tree, we confine the DOI of these tokens to themselves. So these special tokens will have value of 1 as weights over themselves in syntaxguided self attention and we will mask these weights in the following aggregation layer.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and Key Projects of National Natural Science Foundation of China (U1836222 and 61733011).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jon</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06021</idno>
		<title level="m">A fast unified model for parsing and sentence understanding</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing. ICLR. Luheng He</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep semantic role labeling: What works and whats next. ACL</title>
				<meeting><address><addrLine>Kenton Lee, Mike Lewis,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Syntaxaware multilingual semantic role labeling</title>
		<author>
			<persName><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
				<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<title level="m">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">Jan Kleindienst. 2016</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntax-aware neural semantic role labeling with supertags</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constituency Parsing with a Self-Attentive Encoder</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stack-Pointer Networks for Dependency Parsing</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Pramod</forename><surname>Kaushik Mudrakarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kedar</forename><surname>Dhamdhere</surname></persName>
		</author>
		<title level="m">Did the model understand the question? ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>ArXiv:1611.09268v2</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Qiu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03033</idno>
		<title level="m">Option comparison network for multiple-choice reading comprehension</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural semantic role labeling with dependency path embeddings</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bidirectional attention for machine comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linguistically-informed self-attention for semantic role labeling</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving machine reading comprehension with general reading strategies</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13441</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning sentence representation with guidance of human attention</title>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to skim text</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dual comatching network for multi-choice reading comprehension</title>
		<author>
			<persName><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09381</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Subword-augmented embedding for cloze reading comprehension</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
