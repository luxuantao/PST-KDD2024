<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Deep Learning for Face Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">Hybrid Deep Learning for Face Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICCV.2013.188</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution of this work is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep ConvNets in our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of ConvNets are constructed in order to achieve robustness and characterize face similarities from different aspects. The top-layer RBM performs inference from complementary high-level features extracted from different ConvNet groups</head><p>with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has been extensively studied in recent decades <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b34">34]</ref>. This paper addresses the key challenge of computing the similarity of two face images given their large intrapersonal variations in poses, illuminations, expressions, ages, makeups, and occlusions. It becomes more difficult when faces to be compared are acquired in the wild. We focus on the task of face verification, which aims to determine whether two face images belong to the same identity.</p><p>Existing methods generally address the problem in two steps: feature extraction and recognition. In the feature extraction stage, a variety of hand-crafted features are used <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b6">6]</ref>. Although some learning-based feature extraction approaches are proposed, their optimization targets are not directly related to face identity <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b13">13]</ref>. Therefore, the features extracted encode intra-personal variations. More importantly, existing approaches extract features from each image separately and compare them at later stages <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4]</ref>. Some important correlations between the two compared images have been lost at the feature extraction stage.</p><p>At the recognition stage, classifiers such as SVM are used to classify two face images as having the same identity or not <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b13">13]</ref>, or other models are employed to compute the similarities of two face images <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b25">25]</ref>. The purpose of these models is to separate inter-personal variations and intra-personal variations. However, all of these models have been shown to have shallow structures <ref type="bibr" target="#b2">[2]</ref>. To handle large-scale data with complex distributions, large amount of over-completed features may need to be extracted from the face <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b25">25]</ref>. Moreover, since the feature extraction stage and the recognition stage are separate, they cannot be jointly optimized. Once useful information is lost in feature extraction, it cannot be recovered in recognition. On the other hand, without the guidance of recognition, the best way to design feature descriptors to capture identity information is not clear.</p><p>All of the issues discussed above motivate us to learn a hybrid deep network to compute face similarities. A highlevel illustration of our model is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Our model has several unique features, as outlined below.</p><p>(1) It directly learns visual features from raw pixels under the supervision of face identities. Instead of extracting features from each face image separately, the model jointly extracts relational visual features from two face images in comparison. In our model, such relational features are first locally extracted with the automatically learned filter pairs (pairs of filters convolving with the two face images respectively as shown in Figure <ref type="figure" target="#fig_0">1</ref>), and then further processed through multiple layers of the deep convolutional networks (ConvNets) to extract high-level and global features. The extracted features are effective for computing the identity similarities of face images.</p><p>(2) Considering the regular structures of faces, the deep ConvNets in our model locally share weights in higher convolutional layers, such that different mid-or high-level features are extracted from different face regions, which is contrary to conventional ConvNet structures <ref type="bibr" target="#b18">[18]</ref>, and can greatly improve their fitting and generalization capabilities.</p><p>(3) The deep and wide architecture of our hybrid network can handle large-scale face data with complex distributions. The deep ConvNets in our network have four convolutional layers (followed by max-pooling) and two fully-connected layers. In addition, multiple groups of ConvNets are constructed to achieve good robustness and characterize face similarities from different aspects. Predictions from multiple ConvNet groups are pooled hierarchically and then associated by the top-layer RBM for the final inference.</p><p>(4) The feature extraction and recognition stages are unified under a single network architecture. The parameters of the entire pipeline (weights and biases in all the layers) are jointly optimized for the target of face verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>All existing methods for face verification start by extracting features from two faces in comparison separately. A variety of low-level features are commonly used <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b6">6]</ref>, including the hand-crafted features like LBP <ref type="bibr" target="#b23">[23]</ref> and its variants <ref type="bibr" target="#b32">[32]</ref>, SIFT <ref type="bibr" target="#b21">[21]</ref>, Gabor <ref type="bibr" target="#b31">[31]</ref> and the learned LE features <ref type="bibr" target="#b5">[5]</ref>. Some methods generated midlevel features <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b13">13]</ref> with variants of convolutional deep belief networks (CDBN) <ref type="bibr" target="#b19">[19]</ref> or ConvNets <ref type="bibr" target="#b18">[18]</ref>. They are not learned with the supervision of identity matching. Thus variations other than identity are encoded in the features, such as poses, illumination, and expressions, which constitute the main impediment to face recognition.</p><p>Many face recognition models are shallow structures, and need high-dimensional over-completed feature representations to learn the complex mappings from pairs of noisy features to face similarities <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b25">25]</ref>; otherwise, the models may suffer from inferior performance. Many methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b13">13]</ref> used linear SVM to make the same-ordifferent verification decisions. Li et al. <ref type="bibr" target="#b20">[20]</ref> and Chen et al. <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref> factorized the face images as identity variations plus variations within the same identity, and assumed each factor as a Gaussian distribution for closed form solutions. Huang et al. <ref type="bibr" target="#b12">[12]</ref> and Simonyan et al. <ref type="bibr" target="#b25">[25]</ref> learns linear transformations via metric learning.</p><p>Some methods further learn high-level features based on low-level hand-crafted features <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4]</ref>. They are outputs of classifiers that are trained to distinguish faces of different people. All these methods extract features from a single face separately, and the comparison of two face images are deferred in the later recognition stage. Some identity information may have been lost in the feature extraction stage, and it cannot be retrieved in the recognition stage, since the two stages are separated in the existing methods. To avoid the potential information loss and make a reliable decision, a large amount of high-level feature extractors may need to be trained <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>.</p><p>There are a few methods that also used deep models for face verification <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b13">13]</ref>, but extracted features independently from each face. Thus relations between the two faces are not modeled at their feature extraction stages. In <ref type="bibr" target="#b34">[34]</ref>, face images under various poses and lighting conditions were transformed to a canonical view with a convolutional neural network. Then features are extracted from the transformed images. In contrast, we deal with face pairs directly by extracting relational visual features from the two compared faces. The top layer RBM in our model is similar to that of the deep belief net (DBN) proposed by Hinton and Osindero <ref type="bibr" target="#b11">[11]</ref>. However, we use ConvNets instead of stack of RBMs in the lower layers to take the local correlation in images into consideration. Averaging the results of multiple ConvNets has been shown to be an effective way of improving performance <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">15]</ref>, while we will show that our hybrid structure is significantly better than the simple averaging scheme. Moreover, unlike most existing face recognition pipelines, in which each stage is optimized independently, our hybrid ConvNet-RBM model is jointly optimized after pre-training each part separately, which further enhances its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The hybrid ConvNet-RBM model 3.1. Architecture overview</head><p>We detect the two eye centers and mouth center with the facial point detection method proposed by Sun et al. <ref type="bibr" target="#b26">[26]</ref>. Faces are aligned by similarity transformation according to  The lower part of our hybrid model contains 12 groups, each of which contains five ConvNets. Figure <ref type="figure" target="#fig_2">3</ref> shows the structure of one ConvNet. Each ConvNet takes a pair of aligned face regions as input. Its four convolutional layers (followed by max-pooling) extract the relational features hierarchically. Finally, the extracted features pass a fully connected layer and are fully connected to a single neuron in layer L0 (shown in Figure <ref type="figure" target="#fig_1">2</ref>), which indicates whether the two regions belong to the same person. The input region pairs for ConvNets in different groups differ in terms of region ranges and color channels (shown in Figure <ref type="figure" target="#fig_3">4</ref>) to make their predictions complementary. When the size of the input regions changes in different groups, the map sizes in the following layers of the ConvNets will change accordingly. Although ConvNets in the same group take the same kind of region pair as input, they are different in that they are trained with different bootstraps of the training data (Section 4.1). Each input region pair generates eight modes by exchanging the two regions and horizontally flipping each region (shown in Figure <ref type="figure" target="#fig_4">5</ref>). When the eight modes (shown as M1-M8 in Figure <ref type="figure" target="#fig_1">2</ref>) are input to the same  The group prediction is given by two levels of average pooling of ConvNet predictions. Layer L1 (with 5 × 12 neurons) is formed by averaging the eight predictions of the same ConvNet from eight different input modes. Layer L2 (with 12 neurons) is formed by averaging the five neurons in L1 associated with the same group. The prediction variance is greatly reduced after average pooling.</p><p>The top layer of our model in Figure <ref type="figure" target="#fig_1">2</ref> is a Classification RBM <ref type="bibr" target="#b17">[17]</ref>. It merges the 12 group outputs in L2 to give the final prediction. The RBM has two outputs that indicate the probability distribution over the two classes; that is, whether they are the same person. The large number of deep ConvNets means that our model has a high capacity. Directly optimizing the whole network would lead to severe over-fitting. Therefore, we first train each ConvNet separately. Then, by fixing all the ConvNets, the RBM is trained. All the ConvNets and the RBM are trained under supervision with the aim of predicting whether two faces in comparison belong to the same person. These two steps initialize the model to be near a good local minimum. Finally, the whole network is fine-tuned by backpropagating errors from the top-layer RBM to all the lowerlayer ConvNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep ConvNets</head><p>A pair of gray regions forms two input maps of a ConvNet (Figure <ref type="figure" target="#fig_4">5</ref>), while a pair of color regions forms six input maps, replacing each gray map with three maps from RGB channels. The input regions are stacked into multiple maps instead of being concatenated to form one map, which enables the ConvNet to model the relations between the two regions from the first convolutional stage.</p><p>Our deep ConvNets contain four convolutional layers (followed by max-pooling). The operation in each convolutional layer can be expressed as</p><formula xml:id="formula_0">y r j = max 0, b r j + i k r ij * x r i , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where * denotes convolution, x i and y j are the i-th input map and the j-th output map respectively, k ij is the convolution kernel (filter) connecting the i-th input map and the j-th output map, and b j is the bias for the j-th output map. max (0, •) is the non-linear activation function, and is operated element-wise. Neurons with such nonlinearities are called rectified linear units <ref type="bibr" target="#b15">[15]</ref>. Moreover, weights of neurons (including convolution kernels and biases) in the same map in higher convolutional layers are locally shared. r indicates a local region where weights are shared. Since faces are structured objects, locally sharing weights in higher layers allows the network to learn different high-level features at different locations. We find that sharing in this way can significantly improve the fitting and generalization abilities of the network. The idea of locally sharing weights was proposed by Huang et al. <ref type="bibr" target="#b13">[13]</ref>. However, their model is much shallower than ours and the gained improvement is small. Since each stage extracts features from all the maps in the previous stage, relations between the two face regions are modeled; see Figure <ref type="figure" target="#fig_5">6</ref> for examples. As the network goes deeper, more global and higher-level relations between the two regions are modeled. These high-level relational features make it possible for the top layer neurons in ConvNets to predict the high-level concept of whether the two input regions come from the same person. The network output is a two-way softmax,</p><formula xml:id="formula_2">y i = exp(x i ) 2 j=1 exp(x j ) for i = 1, 2</formula><p>, where x i is the total input to an output neuron i, and y i is its output. It represents a probability distribution over the two classes (being the same person or not). Such a probability distribution makes it valid to directly average multiple ConvNet outputs without scaling. The ConvNets are trained by minimizing − log y t , where t ∈ {1, 2} denotes the target class. The loss is minimized by stochastic gradient descent, where the gradient is calculated by backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification RBM</head><p>Classification RBM models the joint distribution between its output neurons y (one out of C classes), input neurons x (binary), and hidden neurons h (binary), as The upper and lower filters in each pair convolve with the two face regions in comparison, respectively, and the results are added. For filter pairs in which one filter varies greatly while the other remains near uniform (column 1, 2), features are extracted from the two input regions separately. For those pairs in which both filters vary greatly, some kind of relations between the two input regions are extracted. Among the latter, some pairs extract simple relations such as addition (column 5) or subtraction (column 6), while others extract more complex relations (column 6, 7). Interestingly, we find that filters in some filter pairs are nearly the same as those in some others, except that the order of the two filters are inversed (columns 1-4). This makes sense since face similarities should be invariant with the order of the two face regions in comparison. (y,x,h) , where E(y, x, h) = −h W x − h Uy − b x − c h − d y. Given input x, the conditional probability of its output y can be explicitly expressed as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(y, x, h) ∝ e −E</head><formula xml:id="formula_3">p(y c | x) = e d c j 1 + e c j +U jc + k W jk x k i e d i j 1 + e c j +U ji + k W jk x k , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where c indicates the c-th class. We discriminatively train the Classification RBM by minimizing the negative log probability of the target class t given input x; that is, minimizing − log p(y t | x). The target can be optimized by computing the exact gradient − ∂ log p(y t |x)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂θ</head><p>, where θ ∈ {W, U, b, c, d} are RBM parameters to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fine-tuning the entire network</head><p>Let N and M be the number of groups and the number of ConvNets in each group, respectively, and C n m (•) be the input-output mapping for the m-th ConvNet in the n-th group. Since the two outputs of the ConvNet represent a probability distribution (summed to 1), when one output is known, the other output contains no additional information. So the hybrid model (and the mapping) only keeps the first output from the ConvNet. Let {I n k } K k=1 be the K possible input modes formed by a pair of face regions of group n.</p><p>Then the n-th ConvNet group prediction can be expressed as</p><formula xml:id="formula_5">x n = 1 M M m=1 1 K K k=1 C n m (I n k ) ,<label>(3)</label></formula><p>where the inner and outer sums are over different input modes (level 1 pooling) and different ConvNets (level 2 pooling), respectively. Given the N group predictions {x n } N n=1 , the final prediction by RBM is max c∈{1,2} {p(y c | x)}, where p(y c | x) is defined in Eq. ( <ref type="formula" target="#formula_3">2</ref>). After separately training each ConvNet and the RBM to derive a good initialization, error is back-propagated from the RBM to all groups of ConvNets and the whole model is fine-tuned. Let L(x) = − log p(y t | x) be the RBM loss function, and α n m be the parameters for the m-th ConvNet in the n-th group. The gradient of the loss w.r.t.</p><formula xml:id="formula_6">α n m is ∂L ∂α n m = ∂L ∂x n ∂x n ∂α n m = 1 MK ∂L ∂x n K k=1 ∂C n m (I n k ) ∂α n m . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>∂L ∂x n can be calculated by the closed form expression of p(y t | x) (Eq. ( <ref type="formula" target="#formula_3">2</ref>)), and</p><formula xml:id="formula_8">∂C n m (I n k ) ∂α n m</formula><p>can be calculated using the back-propagation algorithm in the ConvNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our algorithm on LFW <ref type="bibr" target="#b14">[14]</ref>, which has been used extensively to evaluate algorithms of face verification in the wild. We conduct evaluation under two different settings: (1) 10-fold cross validation under the unrestricted protocol of LFW without using extra data to train the model, and (2) cross-dataset validation in which external data exclusive to LFW is used for training. The former shows the performance with a limited amount of training data, while the latter shows the generalization ability across different datasets. Section 4.1 explains the experimental settings in detail, section 4.2 validates various aspects of model design, and section 4.3 compares our results with state-of-art results in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment settings</head><p>LFW is divided into 10 folds of mutually exclusive people sets. For the unrestricted setting, performance is evaluated using the 10-fold cross-validation. Each time one fold is used for testing and the other nine for training. Results averaged over the 10 folds are reported. The 600 testing pairs in each fold are predefined by LFW and fixed, whereas training pairs can be generated using the identity information in the other nine folds and the number is not limited. This is referred as the LFW training settings.</p><p>For the cross-dataset setting, we use outside data exclusive to LFW for training. PubFig <ref type="bibr" target="#b16">[16]</ref> and WDRef <ref type="bibr" target="#b6">[6]</ref> are two large datasets other than LFW with faces in the wild. However, PubFig only contains 200 people, thus the identity variation is quite limited, while the images in WDRef are not publicly available. Accordingly, we created a new dataset, called the Celebrity Faces dataset (CelebFaces). It contains 87, 628 face images of 5, 436 celebrities from the web, and was assembled by first collecting the celebrity names that do not exist in LFW to avoid any overlap, then searching for the face images for each name on the web. To conduct cross-dataset testing, the model is trained on CelebFaces and tested on the predefined 6, 000 test pairs in LFW. We will refer to this setting as the CelebFaces training settings.</p><p>For both settings, we randomly choose 80% A separate validation dataset is needed during training to avoid overfitting. After each training epoch <ref type="foot" target="#foot_0">1</ref> , we observe the errors on the validation dataset and select the model that provides the lowest validation error. We randomly select 100 people from the training people to generate the validation data. The free parameters in training (the learning rate and its decreasing rate) are selected using view 1 of LFW<ref type="foot" target="#foot_1">2</ref> and are fixed in all the experiments. We report both the average accuracy and the ROC curve. The average accuracy is defined as the percentage of correctly classified face pairs. We assign each face pair to the class with higher probabilities without further learning a threshold for the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Investigation on model design</head><p>Local weight sharing. Our ConvNets locally share weights in the last two convolutional layers. In the second last convolutional layer, maps are evenly divided into 2 × 2 regions, and weights are shared among neurons in each region. In the last convolutional layer, weights are independent for each neuron. We compare our ConvNets   (refer to as S1) with the conventional ConvNets (refer to as S2), where weights in all the convolutional layers are globally shared, on both training errors and test accuracies. Figure <ref type="figure" target="#fig_7">7</ref> and Table <ref type="table" target="#tab_0">1</ref> show the better fitting and generalization abilities of our ConvNets (S1), where locally sharing weights improved the group P1 (we will refer to each group as the type of regions used (Figure <ref type="figure" target="#fig_3">4</ref>)) prediction accuracies by approximately 2% for both the LFW and CelebFaces training settings. The same conclusion holds for ConvNets in other groups. Two-level average pooling in ConvNet groups. The ConvNet group predictions are derived from two levels of average pooling as described in Section 3.1. Figure <ref type="figure" target="#fig_9">8</ref> shows that the performance is consistently improved after each level of average pooling (from L0 to L2) under the LFW training settings. The accuracy increases over 3% on average after the two levels of pooling (L2 compared to L0). The same conclusion holds for the CelebFaces training settings.</p><p>Complementarity of group predictions. We validate that the pooled group predictions are complementary. Given the 12 group predictions (referred as features), we employ a greedy feature selection algorithm. Each time, a feature is added to the feature set, in such a way that the RBM trained on these features provides the highest accuracy on the validation set. The increase of the RBM prediction accuracies would indicate that complementary information  is contained in the added features. In this experiment, the ConvNets are pre-trained and their weights are fixed without jointly fine-tuning the whole network. The experiment is repeated five times, with the training samples for the RBM randomly generated each time. The averaged test results are reported. Figure <ref type="figure" target="#fig_10">9</ref> shows that performance is consistently improved when more features are added. So all the group predictions contain additional information.</p><p>Top-layer RBM and fine-tuning. Since different groups observe different kinds of regions, each group may be good at judging particular kinds of face pairs differently. Continuing to average group predictions may smooth out the patterns in different group predictions. Instead, we let the top-layer RBM in our model learn such patterns. Then the whole model is fine-tuned to jointly optimize all the parts. Moreover, we find that the performance can be further enhanced by averaging five different hybrid ConvNet-RBM models. This is achieved by first training five RBMs (each with a different set of randomly generated training data) with the weights of ConvNets pre-trained and fixed, and then fine-tuning each of the whole ConvNet-RBM network separately. The results are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Method comparison</head><p>We compare our best results on LFW with the state-ofthe-art methods in accuracies (Table <ref type="table" target="#tab_3">3 and 4</ref>) and ROC curves (Figure <ref type="figure" target="#fig_12">10</ref> and 11) respectively. Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure" target="#fig_12">10</ref> are comparisons of methods that follow the LFW unrestricted protocol without using outside data to train the model. Table <ref type="table">4</ref> and Figure <ref type="figure" target="#fig_13">11</ref> report the results when the training data outside LFW is allowed to use. Methods marked with * are published after the submission of this paper. Our ConvNet-RBM model achieves the third best performance in both settings. Although Tom-vs-Pete <ref type="bibr" target="#b3">[3]</ref>, high-dim LBP <ref type="bibr" target="#b7">[7]</ref>, and Fisher vector faces <ref type="bibr" target="#b25">[25]</ref> have better accuracy than our method, there are two important factors to be considered. First, all the three methods used stronger alignment than ours: 95 points in <ref type="bibr" target="#b3">[3]</ref>, 27 points in <ref type="bibr" target="#b7">[7]</ref>, and 9 points in <ref type="bibr" target="#b25">[25]</ref>, while we only use three points for alignment. Berg and Belhumeur <ref type="bibr" target="#b3">[3]</ref> reported 90.47% accuracy with three point (the eyes and mouth) alignment. Chen et al. <ref type="bibr" target="#b7">[7]</ref> reported 6% ∼ 7% accuracy drop if use five point alignment and single scale patches. Second, all the three methods used hand-crafted features (SIFT or LBP) as their base features, while we learn features from raw pixels. The base features used in <ref type="bibr" target="#b7">[7]</ref> and <ref type="bibr" target="#b25">[25]</ref> are densely sampled on landmarks or grids with many different scales and the dimension is particularly high (100K LBP features in <ref type="bibr" target="#b7">[7]</ref> and 1.7M SIFT features in <ref type="bibr" target="#b25">[25]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has proposed a new hybrid ConvNet-RBM model for face verification. The model learns directly and jointly extracts relational visual features from face pairs under the supervision of face identities. Both feature extrac-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) PLDA <ref type="bibr" target="#b20">[20]</ref> 90.07 Joint Bayesian <ref type="bibr" target="#b6">[6]</ref> 90.90 Fisher vector faces <ref type="bibr" target="#b25">[25]</ref> Method Accuracy (%) Associate-predict <ref type="bibr" target="#b33">[33]</ref> 90.57 Joint Bayesian <ref type="bibr" target="#b6">[6]</ref> 92.4 Tom-vs-Pete classifiers <ref type="bibr" target="#b3">[3]</ref> 93.30 High-dim LBP <ref type="bibr" target="#b7">[7]</ref>* 95.17   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The hybrid ConvNet-RBM model. Solid and hollow arrows show forward and back propagation directions.</figDesc><graphic url="image-2.png" coords="1,484.53,244.47,68.50,192.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the hybrid ConvNet-RBM model. Neuron (or feature) number is marked beside each layer.</figDesc><graphic url="image-24.png" coords="3,57.84,222.61,102.76,77.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of one ConvNet. The map numbers and dimensions of the input layer and all the convolutional and max-pooling layers are illustrated as the length, width, and height of cuboids. The 3D convolution kernel sizes of the convolutional layers and the pooling region sizes of the max-pooling layers are shown as the small cuboids and squares inside the large cuboids of maps respectively. Neuron numbers of other layers are marked beside each layer. the three points. Figure 2 is an overview of our hybrid ConvNet-RBM model, which is a cascade of deep ConvNet groups, two levels of average pooling, and Classification RBM.The lower part of our hybrid model contains 12 groups, each of which contains five ConvNets. Figure3shows the structure of one ConvNet. Each ConvNet takes a pair of aligned face regions as input. Its four convolutional layers (followed by max-pooling) extract the relational features hierarchically. Finally, the extracted features pass a fully connected layer and are fully connected to a single neuron in layer L0 (shown in Figure2), which indicates whether the two regions belong to the same person. The input region pairs for ConvNets in different groups differ in terms of region ranges and color channels (shown in Figure4) to make their predictions complementary. When the size of the input regions changes in different groups, the map sizes in the following layers of the ConvNets will change accordingly. Although ConvNets in the same group take the same kind of region pair as input, they are different in that they are trained with different bootstraps of the training data (Section 4.1). Each input region pair generates eight modes by exchanging the two regions and horizontally flipping each region (shown in Figure5). When the eight modes (shown as M1-M8 in Figure2) are input to the same</figDesc><graphic url="image-31.png" coords="3,0.00,90.00,612.00,612.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Twelve face regions used in our network. P1 -P4 are global regions covering the whole face, of size 39 × 31. P1 and P2 (P3 and P4) differ slightly in the ranges of regions. P5 -P12 are local regions covering different face parts, of size 31 × 47. P1, P2, and P5 -P8 are in color. P3, P4, and P9 -P12 are in gray values.</figDesc><graphic url="image-27.png" coords="3,314.00,69.68,239.15,54.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: 8 possible modes for a pair of face regions.</figDesc><graphic url="image-28.png" coords="3,343.69,210.97,179.64,102.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of the learned 4 × 4 filter pairs of the first convolutional layer of ConvNets taking color (line 1) and gray (line 2) input region pairs, respectively.The upper and lower filters in each pair convolve with the two face regions in comparison, respectively, and the results are added. For filter pairs in which one filter varies greatly while the other remains near uniform (column 1, 2), features are extracted from the two input regions separately. For those pairs in which both filters vary greatly, some kind of relations between the two input regions are extracted. Among the latter, some pairs extract simple relations such as addition (column 5) or subtraction (column 6), while others extract more complex relations (column 6, 7). Interestingly, we find that filters in some filter pairs are nearly the same as those in some others, except that the order of the two filters are inversed (columns 1-4). This makes sense since face similarities should be invariant with the order of the two face regions in comparison.</figDesc><graphic url="image-32.png" coords="4,314.71,72.68,239.15,115.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>people from the training data to train the deep ConvNets, and use the remaining 20% people to train the top-layer RBM and fine-tune the entire model. The positive training pairs are randomly formed such that on average each face image appears in k = 6 (3) positive pairs for LFW (CelebFaces) dataset, unless a person does not have enough training images. Given a fixed number of training images, generating more training pairs provides minimal assistance. Negative training pairs are also randomly generated and their number is the same as the number of positive training pairs. In this way, we generate approximately 40, 000 (240, 000) training pairs for the ConvNets and 8, 000 (50, 000) training pairs for the RBM and fine-tuning for LFW (CelebFaces) training dataset. This random process for generating training data is repeated for each ConvNet so that multiple different ConvNets are trained in each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average training set failure rates with respect to the number of training epochs for ConvNets in group P1 with the local (S1) or global (S2) weight-sharing schemes for the LFW and CelebFaces training settings.</figDesc><graphic url="image-39.png" coords="6,80.78,68.56,191.76,124.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>P1 with the local (S1) or global (S2) weight sharing schemes for the LFW and CelebFaces training settings. L0 -L2 refer to the three layers shown in Figure 2. L2 is the final group predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ConvNet prediction accuracies for each group averaged over the 10-fold LFW training settings. L0-L2 refer to the three layers shown in Figure 2.</figDesc><graphic url="image-40.png" coords="6,318.21,68.56,239.52,119.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average RBM prediction accuracies with respect to the number of features selected for the LFW and CelebFaces training settings. The accuracy is consistently improved with the increase of feature numbers.</figDesc><graphic url="image-41.png" coords="6,343.43,236.34,179.76,137.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Table 4 :</head><label>4</label><figDesc>Accuracy comparison of our hybrid ConvNet-RBM model and the state-of-the-art methods that rely on outside training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: ROC comparison of our hybrid ConvNet-RBM model and the state-of-the-art methods under the LFW unrestricted protocol.</figDesc><graphic url="image-45.png" coords="7,325.79,317.99,214.91,161.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: ROC comparison of our hybrid ConvNet-RBM model and the state-of-the-art methods relying on outside training data.</figDesc><graphic url="image-49.png" coords="8,69.42,69.16,214.79,161.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average testing accuracies for ConvNets in group</figDesc><table><row><cell></cell><cell cols="3">L0 (%) L1 (%) L2 (%)</cell></row><row><cell>S1 for LFW</cell><cell>84.78</cell><cell>86.54</cell><cell>88.78</cell></row><row><cell>S2 for LFW</cell><cell>83.54</cell><cell>85.28</cell><cell>86.78</cell></row><row><cell>S1 for CelebFaces</cell><cell>87.71</cell><cell>88.71</cell><cell>89.60</cell></row><row><cell>S2 for CelebFaces</cell><cell>85.65</cell><cell>86.61</cell><cell>87.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Interestingly, though directly averaging the 12 group predictions (group averaging) is suboptimal, it</figDesc><table><row><cell></cell><cell cols="2">LFW (%) CelebFaces (%)</cell></row><row><cell>Best single group</cell><cell>88.78</cell><cell>89.70</cell></row><row><cell>Group averaging</cell><cell>89.97</cell><cell>90.18</cell></row><row><cell>RBM fix</cell><cell>90.93</cell><cell>91.26</cell></row><row><cell>Fine-tuning</cell><cell>91.38</cell><cell>92.23</cell></row><row><cell>Model averaging</cell><cell>91.75</cell><cell>92.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracies of the best prediction results with a single group (best single group), directly averaging the group predictions (group averaging), training a top layer RBM while fixing the weights of ConvNets (RBM fix), fine-tuning the whole hybrid ConvNet-RBM model (finetuning), and averaging the predictions of the five hybrid ConvNet-RBM models (model averaging), for LFW and CelebFaces training settings respectively.</figDesc><table /><note>still improves the best prediction results of a single group (best single group). We achieved our best results with the averaging of five hybrid ConvNet-RBM model predictions (model averaging).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy comparison of our hybrid ConvNet-RBM model and the state-of-the-art methods under the LFW unrestricted protocol.</figDesc><table><row><cell>*</cell><cell>93.03</cell></row><row><cell>High-dim LBP [7]*</cell><cell>93.18</cell></row><row><cell>ConvNet-RBM</cell><cell>91.75</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">One training epoch is a single pass of all the training samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">View 1 is provided by LFW for algorithm development and parameter selecting without over-fitting the test data.<ref type="bibr" target="#b14">[14]</ref>.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work is supported by the General Research Fund sponsored by the Research Grants Council of the Kong Kong SAR (Project No. CUHK 416312 and CUHK 416510) and Guangdong Innovative Research Team Program (No.201001D0104648280).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">PAMI</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Learning deep architectures for AI. Foundations and Trends in Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tom-vs-pete classifiers and identity-preserving alignment for face verification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition with learning-based descriptor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007">2012. 1, 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is that you? metric learning approaches for face identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Large scale strongly supervised ensemble metric learning, with applications to face verification and retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno>TR115</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">NEC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning algorithms for the classification restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="669" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic models for inference about identity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
				<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond simple features: A large-scale feature search approach to unconstrained face recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple one-shots for utilizing class label information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual-space linear discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A unified framework for subspace face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1222" to="1228" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random sampling for subspace face recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="104" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V D</forename><surname>Malsburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>PAMI</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="775" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Descriptor based methods in the wild</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Faces Real-Life Images at ECCV</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An associate-predict model for face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2011. 1</title>
				<meeting>CVPR, 2011. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
