<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decision tree learning with fuzzy labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
							<email>z.qin@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Mathematics</orgName>
								<orgName type="laboratory">Artificial Intelligence Group</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<postCode>BS8 1TR</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Lawry</surname></persName>
							<email>j.lawry@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Mathematics</orgName>
								<orgName type="laboratory">Artificial Intelligence Group</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<postCode>BS8 1TR</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decision tree learning with fuzzy labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC9AFABEEA4C7C298852415C67AAB302</idno>
					<idno type="DOI">10.1016/j.ins.2004.12.005</idno>
					<note type="submission">Accepted 12 December 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Label semantics</term>
					<term>LID3</term>
					<term>Linguistic decision tree</term>
					<term>Mass assignment</term>
					<term>Random set</term>
					<term>Linguistic constraint</term>
					<term>Transparency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label semantics is a random set based framework for ''Computing with Words'' that captures the idea of computation on linguistic terms rather than numerical quantities. Within this new framework, a decision tree learning model is proposed where nodes are linguistic descriptions of variables and leaves are sets of appropriate labels. In such decision trees, the probability estimates for branches across the whole tree is used for classification, instead of the majority class of the single branch into which the examples fall. By empirical experiments on real-world datasets it is verified that our algorithm has better or equivalent classification accuracy compared to three well known machine learning algorithms. By applying a new forward branch merging algorithm, the complexity of the tree can be greatly reduced without significant loss of accuracy. Finally, a linguistic interpretation of trees and classification with linguistic constraints are introduced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0020-0255/$ -see front matter Ó 2004 Elsevier Inc. All rights reserved. doi:10.1016/j.ins.2004.12.005</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning and data mining research has developed rapidly in recent decades. As one of the most successful branches of Artificial Intelligence, it is playing a more and more important role in real-world applications ranging from gene expression to flood prediction. Traditionally machine learning and data mining research has focused on learning algorithms with high classification or prediction accuracy. From another perspective, however, this is not always sufficient for some real world applications that require good algorithm transparency. By the latter we mean that models need to be easily understood and provide information regarding underlying trends and relationships that can be used by practitioners in the relevant fields. Also, uncertainty and imprecision are often inherent in modelling these real-world applications and it is desirable that these should be incorporated into learning algorithms.</p><p>Here we present a high-level knowledge representation framework centered on the Computing with Words (CW) paradigm proposed by Zadeh <ref type="bibr" target="#b19">[20]</ref>, although the underlying semantics of our approach will be quite different. Label semantics <ref type="bibr" target="#b10">[11]</ref> is a random set based semantics for modelling imprecise concepts where the degree of appropriateness of a linguistic expression as a description of a value is measured in terms of how the set of appropriate labels for that value varies across a population. It provides us with a framework for modelling uncertainty with good transparency. Based on this semantics, a new tree-structured model, Linguistic Decision Tree (LDT) is proposed. Linguistic expressions such as small, medium and large are used to learn from data and build a linguistic decision tree guided by information based heuristics. For each branch, instead of labeling it with a certain class (such as positive or negative in binary classification) the probability of members of this branch belonging to a particular class is evaluated from a given training dataset. Unlabeled data is then classified by using probability estimation of classes across the whole decision tree.</p><p>This paper is organized as follows. Section 2 presents the fundamentals of label semantics as well as the methods for data analysis and data mining based on this framework. Section 3 gives a detailed description of the proposed linguistic decision tree model, and an algorithm of building such a decision tree is outlined. We also investigate the effect of using different discretization methods with empirical results given for comparisons. Experimental results for realworld datasets are given, comparing the new algorithm with three well-known machine learning algorithms: C4.5, Naive Bayes and Back Propagation Neural networks. In Section 4, a forward branch merging algorithm is introduced in order to build a compact decision tree without significant loss of accuracy. Experimental results show that the trees with forward merging have better transparency and equivalent or better accuracy than the original LDT. In Section 5, a formal linguistic interpretation of LDT and a method for classification given linguistic constraints is proposed and the results from applying it to a toy problem are given. Prior to the introduction of the formal framework we introduce some important notation used throughout this paper as shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Label semantics</head><p>Label semantics, proposed by Lawry <ref type="bibr" target="#b10">[11]</ref>, is a framework for modelling with linguistic expressions, or labels such as small, medium and large. Such labels are defined by overlapping fuzzy sets which are used to cover the universe of continuous variables. Some recent research has applied this framework to machine learning systems, and in particular to a fuzzy Bayesian model <ref type="bibr" target="#b17">[18]</ref>. Here we will apply label semantics to the problem of decision tree induction. Initially, however, a brief overview of label semantics is given in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Random set based framework</head><p>The fundamental question posed by label semantics is how to use linguistic expressions to label numerical values. The basic idea is that when individuals make assertions, such as Ôx is smallÕ, they are essentially providing information about what labels are appropriate for the values of x. For example, for a variable x into a domain of discourse denoted by X we identify a finite set of linguistic labels LA = {L 1 , . . . , L n } with which to label the values of x. Then, for a specific value x 2 X, an individual I identifies a subset of LA, denoted D I</p><p>x to stand for the description of x given by I, as the set of labels with which it is appropriate to label x. If we allow I to vary across a population V with prior distribution P V , then D I x will also vary and generate a random set denoted D x into the power set of LA. By evaluating the probability of occurrence of a particular set of labels say S, for D x across the population then we obtain a distribution on D x referred to as a mass assignment (see <ref type="bibr" target="#b0">[1]</ref> for details on Mass Assignment theory). We can view the random set D x as a description of the variable x in terms of the labels in LA. More formally, Definition 1 (Label description). For x 2 X the label description of x is a random set from V into the power set of LA, denoted D x , with associated distribution m x , given by</p><formula xml:id="formula_0">8S LA; m x ðSÞ ¼ P V ðfI 2 V jD I x ¼ SgÞ</formula><p>In this framework, appropriateness degrees are used to evaluate how appropriate a label is for describing a particular value of variable x. This measure can be defined based on mass assignments as follows:</p><formula xml:id="formula_1">Definition 2 (Appropriateness degrees). 8x 2 X; 8L 2 LA; l L ðxÞ ¼ X SLA:L2S m x<label>ðSÞ</label></formula><p>This definition provides a relationship between mass assignments and appropriateness degrees. Clearly l L is a function mapping from X into [0, 1] and therefore can technically be viewed as the membership function of a fuzzy set. In simple terms, given a particular value x, the appropriateness degree of L as a label for x where L is represented by fuzzy set F, is the membership value of x belonging to F. The reason we use the new term Ôappropriateness degreeÕ is partly because it more accurately reflects the underlying semantics and partly to highlight the quite distinct calculus based on this framework.</p><p>For example, an expression such as Ôthe score on a dice is smallÕ, as asserted by individual I, is interpreted to mean D I SCORE ¼ fsmallg, where SCORE denotes the value of the score given by a single throw of a particular dice. When I varies across a population V, different sets of labels could be given to describe the variable SCORE, so that we obtain the random set of D SCORE into the power set of LA.</p><p>Example 1. Suppose the variable SCORE with universe {1, 2, 3, 4, 5, 6} gives the outcome of a single throw of a particular dice. Let LA = {small, medium, large} and V = {I 1 , I 2 , I 3 } then a possible definition of D SCORE is given as follows:</p><formula xml:id="formula_2">D I 1 1 ¼ D I 2 1 ¼ D I 3 1 ¼ fsmallg D I 1 2 ¼ fsmall; mediumg; D I 2 2 ¼ D I 3 2 ¼ fsmallg D I 1 3 ¼ D I 2 3 ¼ fmediumg; D I 3 3 ¼ fsmall; mediumg D I 1 4 ¼ fmedium; largeg; D I 2 4 ¼ D I 3 4 ¼ fmediumg D I 2 5 ¼ flargeg; D I 1 5 ¼ D I 3 5 ¼ fmedium; largeg D I 1 6 ¼ D I 2 6 ¼ D I 3 6</formula><p>¼ flargeg Assuming a uniform prior distribution on V, so that P V = 1/jVj, then the mass assignment of D x can be represented according to Definition 1 as follows:</p><formula xml:id="formula_3">8S LA; m x ðSÞ ¼ jfI 2 V jD I x ¼ Sgj jV j<label>ð1Þ</label></formula><p>We can determine mass assignments on D SCORE according to Eq. (1). For example, if SCORE = 4 we have</p><formula xml:id="formula_4">m 4 ðfmedium; largegÞ ¼ jfI 2 V jD I 4 ¼ fmedium; largeggj jV j ¼ jfI 1 gj jV j ¼ 1 3 m 4 ðfmediumgÞ ¼ jfI 2 V jD I 4 ¼ fmediumggj jV j ¼ jfI 2 ; I 3 gj jV j ¼ 2 3</formula><p>We then have the mass assignment for SCORE = 4 as follows:</p><formula xml:id="formula_5">m 4 ¼ fmedium; largeg : 1 3 ; fmediumg : 2 3</formula><p>In the sequel 1/3 and 2/3 are also referred to as the associated mass for {medium, large} and {medium}, respectively. Similarly, we can obtain</p><formula xml:id="formula_6">m 1 ¼ fsmallg : 1; m 2 ¼ fsmallg :<label>2 3</label></formula><p>; fsmall; mediumg :</p><formula xml:id="formula_7">1 3 m 3 ¼ fsmall; mediumg : 1 3 ; fmediumg : 2 3 m 5 ¼ fmedium; largeg : 2 3 flargeg : 1 3 ; m 6 ¼ flargeg : 1</formula><p>The values of m 1 , . . . , m 6 are not dependent on the distribution of the data (i.e. the distribution on {1, . . . , 6}. Rather they are dependent on the sets of appropriate labels assigned to each label by each voter and on the distribution across voters. The latter in this case is assumed to be uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Label semantics for data mining</head><p>Based on this underlying semantics, we can translate numeric data into a set of appropriate labels with associated masses. It is certainly true that a mass assignment on D x determines a unique appropriateness degree for any function but generally the converse does not hold. For example, given l L 1 ðxÞ ¼ 0:3 and l L 2 ðxÞ ¼ 1, then possible sets of appropriate labels with associated masses are as follows: fL 2 g : 0:7; fL 1 ; L 2 g : 0:3 fL 1 g : 0:1; fL 2 g : 0:8; fL 1 ; L 2 g : 0:2 fL 1 g : 0:2; fL 2 g : 0:9; fL 1 ; L 2 g : 0:1</p><formula xml:id="formula_8">Á Á Á Á Á Á Á Á Á Á Á Á Á Á Á</formula><p>In fact, there are potentially an infinite family of random sets satisfying the given constraints <ref type="bibr" target="#b4">[5]</ref>. This problem can be overcome by making the consonance assumptions, according to which we can determine the mass assignment uniquely from the appropriateness degrees as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Consonant mass assignments on labels</head><formula xml:id="formula_9">). Let {b 1 , . . . , b k } = {l L (x) j L 2 LA, l L (x) &gt; 0} ordered such that b t &gt; b t+1 for t = 1,2, . . . , k À 1 then: m x ¼ M t : bt À b tÀ1 ; for t ¼ 1; 2; . . . ; k À 1 M k : b k ; M 0 : 1 À b 1</formula><p>where M 0 = ; and M t = {L 2 LA j l L (x) P b t } for t = 1,2 . . . ,k.</p><p>For the previous example, given l L 1 ðxÞ ¼ 0:3 and l L 2 ðxÞ ¼ 1, we can calculate the consonant mass assignments as follows: The appropriateness degrees are ordered as {b 1 , b 2 } = {1, 0.3} and</p><formula xml:id="formula_10">M 1 = {L 2 }, M 2 = {L 1 , L 2 }. We then can obtain m x ¼ fL 2 g : b 1 À b 2 ; fL 1 ; L 2 g : b 2 ¼ fL 2 g : 0:7; fL 1 ; L 2 g : 0:3</formula><p>Because the appropriateness degrees are sorted in Definition 3 the resulting mass assignments are ''nested'' (see Fig. <ref type="figure">1</ref>). Clearly then, there is a unique consonant mapping to mass assignments for a given set of appropriateness degree values. The justification of the consonance assumption can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. Notice that in some cases we may have non-zero mass associated with the empty set (left-hand diagram of Fig. <ref type="figure">1</ref>). This means that some voters believe that x cannot be described by any labels in LA. This property would add to the complexity of our learning algorithms and hence we avoid it by introducing a full fuzzy covering defined as follows:</p><formula xml:id="formula_11">Definition 4 (Full fuzzy covering). Given a continuous discourse X, LA is called a full fuzzy covering of X if: 8x 2 X; 9L 2 LA l L ðxÞ ¼ 1</formula><p>In another words, the full fuzzy covering assumes that, for any element, there always exists a particular label which all the voters agree is appropriate to describe this data, though the voters may have different opinions on other labels. Unless otherwise stated, we will use N F fuzzy sets with 50% overlap to cover a continuous universe (see Fig. <ref type="figure">2</ref>), so that the appropriateness degrees satisfy: "x 2 X, $i 2 {1, . . . , N F À1} such that l Li ðxÞ ¼ a, l Liþ1 ðxÞ ¼ b and l Lj ðxÞ ¼ 0 for j &lt; i or j &gt; i + 1 and where max(a, b) = 1. In the case that a = 1 according to the full fuzzy covering assumption, then m x has the following form:</p><formula xml:id="formula_12">1 β 1 β 2 M 0 :1-β 1 M 1 : β 1 -β 2 β 3 M 2 : β 2 -β 3 M 3 : β 3 1 =1 β 2 M 0 :1-β 1 =0 M 1 : β 1 -β 2 β 3</formula><formula xml:id="formula_13">m x ¼ fL i g : 1 À b; fL i ; L iþ1 g : b<label>ð2Þ</label></formula><p>It is also important to note that, given definitions for the appropriateness degrees on labels, we can isolate a set of subsets of LA with non-zero masses. These are referred to as focal sets and the appropriate labels with non-zero masses as focal elements, more formally, Definition 5 (Focal set). The focal set of LA is a set of focal elements defined as</p><formula xml:id="formula_14">F ¼ fS LAj9x 2 X; m x ðSÞ &gt; 0g</formula><p>Based on the above assumptions (consonant, full fuzzy covering with 50% overlap) defined on a particular universe, we can then always find the unique and consistent translation from a given data element to a mass assignment on focal elements, specified by the function l L : L 2 LA. We call this the linguistic translation (LT), which provides us with a way of transforming real-valued data into a set of associated masses for appropriate labels. Definition 6 (Linguistic translation). Suppose we are given a numerical data set D = {hx 1 (i), . . . , x n (i)i j i = 1, . . . , N} and focal set on attribute j : F j ¼ fF 1 j ; . . . ; F h j j j j ¼ 1; . . . ; ng, from the linguistic translation, we then obtain linguistic data set LD defined as follows:</p><p>LD ¼ fA 1 ðiÞ; . . . ; A n ðiÞ j i ¼ 1; . . . ; N g A j ðiÞ ¼ fhm xjðiÞ ðF 1 j Þ; . . . ; m xjðiÞ ðF h j j Þig where m xjðiÞ ðF r j Þ is the associated mass of focal element F r j as appropriate labels for data element x j (i) where r = 1, . . . , h j and j = 1, . . . , n.</p><p>For a particular attribute with an associated focal set, linguistic translation is a process of replacing its data elements with the focal element masses of these data elements. For a variable x, it defines a unique mapping from data element x(i) to a vector of associated masses hm x(i) (F 1 ), . . . , m x(i) (F h )i.</p><p>Example 2. Fig. <ref type="figure">2</ref> shows a full fuzzy covering of the universe X = [0, 1] with three fuzzy labels: small, medium and large with 50% overlap. The following focal elements occur: {small}, {small, medium}, {medium}, {medium, large} and {large}, but the sets {small, medium, large} and {small, large} cannot occur since at no point do small, medium and large all overlap and small and large do not overlap. For the data point x 1 = 0.2, the appropriate labels are small and medium, and the appropriateness degrees of these labels are read from the membership values as follows: l small ð0:2Þ ¼ 1; l medium ð0:2Þ ¼ 0:5</p><p>The mass assignment on the appropriate labels can be calculated based on Eq.</p><p>(2) to give m 0:2 ¼ fsmallg : 0:5; fsmall; mediumg : 0:5 Similarly, for x 2 = 0.44, x 3 = 0.78, we obtain m 0:44 ¼ fsmall; mediumg : 0:3; fmediumg : 0:7 m 0:78 ¼ fmedium; largeg : 0:6; flargeg : 0:4</p><p>The linguistic translation can be illustrated as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Linguistic decision tree</head><p>Tree induction learning models have received a great deal of attention over recent years in the fields of machine learning and data mining because of their simplicity and effectiveness. Among them, the ID3 <ref type="bibr" target="#b15">[16]</ref> algorithm for decision trees induction has proved to be an effective and popular algorithm for building decision trees from discrete valued data sets. The C4.5 <ref type="bibr" target="#b16">[17]</ref> algorithm was proposed as a successor to ID3 in which an entropy based approach to crisp partitioning of continuous universes was adopted. One inherent disadvantage of crisp partitioning is that these methods make the induced decision trees sensitive to noise. This noise is not only due to the lack of precision or errors in measured features but is often present in the model itself since the available features may not be sufficient to provide a complete model of the system. For each attribute, disjoint classes are separated with clearly defined boundaries. These boundaries are ÔcriticalÕ since a small change close to these points will probably cause a complete change in classification. Due to the existence of uncertainty and imprecise information in real-world problems, the class boundaries may not be defined clearly. In this case, decision trees may produce high misclassification rates in testing even if they perform well in training <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fuzzy discretization and linguistic decision tree</head><p>Before we introduce a new decision tree model, the disadvantages of crisp discretization and the advantages of fuzzy discretization are discussed. Fig. <ref type="figure" target="#fig_1">3</ref> shows a decision tree in a two-class problem, in which there are two continuous attributes x and y. Using crisp discretization, the decision space is partitioned into a set of non-overlapping subregions A 1 , A 2 and A 3 , which have clear boundaries with each other. The object for classification will definitely fall into one of these areas. For example, the given object (x = 13.5, y = 46.0) will be classified as A 3 , However, if this object is distorted due to noise so that (x = 12.9, y = 46.2), then the object will be misclassified as A 1 (see Fig. <ref type="figure" target="#fig_1">3a</ref>).</p><p>In contrast, consider the use of fuzzy discretization (Fig. <ref type="figure" target="#fig_1">3b</ref>), where the continuous universe is partitioned by overlapped trapezoidal fuzzy sets {x 1 , x 2 } and {y 1 , y 2 }. As shown in right-hand figure, A 1 , A 2 and A 3 generated from fuzzy discretization appear as overlapping subregions with blurred boundaries. The possibility degree of an object belonging to the each class will be given by the membership of pre-defined fuzzy sets. The object will fall in the overlapping area. These results can then aid the human user to make their final decisions or suggest further investigation. Many fuzzy approaches for decision tree learning have been proposed to overcome the weaknesses described above <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, Ref. <ref type="bibr" target="#b12">[13]</ref> gives a comprehensive overview of the fuzzy decision tree literature. Different from other approaches, the algorithm we present here is based on label semantics and emphasises algorithm transparency.</p><formula xml:id="formula_15">LF 1 LF 2 x y LF 3 &lt;13 &gt;=13 &gt;45 &lt;=45 13 45 X Y A 1 A 2 A 3 LF 1 LF 2 x y LF 3 X Y A 1 A 2 A 3 x 1 x 2 y 1 y 2 x 1 y 2 y 1 x 2<label>(a)</label></formula><p>Consider a database D = {hx 1 (i), . . . , x n (i)i j i = 1, . . . , N} where each instance has n attributes and is labeled by one of the classes: {C 1 , . . . , C m }. Unless otherwise stated, we use uniformly distributed fuzzy sets with 50% overlap to discretized each continuous attribute universe and obtain a corresponding linguistic data set LD by applying linguistic translation (Definition 6). A linguistic decision tree is a decision tree where the nodes are the random set label descriptions and the branches correspond to particular focal elements based on LD. More formally: Definition 7 (Linguistic decision tree). A linguistic decision tree is a set of branches with associated class probabilities of the following form:</p><formula xml:id="formula_16">LDT ¼ fhB 1 ; PrðC 1 jB 1 Þ; . . . ; PrðC m jB 1 Þi; . . . hB s ; PrðC 1 jB s Þ; . . . ; PrðC m ÞjB s Þig</formula><p>and a branch B with k nodes is defined as</p><formula xml:id="formula_17">B ¼ hF j 1 ; . . . ; F j k i</formula><p>where, k 6 n and F j 2 F j .</p><p>F j i for i = 1, . . . , k are the branch nodes represented by focal elements of attribute j i and i is the node position in the branch B. Within a LDT (see Fig. <ref type="figure" target="#fig_2">4</ref>) each node splits into branches according to the focal elements of this node (attribute). One attribute is not allowed to appear more than once in a branch, and an attribute which is not currently part of a branch is referred to as a free attribute.</p><formula xml:id="formula_18">LF 4 LF 2 LF 5 Dx 1 Dx 2 Dx 2 LF 6 LF 1 {small 1 } {large 1 } {small 1 , large 1 } LF 3 LF 7 {small 2 } {large 2 } {small 2 , large 2 } {small 2 } {large 2 } {small 2 ,</formula><p>Definition 8 (Free attributes). The set of attributes free to use for expanding a given branch B is defined by</p><formula xml:id="formula_19">ATT B ¼ fx j j 8F 2 F j ; F 6 2 Bg</formula><p>The length of a branch, corresponding to the number of component nodes (attributes), is less than or equal to n, the number of attributes. In a LDT, the length of the longest branch is called the depth of the LDT, which is also less than or equal to n. Each branch has an associated probability distribution on the classes. For example, a LDT shown in Fig. <ref type="figure" target="#fig_2">4</ref> might be obtained from training, the branch hh{large 1 }, {small 2 , large 2 }i, 0.7, 0.3i means the probability of class C 1 is 0.3 and C 2 is 0.7 given attribute 1 that can be only described as large and attribute 2 can be described as small and large. We need to be aware that the linguistic expressions such as small, medium or large for each attribute are not necessarily the same, since they are defined independently on each attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluating class probabilities for a given branch</head><p>According to the definition of LDT (Definition 7), for a branch of a LDT of the form of B ¼ hF j 1 ; . . . ; F j k i, the probability of class C t (t = 1, . . . , m) given B can then be evaluated from the training linguistic dataset LD as follows:</p><formula xml:id="formula_20">PrðC t jBÞ ¼ SðBjLD t Þ SðBjLDÞ ¼ P i2LDt Q k r¼1 m xjrðiÞ ðF jr Þ P i2LD Q k r¼1 m x jr ðiÞ ðF jr Þ<label>ð3Þ</label></formula><p>where S( AE ) is a function for calculating the sum of products of masses associated with focal elements consisting the branch B (see Example 3). LD t is the subset consisting of instances belong to class t and S(BjLD) 5 0. In fact S(BjLD t ) is proportional to the probability that the branch is satisfied and that the class is C t . Similarly, S(BjLD) is proportional to the probability that the branch is satisfied. All probabilities are calculated on the basis of the linguistic database LD. To understand the form of the equation note that the probability branch hF j 1 ; . . . ; F j k i is satisfied by attribute values hx 1 , . . . , x n i is simply taken as being proportional to the product of the probabilities of focal element F j r given x j r (i.e. m xj r ðF j r ÞÞ. This corresponds to the conditional independence assumption that the labels appropriate to describe x jr depends only on the value of that variable, once known, and is independent of the values of all other variables. These products are then summed across the relevant parts of the database to obtain the required probabilities.</p><p>In the case of S(BjLD) = 0, which can occur when the training database for the LDT is small then there is no non-zero linguistic data covered by the branch. In this case, we obtain no information from the database so equal probabilities are assigned to each class:</p><formula xml:id="formula_21">PrðC t jBÞ ¼ 1 m for t ¼ 1; . . . ; m if SðBjLDÞ ¼ 0<label>ð4Þ</label></formula><p>At a particular depth, if one of the class probability reaches a certain threshold, for example 0.9, then we might take the view that this branch is sufficiently discriminating and that further expansion may lead to overfitting. In this case terminating the tree expansion at the current depth will probably help maintain accuracy on the test set. To this end, we employ a threshold probability to determine whether or not a particular branch should terminate.</p><p>Definition 9 (Threshold probability). In the process of building a linguistic decision tree, if the maximum class probability given a particular branch is greater than or equal to a given threshold probability T, then the branch will be terminated at the current depth.</p><p>Obviously, when using this probability-based thresholding, the branches of a tree may have different lengths. For example, see Fig. <ref type="figure" target="#fig_2">4</ref>, where the threshold probability is T = 0.9, so that the 4th branch h{small 1 , large 1 }i is terminated at the depth 1 while the other branches expand to the next depth.</p><p>In the above discussion we have been concerned with continuous (or numerical) attribute, but can we learn with discrete (or nominal) attributes? One problem is that the values of discrete attributes do not have a natural ordering like continuous ones. For example, values for a personÕs age can be sorted in an increasing manner so that the labels young, middle-aged and old, can be meaningfully defined by fuzzy sets. However, if we consider the gender of a person, there are only two possible values: male or female, which are unordered. Hence, partitioning discrete attribute domains using fuzzy labels is problematic. Instead we do not attempt to group discrete values but treat discrete values as distinct labels which do not overlap with each other. Hence, the following focal elements for the attribute ''gender'' are: {male} and {female}. In this representation the associated masses for each focal element will be binary, i.e. either zero or one. For instance,</p><formula xml:id="formula_22">m gender ðfmalegÞ ¼ 1 if gender ¼ male 0 otherwise</formula><p>Missing values can be handled simply by assigning the masses of corresponding focal elements to zeros. For example, in Table <ref type="table" target="#tab_3">2</ref>, the 4th instance has a missing value in Attribute 1. Instead of pre-processing<ref type="foot" target="#foot_2">1</ref> the data, we simply assign the value zero to each mass for focal elements of this missing value.</p><p>Example 3. Consider a two-class problem with 2 attributes, where LA 1 = {small 1 (s 1 ), large 1 (l 1 )} and LA 2 = {small 2 (s 2 ), large 2 (l 2 )}. We assume the focal set F 1 ¼ ffs 1 g; fs 1 ; l 1 g; fl 1 gg and F 2 ¼ ffs 2 g; fs 2 ; l 2 g; fl 2 gg. Suppose that the linguistic database generated from the training database is given in Table <ref type="table" target="#tab_3">2</ref>, and it has two target classes: positive (+) and negative (À). Now suppose we are given two branches of the form:   </p><formula xml:id="formula_23">B 1 ¼ hhfsmall 1 g; fsmall 2 gi; PrðþjB 1 Þ; PrðÀjB 1 Þi B 2 ¼</formula><formula xml:id="formula_24">¼ 0 Â 0 þ 0 Â 0 þ 0 Â 0:3 0 Â 0 þ 0:2 Â 0:5 þ 0 Â 1 þ 0 Â 0 þ 0 Â 0:3 ¼ 0 PrðÀ; B 1 Þ ¼ P i¼2;3 Q r¼1;2 m xj r ðiÞ ðF j r Þ P 5 i¼1 Q r¼1;2 m xj r ðiÞ ðF j r Þ ¼ P i¼2;3 m x 1</formula><formula xml:id="formula_25">0:2 Â 0:5 þ 0 Â 0 0 Â 0 þ 0:2 Â 0:5 þ 0 Â 1 þ 0 Â 0 þ 0 Â 0:3 ¼ 0:1 0:1 ¼ 1</formula><formula xml:id="formula_26">0:4 Â 0:7 þ 0 Â 1 þ 1 Â 0:7 0:4 Â 0:7 þ 0:8 Â 0:5 þ 0:9 Â 0 þ 0 Â 1 þ 1 Â 0:7 ¼ 0:28 þ 0:7 0:28 þ 0:4 þ 0:7 ¼ 0:710 PrðÀ; B 2 Þ ¼ P i¼2;3 Q r¼1;2 m xj r ðiÞ ðF j r Þ P 5 i¼1 Q r¼1;2 m xj r ðiÞ ðF j r Þ ¼ P i¼2;3 m x 1<label>ðiÞ</label></formula><p>ðfs 1 ; l 1 gÞ Â m x 2 ðiÞ ðfs 2 ; l 2 gÞ P 5 i¼1 m x 1 ðiÞ ðfs 1 ; l 1 gÞ Â m x 2 ðiÞ ðfs 2 ; l 2 gÞ</p><formula xml:id="formula_27">¼ 0:8 Â 0:5 þ 0:9 Â 0 0:4 Â 0:7 þ 0:8 Â 0:5 þ 0:9 Â 0 þ 0 Â 1 þ 1 Â 0:7 ¼ 0:4 0:28 þ 0:4 þ 0:7 ¼ 0:290 3.3. Classification</formula><p>Consider classifying a given data vector ỹ ¼ hy 1 ; . . . ; y n i which may not be contained in the training data set D. Firstly, we need to translate ỹ into a linguistic form based on the fuzzy covering of the training data. In the case that, for some attribute the data element appears beyond the range of training data [R min , R max ], we assign the appropriateness degrees of R min or R max to the element depending on which side of the range it appears.</p><p>JeffreyÕs rule: Hence, we can evaluate the probabilities of class C t based on a given LDT consisting of s branches by using JeffreyÕs rule as follows:</p><formula xml:id="formula_28">PrðaÞ ¼ PrðajbÞPrðbÞ þ Prðaj</formula><formula xml:id="formula_29">PrðC t jỹÞ ¼ X s v¼1 PrðC t jB v ÞPrðB v jỹÞ ð<label>5Þ</label></formula><p>where PrðBjỹÞ is the probability of a particular branch given a data element. This can be evaluated by taking the product of associated masses of focal elements of the data element along the branch. More formally,</p><formula xml:id="formula_30">PrðBjỹÞ ¼ X k r¼1 m y j r ðF j r Þ ð<label>6Þ</label></formula><p>In classical decision trees, classification is made according to the class label of the branch in which the data falls. In our approach, the data for classification partially satisfies the constraints represented by a number of branches and the probability estimates across the whole decision tree are then used to obtain an overall classification.</p><p>Example 4. Suppose we are given the linguistic decision tree shown in Fig. <ref type="figure" target="#fig_2">4</ref> for a two-class problem with F 1 ¼ ffsmall 1 g; fsmall 1 ; large 1 g; flarge 1 gg, F 2 ¼ ffsmall 2 ; fsmall 2 ; large 2 g; flarge 2 gg. A data element ỹ ¼ hy 1 ; y 2 i for classification is given such that</p><formula xml:id="formula_31">l small 1 ðy 1 Þ ¼ 1, l large 1 ðy 1 Þ ¼ 0:4 and l small 2 ðy 2 Þ ¼ 0:2, l large 2 ðy 2 Þ ¼ 1.</formula><p>The LDT given in Fig. <ref type="figure" target="#fig_2">4</ref> can be written as Hence, from JeffreyÕs rule (Eq. ( <ref type="formula" target="#formula_29">5</ref>)) we obtain that</p><formula xml:id="formula_32">LDT ¼</formula><formula xml:id="formula_33">PrðC 1 jỹÞ ¼ X 7 v¼1 PrðB v jỹÞPrðC 1 jB v Þ ¼ X v¼2;3;4 PrðB v jỹÞPrðC 1 jB v Þ ¼ 0:12 Â 0:5 þ 0:48 Â 0:6 þ 0:4 Â 0:1 ¼ 0:388 and PrðC 2 jỹÞ ¼ X 7 v¼1 PrðB v jỹÞPrðC 2 jB v Þ ¼ X v¼2;3;4 PrðB v jỹÞPrðC 2 jB v Þ ¼ 0:12 Â 0:5 þ 0:48 Â 0:6 þ 0:4 Â 0:9 ¼ 0:612 3.4. LID3 algorithm</formula><p>Linguistic ID3 (LID3) is the learning algorithm we propose for building the linguistic decision tree based on a given linguistic database. Similar to the ID3 algorithm <ref type="bibr" target="#b15">[16]</ref>, search is guided by an information based heuristic, but the information measurements of a LDT are modified in accordance with label semantics. The measure of information defined for a branch B and can be viewed as an extension of the entropy measure used in ID3.</p><p>Definition 10 (Branch entropy). The entropy of branch B is given by</p><formula xml:id="formula_34">EðBÞ ¼ À X m t¼1 PrðC t jBÞlog 2 ðPrðC t jBÞÞ<label>ð7Þ</label></formula><p>Now, given a particular branch B suppose we want to expand it with the attribute x j . The evaluation of this attribute will be given based on the expected entropy defined as follows.</p><p>Definition 11 (Expected entropy).</p><formula xml:id="formula_35">EEðB; x j Þ ¼ X F j 2Fj EðB [ F j Þ Á PrðF j jBÞ ð<label>8Þ</label></formula><p>where B [ F j represents the new branch obtained by appending the focal element F j to the end of branch B. The probability of F j given B can be calculated as follows:</p><formula xml:id="formula_36">PrðF j jBÞ ¼ SðB [ F j jLDÞ SðBjLDÞ<label>ð9Þ</label></formula><p>We can now define the Information Gain (IG) obtained by expanding branch B with attribute x j as</p><formula xml:id="formula_37">IGðB; x j Þ ¼ EðBÞ À EEðB; x j Þ ð<label>10Þ</label></formula><p>The goal of tree-structured learning models is to make subregions partitioned by branches be less ''impure'', in terms of the mixture of class labels, than the unpartitioned dataset. For a particular branch, the most suitable free attribute for further expanding (or partitioning), is the one by which the ''pureness'' is maximumly increased with expanding. That corresponds to selecting the attribute with maximum information gain. As with ID3 learning, the most informative attribute will form the root of a linguistic decision tree, and the tree will expand into branches associated with all possible focal elements of this attribute. For each branch, the free attribute with maximum information gain will be the next node, from level to level, until the tree reaches the maximum specified depth or the maximum class probability reaches the given threshold probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Non-uniform discretization with fuzzy sets</head><p>In this section, we discuss two alternative discretization techniques that can be used instead of the uniform method described above: percentile-based discretization and entropy-based discretization. Basically, fuzzy discretization provides an interpretation between numerical data and linguistic data based on label semantics. The effectiveness of fuzzy discretization depends much on the algorithmÕs performance based on the linguistic data. The simplest approach is to use uniformly distributed fuzzy sets for discretization. However, in some real-world applications, background knowledge about attributes may be available and can be used directly for discretization rather than an automatic discretization technique. For example, a feature ranging from 1 to 99 to describe human age can be uniformly discretized into 3 intervals: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">33]</ref>, <ref type="bibr">[34,</ref><ref type="bibr">66]</ref> and <ref type="bibr">[67,</ref><ref type="bibr">99]</ref>. But our background knowledge suggesting that the partition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">25]</ref>, [26, 50] and [50, 99] may be more reasonable. However, if no relevant background knowledge is available, the question remains as to whether we can improve on uniform discretization.</p><p>In the LDT model, focal elements are directly used as branches in building the linguistic decision tree but not the fuzzy sets. In fact, there is a unique mapping from trapezoidal fuzzy sets to focal elements which can be represented by triangular functions. <ref type="foot" target="#foot_3">2</ref> Formally, these functions correspond to the m x (F) as x varies, for each focal element F. In order to improve the performance of our algorithm, we need to generate focal elements that are as discriminative as possible and the associated fuzzy sets can then be obtained according to Definitions 2 and 5. See Fig. <ref type="figure" target="#fig_4">5</ref>, for example, where we obtain the discriminative focal elements (asymmetric triangular fuzzy sets) using the percentile-based discretization outlined below and then generate associated trapezoidal fuzzy sets using Definitions 2 and 5. The following two discretization methods are introduced and will be used to generate fuzzy labels in further experimental studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Percentile-based discretization</head><p>In this approach discretization is based on data distribution, so that the attribute universe is partitioned into intervals which each contains approximately the same number of data elements. It is a very intuitive way for generating fuzzy sets. For example, see Fig. <ref type="figure" target="#fig_4">5</ref> where the continuous attribute universe for variable x ranging from 0 to 200 is labeled by 3 fuzzy sets: small, medium and large (N F = 3). According to the assumptions we made in Section 2, there are 5 focal elements. We then need 4 cut points that partition the universe into 5 intervals each containing approximately the same number of examples. The functions of focal elements m x (F) as x varies are drawn in the upper sub-figure and the fuzzy sets obtained are shown in the lower sub-figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Entropy-based discretization</head><p>In this approach, the discretization is based on the expected entropy of the resulting partition. In fact we aim to obtain the partition maximizing the information gain <ref type="bibr" target="#b11">[12]</ref>. For a particular attribute, suppose we have a set of data values S = {x 1 , . . . , x N } according to which we want to define q focal elements, then initially we need to find q À 1 cut points forming a partition of the universe. These boundary points are identified so as to maximize information gain in the following way. Every pair of adjacent data points suggests a potential boundary point: c i = (x i + x i+1 )/2 where i = 1, . . . , N À 1. Now Fayyad <ref type="bibr" target="#b3">[4]</ref> has proved that only the class boundary points can be the boundary points if we are to obtain the maximum information in classification, which implies that c i cannot lead to a partition that has maximum information gain if x i and x i+1 belong to the same class. Therefore, we should generate a candidate set, which contains all of class boundary points, from which we then need to find q À 1 points with which we can maximize the information gain defined by Eq. ( <ref type="formula" target="#formula_38">11</ref>) <ref type="bibr" target="#b11">[12]</ref>:</p><p>GainðS; HÞ ¼ EntropyðSÞ À X v¼1;...;q</p><formula xml:id="formula_38">jS v j jSj EntropyðS v Þ ð<label>11Þ</label></formula><p>where H is a subset of the candidate set containing the q À 1 cut points. The cut points partition the original universe S into q intervals: S 1 . . . , S q . And the entropy is defined in Eq. ( <ref type="formula" target="#formula_39">12</ref>), where m is the number of classes and p i is the percentage of instances belong to a particular class within S:</p><formula xml:id="formula_39">EntropyðSÞ ¼ X m i¼1 Àp i log 2 p i<label>ð12Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Experimental studies</head><p>We evaluated the LID3 algorithm by using 14 datasets taken from the UCI Machine Learning repository <ref type="bibr" target="#b2">[3]</ref>. These datasets have representative properties of real-world data, such as missing values, multi-classes, mixed-type data (numerical, nominal) and unbalanced class distributions, etc. Table <ref type="table" target="#tab_5">3</ref> shows the dataset, the number of classes, the number of instances, the number of numerical (Num.) and nominal (Nom.) attributes and whether or not the database contains missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1.">Experimental strategies</head><p>In the following experiments, unless otherwise stated, attributes are discretized by 2 trapezoidal fuzzy sets with 50% overlap, and classes are evenly splited into two sub-datasets, one half for training and the other half for testing, this is referred to as 50-50 split experiment. The maximal depth is set manually and the results presented in this paper show the best performance of LID3 across a range of depth settings.We also test the LID3 algorithm with different threshold probabilities T ranging from 0.6 to 1.0 in steps of 0.1 and for the different fuzzy discretization methods: uniform (Uni), entropy-based (En) and percen-tile-based (Per). For each dataset, we ran 50-50 random split experiment for 10 times. The average test accuracy with standard deviation are shown in the right-hand side of Table <ref type="table" target="#tab_7">5</ref>, the probability and the depth at which we obtain this accuracy are listed in Table <ref type="table" target="#tab_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2.">The influence of the threshold probability</head><p>As can be seen from the results, the best accuracy is usually obtained with high threshold probabilities T = 0.9 or T = 1.0, especially for datasets with only numerical attributes (such as breast-w, iris, balance, wine) or where numerical attributes play important roles in learning (ecoli, heptitis). Recent work on PETs (Probability Estimation Trees) <ref type="bibr" target="#b14">[15]</ref> also suggests that the full expanded estimation trees give better performance than pruned trees. <ref type="foot" target="#foot_4">3</ref> The reason for this is that the heuristics used to generate small and compact tree by pruning tend to reduce the quality of the probability estimates <ref type="bibr" target="#b14">[15]</ref>. In this context linguistic decision trees can be thought of as a type of probability estimation trees but where the branches correspond to linguistic descriptions of objects.</p><p>The difference in accuracy resulting from varying the threshold probability T is quite data dependent. For instance, Fig. <ref type="figure" target="#fig_5">6</ref> shows the results of four typical a There are nine nominal attributes in the dataset, but three of them are ordered and therefore can be treated as numerical attributes.</p><p>b A particular single split is used.</p><p>datasets: breast-w, heart-statlog, glass and breast-cancer. In breast-w, the accuracy curves are nested relative to increasing values of T. The models with high T values outperform those with lower T values in all depths. Dataset iris, balance, sonar, wine, ecoli also behave in this way. On the other hand, for datasets heart-statlog, pima, liver, heart-c and heptitis, the accuracy curve of T = 0.9 is better than all other T values at certain depths. In addition, datasets glass and ecoli have accuracy curves which are very close to each other and are even identical on some trials. For the breast-cancer dataset the accuracy actually decreases with increasing T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3.">Comparing LID3 to other machine learning algorithms</head><p>From Table <ref type="table" target="#tab_6">4</ref>, we also see that the optimal values of T and depth are relatively invariant across the discretization techniques. Overall the entropy-based and percentile-based discretization methods performed better than the uniform discretization although no statistically significant difference was found between the three methods. We now compare LID3 with different discretization with C4.5, Naive Bayes (N.B.) Learning and Neural Networks (N.N.)<ref type="foot" target="#foot_5">4</ref> using 10 50-50 splits on each dataset and the average accuracy and standard deviation for each test are shown in Table <ref type="table" target="#tab_7">5</ref>.</p><p>The reason of choosing these three particular learning algorithms is as follows; C4.5 is the most well-known tree induction algorithm, Naive Bayes is a simple but effective probability estimation method and neural networks are a blackbox model well known for its high predictive accuracy. We then carried out paired t-tests <ref type="bibr" target="#b11">[12]</ref> with confidence level 90% to compare LID3-Uniform, LID3-Entropy and LID3-Percentile with each of the three algorithms. A summary of the results is shown in Table <ref type="table">6</ref>. Across the data sets, all LID3 algorithms (with different discretization techniques) outperform C4.5, with LID3-Percentile achieving the best results with 10 wins, 4 ties and no losses. The performance of the Naive Bayes algorithm and LID3-Uniform is roughly equivalent although LID3-Entropy and LID3-Percentile outperform Naive Bayes. From Table <ref type="table" target="#tab_7">5</ref>, we can see that the datasets on which Naive Bayes outperforms LID3 are those with a mixture of continuous and discrete attributes, namely heart-c, heart-statlog and heptitis. Most of the comparisons with Neural Network result in ties rather than wins or losses, especially for LID3-Entropy and LID3-Percentile. Due to the limited number and type of datasets we used for evaluation, we may not draw the strong conclusion that LID3 outperforms all the other three algorithms. However we can at least conclude that based on our experiments, the LID3 outperforms C4.5 and has equivalent performance to Naive Bayes and the Neural Networks. For the datasets with only numerical values, LID3 outperforms both C4.5 and Naive Bayes. Between different discretization methods, percentile-based and entropy-based approaches achieve better results than uniform discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Forward merging of branches</head><p>In the last section, we showed that LID3 performs at least as well as and often better than three well-known classification algorithms across a range of datasets. However, even with only two fuzzy sets for discretization, the number of branches increases exponentially with the depth of the tree. Unfortunately, C4.5 9 wins-4 ties-1 losses 9 wins-5 ties-0 losses 10 wins-4 ties-0 losses N.B.</p><p>3 wins-8 ties-3 losses 7 wins-4 ties-3 losses 7 wins-4 ties-3 losses N.N.</p><p>5 wins-6 ties-3 losses 5 wins-8 ties-1 losses 5 wins-8 ties-1 losses the transparency of the LDT decreases with the increasing number of branches.</p><p>To help to maintain transparency by generating more compact trees, a forward merging algorithm based on the LDT model is proposed in this section and experimental results are given to support the validity of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Forward merging algorithm</head><p>As we have seen in Section 3.5, given a continuous universe with a full fuzzy covering of N F trapezoidal fuzzy sets, the focal elements can be represented by q (q = 2N F À 1) triangular functions. If we vary x in an increasing manner, the focal elements then occur in the following natural order F 1 , . . . , F q . We referred to F i and F i+1 as being adjacent focal elements for F i 2 F and i = 1, . . . , q À 1. As described in Section 3, a node (if it is not terminated according to threshold probability T) is fully expanded with its focal elements. See Fig. <ref type="figure">7a</ref> for instance, where the node is expanded into five leaves LF 1 , . . . , LF 5 with the following focal elements: {small}, {small, medium}, {medium}, {medium, large} and {large}, where leaves next to each other (e.g., {small} and {small, medium}) are adjacent focal elements. The branches whose leaves are adjacent focal elements are referred as adjacent branches. If any of two adjacent branches have sufficiently similar class probabilities according to some criteria, these two branches give similar classification results and therefore can then be merged into one branch in order to obtain a more compact tree (see Fig. <ref type="figure">7b</ref>). We employ a merging threshold to determine whether or not two adjacent branches can be merged.</p><p>Definition 12 (Merging threshold). In a linguistic decision tree, if the maximum difference between class probabilities of two adjacent branches B 1 and B 2 is less than or equal to a given merging threshold T m , then the two branches can be merged into one branch. Formally, if  The results for T m = 0 are obtained with N F = 2 and results for other T m values are obtained with N F values listed in the second column of the table. where C = {c 1 , . . . , c m } is the set of classes, then B 1 and B 2 can be merged into one branch MB.</p><formula xml:id="formula_40">T m P max c2C ðjPrðcjB 1 Þ À PrðcjB 2 ÞjÞ ð13Þ<label>(a) (b) (c)</label></formula><formula xml:id="formula_41">N F T m = 0 T m = 0.1 T m = 0.2 T m = 0.3 T m = 0.4 Acc N R Acc N R Acc N R Acc N R Acc N R<label>1</label></formula><p>Definition 13 (Merged branch). A merged branch MB with k nodes is defined as MB ¼ hM j 1 ; . . . ; M j k i where M j ¼ fF 1 j ; . . . ; F w j g is a set of focal elements such that F i j is adjacent to F iþ1 j for i = 1, . . . , w À 1, The associate mass for M j is given by</p><formula xml:id="formula_42">m x ðM j Þ ¼ X w i¼1 m x ðF i j Þ ð<label>14Þ</label></formula><p>where w is the number of merged adjacent focal elements for attribute j.</p><p>Based on Eqs. ( <ref type="formula" target="#formula_20">3</ref>), ( <ref type="formula" target="#formula_21">4</ref>) and ( <ref type="formula" target="#formula_42">14</ref>) we use the following formula to calculate the class probabilities given a merged branch: When the merging algorithm is applied in learning a linguistic decision tree, the adjacent branches meeting the merging criteria will be merged and reevaluated according to Eq. <ref type="bibr" target="#b14">(15)</ref>. Then the adjacent branches after the first round of merging will be examined in a further round of merging, until all adjacent branches cannot be merged further. We then proceed to the next depth. See Fig. <ref type="figure">7b</ref> and<ref type="figure">c</ref> where leaves LF 2 and LF 3 are merged in the first round of merging, and LF 4 and {LF 2 , LF 3 } are then merged further {LF 2 , LF 3 , LF 4 } if they meet the merging criteria in the second round of merging. The merged branches can be by compound expressions as described in a following section. The merging is applied as the tree develops from the root to the maximum depth and hence is referred to as forward merging.</p><formula xml:id="formula_43">PrðC t jMBÞ ¼ P i2LD t Q k r¼1 m xj r ðiÞ ðM j r Þ P i2LD Q k r¼1 m xj r ðiÞ ðM j r Þ<label>ð15Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental studies</head><p>We tested the forward merging algorithm on the UCI datasets listed in Table 3 with 10 50-50 split experiments and the results are shown in Table <ref type="table" target="#tab_8">7</ref>. Obviously, there is a tradeoff between the algorithm accuracy and the algorithm transparency in terms of the number of leaves. The merging threshold T m plays an important role in the accuracy-transparency tradeoff problem. Algorithm accuracy tends to increase while algorithm transparency decreases with decreasing T m and vice versa.</p><p>The number of fuzzy sets N F in the merging algorithm is also a key parameter. Compared to N F = 3, setting N F = 2 can achieve better transparency, but for some datasets, with N F = 2, the accuracy is greatly reduced although the resulting trees have significantly fewer of branches. For example, Figs. <ref type="figure" target="#fig_6">8</ref> and<ref type="figure" target="#fig_7">9</ref> show the change in test accuracy and the number of leaves (or the number of rules interpreted from a LDT) for different T m on the breast-w dataset. Fig. <ref type="figure" target="#fig_6">8</ref> is with N F = 2 and Fig. <ref type="figure" target="#fig_7">9</ref> with N F = 3. Fig. <ref type="figure" target="#fig_6">8</ref> shows that the accuracy is not greatly influenced by merging, but the number of branches are greatly reduced. This is especially true for the curve marked by Ô+Õ corresponding to T m = 0.3 where applying forward merging, the best accuracy (at the depth 4) is only reduced by approximately 1%, whereas, the number of branches is reduced by roughly 84%. However, in Fig. <ref type="figure" target="#fig_7">9</ref>, at the depth 4 with T m = 0.3, the accuracy also reduces about 1% but the number of branches only reduces by 55%. So, for this dataset, we should choose N F = 2 rather than N F = 3.</p><p>However, this is not always the case. For the dataset iris, the change in accuracy and the number of branches against depth with N F = 2 and N F = 3 are shown in Figs. <ref type="figure" target="#fig_8">10</ref> and<ref type="figure" target="#fig_9">11</ref>, respectively. As we can see from Fig. <ref type="figure" target="#fig_8">10</ref>, by applying the forward merging algorithm, the accuracy is greatly changed. The best accuracy with merging is roughly 10% worse than non-merging algorithm. But for N F = 3, as we can see from Fig. <ref type="figure" target="#fig_9">11</ref>, the accuracy is not that greatly reduced compared to N F = 2, and we still obtain a reduced number of branches, compared to the accuracy for T m = 0 obtained from N F = 2. In this case we should prefer N F = 3.</p><p>Table <ref type="table" target="#tab_8">7</ref> shows the results with optimal N F and different T m ranging from 0 to 0.4, where T m = 0 represents no merging. Acc represents the average accuracy from 10 runs of experiments and N R is the average number of rules (leaves). Unless otherwise stated, the results obtained in this section are with the threshold probability set to T = 1. The results for T m from 0.1 to 0.4 are obtained at the depth where the optimal accuracy is found when T m = 0. As we can see from the table, for most cases, the accuracy before and after merging are not significantly different but the number of branches are dramatically reduced. In some cases, the merging algorithm even outperforms the LID3 without merging. The possible reason for this is because the merging algorithm generates self-adapting granularities based on class probabilities. Compared to other methods that discretize attributes independently, merging may generate a more reasonable tree with more appropriate information granules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Linguistic reasoning</head><p>In this section we use label semantics to provide a linguistic interpretation for LDTs. We also use this framework to show how LDTs can be used to classify data with linguistic constraints on attributes. In addition, a method for classification on fuzzy data is proposed and supported with empirical studies on a toy problem. Basically, we interpret the main logical connectives as follows: ØL means that L is not an appropriate label, L 1 ^L2 means that both L 1 and L 2 are appropriate labels, L 1 _ L 2 means that either L 1 or L 2 are appropriate labels, and L 1 ! L 2 means that L 2 is an appropriate label whenever L 1 is. If we consider label expressions formed from LA by recursive application of the connectives then an expression h identifies a set of possible label sets according to the following k-function.</p><p>Definition 14 (k-function). Let h and u be expressions generated by recursive application of the connectives Ø, _, ^and ! to the elements of LA. Then the set of possible label sets defined by a linguistic expression can be determined recursively as follows:</p><p>(i) kðL i ðxÞÞ ¼ fS FjfL i g Sg (ii) kð:hÞ ¼ kðhÞ Intuitively, k(h) corresponds to those subsets of F identified as being possible values of D x by expression h. In this sense the imprecise linguistic restriction Ôx is hÕ on x corresponds to the strict constraint D x 2 k(h) on D x . Hence, we can view label descriptions as an alternative to linguistic variables as a means of encoding linguistic constraints <ref type="bibr" target="#b8">[9]</ref>. Here we consider the linguistic constraints take the form of h = hx 1 is h 1 , . . . , x n is h n i, where h j represents a label expression based on LA j : j = 1, . . . , n.</p><p>Example 5. Given a continuous variable x and LA = {small, medium, large}, suppose we are told that ''x is not large but it is small to medium''. This constraint can be interpreted as the logical expression h x ¼ :large ^ðsmall _ mediumÞ According to Definition 14, the possible label sets of the given linguistic constraint h x are kðh x Þ ¼ kð:large ^ðsmall _ mediumÞÞ ¼ ffsmallg; fsmall; mediumg; fmediumgg</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Linguistic interpretation</head><p>Based on the inverse of the k-function (Definition 14), a set of linguistic rules (or label expressions) can be obtained from a set of possible label sets. For example, suppose we are given the possible label sets {{small}, {small, me-dium}}, {medium}}, which does not have an immediately obvious interpretation. However using the a-function (see below), we can convert this set into a corresponding linguistic expression Ølarge ^(small _ medium) or its logical equivalence. More details about linguistic reasoning can be found in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Definition 15 (a-Function).</p><formula xml:id="formula_44">8F 2 F let NðF Þ ¼ [ F 0 2F:F 0 F F 0 ! À F<label>ð16Þ</label></formula><formula xml:id="formula_45">then a F ¼ L2F L ! ^L2NðF Þ :L !<label>ð17Þ</label></formula><p>We can then map a set of focal sets to label expressions based on the a-function as follows:</p><formula xml:id="formula_46">8R 2 F h R ¼ _ F 2R a F where kðh R Þ ¼ R<label>ð18Þ</label></formula><p>The motivation of this mapping is a follows. Given a focal set {s, m} this states that the labels appropriate to describe the attribute are exactly small and medium. Hence, they include s and m and exclude all other labels that occur in focal sets that are supersets of {s, m}. Given a set of focal sets {{s, m}, {m}} this provides the information that the set of labels is either {s, m} or {m} and hence the sentence providing the same information should be the disjunction of the a sentences for both focal sets. The following example gives the calculation of the a-function.</p><p>Example 6. Let LA = {very small (vs), small (s), medium (m), large (l), very large (vl)} and F ¼ ffvs; sg; fsg; fs; mg; fmg; fm; lg; flg; fl; vlgg. For calculating a {l} , we obtain Also we can also obtain As discussed in the last section, a merged LDT was obtained from a realworld dataset ÔirisÕ at the depth 2 when T m = 0.3 and where LA j = {small j (s j ), medium j (m j ), large j (l j ) j j = 1, . . . , 4}.</p><p>LDT M-iris ¼ fMB We can then translate this tree into a set of linguistic expressions as follows:</p><p>LDT M-iris ¼ fhhs 3 ^:ðm 3 _ l 3 Þi; 1:0000; 0:0000; 0:0000i hhm 3 ^:l 3 ; s 4 ^:ðm 4 _ l 4 Þi; 1:0000; 0:0000; 0:0000i hhm 3 ^:l 3 ; m 4 ^:l 4 i; 0:0008; 0:9992; 0:0000i hhm 3 ^:l 3 ; :s 4 ^m4 ^l4 i; 0:0000; 0:5106; 0:4894i hhm 3 ^:l 3 ; ðs 4 _ m 4 Þ ^l4 i; 0:0000; 0:0556; 0:9444i hhl 3 ; s 4 ^:ðm 4 _ l 4 Þi; 0:3333; 0:3333; 0:3333i hhl 3 ; m 4 ^:l 4 i; 0:000; 0:8423; 0:1577i hhl 3 ; l 4 i; 0:000; 0:0913; 0:9087ig Furthermore, the tree itself can be rewritten as a set of fuzzy rules. For example branch 2 corresponds to the rule: IF attribute 3 is medium but not large and attribute 4 is only small, THEN the class probabilities given this branches are (1.0000, 0.0000, 0.0000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Classification under linguistic constraint</head><p>Consider the vector of linguistic constraint h ¼ hh 1 ; . . . ; h n i, where h j is the linguistic constraints on attribute j. We can evaluate a probability value for class C t conditional on this information using a given linguistic decision tree as follows. The mass assignment given a linguistic constraint h is evaluated by</p><formula xml:id="formula_47">8F j 2 F j ; m h j ðF j Þ ¼ pmðF j Þ P F j 2kðh j Þ pmðF j Þ if F j 2 kðh j Þ 0 otherwise 8 &lt; :<label>ð19Þ</label></formula><p>where pm(F j ) is the prior mass for focal elements F j 2 F j derived from the prior distribution p(x j ) on X j as follows:</p><formula xml:id="formula_48">pmðF j Þ ¼ Z Xj m x ðF j Þpðx j Þdx j<label>ð20Þ</label></formula><p>Usually, we assume that p(x j ) is the uniform distribution over X j so that</p><formula xml:id="formula_49">pmðF j Þ / Z Xj m x ðF j Þdx j<label>ð21Þ</label></formula><p>More details on calculation of mass assignment given a linguistic constraint are given in Example 7. For branch B with s nodes, the probability of B given h is evaluated by</p><formula xml:id="formula_50">PrðBj hÞ ¼ Y k r¼1 m hjr ðF jr Þ ð<label>22Þ</label></formula><p>and therefore, by JeffreyÕs rule <ref type="bibr" target="#b6">[7]</ref> PrðC t j hÞ ¼ X s v¼1 PrðC t jB v ÞPrðB v j hÞ ð 23Þ</p><p>Example 7. Given the LDT in Example 4 Section 3.3 suppose we know that for a particular data element ''x 1 is not large and x 2 is small''. We then can translate this knowledge into the following linguistic constraint vector:</p><formula xml:id="formula_51">h ¼ hh 1 ; h 2 i ¼ h:large 1 ; small 2 i</formula><p>By applying the k function (Definition 14), we can generate the associated label sets, so that kð:large 1 Þ ¼ ffsmall 1 gg; kðsmall 2 Þ ¼ ffsmall 2 g; fsmall 2 ; large 2 gg suppose the prior mass assignments are pm 1 ¼ fsmall 1 g : 0:4; fsmall 1 ; large 1 g : 0:3; flarge 1 g : 0:3 pm 2 ¼ fsmall 2 g : 0:3; fsmall 2 ; large 2 g : 0:2; flarge 2 g : 0:5</p><p>From this, according to Eq. ( <ref type="formula" target="#formula_47">19</ref>) we obtain that </p><formula xml:id="formula_52">m h 1 ¼ fsmall 1 g : 0:4=0:4 ¼ fsmall 1 g : 1 m h 2 ¼</formula><formula xml:id="formula_53">1 j hÞ ¼ X 7 v¼1 PrðB v j hÞPrðC 1 jB v Þ ¼ X v¼1;2 PrðB v j hÞPrðC 1 jB v Þ ¼ 0:6 Â 0:3 þ 0:4 Â 0:5 ¼ 0:38 PrðC 2 j hÞ ¼ X 7 v¼1 PrðB v j hÞPrðC 2 jB v Þ ¼ X v¼1;2 PrðB v j hÞPrðC 2 jB v Þ ¼ 0:6 Â 0:7 þ 0:4 Â 0:5 ¼ 0:62</formula><p>The methodology for classification under linguistic constraints allows us to fuse the background knowledge in linguistic form into classification. This is one of the advantages of using high-level knowledge representation language models such as label semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Classification given fuzzy data</head><p>In previous sections LDTs have only been used to classify crisp data where objects are described in terms of precise attribute values. However, in many real-world applications limitations of measurement accuracy means that only imprecise values can be realistically obtained. In this section we introduce the idea of fuzzy data and show how LDTs can be used for classification in this context. Formally, a fuzzy database is defined to be a set of elements or objects each described by linguistic expressions rather than crisp values. In other words FD ¼ fhh 1 ðiÞ; . . . ; h n ðiÞi : i ¼ 1; . . . ; N g Currently there are very few benchmark problems of this kind with fuzzy attribute values. This is because, traditionally only crisp data values are recorded even in cases where this is inappropriate. Hence, we have generated a fuzzy database from a toy problem where the aim is to identify the interior of a figure of eight shape. Specifically, a figure of eight shape was generated according to the equation x ¼ 2 ðÀ0:5Þ ðsinð2tÞ À sinðtÞ and y ¼ 2 ðÀ0:5Þ ðsinð2tÞ þ sinðtÞÞ where t 2 [0, 2p] (see Fig. <ref type="figure" target="#fig_13">13</ref>). Points in [À1.6, 1.6] 2 are classified as legal if they lie within the ÔeightÕ shape (marked with •) and illegal if they lie outside (marked with points).</p><p>To form the fuzzy database we first generated a crisp database by uniformly sampling 961 points across [À1.6, 1.6] 2 . Then each data vector hx 1 , x 2 iwas converted to a vector of linguistic expressions hh 1 ; h 2 i as follows: h j ¼ h Rj where R j ¼ fF 2 F j : m x j ðF Þ &gt; 0g A LDT was then learnt by applying the LID3 algorithm to the crisp database. This tree was then used to classify both the crisp and fuzzy data. The results are shown in Table <ref type="table" target="#tab_12">8</ref> and the results with N F = 7 are shown in Fig. <ref type="figure" target="#fig_12">12</ref>.</p><p>As we can see from Table <ref type="table" target="#tab_12">8</ref>, our model gives a reasonable approximation of the legal data area, though it is not as accurate as testing on crisp data. The   accuracy increases with N F the number of fuzzy sets used for discretization. These results show that LDT model can perform well in dealing with fuzzy and ambiguous data. Here the ÔeightÕ problem is also used for testing classification with linguistic constraints in the following example.</p><p>Example 8. Suppose a LDT is trained on the ÔeightÕ database where each attribute is discretized by five fuzzy sets uniformly: very small (vs), small (s), medium (m), large (l) and very large (vl). Further, suppose we are given the following description of data points:</p><formula xml:id="formula_54">h1 ¼ hx is vs _ s ^:m; y is vs _ s ^:mi h2 ¼ hx is m ^l; y is s ^mi h3 ¼ hx is s ^m; y is l _ vli</formula><p>Experimental results obtained based on the approach introduced in Section 5.2 are as follows:</p><p>PrðC 1 j h1 Þ ¼ 1:000; PrðC 2 j h1 Þ ¼ 0:000 PrðC 1 j h2 Þ ¼ 0:000; PrðC 2 j h2 Þ ¼ 1:000 PrðC 1 j h3 Þ ¼ 0:428; PrðC 2 j h3 Þ ¼ 0:572</p><p>As we can see from Fig. <ref type="figure" target="#fig_13">13</ref>, the above 3 linguistic constraints roughly correspond to the area 1, 2 and 3, respectively. By considering the occurrence of legal and illegal examples within these areas, we can verify the correctness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, a decision tree learning algorithm is proposed based on a random set framework for Computing with Words referred to as label semantics. LID3 is proposed as a modified ID3 algorithm based on label semantics. Unlike classical decision trees, the new algorithm uses probability estimation based on linguistic labels. The linguistic labels are based on fuzzy discretization using a number of different methods including uniform partitioning, a percentile-based partitioning and an entropy-based partitioning. We found that the percentile-based discretization and entropy-based discretization outperform uniform discretization, but no statistically significance was found. By testing our new model on real-world datasets and comparing with three well-know machine learning algorithms, we found that LID3 outperformed C4.5 on all given datasets and outperforms Naive Bayes on datasets with numerical attributes only. Also it has equivalent classification accuracy and better transparency when compared to back propagation Neural Networks.</p><p>In order to obtain a compact tree, a forward merging algorithm was proposed and the experimental results show that the number of branches can be greatly reduced without a significant loss in accuracy. In the last section, a formal method for interpreting a linguistic decision tree as a set of logical rules of labels is proposed. It is also show how LDT can be used to classify fuzzy data where objects are only described in terms of linguistic expressions. Future work will focus on extending the LDT model from classification problems to prediction problems and to allow information fusion so that background knowledge can be incorporated into the tree induction process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 βFig. 1 .Fig. 2 .</head><label>312</label><figDesc>Fig. 1. Calculating mass assignments given the consonance assumption. The right-hand figure is with a full fuzzy covering while the left-hand figure is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustrations of crisp discretization and fuzzy discretization for decision tree models. (This figure was inspired by the similar figure appearing in [14].)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. An example of a linguistic decision tree on a binary classification problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>:bÞPrð:bÞ is used for classifying a new data element, where Pr(b) and Pr(Øb) are considered as the beliefs in b and not b, respectively [7]. This can be generalised when given a new condition c: PrðajcÞ ¼ PrðajbÞPrðbjcÞ þ Prðaj:bÞPrð:bjcÞ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. An example illustrating the relation between fuzzy sets and focal elements based on given fuzzy sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Comparisons of accuracy at different depth with threshold probability ranges from 0.6 to 1.0 on four typical datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The change in accuracy and number of leaves as T m varies on the breast-w dataset with N F = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The change in accuracy and number of leaves as T m varies on the breast-w dataset with N F = 3. While the dot trial T m = 0 is with N F = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The change in accuracy and number of leaves as T m varies on the iris dataset with N F = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The change in accuracy and number of leaves as T m varies on the iris dataset with N F = 3. While the dot trial T m = 0 is with N F = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc>iii) kðh ^uÞ ¼ kðhÞ \ kðuÞ (iv) kðh _ uÞ ¼ kðhÞ [ kðuÞ (v) kðh ! uÞ ¼ kðhÞ [ kðuÞ It should also be noted that the k-function provides us with notion of logical equivalence for label expressions h F u () kðhÞ ¼ kðuÞ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F 0 2 FF 0 !</head><label>20</label><figDesc>: F 0 flg ¼ ffm; lg; flg; fl; vlgg ¼ fm; l; vlg NðflgÞ ¼ [ F 0 2F:F 0 F À flg ¼ fl; vl; mg À flg ¼ fvl; mg ^ð:m ^:vlÞ ¼ :m ^l ^:vl Also we can also obtain a fm;lg ¼ m ^l; a fl;vlg ¼ l ^vl Hence, a set of label sets {{m, l}, {l}, {l, vl}} can be represented by a linguistic expression as follows: h ffm;lg;flg;fl;vlgg ¼ a fm;lg _ a flg _ a fl;vlg ¼ ðm ^lÞ _ ð:m ^l:vlÞ _ ðl ^vlÞ F large where Ô F Õ represents logical equivalence (see Definition 14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Classification on crisp dataset (left) and fuzzy data without masses (right), where each attribute is discretized uniformly by 7 fuzzy sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Testing on the ÔeightÕ problem with linguistic constraints h, where each attribute is discretized by five trapezoidal fuzzy sets: very small, small, medium, large and very large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Notation for this paper</figDesc><table><row><cell>X</cell><cell>Continuous universe</cell><cell>LA</cell><cell>Set of linguistic labels</cell></row><row><cell>L</cell><cell>Linguistic label</cell><cell>l L (x)</cell><cell>Appropriate degree of</cell></row><row><cell>D</cell><cell>Numerical database</cell><cell></cell><cell>using L to label x</cell></row><row><cell>LD</cell><cell>Linguistic database from</cell><cell>N</cell><cell>Number of instances</cell></row><row><cell></cell><cell>linguistic translation of D</cell><cell>n</cell><cell>Number of attribute</cell></row><row><cell>C</cell><cell>Set of target attributes</cell><cell>m</cell><cell>Number of classes</cell></row><row><cell>N F</cell><cell>Number of fuzzy sets</cell><cell>s</cell><cell>Number of branches</cell></row><row><cell>m x</cell><cell>Mass assignment on x</cell><cell>F</cell><cell>Focal set</cell></row><row><cell>F</cell><cell>Focal element</cell><cell>m x (F)</cell><cell>Associated mass of F</cell></row><row><cell>M</cell><cell>A set of focal</cell><cell>k</cell><cell>The length of a branch</cell></row><row><cell></cell><cell>elements for merging</cell><cell>LDT</cell><cell>Linguistic decision tree</cell></row><row><cell>MB</cell><cell>Merged branch</cell><cell>B</cell><cell>Branch of a LDT</cell></row><row><cell>T</cell><cell>Threshold for termination</cell><cell>T m</cell><cell>Threshold for merging</cell></row><row><cell>h</cell><cell>Linguistic expressions</cell><cell>pm</cell><cell>Prior mass assignment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ðiÞ ðfs 1 gÞ Â m x 2 ðiÞ ðfs 2 gÞ P 5 i¼1 m x 1 ðiÞ ðfs 1 gÞ Â m x 2 ðiÞ ðfs 2 gÞ</figDesc><table><row><cell>P P 5 i¼1;4;5 i¼1 r¼1;2 Q r¼1;2 Q m xj r ðiÞ ðF j r Þ m xj r ðiÞ ðF j r Þ</cell><cell>¼</cell><cell>P</cell><cell>i¼1;4;5 m x 1</cell></row></table><note><p><p><p>hhfsmall 1 ; large 1 g; fsmall 2 ; large 2 gi; PrðþjB 2 Þ; PrðÀjB 2 Þi These two branches are evaluated according to Eqs. (</p>3</p>) and (4) so that Prðþ; B 1 Þ ¼</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ðiÞ ðfs 1 gÞ Â m x 2 ðiÞ ðfs 2 gÞ P 5 i¼1 m x 1 ðiÞ ðfs 1 gÞ Â m x 2 ðiÞ ðfs 2 gÞ ¼</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>ðiÞ ðfs 1 ; l 1 gÞ Â m x 2 ðiÞ ðfs 2 ; l 2 gÞ P 5 i¼1 m x 1 ðiÞ ðfs 1 ; l 1 gÞ Â m x 2 ðiÞ ðfs 2 ; l 2 gÞ ¼</figDesc><table><row><cell cols="5">A small-scale linguistic dataset with only two attributes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Instance</cell><cell cols="2">Attribute 1 (x 1 )</cell><cell></cell><cell cols="2">Attribute 2 (x 2 )</cell><cell></cell><cell>Class</cell></row><row><cell></cell><cell>{s 1 }</cell><cell>{ s 1 , l 1 }</cell><cell>{ l 1 }</cell><cell>{ s 2 }</cell><cell>{ s 2 , l 2 }</cell><cell>{ l 2 }</cell><cell></cell></row><row><cell>1</cell><cell>0</cell><cell>0.4</cell><cell>0.6</cell><cell>0</cell><cell>0.7</cell><cell>0.3</cell><cell>+</cell></row><row><cell>2</cell><cell>0.2</cell><cell>0.8</cell><cell>0</cell><cell>0.5</cell><cell>0.5</cell><cell>0</cell><cell>À</cell></row><row><cell>3</cell><cell>0</cell><cell>0.9</cell><cell>0.1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>À</cell></row><row><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>+</cell></row><row><cell>5</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0.3</cell><cell>0.7</cell><cell>0</cell><cell>+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>According to Eq. (6), we obtain that PrðB 2 jỹÞ ¼ m y 1 ðfsmall 1 gÞ Â m y 2 ðfsmall 2 ; large 2 gÞ ¼ 0:6 Â 0:2 ¼ 0:12</figDesc><table /><note><p><p>fB 1 ; B 2 ; B 3 ; B 4 ; B 5 ; B 6 ; B 7 g ¼ fhhfsmall 1 g; fsmall 2 gi; 0:3; 0:7i; hhfsmall 1 g; fsmall 2 ; large 2 gi; 0:5; 0:5i; hhfsmall 1 g; flarge 2 gi; 0:6; 0:4i; hhfsmall 1 ; large 1 gi; 0:1; 0:9i; hhflarge 1 g; fsmall 2 gi; 0:6; 0:4i hhflarge 1 g; fsmall 2 ; large 2 gi; 0:7; 0:3i hhflarge 1 g; flarge 2 gi; 0:2; 0:8ig</p>The mass assignments on hy 1 , y 2 i are m y 1 ¼ fsmall 1 ; large 1 g : 0:4; fsmall 1 g : 0:6 m y 2 ¼ fsmall 2 ; large 2 g : 0:2; flarge 2 g : 0:8 PrðB 3 jỹÞ ¼ m y 1 ðfsmall 1 gÞ Â m y 2 ðflarge 2 gÞ ¼ 0:6 Â 0:8 ¼ 0:48 PrðB 4 jỹÞ ¼ m y 1 ðfsmall 1 ; large 1 gÞ ¼ 0:4 PrðB 1 jỹÞ ¼ PrðB 5 jỹÞ ¼ PrðB 6 jỹÞ ¼ PrðB 7 jỹÞ ¼ 0</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Descriptions of datasets from UCI repository</cell><cell></cell><cell></cell><cell></cell></row><row><cell>No.</cell><cell>Dataset</cell><cell>Classes</cell><cell>Size</cell><cell>Missing values</cell><cell cols="2">Num. of attributes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Numerical</cell><cell>Nominal</cell></row><row><cell>1</cell><cell>Balance</cell><cell>3</cell><cell>625</cell><cell>No</cell><cell>4</cell><cell>0</cell></row><row><cell>2</cell><cell>Breast-cancer a</cell><cell>2</cell><cell>286</cell><cell>Yes</cell><cell>3</cell><cell>6</cell></row><row><cell>3</cell><cell>Breast-w</cell><cell>2</cell><cell>699</cell><cell>No</cell><cell>9</cell><cell>0</cell></row><row><cell>4</cell><cell>Ecoli</cell><cell>8</cell><cell>336</cell><cell>No</cell><cell>7</cell><cell>1</cell></row><row><cell>5</cell><cell>Glass</cell><cell>6</cell><cell>214</cell><cell>No</cell><cell>9</cell><cell>0</cell></row><row><cell>6</cell><cell>Heart-c</cell><cell>2</cell><cell>303</cell><cell>Yes</cell><cell>6</cell><cell>7</cell></row><row><cell>7</cell><cell>Heart-Statlog</cell><cell>2</cell><cell>270</cell><cell>No</cell><cell>7</cell><cell>6</cell></row><row><cell>8</cell><cell>Heptitis</cell><cell>2</cell><cell>155</cell><cell>Yes</cell><cell>6</cell><cell>13</cell></row><row><cell>9</cell><cell>Ionosphere</cell><cell>2</cell><cell>351</cell><cell>No</cell><cell>34</cell><cell>0</cell></row><row><cell>10</cell><cell>Iris</cell><cell>3</cell><cell>150</cell><cell>No</cell><cell>4</cell><cell>0</cell></row><row><cell>11</cell><cell>Liver</cell><cell>2</cell><cell>345</cell><cell>No</cell><cell>6</cell><cell>0</cell></row><row><cell>12</cell><cell>Pima</cell><cell>2</cell><cell>768</cell><cell>No</cell><cell>8</cell><cell>0</cell></row><row><cell>13</cell><cell>Sonar b</cell><cell>2</cell><cell>2 0 8</cell><cell>N o</cell><cell>6 0</cell><cell>0</cell></row><row><cell>14</cell><cell>Wine</cell><cell>3</cell><cell>178</cell><cell>No</cell><cell>14</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Summary of the threshold probabilities and depths for obtaining the best accuracy with different discretization methods on the given datasets</figDesc><table><row><cell>No.</cell><cell>LID3-Uniform</cell><cell></cell><cell>LID3-Entropy</cell><cell></cell><cell>LID3-Percentile</cell><cell></cell></row><row><cell></cell><cell>Threshold</cell><cell>Depth</cell><cell>Threshold</cell><cell>Depth</cell><cell>Threshold</cell><cell>Depth</cell></row><row><cell>1</cell><cell>1.0</cell><cell>4</cell><cell>1.0</cell><cell>4</cell><cell>1.0</cell><cell>4</cell></row><row><cell>2</cell><cell>0.7</cell><cell>2</cell><cell>0.7</cell><cell>2</cell><cell>0.7</cell><cell>2</cell></row><row><cell>3</cell><cell>1.0</cell><cell>4</cell><cell>1.0</cell><cell>3</cell><cell>1.0</cell><cell>3</cell></row><row><cell>4</cell><cell>1.0</cell><cell>7</cell><cell>1.0</cell><cell>7</cell><cell>1.0</cell><cell>7</cell></row><row><cell>5</cell><cell>0.9</cell><cell>9</cell><cell>0.8</cell><cell>9</cell><cell>0.8</cell><cell>8</cell></row><row><cell>6</cell><cell>0.9</cell><cell>3</cell><cell>0.9</cell><cell>4</cell><cell>0.9</cell><cell>3</cell></row><row><cell>7</cell><cell>0.9</cell><cell>3</cell><cell>0.9</cell><cell>3</cell><cell>0.9</cell><cell>4</cell></row><row><cell>8</cell><cell>0.9</cell><cell>4</cell><cell>0.9</cell><cell>4</cell><cell>0.9</cell><cell>3</cell></row><row><cell>9</cell><cell>0.9</cell><cell>6</cell><cell>0.9</cell><cell>6</cell><cell>0.9</cell><cell>6</cell></row><row><cell>10</cell><cell>1.0</cell><cell>3</cell><cell>1.0</cell><cell>3</cell><cell>1.0</cell><cell>3</cell></row><row><cell>11</cell><cell>0.9</cell><cell>5</cell><cell>1.0</cell><cell>5</cell><cell>1.0</cell><cell>5</cell></row><row><cell>12</cell><cell>0.9</cell><cell>5</cell><cell>0.9</cell><cell>4</cell><cell>0.9</cell><cell>3</cell></row><row><cell>13</cell><cell>1.0</cell><cell>8</cell><cell>1.0</cell><cell>8</cell><cell>1.0</cell><cell>8</cell></row><row><cell>14</cell><cell>1.0</cell><cell>4</cell><cell>1.0</cell><cell>5</cell><cell>1.0</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Accuracy of LID3 based on different discretization methods and three other well-known machine learning algorithms</figDesc><table><row><cell cols="2">No. C4.5</cell><cell>N.B.</cell><cell>N.N.</cell><cell>LID3-U</cell><cell>LID3-E</cell><cell>LID3-P</cell></row><row><cell>1</cell><cell cols="6">79.20 ± 1.53 89.46 ± 2.09 90.38 ± 1.18 83.80 ± 1.19 83.07 ± 3.22 86.23 ± 0.97</cell></row><row><cell>2</cell><cell cols="6">69.16 ± 4.14 71.26 ± 2.96 66.50 ± 3.48 73.06 ± 3.05 73.47 ± 2.66 73.06 ± 3.05</cell></row><row><cell>3</cell><cell cols="6">94.38 ± 1.42 96.28 ± 0.73 94.96 ± 0.80 96.43 ± 0.70 96.11 ± 0.78 96.11 ± 0.89</cell></row><row><cell>4</cell><cell cols="6">78.99 ± 2.23 85.36 ± 2.42 82.62 ± 3.18 85.41 ± 1.94 86.53 ± 1.28 85.59 ± 2.19</cell></row><row><cell>5</cell><cell cols="6">64.77 ± 5.10 45.99 ± 7.00 64.30 ± 3.38 65.96 ± 2.31 65.60 ± 2.57 65.87 ± 2.32</cell></row><row><cell>6</cell><cell cols="6">75.50 ± 3.79 84.24 ± 2.09 79.93 ± 3.99 76.71 ± 3.81 78.09 ± 3.58 77.96 ± 2.88</cell></row><row><cell>7</cell><cell cols="6">75.78 ± 3.16 84.00 ± 1.68 78.89 ± 3.05 76.52 ± 3.63 78.07 ± 3.63 79.04 ± 2.94</cell></row><row><cell>8</cell><cell cols="6">76.75 ± 4.68 83.25 ± 3.99 81.69 ± 2.48 82.95 ± 2.42 83.08 ± 2.82 83.08 ± 1.32</cell></row><row><cell>9</cell><cell cols="6">89.60 ± 2.13 82.97 ± 2.51 87.77 ± 2.88 88.98 ± 2.23 89.11 ± 2.30 88.01 ± 1.83</cell></row><row><cell>10</cell><cell cols="6">93.47 ± 3.23 94.53 ± 2.63 95.87 ± 2.70 96.00 ± 1.26 96.13 ± 1.60 96.40 ± 1.89</cell></row><row><cell>11</cell><cell cols="6">65.23 ± 3.86 55.41 ± 5.39 66.74 ± 4.89 58.73 ± 1.99 64.62 ± 2.80 69.25 ± 2.84</cell></row><row><cell>12</cell><cell cols="6">72.16 ± 2.80 75.05 ± 2.37 74.64 ± 1.41 76.22 ± 1.81 76.22 ± 1.85 76.54 ± 1.34</cell></row><row><cell>13</cell><cell cols="6">70.48 ± 0.00 70.19 ± 0.00 81.05 ± 0.00 86.54 ± 0.00 87.50 ± 0.00 89.42 ± 0.00</cell></row><row><cell>14</cell><cell cols="6">88.09 ± 4.14 96.29 ± 2.12 96.85 ± 1.57 95.33 ± 1.80 95.78 ± 1.80 95.89 ± 1.96</cell></row><row><cell cols="2">Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Summary of comparisons of LID3 based on different discretization methods with three other well-</cell></row><row><cell cols="3">known machine learning algorithms</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LID3-Uniform vs.</cell><cell></cell><cell>LID3-Entropy vs.</cell><cell cols="2">LID3-Percentile vs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>Comparisons of accuracy Acc and the number of leaves (rules) N R with different merging thresholds T m across a set of UCI datasets No.</figDesc><table><row><cell></cell><cell>LF 5</cell><cell></cell></row><row><cell>{l}</cell><cell>LF 5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>LF 5</cell></row><row><cell></cell><cell>LF 4</cell><cell></cell></row><row><cell>{m,l}</cell><cell>LF 4</cell><cell></cell></row><row><cell>{m}</cell><cell>LF 3</cell><cell>{LF 2, LF 3, LF 4 }</cell></row><row><cell>{s, m }</cell><cell>{LF 2, LF 3 }</cell><cell></cell></row><row><cell></cell><cell>LF 2</cell><cell>LF 1</cell></row><row><cell>{s}</cell><cell>LF 1</cell><cell></cell></row><row><cell></cell><cell>LF 1</cell><cell></cell></row><row><cell></cell><cell>Fig. 7. Illustration of branch merging.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1 ; MB 2 ; MB 3 ; MB 4 ; MB 5 ; MB 6 ; MB 7 ; MB 8 g ¼ fhhfs 3 gi; 1:0000; 0:0000; 0:0000i hhffs 3 ; m 3 g; fm 3 gg; fs 4 gi; 1:0000; 0:0000; 0:0000i hhffs 3 ; m 3 g; fm 3 gg; ffs 4 ; m 4 g; fm 4 ggi; 0:0008; 0:9992; 0:0000i hhffs 3 ; m 3 g; fm 3 gg; fm 4 ; l 4 gi; 0:0000; 0:5106; 0:494i hhffs 3 ; m 3 g; fm 3 gg; fl 4 gi; 0:0000; 0:0556; 0:9444i hhffm 3 ; l 3 g; fl 3 gg; fs 4 gi; 0:3333; 0:3333; 0:3333i hhffm 3 ; l 3 g; fl 3 gg; ffs 4 ; m 4 g; fm 4 ggi; 0:000; 0:8423; 0:1577i hhffm 3 ; l 3 g; fl 3 gg; ffm 4 ; l 4 g; fl 4 gi; 0:000; 0:0913; 0:9087ig</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>fsmall 2 g : 0:3=ð0:3 þ 0:2Þ; fsmall 2 ; large 2 g : 0:2=0:2ð0:2 þ 0:3Þ ¼ fsmall 2 g : 0:6; fsmall 2 ; large 2 g : 0:4This givesPrðB 1 j hÞ ¼ m h 1 ðfsmall 1 gÞ Â m h 2 ðfsmall 2 gÞ ¼ 1 Â 0:6 ¼ 0:6</figDesc><table /><note><p>PrðB 2 j hÞ ¼ m h 1 ðfsmall 1 gÞ Â m h 2 ðfsmall 2 ; large 2 gÞ ¼ 1 Â 0:4 ¼ 0:4 PrðB 3 j hÞ ¼ PrðB 4 j hÞ ¼ PrðB 5 j hÞ ¼ PrðB 6 j hÞ ¼ PrðB 7 j hÞ ¼ 0 Hence, according to JeffreyÕs rule PrðC</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc>Classification accuracy based on crisp data and fuzzy data on the ÔeightÕ problem</figDesc><table><row><cell></cell><cell>N F = 3</cell><cell>N F = 4</cell><cell>N F = 5</cell><cell>N F = 6</cell><cell>N F = 7</cell></row><row><cell>Crisp data</cell><cell>87.72%</cell><cell>94.17%</cell><cell>95.94%</cell><cell>97.29%</cell><cell>98.54%</cell></row><row><cell>Fuzzy data</cell><cell>79.29%</cell><cell>85.85%</cell><cell>89.39%</cell><cell>94.17%</cell><cell>95.01%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Z. Qin, J. Lawry / Information Sciences 172 (2005) 91-129</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Z. Qin, J. Lawry / Information Sciences 172 (2005) 91-129</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>Some data pre-processing techniques replace the missing values with the mean of this attribute within a numerical attribute or categorize the missing values as a new value within a nominal attribute.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>The focal elements at the two extreme sides are still trapezoidal and not triangular, for example see Fig.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>In classical decision tree learning, pruning can reduce overfitting so that the pruned trees have better generalization and perform better then unpruned trees. However, this is not the case for PETs<ref type="bibr" target="#b14">[15]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>WEKA<ref type="bibr" target="#b18">[19]</ref> is used to generate the results of J48 (C4.5 in WEKA) unpruned tree, Naive Bayes and Neural Networks with default parameter settings.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Pilsworth</surname></persName>
		</author>
		<title level="m">Fril-fuzzy and evidential reasoning in artificial intelligence</title>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mass assignment fuzzy ID3 with applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Unicom Workshop on Fuzzy Logic: Applications and Future Directions</title>
		<meeting>the Unicom Workshop on Fuzzy Logic: Applications and Future Directions<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="278" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="&lt;http://www.ics.uci.edu/~mlearn/MLRepository.html&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-interval discretization of continuous-valued attributes for classification learning</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Chambery France</publisher>
			<date type="published" when="1993-09">August-September 1993</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fuzzy sets as equivalence classes of random sets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuzzy Sets and Possibility Theory</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Yager</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="327" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzy decision trees: issues and methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Janikow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The logic of decision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Gordon &amp; Breach</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A framework for linguistic modelling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Query evaluation from linguistic prototypes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 10Õth IEEE International Conference on Fuzzy Systems</title>
		<meeting>10Õth IEEE International Conference on Fuzzy Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A voting mechanism for fuzzy logic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="315" to="333" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Symbolic and Quantitative Approaches to Reasoning with UncertaintyLecture Notes in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="374" to="384" />
		</imprint>
	</monogr>
	<note>Label Semantics: a formal framework for modelling with words</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A complete fuzzy decision tree technique</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="221" to="254" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Soft discretization to enhance the continuous decision trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/ PKDD Workshop: IDDM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tree induction for probability-based ranking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="199" to="215" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Linguistic modelling using a semi-Naive Bayes framework, IPMU-2002</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Randon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Annecy, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<ptr target="&lt;http://www.cs.wai-kato.ac.nz/~ml/weka/&gt;" />
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques with JavaMorgan Kaufmann</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fuzzy logic = computing with words</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
