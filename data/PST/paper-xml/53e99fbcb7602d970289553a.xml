<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Resource Management for Cloud Computing Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Younge</surname></persName>
							<email>ajyounge@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gregor</forename><surname>Von Laszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizhe</forename><surname>Wang</surname></persName>
							<email>wanglize@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Pervasive Technology Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sonia</forename><surname>Lopez-Alarcon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Rochester Institute of Technology Rochester</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>NY, IN</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Warren</forename><surname>Carithers</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Rochester Institute of Technology Rochester</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<region>NY, IN</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Resource Management for Cloud Computing Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD94058324E201B44404C38150FB2DAE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud Computing</term>
					<term>Green Computing</term>
					<term>Virtual ization</term>
					<term>Scheduling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The notion of Cloud computing has not only re shaped the field of distributed systems but also fundamentally changed how businesses utilize computing today. While Cloud computing provides many advanced features, it still has some shortcomings such as the relatively high operating cost for both public and private Clouds. The area of Green computing is also becoming increasingly important in a world with limited energy resources and an ever-rising demand for more computational power. In this paper a new framework is presented that provides efficient green enhancements within a scalable Cloud computing architecture. Using power-aware scheduling techniques, variable resource management, live migration, and a minimal virtual ma chine design, overall system efficiency will be vastly improved in a data center based Cloud with minimal performance overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>For years visionaries in computer science have predicted the advent of utility-based computing will reign champion. This concept dates back to John McCarthy's vision stated at the MIT centennial celebrations in 1961.</p><p>"If computers of the kind I have advocated become the computers of the future, then computing may someday be organized as a public utility just as the telephone system is a public utility ... The computer utility could become the basis of a new and impor tant industry."</p><p>Only recently has the hardware and software been available to support the concept of utility computing on a large scale.</p><p>The concepts inspired by the notion of utility computing have recently combined with the requirements and standards of Web 2.0 [1] to create Cloud computing <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Cloud computing is defined as, "A large-scale distributed computing paradigm that is driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are deliv ered on demand to external customers over the Internet."</p><p>As new distributed computing technologies like Clouds become increasingly popular, the dependence on power also increases. Currently it is estimated that data centers consume 0.5 percent of the world's total electricity usage <ref type="bibr" target="#b3">[4]</ref> and if current demand continues, is projected to quadruple by 2020.</p><p>978-1-4244-7614-5/10/$26.00 ©20 1 0 IEEE Email: slaeec@rit.edu, wrc@cs.rit.edu</p><p>In 2005, the total energy consumption for servers and their cooling units was projected at 1.2% the total U.S. energy consumption and doubling every 5 years <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The majority of the energy used in today's society is generated from fossil fuels which produce harmful CO2 emissions. Therefore, it is imperative to enhance the efficiency and potential sustainabil ity of large data centers.</p><p>One of the fundamental aspects of virtualization technolo gies employed in Cloud environments is resource consoli dation and management. Using hypervisors within a cluster environment allows for a number of standalone physical machines to be consolidated to a virtualized environment, thereby requiring less physical resources than ever before. While this improves the situation, it often is inadequate. Large Cloud deployments require thousands of physical machines and megawatts of power. Therefore, there is a need to create an efficient Cloud computing system that utilizes the strengths of the Cloud while minimising its energy footprint.</p><p>In order to correctly and completely unify a Green aspect to the next generation of Distributed Systems, a set of guidelines needs to persist. These guidelines much represent a path of sustainable development that can be integrated into data center construction and management as a whole. While the frame work provided in this paper represents many promising ways to reduce power consumption, true sustainable development also depends on finding a renewable and reliable energy source for the data center itself. When combined, many of today's limits in the size of data centers will begin to deteriorate. This paper is organized as follows. Section II investigates previous research in minimizing power consumption with emphasis on Clouds. Section III presents a novel Green Cloud framework and its components, while Section IV and V detail certain components of the framework. In Section VI we assemble the components discussed and evaluate the potential energy savings. Finally, we conclude and provide insight for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH</head><p>In order to accurately depict the research presented in this article, the topics of Cloud computing, Grid computing, Clusters and Green computing will be reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Clouds</head><p>Cloud computing is becoming one of the most explosively expanding technologies in the computing industry today. It enables users to migrate their data and computation to a remote location with minimal impact on system performance <ref type="bibr" target="#b6">[7]</ref>. Ty pically this provides a number of benefits which could not otherwise be realized. These benefits include:</p><p>• Scalable -Clouds are designed to deliver as much com puting power as any user wants. While in practice the un derlying infrastructure is not infinite, the cloud resources are projected to ease the developer's dependence on any specific hardware.</p><p>• Quality of Service (QoS) -Unlike standard data cen ters and advanced computing resources, a well designed Cloud can project a much higher QoS than typically possible. This is due to the lack of dependence on specific hardware, so any physical machine failures can be mitigated without the user's knowledge.</p><p>• Specialized Environment -Within a Cloud, the user can utilize custom tools and services to meet their needs. This can be to use the latest library, toolkit, or to support legacy code within new infrastructure.</p><p>• Cost Effective -Users finds only the hardware required for each project. This greatly reduces the risk for institutions who may be looking to build a scalable system. Thus providing greater flexibility since the user is only paying for needed infrastructure while maintaining the option to increase services as needed in the future.</p><p>• Simplified Interface -Whether using a specific applica tion, a set of tools or Web services, Clouds provide access to a potentially vast amount of computing resources in an easy and user-centric way. We have investigated such an interface within Grid systems through the use of the Cyberaide project <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>There are a number of underlying technologies, services, and infrastructure-level configurations that make Cloud com puting possible. One of the most important technologies is the use of virtualization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr">[11]</ref>. Virtualization is a way to abstract the hardware and system resources from a op erating system. This is typically performed within a Cloud environment across a large set of servers using a Hypervisor or Virtual Machine Monitor (VMM) which lies in between the hardware and the Operating System (OS). From here, one or more virtualized OSs can be started concurrently as seen in Figure <ref type="figure" target="#fig_7">1</ref>, leading to one of the key advantages of Cloud computing. This, along with the advent of multi-core processing capabilities, allows for a consolidation of resources within any data center. It is the Cloud's job to exploit this capability to its maximum potential while still maintaining a given QoS.</p><p>Virtualization is not specific to Cloud computing. IBM orig inally pioneered the concept in the 1960's with the M44/44X systems. It has only recently been reintroduced for general use on x86 platforms. Today there are a number of Clouds that offer Infrastructure as a Service (laaS). The Amazon Elastic Compute Cloud (EC2) <ref type="bibr" target="#b11">[12]</ref>, is probably the most popular of which and is used extensively in the IT industry. Eucalyptus <ref type="bibr" target="#b12">[13]</ref> is becoming popular in both the scientific and industry communities. It provides the same interface as EC2 and allows users to build an EC2-like cloud using their own internal resources. Other scientific Cloud specific projects exist such as OpenNebula <ref type="bibr" target="#b13">[14]</ref>, In-VIGO [15], and Cluster-on-Demand <ref type="bibr" target="#b15">[16]</ref>. They provide their own interpretation of private Cloud services within a data center. Using a Cloud deployment overlaid on a Grid computing system has been explored by the Nimbus project <ref type="bibr" target="#b16">[17]</ref> with the Globus To olkit <ref type="bibr" target="#b17">[18]</ref>. All of these clouds leverage the power of virtualization (typically using the Xen hypervisor) to create an enhanced data center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Green Computing</head><p>The past few years has seen an increase in research on devel oping efficient large computational resources. Supercomputer performance has doubled more than 3000 times in the past 15 to 20 years, the performance per watt has increased 300 fold and performance per square foot has only doubled 65 times <ref type="bibr" target="#b18">[19]</ref> in the same period of time. This lag in Moore's Law over such an extended period of time in computing history has created the need for more efficient management and consolidation of data centers. This can be seen in figure 2 <ref type="bibr" target="#b19">[20]</ref>.</p><p>Much of the recent work in Green computing focuses on Supercomputers and Cluster systems. Currently the fastest Supercomputer in the world is the IBM Roadrunner at Los Alamos National Laboratory <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr">[22]</ref>, which was funda mentally designed for power efficiency. However, Roadrunner consumes several Megawatts of power <ref type="bibr" target="#b22">[23]</ref> (not including cooling) and costs millions of dollars to operate every year. The second fastest Supercomputer is Jaguar at Oak Ridge National Laboratory. While Jaguar too has a number of power saving features developed by Sandia, Oak Ridge and Cray <ref type="bibr" target="#b23">[24]</ref> such as advanced power metering at the CPU level, 480 volt power supplies, and an advanced cooling system developed by Cray, the system as a whole still consumes almost 7 Megawatts of power.</p><formula xml:id="formula_0">8000% 7000% 6000% 5000% 3000% PERFORMANCE 2000% PE Ff WA T T 1000% O % .. ������ ________ __ N o o N C") o o N � o o N U"J o o N &lt;0 o o N r o o N Fig. 2.</formula><p>Performance increases much faster than performance per watt of energy consumed <ref type="bibr" target="#b19">[20]</ref> One technique being explored is the use of Dynamic Voltage and Frequency Scaling (DVFS) within Clusters and Supercomputers <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. By using DVFS one can lower the operating frequency and voltage, which results in de creased power consumption of a given computing resource considerably. This technique was originally used in portable and laptop systems to conserve battery power, and has since migrated to the latest server chipsets. Current technologies exist within the CPU market such as Intel's SpeedStep and AMD's PowerNow! technologies. These dynamically raise and lower both frequency and CPU voltage using ACPI P-states <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, DVFS techniques are used to scale down the frequency by 400Mhz while sustaining only a 5% performance loss.  <ref type="figure">-+---+---+--f---+---+--+--+---+--+</ref>  A power-aware Cluster supports multiple power and per formance modes on processors with frequencies that can be turned up or down. This allows for the creation of an efficient scheduling system that minimizes power consumption of a system while attempting to maximise performance. The scheduler performs the energy-performance trade-off within a cluster. Combining various power efficiency techniques for data centers with the advanced feature set of Clouds could yield drastic results, however currently no such system exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GREEN CLOUD FRAMEW ORK</head><p>There is a pressing need for an efficient yet scalable Cloud computing system. This is driven by the ever-increasing demand for greater computational power countered by the continual rise in use expenditures, both economical and en vironmental. Both business and institutions will be required to meet these needs in a rapidly changing environment.</p><p>We present a novel Green computing framework that is applied to the Cloud in order to meet the goal of reducing power consumption. This framework is meant to define effi cient computing resource management and Green computing technologies can be adapted and applied to Cloud systems.  Within the framework, there are two major areas which can lead to improvements. First, we can expand upon the baseline functioning of virtual machines in a cloud environment. This is first done with deriving a more efficient scheduling system for VMs. The Scheduling section addresses the placement of VMs within the Cloud infrastructure while minimizing the operating costs of the Cloud itself. This is typically achieved by optimising either power of the server equipment itself or the overall temperature within the data center. Due to the inherent disposability and mobility of VMs within a semi homogeneous data center, we can leverage the ability to move and manage the VMs to further improve efficiency. The image management section attempts to control and manipulate the size and placement of VM images in various ways to conserve power and remove unnecessary bloat. Furthermore, the design of the virtual machine images can also lead to a drastic power savings.</p><p>While these operational and runtime chances can have a drastic impact, however more static data center level design decisions should also be included. Using more efficient Air Conditioning units, employing exterior "free" cooling, using completely separated hot and cold isles, or simply picking more efficient power supplies for the servers can lead to incremental but substantial improvements. While this may be outside the scope of this paper, the integrated components of the Green Cloud framework in Figure <ref type="figure" target="#fig_3">4</ref> provide a sustainable development platform which has the largest potential impact factor to drastically reduce power requirements within a Cloud data center. Although the potential is great, combining each factor together in such a unified framework and deploying it to a large scale Cloud poses many challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VIRTUAL MACHINE SCHEDULING &amp; MANAGEMENT</head><p>While Supercomputer and Cluster scheduling algorithms are designed to schedule individual jobs and not virtual machines, some of the concepts can be translated to the Cloud. We have already conducted such research in <ref type="bibr" target="#b28">[29]</ref>. In many service oriented scientific Cloud architectures, new VMs are created to perform some work. The idea is similar to sand boxing work within a specialized environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Power-aware VM Scheduling</head><p>Currently, there are two competing types of Green schedul ing systems for Supercomputers; power-aware and thermal aware scheduling. In thermal-aware scheduling <ref type="bibr" target="#b29">[30]</ref>, jobs are scheduled in a manner that minimizes the overall data center temperature. The goal is not always to conserve the energy used to the servers, but instead to reduce the energy needed to operate the data center cooling systems. In power-aware scheduling <ref type="bibr" target="#b25">[26]</ref>, jobs are scheduled to nodes in such a way to minimize the server's total power. The largest operating cost incurred in a Cloud data center is in operating the servers. As such, we concentrate on power-aware scheduling in this paper.  Figure <ref type="figure" target="#fig_6">5</ref> illustrates the motivation behind power-aware VM scheduling. This graphic documents our recent research find ings regarding watts of energy consumed verses the number of processing cores in use. The power consumption curve illustrates that as the number of processing cores increases, the amount of energy used does not increase proportionally. In fact that change in power consumption decreases. When using only one processing core, the change in power consumption incurred by using another processing core is over 20 watts. The change from 7 processing cores to all 8 processing cores results in an increase of only 3.5 watts.</p><p>The impact of this finding is substantial. In a normal round robin VM scheduling system like the one in Eucalyptus, the load of VMs is distributed evenly to all servers within the data center. While this may be a fair scheduler, in practice it is very inefficient. The result is that each time the scheduler distributes VMs to a processor, the power consumption increases by its greatest potential. In contrast, this research demonstrates that if the scheduler distributes the VMs with the intent to fully utilize all processing cores within each node, the power consumption is decreased dramatically. Therefore, there is a large need for an advanced scheduling algorithm which incorporates the findings in Figure <ref type="figure" target="#fig_6">5</ref>. To meet this need we propose Algorithm 1, a new greedy-based algorithm to minimise power consumption. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>END IF END IF END FOR END FOR wait for interval t END WHILE</head><p>Algorithm 1 is a VM scheduling algorithm that minimizes power consumption within the data center. This task is accom plished by continually loading each node with as many VMs as possible. In Algorithm 1 the pool acts as a collection of nodes remain static after initialization. While not in the algorithm, the pool can be initialized by a priority based evaluations system to either maximise performance or further minimise power consumption. At a specified interval t the algorithm runs through each VM in the queue waiting to be scheduled. The first node in the priority pool is selected and evaluated to see if it has enough virtual cores and capacity available for the new VM. If it does, it is scheduled, the pei is decremented by one, and this processes is continued until the VM queue is empty. When a VM finishes its execution and terminates, it reports it back the scheduler and pei is increased by one to signify a core of machine i is freed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VM Management</head><p>Another key aspect of a Green Cloud framework is vir tual machine image management. By using virtualization technologies within the Cloud, a number of new techniques become possible. Idle physical machines in a Cloud can be dynamically shutdown and restarted to conserve energy during low load situations. A similar concept was achieved in Grid systems though the use of Condor Glide-In <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> add on to Condor, which dynamically adds and removes machines form the resource pool. This concept of shutting down unused machines will have no effect on power consumption during peak load as all machines will be running. However in practice Clouds almost never run at full capacity as this could result in a degradation of the QoS. Therefore by design, fast dynamic shutdown and startup of physical machines could have a drastic impact on power consumption, depending on the load of the Cloud at any given point in time.</p><p>The use of live migration features within Cloud systems <ref type="bibr" target="#b32">[33]</ref> is a recent concept. Live migration is presently used for proactive fault tolerance by seamlessly moving VMs away from failing hardware to stable hardware without the user noticing a change <ref type="bibr" target="#b33">[34]</ref> in a virtualized environment. Live migration can be applied to Green computing in order to migrate away machines. VMs can be shifted from low load to medium load servers when needed. Low load servers are subsequently shutdown when all VMs have migrated away, thus conserving the energy required to run the low load idle servers. When using live migration, the user is completely unaware of a change and there is only a 60 to 300ms delay, which is acceptable by most standards. This process of dynamically allocating and deallocating physical machines is complimentary to our scheduling system outlines in Algorithm 1. As the scheduling algorithm executes, it will leave a number of machines idling, potentially for long periods of time, creating an optimal state for the VM manage ment system. The machines that are left idle can shut down as illustrated in Figure <ref type="figure">6</ref>. When load increases, we use Wake on LAN (WOL) to start them back up. This control can be easily monitored and implemented as a daemon running on the Cloud head node or scheduler. This effectively displays the goal of the Green Cloud Framework: while any single power saving technique can be beneficial, the calculated combination of multiple techniques from a systems-level perspective can yield significant power savings when compared to their individual implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SERVICE ORIENTED VIRTUAL MACHINE IMAGE</head><p>While scheduling and management of virtual machines within a private Cloud environment is important, one must realize what is actually being scheduled. In a normal Cloud environment like the Amazon's EC2 <ref type="bibr" target="#b11">[12]</ref>, full Operating System VMs are scheduled, often to carry out specific tasks in mass. These VM instances contain much more than they need to in order to support a wide variety of hardware software and varying user tasks. While this is ideal for a desktop based environment, it leads to wasted time and energy in a server based solution. A hypervisor provides the same virtualized hardware to each VM and each VM is typically designed for a specific task. In essence, we want the OS within the VM to act only as a light wrapper which supports a few specific but refined tasks or services, and not an entire desktop/application suite. In order to accomplish this task, we need to concentrate on two areas; VM image size and boot time.</p><p>Normal x86 hardware can vary widely, so most modern operating systems including Linux are able to detect various hardware and load modules on the fly upon startup. This is not an issue with a virtual machine environment since the hardware is standardized and known in advance. It is common for the boot to spend 15 seconds running modprobe to load only a single module. The modules in the system and many of the time consuming probing functions can be reduced upon bootup within a VM environment. In <ref type="bibr" target="#b34">[35]</ref> considerable amount of time is saved by changing the IDE delay times for probing new hardware.</p><p>Another technique for reducing the boot time is to orches trate the boot sequence in a more efficient way. Often many daemons and applications are loaded for general use and in the case of a lightweight VM instance, aren't needed and can be removed. This includes standalone server applications like Window managers and the X 11 windowing system. This would also remove the system's disk footprint considerably to save valuable hard drive space in distributed file systems as well as network traffic when migrating the machines.</p><p>Boot time can be further improved by creating a new order which maximises both the CPU utilization and 110 throughput. The use of bootchart <ref type="bibr" target="#b35">[36]</ref> can profile where bootup system inefficiencies occur and to allow for optimization of the boot sequence. Another useful tool is readahead <ref type="bibr" target="#b36">[37]</ref>. Readahead profiles the system startup sequence and uses file pre-fetching techniques to to load files into memory before they are requested. Therefore an application reads directly from system memory and does not have to wait for disk seek-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. POWER CONSUMPTION ANALY SIS</head><p>In order to validate our framework it is important to inves tigate its feasibility within an actual virtual machine cluster environment. This section discusses the implementation of our scheduling algorithm as it is applied to the OpenNebula project in a multi-core cluster and evaluates the creation of a new lightweight VM image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scheduling Analysis</head><p>OpenNebula <ref type="bibr" target="#b13">[14]</ref> is an open source distributed virtual ma chine manager for dynamic allocation of virtual machines in a resource pool. The OpenNebula core components accept user requirements via the OpenNebula interface, and then place virtual machines in compute nodes within the cluster.</p><p>The OpenNebula scheduler is an independent component that provides policies for virtual machine placement. We choose the OpenNebula project because of this compartmental ized design as it allows for integration of our custom schedul ing algorithm. The default scheduler provides a scheduling policy based on rank, which allocates compute resources for virtual machines. Scheduling algorithm 1 is implemented by modifying the OpenNebula scheduler to reflect the desired results in Section IV. To evaluate the energy savings of Algorithm 1, we consider the following small OpenNebula pool of just 4 servers. Each server within the pool is a 2.6Ghz Intel Core i7 920 with 12GB of RAM. We assume each server can hold 8 VMs as it has 8 virtual cores. At idle, they consume 105 Watts of power and under 100% load they consume 170 Watts (see Figure <ref type="figure" target="#fig_6">5</ref>). If we execute the default OpenNebula scheduler to schedule 8 virtual machines each running CPU-bound tasks, each server would gain 2 VMs and would consume 138 Watts with a total pool power consumption of 552 Watts. However when Algorithm 1 is used, all the VMs are scheduled to the first machine in the pool. This one machine operates at the full 170 Watts, however all other machines idle at 105 Watts, resulting in a pool power consumption of 485 Watts. Therefore, using our power based scheduling algorithm, we conserve 12% of the system's power on only 4 machines on a normal load, as seen in Figure <ref type="figure" target="#fig_9">7</ref>. While this experiment is only with 4 nodes, the results are clearly scalable to a large cluster and will be magnified as higher node core counts are deployed. If the live migration and shutdown strategy is also deployed, some servers could be dynamically shutdown to further conserve energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VM Image Analysis</head><p>In order to evaluate the performance of our VM image design, we must create a prototype. There are two paths available to build such a VM OS image. The first is a bottom up approach where a basic Linux kernel is built upon to reach the minimal feature set needed. This requires an entirely new distribution from scratch. While this may be the "cleanest" way, it would require a large development team and is therefore infeasible for this project. The other option involves a top-down approach of taking a common distribution and removing certain components from it, making for a lighter and faster sub-distribution. This route is more practical as it does not require reinventing the wheel and the option to keep components such as a package management system and a large distribution library are maintained.</p><p>Following the second approach, a custom Linux image was created to illustrate the possibility of a fast and lightweight VM OS. Starting with Ubuntu Linux version 9.04 Jaunty, all unnecessary packages were removed, including the Gnome window manager and XII. By removing these multitude of packages, the system image is reduced from 4Gb to only 636Mb. This minimization speeds up migration of the image from one server to another as there is less network traffic during the movement phase. A number of other packages, libraries and boot level daemons were also removed from the startup process. At the final stage, the image is a minimal Linux installation with only the bare necessity components. One thing that was left was the Synaptic package management system, so if any tools or libraries are needed it is a trivial process to have them installed on the system. While the package management system does take up some room, it is well worth the added extendability it provides to the system. A number of kernel modules were also removed form the 2.6.28-11 kernel to speed up the kernel init and mod probe processes as much as possible.</p><p>To test the speed of the custom image, both it and a basic Ubuntu 9.04 installation were moved to a VMWare server with 2.5Ghz Intel Core 2 Duo and 4GB of ram. The standard Ubuntu image booted from BIOS in 38 seconds. With our custom VM image, boot time was reduced dramatically to just 8 seconds. By comparing the boot charts in figures 8 and 9, we can see there is a drastic change in boot time, resulting in the boot time decreased by 30 seconds. Instead of a large amount of VO blocking, all disk I/O is done at once towards the beginning, allowing for much higher utilization of the CPU. While a boot time of 8 seconds is a considerable improvement, we can do better. The kernel still takes a full 2 seconds to load, however with some improvements a second or more could possibly be saved.  Consider a VM which is started on a machine that requires 250 watts of power. Spending 30 seconds on booting up the VM results in 2.08 wh or .002 Kwh of energy used. While this savings of .002Kwh or 30 seconds doesn't seem like much, its effects are actually quite significant. In just a small private Cloud system, it is common for 100 VMs to be created every hour, depending on system load and utilization. As such, there is over 1750 Kwh wasted per year. Thus these changes in the VM image lead to hundreds or thousands of dollars in savings. Furthermore, the power savings realized of using lightweight VM images on a 10 Megawatt facility where thousands of VMs are started every minute equate to tens or hundreds of thousands of dollars a year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>As the prevalence of Cloud computing continues to rise, the need for power saving mechanisms within the Cloud also increases. In this paper we have presented a novel Green Cloud framework for improving system efficiency in a data center. To demonstrate the potential of our framework, we have presented new energy efficient scheduling, VM system image, and image management components that explore new ways to conserve power. Though our research presented in this paper, we have found new ways to save vast amounts of energy while minimally impacting performance.</p><p>Not only do the components discussed in this paper com pliment each other, they leave space for future work. Future opportunities could explore a scheduling system that is both power-aware and thermal-aware to maximise energy savings both from physical servers and the cooling systems used. Such a scheduler would also drive the need for better data center de signs, both in server placements within racks and closed-loop cooling systems integrated into each rack. While a number of the Cloud techniques are discussed in this paper, there is a growing need for improvements in Cloud infrastructure, both in the academic and commercial sectors. We believe Green computing will be one of the fundamental components of the next generation of Cloud computing technologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Virtual Machine Abstraction</figDesc><graphic coords="2,337.93,55.68,198.72,166.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 . 0</head><label>0</label><figDesc>DOKPiiiiQi mAii«:im =======�;::; ;;;i i � =",,� 1.00 a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Possible energy to performance trade-off . Here you can see a 18% reduction in frequency contributes to only a 5% performance loss.<ref type="bibr" target="#b27">[28]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Green Cloud Framework. Shaded items represent topics discussed in this paper.</figDesc><graphic coords="3,312.96,211.20,250.56,150.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure 4 illustrates a comprehensive Green Cloud frame work for maximising performance per watt within a Cloud. This framework outlines the major areas are VM scheduling, VM image management, and advanced data center design.Within the framework, there are two major areas which can lead to improvements. First, we can expand upon the baseline functioning of virtual machines in a cloud environment. This is first done with deriving a more efficient scheduling system for VMs. The Scheduling section addresses the placement of VMs within the Cloud infrastructure while minimizing the operating costs of the Cloud itself. This is typically achieved by optimising either power of the server equipment itself or the overall temperature within the data center. Due to the inherent disposability and mobility of VMs within a semi homogeneous data center, we can leverage the ability to move and manage the VMs to further improve efficiency. The image management section attempts to control and manipulate the size and placement of VM images in various ways to conserve power and remove unnecessary bloat. Furthermore, the design of the virtual machine images can also lead to a drastic power savings.While these operational and runtime chances can have a drastic impact, however more static data center level design decisions should also be included. Using more efficient Air Conditioning units, employing exterior "free" cooling, using completely separated hot and cold isles, or simply picking more efficient power supplies for the servers can lead to</figDesc><graphic coords="3,52.81,460.79,252.48,110.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Power consumption curve of an Intel Core i7 920 CPU</figDesc><graphic coords="4,47.05,420.47,252.48,188.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1</head><label>1</label><figDesc>Power based scheduling of VMs FOR i = 1 TO i :::; Ipooll DO pei = num cores in paoli END FOR WHILE (true) FOR i = 1 TO i :::; Iqueuel DO vm = queuei FOR j = 1 TO j :::; Ipooll DO IF pej 2: 1 THEN IF check capacity vm on pej THEN schedule vm on pej pej -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Node 2 14 Fig. 6 .</head><label>2146</label><figDesc>Fig. 6. Virtual Machine management dynamic shutdown technique</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of Scheduling power savings</figDesc><graphic coords="6,49.92,460.79,250.56,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Bootup chart of the default Ubuntu Linux VM image</figDesc><graphic coords="7,131.53,210.24,81.60,53.76" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Web 2.0: A New Wave of Innovation for Teaching and Learning?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Market-oriented cloud computing: Vision, hype, and reality for delivering it services as computing utilities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on High Performance Computing and Communications</title>
		<meeting>the 10th IEEE International Conference on High Performance Computing and Communications<address><addrLine>Los Alamitos, CA. USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE CS Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cloud Computing and Grid Computing 360-Degree Compared</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grid Computing Environments Workshop, 200B. GCE&apos;OB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to cut data centre carbon emissions?</title>
		<author>
			<persName><forename type="first">W</forename><surname>Forrest</surname></persName>
		</author>
		<ptr target="http://www.computerweekly.comlArticles/2008/12/05/233748/how-to-cut-data-centre-carbon-emissions.htm" />
	</analytic>
	<monogr>
		<title level="m">Website, Decem ber</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating total power consumption by servers in the US and the world</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koomey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Final report. February</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Report to congress on server and data center energy efficiency public law 109-431</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
			<publisher>Webpage</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud computing: a perspective study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing</title>
		<imprint>
			<publisher>WangLYAH</publisher>
		</imprint>
	</monogr>
	<note>to appear in 2010</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiment and Workflow Management Using Cyberaide Shell</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mahinthakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Workshop on Workflow Systems in e-Science (WSES 09) in conjunction with 9th IEEE International Symposium on Cluster Computing and the Grid</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cyberaide JavaScript: A JavaScript Commodity Grid Kit</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCEOB at SCOB</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">Nov. 16 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xen and the art of virtu alization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 19th ACM Symposium on Operating Systems Principles<address><addrLine>New York, U. S. A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding Full Virtualization, Paravirtualization, and Hardware Assis</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="http://www.vmware.comlfiles/pdfIYMware_paravirtualization.pdf" />
	</analytic>
	<monogr>
		<title level="j">VMware, Tech. Rep</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Elastic Compute Cloud</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="http://aws.amazon.comlec21" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Eucalyptus Open-source Cloud-computing System</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grzegorczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obertelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zagorodnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Cloud Computing and Its Applications</title>
		<meeting>Cloud Computing and Its Applications</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open NEbula: The Open Source Virtual Machine Manager for Cluster Computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fontan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Open Source Grid and Cluster Software Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From virtualized resources to virtual computing Grids: the In-VIGO system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adabala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krsul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matsunaga</surname></persName>
		</author>
		<author>
			<persName><surname>Tsugawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Compo Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="896" to="909" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic virtual clusters in a grid site manager</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sprenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Symposium on High Performance Distributed Computing</title>
		<imprint>
			<date type="published" when="2003">2003. Proceedings, 2003</date>
			<biblScope unit="page" from="90" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual Workspaces in the Grid</title>
		<author>
			<persName><forename type="first">K</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galron</surname></persName>
		</author>
		<ptr target="http://workspace.globus.org/papersIYW_EuroPar05.pdf" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3648</biblScope>
			<biblScope unit="page" from="421" to="431" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globus: A Metacomputing Infrastructure Toolkit</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<ptr target="ftp://ftp.globus.orglpub/globus/papers/globus.pdf" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Supercomputer Applications</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="128" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Green500 List: Encouraging Sustainable Supercomputing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chun Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Cameron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="50" to="55" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The efficiency challenge: Balancing total cost of ownership with real estate demands</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cherokee International, Tech. Rep</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entering the petaflop era: the architecture and performance of Roadrunner</title>
		<author>
			<persName><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hoisie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kerbyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sancho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACMffEEE conference on Supercomputing</title>
		<meeting>the 2008 ACMffEEE conference on Supercomputing<address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press Piscataway</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Top 500 supercomputers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meuer</surname></persName>
		</author>
		<author>
			<persName><surname>Strohmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-11">November 2008</date>
		</imprint>
	</monogr>
	<note>website</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making a case for a green500 list</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chun Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Parallel and Distributed Processing Sympo sium (IPDPS 2006)/ Workshop on High Performance -Power Aware Computing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Topics on Measuring Real Power Usage on High Performance Computing Platforms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laros</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pedretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Swan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IEEE Cluster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Feasibility Analysis of Power Aware ness in Commodity-Based High-Performance Clusters</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chun Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLUSTER</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A power-aware run-time system for high performance computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACMffEEE con ference on Supercomputing</title>
		<meeting>the 2005 ACMffEEE con ference on Supercomputing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bodas</surname></persName>
		</author>
		<ptr target="http://www.intel.comlidf/us/spr2003/presentationslS03USMODS137_OS.pdf" />
		<title level="m">Data Center Power Management and Benefits to Modular Computing,&quot; in Intel Developer Forum</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Green Supercomputing Comes of Age</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IT PROFESSIONAL</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Power-aware scheduling of virtual machines in dvfs-enabled clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Cluster</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Energy-Efficient Thermal-Aware Task Scheduling for Homogeneous High-Performance Computing Data Centers: A Cyber-Physical Approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varsamopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1458" to="1472" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GlideCNAF: A Purely Condor Glide-in Based CDF Analysis Farm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><surname>Sfiligoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CDFIDOC/COMP UPGIPUBLICI7630P</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Condor -A Hunter of Idle Workstations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">1</forename><surname>Litzkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mutka</surname></persName>
		</author>
		<ptr target="http://www.cs.wisc.edulcondor/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Distributed Computing Systems (ICDCS)</title>
		<meeting>the 8th International Conference on Distributed Computing Systems (ICDCS)<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1988-06">Jun. 1988</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Live migration of virtual machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Limpach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACMIUSENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 2nd ACMIUSENIX Symposium on Networked Systems Design and Implementation (NSDI)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proactive fault tolerance for hpc with xen virtualization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;07: Proceedings of the 21st annual international conference on Supercomputing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Methods to improve bootup time in Linux</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the Ottawa Linux Symp</title>
		<meeting>of the Ottawa Linux Symp</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bootchart</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mahkovec</surname></persName>
		</author>
		<ptr target="http://www.bootchart.orgl" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Webpage</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zak</surname></persName>
		</author>
		<ptr target="https://fedorahosted.orglreadahead/" />
		<imprint/>
	</monogr>
	<note>Readahead,&quot; Webpage. [Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
