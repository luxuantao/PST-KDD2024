<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-05">5 July 2011.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nikolaos</forename><surname>Mittas</surname></persName>
							<email>nmittas@csd.auth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lefteris</forename><surname>Angelis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-05">5 July 2011.</date>
						</imprint>
					</monogr>
					<idno type="MD5">13D3C2A72AA667EAC89ABC6C869031B2</idno>
					<idno type="DOI">10.1109/TSE.2012.45</idno>
					<note type="submission">received 21 July 2011; revised 24 Feb. 2012; accepted 26 June 2011;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cost estimation</term>
					<term>management</term>
					<term>metrics/measurement</term>
					<term>statistical methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE importance and the significant role of Software Cost Estimation (SCE) to the well-balanced management of a forthcoming project are clearly portrayed through the introduction and utilization of a large number of techniques during the past decades <ref type="bibr" target="#b0">[1]</ref>.</p><p>The rapidly increased need of large-scaled and complex software systems leads managers to settle SCE as one of the most vital activities that is closely related with the success or failure of the whole development process. Inaccurate estimates can be proved catastrophic to both developers and customers since they can cause the delay of the product deliverables or, even worse, the cancellation of a contract.</p><p>Due to the above-mentioned requirements, interest has been focused on the open research problem of the selection of the "best" estimation technique. According to an extended systematic review of studies <ref type="bibr" target="#b0">[1]</ref>, the most common research topic of SCE is the introduction and evaluation of estimation methods. On the other hand, the variety of prediction methods is also associated with contradictory and inconsistent findings regarding the superiority of one technique over others.</p><p>The most determining factor for these controversial results seems to be an inherent characteristic of prediction systems, i.e., their strong dependency on the kind of available data (types and number of project attributes and sample size) used in model fitting <ref type="bibr" target="#b1">[2]</ref>. The complexity of building an accurate model swiftly increases if we consider the alternative variations of a generic estimation method (e.g., regression analysis). In several studies researchers have based their inferences on a small number of datasets, so generalization of findings may be quite misleading.</p><p>Furthermore, there is an ongoing discussion and lack of convergence regarding the appropriateness of the error measures used for the comparison of alternative models <ref type="bibr" target="#b2">[3]</ref>. Although Mean Magnitude of Relative Error (MMRE) has been criticized as a problematic accuracy measure to select the "best" model <ref type="bibr" target="#b3">[4]</ref>, it continues to be considered as the main indicator for the performance of SCE methods.</p><p>A certain limitation of several past studies is comparison without using appropriate statistical hypothesis testing. This can lead to erroneous results and groundless generalizations regarding the predictive accuracy of estimation techniques <ref type="bibr" target="#b4">[5]</ref>. Although comparison of methods without statistical tests may lead to unsound results <ref type="bibr" target="#b5">[6]</ref>, many recent papers still base their findings solely on single indicators <ref type="bibr" target="#b6">[7]</ref>.</p><p>Another source of bias can also be the statistical procedure that is used when comparing multiple prediction techniques. In the case of a simple comparison between two competitive models, the null hypothesis is examined via a classical statistical test (i.e., paired t-test or Wilcoxon signed rank test). With more than two comparative models, the meaning of "significant difference" becomes more complicated, and the problems associated with it are known in statistics as the "multiple comparisons problems." Due to the large number of proposed cost estimation methods, it is necessary for project managers to systematically base their choice of the most accurate model on well-established statistical procedures in order to diminish the uncertainty threatening the estimation process <ref type="bibr" target="#b7">[8]</ref>. However, to the best of our knowledge, the problem of simultaneous comparisons among multiple prediction models has not been studied yet in the sense that there is no statistical procedure which can identify the significant differences between a number of cost estimation methods and at the same time be able to rank and cluster them, designating the best ones.</p><p>All of the issues discussed above lead us to conclude that there is an imperative need to investigate what the state of the art in statistics is before trying to derive conclusions and unstable results concerning the superiority of a prediction model over others for a particular dataset. The answer to this problem cannot constitute a unique solution since the notion of "best" is quite subjective. In fact, a practitioner can always rank the prediction models according to a predefined accuracy measure, but the critical issue is to identify how many of them are evidently the best, in the sense that their difference from all the others is statistically significant. Hence, the research question of finding the "best" prediction technique can be restated as a problem of identifying a subset or a group of best techniques.</p><p>The aim of the paper is therefore to propose a statistical framework for comparative SCE experiments concerning multiple prediction models. It is worth mentioning that the setup of the current study was also inspired by an analogous attempt dealing with the problem of comparing classification models in Software Defect Prediction, a research area that is also closely related to the improvement of software quality <ref type="bibr" target="#b8">[9]</ref>.</p><p>The proposed methodology is based on the analysis of a Design of Experiment (DOE) or Experimental Design, a basic statistical tool in many applied research areas such as engineering, financial, and medical sciences. In the field of SCE it has not yet been used in a systematic manner. Generally, DOE refers to the process of planning, designing, and analyzing an experiment in order to derive valid and objective conclusions effectively and efficiently by taking into account, in a balanced and systematic manner, the sources of variation <ref type="bibr" target="#b9">[10]</ref>. In the present study, DOE analysis is used to compare different cost prediction models by taking into account the blocking effect, i.e., the fact that they are applied repeatedly on the same training-test datasets.</p><p>The proposed statistical methodology is also based on an algorithmic procedure which is able to produce nonoverlapping clusters of prediction models, homogeneous with respect to their predictive performance. For this purpose, we utilize a specific test from the generic class of multiple comparisons procedures, namely, the Scott-Knott test <ref type="bibr" target="#b10">[11]</ref>, which ranks the models and partitions them into clusters.</p><p>The proposed statistical framework is applied on a relatively large-scale set of 11 methods over six publicdomain datasets from the PROMISE repository <ref type="bibr" target="#b11">[12]</ref> and the International Software Benchmarking Standards Group (ISBSG) <ref type="bibr" target="#b12">[13]</ref>. Finally, in order to address the disagreement on the performance measures, we apply the whole analysis on three functions of error that measure different important aspects of prediction techniques: accuracy, bias, and spread of estimates.</p><p>The rest of the paper is organized as follows: In Section 2, we summarize related work and we specify the contribution of the current study. In Section 3, we present the limitations of current approaches for multiple comparisons of models and we analytically describe the proposed procedure based on the Scott-Knott test. In Section 4, we demonstrate the experimental setup of this study in a systematic manner, whereas in Section 5 we accumulate the results of the analysis. In Section 6, we perform some sensitivity analysis for two small datasets. Finally, certain limitations of the study are given in Section 7, whereas in Section 8 we conclude by discussing the novel findings of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK AND CONTRIBUTION</head><p>During the last decades there has been evolving research concerning the identification of the best SCE method <ref type="bibr" target="#b0">[1]</ref>. Researchers strive to introduce prediction techniques including expert judgment, algorithmic, statistical, and machine learning methods. The usual practice of these studies is to compare the proposed estimation method with established models on a small number of datasets.</p><p>Earlier studies based their inference regarding the superiority of a prediction method against a comparative one on accuracy measures computed through a validation procedure. Despite the novelty and the promising results of each estimation technique, the researchers' interest has been rapidly focused on the problem of inconsistent findings regarding the determination of the best estimation method, while at the same time they started investigating the reasons behind unstable results.</p><p>Miyazaki et al. <ref type="bibr" target="#b13">[14]</ref> claimed that the "de facto" MMRE accuracy measure tends to advance models that underestimate the actual effort ,while Kitchenham et al. <ref type="bibr" target="#b2">[3]</ref> indicated the variation of accuracy measures as a primary source of inconclusive studies. Toward this direction, Foss et al. <ref type="bibr" target="#b3">[4]</ref> investigated the basis of this criticism through a simulation study, proposing alternative accuracy indicators and concluding that there is need for applying wellestablished statistical procedures when conducting SCE experiments.</p><p>Myrtveit et al. <ref type="bibr" target="#b7">[8]</ref> extended the above-mentioned findings and pointed out that inconsistent results are not caused only by accuracy measures but also by unreliable research procedures. Through a simulation study, they studied the consequences of three main ingredients of the comparison process: the single data sample, the accuracy indicator, and the cross-validation procedure. Furthermore, they provided possible explanations for the lack of convergence, such as the small sample size of many software studies and the splitting of training and test sets in the validation procedure that affects the comparison, even for samples drawn from the same populations. The researchers also inferred that the conclusions on "which model is best" to a large degree depend on the chosen accuracy indicator and that different indicators can lead to contradicting results.</p><p>Mittas and Angelis <ref type="bibr" target="#b4">[5]</ref> showed that the usual practice of promoting a model against a competitive one just by reporting an indicator can lead to erroneous results since these indicators are single statistics of error distributions, usually highly skewed and nonnormal. In this regard, they proposed resampling procedures for hypothesis testing, such as permutation tests and bootstrap techniques for the construction of robust confidence intervals.</p><p>Recently, Menzies et al. <ref type="bibr" target="#b14">[15]</ref> studied the problem of "conclusion instability" through the COSEEKMO toolkit that supports 15 parametric learners with row and column preprocessors based on two different sets of tuning parameters. In order to evaluate the predictive power of the alternative models, they used performance ranks, whereas the selection of the best method was based on a heuristic metric. Their experiments on COCOMO-style datasets concluded that there were four best methods and not just a single option.</p><p>The Scott-Knott test presented here was used in another context in <ref type="bibr" target="#b15">[16]</ref>, for combining classifiers applied to large databases. Specifically, the Scott-Knott test and other statistical tests were used for the selection of the best subgroup among different classification algorithms and the subsequent fusion of the models' decisions in this subgroup via simple methods, like weighted voting. In that study extensive experiments with very large datasets showed that the Scott-Knott test provided the highest accuracy in difficult classification problems. Hence, the choice of the test for the present paper was motivated by former results obtained by one of the authors.</p><p>In <ref type="bibr" target="#b16">[17]</ref>, Dem sar discusses the issue of statistical tests for comparisons of several machine learning classifiers on multiple datasets reviewing several statistical methodologies. The method proposed as more suitable is the nonparametric analogue of ANOVA, i.e., the Friedman test, along with the corresponding Nemenyi post hoc test. The Friedman test ranks all the classifiers separately for each dataset and then uses the average ranks of algorithms to test whether all classifiers are equivalent. In case of differences, the Nemenyi test performs all the pairwise comparisons between classifiers to identify the significant differences. This method is used by Lessmann et al. <ref type="bibr" target="#b8">[9]</ref> for the comparison of classifiers for prediction of defected modules. The methodology described in our paper, apart from the fact that is applied to a different problem, i.e., the SCE where cost and prediction errors are continuous variables, has fundamental differences regarding the goals, the way it is used, and the output.</p><p>Specifically, the algorithm we propose ranks and clusters the cost prediction models based on the errors measured for a particular dataset. Therefore, each dataset has its own set of "best" models. This is more realistic in SCE practice since each software development organization has its own dataset and wants to find the models that best fit its data rather than trying to find a globally best model which is unfeasible. Furthermore, the clustering as an output is different from the output of pairwise comparisons tests, like the Nemenyi test. A pairwise test, for example, can possibly indicate that models A and B are equivalent, models B and C are also equivalent, but models A and C are different. The grouping of model B is therefore questionable. For larger numbers of models the overlapping homogeneous groups resulting from pairwise tests are ambiguous and problematic in interpretation. On the other hand, a ranking and clustering algorithm provides clear groupings of models, designating the group of best models for a particular dataset.</p><p>The goal of this paper is to further extend the research concerning the comparison and ranking of multiple alternative SCE models. We propose a framework for conducting comparative experiments and we present an evaluation of this analysis over different datasets and prediction models. The framework addresses problems caused by different sources of bias, inherent in comparative studies, toward the following directions:</p><p>. Alternative prediction models ranging from regression approaches to analogy-based and machine learning techniques are studied in order to cover a wide range of estimation methods proposed so far in the literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LIMITATIONS OF ESTABLISHED PROCEDURES AND DESCRIPTION OF THE PROPOSED METHODOLOGY</head><p>In this section, we present the main aspects of the proposed methodology. First, we discuss problems of the procedures used to compare a set of candidate prediction methods. Second, we describe in detail the algorithm based on the Scott-Knott test, which addresses the limitations of the established techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problems with Comparison of Multiple Prediction Techniques</head><p>The important role of well-established statistical comparisons in SCE is highlighted in many recent studies, especially during the last decade (see, indicatively, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and <ref type="bibr" target="#b19">[20]</ref>), where the findings are derived through formal statistical hypothesis testing. Indeed, the researchers use parametric as well as nonparametric procedures, whereas there has also been increasing interest <ref type="bibr" target="#b6">[7]</ref> for more robust statistical tests such as permutation tests and bootstrapping techniques for the construction of confidence intervals <ref type="bibr" target="#b4">[5]</ref>.</p><p>The problem we address in this paper belongs to a generic class in statistics known as "multiple hypothesis testing" and can be defined as the procedure of testing more than one hypothesis simultaneously <ref type="bibr" target="#b20">[21]</ref>.</p><p>Briefly describing the problem, the conclusions derived from a statistical hypothesis test are always subject to uncertainty. For this reason, we specify an acceptable maximum probability of rejecting the null hypothesis when it is true and this is referred to as a "Type I error" <ref type="bibr" target="#b21">[22]</ref>. In the case of multiple comparison problems, when several hypotheses are carried out, the probability that at least a Type I error occurs increases dramatically with the number of hypotheses.</p><p>The problem can be easily described through the following example. Suppose, we wish to compare five hypothetical models, setting the significance level at a ¼ 0:05. For each test of an overall set of 10 pairwise tests, the probability of making a Type I error equals a ¼ 0:05 and therefore the probability of not making a Type I error equals 1 À 0:05 ¼ 0:95. In the case of 10 comparisons the probability of no Type I error is 0:95 10 % 0:60. So, with a level of a ¼ 0:05 for each of the 10 tests, the probability of erroneously rejecting a null hypothesis is 0.40.</p><p>The problem of this error inflation can be treated by adjusting the overall (or family) error, but still the execution of a large number of pairwise comparisons is not so straightforwardly interpretable since the resulting groupings are overlapping. For this reason, various intelligent techniques have been proposed in the statistical literature in order to perform targeted multiple comparisons, i.e., comparisons that answer specific research questions. For a thorough treatment of the multiple comparisons problems we refer to the classic book by Hochberg and Tamhane <ref type="bibr" target="#b22">[23]</ref>. The problem of ranking and clustering cost estimation models according to their accuracy can be handled by the algorithm, which is based on the Scott-Knott test, that we describe in Section 3.3.</p><p>Apart from the obvious advantage of the Scott-Knott test to produce nonoverlapping groups, researchers at the Universidade Federal de Lavras, Brazil, have evaluated the power of the test (i.e., the probability that the test will reject the null hypothesis when the null hypothesis is actually false) using Monte Carlo simulations. Their studies <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>, reviewed also in <ref type="bibr" target="#b25">[26]</ref>, have shown that for the Scott-Knott test the comparison-wise and the experimentwise error rates (i.e., the number of Type I errors divided by the number of comparisons and experiments, respectively) are in accordance with the predefined significance level while the power is high. Comparatively with other post hoc tests like the Tukey and the Newman-Keuls tests, the Scott-Knott test was found to be more powerful and robust under a wide variety of experimental situations and assumptions. In some cases the power of Scott-Knott test was eight times higher than the Tukey test. Furthermore, the test has been chosen for two recent studies <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref> due to its robustness, low error rates, and discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Design</head><p>The whole study is organized according to the formal principles of the experimental design or design of experiment (DOE). DOE refers to a systematic planning in order to maintain control over all factors that may affect the result of an experiment <ref type="bibr" target="#b9">[10]</ref>. We have to point out that the term "experimental design" is often used informally in SCE by researchers to generally describe the conditions, the assumptions, the data, and the procedures of a study.</p><p>DOE constitutes an entire branch area in statistics involving fundamental concepts that have to be specified and controlled in advance. The basic element of a DOE is the experimental unit, which is the "object" on which the researcher wishes to measure a response variable. The purpose is to study the effect of one or more factors (categorical variables) on the response variable. The different categories of a factor are known as levels or treatments.</p><p>In the case of our experimental setup, the predictive performance of each competitive model is evaluated through a k-fold cross-validation approach in which the original dataset is randomly partitioned into k subsamples of equal size. During a repeated procedure, each one of the subsamples is considered as the validation sample (test set) and the remaining k À 1 subsamples as the training sets used for fitting the models. Then, the local measures of error (Table <ref type="table" target="#tab_1">1</ref>) are computed for each project of the test set. Each test set gives a global measure of error (Table <ref type="table">2</ref>). The error measures are discussed in detail in Section 4.3. The error measures from all the experimental runs are transformed so as to be normally distributed. These values are used as an input for the Scott-Knott algorithm. In our study, we used k ¼ 10 as the number of folds.</p><p>Following the terminology of a DOE, the k ¼ 10 different folds of the above-mentioned procedure can be considered as the experimental units of our context; the comparative prediction models represent the treatments and the response variable is the normalized expression of a measure in Table <ref type="table">2</ref>. The purpose of the experiment is to investigate the effect of different treatments (models) on a response variable (error measure), i.e., to test the differences of the predictive performance of different models.</p><p>Depending on the number of factors and the sources of variation that can be accounted for, on the response variable it is important to identify the most appropriate DOE technique that has to be adopted, i.e., the arrangement of the factors that will provide statistically valid results. The statistical literature offers a plethora of DOEs which can be utilized in different problems under certain conditions <ref type="bibr" target="#b9">[10]</ref>.</p><p>One problem that came up in SCE experiments is the known or suspected variation due to the split of the training and test sets. Myrtveit et al. <ref type="bibr" target="#b7">[8]</ref> claim that the conclusions about the best prediction model are, to some extent, dependent on the validation procedure that is followed in a study, whereas Shepperd and Kadoda <ref type="bibr" target="#b1">[2]</ref> also argue that the results frequently differ between the pairs of training sets that have been chosen from the underlying dataset. From what we have discussed above, it is essential for the researchers to take into account in their inferences for the superiority of a prediction technique the additional variability that can be caused from the heterogeneity of the validation sets. This is the reason why we decided to employ a specific type of DOE, namely, the Repeated Measures Design, which is equivalent to the Randomized Complete Block Design <ref type="bibr" target="#b9">[10]</ref>.</p><p>The RCBD setup incorporates an additional factor, the so-called block, which takes into account the grouping of similar experimental units. The incorporation of this extra factor is considered advantageous in order to identify true differences between treatments or, equivalently, the true treatment effect. Indeed, when different treatments are applied to similar (or the same) experimental units which form, in any sense, a block, there is a source of variation between blocks which cannot be explained by the difference between treatments. This source of variation is represented by the block factor that is considered in the analysis. In our context, the splitting of data into different training-test pairs represents the blocking factor, i.e., each block is a specific pair of a training-test subsets, where all models are applied and validated. Illustrating all the notions described above, suppose that one wishes to compare the prediction performance of four hypothetical models (A, B, C, and D) based on the evaluation of an error function E. The process of the 10fold cross validation for each comparative model results in 10 values of global error (MeanE). Hence, the RCBD adopted in our experimental setup can be explicitly described in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Scott-Knott Test</head><p>The Scott-Knott test <ref type="bibr" target="#b10">[11]</ref> is a multiple comparison procedure based on principles of cluster analysis. The clustering refers to the treatments (methods or in our case models) being compared and not to the individual cases, while the criterion for clustering together treatments is the statistical significance of differences between their mean values <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Our preference for the Scott-Knott test relies on a specific desirable characteristic of the method, i.e., that it is able to separate the models into nonoverlapping groups. In our case, the values of the response variable that is affected by the models are translated to expressions of the prediction errors derived from the models being compared. The algorithm we describe next is therefore able to rank and cluster prediction models according to their accuracy.</p><p>Suppose that we want to compare the predictive accuracy of d alternative models on a specific dataset through the utilization of a functional expression of the error e. We also assume that, following a standard validation procedure, there are k pairs of training-test datasets, i.e., the original dataset is divided k times in training test subsets. All models are applied repeatedly to each one of these pairs, i.e., each model is trained and validated using the same pair. The predictions of the test dataset yield an overall measure of accuracy which is the value of the response variable for the unique combination of a model and a training-test dataset. Therefore, for a specific dataset we have d Á k measurements.</p><p>The original methodology, as described in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b22">[23]</ref>, does not take into account the repetitive character of the experiment, i.e., it just considers that we have, for each model k, different measurements. The Scott-Knott procedure follows and uses the one-way analysis of variance (oneway ANOVA) which tests the null hypothesis that the treatment means are all the same or, equivalently, that there is no statistical difference between the means of the accuracy measures obtained by the compared models <ref type="bibr" target="#b28">[29]</ref>. However, the alternative hypothesis is that the models can be partitioned into two mutually exclusive and collectively exhaustive nonempty subsets.</p><p>Obviously, when ANOVA shows that there is no statistical evidence for rejecting the null hypothesis, the comparative prediction models form a homogeneous group and cannot be clustered anymore. However, when ANOVA finds at least one significant difference, the Scott-Knott algorithm uses the ratio of the so-called "Sum of Squares due to Error" (SSE) and the corresponding degrees of freedom from ANOVA to estimate the variance 2 of the random error, i.e., the part of the response variable that cannot be explained by the treatment effect.</p><p>This property of the procedure (Step 4 below) which essentially uses the ANOVA table gives the opportunity to include in our analysis another factor, in addition to the treatment factor. In cases when all treatments are applied to each one of the experimental units, the DOE is known as repeated measures design. Since the differences among responses from different experimental units receiving the same treatment may be large, it is better to consider in our analysis one more factor accounting for the differences between units. Otherwise, the detection of real differences between treatments becomes problematic. The ANOVA for repeated measures designs is equivalent to the analysis of randomized complete block designs, where the experimental units are considered as blocks <ref type="bibr" target="#b28">[29]</ref>.</p><p>In our case, the experimental setup is a design where the d models are the treatments and the experimental units are the k training-test pairs derived from the original dataset. The models are all trained and tested to each one of the pairs. In this setup, the ANOVA table accounts for the blocking effect, produces different SSE and degrees of freedom so that the estimation of 2 is different.</p><p>Therefore, the following algorithm can be used either without considering blocks, as a single-factor analysis of experiment, or by considering the blocking effect. We believe that the blocking effect i.e., the effect of the training-test dataset pair, should be taken into account since in all cases we tested it was found statistically significant. The procedure we propose consists of consecutive steps aiming at a maximum differentiation between groups at each stage. Each group that is formed can be partitioned again if the new groups are significantly different. The whole process is fully described by the following steps:</p><p>1. Sort the means of the error measures " e j , j ¼ 1; . . . ; d, for each model in ascending order:</p><formula xml:id="formula_0">" e ð1Þ " e ð2Þ Á Á Á " e ðdÞ :<label>ð1Þ</label></formula><p>2. For each " e ðjÞ , j ¼ 1; . . . ; d À 1, separate the group of all ordered means E into two subgroups E 1 ¼ f" e ð1Þ ; . . . ; " e ðjÞ g and E 2 ¼ f" e ðjþ1Þ ; . . . ; " e ðdÞ g and compute the between-groups sum of squares:</p><formula xml:id="formula_1">G j ¼ kðjE 1 jð" e E 1 À " e E Þ 2 þ jE 2 jð" e E 2 À " e E Þ 2 Þ;<label>ð2Þ</label></formula><p>where jE 1 j, jE 2 j are the cardinalities of the two subgroups and " e E ; " e E 1 ; " e E 2 are the means of groups E, E 1 and E 2 , respectively:</p><formula xml:id="formula_2">" e E ¼ 1 d X d j¼1 " e ðjÞ ; " e E 1 ¼ 1 jE 1 j X j2E 1 " e ðjÞ ;" e E 2 ¼ 1 jE 2 j X j2E 2 " e ðjÞ :<label>ð3Þ</label></formula><p>3. Find the partition that maximizes the value of the above sum of squares:</p><formula xml:id="formula_3">G j Ã ¼ max G j ; j ¼ 1; . . . ; d È É<label>ð4Þ</label></formula><p>4. Compute from the ANOVA table the s 2 , the estimation of 2 (the variance that cannot be explained by the factors, i.e., the treatments and the blocks), by dividing the SSE by the corresponding degrees of freedom. Next, compute the statistic:</p><formula xml:id="formula_4">¼ 2ð À 2Þ G j Ã s 2 ;<label>ð5Þ</label></formula><p>which has approximately a 2 distribution where the degrees of freedom are given by ¼ k ð À 2Þ = (rounded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">If &gt; 2</head><p>;a (where is a predefined significance level), then the same test is applied to each group separately. If &lt; 2</p><p>;a , then all means belong to the same homogeneous group. The procedure is continued by splitting each group into two subgroups if the -criterion is significant; otherwise, by identifying a homogeneous group until no groups can be split further. The criterion for splitting the groups is based on the assumption that the distribution of in ( <ref type="formula" target="#formula_4">5</ref>) is approximately a chi-square distribution. This is a theoretical result of another fundamental assumption, i.e., that the error measures yielded in the experiment are normally distributed. However, experience shows that the distributions of errors are usually skewed and certainly not normal. Therefore, it is necessary to apply a transformation on them in order to normalize the error values before we apply the algorithm described above. For this, we applied Blom's transformation <ref type="bibr" target="#b29">[30]</ref> that utilizes the ranks r i of the error values e i and the inverse of the cumulative normal distribution function È À1 ðxÞ.</p><p>The formula for the Blom transformation is</p><formula xml:id="formula_5">È À1 r i À 3=8 n þ 1=4 :<label>ð6Þ</label></formula><p>The transformation produces normally distributed numerical values, which are used instead of the original error measures as input for the algorithm. As an example, we present the original (Fig. <ref type="figure" target="#fig_0">1</ref>) and the transformed (Fig. <ref type="figure">2</ref>) values of MMRE for the KEM dataset. It is important to note that the Blom transformation is monotonous and therefore the order of the values is kept intact. The output of the algorithm is a ranking of the models according to their transformed error measures and, moreover, a clustering scheme where each cluster consists of the sorted models that do not have significant difference in their error measures.</p><p>It is also important to note that , i.e., the predefined significance level, is not the nominal error rate of the Type I error of the whole algorithm (see <ref type="bibr" target="#b25">[26]</ref>). If we set ¼ 0:05, then each decision of the Scott-Knott procedure to divide or not a subgroup has a Type I error rate of 5 percent, but the overall Type I error rate of the algorithm depends on the number of tests performed. This section provides details concerning the setup of the framework and the experimental design of the study. The basic idea of the experimental setup was to take into account: 1) different cost prediction methods covering a major part of the variety of the proposed methodologies that have appeared so far in the SCE literature and which are governed by a diversity of principles, 2) different datasets, and 3) different measures of error. Moreover, the experiment was designed to take into account the effect of training-test splitting of each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparative Prediction Models</head><p>The 11 selected methods can be grouped into three main categories that are regression-based models, analogy-based techniques, and machine learning methods. All these models are well-established methods, there is a vast literature on them, and they have been already applied in SCE. In Table <ref type="table">4</ref>, we present a list of the methods being compared along with a brief description in order to give the basic principles of each method.</p><p>The choice of the alternative prediction techniques was also based on the conclusions of a systematic review on SCE studies. Jorgensen and Shepperd <ref type="bibr" target="#b0">[1]</ref> pointed out that the regression-based models dominate since half of all studies deal with the problem of fitting, improvement, or comparison of a regression model. Furthermore, they claim that the researchers' interest for the analogy-based techniques is steadily increased during the last decade. Finally, the distribution of estimation methods also reveals that the proportion of machine learning techniques (i.e., Classification and Regression Trees and Neural Networks) presents an increasing trend.</p><p>It is obvious that the prediction techniques used in our experimentation presuppose the tuning of certain parameters in order to build meaningful models. For example, the ratio-scaled variables of regression-based models are checked in order to investigate whether the normality assumption is satisfied, whereas the nominal and ordinal variables are replaced with new dummy variables and then a stepwise procedure is adopted to extract the most significant independent variables. In the case of analogybased methods, we use the Kaufman-Rousseeuw dissimilarity measure <ref type="bibr" target="#b40">[41]</ref>, taking into account various types of variables, whereas the selection of the best number of the "neighbor" projects is determined through the leave-one-out cross-validation procedure <ref type="bibr" target="#b36">[37]</ref>. The exclusion of irrelevant variables is based on published studies ( <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>) that utilize a brute-force algorithm.</p><p>Regarding neural network models, we specify the number of nodes for hidden layers <ref type="bibr" target="#b37">[38]</ref>. In particular, we adopted the RMiner for NN <ref type="bibr" target="#b43">[44]</ref>, a library for R language that facilitates the use of data mining techniques in classification and regression tasks. The network includes one hidden layer of H neurons and the output neuron uses a linear function. In RMiner, the NN hyperparameter H is optimized using a grid search with a backward selection algorithm, whereas to avoid overfitting, an internal k-fold process is used. After selecting the best parameter, the model is retrained with all training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4 Prediction Models of the Comparative Experimentation</head><p>As far as the CART model is concerned, we utilize the Recursive Partitioning <ref type="bibr" target="#b44">[45]</ref> algorithm as implemented in S-PLUS <ref type="bibr" target="#b45">[46]</ref> in which the model is fitted using binary recursive partitioning whereby the data are successively split along coordinate axes of the predictor variables so that at any node, the split which maximally distinguishes the response variable in the left and the right branches is selected. Splitting continues until nodes are pure or data are too sparse, according to the recommendations of S-PLUS manual <ref type="bibr" target="#b45">[46]</ref>.</p><p>Finally, for the case of the Naive Bayes classifier, the methodology computes the conditional a-posterior probabilities of the dependent variable given the independent predictors using the Bayes rule <ref type="bibr" target="#b38">[39]</ref>, whereas a stepwise procedure for the selection of the best subset of attributes is also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>The datasets for the experimentation are derived from two sources, namely, the PROMISE repository <ref type="bibr" target="#b11">[12]</ref> and the International Software Benchmarking Standards Group (ISBSG, release 10) <ref type="bibr" target="#b12">[13]</ref>. The main reason for this selection was that these datasets have been extensively used to empirically validate or justify a large amount of research results, whereas they are also publically available. Each dataset contains a different number of projects and a set of independent variables with mixed-type characteristics (Table <ref type="table">5</ref>), whereas the dependent variable that has to be predicted is the actual effort. Another criterion for the selection of the datasets was the ability to apply all the competitive prediction methods on them. Therefore, we did not consider datasets with too many categorical variables which cause problems to certain methods like regression and Neural Networks.</p><p>The ISBSG repository contains 4,106 software projects from more than 25 countries, but most of the variables have a large amount of missing values. Keeping in mind the guidelines of ISBSG suggesting filtering of the data projects, we decided to discard the projects with missing values. Moreover, an important issue in SCE is the utilization of datasets with high quality in the process of evaluation and comparison of prediction models <ref type="bibr" target="#b46">[47]</ref>. Due to this fact and the instructions of the ISBSG organization that point out not taking into account projects with low quality, we based our analysis on projects marked with "A" in Data Quality Rating and UFP Rating. Finally, the independent characteristics utilized in the construction of the alternative models are the same as in <ref type="bibr" target="#b47">[48]</ref> in order to retain the compatibility with other studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accuracy Measures</head><p>In Section 2, we have already mentioned that the overwhelming majority of SCE studies base their inferences on MMRE. However, MMRE has been criticized as an inappropriate measure since it tends to favor models that underestimate the actual cost of projects <ref type="bibr" target="#b13">[14]</ref>.</p><p>During the last decade there has been a thorough discussion concerning the determination of the most appropriate error function without essential convergence. Having in mind that all the accuracy indicators exhibit certain advantages, while at the same time they suffer from either a flaw or a limitation <ref type="bibr" target="#b3">[4]</ref>, the basic key for resolving the problem is to realize what is really measured by each error function <ref type="bibr" target="#b2">[3]</ref>.</p><p>The lesson learned from the above-mentioned discussion is that there is a need for utilization of three different error functions measuring three aspects of the prediction performance of comparative models. More precisely, Absolute Error (AE) is used in order to evaluate the accuracy of models, whereas error ratio z has been adopted as a measure of bias accounting for underestimations (z &lt; 1) or overestimations (z &gt; 1) with an optimum value of 1 <ref type="bibr" target="#b2">[3]</ref>. The most widely known MRE indicator was also used since, according to Kitchenham et al. <ref type="bibr" target="#b2">[3]</ref>, it provides a measure of the spread of the error ratio z.</p><p>The local measures of error (Table <ref type="table" target="#tab_1">1</ref>) that are computed through the actual (Y A ) and the estimated (Y E ) values of each single project i constitute the basis for the evaluation of the overall prediction performance of the comparative models by computing a statistic (i.e., mean) for a set of n test cases (Table <ref type="table">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we present the results of the experiments conducted on six datasets. Tables <ref type="table">6</ref> and<ref type="table">7</ref> summarize the results of all experimental runs. Each dataset corresponds to a different DOE where each model was applied to 10 test sets and then the values of the error measures were transformed and used as an input for the Scott-Knott algorithm. So, the ranking was based on the means of the transformed values and not the means of the original values of the global measures.</p><p>Table <ref type="table">6</ref> presents the results of the ANOVA procedure on which the clustering algorithm was based. We can see that for each dataset and error measure we have two columns, labeled randomized complete block design and complete randomized design (CRD). The first one takes into account the effect of the models (treatment effect) and the blocking factor (block effect), while the second only the model effect. For each one of the ANOVA tests, we report the significance (p-value) and, in parentheses, the eta-squared statistic, which provides an estimate of the effect size in ANOVA. The effect size is a measure of the importance of a result in the population and for the eta-squared the benchmark values are: 0.01 (small), 0.06 (medium), and 0.14 (large) <ref type="bibr" target="#b48">[49]</ref>.</p><p>The columns of Table <ref type="table">7</ref> contain the output of the algorithm for each different dataset under a specific global measure of error. The models are ranked starting from the best to the worst, i.e., lower rank means better model. Models that do not have significant difference are in consecutive cells of the table, shaded with the same color. So the shading of the cells represents the clustering of the models. The ranking of models is the same for both designs (RCBD and CRD), but the clustering is not necessarily the same. The ticks in the CRD columns indicate exactly this fact, i.e., that the ranking is exactly the same, so there is no need to write the names of the again.</p><p>The last column of Table <ref type="table">7</ref> presents the mean rank (MR) for each model over all datasets. Moreover, the models are ranked according to their overall performance in all datasets.</p><p>As we can observe from Table <ref type="table">7</ref>, it is clear that the common belief that different dataset characteristics may favor different prediction models is confirmed in our experiments. Generally, the prediction performance of each model varies from dataset to dataset and it is impossible to extract a global conclusion. Overall, OLS and CART seem to be the best-rated and worst-rated estimation techniques since they have the smallest and the highest values of MRs, respectively.</p><p>An interesting conclusion is also derived from the three indicators of error within each dataset. Table <ref type="table">7</ref> suggests that there is a ranking instability of different models across the same dataset. For example, based on the results of ISBSG dataset, NB presents the best performance in terms of MMRE and Meanz, but the second worst for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>Significance Values and Effect Size (Eta-Squared) of ANOVA case of MAE. Hence, these findings reinforce the conclusions of other researchers (i.e., <ref type="bibr" target="#b7">[8]</ref>), signifying the important role of the selection of accuracy indicator in comparative studies. Moreover, we also have to recall that different error functions quantify different aspects of prediction performance <ref type="bibr" target="#b2">[3]</ref>. Subsequently, although there is a ranking instability for NB in the above-mentioned example, it is crucial to acquire the significant information that is hidden in this lack of convergence and to interpret the remarkable findings with caution. So, NB presents the least biased predictions (Meanz) with the smallest spread (MMRE) but at the same time NB suffers from inaccuracy (MAE) problems.</p><p>Although the rankings of models clearly portray an overview of their predictive performance, it is essential to statistically test whether some models are superior to others since these observed differences could reasonably occur "just by chance." Next, we present the results of the formal comparison of models through the inferential statistical procedures of RCBD and Scott-Knott test.</p><p>The first remarkable issue from Table <ref type="table">6</ref> (RCBD columns) is that the blocking factor (choice of trainingtest datasets) shows a statistically significant effect (p &lt; 0:001 and large effect size) for all the experiments. Thus, the choice to consider the variability caused by the splitting of data into training and test sets is justified. Otherwise, it is possible to derive erroneous results.</p><p>In order to illustrate this issue, except for the results of RCBD (Tables <ref type="table">6</ref> and<ref type="table">7</ref>), we also present for each dataset the ANOVA results (Table <ref type="table">6</ref>) and the groups of homogeneous models (Table <ref type="table">7</ref>) in terms of prediction performance derived from the Scott-Knott algorithm without taking into account the blocking factor (columns labeled as CRD). We can clearly see that there are significant differences in the ANOVA results and the formation of groups between RCBD and CRD. For example, the Scott-Knott algorithm resulted in three groups of statistically different models in the case of the ISBSG dataset when MAE was examined and when the blocking factor was taken into consideration (RCBD). On the other hand, for CRD no significant differences were found, meaning that all the models presented similar prediction accuracy.</p><p>As we have already mentioned, in Table <ref type="table">7</ref> significant differences in performances across the alternative models are indicated through differently shaded cells. As the shading of rows becomes darker, the performance of the group of the estimation models with the same color becomes worse. For example, in the case of the TEL dataset, the Scott-Knott test results in two homogeneous groups of estimation models for MMRE and MAE. The best group (white cells) includes all except for the last three models (CART, NB, and NN) that belong to the second group. An initial interesting conclusion is the case of the KEM dataset, in which the Scott-Knott test does not find any statistically significant difference among the competitive techniques.</p><p>The results of the Scott-Knott procedure are also presented in a graphical manner for two cases (Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref>). The diagram plots comparative models (x-axis) against the transformed mean errors (y-axis), whereby all methods are sorted according to their ranks. The vertical dashed lines indicate which models give statistically different results and thus are clustered into homogeneous group. For example, in Fig. <ref type="figure">3</ref>, we present the results of the ISBSG dataset for the experiment with MMRE. The Scott-Knott algorithm resulted in four homogeneous groups of models with similar performances. Each small vertical solid line represents the prediction performance for each one of the competitive models and, more precisely, the asterisk depicts the mean value of the transformed error function, whereas the bounds of a 95 percent confidence interval for the mean derived from the 10 folds are shown through the lower and upper ends of the lines, respectively. Furthermore, the horizontal lines represent the mean transformed errors for each one of the four different groups. As we can also observe from Fig. <ref type="figure">4</ref> (referring to the KEM dataset and the MMRE), the whole analysis does not provide statistical evidence for different prediction performances among the comparative models and for this reason all models are clustered into one group.</p><p>Of course, the results of statistical tests should be interpreted using technical details of models and methods. For example, NB seems to produce inaccurate estimates for the case of the TEL dataset. This may be due to the philosophy of the NB technique since this specific model classifies the effort of a forthcoming project into predefined categories. Keeping in mind the small size of the TEL dataset (Table <ref type="table">5</ref>), we can easily observe that each category contains a small number of projects, which in turn may result in low predictive power of the method. Furthermore, the small size of projects in the KEM dataset can also be the most important factor leading the Scott-Knott test to not separate the means of error functions into nonoverlapping groups. Generally, the application of Scott-Knott tests for all datasets reveals one of the most appealing findings of this study: Despite the large divergences of error functions among alternative prediction models, there is no statistical evidence that some methods differ significantly. Hence, the notion of the "best" estimation technique should be revised, whereas at the same time it is probably more proper to refer to the "best group of estimation techniques."</p><p>Moreover, the size of the samples in which the alternative prediction models are built plays a critical role in SCE experiments. Indeed, Table <ref type="table">7</ref> shows that there is no evidence of statistical differences among alternative prediction models in small datasets (i.e., KEM, TEL, etc.).</p><p>This pattern in the results also brings to light a conviction concerning the vanity of using many estimation methods on small datasets since they cannot be more informative despite their promising and sophisticated principles.</p><p>Concerning the predictive power of the alternative models, there are also some interesting findings that are derived from this study.</p><p>First, it seems that OLS performs generally well for all datasets since this specific form of regression-based techniques gives the best ranking for seven out of 18 experiments (Table <ref type="table">7</ref>), whereas the MRs for the three global measures of error are the lowest. Furthermore, Scott-Knott tests suggest that OLS can be categorized into the best group of estimation techniques in all except two experiments (MMRE and Meanz of ISBSG dataset). This result is in accordance with the literature if we consider that the relation between software effort and size has been modeled as exponential in the sense that the natural logarithm of effort is expressed as a linear function of the logarithm of size (there is extensive literature on the form of the relation between size and effort; see, for example, <ref type="bibr" target="#b49">[50]</ref>). Regarding the other regression-based models, although these forms (LTS, LMS, and RobMM) provide robust estimations when there are outliers in the data, in our experiments they present similar or even worse predictive performance than OLS. A possible explanation for this may be the lack of bad quality data points that can influence the accuracy of OLS.</p><p>Second, the predictive accuracy of analogy-based methods seems to be similar despite the adjustments that may be applied to the final estimation. This is clearly inferred from the results of Scott-Knott tests, where the analogy-based techniques are clustered together in the same group of methods for all experiments.</p><p>Finally, we can also notice that machine learning methods (Bag, NN, NB) provide accurate estimations for a few experiments, constituting an alternative choice to regression and analogy-based methods. On the other hand, it seems that CART is not able to catch the strong linear dependency between effort and size of projects and this may be the main reason for the poor predictive performance. This result is also consistent with the findings of Kitchenham <ref type="bibr" target="#b50">[51]</ref>, suggesting that only in the cases of a large number of projects contained in a dataset should a practitioner consider using CART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SENSITIVITY ANALYSIS</head><p>We have already mentioned in Section 3 that the validation procedure can play a significant role in the conclusions derived due to the added variability of splitting a dataset into a training-test pair <ref type="bibr" target="#b7">[8]</ref>, whereas the results can frequently differ. More recently, Menzies and Shepperd <ref type="bibr" target="#b51">[52]</ref> also pointed out that the validation procedure may be a potential source of noise in the experiments since minor changes to the training data can lead to large conclusion instabilities in the internal parameters of prediction models.</p><p>Kohavi <ref type="bibr" target="#b52">[53]</ref> suggested that if the prediction algorithm is stable for a given dataset, the variance of the crossvalidation estimates should be approximately the same, independent of the number of folds. He also remarked that as the sample sizes get smaller and the number of folds decreases, the instability of the training sets leads to an increase in variance of the whole procedure recommending the 10-fold cross-validation scheme as it tends to provide less biased estimation of the accuracy.</p><p>Efron <ref type="bibr" target="#b53">[54]</ref> conducted five sampling experiments and compared leave-one-out cross validation and several other methods in order to investigate some related estimators which seem to offer considerably improved estimation in small samples. The results indicate that leave-one-out cross validation gives nearly unbiased estimates of the accuracy, but often with unacceptably high variability, particularly for small samples.</p><p>Hence, the choice of the validation procedure is a critical topic affecting the derived conclusions of an experiment, whereas the choice of the number of folds can also contribute to the instability of the results, especially for small datasets. For this reason, we decided to perform some sensitivity analysis for two small datasets (ALB and TEL) in which the Scott-Knott procedure results in different homogeneous groups of estimation models for MMRE for the initial experiments. More precisely, we conduct the whole analysis for k ¼ 3 and k ¼ n. The latter approach is the special case of k-fold cross validation, where the number of folds equals the number of instances in the dataset, and is essentially equivalent to the leave-one-out cross validation (LOOCV), which takes advantage of the raw measurements of errors. It is important to emphasize here that the proposed algorithm is applicable to any validation schema.</p><p>The results of the Scott-Knott test for the three different numbers of folds (k ¼ 3; 10; n) are given in Table <ref type="table" target="#tab_5">8</ref>. We also have to note that the ANOVA tests under the RCBD setup indicate that the effect of the blocking factor is statistically significant for all the experiments. Moreover, the findings of Table <ref type="table" target="#tab_5">8</ref> show that changes to the splitting of training and test datasets lead to quite different results. For the case of the TEL dataset, the Scott-Knott procedure results in two homogeneous groups of prediction models, but it is also clear and reasonable that there is a ranking instability for the different numbers of folds, whereas the homogeneous groups do not contain the same prediction models. On the other hand, it is worth noting that a subset of models (OLS, LTS, RobMM, and EbA_Adj1) is always included in the best group, despite the number of folds of the validation procedure. The results for the ALB dataset are even more representative of the prediction capability of the alternative models since three methods (OLS, LTS, and LMS) constitute a set of best estimation techniques. In conclusion, our results from sensitivity analysis are in accordance with former studies which recommend the use of different validation schemes when comparing models, especially when we use small datasets.</p><p>Another question that deserves some further investigation is why one should follow the analysis based on the Scott-Knott procedure and not perform one of the more known and, in a sense, more traditional post-hoc tests. Indeed, the literature on multiple comparisons <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> contains a variety of post-hoc techniques, like the Bonferroni, Scheffe, Tukey, and Duncan tests, among many others.</p><p>The most significant drawback of the traditional posthoc procedures is the fact that they typically produce overlapping groups. So, with a large number of treatments (prediction models in our case), the interpretation of the derived results is often difficult and ambiguous.</p><p>In order to illustrate the difficulties in the interpretation of the results in the multiple comparison problem of alternative prediction models, we conducted the whole experimentation using the traditional Tukey's post-hoc test. The outcomes given in Table <ref type="table">9</ref> reveal that Tukey's test also results in homogeneous groups of prediction models for a few datasets and accuracy measures. On the other hand, it is also obvious that the prediction models are separated into overlapping groups, causing interpretation and decision problems regarding the superiority of a method against some other.</p><p>For example, the analysis of the ALB dataset for the case of MMRE indicates that there are two homogeneous groups of prediction models. The first "best" group includes a set of 10 models that are LTS, OLS, LMS, RobMM, NN, NB, Bag, EbA_Adj1, EbA_Adj2, and EbA. In the second group, there are many prediction models that are common with the first group, such as NN, NB, Bag, EbA_Adj1, EbA_Adj2, and EbA, <ref type="bibr">TABLE 9</ref> Experimental Results with the TUKEY Post-Hoc Tests whereas CART is the only model that is not included in the first group. On the other hand, the analysis conducted through the Scott-Knott algorithm results in a set of three homogeneous groups in which models are separated without ambiguity. This problem is even worse in case of the ISBSG dataset for MMRE, where there are four homogeneous with many models common to two (OLS and EbA_Adj2) or three groups (RobMM, LTS, NN, EbA_Adj1, EbA, and Bag).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">THREATS TO VALIDITY</head><p>Regarding the experiments conducted in this study, we have to discuss some validity issues. A primary source of bias can stem from the selection of the datasets that are used.</p><p>In this regard, we state that these repositories contain data from wide range of projects and, more importantly, they are in the public domain. Hence, all the experiments can be verified by a replicate study and compared with other findings from previous studies.</p><p>The second validity issue is related to the selection of the competitive models. Keeping in mind that there is a plethora of proposed SCE techniques, it is infeasible to include the whole range of possible candidates of methods. However, we chose to evaluate techniques which are representative of the three main SCE categories. Furthermore, our goal was to select techniques that are considered well-established since researchers utilize and study their performances in various experimental setups within the SCE domain. It is also important to note that the performance of all models, even the simplest ones, depends on choices of certain parameters. In the present study, we cannot claim that we compared the best possible model from each class. Instead, we chose representative models that are fitted reasonably well to our datasets.</p><p>Of course, we recognize that the study does not include techniques belonging to the general class of expert-judgment methods. Expert-based methods presuppose the participation of a group of capable and skillful practitioners in the development process in order to derive accurate estimations. In this paper, we decided to explore the predictive power of algorithmic-based models based on training datasets, but it is a challenge for future work to replicate the present study by taking into account expert judgment.</p><p>Regarding the bias of the validation procedure, we applied a k-fold cross validation instead of a hold-out method that splits the available data into two nonoverlapping parts, one for training and the other for testing. The basic advantage of the former method is that all observations are used for both training and validation, and each observation is used for validation exactly once. On the contrary, the holdout strategy does not use all the available data and the results are highly dependent on the choice of the training-test partition. Although the k-fold cross validation may also have certain limitations due to the splitting of training and test sets, the risk is minimized through the repeated drawn samples. More importantly, we also consider an experimental design and the corresponding analysis that takes into account the extra variability due to validation procedure, trying to smooth the consequences of the random sampling of data.</p><p>The ANOVA procedure which is the basis of our approach involves threats when basic assumptions are violated. These assumptions involve normally distributed response variables and homogeneity of variances. In our study, we used a transformation for deriving a normally distributed response variable, reducing the effect of skewness. However, the presence of heterogeneous variances and their effect on the algorithm is an open research question.</p><p>Finally, the lack of agreement concerning the measures of error in SCE experiments is also taken into account through the examination of different error functions. The results of the comparisons we made are not generalized, in the sense that for a specific dataset, a prediction candidate can present the best performance in terms of a certain aspect of accuracy, but at the same time, this model can also suffer from inaccuracy problems based on an alternative aspect of error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this paper, we deal with a critical research issue in software cost estimation concerning the simultaneous comparison of alternative prediction models, their ranking and clustering in groups of similar performance. We examined the predictive power of 11 models over six public domain datasets.</p><p>The whole procedure is settled on well-established statistical methodologies taking into consideration the multiple comparison problems. Keeping in mind the critical role of the adoption of reliable practices in the development process for both project managers and customers, we proposed a formal framework and structured guidelines in order to reinforce the knowledge acquisition and diminish the inherent uncertainty in SCE.</p><p>In this regard, we proposed certain directions concerning the utilization of alternative prediction models from different classes of estimation techniques, different datasets from public domain repositories, alternative error functions measuring different aspects of predictive performance, an experimental design in order to overcome the problem of splitting the datasets in the validation process, and more importantly, an efficient hypothesis testing procedure that signifies whether a set of prediction models gives statistically better results than another set of comparative models.</p><p>We have to emphasize that the experimentation section is used as a means for illustrating how the whole framework can be evaluated on the comparison setup, contributing to the systematic research of the performances of any kind of prediction techniques. Thus, it is not our purpose to determine the superiority of any prediction method and, even more, it is not wise to generalize the derived findings for the population of software projects.</p><p>On the other hand, the derived results of our experimentation either bring to the surface a few significant conclusions or confirm other essential results from past studies. Although the indicators from alternative models designate generally different predictive performances for the datasets, the proposed statistical hypothesis testing through the Scott-Knott test verifies that the predictive accuracy of a set of methods does not confirm a statistically significant difference among them. This suggests that the notion of the "best" estimation method may not have been so well defined thus far in the previous SCE research. Hence, there is a need to relocate the basis of the whole research. In other words, when a practitioner wishes to perform a comparative study, she or he ought to a set of best estimation models and not just a single one. Moreover, it seems that there is a global solution since alternative methods can exhibit a few advantages in terms of certain aspects of prediction performance, but at the same time these candidates may suffer in terms of another aspect of accuracy. In order to overcome these inconsistencies, managers should utilize their experience and lead the whole process through the necessities that arise in each case. Therefore, it is to emphasize here that the proposed method is an aid to the process of decision making by ranking and clustering the candidate models. However, it does not make decisions itself. The final decision is left to the expert and depends on several issues, even on personal criteria like experience, preference of statistical software, etc.</p><p>Another interesting finding concerns the utilization of complicated and more sophisticated models. It seems that very often a linear model is adequate enough to catch the trend between effort and other cost drivers of projects. Therefore, in certain cases it may be useless to strive to introduce new, highly complicated algorithms which, in practice, just cannot provide any further improvement. Finally, it is our strong belief that new estimation techniques should be tested and compared using appropriate statistical procedures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Original MMRE values for the KEM dataset. Fig. 2. Tranformed MMRE values for the KEM dataset.</figDesc><graphic coords="6,49.32,51.17,204.78,160.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Plot of the Scott-Knott algorithm (MMRE-ISBSG). Fig. 4. Plot of the Scott-Knott algorithm (MMRE-KEM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>TABLE 2</cell></row><row><cell>Measures of Error</cell><cell>Global Measures of Error</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 Randomized</head><label>3</label><figDesc>Complete Block Design of Experimentation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5 Datasets Characteristics</head><label>5Characteristics</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 7 Experimental Results</head><label>7Results</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 8 Sensitivity</head><label>8</label><figDesc>Analysis for Small Datasets (MMRE)</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the reviewers for their constructive comments which helped them to improve the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nikolaos Mittas received the BSc degree in mathematics from the University of Crete and the MSc and PhD degrees in informatics from the Aristotle University of Thessaloniki (A.U.Th). His research interests include application of statistics, especially computational statistics, to cost estimation of software projects, and generally to data from software projects.</p><p>Lefteris Angelis received the BSc degree in mathematics and the PhD degree in statistics from the Aristotle University of Thessaloniki (A.U.Th.). Currently, he is an associate professor in the Department of Informatics of A.U.Th. His research interests include statistical methods with applications to information systems and software engineering, computational methods in mathematics and statistics, planning of experiments, and simulation techniques.</p><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Systematic Review of Software Development Cost Estimation Studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="53" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing Software Prediction Techniques Using Simulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kadoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1014" to="1022" />
			<date type="published" when="2001-11">Nov. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What Accuracy Statistics Really Measure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pickard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proc. Software Eng</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Simulation Study of the Model Evaluation Criterion MMRE</title>
		<author>
			<persName><forename type="first">T</forename><surname>Foss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stensrud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Myrtveit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="985" to="995" />
			<date type="published" when="2003-11">Nov. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparing Cost Prediction Models by Resampling Techniques</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mittas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Systems and Software</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="616" to="632" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human Performance Estimating with Analogy and Regression Models: An Empirical Validation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stensrud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Myrtveit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Fifth Int&apos;l Software Metrics Symp</title>
		<meeting>IEEE Fifth Int&apos;l Software Metrics Symp</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="page" from="205" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Why Comparative Effort Prediction Studies May Be Invalid</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Fifth Int&apos;l Conf. Predictor Models in Software Eng</title>
		<meeting>ACM Fifth Int&apos;l Conf. Predictor Models in Software Eng</meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reliability and Validity in Comparative Studies of Software Prediction Models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Myrtveit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stensrud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="380" to="391" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2008-08">July/Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Design of Experiments for Engineers and Scientists</title>
		<author>
			<persName><forename type="first">J</forename><surname>Antony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Butterworth-Heinenmann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Cluster Analysis Method for Grouping Means in the Analysis of Variance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="507" to="512" />
			<date type="published" when="1974-09">Sept. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The PROMISE Repository of Software Engineering Databases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Sayyad</forename><surname>Shirabad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<ptr target="http://promise.site.uottawa.ca/SERepository" />
	</analytic>
	<monogr>
		<title level="m">School of Information Technology and Eng</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Ottawa</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<ptr target="http://www.isbsg.org" />
	</analytic>
	<monogr>
		<title level="j">ISBSG Data Set</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust Regression for Developing Software Estimation Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terakado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Systems and Software</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stable Rankings for Different Effort Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hihn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="437" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective Fusion of Heterogeneous Classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="511" to="525" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dem Sar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Replicated Assessment and Comparison of Common Software Cost Modeling Techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wieczorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc 22nd. IEEE Int&apos;l Conf. Software Eng</title>
		<imprint>
			<biblScope unit="page" from="377" to="386" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Empirical Study of Maintenance and Development Accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfleeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Systems and Software</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="77" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross versus Within-Company Cost Estimation Studies: A Systematic Review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Travassos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="316" to="329" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">Simultaneous Statistical Inference</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>second ed. McGraw-Hill</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Handbook of Parametric and Nonparametric Statistical Procedures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sheskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamhane</surname></persName>
		</author>
		<title level="m">Multiple Comparison Procedures</title>
		<imprint>
			<publisher>Wiley &amp; Sons</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of Power and Type I Error Rates of Scott-Knott&apos;s Test by the Method of Monte Carlo</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bearzoti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cincias Agrote´cnicas</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Power and Type I Errors Rate of Scott-Knott, Tukey and Newman-Keuls Tests under Normal and No-Normal Distributions of the Residues</title>
		<author>
			<persName><forename type="first">L</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revista de Matema ´tica e Estatı ´stica</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ScottKnott: A Package for Performing the Scott-Knott Clustering Algorithm in R</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jelihovschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The R J</title>
		<imprint/>
	</monogr>
	<note>article in press</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Assessing Insecticide and Fungicide Effects on the Culturable Soil Bacterial Community by Analyses of Variance of Their DGGE Fingerprinting Data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rumjanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Soil Biology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="466" to="472" />
			<date type="published" when="2009-12">Sept.-Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Influence of Dietary Oil Sources on Muscle Composition and Plasma Lipoprotein Concentrations in Nile Tilapia, Oreochromis Niloticus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murgas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. World Aquaculture Soc</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Design and Analysis of Experiments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Blom</surname></persName>
		</author>
		<title level="m">Statistical Estimates and Transformed Beta</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Function Points Analysis: An Empirical Study of Its Measurement Processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="895" to="910" />
			<date type="published" when="1996-12">Dec. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Software Metrics Data Analysis-Exploring the Relative Performance of Some Commonly Used Modeling Techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Macdonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Eng</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="316" />
			<date type="published" when="1999-12">Dec. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using Public Domain Metrics to Estimate Software Development Effort</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jeffery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wieczorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Seventh Int&apos;l Software Metrics Symp</title>
		<imprint>
			<biblScope unit="page" from="16" to="27" />
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating Software Project Effort Using Analogies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schofield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="736" to="743" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Comparative Study of Cost Estimation Models for Web Hypermedia Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Counsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="196" />
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Flexible Method for Software Effort Estimation by Analogy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ruhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Emran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="106" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving Analogy-Based Software Cost Estimation by a Method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mittas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Athanasiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date>Feb</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Comparison of Software Effort Estimation Techniques: Using Function Points with Neural Networks, Case-Based Reasoning and Regression Models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Finnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wittig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Desharnais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Systems and Software</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="289" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Probabilistic Model for Predicting Software Development Effort</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pendharkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="615" to="624" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Pattern Recognition Approach for Software Engineering Data Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="931" to="942" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Effort Estimation Using Analogy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int&apos;l Conf. Software Eng</title>
		<meeting>18th IEEE Int&apos;l Conf. Software Eng</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="170" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analogy-X: Providing Statistical Inference to Analogy-Based Software Cost Estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeffery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="484" />
			<date type="published" when="2008-08">July/Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Data Mining with Neural Networks and Support Vector Machines Using the R/rminer Tool</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Mining Application and Theoretical Aspects</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6171</biblScope>
			<biblScope unit="page" from="572" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Classification and Regression Trees, Wadsworth Int&apos;l Group</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<orgName type="collaboration">2. Insightful Corp</orgName>
		</author>
		<title level="m">SPLUS 6 for Windows, Guide to Statistics</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data Sets and Data Quality in Software Engineering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liebchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int&apos;l Workshop Predictor Models in Software Eng</title>
		<meeting>Fourth Int&apos;l Workshop Predictor Models in Software Eng</meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Analogy-X for Software Cost Estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Keung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Second ACM-IEEE Int&apos;l Symp. Empirical Software Eng. and Measurement</title>
		<imprint>
			<biblScope unit="page" from="294" to="296" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ellis</surname></persName>
		</author>
		<title level="m">The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Question of Scale Economies in Software-Why Cannot Researchers Agree?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Procedure for Analyzing Unbalanced Data Sets</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitchenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="278" to="301" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Special Issue on Repeatable Results in Software Engineering Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int&apos;l Joint Conf</title>
		<meeting>14th Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating the Error Rate of a Prediction Rule Improvement on Cross-Validation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="316" to="330" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
