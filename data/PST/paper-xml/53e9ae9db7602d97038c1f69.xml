<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiscale Image Segmentation Using Wavelet-Domain Hidden Markov Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hyeokho</forename><surname>Choi</surname></persName>
							<email>choi@ece.rice.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>77005-1892</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>77005-1892</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiscale Image Segmentation Using Wavelet-Domain Hidden Markov Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">19D35C22A1D80B1928DBDA1A5D5ACEDF</idno>
					<note type="submission">received October 26, 1999; revised May 31, 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hidden Markov tree</term>
					<term>segmentation</term>
					<term>texture modeling</term>
					<term>wavelets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new image texture segmentation algorithm, HMTseg, based on wavelets and the hidden Markov tree (HMT) model. The HMT is a tree-structured probabilistic graph that captures the statistical properties of the coefficients of the wavelet transform. Since the HMT is particularly well suited to images containing singularities (edges and ridges), it provides a good classifier for distinguishing between textures. Utilizing the inherent tree structure of the wavelet HMT and its fast training and likelihood computation algorithms, we perform texture classification at a range of different scales. We then fuse these multiscale classifications using a Bayesian probabilistic graph to obtain reliable final segmentations. Since HMTseg works on the wavelet transform of the image, it can directly segment wavelet-compressed images without the need for decompression into the space domain. We demonstrate the performance of HMTseg with synthetic, aerial photo, and document image segmentations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>partitions the image into regions that maximize the likelihood over the regions. Maximum a posteriori (MAP) segmentation in addition weights the likelihoods by the prior probability of each .</p><p>The two key ingredients to any segmentation scheme are 1) a description of the possible image regions and 2) a set of joint pixel pdfs . The primary difficulty in image segmentation arises because there are simply too many possible region shapes, and it is intractable to specify the joint pixel pdf for each possibility. Moreover, even if the joint density could be specified for each possible region shape, the cost of computing the optimal ML or MAP segmentation would be prohibitive. In practice, we must impose structures on both the possible image regions and on the pixel pdfs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiscale Image Segmentation</head><p>Many segmentation algorithms employ a classification window of some size in the hope that all pixels in the window belong to the same class. A typical segmentation then consists of classifying each window of pixels followed by some post-processing.</p><p>Clearly, the size of the classification window is crucial. A large window usually enhances the classification reliability (because many pixels provide rich statistical information) but simultaneously risks having pixels of different classes inside the window. Thus, a large window produces accurate segmentations in large, homogeneous regions but poor segmentations along the boundaries between regions. A small window reduces the possibility of having multiple classes in the window but sacrifices classification reliability due to the paucity of statistical information. Thus, a small window is more appropriate near the boundaries between regions.</p><p>To capture the properties of each image region to be segmented, both the large and small scale behaviors should be utilized to properly segment both large, homogeneous regions and detailed boundary regions. In multiscale segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> the results of many classification windows of different sizes are combined to obtain an accurate segmentation at fine scales.</p><p>In this paper, we will employ the dyadic squares (or blocks) to implement classification windows of different sizes. Given an initial square image of pixels, the dyadic squares are obtained simply by recursively dividing the image into four square subimages of equal size [see Fig. <ref type="figure" target="#fig_0">1(a)</ref>]. Since the four "child" squares nest inside their "parent" square at the next coarser scale, the dyadic squares have a convenient quad-tree structure; each node in the quad tree in Fig. <ref type="figure" target="#fig_0">1</ref>(b) corresponds to a dyadic square. Denote a dyadic square at scale  by (with an abstract index enumerating the squares at this scale). At the two extremes, (root of the tree) is the entire image , and each (leaf of the tree) is an individual pixel. Given a random field image , the dyadic squares are also random fields, denoted . In the sequel, when we speak of a generic square, we will often drop the .</p><p>With this structure for representing regions, we will segment images by estimating the class label for each dyadic square . This estimation requires a pixel pdf model for each class that is suited to the dyadic squares. Help is close at hand with the dyadic wavelet decomposition and wavelet-based statistical models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiscale Statistical Models and Wavelets</head><p>Models of different image textures play a fundamental role in image classification and segmentation, since the complete joint pixel pdf is typically overly complicated or unavailable in practice. Transform-domain models are based on the idea that often a linear, invertible transform will "restructure" an image, leaving transform coefficients whose structure is simpler to model. Most real-world images, especially gray-scale texture images, are well characterized by their singularity (edge and ridge) structure. The wavelet transform provides a powerful transform domain for modeling singularity-rich images <ref type="bibr" target="#b6">[7]</ref>.</p><p>The wavelet transform can be interpreted as a multiscale edge detector that represents the singularity content of an image at multiple scales and three different orientations. Wavelets overlying a singularity yield large wavelet coefficients; wavelets overlying a smooth region yield small coefficients. Four wavelets at a given scale nest inside one at the next coarser scale, giving rise to a quad-tree structure of wavelet coefficients that mirrors that of the dyadic squares [see Fig. <ref type="figure" target="#fig_1">2(a)</ref>]. In particular, with the Haar wavelet transform, each wavelet coefficient node in the wavelet quad-tree corresponds to a wavelet supported exactly on the corresponding dyadic image square.</p><p>In combination, the multiscale singularity detection property and tree structure imply that image singularities manifest themselves as cascades of large wavelet coefficients through scale along the branches of the quad-tree <ref type="bibr" target="#b6">[7]</ref>. Conversely, smooth regions lead to cascades of small coefficients.</p><p>This multiscale singularity characterization makes the wavelet domain natural for modeling texture images. A number of statistical models have been developed for modeling textures <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>; here we concentrate on the hidden Markov tree (HMT) model of Crouse et al. <ref type="bibr" target="#b11">[12]</ref>. The HMT approximates both the marginal and joint wavelet coefficient statistics. The HMT associates with each wavelet coefficient a (hidden) state variable that controls whether it is "large" or "small." The marginal density of each coefficient is then modeled as a two-density Gaussian mixture, using a large variance Gaussian for the large state and a small variance Gaussian for the small state. This Gaussian mixture closely matches the nonGaussian wavelet coefficient marginal statistics observed in natural images <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The HMT captures the persistence across scale of large/small coefficients using Markov-1 dependencies between the hidden states that chain across scale in a tree structure that parallels that of the wavelet coefficients and dyadic squares [see Fig. <ref type="figure" target="#fig_1">2(c)</ref>]. Grouping the model parameters (Gaussian mixture variances and Markov state transition probabilities) into a vector , the HMT can be viewedasahigh-dimensionalyethighlystructuredGaussianmixture model that approximates the overall joint pdf of the wavelet coefficients .</p><p>The computational efficiency of the wavelet transform carries over to HMT-based processing. The HMT model parameters can be estimated using the iterative expectation-maximization (EM) algorithm at a cost of computations per iteration <ref type="bibr" target="#b11">[12]</ref>. More importantly, given the wavelet transform of a test image and a set of HMT parameters , computation of the likelihood that is a realization of the HMT model requires only a simple upsweep through the HMT tree from leaves to root <ref type="bibr" target="#b11">[12]</ref>.</p><p>The HMT has a convenient nesting structure that matches that of the dyadic squares. Each subtree of the HMT is itself an HMT, with the HMT subtree rooted at node modeling the statistical behavior of the wavelet coefficients corresponding to the dyadic square . Serendipitously, the partial likelihood calculations obtained at intermediate scales of the HMT tree as part of the leaves-to-root upsweep give the likelihoods of each dyadic subsquare of the image under the HMT model (more details in Section IV-B).</p><p>These tools enable a simple multiscale image classification algorithm. Suppose that for each texture class we have specified or trained an HMT with parameters . Now, given the wavelet transform of an image consisting of a montage of these textures, applying the above multiscale likelihood calculation to each HMT yields the likelihoods , for each dyadic subimage . With the multiscale likelihoods at hand, the simplest ML classification <ref type="bibr" target="#b0">(1)</ref> then informs us of the most likely label for each dyadic subimage . This classification process, which we call the raw ML segmentation, can be completed in just computations for an -pixel image. It yields a set of different segmentations , , one for each different scale of dyadic square.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the process. After training HMT models on the grass and wood textures from Fig. <ref type="figure" target="#fig_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interscale Decision Fusion</head><p>While quick and easy, as Fig. <ref type="figure" target="#fig_2">3(d</ref>) attests, the raw ML segmentations suffer from the classical "blockiness versus robustness" tradeoff that leaves no single desirable. To obtain a high-quality segmentation, clearly we should combine the multiscale results to benefit from both the robustness of large block sizes and the resolution of small block sizes.</p><p>Since finer scale dyadic squares nest inside coarser scale squares, the dyadic squares will be statistically dependent across scale for images consisting of fairly large, homogeneous regions. Hence, (reliable) coarse-scale information should be able to help guide (less reliable) finer-scale decisions.</p><p>If the dyadic square was classified as class , then it is quite likely that its four children squares at scale belong to the same class, especially when is large (at fine scales). Hence, we will guide the classification decisions for the child squares based on the decision made for their parent square. This will tend to make the class labels of the four children the same unless their likelihood values strongly indicate otherwise, thus reducing the number of misclassifications due to slight perturbations in child likelihood values. In addition to the parent square, we can also use the neighbors of the parent to guide the decision process. Similar multiscale decision ideas have been successfully applied to document segmentation in <ref type="bibr" target="#b5">[6]</ref>.</p><p>To exploit the parent-child dependencies between the dyadic squares, we will build yet another tree-structured probability model, the labeling tree (more details in Section IV-C). Akin to the HMT, the labeling tree models the dependencies between dyadic squares across scale in a Markov-1 fashion, where the dyadic squares at scale are assumed to depend only on the squares at scale . (Dependencies between squares within the same scale are captured through the squares' common ancestors.) Using tree-based modeling, we gain tremendous "economies of scale" <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Markov modeling leads us to a simple scale-recursive classification of the dyadic squares, where we classify based on its likelihood and guidance from the previous scale . This Bayesian interscale decision fusion computes a MAP estimate of the class label of each dyadic square . Stopping the fusion at scale , we obtain the MAP segmentation . As we see from Fig. <ref type="figure" target="#fig_2">3(e)</ref>, multiscale decision fusion greatly improves the robustness and accuracy of the segmentation.</p><p>Combining the above tools results in a robust and accurate yet simple and efficient segmentation algorithm that we call HMTseg <ref type="bibr" target="#b17">[18]</ref>. It relies on three separate tree structures: the wavelet transform quad-tree, the HMT, and the labeling tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Related Work</head><p>We group related work in texture classification and segmentation into four broad classes.</p><p>Markov Random Fields: Markov random fields (MRFs) <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> have been extensively applied to model the pixel pdf . However, while they enable spatially local processing, they capture only local interactions and thus have only a limited ability to describe large scale behavior. MRF's can be improved by incorporating more neighboring pixels, but this rapidly increases their complexity. More recently, there have been attempts to approximate MRFs using tree structured models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Scaling Coefficient Models: Multiscale autoregressive models approximate the multiscale statistics of the scaling coefficients rather than wavelet coefficients <ref type="bibr" target="#b4">[5]</ref>. While the main advantage of multiscale image segmentation is to avoid the ad hoc choice of the classification window size, the divide-and-conquer segmentation algorithm of <ref type="bibr" target="#b4">[5]</ref> (and a similar one in <ref type="bibr" target="#b22">[23]</ref>) still requires a proper choice.</p><p>Wavelet-Domain Features: Wavelet-domain features are not new to texture classification and segmentation. In <ref type="bibr" target="#b23">[24]</ref>, Unser extracts parameters at different wavelet scales to facilitate texture classification. Li et al. <ref type="bibr" target="#b24">[25]</ref> employ wavelet coefficient statistics to classify different textures in document images. Gross et al. <ref type="bibr" target="#b25">[26]</ref> use a neural network to model the textural features of wavelet coefficients for classification purposes.</p><p>Multiscale Decision Algorithms: The multiscale labeling model of Bouman et al. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> does not use an explicit model for the image pixels. Rather it indirectly models the pixel pdf using a multiscale model of the class labels only. The technique in <ref type="bibr" target="#b3">[4]</ref> is a general systematic method for combining multiscale information. However, because it considers only the behavior of the class labels across scale without actually considering the joint statistics of the image pixels (it assumes that the pixels are independent given the class label), the algorithm is useful only for certain types of images (such as the SAR images considered in <ref type="bibr" target="#b3">[4]</ref>) in which pixel values can be considered independent. The algorithms recently proposed in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> generalize <ref type="bibr" target="#b3">[4]</ref> further. However, because these algorithms still do not perform direct modeling and decision of class labels at multiple scales, they require complicated statistical learning methods based on manually prepared training data. Furthermore, they still model the wavelet coefficients as independent, which is not accurate for singularity-rich data such as textures, because of the strong residual correlations between wavelet coefficients. In followup work to <ref type="bibr" target="#b24">[25]</ref>, Li et al. <ref type="bibr" target="#b5">[6]</ref> propose a divide-and-conquer multiscale decision algorithm that incorporates deterministic context information. While intuitively appealing, their proposed decision rules are deterministic and presented with little justification.</p><p>As far as we know, HMTseg is the first attempt to combine parametric wavelet-domain statistical modeling, direct likelihood calculation, and multiscale Bayesian decision fusion (via the labeling tree, which is inspired by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and <ref type="bibr" target="#b16">[17]</ref>). We believe that these features give HMTseg several distinct advantages over existing segmentation techniques. In particular, since we obtain the multiscale likelihoods and classifications directly through the HMTs, the multiscale information fusion simplifies considerably. As a result, unlike the algorithms in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we are able to extract the labeling tree parameters from the given image to be segmented, without additional training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Paper Organization</head><p>In Sections II and III, we study two of the basic ingredients of HMTseg: the wavelet transform and the wavelet HMT model.</p><p>We describe the third basic element, the labeling tree, and construct the algorithm in Section IV. Section V demonstrates the performance of HMTseg through a number of examples. We conclude in Section VI by pointing to some remaining issues and suggesting directions for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WAVELET TRANSFORM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wavelet Transform and Dyadic Squares</head><p>The wavelet transform represents the singularity content of an image at multiple scales. There are several different interpretations; we will find the pyramidal multiscale construction for discrete images cleanest for our purposes <ref type="bibr" target="#b26">[27]</ref>.</p><p>We will focus on the simplest wavelet transform, that of Haar. The construction of Haar wavelet coefficients of an image can be explained using four The filtering and downsampling process can now be continued on the image and the procedure iterated up to times [see Fig. <ref type="figure" target="#fig_1">2(a)</ref>].</p><p>The scaling coefficient matrices , are progressively smoothed versions of the original image . The wavelet coefficient matrices , , and are high-and band-pass filtered, edge-detected, versions of the image that respond strongly to edges in the horizontal, vertical, and diagonal orientations, respectively. For example, the wavelet coefficient , , is large if the image block contains a horizontal edge and small otherwise.</p><p>The iterative computation of each Haar wavelet coefficient from a block in a finer-scale image leads naturally to a quad-tree structure on the wavelet coefficients in each subband, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>(a) and (b) <ref type="bibr" target="#b27">[28]</ref>. First assume that we carry out the iterated filtering to scale , and consider only the subband. Then the root of the tree lies at and the leaves at , . As we move down the tree, we move from coarse to fine scale, adding details as we go. More specifically, each parent wavelet coefficient analyzes the same region in the original image as its four children , , , and</p><p>. Coefficients on the path to the root are ancestors; coefficients on the paths to the leaves are descendents. If we terminate the iterated filtering at a scale , then there will be more than one coarsest scale wavelet coefficient in each subband, leading to a forest of quad-trees in each subband <ref type="bibr" target="#b11">[12]</ref>.</p><p>To keep the notation manageable in the sequel, let denote the collection of all wavelet coefficients, and let , , denote the collections of all coefficients in the respective subbands. Let denote a generic wavelet coefficient, with the subband under consideration determined by context. In our statistical modeling framework, we will regard as a realization of the random variable and as a realization of the wavelet random field . Define by the scale of coefficient in the subband quad-tree. Define as the parent of tree node . In a given subband, define as the subtree of wavelet coefficients with root node ; that is, contains coefficient and all of its descendents [see Fig. <ref type="figure" target="#fig_1">2(b)</ref>].</p><p>With the 2-D Haar wavelet transform, there is an obvious correspondence between the wavelet coefficients and the dyadic squares [recall Fig. <ref type="figure" target="#fig_0">1(a)</ref>], which are obtained by iteratively dividing the image into equal-size quadrants. Recall that denotes a dyadic square at scale , with an abstract index for the square. (In the sequel, superscripts will always denote scale and subscripts will always denote position within a scale.) Each is obtained by dividing a square at scale (the "parent" square, ) into four quadrants (the "child" squares). To each dyadic square of pixels there corresponds a unique set of wavelet coefficients with a special property: all wavelet coefficients in the subtree rooted at depend exclusively on the pixel values in .</p><p>The same procedure of wavelet transform construction can be applied to other wavelet systems besides the Haar. While larger wavelet filters are more appropriate for representing smooth images, the Haar system is more appropriate for our purpose of classifying dyadic squares, due to its direct connection with the dyadic squares. We will see that the Haar system is more than adequate for the HMTseg algorithm.</p><p>Because each wavelet coefficient is computed locally, the wavelet transform efficiently represents with large coefficients only the edges of images, resulting in a very sparse and compact representation of singularity-rich images <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Furthermore, the approximate decorrelation of wavelet coefficients <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref> enables us to use very simple local modeling of joint statistics between wavelet coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TWO-DIMENSIONAL HIDDEN MARKOV TREE MODEL</head><p>The wavelet hidden Markov tree (HMT) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref> models both the nonGaussian marginal pdfs of the wavelet coefficients and the persistence of large/small coefficients across scale. As indicated in <ref type="bibr" target="#b11">[12]</ref>, HMT models can be developed for wavelet transforms of any dimensionality; here we focus on 2-D images and thus quad trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling the Marginal Distribution</head><p>The energy compaction property <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref> of the wavelet transform implies that the transform of most real-world images consists of a small number of large coefficients and a large number of small coefficients. We can consider the population of large coefficients as outcomes of a pdf with a large variance. Similarly, we can consider the collection of small coefficients as outcomes of a pdf with a small variance. Hence, the pdf of each wavelet coefficient is well approximated by a two-density Gaussian mixture model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. <ref type="foot" target="#foot_1">2</ref>To each wavelet coefficient , we associate a discrete hidden state that takes on the values , signifying the small and large variance, with probability mass function <ref type="bibr">(pmf)</ref> . Conditioned on , is Gaussian with mean and variance . Thus, the overall pdf of is given by S L</p><p>(2) where and . In Fig. <ref type="figure" target="#fig_1">2(c</ref>) we depict the wavelet coefficients using black nodes and their associated hidden states using white nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modeling Across-Scale Dependencies</head><p>Working in the Gaussian mixture marginal distribution framework, we can characterize the dependencies between the wavelet coefficients by specifying the joint pmf of the hidden states. Thanks to the approximate decorrelating power of the wavelet transform <ref type="bibr" target="#b6">[7]</ref>, the most important correlations are the parent-child interactions due to magnitude persistence across scale <ref type="bibr" target="#b11">[12]</ref>. <ref type="foot" target="#foot_2">3</ref>To capture the scale persistence of coefficient magnitudes, we connect the hidden states in a directed Markov-1 probabilistic graph <ref type="bibr" target="#b11">[12]</ref>. For each parent-child pair of hidden states , the state transition probabilities for represent the probability for to be small/large when its parent is small/large. For each , we thus have the state transition probability matrix </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The 2-D HMT Model</head><p>A complete 2-D image wavelet transform comprises three subbands with three parallel quad-tree structures [as in Fig. <ref type="figure" target="#fig_1">2(b)</ref>]. In particular, node in the , , and quad trees corresponds to the same dyadic square in the image. While the three subbands are clearly dependent on each other, for tractability reasons, we assume the following.</p><p>Subband Independence Assumption: The three subbands of the 2-D wavelet transform are statistically independent.</p><p>The complete 2-D wavelet HMT model consists of three HMT's [one for each wavelet subband as shown in Fig. <ref type="figure" target="#fig_1">2(c)</ref>]. Denoting the parameter vectors for the three subband HMTs as and , respectively, we have . The HMT is thus a parametric model (multidimensional Gaussian mixture) for the joint pdf of the wavelet coefficients. Under the subband independence assumption, we can write <ref type="bibr" target="#b2">(3)</ref> In addition to modeling the wavelet coefficients, we can separately model the scaling coefficients-using a mixture density, for example <ref type="bibr" target="#b11">[12]</ref>. However, for HMTseg, we will intentionally ignore the scaling coefficients. Since the scaling coefficients equal local pixel averages, we thus build into our statistical modeling independence to the local brightness level. This is a desirable feature for many segmentation applications, because even in regions of homogeneous statistical properties, the local brightness often varies in different parts of the region.</p><p>As it stands, the HMT has a large number of parameters (approximately for an -pixel image). This can make model training difficult when only a small number of training images are available. Fortunately, wavelet coefficients at the same scale tend to exhibit similar statistical characteristics <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>; thus we can use just one set of parameters for each scale. This nodal tying reduces the number of parameters considerably (to approximately for a -scale transform), avoiding the risk of overfitting the model <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTISCALE SEGMENTATION USING HMT MODELS</head><p>We now describe the HMTseg algorithm's HMT training, fast HMT-based likelihood calculation, and multiscale Bayesian decision fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training the HMT Models</head><p>Before we begin, we must acquire training data representative of each texture and train an HMT model for each. We typically obtain training images either by picking homogeneous regions from the given image or from completely different images having homogeneous regions representative of the candidate textures. For each class , we train a 2-D wavelet HMT model using the iterative expectation-maximization (EM) algorithm <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>The EM algorithm finds the locally optimal (in the ML sense) set of model parameters for a given set of training data. In each iteration, the E step defines a likelihood surface based on the current parameters. The M step then updates the parameters to maximize the likelihood that the training data came from the model <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The EM algorithm derived for 1-D HMT models in <ref type="bibr" target="#b11">[12]</ref> applies with little modification in 2-D if we interpret the parent-child relations between nodes appropriately for quad-trees. <ref type="foot" target="#foot_3">4</ref> In the HMT, each EM iteration consists of an up/down sweep through the tree [ cost for wavelet coefficients].</p><p>To maximize the spatial shift invariance of HMTseg, we can either use intra-scale tying (using the same mixture variances and state transition matrices for all wavelet coefficients at the same scale) as proposed in <ref type="bibr" target="#b11">[12]</ref> or prepare a training set containing different shifts of the training textures. In our experiments, we have found only minimal performance reduction (but significant parameter reduction) using intra-scale tying. Furthermore, in all of our experiments, reliable HMT training required fewer than ten training images.</p><p>Given training images of pixels each, the total computational cost per EM iteration is . While the total training cost can be enormous for large and , we have several avenues for reducing the amount of computation. First, intra-scale tying significantly reduces both the number of parameters to train and the number of required training images . Second, we note that coarse-scale wavelet coefficients correspond to large dyadic squares that likely contain a number of different textures. Since these squares supply little information for segmentation, we can clip the upper scales from the HMT, creating a forest of smaller HMTs. These smaller HMT's naturally share the same parameters and thus reduce the size of our required training images (more on this in Section IV-E).</p><p>While EM training is notorious for its slow convergence, in our experiments (including the two examples in Section V) we reached convergence in fewer than twenty iterations using an intelligent parameter initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiscale Likelihood Computation</head><p>Given a set of 2-D HMT model parameters and the wavelet transform of a test image, it is straightforward to compute the likelihood that the image was generated by the model <ref type="bibr" target="#b11">[12]</ref>. Furthermore, thanks to the dyadic multiscale structure of the wavelet transform and the HMT, we can obtain the likelihoods of all dyadic squares of the image simultaneously in a single upward sweep through the tree [a fast algorithm].</p><p>Consider first the likelihood calculation for a subtree of wavelet coefficients rooted at in one of the subbands <ref type="bibr" target="#b11">[12]</ref>. Suppose this subband has HMT parameters . Given the conditional likelihoods obtained by sweeping up the quad-tree from the leaves to node (see <ref type="bibr" target="#b11">[12]</ref>), the likelihood of the coefficients in can be computed as Now recall the connection with the dyadic squares. It is easy to see that the wavelet coefficients of the square consist of the triple , each a subtree of one of the three wavelet subband quad-trees. Using three HMT upsweeps, we can easily compute the likelihood (4) for each of these subtrees. Then, using the subband independence assumption, we have <ref type="bibr" target="#b4">(5)</ref> Using ( <ref type="formula">5</ref>), we can compute the likelihood under each texture model of each dyadic square down to block scale.</p><p>Direct block-by-block comparison of the likelihoods (1) computed using (5) yields the ML raw segmentations at a range of scales [recall Fig. <ref type="figure" target="#fig_2">3(d)</ref>]. (We discuss how to carry this down to pixel-sized blocks in Section IV-D.) We refer to this block-byblock classification as "raw," because we do not exploit any possible relationships between the classifications at different scales. We expect the raw decisions to be more reliable at coarser scales (where we have more image pixels per block) but more finely localized at finer scales (where the blocks are smaller). Clearly it is in our best interests to overcome this blockiness versus robustness tradeoff by folding the coarse-through-fine likelihoods into our final segmentation recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Context-Based Interscale Fusion</head><p>We can improve the raw segmentation considerably by considering the dependencies between the class decisions at different scales. We will do this by modeling the multiscale dependencies between the dyadic blocks. Our approach is inspired by the Bayesian multiscale segmentation framework of Bouman et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>1) Bayesian Segmentation: In a Bayesian segmentation framework, we treat each class label as a random variable taking a value from . Given the posterior distribution of given the image , the MAP classification of dyadic square corresponds to the class label that maximizes the posterior distribution <ref type="bibr" target="#b5">(6)</ref> By Bayes rule, the posterior is given by <ref type="bibr" target="#b6">(7)</ref> Let denote the collection of all dyadic squares at scale ; note that each contains complete information on the image . A posterior equivalent to (7) is thus <ref type="bibr" target="#b7">(8)</ref> Since computation and maximization of ( <ref type="formula">8</ref>) is intractable in practice, we will perform a succession of manipulations and simplifications to arrive at a practical MAP classifier. Just as the HMT models the pdfs and by echoing the structure of the wavelet coefficient quad tree, we will construct a probabilistic tree to model the posterior (8) based on the dyadic square quad tree of Fig. <ref type="figure" target="#fig_0">1(b)</ref>. The resulting labeling tree model will capture the interscale dependencies between dyadic blocks and their class labels and enable a multiscale Bayesian decision fusion. There are many ways to capture these multiscale dependencies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>; here we outline one possible approach that balances accuracy with tractability.</p><p>2) Image Model with Hidden Class Labels: Rather than modeling the joint statistics of the dyadic squares directly, we will model the statistics of the associated class labels . We assume that class label controls the textural properties of square ; that is, each is generated based on the distribution independently of all other and , . Let denote the collection of all class labels at scale . Then, given , all are independent (9)</p><p>The class labels play a role analogous to the hidden states in the HMT (recall that given the values of the states, all wavelet coefficients are independent <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>).</p><p>Under the conditional independence of the s given , our MAP classification problem transforms to maximizing the posterior [recall <ref type="bibr" target="#b7">(8)</ref>], the marginal of (</p><p>Here we have used <ref type="bibr" target="#b8">(9)</ref>. Unfortunately, marginalizing <ref type="bibr" target="#b9">(10)</ref> to obtain (8)-integrating out all , -for the MAP decision statistic is difficult in general, unless the joint distribution has a special structure.</p><p>3) Multiscale Prior and Contexts: To simplify the determination and marginalization of the joint prior distribution in <ref type="bibr" target="#b9">(10)</ref>, we assume that the joint distribution of the class labels at scale is completely determined by the at the previous coarser scale. Combined with the assumption that is conditionally independent of all given , we have that , , and form a Markov chain: . This heuristic models the interscale dependencies between the class labels that we motivated in Section I-D. Thus, given , the s at scale are independent, and we can write the multiscale prior distribution <ref type="bibr" target="#b10">(11)</ref> Due to the high dimensionality of the conditioning vector , estimating the marginalized class prior distribution still requires a prohibitive amount of training data. Contexts provide a useful further simplification of (11) <ref type="bibr" target="#b32">[33]</ref>. To each dyadic square with hidden class label , we assign the (deterministic) context vector , which is formed from information about the . The triple forms a Markov-1 chain [see Fig. <ref type="figure" target="#fig_6">4(a)</ref>]. That is, encodes sufficient information regarding such that, given its value, we can treat and as independent of all other and . If is chosen as a discrete vector of small dimension, then it simplifies the modeling considerably. In the multiscale prior model ( <ref type="formula">11</ref>), we let be a func-tion of the . Let be the collection of all contexts at scale .</p><p>The choice of a good context model is crucial to the performance of HMTseg. We have a trade-off between the complexity of the context and the accuracy of the model. Among many candidate contexts, we can determine the effective contexts based on known training data. In some sense, the decision-tree based algorithm in <ref type="bibr" target="#b15">[16]</ref> is a general form of the context-based fusion algorithm applicable when sufficient training data is available for reliable estimation of the decision parameters.</p><p>Contexts allow us to write</p><p>Since is independent of given (by the Markov-1 property), conditioning <ref type="bibr" target="#b9">(10)</ref> on the contexts yields <ref type="bibr" target="#b12">(13)</ref> and the marginalized, context-based posterior <ref type="bibr" target="#b13">(14)</ref> This is a greatly simplified version of the MAP posterior <ref type="bibr" target="#b6">(7)</ref> for use in the MAP equation <ref type="bibr" target="#b5">(6)</ref>. Here, the are the likelihoods of the dyadic square given the different class values , which are computed using an HMT likelihood upsweep on each texture model. The prior supplies information on the provided by the s through . 4) Context Labeling Tree: The interscale dependency modeling between the class labels (11) yields a tree of class labels, where the dependencies march down the tree in a Markov fashion. Compared with dependency modeling at each individual scale (with, say, a MRF), causal tree-based dependency modeling is both simple and effective.</p><p>While each context is potentially a function of all at scale , here we will employ a simplified tree organization: each at scale will receive information from nine scale class labels, the parent label plus the parent's eight nearest neighboring [see Fig. <ref type="figure" target="#fig_6">4(b)</ref>]. We term this context organization the context labeling tree. The limit of coarser scale information to just nine blocks is easily justified by noting that will receive information from a region of pixels centered around and 36 times larger than its square .</p><p>While the context choice is very general and we may need a very complex context to accurately summarize the information conveyed by the , over-complicated contexts run the risk of context dilution, especially with insufficient training data <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>. 5  Inspired by the success of hybrid tree model in <ref type="bibr" target="#b3">[4]</ref>, we will employ a simple context structure in HMTseg. Each context vector will contain two entries: the value of the class label of the parent square (which will be a MAP estimate in practice) and the majority vote of the class labels of the parent plus its eight neighbors. Given different textures, each context can take on different values. Let the number of different values can take be ( in the algorithm); thus, . While this simple context is ad hoc, it is more than sufficient for demonstrating the effectiveness of multiscale decision fusion. Furthermore, context training can be accomplished reliably based on the given image only, without requiring extra training data for estimating the context related probabilities.</p><p>Since the s at scale depend on the s from scale , we will evaluate and maximize ( <ref type="formula">14</ref>) in a multiscale, coarse-to-fine manner to fuse the HMT likelihoods (precomputed as in Section IV-B) using the labeling tree prior . Our fusion will pass the MAP decisions down through scale to aid the segmentation of fine scale dyadic squares. The result is simple, yet effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Interscale Fusion EM Algorithm:</head><p>The fusion proceeds as follows. Start at a coarse enough scale such that the ML raw segmentations are statistically reliable. Use these and all coarser ML decisions as the MAP decisions . This is entirely reasonable; at coarse scales (large dyadic squares), the next coarser scale (very large dyadic squares) provides little prior information for segmentation. Now move down to the next finer level . Fix the context values from the at scale (from the parent label and its eight nearest neighbors). We are given the likelihood in ( <ref type="formula">14</ref>) from the HMT likelihood computation step. Hence, after computing , we can choose the label for that maximizes the product <ref type="bibr" target="#b13">(14)</ref>. To compute , we use an ML estimate averaged over the collection of all dyadic squares at scale . Since this collection is precisely the image , we can write (by the chain rule of conditioning) <ref type="bibr" target="#b14">(15)</ref> Here we sum over the candidate textures and use the fact that all blocks at the same scale are independent given the contexts . Because represents the relation between the context and the class label, it is reasonable to assume that it is same for all within each scale. The ML estimate of is that which maximizes the likelihood of the image given the s [given in <ref type="bibr" target="#b14">(15)</ref>]. Maximizing <ref type="bibr" target="#b14">(15)</ref> is possible because the likelihoods are already available from the multiscale 5 Effective contexts can be selected from a library of possible contexts using a classification algorithm such as that proposed in <ref type="bibr" target="#b15">[16]</ref>, provided that sufficient manually prepared training data are available.</p><p>HMT likelihood computations. Note that is chosen in the ML sense by averaging over the entire image in <ref type="bibr" target="#b14">(15)</ref>.</p><p>The EM algorithm again comes to our rescue; in fact, we can use it to compute and maximize the posterior ( <ref type="formula">14</ref>) directly. We do not specify directly, but rather specify and apply Bayes rule <ref type="bibr" target="#b15">(16)</ref> Assuming these probabilities to be constant at each scale, set <ref type="bibr" target="#b16">(17)</ref> for all in scale and , . The set of probabilities can be computed using an EM algorithm on the context labeling tree (see the Appendix for details). The context-based Bayes classification then finds the class label that maximizes the contextual posterior distribution from <ref type="bibr" target="#b13">(14)</ref> [see <ref type="bibr" target="#b17">(18)</ref> in the Appendix].</p><p>While EM iterations are necessary at each scale to estimate the fusion parameters and , we note that the algorithm converges rapidly with the initial parameters set to the values estimated in the previous coarser scale. This is because the parameters change little from scale to scale, especially at fine scales where EM iterations are more expensive. Furthermore, at very fine scales, we can actually use the fusion parameters estimated in the coarser scales without re-estimation. This is particularly helpful when the likelihoods at very fine scales are less robust and maximizing in <ref type="bibr" target="#b14">(15)</ref> does not give the desired s. We employed this technique in the document segmentation example of Section V-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pixel-Level Segmentation</head><p>Since the Haar wavelet HMT characterizes the joint statistics of the dyadic image squares only down to blocks, we do not directly obtain pixel-level segmentations. While the collection of all wavelet and scaling coefficients completely characterizes the original image, the HMT subband independence assumption and the fact that we ignore the scaling coefficients limit our reach to blocks. Pixel-level segmentation requires a model for the pixel brightness of each texture class. However, obtaining an appropriate model can be difficult, since in many images the local brightness varies considerably due to shading, etc. For such images, the block segmentations will be far more robust, since they rely on inter-pixel dependencies and not local brightness.</p><p>Pixel brightness corresponds to the pdf of a single pixel. For our purposes, we fit a Gaussian mixture to the pixel values for each training texture. We can then compute the likelihood of each pixel and extend the above interscale scale fusion algorithm from blocks to the pixel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Issues</head><p>As described above, the interscale fusion algorithm starts at the root node of the context labeling tree and descends to the finest scale to combine all possible coarse scale information. However, at very coarse scales, the likelihoods of the dyadic squares do not contain significant information, since the squares are large and hence are likely to contain several differently textured regions. When fusing multiscale classification results, we therefore ignore the information at very coarse scales.</p><p>Ignoring the coarsest scales has the side benefit. As described in Section IV-A, reducing the size of the HMT reduces the computation required for training and likelihood determination. If we start fusing at scale , then we only need the wavelet coefficients, HMT models, and likelihoods at scales . With the Haar transform, starting at scale is equivalent to dividing the image into the dyadic squares and then performing the HMT likelihood computation independently on each of these squares. This saves a considerable amount of computation and reduces the size of the required homogeneous training images to . In practice, we set the starting scale such that the coarsest raw segmentations have sufficient reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Summary of HMTseg Algorithm</head><p>The final segmentation algorithm consists of three steps; HMT training, multiscale likelihood computation, and multiscale fusion.</p><p>HMTseg Algorithm 1) Train wavelet-domain HMT models for each texture using homogeneous training images. To obtain pixel-level segmentation, also train a pixel brightness pdf model. 2) Compute multiscale likelihoods. Using the likelihood computation algorithm for the HMT model <ref type="bibr" target="#b11">[12]</ref> and (4), compute the likelihood of each dyadic image square at each different scale. This gives the likelihoods for each dyadic square. If the trained HMT model is smaller than the test image, repeat the likelihood computations for image subblocks assuming that the blocks are independent (see Section IV-E). 3) Fuse multiscale likelihoods using the labeling tree to form the multiscale MAP classification. Pick the starting scale such that the ML classifications of the s at scale are reliable enough to obtain the s. Estimate the parameters and to maximize in (15) using the EM algorithm in the Appendix. Each EM iteration updates the contextual posterior distribution . When converged, determine the that maximizes . Continue fusion at scale based on the formed using the s obtained at scale . Continue the process for all scales until the finest scale is reached.</p><p>V. EXAMPLES Fig. <ref type="figure" target="#fig_2">3</ref> demonstrated the HMTseg process on a synthetic data example. Here we illustrate two real-world image segmentation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Aerial Photo Segmentation</head><p>We trained wavelet HMTs for "sea" and "ground" textures using hand-segmented blocks from the aerial photo <ref type="bibr" target="#b13">[14]</ref> in Fig. <ref type="figure" target="#fig_7">5</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Image x x x divided into dyadic squares d d d at different scales. Each dyadic square can be associated with a subtree of Haar wavelet coefficients. (b) Quad-tree structure of dyadic squares. The dyadic square d d d splits into four child squares at scale j.</figDesc><graphic coords="2,71.04,62.28,451.20,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Parent-child dependencies of the three 2-D wavelet transform subbands: Each arrow points from a parent wavelet coefficient to its four children at the next finer scale. (b) More detailed view of the quad-tree structure for one subband. Each black node corresponds to a wavelet coefficient. The figure also illustrates our tree indexing notation: T is the subtree of coefficients rooted at node i, and (i) is the parent of node i. (c) A 2-D wavelet hidden Markov tree (HMT) model for one subband. We model each wavelet coefficient (black node) as a Gaussian mixture controlled by a hidden state variable (white node). To capture the persistence across scale property of wavelet transforms, we connect the states vertically across scale in Markov-1 chains.</figDesc><graphic coords="2,66.30,185.82,460.80,105.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. HMTseg applied to a synthetic test image. (a) A 512 2512 grass texture image [14]. (b) A 5122 512 wood texture image [14]. (c) A 64 264 grass/wood mosaic test image x x x to be segmented. (d) Raw HMT-based multiscale classifications ĉ c c of x x x for 828, 424, 222, and pixel-sized dyadic squares. Classification accuracy increases with block size (toward coarser scales), because more statistical information is available for the class label decision. However, this comes at a cost of reduced boundary resolution. (e) Final segmentations ĉ c c</figDesc><graphic coords="3,61.50,62.25,468.00,417.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) and (b), we per-formed the multiscale classification (1) on the test image (c) to obtain the raw segmentations (d) at various scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For typical gray-scale images, we expect S S and L L to be large due to the persistence property.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>directly from (or computed during training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Context, class label, and dyadic square form a Markov-1 chain: v v v ! C ! D D D . (b) Context labeling tree. The context of the child square is</figDesc><graphic coords="8,134.88,62.28,323.52,100.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Aerial photo segmentation using HMTseg. (a) A 1024 2 1024 aerial photo and (b) 256 2 256 test subimage x x x. The homogeneous ground/sea regions outside the region (b) were used to train two HMT's. (c) Raw HMT-based multiscale classifications c c c of x x x for 8 2 8, 4 2 4, 2 2 2, and pixel-sized dyadic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Document segmentation using HMTseg. (a) A 512 2 512 training image was hand-segmented, and homogeneous regions were used to train HMTs for text, image, and background textures. (b) A 512 2 512 test image x x x. (c) Raw HMT-based multiscale classifications c c c of x x x for 8 2 8, 4 2 4, 2 2 2, and pixel-sized dyadic squares. Black, gray, and white represent text, image, and background, respectively. Classification accuracy clearly decreases at fine scales. (d) Final segmentations c c c using Bayesian context-based interscale fusion correctly classify even the angled text on the books. Adding a fourth class (large text) would allow us to correctly classify the text at the bottom of x x x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a). For training data, we extracted homogeneous ground [upper-left corner of Fig. 5(a)] and sea [lowerright corner of Fig. 5(a)] images. Then, from each image, we randomly picked ten (overlapping) blocks. With this training data and intra-scale tying in the HMT models, the EM training algorithm converged in less than 15 iterations.Choosing for the starting scale (corresponding to 6-scale quad-trees on image blocks), we segmented the test image in Fig.5(b). Fig.5(c) shows the raw classification results. Pixel-level raw segmentation was obtained using 2-density Gaussian mixture models for pixel brightness of the ground and sea textures. Fig.5(d) illustrates the segmentation resulting from coarse-to-fine interscale fusion. Except for some segmentation</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We denote deterministic quantities using small letters, random variables using capital letters, and vectors using boldface letters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We can use more than two mixture densities to provide a fit to the actual f (w ) with any desired fidelity. In practice, however, we have seen no real performance benefit to using more than two.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p><ref type="bibr" target="#b2">3</ref> While the HMT focuses on across-scale dependencies, it does not ignore within-scale dependencies. Correlations between coefficients at the same scale are modeled through their mutual ancestors in the HMT quad tree.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The only necessary change is in [12, p. 900, Eq. (22)], where the product now covers four child coefficients on the quad tree rather than two children on the binary tree. For more information on probabilistic graphs and training algorithms, see<ref type="bibr" target="#b31">[32]</ref>.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the NSF under Grants MIP-9457438 and CCR-9973188, by DARPA/AFOSR Grant F49620-97-1-0513, the ONR under Grant N00014-99-1-0813, and the Texas Instruments Leadership University Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>errors in the upper middle part of the image (caused by the ground there having a concrete texture more resembling sea), we observe excellent segmentation results at all scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Document Segmentation</head><p>We trained HMT and pixel brightness models for "text," "image," and "background" textures using hand-segmented blocks from the document in Fig. <ref type="figure">6</ref>(a) <ref type="bibr" target="#b33">[34]</ref>. Again, we randomly picked ten homogeneous regions for each texture from Fig. <ref type="figure">6</ref>(a) as training data set. The EM trainings of the models converged within 20 iterations.</p><p>Choosing for the starting scale (corresponding to 6-scale quad-trees on image blocks), we segmented the test image in Fig. <ref type="figure">6</ref>(b). Fig. <ref type="figure">6</ref>(c) shows the raw classification results. As expected, we observe many classification errors. The pixel-level segmentation, in particular, is not reliable (all text was classified as imagery). Fig. <ref type="figure">6(d</ref>) illustrates the segmentation resulting from coarse-to-fine interscale fusion. All text regions are segmented well, including the text surrounded by images on the books. At the bottom, we observe the large-font title text segmented as imagery. This is because the homogeneous texture inside each large letter has properties more similar to imagery than (small-font) text. The background regions are correctly segmented, even though the brightness of the background varies in different areas and is corrupted by a noise-like feature caused by text on the reverse side of the page. In the fusion step, we estimated the fusion parameters only down to block scale, since incorrect pixel likelihoods make the estimation unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we have developed a new framework for multiscale Bayesian image segmentation based on wavelet-domain HMT models. By concisely modeling and fusing the statistical behavior of textures at multiple scales, the HMTseg algorithm produces a robust and accurate segmentation of texture images. HMTseg yields not one final segmentation but a range of segmentations at different scales.</p><p>While we have illustrated with an aerial photograph and a document image, HMTseg can be applied to many different image types, including radar/sonar images <ref type="bibr" target="#b34">[35]</ref> and medical images. Furthermore, because the HMT modeling framework extends trivially to higher-dimensional data, we can employ HMTseg to segment multidimensional data such as geophysical surveys. One-dimensional signals, such as speech and geophysical well-logs, are also within HMTseg's purview.</p><p>As an added bonus, HMTseg has the potential to segment wavelet-compressed data directly without re-expanding to the space domain. HMTseg thus provides a natural vehicle for developing joint segmentation/compression algorithms <ref type="bibr" target="#b35">[36]</ref>.</p><p>Promising avenues for future HMTseg research include the investigation of wavelet basis representation different from Haar, simplified universal HMT modeling <ref type="bibr" target="#b29">[30]</ref>, more accurate (but complicated) interscale fusion algorithms, and the analysis of multiscale classification errors <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX EM ALGORITHM FOR CONTEXT LABELING TREE</head><p>Our goal is to find maximizing in <ref type="bibr" target="#b14">(15)</ref>. We precompute the conditional likelihoods for all using (5) by sweeping up the HMTs from the leaves to node <ref type="bibr" target="#b11">[12]</ref>. Recall the definitions of , , and from <ref type="bibr" target="#b16">(17)</ref>. The EM algorithm runs as follows.</p><p>Initialize: Set and choose . (A natural choice for is the set of parameters obtained in the previous, next coarser scale.)</p><p>Expectation (E): Given , calculate (using Bayes rule) ( <ref type="formula">18</ref>)</p><p>Maximization (M): Update the elements of <ref type="bibr" target="#b18">(19)</ref> for each <ref type="bibr" target="#b19">(20)</ref> Iterate: Increment and apply and until converged.</p><p>Hyeokho Choi (M'98) was born in Korea in 1969. He received the B.S. degree in control and instrumentation engineering (summa cum laude) from Seoul National University, Seoul, Korea, in 1991, and the M.S. and Ph.D degrees in electrical engineering from the University of Illinois at Urbana-Champaign, in 1993 and 1998, respectively. His research was in the area of computed imaging systems and signal processing.</p><p>Since January 1998, he has been at Rice University, Houston, TX, where he is currently a Research Professor with the Department of Electrical and Computer Engineering. His current research interests lie in the area of statistical signal processing, pattern recognition, wavelet theory, and imaging systems. In 1986, he was a Research Engineer with Omron Tateisi Electronics, Kyoto, Japan. After spending 1992-1993 with the Signal Processing Laboratory of Ecole Normale Supérieure, Lyon, France, he joined Rice University, Houston, TX, where he is currently a Professor of electrical and computer engineering. He spent Autumn 1998 at the Isaac Newton Institute, Cambridge University, Cambridge, U.K., as the Rosenbaum Fellow. His research interests lie in the area of signal and image processing and include wavelets, probabilistic models, networks, and time-frequency analysis. He serves on the editorial board of Applied and Computational Harmonic Analysis.</p><p>Dr </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph., Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="100" to="132" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An estimation-theoretic approach to terrain image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Therrien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph., Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of noisy and textured images using Gibbs random fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Derin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern. Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="1987-01">Jan. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multiscale random field model for Bayesian image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiscale segmentation and anomaly enhancement of SAR imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fosgate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Krim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="20" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiscale image classification by hierarchical modeling with two dimensional hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1826" to="1841" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<title level="m">A Wavelet Tour of Signal Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical models for images: Compression, restoration and synthesis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Asilomar Conf. Signals, Systems, Computers</title>
		<meeting>31st Asilomar Conf. Signals, Systems, Computers<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian approach to best basis selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pesquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Krim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hamman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing &apos;96</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal essing &apos;96<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="2634" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wavelet thresholding via a Bayesian approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Abramovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sapatinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="725" to="749" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of multiresolution image denoising schemes using generalized-Gaussian priors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Signal Processing Int. Symp. Time-Frequency Time-Scale Analysis</title>
		<meeting>IEEE Signal essing Int. Symp. Time-Frequency Time-Scale Analysis<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-09">Oct. 6-9, 1998</date>
			<biblScope unit="page" from="633" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelet-based statistical signal processing using hidden Markov models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="886" to="902" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive Bayesian wavelet shrinkage</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The usc-sipi image database</title>
		<ptr target="http://sipi.usc.edu/services.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tree approximations to Markov random fields</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Doerschuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="402" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Trainable context model for multiscale segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing &apos;98</title>
		<meeting>IEEE Int. Conf. Image essing &apos;98<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04-07">Oct. 4-7, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiscale document segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T 50th Annu. Conf</title>
		<meeting>IS&amp;T 50th Annu. Conf<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">May 18-23, 1997</date>
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image segmentation using wavelet-domain classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Math. Modeling, Bayesian Estimation, Inverse Problems</title>
		<meeting>SPIE Conf. Math. Modeling, Bayesian Estimation, Inverse Problems<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<biblScope unit="volume">3816</biblScope>
			<biblScope unit="page" from="306" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial interaction and the statistical analysis of lattice systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="192" to="225" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification of textures using Gaussian Markov random fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="959" to="963" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiresolution Gauss-Markov random field models for texture segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical stochastic modeling of SAR imagery for segmentation/compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Krim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="458" to="468" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Texture classification and segmentation using wavelet frames</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1549" to="1560" />
			<date type="published" when="1995-11">Nov. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text and picture segmentation by the distribution analysis of wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing &apos;98</title>
		<meeting>IEEE Int. Conf. Image essing &apos;98<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiscale image texture analysis in wavelet spaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dreger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Processing &apos;94</title>
		<meeting>IEEE Int. Conf. Image essing &apos;94<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wavlets</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Subband</forename><surname>Coding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Embedded image coding using zerotrees of wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3445" to="3462" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale hidden Markov models for Bayesian image analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bayesian Inference in Wavelet Based Models</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Vidakovic</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="243" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian tree-structured image modeling using wavelet-domain hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1056" to="1068" />
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989-02">Feb. 1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<title level="m">Graphical Models for Machine Learning and Digital Communication</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplified wavelet-domain hidden Markov models using contexts</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Asilomar Conf. Signals, Systems, Computers</title>
		<meeting>31st Asilomar Conf. Signals, Systems, Computers<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale document segmentation using wavelet-domain hidden Markov models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IST/SPIE 12th Annu. Symp. Electronic Imaging</title>
		<meeting>IST/SPIE 12th Annu. Symp. Electronic Imaging<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-01">2000. Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiscale SAR image segmentation using wavelet-domain hidden Markov tree models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 14th Int. Symp. Aerospace/Defense Sensing, Simulation, Control</title>
		<meeting>SPIE 14th Int. Symp. Aerospace/Defense Sensing, Simulation, Control<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-04">April 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixed raster content (MRC) model for compound image compression</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SPIE Symp. Electronic Imaging, Visual Communication, Image Processing</title>
		<meeting>IS&amp;T/SPIE Symp. Electronic Imaging, Visual Communication, Image essing<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analysis of wavelet-domain multiscale classification using Kullback-Leibler distances</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">Asilomar Conf. Signals, Systems, Computers</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
