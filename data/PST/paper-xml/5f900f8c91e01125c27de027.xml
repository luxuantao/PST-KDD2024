<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BYOL works even without batch statistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-20">20 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
							<email>jbgrill@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
							<email>altche@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
							<email>corentint]@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
							<email>fstrub@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soham</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Imperial College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BYOL works even without batch statistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-20">20 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.10241v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach for image representation. From an augmented view of an image, BYOL trains an online network to predict a target network representation of a different augmented view of the same image. Unlike contrastive methods, BYOL does not explicitly use a repulsion term built from negative pairs in its training objective. Yet, it avoids collapse to a trivial, constant representation. Thus, it has recently been hypothesized that batch normalization (BN) is critical to prevent collapse in BYOL. Indeed, BN flows gradients across batch elements, and could leak information about negative views in the batch, which could act as an implicit negative (contrastive) term. However, we experimentally show that replacing BN with a batch-independent normalization scheme (namely, a combination of group normalization and weight standardization) achieves performance comparable to vanilla BYOL (73.9% vs. 74.3% top-1 accuracy under the linear evaluation protocol on ImageNet with ResNet-50). Our finding disproves the hypothesis that the use of batch statistics is a crucial ingredient for BYOL to learn useful representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised image representation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> have achieved downstream performance that rivals those of supervised pre-training on ImageNet <ref type="bibr" target="#b4">[5]</ref>. Current self-supervised methods rely on image transformations to generate different views from an input image while preserving semantic information. Among the most successful algorithms, contrastive methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> use a loss function that balances out two terms: a term associated to the positive pairs (that we refer to as the positive term) encouraging representations from views of the same image to be similar, and a term associated to negative pairs (a negative term) which encourages representations to be spread out.</p><p>Taking a different route, other approaches manage to avoid the contrastive paradigm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Among them, BYOL <ref type="bibr" target="#b3">[4]</ref> learns its representation by predicting the target network representation of a view from the online representation of another view of the same image. However, such a setup has obvious collapsed equilibria where the representation is constant, and thus can be predicted from any input. This has raised the question of how BYOL could even work without a negative term nor an explicit mechanism to prevent collapse. Experimental reports <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> suggest that the use of batch normalization, BN <ref type="bibr" target="#b15">[16]</ref>, in BYOL's network is crucial to achieve good performance. These reports hypothesise that the BN used in BYOL's network could implicitly introduce a negative term.</p><p>We experimentally confirm the particular importance of BN in BYOL: removing all instances of BN in the network prevents BYOL from learning anything at all in the classic setting, see Section 3.1 and Table <ref type="table" target="#tab_1">1</ref>. However, our experimental results given in Table <ref type="table" target="#tab_2">2</ref> go against some interpretations proposed notably in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. In particular, we refute the following hypotheses:</p><p>• (H1) BYOL needs BN because BN provides an implicit negative term required to avoid collapse. In Section 3.2, we show that BYOL avoids collapse and achieves 65.7% top-1 accuracy on ImageNet under the linear evaluation protocol <ref type="bibr" target="#b16">[17]</ref> without using any normalization during training, by using both a better initialization scheme and retaining the additional trainable parameters scaling and bias (γ and β) introduced by BN. Therefore, unlike (H1), we hypothesize that the main role of BN is to make the network more robust to cases when the initialization is scaled improperly. Indeed, proper initialization is critical for deep nets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and BYOL suffers from a bad initialization in two ways: (i) as for any deep network, it makes optimization difficult and (ii) BYOL's target network outputs will be ill-conditioned, which initially provides poor targets for the online network. , where the z i θ are projections from views of all images (including z ′ θ but not z θ ), and τ is a temperature parameter. The first term of this loss (the positive term) encourages projections of views of the same image to become similar, while the second term (the negative term) makes projections of views from different images more dissimilar. Such a loss has a strong theoretical underpinning: minimizing this loss is equivalent to maximizing a lower bound on the mutual information between the representation of two views <ref type="bibr" target="#b23">[24]</ref> which is tight when the function approximator is sufficiently expressive.</p><p>BYOL BYOL trains its representation using both an online network (parameterized by θ) and a target network (parameterized by ξ). As a part of the online network, it further defines a predictor network q θ that is used to predict target projections z ′ ξ using online projections z θ as inputs. Accordingly, the parameters of the online projection are updated following the gradients of the prediction loss</p><formula xml:id="formula_0">BYOL θ = − q θ (z θ ), z ′ ξ q θ (z θ ) • z ′ ξ •</formula><p>In turn, the target network weights ξ are updated as an exponential moving average of the online network's weights, i.e. ξ ← (1 − η)ξ + ηθ, with η being a decay parameter. As q θ (z θ ) is a function of v and z ′ ξ is a function of v ′ , BYOL's loss can be seen as a measure of similarity between the views v and v ′ and therefore resembles the positive term of the InfoNCE loss.</p><p>Group normalization (GN) GN <ref type="bibr" target="#b21">[22]</ref> is an activation normalization method, like BN <ref type="bibr" target="#b15">[16]</ref>, layer normalization (LN <ref type="bibr" target="#b24">[25]</ref>), and instance normalization (IN <ref type="bibr" target="#b25">[26]</ref>). For an activation tensor X of dimensions (N, H, W, C), GN first splits channels into G equally-sized groups, then normalizes activations with the mean and standard deviation computed over disjoint slices of size (1, H, W, C/G). The number of groups G thus trades off between normalization over all channels (G = 1, equivalent to LN), and normalization over a single one (G = C, equivalent to IN). Importantly, GN operates independently on each batch element and therefore it does not rely on batch statistics.</p><p>Weight standardization (WS) WS normalizes the weights corresponding to each activation using weight statistics. Each row of the weight matrix W is normalized to get a new weight matrix W which is directly used in place of W during training. Only the normalized weights W are used to compute convolution outputs but the loss is differentiated with respect to non-normalized weights W,</p><formula xml:id="formula_1">W i,j = W i,j − µ i σ i , with µ i = 1 I I j=1 W i,j and σ i = ε + 1 I I j=1 (W i,j − µ i ) 2 ,</formula><p>where I is the input dimension (product of input channel dimension and kernel spatial dimension); we set ε = 10 −4 . Contrary to BN, LN, and GN, WS does not create additional trainable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>While many metrics can be used to evaluate self-supervised representations, we focus on classification accuracy on ImageNet <ref type="bibr" target="#b4">[5]</ref> under the standard linear evaluation protocol <ref type="bibr" target="#b16">[17]</ref> with a ResNet-50 architecture, with the same setup as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4]</ref>. Unless otherwise specified, we follow the training setup and hyperparameters described in <ref type="bibr" target="#b3">[4]</ref> when training BYOL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Removing BN causes collapse</head><p>In Table <ref type="table" target="#tab_1">1</ref>, we explore the impact of using different normalization schemes in SimCLR and BYOL, by using either BN, LN, or removing normalization in each component of BYOL and SimCLR, i.e., the encoder, the projector (for SimCLR and BYOL), and the predictor (for BYOL only). First, we observe that removing all instances of BN in BYOL leads to performance that is no better than random. Noticeably, this is specific to BYOL as SimCLR still performs reasonably well in this regime. Nevertheless, solely applying BN to the ResNet encoder is enough for BYOL to achieve high performance. <ref type="foot" target="#foot_0">2</ref>From these observations, <ref type="bibr" target="#b13">[14]</ref> hypothesizes that BN implicitly introduces a negative contrastive term, which acts as a crucial component to stabilize training (H1). This hypothesis may seem further supported by the performance difference between SimCLR and BYOL when replacing BN (which uses batch statistics) with LN which does not.</p><p>However, we observe that BN seems to be mainly useful in the ResNet encoder, for which standard initializations are known to lead to poor conditioning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Also BYOL might be even more affected by improper initialization as it creates its own targets. Rather than (H1), we therefore hypothesize that the main contribution of BN in BYOL is to compensate for improper initialization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proper initialization allows working without BN</head><p>To confirm this assumption, we design the following protocol to mimic the effect of BN on initial scalings and training dynamics, without using or backpropagating through batch statistics. Before training, we compute per-activation BN statistics for each layer by running a single forward pass of the network with BN on a batch of augmented data. We then remove then batch normalization layers, but retain the scale and offset parameters γ and β trainable, and initialize them as</p><formula xml:id="formula_2">γ k init = γ k 0 σ k and β k init = −µ k • γ k init ,</formula><p>where γ k init and β k init are the initialization of the additional trainable parameters corresponding to the k-th removed BN, and µ k and σ k are the batch statistics computed during the first pass for the k-th removed BN. Additionally, we set γ k 0 = 0 if the k-th removed BN corresponds to the final BN layer in a residual block, and γ k 0 = 1 otherwise. This is similar to what is done in <ref type="bibr" target="#b29">[30]</ref>, except that we further rescale the initialization of the scale and offset parameters by a data-dependent quantity, in the spirit of <ref type="bibr" target="#b30">[31]</ref>. Such setup keeps the initial scaling effect of BN, while avoiding the computation of any batch statistic during training, thus discarding any potential implicit contrastive term.</p><p>We use the exact same hyperparameters as for vanilla BYOL (i.e., base learning rate of 0.2, weight decay of 1.5 • 10 −6 and decay rate of 0.996), except that we increase the number of warmup epochs from 10 to 50. After 1000 epochs, this representation achieves 65.7% top-1 accuracy in the linear evaluation setting compared to 74.3% for the baseline. These results are reported in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Despite its comparatively low performance, the trained representation still provides considerably better classification results than a random ResNet-50 backbone, and is thus necessarily not collapsed. This confirms that BYOL does not need BN to prevent collapse. It also confirms that one of the effects of BN is to provide better initial scalings and training dynamics, and that, contrary to SimCLR, these are required for BYOL to perform well. In the previous section, we have shown that BYOL can learn a non-collapsed representation without using BN. Yet, BYOL performs worse in this regime. This only disproves (H1), but BN could still both provide better initial scaling and an implicit contrastive term, responsible for some of the performance. To study this hypothesis, we explore other refined element-wise normalization procedures. More precisely, we apply weight standardization to convolutional and linear parameters by weight standardized alternatives, and replace all BN by GN layers.</p><p>To train the network, we use the same hyperparameters as in BYOL except for the weight decay, set to 3 • 10 −8 instead of 1.5 • 10 −6 , the base learning rate set to 0.24 instead of 0.2 and the target update rate, set to 0.999 instead of 0.996; we also set the number of groups for GN to G = 16. With this setup, BYOL (+GN +WS) achieves 73.9% top-1 accuracy after 1000 epochs.</p><p>As neither GN nor WS compute batch statistics, this version of BYOL cannot compare elements from the batch, and therefore it likewise cannot implement a batch-wise implicit contrastive mechanism. Therefore, we experimentally show that BYOL can maintain most of its performance even without a hypothetical implicit contrastive term provided by BN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Unlike contrastive methods, the loss used in BYOL does not explicitly include a negative term that would encourage its representations to spread apart. Nonetheless, BYOL's representation does not collapse during training, and BN has been hypothesized to fill the crucial role of an implicit negative term by leaking batch statistics into the gradient. We refute this hypothesis, and show that BYOL can achieve competitive results without using batch statistics. In particular, BYOL achieves 65.7% top-1 accuracy when removing BN and changing the initialization. Moreover, BYOL achieves a competitive 73.9% top-1 accuracy by replacing BN with a normalization scheme operating element-wise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• (H2) BYOL cannot achieve competitive performance without the implicit contrastive effect provided by batch statistics. In Section 3.3, we show that most of this performance gap-65.7% achieved without BN vs. 74.3% achieved with BN-can be bridged without using batch statistics. Specifically, if we replace BN with a combination of group normalization, GN<ref type="bibr" target="#b21">[22]</ref>, and weight standardization, WS<ref type="bibr" target="#b22">[23]</ref>, while keeping standard initializations, BYOL achieves 73.9% top-1 accuracy.</figDesc><table /><note>negative term</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation results on normalization, per network component:The numbers correspond to top-1 linear accuracy (%), 300 epochs on ImageNet, averaged over 3 seeds.</figDesc><table><row><cell>Encoder</cell><cell></cell><cell>BN</cell><cell></cell><cell></cell><cell></cell><cell>LN</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>Projector</cell><cell>BN</cell><cell></cell><cell>-</cell><cell></cell><cell>LN</cell><cell></cell><cell>-</cell><cell></cell><cell>BN</cell><cell></cell><cell>LN</cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>Predictor</cell><cell>BN</cell><cell>-</cell><cell>BN</cell><cell>-</cell><cell>LN</cell><cell>-</cell><cell>LN</cell><cell>-</cell><cell>BN</cell><cell>-</cell><cell>LN</cell><cell>-</cell><cell>BN</cell><cell>LN</cell><cell>-</cell></row><row><cell>BYOL</cell><cell cols="15">73.2 73.2 72.0 72.1 0.1 5.4 0.1 0.1 62.6 0.1 0.1 0.1 61.1 0.1 0.1</cell></row><row><cell>SimCLR</cell><cell>69.3</cell><cell></cell><cell>68.5</cell><cell></cell><cell>68.0</cell><cell></cell><cell>67.8</cell><cell></cell><cell>53.8 3</cell><cell></cell><cell>56.7</cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of our results: top-1 accuracy with linear evaluation on ImageNet, at 1000 epochs.</figDesc><table><row><cell>BYOL variant</cell><cell cols="4">Vanilla BN No BN Modified init. GN + WS</cell></row><row><cell>Uses batch statistics</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell>Top-1 accuracy (%)</cell><cell>74.3</cell><cell>0.1</cell><cell>65.7</cell><cell>73.9</cell></row><row><cell cols="3">3.3 Using GN with WS leads to competitive performance</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Some of these observations differ from the ones initially reported in<ref type="bibr" target="#b13">[14]</ref>. Specifically, the authors observed a collapse when removing BN in BYOL's predictor and projector. This difference could be linked to the use of the SGD optimizer instead of LARS<ref type="bibr" target="#b26">[27]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Unstable in late training: three seeds ending at</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="48" xml:id="foot_2">.4%, 57.9%, 56.1%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank the following people for their help throughout the process of writing this paper, in alphabetical order: Jean-Baptiste Alayrac, Bernardo Avila Pires, Nathalie Beauguerlange, Elena Buchatskaya, Jeffrey De Fauw, Sander Dieleman, Carl Doersch, Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Olivier Henaff, Koray Kavukcuoglu, Pauline Luc, Katrina McKinney, Rémi Munos, Aaron van den Oord, Jason Ramapuram, Adria Recasens, Karen Simonyan, Oriol Vinyals and the DeepMind team. We would like to also thank the authors of the following papers for fruitful discussions: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECML)</title>
				<meeting>European Conference on Computer Vision (ECML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectralnet: Spectral clustering using deep neural networks</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding self-supervised and contrastive learning with bootstrap your own latent (BYOL</title>
		<author>
			<persName><forename type="first">Abe</forename><surname>Fetterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Albrecht</surname></persName>
		</author>
		<ptr target="https://untitled-ai.github.io/understanding-self-supervised-contrastive-learning.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding self-supervised learning with dual deep networks</title>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECML)</title>
				<meeting>European Conference on Computer Vision (ECML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mean field residual networks: On the edge of chaos</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Residual learning without normalization via better initialization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECML)</title>
				<meeting>the European Conference on Computer Vision (ECML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Weight standardization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scaling SGD batch size to 32k for imagenet training</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding batch normalization</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carla</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A mean field theory of batch normalization</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data-dependent initializations of convolutional neural networks</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06856</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Max</forename><surname>Schwarzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05929</idno>
		<title level="m">Data-efficient reinforcement learning with momentum predictive representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
