<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do You Hear What I Hear? Fingerprinting Smart Devices Through Embedded Acoustic Components</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anupam</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikita</forename><surname>Borisov</surname></persName>
							<email>nikita@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Caesar</surname></persName>
							<email>caesar@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do You Hear What I Hear? Fingerprinting Smart Devices Through Embedded Acoustic Components</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">904277992131FE411E36EB8F8938723C</idno>
					<idno type="DOI">10.1145/2660267.2660325</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Miscellaneous -Security</term>
					<term>H.5.1 [Multimedia Information Systems]: Audio input/output Fingerprinting</term>
					<term>Privacy</term>
					<term>Acoustic feature</term>
					<term>Microphone</term>
					<term>Speaker</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The widespread use of smart devices gives rise to privacy concerns. Fingerprinting smart devices can jeopardize privacy by allowing remote identification without user awareness. We study the feasibility of using microphones and speakers embedded in smartphones to uniquely fingerprint individual devices. During fabrication, subtle imperfections arise in device microphones and speakers, which induce anomalies in produced and received sounds. We exploit this observation to fingerprint smartphones through playback and recording of audio samples. We explore different acoustic features and analyze their ability to successfully fingerprint smartphones. Our experiments show that not only is it possible to fingerprint devices manufactured by different vendors but also devices that have the same maker and model; on average we were able to accurately attribute 98% of all recorded audio clips from 50 different Android smartphones. Our study also identifies the prominent acoustic features capable of fingerprinting smart devices with a high success rate, and examines the effect of background noise and other variables on fingerprinting accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Mobile devices, including smartphones, PDAs, and tablets, are quickly becoming widespread in modern society. In 2012 a total of 1.94 billion mobile devices were shipped, of which 75% were smart and highly-featured phones <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. Canalys predicted that the mobile device market will reach 2.6 billion units by 2016, with smartphones and tablets continuing to dominate shipments <ref type="bibr" target="#b13">[14]</ref>. The rapid uptake of intelligent mobile devices is not surprising, due to the numerous advantages they provide consumers, from entertainment and social applications to business and advanced computing capabilities. However, smartphones, with all their interactive, location-centric, and connectivity-based features impose threatening concerns on user privacy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>In this paper we thoroughly analyze a technique for fingerprinting the hardware of smartphones. The observation is that even if the software on mobile devices is strengthened <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70]</ref>, hardwarelevel idiosyncrasies in microphones and speakers can be used to fingerprint physical devices. During manufacturing, imperfections are introduced in the analog circuitry of these components, and as such, two microphones and speakers are never alike. Through an observational study, we find that these imperfections are substantial enough, and prevalent enough, that we can reliably distinguish between devices by passively recording audio streams, and conducting simple spectral analyses on the recorded audio streams. Our approach can substantially simplify the ability for an adversary to track and identify people in public locations which can threaten the privacy of mobile device users. Our approach requires small amounts of data -for example, we show that with our technique, an adversary could even use the short ringtones produces by mobile device speakers to reliably track users in public environments. Alternatively, a stealthy app (e.g., an online game) can access the microphone to uniquely distinguish all users running the app.</p><p>Our approach centers around the development of a fingerprinting mechanism, which aims to "pull out" imperfections in device circuitry. Our mechanism has two parts: a method to extract auditory fingerprints and a method to efficiently search for matching fingerprints from a database. To generate fingerprints of speakers we record audio clips played from smartphones onto an external device (i.e., laptop/PC) and vice versa for generating fingerprints of microphones. We also generate the combined fingerprint of speaker and microphone by playing and recording audio clips simultaneously on smartphones. We use two different classifiers to evaluate our fingerprinting approach. Moreover, we test our fingerprinting approach for different genre of audio clips at various frequencies. We also study various audio features that can be used to accurately fingerprint smartphones. Our studies reveals that melfrequency cepstral coefficients (MFCCs) are the dominant features for fingerprinting smartphones. Lastly, we analyze the sensitivity of our fingerprinting approach against different factors like sampling frequency, distance between speaker and recorder, training set size and ambient background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. Our contributions are summarized below:</head><p>• We analyze the feasibility of fingerprinting smart devices by leveraging the manufacturing idiosyncrasies of microphones and speakers embedded in smart devices.</p><p>• We study a large spectrum of existing audio features and their ability to accurately fingerprint smartphones. We find that mel-frequency cepstral coefficients (MFCCs) perform particularly well in fingerprinting smartphones. • We investigate two different classifiers to evaluate our fingerprinting approach. We conclude that Gaussian mixture models (GMMs) are more effective compared to k-NN classifiers in classifying recorded audio fingerprints. • We perform experiments across several different genres of audio excerpts. We also analyze how different factors like sampling frequency, distance between speaker and recorder, training set size and ambient background noise impact our fingerprinting accuracy.</p><p>Roadmap. The remainder of this paper is organized as follows.</p><p>We give an overview of our fingerprinting approach in Section 2.</p><p>We discussed related work in Section 3. In Section 4, we discuss why microphones and speakers integrated in smartphones can be used to generate unique fingerprints. In Section 5, we describe the different audio features considered in our experiments, along with the classification algorithms used in our evaluation. We present our experimental results in Section 6. We also discuss some limitations of our approach in Section 7. Finally, we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW</head><p>In this section we give an overview of our approach and present several viable attack scenarios. We also identify the key challenges that we address in this paper.</p><p>The key observation behind our work is that imperfections in smart device hardware induce unique signatures on the received and transmitted audio streams, and these unique signatures, if identified, can be used by an adversary to fingerprint the device. We consider three fingerprinting scenarios: speaker, microphone, and joint speaker-microphone fingerprinting. In the first case, an attacker in a public environment, such as a cafe or shopping mall, records audio generated by a smartphone speaker, such as a ringtone. The attacker can then use the recorded audio samples to track and identify users. Alternately, the attacker may obtain audio recorded by a smartphone microphone and use that to identify the user who made the recording; this can have forensic applications. A third way to track users is to convince them to install a malicious application (e.g., a free online game), which can play and record audio clips using the device's speaker and microphone. The app can then stealthily upload the recorded audio clips to the attacker (e.g., piggybacking it on log-in information or game state), who can then use the audio samples to uniquely distinguish each user. To do this, the application would require access to both the speaker and microphone, as well as network access, but such permissions are not unusual for applications and are unlikely to raise alarm, especially given that a significant portion of the users cannot comprehend the full consequences of smartphone permissions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Our approach consists of two main tasks. The first task is acquiring a set of audio samples for analysis in the first place. To do this, we have a listener module, responsible for receiving and recording device audio. The listener module could be deployed as an application on the smart device (many mobile OSes allow direct access to microphone inputs), or as a stand-alone service (e.g., the adversary has a microphone in a public setting to pick up device ringtones). The next task is to effectively identify device signatures from the received audio stream. To do this, we have an analyzer module, which leverages signal processing techniques to localize spectral anomalies, and constructs a 'fingerprint' of the auditory characteristics of the device. A critical part of this task involves determining what sort of acoustic features and audio analysis techniques are most effective in identifying unique signatures of device-hardware. There are a large number of audio properties which could be used (spectral entropy, zero crossings, etc.) as well as a broad spectrum of analysis algorithms that can be used to summarize these properties (principle component analysis, linear discriminant analysis, feature selection, etc.). We will study alternative properties to characterize hardware-induced auditory anomalies in Section 5.1 as well as algorithms for effectively clustering them in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>Fingerprints have long been used as one of the most common bio-metrics in identifying human beings <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b60">61]</ref>. The same concept was extended to identifying and tracking unique mobile transmitters by the US government during 1960s <ref type="bibr" target="#b46">[47]</ref>. Later on with the emergence of cellular networks people were able to uniquely identify transmitters by analyzing the externally observable characteristics of the emitted radio signals <ref type="bibr" target="#b59">[60]</ref>.</p><p>Physical devices are usually different at either the software or hardware level even if they are produced by the same vendor. In terms of software based fingerprinting, researchers have looked at fingerprinting techniques that differentiate between unique devices over a Wireless Local Area Network (WLAN) simply through a timing analysis of 802.11 probe request frames <ref type="bibr" target="#b29">[30]</ref>. Others have looked at exploiting the difference in firmware and device driver running on IEEE 802.11 compliant devices <ref type="bibr" target="#b36">[37]</ref>. 802.11 MAC headers have also been used to track unique devices <ref type="bibr" target="#b39">[40]</ref>. Pang et al. <ref type="bibr" target="#b56">[57]</ref> were able to exploit traffic patterns to carry out device fingerprinting. Open source toolkits like Nmap <ref type="bibr" target="#b49">[50]</ref> and Xprobe <ref type="bibr" target="#b67">[68]</ref> can remotely fingerprint an operating system by identifying unique responses from the TCP/IP networking stack.</p><p>Another angle to software based fingerprinting is to exploit applications like browsers to carry out device fingerprinting <ref type="bibr" target="#b32">[33]</ref>. Yen et al. <ref type="bibr" target="#b68">[69]</ref> were successful at tracking users with high precision by analyzing month-long logs of Bing and Hotmail. Researchers have also been able to exploit JavaScript and popular third-party plugins like Flash player to obtain the list of fonts installed in a device which then enabled them to uniquely track users <ref type="bibr" target="#b17">[18]</ref>. Other researchers have proposed the use of performance benchmarks for differentiating between JavaScript engines <ref type="bibr" target="#b53">[54]</ref>. Furthermore, browsing history can be exploited to fingerprint and track web users <ref type="bibr" target="#b55">[56]</ref>. The downside of software based fingerprints is that such fingerprints are generated from the current configuration of the system which is not static, rather it is likely to change over time.</p><p>Hardware based fingerprinting approaches rely on some static source of idiosyncrasies. It has been shown that network devices tends to have constant clock skews <ref type="bibr" target="#b52">[53]</ref> and researchers have been able to exploit these clock skews to distinguish devices through TCP and ICMP timestamps <ref type="bibr" target="#b45">[46]</ref>. However, clock skew rate is highly dependent on the experimental environment <ref type="bibr" target="#b66">[67]</ref>. Researchers have also extensively looked at fingerprinting the unique transient characteristics of radio transmitters (also known as RF fingerprinting). RF fingerprinting has been shown as a means of enhancing wireless authentication <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55]</ref>. It has also been used for location detection <ref type="bibr" target="#b57">[58]</ref>. Manufacturing imperfections in network interface cards (NICs) have also been studied by analyzing analog signals transmitted from them <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>. More recently Dey et al. have studied manufacturing idiosyncrasies inside smartphone accelerometer to distinguish devices <ref type="bibr" target="#b30">[31]</ref>. However, their approach requires some form of external stimulation/vibration to successfully capture the manufacturing imperfection of the on-board accelerometer. Moreover, there are different contexts in which audio prints can be more useful, e.g., software that is not allowed to access the accelerome-ter, as well as an external adversary who fingerprints nearby phones with a microphone.</p><p>Our work is inspired by the above work in hardware-based fingerprinting, but we focus on fingerprinting on-board acoustic components like speakers and microphones. In this setting, Clarkson's work <ref type="bibr" target="#b25">[26]</ref> is perhaps the most closely related to ours. He showed that it is possible to distinguish loudspeakers by analyzing recorded audio samples emitting from them. However, his experiments used special audio clips that contained 65 different frequencies, whereas we are using common audio excerpts like ringtones. Moreover, his experiments ignored the subtlety introduced by microphones. In fact in one experiment, though statistically not meaningful as it tested only two similar microphones, he found no variation across microphones. We, on the other hand found that microphones can vary across different units. Finally, his study did not thoroughly analyze the different acoustic features that can be used to successfully carry out device fingerprinting. As a result, he was able to achieve only 81% accuracy in distinguishing heterogeneous loudspeakers.</p><p>Audio fingerprinting has a rich history of notable research <ref type="bibr" target="#b22">[23]</ref>. There are studies that have looked at classifying audio excerpts based on their content <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b64">65]</ref>. Others have looked at distinguishing human speakers from audio segments <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. There has also been work on exploring various acoustic features for audio classification <ref type="bibr" target="#b51">[52]</ref>. One of the more popular applications of audio fingerprinting has been music genre and artist recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Our work takes advantage of the large set of acoustic features that have been explored by existing work in audio fingerprinting. However, instead of classifying the content of audio segments, we utilize acoustics features to capture the manufacturing imperfections of microphones and speakers embedded in smart devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SOURCE OF FINGERPRINTS</head><p>In this section we will take a closer look at the microphones and speakers embedded on today's smartphones. This will provide an understanding of how microphones and speakers can act as a potential source for unique fingerprints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Closer Look at Microphones</head><p>Microphones in modern smartphones are based on Micro Electro Mechanical Systems (MEMS) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. To enhance active noise and echo canceling capabilities, most smartphones today have more than one MEMS microphone. For example, the iPhone 5 has a total of three embedded MEMS microphones <ref type="bibr" target="#b9">[10]</ref>. According to the IHS-iSuppli report, Apple and Samsung were the top consumers of MEMS microphones in 2012, accounting for a combined 54% of all shipped MEMS microphones <ref type="bibr" target="#b16">[17]</ref>. . A MEMS microphone, sometimes called a microphone chip or silicon microphone, consists of a coil-less pressure-sensitive diaphragm directly etched into a silicon chip. It is comprised of a MEMS die and a complementary metal-oxide-semiconductor (CM-OS) die combined in an acoustic housing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. The CMOS often includes both a preamplifier as well as an analog-to-digital (AD) converter. Modern fabrication techniques enable highly compact deigns, making them well suited for integration in digital mobile devices. The internal architecture of a MEMS microphone is shown on Figure <ref type="figure">1</ref>. From the figure we can see that the MEMS microphone's physical design is based on a variable capacitor consisting of a highly flexible diaphragm in close proximity to a perforated, rigid back-plate. The perforations permit the air between the diaphragm and back-plate to escape. When an acoustic signal reaches the diaphragm through the acoustic holes, the diaphragm is set in motion. This mechanical deformation causes capacitive change which in turn causes voltage change. In this way sound pressure is converted into an electrical signal for further processing. The back-chamber acts as a acoustic resonator and the ventilation hole allows the air compressed inside the back chamber to flow out, allowing the diaphragm to move back into its original place.</p><p>The sensitivity of the microphone depends on how well the diaphragm deflects to acoustic pressure; it also depends on the gap between the static back-plate and the flexible diaphragm. Unfortunately, even though the manufacturing process of these microphones has been streamlined, no two chips roll off the assembly line functioning in exactly the same way. Imperfections can arise for the following reasons: slight variations in the chemical composition of components from one batch to the next, wear in the manufacturing machines or changes in temperature and humidity. While subtle imperfections in the microphone chips may go unnoticed by human ears, computationally such discrepancies may be sufficient to discriminate them, as we later show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Closer Look at Microspeakers</head><p>Micro-speakers are a scaled down version of a basic acoustic speaker. So lets first look at how speakers work before we discuss how microspeakers can be used to generate unique fingerprints. Figure <ref type="figure" target="#fig_2">2(a)</ref> shows the basic components of a speaker. The diaphragm is usually made of paper, plastic or metal and its edges are connected to the suspension. The suspension is a rim of flexible material that allows the diaphragm to move. The narrow end of the diaphragm's cone is connected to the voice coil. The voice coil is attached to the basket by a spider (damper), which holds the coil in position, but allows it to move freely back and forth. A permanent magnet is positioned directly below the voice coil.</p><p>Sound waves are produced whenever electrical current flows through the voice coil, which acts as an electromagnet. Running varying electrical current through the voice coil induces a varying magnetic field around the coil, altering the magnetization of the metal it is wrapped around. When the electromagnet's polar orientation switches, so does the direction of repulsion and attraction. In this way, the magnetic force between the voice coil and the permanent magnet causes the voice coil to vibrate, which in turn vibrates the speaker diaphragm to generate sound waves.  <ref type="bibr" target="#b23">[24]</ref>. The components are similar to that of a basic speaker; the only difference is the size and fabrication process <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b62">63]</ref>. The amplitude and frequency of the sound wave produced by the speaker's diaphragm is dictated respectively by the distance and rate at which the voice coil moves. Each speaker component can introduce variations into the generated sound. For example, variations in the electromagnetic properties of the driver can cause differences in the rate and smoothness at which the diaphragm moves. Therefore, due to the inevitable variations and imperfections of the manufacturing process, no two speakers are going to be alike, resulting in subtle differences in the produced sound. In our work, we develop techniques to computationally localize and evaluate these differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FEATURES AND ALGORITHMS USED</head><p>In this section we briefly describe the acoustic features that we used in generating device fingerprints. We also discuss the classification algorithms used in identifying the devices from which the fingerprints originated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Audio Features</head><p>Given our knowledge that imperfections exist in device audio hardware, we now need some way to detect them. To do this, our approach identifies acoustic features from an audio stream, and uses the features to construct a fingerprint of the device. Computing acoustic features from an audio stream has been a subject of much research <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">65]</ref>. To gain an understanding of how a broad range of acoustic features are affected by device imperfections we investigate a total of 15 acoustics features (listed in Table <ref type="table" target="#tab_0">1</ref>), all of which have been well-documented by researchers. Due to space limitation we exclude detailed description of each acoustic feature, however, an elaborate description of the audio features is available in our technical report <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Algorithms</head><p>Next, we need some way to leverage the set of features to perform device identification. To achieve this, we leverage a classification algorithm, which takes observations (features) from the observed device as input, and attempts to classify the device into one of several previously-observed sets.</p><p>To do this, our approach works as follows. First, we perform a training step, by collecting a number of observations from a set of devices. Each observation (data point) corresponds to a set of features observed from that device, represented as a tuple with one dimension per feature. As such, data points can be thought of as existing in a hyper-dimensional space, with each axis corresponding to the observed value of a corresponding feature. Our approach then applies a classification algorithm to build a representation of these data points, which can later be used to associate new observations with device types. When a new observation is collected, the classification algorithm returns the most likely device that caused the observation.</p><p>To do this effectively, we need an efficient classification algorithm. In our work, we compare the performance of two alternate approaches described below: k-nearest neighbors (associates an incoming data point with the device corresponding to the nearest "learned" data points), and Gaussian mixture models (computes a probability distribution for each device, and determines the maximal likely association).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-NN:</head><p>The k-nearest neighbors algorithm (k-NN) is a nonparametric lazy learning algorithm. The term "non-parametric" means that the k-NN algorithm does not make any assumptions about the underlying data distribution, which is useful in analyzing realworld data with complex underlying distribution. The term "lazy learning" means that the k-NN algorithm does not use the training data to make any generalization, rather all the training data are used in the testing phase making it computationally expensive (however, optimizations are possible). The k-NN algorithm works by first computing the distance from the input data point to all training data points and then classifies the input data point by taking a majority vote of the k closest training records in the feature space <ref type="bibr" target="#b31">[32]</ref>. The best choice of k depends upon the data; generally, larger values of k reduce the effect of noise on the classification, but make boundaries between classes less distinct. We will discuss more about the choice of k in Section 6.</p><p>GMM: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. The unknown patterns and mixture weights are estimated from training samples using an expectation-maximization (EM) algorithm <ref type="bibr" target="#b28">[29]</ref>. During the matching phase the fingerprint for an unknown recording is first compared with a database of pre-computed GMMs and then the class label of the GMM that gives the highest likelihood is returned as the expected class for the unknown fingerprint. GMMs are often used in biometric systems, most notably in human speaker recognition systems, due to their capability of representing a large class of sample distributions <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b64">65]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>In this section we perform a series of experiments to evaluate how well we can fingerprint smartphones by exploiting the manufacturing idiosyncrasies of microphones and speakers embedded in them. We start by describing how we performed our experiments (Section 6.1). Next, we briefly discuss the setup for fingerprinting devices through speaker, microphone and a combination of both (Section 6.2). We then discuss a framework for determining the dominant (most-relevant) set of audio features that can be used in fingerprinting smartphones (Section 6.3). Then, we look at fingerprinting devices, first, of different maker-and-model (Section 6.4), followed by devices of same maker-and-model (Section 6.5) and finally, a combination of both with multiple units of different models (Section 6.6). The performance of our approach is affected by certain aspects of the operating environment, and we the study sensitivity to such factors in Section 6.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>To perform our experiments, we constructed a small testbed environment with real smartphone device hardware. In particular, our default environment consisted of a 266 square foot (14'x19') office room, with nine-foot dropped ceilings with polystyrene tile, comprising a graduate student office in a University-owned building. The room was filled with desks and chairs, and opens out on a public hall with foot traffic. The room also receives a minimal amount of ambient noise from air conditioning, desktop computers, and florescent lighting. We placed smartphones in various locations in the room. To emulate an attacker, we placed an ACER Aspire 5745 laptop in the room. To investigate performance with inexpensive hardware, we used the laptop's built-in microphone to collect audio samples. We investigate how varying this setup affects performance of the attack in Section 6.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Devices and tools:</head><p>We tested our device fingerprinting on devices from five different manufacturers. We also investigate different genres of audio excerpts. Table <ref type="table" target="#tab_2">3</ref> describes the different types of audio excerpts used in our experiments. Duration of the audio clips varies from 3 to 10 seconds. The default sampling frequency of all audio excerpts is 44.1kHz unless explicitly stated otherwise. All audio clips are stored in WAV format using 16-bit pulse-code-modulation (PCM) technique. For analysis we leverage the following audio tools and analytic modules: MIRtollbox <ref type="bibr" target="#b12">[13]</ref>, Netlab <ref type="bibr" target="#b14">[15]</ref>, Audacity <ref type="bibr" target="#b2">[3]</ref> and the Android app Hertz <ref type="bibr" target="#b5">[6]</ref>. Both MIRtoolbox and Netlab are MATLAB modules providing a rich set of functions for analyzing and extracting audio features. Audacity and Hertz are mainly used for recording audio clips on computers and smartphones respectively.</p><p>For analyzing and matching fingerprints we use a desktop machine with the following configuration: Intel i7-2600 3.4GHz processor with 12GiB RAM. We found that the average time required to match a new fingerprint was around 5-10 ms for k-NN classifier and around 0.5-1 ms for GMM classifier.</p><p>Evaluation metrics: We use standard multi-class classification metrics-precision, recall, and F1-score <ref type="bibr" target="#b63">[64]</ref>-in our evaluation. Assuming there are fingerprints from n classes, we first compute the true positive (T P ) rate for each class, i.e., the number of traces from the class that are classified correctly. Similarly, we compute the false positive (F P ) and false negative (F N ), as the number of wrongly accepted and wrongly rejected traces, respectively, for each class i (1 ≤ i ≤ n). We then compute precision, recall, and the F1-score for each class using the following equations:</p><formula xml:id="formula_0">Precision, P ri = T Pi/(T Pi + F Pi) (1) Recall, Rei = T Pi/(T Pi + F Ni) (2) F1-Score, F1 i = (2 × P ri × Rei)/(P ri + Rei)<label>(3)</label></formula><p>The F1-score is the harmonic mean of precision and recall; it provides a good measure of overall classification performance, since precision and recall represent a trade-off: a more conservative classifier that rejects more instances will have higher precision but lower recall, and vice-versa. To obtain the overall performance of Each audio excerpt is recorded/played 10 times, 50% of which is used for training and the remaining 50% is used for testing. We report the maximum evaluation obtained by varying the number of neighbors (k) from 1 to 5 for the k-NN classifier and considering 1 to 5 Gaussian distributions per class. Since GMM parameters are produced by the randomized EM algorithm, we perform 10 parameter-generation runs for each instance and report the average classification performance. We also compute the 95% confidence interval, but we found it to be less than 0.01 and therefore, do not report it in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Fingerprinting Acoustic Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Process of Fingerprinting Speaker</head><p>An attacker can leverage our technique to passively observe audio signals (e.g., ringtones) emitting from device speakers in public environments. To investigate this, we first look at fingerprinting speakers integrated inside smartphones. For fingerprinting speakers we record audio clips played from smartphones onto a laptop and we then extract acoustic features from the recorded audio excerpts to generate fingerprints as shown in Figure <ref type="figure" target="#fig_3">3</ref>. We look at devices manufactured by both the same vendor and different vendors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Process of Fingerprinting Microphone</head><p>Attackers may also attempt to fingerprint devices by observing imperfections in device microphones, for example by convincing the user to install an application on their phone, which can observe inputs from the device's microphone. To investigate the feasibility of this attack, we next look at fingerprinting microphones embedded in smartphones. To do this, we record audio clips played from a laptop onto smartphones as shown in Figure <ref type="figure" target="#fig_4">4</ref>. Again we consider devices made by both the same vendor and different vendors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Process of Fingerprinting both Speaker and Mic</head><p>An attacker may attempt to fingerprint devices by observing imperfections in both device microphone and speaker, for example by convincing the user to install a game on their phone which requires access to device speaker and microphone to interact with the game (something like Talking Tom Cat). The attacker could potentially play a theme song at the start of the game and at the same time make a recording of the audio clip. To investigate the feasibility of this attack, we build an android app that plays and records audio clips simultaneously and uploads the data to a remote server. The recorded audio clips would then enable the attacker to characterize the imperfections of microphones and speakers embedded inside smartphones. Figure <ref type="figure" target="#fig_5">5</ref> summarizes the whole process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Feature Exploration</head><p>At first glance, it seems that we should use all features at our disposal to identify device types. However, including too many features can worsen performance in practice, due to their varying accuracies and potentially-conflicting signatures. Hence, in this section, we provide a framework to explore all the 15 audio features described in Section 5.1 and identify the dominating subset of all the features, i.e., which combination of features should be used. For this purpose we adopt a well known machine learning strategy known as feature selection <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b65">66]</ref>. Feature selection is the process of reducing dimensionality of data by selecting only a subset of the relevant features for use in model construction. The main assumption in using feature selection technique is that the data may contain redundant features. Redundant features are those which provide no additional benefit than the currently selected features. Feature selection techniques are a subset of the more general field of feature extraction, however, in practice they are quite different from each other. Feature extraction creates new features as functions of the original features, whereas feature selection returns a subset of the features. Feature selection is preferable to feature extraction when the original units and meaning of features are important and the modeling goal is to identify an influential subset. When the features themselves have different dimensionality, and numerical transformations are inappropriate, feature selection becomes the primary means of dimension reduction.</p><p>Feature selection involves the maximization of an objective function as it searches through the possible candidate subsets. Since exhaustive evaluation of all possible subsets are often infeasible (2 N for a total of N features) different heuristics are employed. We use a greedy search strategy known as sequential forward selection (SFS) where we start off with an empty set and sequentially add the features that maximize our objective function. The pseudo code of our feature selection algorithm is described in Algorithm 1.</p><p>The algorithm works as follows. First, we compute the F1-score that can be achieved by each feature individually. Next, we sort the feature set based on the achieved F1-score in descending order. Then, we iteratively add features starting from the most dominant one and compute the F1-score of the combined feature subset. If adding a feature increases the F1-score seen so far we move on to the next feature, else we remove the feature under inspection. Having traversed through the entire set of features, we return the subset of features that maximizes our device classification task. Note that this is a greedy approach, therefore, the generated subset might not Algorithm 1 Sequential Feature Selection</p><formula xml:id="formula_1">Input: Input feature set F Output: Dominant feature subset D F 1_score ← [] for f ∈ F do F 1_score[f ] ← Classif y(f ) end for F ← sort(F, F 1_score) #In descending order max_score ← 0 D ← ∅ for f ∈ F do D ← D ∪ f temp ← Classif y(D) if temp &gt; max_score then max_score ← temp else D ← D -{f } end if end for return D</formula><p>always provide optimal F1-score. However, for our purpose, we found this approach to perform well, as we demonstrate in latter sections. We test our feature selection algorithm for all three types of audio excerpts listed in Table <ref type="table" target="#tab_2">3</ref>. We evaluate the F1-score using both k-NN and GMM classifiers.</p><p>Note that the audio excerpts used for feature exploration and the ones used for evaluating our fingerprinting approach in the following sections are not identical. We use different audio excerpts belonging to the three categories listed in Table <ref type="table" target="#tab_2">3</ref>, so as to not bias our evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Different Maker-and-Model Devices</head><p>In this section we look at fingerprinting smartphones manufactured by five different vendors. We take one representative smartphone from each row of Table <ref type="table" target="#tab_1">2</ref> giving us a total of 7 different smartphones. We look at fingerprinting these devices first by using the microphone and speaker individually and next, by combining both microphone and speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Feature Exploration</head><p>First, we look at exploring different acoustic features with the goal of obtaining the dominant subset of features. Table <ref type="table">4</ref> highlights the maximum F1-score achieved by each acoustic feature for the three different types of audio excerpt. The maximum F1-score is obtained by varying k from 1 to 5 (for k-NN classifier) and also considering 1 to 5 gaussian distributions per class (for GMM classifier). Each type of audio is recorded 10 times giving us a total of 70 samples from the 7 representative handsets; 50% of which (i.e., 5 samples per handset) is used for training and the remaining 50% is used for testing. All the training samples are labeled with their corresponding handset identifier. Both classifiers return the class label for each audio clip in the test set and from that we compute F1score. The table also highlights the subset of features selected by our sequential feature selection algorithm and their corresponding F1-score. We find that most of the time MFCCs are the dominant features for all categories of audio excerpt.</p><p>To get a better understanding of why MFCCs are the dominant acoustic features we plot the MFCCs of a given audio excerpt from three different handsets on Figure <ref type="figure" target="#fig_6">6</ref>. All the coefficients are ranked in the same order for the three handsets. We can see that the magnitude of the coefficients vary across the handsets. For example, coef-ficient 3 and 5 vary significantly across the three handsets. Hence, MFCCs are highly suitable features for fingerprinting smartphones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Fingerprinting using Speaker</head><p>We test our fingerprinting approach using three different types of audio excerpt. Each audio sample is recorded 10 times, 50% of which is used for training and the remaining 50% is used for testing. We repeat this procedure for the three different types of audio excerpt. Table <ref type="table" target="#tab_3">5</ref> summarizes our findings (values are reported as percentages). We simply use the acoustic features obtained from our sequential feature selection algorithm as listed in Table <ref type="table">4</ref>. From Table <ref type="table" target="#tab_3">5</ref> we see that we can successfully (with a maximum F1-score of 100%) identify which audio clip came from which smartphone. Thus, fingerprinting smartphones manufactured by different vendors seems very much feasible using only few acoustic features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Fingerprinting using Microphone</head><p>Similar to speakers, we find microphone properties differ quite substantially across vendors. We exploit this phenomenon to fingerprint smartphones through microphones. To test our hypothesis we test our fingerprinting approach using three different types of audio excerpt. Each audio sample is again recorded 10 times; we use 50% for training and the other 50% for testing. Table <ref type="table" target="#tab_4">6</ref> summarizes our findings (values are reported as percentages). We use the same set of features obtained from our sequential feature selection algorithm as listed in Table <ref type="table">4</ref>. From Table <ref type="table" target="#tab_4">6</ref> we see that we can achieve an F1-score of over 97%. These results suggest that smartphones can be successfully fingerprinted through microphones too. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.4">Fingerprinting using Microphone and Speaker</head><p>We now look at fingerprinting smartphones through both microphones and speakers. For this experiment we build an android app to collect data from different smartphones. Our app plays and records different audio clips simultaneously and uploads the data to a remote server. As we are using an android app for our data collection, we had to exclude iPhone5 and Sony Ericsson W518 handset from this experiment (reducing our pool of handsets to 5 devices). Again each audio sample is recorded 10 times, half of which is used for training and the other half for testing. We use the features obtained from Table <ref type="table">4</ref>. Table <ref type="table" target="#tab_5">7</ref> summarizes our findings (values are reported as percentages). We see that we can achieve an F1-score of 100%. Thus, a malicious app having access to only speaker and microphone can successfully fingerprint smartphones.  We can see that some of the coefficients vary significantly, thus enabling us to exploit this feature to fingerprint smartphones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Same Maker-and-Model Devices</head><p>In this section, we look at fingerprinting smartphones manufactured by the same vendor and are of the same model. From Table <ref type="table" target="#tab_1">2</ref> we see that we have 15 Motorola Droid A855 handsets which is the largest number among all the other different types of smartphones in our collection. We therefore use these 15 devices for all the experiments in this section. We found that fingerprinting smartphones of the same maker-and-model was relatively a tougher problem. We again look at fingerprinting these devices, first, through the microphone and speaker individually and then by combining both the microphone and speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Feature Exploration</head><p>First, we determine the dominating subset of acoustic features that can be used for fingerprinting smartphones of the same model. To obtain the fingerprinting data we record audio clips played from 15 Motorola Droid A855 handsets. Each type of audio is recorded 10 times giving us a total of 150 samples from the 15 handsets; 50% of which is used for training and the remaining 50% is used for testing. Table <ref type="table" target="#tab_8">8</ref> shows the maximum F1-score achieved by each acoustic feature for the three different types of audio excerpt. The table also highlights the dominating subset of features selected by our sequential feature selection algorithm. We again find that MFCCs are the dominant features for all categories of audio excerpt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Fingerprinting using Speaker</head><p>We now look at fingerprinting the 15 Motorola Droid A855 handsets. Table <ref type="table" target="#tab_6">9</ref> highlights our findings. We test our fingerprinting approach against three different forms of audio excerpt. We use the acoustic features obtained from our sequential feature selection algorithm as listed in Table <ref type="table" target="#tab_8">8</ref>. From Table <ref type="table" target="#tab_6">9</ref>, we see that we can achieve an F1-score of over 94% in identifying which audio clip originated from which handset. Thus fingerprinting smartphones through speaker seems to be a viable option. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Fingerprinting using Microphone</head><p>We now investigate fingerprinting similar smartphones manufactured by the same vendor through microphone-sourced input. We again use 15 Motorola Droid A855 handsets for these experiments. We use the features obtained through Algorithm 1 which are listed in Table <ref type="table" target="#tab_8">8</ref>. Table <ref type="table" target="#tab_7">10</ref> shows our findings. We see similar results compared to fingerprinting speakers. We were able to achieve an F1-score of 95% in identifying the handset from which the audio excerpt originated. Thus fingerprinting smartphones through microphones also appears to be a feasible option. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.4">Fingerprinting using Microphone and Speaker</head><p>We now look at the effect of combining microphone and speaker in fingerprinting similar smartphones. We use our android app to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">All Combination of Devices</head><p>In this section we look at fingerprinting all the devices in our collection (i.e., 50 android smartphones after excluding iphone5 and Sony Ericsson W518). We combine microphone and speaker to generate the auditory fingerprint of smartphones. We do so because in the previous sections we found that combining speaker and microphone yielded the highest accuracy. First, we perform acoustic feature exploration to determine the dominant features. Table <ref type="table" target="#tab_10">12</ref> highlights our findings. We see that again MFCCs are the dominant features for all categories of audio excerpt. This is expected as we saw similar outcomes in Table <ref type="table">4</ref> and Table <ref type="table" target="#tab_8">8</ref>.</p><p>Next, we evaluate how effectively we can fingerprint the 50 android smartphones. The setting is similar to all the previous experiments where each audio clip is recorded 10 times, 50% of which is used for training and the remaining 50% for testing. We use our android app to collect all the audio samples. Table <ref type="table" target="#tab_11">13</ref> shows our obtained fingerprinting results. We see that we can obtain an F1score of over 98% in fingerprinting all the 50 smartphones. This result suggests that a malicious app having access to microphone and speaker can easily fingerprint smartphones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Sensitivity Analysis</head><p>In this section we investigate how different factors such as audio sampling rate, training set size, the distance from audio source to recorder, and background noise impact our fingerprinting performance. Such investigations will help us determine the conditions under which our fingerprinting approach will be feasible, specially if the attacker is tracking devices in public locations. For the fol-   <ref type="table" target="#tab_10">12</ref> lowing set of experiments we only focus on fingerprinting similarmodel smartphones from the same vendor (as this has been shown to be a tougher problem in the previous section) and consider only fingerprinting speakers as this is applicable to the scenario where the attacker is tracking devices in public locations. We also consider recording only ringtones (i.e., audio clips belonging to our defined 'Instrumental' category in Table <ref type="table" target="#tab_2">3</ref>) for the following experiments. Since we are recording ringtones we use the features highlighted in Table <ref type="table" target="#tab_8">8</ref> under the 'Instrumental' category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.1">Impact of Sampling Rate</head><p>First, we investigate how the sampling rate of audio signals impacts our fingerprinting precision. To do this, we record a ringtone at the following three frequencies: 8kHz, 22.05kHz and 44.1kHz. Each sample is recorded 10 times with half of them being used for training and the other half for testing. Figure <ref type="figure" target="#fig_7">7</ref> shows the average precision and recall obtained under different sampling rates. As we can see from the figure, as sampling frequency decreases, the precision/recall also goes down. This is understandable, because the higher the sampling frequency the more fine-tuned information we have about the audio sample. However, the default sampling fre-quency on most hand-held devices today is 44.1kHz <ref type="bibr" target="#b3">[4]</ref>, with some of the latest models adopting even higher sampling rates [1]. We, therefore, believe sampling rate will not impose an obstacle to our fingerprinting approach, and in future we will be able to capture more fine grained variations with the use of higher sampling rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.2">Varying Training Size</head><p>Next, we consider performance of the classifiers in the presence of limited training data. For this experiment we vary the training set size from 10% to 50% (i.e., from 1 to 5 samples per class) of all available samples. Table <ref type="table" target="#tab_12">14</ref> shows the evolution of the F1-score as training set size is increased (values are reported as percentages). We see that as the training set size increases the F1-score also rises which is expected. However, we see that with only three samples per class we can achieve an F1-score of over 90%. This suggests that we do not need too many training samples to construct a good predictive model. Next, we inspect the impact of distance between the audio source (i.e., smartphone) and recorder (i.e., laptop/PC) on fingerprinting precision. For this experiment we use a separate external microphone as the signal capturing capacity of the microphone embedded inside a laptop degrades drastically as distance increases. We use the relatively inexpensive ($44.79) Audio-Technica ATR-6550 shotgun microphone for this experiment and vary the distance between the external microphone and smartphone from 0.1 meter to 5 meters. Table <ref type="table" target="#tab_3">15</ref> summarizes the F1-scores obtained as the distance between the smartphone and microphone varies. We see that as distance increases, F1-score decreases. This is expected, because the longer the distance between the smartphone and microphone, the harder it becomes to capture the minuscule deviations between audio samples. However, we see that even up to two meters distance we can achieve an F1-score of 93%. This suggests that our device fingerprinting approach works only up to a certain distance using any commercial microphones. However, using specialized microphones, such as parabolic microphones (usually used in capturing animal sounds from a far distance) could help in increasing the fingerprinting precision at even longer distances. In this section we investigate how ambient background noise impacts the performance of our fingerprinting technique. For this experiment we consider scenarios where there is a crowd of people using their smart devices and we are trying to fingerprint those devices by capturing audio signals (in this case ringtones) from the surrounding environment. Table <ref type="table" target="#tab_14">16</ref> highlights the four different scenarios that we are considering. To emulate such environment, external speakers (2 pieces) are placed between the smartphone and microphone while recording is taking place. The external speakers are constantly replaying the respective ambient noise in the background. We consider a distance of two meters from the audio source to recorder. The ambient background sounds were obtained from PacDV <ref type="bibr" target="#b1">[2]</ref> and SoundJay <ref type="bibr" target="#b15">[16]</ref>. We also compute the signalto-noise (SNR) ratio between the original ringtone and the different ambient background noises. The RMS (root-mean-square) value of the different background noises varied from approximately 13% (17.77 dB) to 18% (14.92 dB) of the RMS value of the ringtone under consideration. Table <ref type="table" target="#tab_14">16</ref> shows our findings (values are reported as percentages). We can see that even in the presence of various background noise we can achieve an F1-score of over 91%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION AND LIMITATIONS</head><p>Our approach has a few limitations. First, we experimented with 52 devices manufactured by different vendors; it is possible that a larger target device pool would lower accuracy. That said, distinctions across different device types are more clear; additionally, audio fingerprints may be used in tandem with other techniques, like accelerometer fingerprinting <ref type="bibr" target="#b30">[31]</ref>, to better discriminate between devices. Secondly, most of the experiments took place in a lab setting. However, we studied the impact of ambient background noise and still found our approach to be applicable. Lastly, all the phones used in our experiments were not in mint condition and some of the idiosyncrasies of individual microphones and speakers may have been the result of uneven wear and tear on each device; we believe, however, that this is likely to occur in the real world as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>In this paper we show that it is feasible to fingerprint smart devices through on-board acoustic components like microphones and speakers. As microphones and speakers are one of the most standard components present in almost all smart devices available today, this creates a key privacy concern for users. To demonstrate the feasibility of this approach, we collect fingerprints from 52 different smartphones covering a total of five different brands of smartphones. Our studies show that it is possible to successfully fingerprint smartphones through microphones and speakers, not only under controlled environments, but also in the presence of ambient noise. We believe our findings are important steps towards understanding the full consequences of fingerprinting smart devices through acoustic channels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 (</head><label>2</label><figDesc>b) shows a typical MEMS microspeaker chip and Figure 2(c) shows the components inside the microspeaker</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: The internal architecture of MEMS microphone chip used in smartphones.</figDesc><graphic coords="4,322.72,202.66,131.49,130.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>= 2 ×</head><label>2</label><figDesc>the system we compute average values in the following way: Avg. Precision, AvgPr = AvgP r × AvgRe AvgP r + AvgRe (6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Steps of fingerprinting speakers.</figDesc><graphic coords="6,53.85,387.23,238.80,63.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Steps of fingerprinting microphones.</figDesc><graphic coords="6,53.85,590.48,238.80,63.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Steps of fingerprinting both microphones and speakers.</figDesc><graphic coords="6,328.83,170.23,214.86,68.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: MFCCs of the same audio sample taken from three different handsets manufactured by the same vendor. We can see that some of the coefficients vary significantly, thus enabling us to exploit this feature to fingerprint smartphones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Impact of sampling frequency on precision/recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Explored acoustic features</figDesc><table><row><cell>#</cell><cell>Feature</cell><cell>Dimension</cell><cell>Description</cell></row><row><cell>1</cell><cell>RMS</cell><cell>1</cell><cell>Square root of the arithmetic mean of the squares of the signal strength at various frequencies</cell></row><row><cell>2</cell><cell>ZCR</cell><cell>1</cell><cell>The rate at which the signal changes sign from positive to negative or back</cell></row><row><cell>3</cell><cell>Low-Energy-Rate</cell><cell>1</cell><cell>The percentage of frames with RMS power less than the average RMS power for the whole audio signal</cell></row><row><cell>4</cell><cell>Spectral Centroid</cell><cell>1</cell><cell>Represents the center of mass of a spectral power distribution</cell></row><row><cell>5</cell><cell>Spectral Entropy</cell><cell>1</cell><cell>Captures the peaks of a spectrum and their locations</cell></row><row><cell>6</cell><cell>Spectral Irregularity</cell><cell>1</cell><cell>Measures the degree of variation of the successive peaks of a spectrum</cell></row><row><cell>7</cell><cell>Spectral Spread</cell><cell>1</cell><cell>Defines the dispersion of the spectrum around its centroid</cell></row><row><cell>8</cell><cell>Spectral Skewness</cell><cell>1</cell><cell>Represents the coefficient of skewness of a spectrum</cell></row><row><cell>9</cell><cell>Spectral Kurtosis</cell><cell>1</cell><cell>Measure of the flatness or spikiness of a distribution relative to a normal distribution</cell></row><row><cell>10</cell><cell>Spectral Rolloff</cell><cell>1</cell><cell>Defines the frequency below which 85% of the distribution magnitude is concentrated</cell></row><row><cell cols="2">11 Spectral Brightness</cell><cell>1</cell><cell>Computes the amount of spectral energy corresponding to frequencies higher than a given cut-off threshold</cell></row><row><cell>12</cell><cell>Spectral Flatness</cell><cell>1</cell><cell>Measures how energy is spread across the spectrum</cell></row><row><cell>13</cell><cell>MFCCs</cell><cell>13</cell><cell>Compactly represents spectrum amplitudes residing inside the mel-frequency range</cell></row><row><cell>14</cell><cell>Chromagram</cell><cell>12</cell><cell>Representation of the distribution of energy along the 12 distinct semitones or pitch classes</cell></row><row><cell>15</cell><cell>Tonal Centroid</cell><cell>6</cell><cell>Maps a chromagram onto a six-dimensional Hypertorus structure</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Table 2 highlights the model and quantities of the different phones used in our experiments. Types of phones used</figDesc><table><row><cell>Maker</cell><cell>Model</cell><cell>Quantity</cell></row><row><cell>Apple</cell><cell>iPhone 5</cell><cell>1</cell></row><row><cell>Google</cell><cell>Nexus One Nexus S</cell><cell>14 8</cell></row><row><cell>Samsung</cell><cell>Galaxy S3 Galaxy S4</cell><cell>3 10</cell></row><row><cell>Motorola</cell><cell>Droid A855</cell><cell>15</cell></row><row><cell>Sony Ericsson</cell><cell>W518</cell><cell>1</cell></row><row><cell>Total</cell><cell></cell><cell>52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Types of audio excerpts</figDesc><table><row><cell>Type</cell><cell>Description</cell><cell>Variations</cell></row><row><cell>Instrumental</cell><cell>Musical instruments playing together, e.g., ringtone</cell><cell>4</cell></row><row><cell>Human speech</cell><cell>Small segments of human speech</cell><cell>4</cell></row><row><cell>Song</cell><cell>Combination of human voice &amp; instrumental sound</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Fingerprinting different smartphones using speaker output</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="8">Features  *  AvgP r AvgRe AvgF 1 Features  *  AvgP r AvgRe AvgF 1</cell></row><row><cell>Instrumental</cell><cell>[1,7]</cell><cell>97.6</cell><cell>97.1</cell><cell>97.4</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Human speech</cell><cell>[13]</cell><cell>95.2</cell><cell>94.3</cell><cell>94.8</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Song</cell><cell>[15]</cell><cell>97.6</cell><cell>97.1</cell><cell>97.4</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">*  Features taken from Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Fingerprinting different smartphones using mic</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="4">Features  *  AvgP r AvgRe AvgF 1</cell><cell cols="4">Features  *  AvgP r AvgRe AvgF 1</cell></row><row><cell>Instrumental</cell><cell>[13,1]</cell><cell>95.2</cell><cell>94.3</cell><cell>94.8</cell><cell>[13,1,7]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Human speech</cell><cell>[15,9,1]</cell><cell>95.2</cell><cell>94.3</cell><cell>94.8</cell><cell>[13,15,11]</cell><cell>97.6</cell><cell>97.1</cell><cell>97.4</cell></row><row><cell>Song</cell><cell>[13,1,12]</cell><cell>97.6</cell><cell>97.1</cell><cell>97.4</cell><cell>[13,1,9]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">*  Features taken from Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Fingerprinting different smartphones using mic &amp; speaker</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Features  Instrumental [10]</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Human speech</cell><cell>[12]</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Song</cell><cell>[10]</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">*  Features taken from Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* AvgP r AvgRe AvgF 1 Features * AvgP r AvgRe AvgF 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>Fingerprinting similar smartphones using speaker output</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Features  Instrumental [13,14]</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>[13,14]</cell><cell>98.4</cell><cell>98.1</cell><cell>98.3</cell></row><row><cell>Human speech</cell><cell>[13]</cell><cell>98.9</cell><cell>98.7</cell><cell>98.8</cell><cell>[13,14]</cell><cell>98.9</cell><cell>98.7</cell><cell>98.8</cell></row><row><cell>Song</cell><cell>[13,7]</cell><cell>93.2</cell><cell>92</cell><cell>92.6</cell><cell>[13,14]</cell><cell>95.6</cell><cell>93.3</cell><cell>94.5</cell></row><row><cell cols="3">*  Feature numbers taken from Table 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* AvgP r AvgRe AvgF 1 Features * AvgP r AvgRe AvgF 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Fingerprinting similar smartphones using microphone</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="8">Features  *  AvgP r AvgRe AvgF 1 Features  *  AvgP r AvgRe AvgF 1</cell></row><row><cell>Instrumental</cell><cell>[13,8,12]</cell><cell>95.9</cell><cell>94.7</cell><cell>95.3</cell><cell>[13,8,12]</cell><cell>96</cell><cell>94.7</cell><cell>95.3</cell></row><row><cell>Human speech</cell><cell>[13]</cell><cell>98.9</cell><cell>98.7</cell><cell>98.8</cell><cell>[13,14]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Song</cell><cell>[13,14,10]</cell><cell>96.4</cell><cell>96</cell><cell>96.2</cell><cell>[13,14]</cell><cell>96.5</cell><cell>95.7</cell><cell>96.1</cell></row><row><cell cols="3">*  Feature numbers taken from Table 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Feature exploration using sequential forward selection technique for same model smartphones per audio clip, of which is used for training and the remaining half for testing. Table11highlights our We see that we were able to fingerprint all test samples accurately. Thus combining the idiosyncrasies of both the speaker and microphone seems to be the best option to distinguish smartphones of same maker and model. So, if a malicious app can get access to the speaker (which does not require explicit permission) and microphone (which may require explicit permission, but many games nowadays require access to microphone anyway) it can successfully track individual devices.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Fingerprinting Speakers</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Fingerprinting Microphones</cell><cell></cell><cell></cell><cell cols="4">Fingerprinting Speakers and Microphones</cell></row><row><cell>#</cell><cell>Feature</cell><cell cols="2">Instrumental</cell><cell cols="4">Maximum F1-Score (%) Human Speech</cell><cell cols="2">Song</cell><cell cols="2">Instrumental</cell><cell cols="3">Maximum F1-Score (%) Human Speech</cell><cell>Song</cell><cell></cell><cell cols="4">Maximum F1-Score (%) Instrumental Human Speech</cell><cell>Song</cell></row><row><cell></cell><cell></cell><cell>k-NN</cell><cell cols="4">GMM k-NN GMM</cell><cell cols="2">k-NN</cell><cell>GMM</cell><cell>k-NN</cell><cell>GMM</cell><cell></cell><cell>k-NN</cell><cell>GMM</cell><cell>k-NN</cell><cell>GMM</cell><cell cols="4">k-NN GMM k-NN GMM k-NN GMM</cell></row><row><cell>1</cell><cell>RMS</cell><cell>34.9</cell><cell cols="2">33.8</cell><cell>16.6</cell><cell>12.3</cell><cell>20</cell><cell></cell><cell>25.7</cell><cell>40.2</cell><cell>36.9</cell><cell></cell><cell>19.6</cell><cell>20.1</cell><cell>23.5</cell><cell>28.6</cell><cell>87.2</cell><cell>83</cell><cell>92.1</cell><cell>93</cell><cell>89.5</cell><cell>89.2</cell></row><row><cell>2</cell><cell>ZCR</cell><cell>29.7</cell><cell cols="2">26.5</cell><cell>12.2</cell><cell>14.4</cell><cell>13</cell><cell></cell><cell>7.1</cell><cell>22.7</cell><cell>29.6</cell><cell></cell><cell>26.2</cell><cell>22.6</cell><cell>44.5</cell><cell>41.9</cell><cell>59.8</cell><cell>60.5</cell><cell>56.8</cell><cell>58.4</cell><cell>67.6</cell><cell>75.2</cell></row><row><cell>3</cell><cell>Low-Energy-Rate</cell><cell>12.5</cell><cell cols="2">14.8</cell><cell>15</cell><cell>5.7</cell><cell>21.8</cell><cell></cell><cell>18.7</cell><cell>22.6</cell><cell>24.8</cell><cell></cell><cell>5.2</cell><cell>7.4</cell><cell>10.7</cell><cell>13.4</cell><cell>67.4</cell><cell>70.9</cell><cell>30.1</cell><cell>36.7</cell><cell>69.7</cell><cell>64.5</cell></row><row><cell>4</cell><cell>Spectral Centroid</cell><cell>28</cell><cell cols="2">30.5</cell><cell>12.2</cell><cell>19</cell><cell>39.9</cell><cell></cell><cell>40.3</cell><cell>17.3</cell><cell>24.8</cell><cell></cell><cell>16.6</cell><cell>12.9</cell><cell>33.7</cell><cell>35.7</cell><cell>30.1</cell><cell>27.5</cell><cell>25.7</cell><cell>30.1</cell><cell>35.1</cell><cell>32.7</cell></row><row><cell>5</cell><cell>Spectral Entropy</cell><cell>20.9</cell><cell cols="2">19.8</cell><cell>14.2</cell><cell>16.6</cell><cell>33.9</cell><cell></cell><cell>26.3</cell><cell>29.1</cell><cell>22.2</cell><cell></cell><cell>15.2</cell><cell>15.1</cell><cell>40.3</cell><cell>36</cell><cell>78.5</cell><cell>70.3</cell><cell>52.9</cell><cell>54.8</cell><cell>81.8</cell><cell>81.8</cell></row><row><cell>6</cell><cell>Spectral Irregularity</cell><cell>14.5</cell><cell cols="2">11.7</cell><cell>7.4</cell><cell>14.7</cell><cell>11.8</cell><cell></cell><cell>17.5</cell><cell>12.6</cell><cell>16.3</cell><cell></cell><cell>13.2</cell><cell>17.9</cell><cell>15.8</cell><cell>18.6</cell><cell>63.9</cell><cell>54.6</cell><cell>32.5</cell><cell>33.6</cell><cell>62.5</cell><cell>67.1</cell></row><row><cell>7</cell><cell>Spectral Spread</cell><cell>36.4</cell><cell cols="2">43.7</cell><cell>11.3</cell><cell>14.3</cell><cell>35.2</cell><cell></cell><cell>38.4</cell><cell>17.2</cell><cell>22.6</cell><cell></cell><cell>16.4</cell><cell>14.9</cell><cell>36.2</cell><cell>34.8</cell><cell>84.6</cell><cell>81.3</cell><cell>67.6</cell><cell>62.8</cell><cell>87</cell><cell>87.4</cell></row><row><cell>8</cell><cell>Spectral Skewness</cell><cell>33.9</cell><cell cols="2">29.1</cell><cell>13.3</cell><cell>15.5</cell><cell>31.5</cell><cell></cell><cell>40.3</cell><cell>31.8</cell><cell>28.1</cell><cell></cell><cell>20.8</cell><cell>13.7</cell><cell>38</cell><cell>43.1</cell><cell>85.7</cell><cell>88.3</cell><cell>58.7</cell><cell>54.4</cell><cell>70.9</cell><cell>68.9</cell></row><row><cell>9</cell><cell>Spectral Kurtosis</cell><cell>30.5</cell><cell cols="2">29.1</cell><cell>11.6</cell><cell>16</cell><cell>31.1</cell><cell></cell><cell>36.8</cell><cell>28.5</cell><cell>26.1</cell><cell></cell><cell>20.3</cell><cell>14</cell><cell>45.8</cell><cell>39.2</cell><cell>80.3</cell><cell>80.6</cell><cell>51.9</cell><cell>49.9</cell><cell>82.2</cell><cell>76.2</cell></row><row><cell>10</cell><cell>Spectral Rolloff</cell><cell>40.4</cell><cell>39</cell><cell></cell><cell>14.9</cell><cell>14.3</cell><cell>38.7</cell><cell></cell><cell>41.1</cell><cell>30</cell><cell>32.8</cell><cell></cell><cell>15.1</cell><cell>11.6</cell><cell>46.1</cell><cell>44</cell><cell>79.4</cell><cell>73.4</cell><cell>46.9</cell><cell>51.5</cell><cell>77.2</cell><cell>71.8</cell></row><row><cell>11</cell><cell>Spectral Brightness</cell><cell>32.1</cell><cell cols="2">31.6</cell><cell>18.9</cell><cell>21.8</cell><cell>18.5</cell><cell></cell><cell>17.9</cell><cell>22.5</cell><cell>20.3</cell><cell></cell><cell>12.6</cell><cell>16</cell><cell>33.1</cell><cell>27.4</cell><cell>86</cell><cell>88</cell><cell>75.2</cell><cell>69.2</cell><cell>87.5</cell><cell>79.5</cell></row><row><cell>12</cell><cell>Spectral Flatness</cell><cell>34.9</cell><cell>31</cell><cell></cell><cell>19.8</cell><cell>13.3</cell><cell>32.4</cell><cell></cell><cell>30</cell><cell>24.6</cell><cell>23.8</cell><cell></cell><cell>17.2</cell><cell>12.2</cell><cell>39.2</cell><cell>35.5</cell><cell>79.8</cell><cell>79</cell><cell>45.5</cell><cell>45.4</cell><cell>86.4</cell><cell>87.2</cell></row><row><cell>13</cell><cell>MFCCs</cell><cell>90.4</cell><cell cols="2">96.5</cell><cell>91.3</cell><cell>97.5</cell><cell>90</cell><cell></cell><cell>91.4</cell><cell>89</cell><cell>93.5</cell><cell></cell><cell>98.8</cell><cell>96.2</cell><cell>94.1</cell><cell>97.5</cell><cell>100</cell><cell>100</cell><cell>98.7</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>14</cell><cell>Chromagram</cell><cell>79.1</cell><cell cols="2">70.6</cell><cell>72.9</cell><cell>66</cell><cell>80.6</cell><cell></cell><cell>80</cell><cell>71.5</cell><cell>55.3</cell><cell></cell><cell>75</cell><cell>88.7</cell><cell>87.3</cell><cell>85.3</cell><cell>98.8</cell><cell>95.8</cell><cell>97.6</cell><cell>100</cell><cell>100</cell><cell>96.5</cell></row><row><cell>15</cell><cell>Tonal Centroid</cell><cell>77</cell><cell>60</cell><cell></cell><cell>65.4</cell><cell>53.4</cell><cell>63.6</cell><cell></cell><cell>53.8</cell><cell>67.8</cell><cell>51.3</cell><cell></cell><cell>70</cell><cell>70.8</cell><cell>83.1</cell><cell>79.4</cell><cell>98.8</cell><cell>94.8</cell><cell>95.2</cell><cell>92.7</cell><cell>100</cell><cell>98.8</cell></row><row><cell cols="2">Sequential Feature Selection</cell><cell cols="3">[13,14] [13,14]</cell><cell>[13]</cell><cell cols="4">[13,14] [13,7] [13,14]</cell><cell cols="3">[13,8,12] [13,8,12]</cell><cell>[13]</cell><cell cols="3">[13,14,2] [13,14,10] [13,14]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell></row><row><cell></cell><cell>Max F1-Score</cell><cell>97.5</cell><cell cols="2">97.7</cell><cell>93.7</cell><cell>98.2</cell><cell>91.5</cell><cell></cell><cell>92.9</cell><cell>93</cell><cell>96.7</cell><cell></cell><cell>98.8</cell><cell>97.5</cell><cell>96.3</cell><cell>97.9</cell><cell></cell><cell>100</cell><cell>98.7</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">collect 10 samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Fingerprinting similar smartphones using mic &amp; speaker</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="8">Features  *  AvgP r AvgRe AvgF 1 Features  *  AvgP r AvgRe AvgF 1</cell></row><row><cell>Instrumental</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Human speech</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Song</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">*  Features taken from Table 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Feature exploration using sequential forward selection technique for all smartphones</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Maximum F1-Score (%)</cell><cell></cell></row><row><cell>#</cell><cell>Feature</cell><cell cols="2">Instrumental</cell><cell cols="2">Human Speech</cell><cell cols="2">Song</cell></row><row><cell></cell><cell></cell><cell cols="6">k-NN GMM k-NN GMM k-NN GMM</cell></row><row><cell>1</cell><cell>RMS</cell><cell>82.7</cell><cell>80</cell><cell>87.3</cell><cell>84</cell><cell>78.7</cell><cell>76.8</cell></row><row><cell>2</cell><cell>ZCR</cell><cell>51.3</cell><cell>48.2</cell><cell>50.3</cell><cell>45.9</cell><cell>48.5</cell><cell>45.9</cell></row><row><cell>3</cell><cell>Low-Energy-Rate</cell><cell>45.2</cell><cell>40.6</cell><cell>19.4</cell><cell>15.4</cell><cell>31.9</cell><cell>33.8</cell></row><row><cell>4</cell><cell>Spectral Centroid</cell><cell>35.6</cell><cell>34.7</cell><cell>23.7</cell><cell>25.8</cell><cell>25.7</cell><cell>30.1</cell></row><row><cell>5</cell><cell>Spectral Entropy</cell><cell>56.2</cell><cell>60.8</cell><cell>46.3</cell><cell>48.1</cell><cell>67.7</cell><cell>67.7</cell></row><row><cell>6</cell><cell>Spectral Irregularity</cell><cell>46.1</cell><cell>47</cell><cell>25.9</cell><cell>23.6</cell><cell>26.9</cell><cell>35.3</cell></row><row><cell>7</cell><cell>Spectral Spread</cell><cell>57.4</cell><cell>57</cell><cell>54.2</cell><cell>49.7</cell><cell>70.9</cell><cell>74.1</cell></row><row><cell>8</cell><cell>Spectral Skewness</cell><cell>50.3</cell><cell>53.9</cell><cell>34.5</cell><cell>32.5</cell><cell>52.7</cell><cell>59.9</cell></row><row><cell>9</cell><cell>Spectral Kurtosis</cell><cell>45</cell><cell>47.7</cell><cell>37.1</cell><cell>38.6</cell><cell>51.5</cell><cell>54.2</cell></row><row><cell>10</cell><cell>Spectral Rolloff</cell><cell>49.5</cell><cell>53.5</cell><cell>48.4</cell><cell>45.9</cell><cell>59.1</cell><cell>62.8</cell></row><row><cell>11</cell><cell>Spectral Brightness</cell><cell>52.1</cell><cell>54.5</cell><cell>38.1</cell><cell>35.3</cell><cell>59.2</cell><cell>61.7</cell></row><row><cell>12</cell><cell>Spectral Flatness</cell><cell>61</cell><cell>60.1</cell><cell>61.6</cell><cell>63.4</cell><cell>67.3</cell><cell>68.3</cell></row><row><cell>13</cell><cell>MFCCs</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>99.6</cell><cell>100</cell><cell>99.6</cell></row><row><cell>14</cell><cell>Chromagram</cell><cell>96.2</cell><cell>93.4</cell><cell>98.9</cell><cell>95.8</cell><cell>99.6</cell><cell>98.2</cell></row><row><cell>15</cell><cell>Tonal Centroid</cell><cell>96</cell><cell>89</cell><cell>95.5</cell><cell>91.8</cell><cell>98.5</cell><cell>98.5</cell></row><row><cell cols="2">Sequential Feature Selection</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell><cell>[13]</cell></row><row><cell></cell><cell>Max F1-Score</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>99.6</cell><cell>100</cell><cell>99.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>Fingerprinting all smartphones using mic &amp; speaker</figDesc><table><row><cell>Audio</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Features  Instrumental [13]</cell><cell>99.3</cell><cell>98.8</cell><cell>99</cell><cell>[13]</cell><cell>98.6</cell><cell>98.1</cell><cell>98.3</cell></row><row><cell>Human speech</cell><cell>[13]</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>[13]</cell><cell>99.4</cell><cell>99.2</cell><cell>99.3</cell></row><row><cell>Song</cell><cell>[13]</cell><cell>99.7</cell><cell>99.6</cell><cell>99.6</cell><cell>[13]</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="2">*  Features taken from Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* AvgP r AvgRe AvgF 1 Features * AvgP r AvgRe AvgF 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>Impact of varying training size</figDesc><table><row><cell>Training</cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell></row><row><cell>samples</cell><cell></cell><cell cols="2">Features [13,14]  *</cell><cell></cell><cell cols="2">Features [13,14]  *</cell></row><row><cell cols="7">per class AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1</cell></row><row><cell>1</cell><cell>42</cell><cell>49.3</cell><cell>45.3</cell><cell>50</cell><cell>53.3</cell><cell>51.6</cell></row><row><cell>2</cell><cell>79.2</cell><cell>80</cell><cell>79.6</cell><cell>80.4</cell><cell>80</cell><cell>80.2</cell></row><row><cell>3</cell><cell>91.3</cell><cell>89.3</cell><cell>90.2</cell><cell>91.7</cell><cell>89.3</cell><cell>90.5</cell></row><row><cell>4</cell><cell>95.3</cell><cell>94.7</cell><cell>95</cell><cell>95.6</cell><cell>94.7</cell><cell>95.1</cell></row><row><cell>5</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>98.4</cell><cell>98.1</cell><cell>98.3</cell></row><row><cell cols="4">*  Feature numbers taken from Table 8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">6.7.3 Varying Distance between Speaker and Recorder</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table Impact</head><label>Impact</label><figDesc>Feature numbers taken from Table86.7.4 Impact of Ambient Background Noise</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">of varying distance</cell><cell></cell></row><row><cell></cell><cell></cell><cell>k-NN</cell><cell></cell><cell></cell><cell>GMM</cell><cell></cell></row><row><cell>(in meters)</cell><cell cols="6">Features [13,14] AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1 Features [13,14]  *</cell></row><row><cell>0.1</cell><cell>96.7</cell><cell>96</cell><cell>96.3</cell><cell>98.4</cell><cell>98.1</cell><cell>98.3</cell></row><row><cell>1</cell><cell>92.7</cell><cell>91.5</cell><cell>92</cell><cell>95.2</cell><cell>94.7</cell><cell>94.9</cell></row><row><cell>2</cell><cell>88.2</cell><cell>87.6</cell><cell>87.9</cell><cell>94.5</cell><cell>92</cell><cell>93.2</cell></row><row><cell>3</cell><cell>76.7</cell><cell>76</cell><cell>76.3</cell><cell>78.9</cell><cell>84</cell><cell>81.4</cell></row><row><cell>4</cell><cell>70.2</cell><cell>64</cell><cell>67</cell><cell>76.8</cell><cell>76</cell><cell>76.4</cell></row><row><cell>5</cell><cell>64.5</cell><cell>62.7</cell><cell>63.6</cell><cell>77</cell><cell>73.3</cell><cell>75.1</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 :</head><label>16</label><figDesc>Impact of ambient background noise Feature numbers taken from Table8</figDesc><table><row><cell>Environments</cell><cell>SNR (dB)</cell><cell cols="6">k-NN Features [13,14]  *  AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1 GMM Features [13,14]  *</cell></row><row><cell cols="2">Shopping Mall 15.85</cell><cell>88.8</cell><cell>85.3</cell><cell>87</cell><cell>95.1</cell><cell>93.3</cell><cell>94.2</cell></row><row><cell cols="2">Restaurant/Cafe 17.77</cell><cell>90.5</cell><cell>89.7</cell><cell>90.1</cell><cell>92.5</cell><cell>90.7</cell><cell>91.6</cell></row><row><cell>City Park</cell><cell>15.43</cell><cell>91.7</cell><cell>90</cell><cell>90.8</cell><cell>95.2</cell><cell>94.1</cell><cell>94.6</cell></row><row><cell>Airport Gate</cell><cell>14.92</cell><cell>91.3</cell><cell>89.5</cell><cell>90.4</cell><cell>94.5</cell><cell>93.3</cell><cell>93.9</cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Thomas S. Benjamin for his valuable input during the initial phase of the project and all the anonymous reviewers for their valuable feedback. We would specially like to thank Romit Roy Choudhury and his group at UIUC for providing us with the bulk of smartphones used in our experiments. On the same note we would like to extend our gratitude to the Computer Science department at UIUC for providing us with the Motorola Droid phones. This paper reports on work that was supported in part by NSF CNS 0953655.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Feature exploration using sequential forward selection technique for different maker-and-model smartphones # Feature Fingerprinting Speakers Fingerprinting Microphones Fingerprinting Speakers and Microphones Maximum F1-Score (%) Maximum F1-Score (%) Maximum F1-Score (%) Instrumental Human Speech Song Instrumental Human Speech Song Instrumental Human Speech Song k-NN GMM k-NN GMM k-NN GMM k-NN GMM k-NN GMM k-NN GMM k</title>
		<ptr target="-NNGMMk-NNGMMk-NNGMM" />
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.pacdv.com/sounds/ambience_sounds.html" />
		<title level="m">Ambient Sound Effects</title>
		<imprint>
			<date type="published" when="2014">05/15/2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Audacity is free, open source, cross-platform software for recording and editing sounds</title>
		<idno>05/15/2014</idno>
		<ptr target="http://audacity.sourceforge.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://www.wolfsonmicro.com/documents/uploads/misc/en/Audio4Smartphones.pdf" />
		<title level="m">Audio 4 Smartphones -Wolfson Microelectronics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://mobithinking.com/mobile-marketing-tools/latest-mobile-stats/a" />
		<title level="m">Global mobile statistics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="https://play.google.com/store/apps/details?id=uk.ac.cam.cl.dtg.android.audionetworking.hertz" />
		<title level="m">Hertz, the WAV recorder</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="http://www.eeherald.com/section/design-guide/mems-microphone.html" />
		<title level="m">How MEMS Microphones Fucntion</title>
		<imprint>
			<date type="published" when="2014">05/15/2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://www.gartner.com/newsroom/id/2335616" />
		<title level="m">IPhone and Android Apps Breach Privacy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="http://online.wsj.com/article/SB10001424052748704694004576020083703574602.html" />
		<title level="m">IPhone and Android Apps Breach Privacy</title>
		<imprint>
			<date type="published" when="2014">05/15/2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://www.digikey.com/supply-chain-hq/us/en/articles/semiconductors/mems-microphone-market-revenues-soar-42-in-2012/1497" />
		<title level="m">MEMS microphone market</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://www.comsol.com/blogs/mems-microphone-model-presented-asa-166-san-francisco/" />
		<title level="m">MEMS Microphone Model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://electroiq.com/blog/2013/02/mems-microphone-shipments-to-climb-30-percent-this-year/" />
		<title level="m">MEMS microphone shipments to climb 30 percentage in 2013</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mirtoolbox</surname></persName>
		</author>
		<idno>05/15/2014</idno>
		<ptr target="https://www.jyu.fi/hum/laitokset/musiikki/en/research/coe/materials/mirtoolbox" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mobile device market to reach 2.6 billion units by</title>
		<idno>05/15/2014</idno>
		<ptr target="http://www.canalys.com/newsroom/mobile-device-market-reach-26-billion-units-2016" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Algorithms for Pattern Recognition</title>
		<author>
			<persName><surname>Netlab</surname></persName>
		</author>
		<idno>05/15/2014</idno>
		<ptr target="http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/book/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="http://www.soundjay.com/ambient-sounds.html" />
		<title level="m">SOUNDJAY-Ambient Sound Effects</title>
		<imprint>
			<date type="published" when="2014">05/15/2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<idno>05/15/2014</idno>
		<ptr target="http://www.isuppli.com/MEMS-and-Sensors/MarketWatch/pages/Top" />
		<title level="m">Top MEMS Microphone Suppliers</title>
		<imprint/>
	</monogr>
	<note type="report_type">MEMS-Microphone-Suppliers-All-CanCounton-Apple-for-Clear-and-Resounding-Success</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FPDetective: dusting the web for fingerprinters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Juarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikiforakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gürses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piessens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preneel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM conference on Computer and Communications Security, CCS &apos;13</title>
		<meeting>the 2013 ACM conference on Computer and Communications Security, CCS &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1129" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio Thumbnailing of Popular Music Using Chroma-based Representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bartsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Wakefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="104" />
			<date type="published" when="2005-02">Feb 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Tutorial on Text-Independent Speaker Verification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bimbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Magrin-Chagnolleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Merlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petrovska-Delacretaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="430" to="451" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wireless Device Identification with Radiometric Signatures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Brik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International on Mobile Computing and Networking</title>
		<meeting>the 14th ACM International on Mobile Computing and Networking</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker recognition: tutorial</title>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1437" to="1462" />
			<date type="published" when="1997-09">Sep 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Review of Audio Fingerprinting</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. VLSI Signal Process. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2005-11">Nov 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Speaker, yoke thereof and method for manufacturing yoke</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<ptr target="http://www.google.com/patents/US8094867" />
		<imprint>
			<date type="published" when="2012-01">Jan 2012</date>
			<biblScope unit="page">867</biblScope>
		</imprint>
	</monogr>
	<note>US Patent 8,094</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A silicon microspeaker for hearing instruments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Micromechanics and Microengineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="859" to="866" />
			<date type="published" when="2004-07">Jul 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Breaking Assumptions: Distinguishing Between Seemingly Identical Items Using Cheap Sensors</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Clarkson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Princeton, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Suspect Identities: A History of Fingerprinting and Criminal Identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fingerprinting smart devices through embedded acoustic components</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caesar</surname></persName>
		</author>
		<idno>CoRR, abs/1403</idno>
		<ptr target="http://arxiv.org/abs/1403.3366" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3366</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying Unique Devices Through Wireless Fingerprinting</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C C</forename><surname>Desmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Pheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Conference on Wireless Network Security, WiSec &apos;08</title>
		<meeting>the 1st ACM Conference on Wireless Network Security, WiSec &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AccelPrint: Imperfections of Accelerometers Make Smartphones Trackable</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nelakuditi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Network and Distributed System Security Symposium, NDSS&apos;14</title>
		<meeting>the 20th Annual Network and Distributed System Security Symposium, NDSS&apos;14</meeting>
		<imprint>
			<date type="published" when="2014-02">Feb 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How Unique is Your Web Browser?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eckersley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Privacy Enhancing Technologies, PETS&apos;10</title>
		<meeting>the 10th International Conference on Privacy Enhancing Technologies, PETS&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PiOS: Detecting Privacy Leaks in iOS Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Egele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kruegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kirda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Network and Distributed System Security Symposium, NDSS &apos;11</title>
		<meeting>the 17th Annual Network and Distributed System Security Symposium, NDSS &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TaintDroid: An Information-flow Tracking System for Realtime Privacy Monitoring on Smartphones</title>
		<author>
			<persName><forename type="first">W</forename><surname>Enck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;10</title>
		<meeting>the 9th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Android Permissions: User Attention, Comprehension, and Behavior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Felt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Egelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Usable Privacy and Security, SOUPS &apos;12</title>
		<meeting>the 8th Symposium on Usable Privacy and Security, SOUPS &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Passive Data Link Layer 802.11 Wireless Device Driver Fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tabriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Neagoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Randwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Security Symposium</title>
		<meeting>the 15th USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Device identification via analog signal fingerprinting: A matched approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Network and Distributed System Security Symposium</title>
		<meeting>the 13th Annual Network and Distributed System Security Symposium</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AndroidLeaks: Automatically Detecting Potential Privacy Leaks in Android Applications on a Large Scale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crussell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Trust and Trustworthy Computing, TRUST&apos;12</title>
		<meeting>the 5th International Conference on Trust and Trustworthy Computing, TRUST&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="291" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence Number-Based MAC Address Spoof Detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8th International Symposium on Recent Advances in Intrusion Detection, RAID &apos;05</title>
		<meeting>8th International Symposium on Recent Advances in Intrusion Detection, RAID &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Content-based audio classification and retrieval by support vector machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="215" />
			<date type="published" when="2003-01">Jan 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An Introduction to Variable and Feature Selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03">Mar 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Highly Robust Audio Fingerprinting System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Symposium on Music Information Retrieval</title>
		<meeting>the 2002 International Symposium on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="107" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Compact and Low-Cost MEMS Loudspeaker for Digital Hearing Aids</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Je</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bakkaloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="348" to="358" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Conundrum of Permissions: Installing Applications on an Android Smartphone</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Consolvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cranor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Financial Cryptography and Data Security</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Remote Physical Device Fingerprinting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Claffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Dependable Secur. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2005-04">Apr 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Specific emitter identification (SEI) and classical parameter fusion technology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WESCON &apos;93</title>
		<imprint>
			<date type="published" when="1993-09">Sep 1993</date>
			<biblScope unit="page" from="377" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Comparative Study on Content-based Music Genre Classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Securing Wireless Systems via Lower Layer Enforcements</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Trappe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Workshop on Wireless Security, WiSe &apos;06</title>
		<meeting>the 5th ACM Workshop on Wireless Security, WiSe &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Nmap: a free network mapping and security scanning tool</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lyon</surname></persName>
		</author>
		<idno>05/15/2014</idno>
		<ptr target="http://nmap.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">App Attack: Surviving the Explosive Growth of Mobile Apps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mahaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hering</surname></persName>
		</author>
		<ptr target="https://media.blackhat.com/bh-us-10/presentations/Mahaffey_Hering/Blackhat-USA-2010-Mahaffey-Hering-Lookout-App-Genome-slides.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Features for Audio and Music Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Breebaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 International Symposium on Music Information Retrieval</title>
		<meeting>the 2003 International Symposium on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Estimation and removal of clock skew from network delay measurements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Skelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual IEEE International Conference on Computer Communications, INFOCOM &apos;99</title>
		<meeting>the 18th Annual IEEE International Conference on Computer Communications, INFOCOM &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fingerprinting Information in JavaScript Implementations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bogenreif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yilek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of W2SP</title>
		<meeting>W2SP</meeting>
		<imprint>
			<date type="published" when="2011-05">2011. May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Device fingerprinting to enhance wireless security using nonparametric Bayesian method</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
			<biblScope unit="page" from="1404" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Why Johnny Can&apos;t Browse in Peace: On the Uniqueness of Web Browsing History Patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Olejnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castelluccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs)</title>
		<meeting>the 5th Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">11 User Fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Greenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International Conference on Mobile Computing and Networking</title>
		<meeting>the 13th Annual International Conference on Mobile Computing and Networking</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">802</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust Location Distinction Using Temporal Link Signatures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kasera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual ACM International Conference on Mobile Computing and Networking, MobiCom &apos;07</title>
		<meeting>the 13th Annual ACM International Conference on Mobile Computing and Networking, MobiCom &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Speaker verification using adapted gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Quatieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="19" to="41" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cellular security: better, but foes still lurk</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riezenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="39" to="42" />
			<date type="published" when="2000-06">Jun 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Information fusion in biometrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2115" to="2125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Shabtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fledel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kanonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Google Android: A Comprehensive Security Assessment</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards high fidelity high efficiency MEMS microspeakers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lefeuvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woytasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dufour-Gergam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosseboeuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lemarquand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemarquand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Sensors</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2426" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A Comparative Study on Feature Selection in Text Categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Machine Learning, ICML &apos;97</title>
		<meeting>the Fourteenth International Conference on Machine Learning, ICML &apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Environment-aware clock skew estimation and synchronization for wireless sensor networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual IEEE International Conference on Computer Communications, INFOCOM &apos;12</title>
		<meeting>the 31st Annual IEEE International Conference on Computer Communications, INFOCOM &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Yarochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kydyraliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Arkin</surname></persName>
		</author>
		<ptr target="http://ofirarkin.wordpress.com/xprobe/" />
		<title level="m">Xprobe project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Host Fingerprinting and Tracking on the Web:Privacy and Security Implications</title>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the 19th Annual Network and Distributed System Security Symposium, NDSS&apos;12</title>
		<meeting>eddings of the 19th Annual Network and Distributed System Security Symposium, NDSS&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Taming Information-Stealing Smartphone Applications (on Android)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Freeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Trust and Trustworthy Computing</title>
		<meeting>the 4th International Conference on Trust and Trustworthy Computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="93" to="107" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
