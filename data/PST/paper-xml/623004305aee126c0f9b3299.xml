<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAND+: Scalable Graph Random Neural Networks</title>
				<funder ref="#_qucTtGf">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-12">12 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tinglin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
							<email>ziqiyin18@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
							<email>evgeny.kharlamov@de.bosch.com</email>
							<affiliation key="aff3">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAND+: Scalable Graph Random Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-12">12 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3512044</idno>
					<idno type="arXiv">arXiv:2203.06389v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>? Computing methodologies ? Semi-supervised learning settings</term>
					<term>? Information systems ? Social networks Graph Neural Networks</term>
					<term>Scalability</term>
					<term>Semi-Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have been widely adopted for semisupervised learning on graphs. A recent study shows that the graph random neural network (GRAND) model can generate state-ofthe-art performance for this problem. However, it is difficult for GRAND to handle large-scale graphs since its effectiveness relies on computationally expensive data augmentation procedures. In this work, we present a scalable and high-performance GNN framework GRAND+ for semi-supervised graph learning. To address the above issue, we develop a generalized forward push (GFPush) algorithm in GRAND+ to pre-compute a general propagation matrix and employ it to perform graph data augmentation in a mini-batch manner. We show that both the low time and space complexities of GFPush enable GRAND+ to efficiently scale to large graphs. Furthermore, we introduce a confidence-aware consistency loss into the model optimization of GRAND+, facilitating GRAND+'s generalization superiority. We conduct extensive experiments on seven public datasets of different sizes. The results demonstrate that GRAND+ 1) is able to scale to large graphs and costs less running time than existing scalable GNNs, and 2) can offer consistent accuracy improvements over both full-batch and scalable GNNs across all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph structure is a commonplace of both our physical and virtual worlds, such as social relationships, chemical bonds, and information diffusion. The inherit incompleteness of the real-world graph data sparks enormous interests in the problem of semi-supervised learning on graphs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref>. To date, graph neural networks (GNNs) have been considered by many as the de facto way to address this problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Briefly, GNNs leverage the graph structure among data samples to facilitate model predictions, enabling them to produce prominent performance improvements over traditional semi-supervised learning methods <ref type="bibr" target="#b31">[32]</ref>.</p><p>However, there are remaining challenges for GNN-based semisupervised learning solutions. Notably, the generalization of GNNs usually does not form their strengths, as most of them only use a supervised loss to learn parameters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. This setup makes the model prone to overfit the limited labeled samples, thereby degrading the prediction performance over unseen samples. To overcome this issue, the graph random neural network (GRAND) <ref type="bibr" target="#b11">[12]</ref> designs graph data augmentation and consistency regularization strategies for GNNs. These designs enable it to bring significant performance gains over existing GNNs for semi-supervised learning on Cora, Citeseer and Pubmed.</p><p>Specifically, GRAND develops the random propagation operation to generate effective structural data augmentations. It is then trained with both the supervised loss on labeled nodes and the consistency regularization loss on different augmentations of unlabeled nodes. To achieve a good graph augmentation, random propagation in GRAND proposes to use a mixed-order adjacency matrix to propagate the feature matrix. The propagation essentially requires the power iteration of the adjacency matrix at every training step, making it computationally challenging to scale GRAND to large-scale graphs.</p><p>To address this issue, we present the GRAND+ framework for large-scale semi-supervised learning on graphs. GRAND+ is a scalable GNN consistency regularization method. In GRAND+, we introduce efficient approximation techniques to perform random propagation in a mini-batch manner, addressing the scalability limitation of GRAND. Furthermore, we improve GRAND by adopting a confidence-aware loss for regulating the consistency between different graph data augmentations. This design stabilizes the training process and provides GRAND+ with good generalization. Specifically, GRAND+ comprises the following techniques:</p><p>? Generalized feature propagation: We propose a generalized mixedorder matrix to perform random feature propagation. Such matrix offers a set of tunable weights to control the importance of different orders of neighborhoods and thus offers a flexible mechanism for dealing with complex real-world graphs. ? Efficient approximation: Inspired by recent matrix approximation based GNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>, GRAND+ adopts an approximation method-Generalized Forward Push (GFPush)-to efficiently calculate the generalized propagation matrix. This enables GRAND+ to perform random propagation and model learning in a mini-batch manner, offering the model with significant scalability. ? Confidence-aware loss: We design a confidence-aware loss for the GRAND+ regularization framework. This helps filter out potential noises during the consistency training by ignoring highly uncertain unlabeled samples, thus improving the generalization performance of GRAND+.</p><p>We conduct comprehensive experiments on seven public graph datasets with different genres and scales to demonstrate the performance of GRAND+. Overall, GRAND+ yields the best classification results compared to ten GNN baselines on three benchmark datasets and surpasses five representative scalable GNNs on the other four relatively large datasets with efficiency benefits. For example, GRAND+ achieves a state-of-the-art accuracy of 85.0% on Pubmed. On MAG-Scholar-C with 12.4 million nodes, GRAND+ is about 10 fold faster than FastGCN and GraphSAINT, and offers a 4.9% accuracy gain over PPRGo-previously the fastest method on this dataset-with a comparable running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SEMI-SUPERVISED GRAPH LEARNING 2.1 Problem</head><p>In graph-based semi-supervised learning, the data samples are organized as a graph ? = (? , ?), where each node ? ? ? represents a data sample and ? ? ? ?? is a set of edges that denote the relationships between each pair of nodes. We use A ? {0, 1} |? |? |? | to represent ?'s adjacency matrix, with each element A(?, ?) = 1 indicating that there exists an edge between ? and ?, otherwise A(?, ?) = 0. D is the diagonal degree matrix where D(?, ?) = ? A(?, ?). ? is used to denote the graph ? with added self-loop connections. The corresponding adjacency matrix is A = A + I and the degree matrix is D = D + I.</p><p>In this work, we focus on the classification problem, in which each sample ? is associated with 1) a feature vector X ? ? X ? R |? |?? ? and 2) a label vector Y ? ? Y ? {0, 1} |? |?? with ? representing the number of classes. In the semi-supervised setting, only limited nodes ? ? ? have observed labels (0 &lt; |?| ? |? |), and the labels of remaining nodes ? = ? -? are unseen. The objective of semi-supervised graph learning is to infer the missing labels Y ? for unlabeled nodes ? based on graph structure ?, node features X, and the observed labels Y ?<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>Graph neural networks (GNNs) have been widely adopted for addressing the semi-supervised graph learning problem. In this part, we review the progress of GNNs with an emphasis on their largescale solutions to semi-supervised graph learning.</p><p>Graph Convolutional Network. The graph convolutional network (GCN) <ref type="bibr" target="#b18">[19]</ref> generalizes the convolution operation into graphs. Specifically, the ?-th GCN layer is defined as:</p><formula xml:id="formula_0">H (? +1) = ? ( ?H (? ) W (? ) ),<label>(1)</label></formula><p>where H (?) denotes the hidden node representations of the ?-th layer with</p><formula xml:id="formula_1">H (0) = X, ? = D -1 2 A D -1 2</formula><p>is the symmetric normalized adjacency matrix of ?, W (?) denotes the weight matrix of the ?th layer, and ? (?) denotes the activation function. In practice, this graph convolution procedure would be repeated multiple times, and the final representations are usually fed into a logistic regression layer for classification. Simplified Graph Convolution. By taking a closer look at Equation 1, we can observe that graph convolution consists of two operations: feature propagation ?H (?) and non-linear transformation ? (?). Wu et al. <ref type="bibr" target="#b28">[29]</ref> simplify this procedure by removing the non-linear transformations in hidden layers. The resulting simplified graph convolution (SGC) is formulated as:</p><formula xml:id="formula_2">? = softmax( ?? XW),<label>(2)</label></formula><p>where ?? X is considered as a simplified ? -layer graph convolutions on X, W refers to the learnable weight matrix for classification, and ? denotes the model's predictions.</p><p>GNNs with Mixed-Order Propagation. As pointed by Li et al. <ref type="bibr" target="#b22">[23]</ref>, ?? X will converge to a fix point as ? increases according to the Markov chain convergence theorem, namely, the oversmoothing issue. To address it, a typical kind of methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> suggest to use a more complex mixed-order matrix for feature propagation. For example, APPNP <ref type="bibr" target="#b19">[20]</ref> adopts the truncated personalized PageRank (ppr) matrix ? ppr sym = ? ?=0 ? (1 -?) ? ?? , where the hyperparameter ? ? (0, 1] denotes the teleport probability, allowing the model to preserve the local information even when ? ? +?. Scalable GNNs. Broadly, there are three categories of methods proposed for making GNNs scalable: 1) The node sampling methods employ sampling strategies to speed up the recursive feature aggregation procedure. The representative methods include Graph-SAGE <ref type="bibr" target="#b13">[14]</ref>, FastGCN <ref type="bibr" target="#b6">[7]</ref>, and LADIES <ref type="bibr" target="#b33">[34]</ref>; 2) The graph partition methods attempt to divide the original large graph into several small sub-graphs and run GNNs on sub-graphs. This category consists of Cluster-GCN <ref type="bibr" target="#b9">[10]</ref> and GraphSAINT <ref type="bibr" target="#b30">[31]</ref>; 3) The matrix approximation methods follow the design of SGC <ref type="bibr" target="#b28">[29]</ref> to decouple feature propagation and non-linear transformation, and to utilize some approximation methods to accelerate feature propagation. The proposed GRAND+ framework is highly related to matrix approximation based methods such as PPRGo <ref type="bibr" target="#b4">[5]</ref> and GBP <ref type="bibr" target="#b7">[8]</ref>. We will analyze their differences in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE GRAND+ FRAMEWORK</head><p>In this section, we briefly review the graph random neural network (GRAND) and present its scalable solution GRAND+ for large-scale semi-supervised graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Graph Random Neural Network</head><p>Recently, Feng et al. <ref type="bibr" target="#b11">[12]</ref> introduce the graph neural neural network (GRAND) for semi-supervised node classification. GRAND is a GNN </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? "</head><formula xml:id="formula_3">! (#) ? " % (#) ? " &amp; (#) ? " ' (#) ? " ! ? " % ? " &amp; ? " ' ! " # $ % &amp; '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-k sparsification</head><p>Label 0 Label 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unlabeled</head><p>Graph Data (a) Sub-Matrix Approximation consistency regularization framework that optimizes the prediction consistency of unlabeled nodes in different augmentations. Specifically, it designs random propagation-a mixed-order propagation strategy-to achieve graph data augmentations. First, the node features X are randomly dropped with DropNode-a variant of dropout. Then the resultant corrupted feature matrix is propagated over the graph with a mixed-order matrix. Instead of the PPR matrix, GRAND uses an average pooling matrix ? avg sym = ? ?=0 ?? /(? + 1) for propagation. Formally, the random propagation strategy is formulated as:</p><formula xml:id="formula_4">X = ? avg sym ? diag(z) ? X, z ? ? Bernoulli(1 -?),<label>(3)</label></formula><p>where z ? {0, 1} |? | denotes the random DropNode masks drawn from Bernoulli(1 -?), and ? represents DropNode probability. In doing so, the dropped information of each node is compensated by its neighborhoods. Under the homophily assumption of graph data, the resulting matrix X can be seen as an effective data augmentation of the original feature matrix X. Owing to the randomness of DropNode, this method can in theory generate exponentially many augmentations for each node.</p><p>In each training step of GRAND, the random propagation procedure is performed for ? times, leading to ? augmented feature matrices {X (?) |1 ? ? ? ? }. Then all the augmented feature matrices are fed into an MLP to get ? predictions. During optimization, GRAND is trained with both the standard classification loss on labeled data and an additional consistency regularization loss <ref type="bibr" target="#b3">[4]</ref> on the unlabeled node set ? , that is,</p><formula xml:id="formula_5">1 ? ? |? | ?? ??? ? ?? ?=1 ?(?) ? -Y ? 2 2 , Y ? = ? ?? ?=1 1 ? ?(?) ? ,<label>(4)</label></formula><p>where ?(?)</p><p>? is MLP's prediction probability for node ? when using X (?) ? as input. The consistency loss provides an additional regularization effect by enforcing the neural network to give similar predictions for different augmentations of unlabeled data. With random propagation and consistency regularization, GRAND achieves better generalization capability over conventional GNNs <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability of GRAND.</head><p>In practice, the ?-th power of the adjacency matrix ?? is computationally infeasible when ? is large <ref type="bibr" target="#b24">[25]</ref>. To avoid this issue, GRAND adopts the power iteration to directly calculate the entire augmented feature matrix X (in Equation <ref type="formula" target="#formula_4">3</ref>), i.e., iteratively calculating and summing up the product of ? and ?? ? diag(z) ? X for 0 ? ? &lt; ? . This procedure is implemented with the sparse-dense matrix multiplication and has a linear time complexity of O (|? | + |?|). However, it needs to be performed for ? times at every training step to generate different feature augmentations. Thus the total complexity of ? training steps becomes O (? ? ? ? (|? | + |?|)), which is prohibitively expensive when dealing with large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of GRAND+</head><p>We present GRAND+ to achieve both scalability and accuracy for graph based semi-supervised learning. It follows the general consistency regularization principle of GRAND and comprises techniques to make it scalable to large graphs while maintaining GRAND's flexibility and generalization capability.</p><p>Briefly, instead of propagating features with power iteration, we develop an efficient approximation algorithm-generalized forward push (GFPush)-in GRAND+ to pre-compute the required row vectors of propagation matrix and perform random propagation in a mini-batch manner. The time complexity of this procedure is controlled by a predefined hyperparameter, avoiding the scalability limitation faced by GRAND. Furthermore, GRAND+ adopts a new confidence-aware loss for consistency regularization, which makes the training process more stable and leads to better generalization performance than GRAND. Propagation Matrix. In GRAND+, we propose the following generalized mixed-order matrix for feature propagation:</p><formula xml:id="formula_6">? = ? ?? ?=0 ? ? ? P ? , P = D -1 A,<label>(5)</label></formula><p>where ? ?=0 ? ? = 1 and ? ? ? 0, P is the row-normalized adjacency matrix. Different from the propagation matrices used in GRAND and other GNNs, the form of ? adopts a set of tunable weights {? ? |0 ? ? ? ? } to fuse different orders of adjacency matrices. By adjusting ? ? , GRAND+ can flexibly manipulate the importance of different orders of neighborhoods to suit the diverse graphs of distinct structural properties in the real world. Training Pipeline. To achieve fast training, GRAND+ abandons the power iteration method which directly calculates the entire augmented feature matrix X, and instead computes each augmented feature vector separately for each node. Ideally, the augmented feature vector X ? of node ? is calculated by:</p><formula xml:id="formula_7">X ? = ?? ??N ? ? z ? ? ?(?, ?) ? X ? , z ? ? Bernoulli(1 -?).<label>(6)</label></formula><p>Here we use ? ? to denote the row vector of ? corresponding to node ?, N ? ? is used to represent the indices of non-zero elements of ? ? , ?(?, ?) denotes the ?-th element of ? ? . This paradigm allows us to generate augmented features for only a batch of nodes in each training step, and thus enables us to use efficient mini-batch gradient descent for optimization.</p><p>However, it is difficult to calculate the exact form of ? ? in practice. To address this problem, we develop several efficient methods to approximate ? ? in GRAND+. The approximation procedure consists of two stages. In the first stage, we propose an efficient method Generalized Forward Push (GFPush) to compute an error-bounded approximation ? ? for the row vector ? ? . In the second stage, we adopt a top-? sparsification strategy to truncate ? ? to only contain the top ? largest elements. The obtained sparsified row approximation ? (?) ? is used to calculate X ? as a substitute of ? ? (Eq. 6). For efficiency, it is required to pre-compute the corresponding row approximations for all nodes used in training. In addition to labeled nodes, GRAND+ also requires unlabeled nodes to perform consistency regularization during training. To further improve efficiency, instead of using the full set of ? , GRAND+ samples a smaller subset of unlabeled nodes ? ? ? ? for consistency regularization. As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, the training pipeline of GRAND+ consists of three steps: ? Sub-matrix approximation. We obtain a sparsified row approximation ? (?) ? for each node ? ? ? ? ? ? through GFPush and top-? sparsification. The resultant sparsified sub-matrix is used to support random propagation.</p><p>? Mini-batch random propagation. At each training step, we sample a batch of nodes from ? ? ? ? and generate multiple augmentations for each node in the batch with the approximated row vector. ? Confidence-aware consistency training. We feed the augmented features into an MLP to get corresponding predictions and optimize the model with both supervised loss and confidence-aware consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sub-Matrix Approximation</head><p>Generalized Forward Push (GFPush). It can be observed that the row-normalized adjacency matrix P = D -1 A is also the reverse random walk transition probability matrix <ref type="bibr" target="#b7">[8]</ref> on ?, where row vector P ? denotes random walk transition probabilities starting from node ?. Based on this fact, we propose an efficient algorithm called Generalized Forward Push (GFPush) to approximate row vector ? ? = ? ?=0 ? ? P ? ? with a bounded error. GFPush is inspired by the Forward Push <ref type="bibr" target="#b1">[2]</ref> algorithm for approximating personalized PageRank vector, while has much higher flexibility with the ability to approximate the generalized mixed-order matrix ?. The core idea of GFPush is to simulate an ? -step random walk probability diffusion process from ? with a series of pruning operations for acceleration. To achieve that, we should maintain a pair of vectors at each step ? (0 ? ? ? ? ): 1) Reserve vector q (?) ? R |? | , denoting the probability masses reserved at step ?; 2) Residue vector r (?) ? R |? | , representing the probability masses to be diffused beyond step ?.</p><p>Algorithm 1 shows the pseudo-code of GFPush. At beginning, r (0) and q (0) are both initialized as the indicator vector e (?) where e (?) ? = 1 and e (?) ? = 0 for ? ? ?, meaning the random walk starts from ? with the probability mass of 1. Other reserve and residue vectors (i.e., r (?) and q (?) , 1 ? ? ? ? ) are set to ? 0. Then the algorithm iteratively updates reserve and residue vectors with ? steps. In the ?-th iteration, the algorithm conducts a push operation (Line 5-9 of algorithm 1) for node ? which satisfies r</p><formula xml:id="formula_8">(?-1) ? &gt; d ? ? ? ??? .</formula><p>Here d ? = D(?, ?) represents the degree of ?, ? ??? is a predefined threshold. In the push operation, the residue r (?-1) ? of ? is evenly distributed to its neighbors, and the results are stored into the ?-th residue vector r (?) . Meanwhile, the reserve vector q (?) is also updated to be identical with r (?) . After finishing the push operation on ?, we reset r (?-1) ? to 0 to avoid duplicate updates. To gain more intuition of this procedure, we could observe that r (?-1) ? /d ? is the conditional probability that a random walk moves from ? to a neighboring node ?, conditioned on it reaching ? with probability r (?-1) ? at the previous step. Thus each push operation on ? can be seen as a one-step random walk probability diffusion process from ? to its neighborhoods. To ensure efficiency, GFPush only conducts push operations for node ? whose residue value is greater than d ? ?? ??? . Thus when the ?-th iteration is finished, q (?) can be seen as an approximation of the ?-step random walk transition vector P ? ? . And ? ? = ? ?=0 ? ? q (?) is accordingly considered as the approximation of ? ? as returned by the algorithm. Theoretical Analysis. We have the following theorem about the bounds of time complexity, memory complexity, and approximation error of GFPush. Theorem 1. Algorithm 1 has O (? /? ??? ) time complexity and O (? /? ??? ) memory complexity, and returns ? ? as an approximation of ? ? with the ? 1 error bound:</p><formula xml:id="formula_9">? ? ? -? ? ? 1 ? ? ? (2|?| + |? |) ? ? ??? . Proof. See Appendix A.2.</formula><p>? Theorem 1 suggests that the approximation precision and running cost of GFPush are negatively correlated with ? ??? . In practice, we could use ? ??? to control the trade-off between efficiency and approximation precision. Top-? Sparsification. To further reduce training cost, we perform top-? sparsification for ? ? . In this procedure, only the top-? largest elements of ? ? are preserved and other entries are set to 0. Hence the resultant sparsified transition vector ? (?) ? has at most ? nonzero elements. In this way, the model only considers the ? most Algorithm 1: GFPush Input : Self-loop augmented graph ?, propagation step ? , node ?, threshold ? ??? , weight coefficients ? ? , 0 ? ? ? ? . Output : An approximation ? ? of transition vector ? ? of node ?.</p><p>1 r (?) ? ? 0 for ? = 1, ..., ? ; r (0) ? e (? ) (e</p><formula xml:id="formula_10">(? ) ? = 1, e (? )</formula><p>? = 0 for ? ? ?). 2 q (?) ? ? 0 for ? = 1, ..., ? ; q (0) ? e (? ) . important neighborhoods for each node in random propagation, which is still expected to be effective based on the clustering assumption <ref type="bibr" target="#b5">[6]</ref>. Similar technique was also adopted by PPRGo <ref type="bibr" target="#b4">[5]</ref>. We will empirically examine the effects of ? in Section 4.5. Parallelization. In GRAND+, we need to approximate row vectors for all nodes in ? ? ? ? . It could be easily checked that different row approximations are calculated independently with each other in GFPush. Thus we can launch multiple workers to approximate multiple vectors simultaneously. This procedure is implemented with multi-thread programming in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mini-Batch Random Propagation</head><p>GRAND+ adopts the sparsified row approximations of ? to perform random propagation in a mini-batch manner. Specifically, at the ?-th training step, we randomly sample a batch of labeled nodes ? ? from ?, and a batch of unlabeled nodes ? ? from ? ? . Then we calculate augmented feature vector X ? for node ? ? ? ? ? ? ? by:</p><formula xml:id="formula_11">X ? = ?? ??N (? ) ? z ? ? ? (? ) (?, ?) ? X ? , z ? ? Bernoulli(1 -?),<label>(7)</label></formula><p>where Random Propagation for Learnable Representations. In Equation 7, the augmented feature vector X ? is calculated with raw features X. However, in some real applications (e.g., image or text classification), the dimension of X might be extremely large, which will incur a huge cost for calculation. To mitigate this issue, we can employ a linear layer to transform each X ? to a low-dimensional hidden representation H ? ? R ? ? firstly, and then perform random propagation with H:</p><formula xml:id="formula_12">N (?) ? denotes the non-zero indices of ? (?) ? , X ? ? R ? ?</formula><formula xml:id="formula_13">X ? = ?? ??N (? ) ? z ? ? ? (? ) (?, ?) ? H ? , H ? = X ? ? W (0) ,<label>(8)</label></formula><p>where W (0) ? R ? ? ?? ? denotes learnable transformation matrix. In this way, the computational complexity of this procedure is reduced to O (? ? ? ? ? ? ), where ? ? ? ? ? denotes the dimension of H ? .</p><p>Prediction. During training, the augmented feature vector X (?) ? is fed into an MLP model to get the corresponding outputs:</p><formula xml:id="formula_14">?(?) ? = MLP(X (?) ? , ?),<label>(9)</label></formula><p>where ?(?)</p><p>? ? [0, 1] ? denotes the prediction probabilities of ?. ? represents MLP's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Confidence-Aware Consistency Training</head><p>GRAND+ adopts both supervised classification loss and consistency regularization loss to optimize model parameters during training. The supervised loss is defined as the average cross-entropy over multiple augmentations of labeled nodes:</p><formula xml:id="formula_15">L ??? = - 1 |? ? | ? ? ?? ???? ? ?? ?=1 Y ? ? log( ?(?) ? ).<label>(10)</label></formula><p>Confidence-Aware Consistency Loss. Inspired by recent advances in semi-supervised learning <ref type="bibr" target="#b3">[4]</ref>, GRAND adopts an additional consistency loss to optimize the prediction consistency of multiple augmentations of unlabeled data, which is shown to be effective in improving generalization capability. GRAND+ also follows this idea, while adopts a new confidence-aware consistency loss to further improve effectiveness. Specifically, for node ? ? ? ? , we first calculate the distribution center by taking the average of its ? prediction probabilities, i.e., Y ? = ? ?=1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?(?)</head><p>? /?. Then we apply sharpening <ref type="bibr" target="#b16">[17]</ref> trick over Y ? to "guess" a pseudo label Y ? for node ?. Formally, the guessed probability on the ?-th class of node ? is obtained via:</p><formula xml:id="formula_16">Y(?, ?) = Y(?, ?) 1 ? / ?-1 ?? ?=0 Y(?, ?) 1 ? ,<label>(11)</label></formula><p>where 0 &lt; ? ? 1 is a hyperparameter to control the sharpness of the guessed pseudo label. As ? decreases, Y ? is enforced to become sharper and converges to a one-hot distribution eventually. Then the confidence-aware consistency loss on unlabeled node batch ? ? is defined as:</p><formula xml:id="formula_17">L ??? = 1 |? ? | ? ? ?? ???? I(max(Y ? ) ? ? ) ? ?? ?=1 D ( Y ? , ?(?) ? ),<label>(12)</label></formula><p>where I(max(Y ? ) ? ?) is an indicator function which outputs 1 if max(Y ? ) ? ? holds, and outputs 0 otherwise. 0 ? ? &lt; 1 is a predefined threshold. D (?, ?) is a distance function which measures the distribution discrepancy between ? and ?. Here we mainly consider two options for D: ? 2 distance and KL divergence.</p><p>Compared with the consistency loss used in GRAND (Cf. Equation 4), the biggest advantage of L ??? is that it only considers "highly confident" unlabeled nodes determined by threshold ? in optimization. This mechanism could reduce the potential training noise by filtering out uncertain pseudo-labels, further improving model's performance in practice. Combining L ??? and L ??? , the final loss for model optimization is defined as: </p><formula xml:id="formula_18">L = L ??? + ? (? ) L ??? ,<label>(13)</label></formula><formula xml:id="formula_19">?(??? ) = MLP(? ? (1 -?) ? X, ?). 18 return ?(??? ) .</formula><p>where ?(?) is a linear warmup function <ref type="bibr" target="#b12">[13]</ref> which increases linearly from 0 to the maximum value ? ??? as training step ? increases. Model Inference. After training, we need to infer the predictions for unlabeled nodes. GRAND+ adopts power iteration to calculate the exact prediction results for unlabeled nodes during inference:</p><formula xml:id="formula_20">?(??? ) = MLP( ? ?? ?=0 ? ? ( D -1 A) ? ? (1 -?) ? X, ?),<label>(14)</label></formula><p>where we rescale X with (1 -?) to make it identical with the expectation of the DropNode perturbed features used in training. Note that unlike GRAND, the above power iteration process only needs to be performed once in GRAND+, and the computational cost is acceptable in practice. Compared with obtaining predictions with GFPush as done in training, this inference strategy could provide more accurate predictions in theory. Algorithm 2 shows the entire training and inference procedure of GRAND+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model Analysis</head><p>Complexity Analysis. We provide detailed analyses for the time complexities of GRAND+'s different learning stages. GRAND+ vs. PPRGo and GBP. Similar with GRAND+, PPRGo <ref type="bibr" target="#b4">[5]</ref> and GBP <ref type="bibr" target="#b7">[8]</ref> also adopt matrix approximation methods to scale GNNs. However, GRAND+ differs from the two methods in several key aspects. PPRGo scales up APPNP by using Forward Push <ref type="bibr" target="#b1">[2]</ref> to approximate the ppr matrix. Compared with PPRGo, GRAND+ is more flexible in real applications thanks to the adopted generalized propagation matrix ? and GFPush algorithm. GBP also owns this merit by using the generalized PageRank matrix <ref type="bibr" target="#b21">[22]</ref> for feature propagation. However, it directly approximates the propagation results of raw features through bidirectional propagation <ref type="bibr" target="#b2">[3]</ref>, whose computational complexity is linear with the raw feature dimension, rendering it difficult to handle datasets with high-dimensional features. Moreover, different from PPRGo and GBP designed for the general supervised classification problem, GRAND+ makes significant improvements for semi-supervised setting by adopting random propagation and consistency regularization to enhance generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setup</head><p>Baselines. In our experiments, we compare GRAND+ with five state-of-the-art full-batch GNNs-GCN <ref type="bibr" target="#b18">[19]</ref>, GAT <ref type="bibr" target="#b27">[28]</ref>, APPNP <ref type="bibr" target="#b19">[20]</ref>, GCNII <ref type="bibr" target="#b8">[9]</ref> and GRAND <ref type="bibr" target="#b11">[12]</ref>, as well as five representative scalable GNNs-FastGCN <ref type="bibr" target="#b6">[7]</ref>, GraphSAINT <ref type="bibr" target="#b30">[31]</ref>, SGC <ref type="bibr" target="#b28">[29]</ref>, GBP <ref type="bibr" target="#b7">[8]</ref> and PPRGo <ref type="bibr" target="#b4">[5]</ref>. For GRAND+, we implement three variants with different settings for propagation matrix ? (Cf. Equation <ref type="formula" target="#formula_6">5</ref>):</p><p>? GRAND+ (P): Truncated ppr matrix ? ppr = ? ?=0 ? (1 -?) ? P ? . ? GRAND+ (A): Average pooling matrix ? avg = ? ?=0 P ? /(? + 1). ? GRAND+ (S): Single order matrix ? single = P ? .</p><p>Datasets. The experimnents are conducted on seven public datasets of different scales, including three widely adopted benchmark graphs-Cora, Citeseer and Pubmed <ref type="bibr" target="#b29">[30]</ref>, and four relatively large graphs-AMiner-CS <ref type="bibr" target="#b11">[12]</ref>, Reddit <ref type="bibr" target="#b13">[14]</ref>, Amazon2M <ref type="bibr" target="#b9">[10]</ref> and MAGscholar-C <ref type="bibr" target="#b4">[5]</ref>. For Cora, Citeseer and Pubmed, we use public data splits <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. For AMiner-CS, Reddit, Amazon2M and MAG-Scholar-C, we use 20?#classes nodes for training, 30?#classes nodes for validation and the remaining nodes for test. The corresponding statistics are summarized in Table <ref type="table" target="#tab_3">1</ref>. More details for the setup and reproducibility can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Benchmark Datasets</head><p>To evaluate the effectiveness of GRAND+, we compare it with 10 GNN baselines on Cora, Citeseer and Pubmed. Following the community convention, the results of baseline models on the three benchmarks are taken from the previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. For GRAND+, we conduct 100 trials with random seeds and report the average accuracy and the corresponding standard deviation over  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Large Graphs</head><p>To justify the scalability of GRAND+, we further compare it with five scalable GNN baselines on four large graphs, i.e., AMiner-CS, Reddit, Amazon2M and MAG-Scholar-C. Note that the feature dimension of MAG-Scholar-C is huge (i.e., 2.8M features per node).</p><p>To enable GRAND+ to deal with it, a learnable linear layer is added before random propagation to transform the high-dimensional node features to low-dimensional hidden vectors (Cf. Equation <ref type="formula" target="#formula_13">8</ref>). For a fair comparison, we conduct careful hyperparameter selection for all methods (Cf. Appendix A.1). We run each model for 10 trails with random splits, and report its average accuracy and average running time (including preprocessing time, training time and inference time) over the trials. The results are summarized in Table <ref type="table" target="#tab_5">3</ref>.</p><p>We interpret the results of Table <ref type="table" target="#tab_5">3</ref> from two perspectives. First, combining with the results in Table <ref type="table" target="#tab_4">2</ref>, we notice that the three variants of GRAND+ exhibit big differences across these datasets: On Cora and Citeseer, GRAND+ (P) achieves better results than GRAND+ (A) and GRAND+ (S); On Pubmed, Reddit and Scholar-C, GRAND+ (A) surpasses the other two variants; On AMiner-CS and Amazon2M, GRAND+ (S) gets the best classification results. This indicates that the propagation matrix plays a critical role in this task, and further suggests that GRAND+ could flexibly deal with different graphs by adjusting the generalized mixed-order matrix ?. Second, we observe GRAND+ consistently surpasses all baseline methods in accuracy and gets efficient running time on the four datasets. Importantly, on the largest graph MAG-Scholar-C, GRAND+ could succeed in training and making predictions in around 10 minutes, while SGC and GBP require more than 24 hours to finish, because the two methods are designed to directly propagate the high-dimensional raw features in pre-processing step. Compared with FastGCN and GraphSAINT, GRAND+ (S) achieves 8? and 12? acceleration respectively. When compared with PPRGo, the fastest model on this dataset in the past, GRAND+ (S) gets 4.9% improvement in accuracy while with a comparable running time. These results indicate GRAND+ scales well on large graphs and further emphasize its excellent performance.</p><p>We also report the accuracy and running time of GRAND on AMiner-CS. Note that it can not be executed on the other three large datasets due to the out-of-memory error. As we can see, GRAND+ achieves over 40? acceleration in terms of running time over GRAND on AMiner-CS, demonstrating the effectiveness of the proposed approximation techiniques in improving efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization Improvements</head><p>In this section, we quantitatively investigate the benefits of the proposed confidence-aware consistency loss L ??? to model's generalization capability. In GRAND+, L ??? is mainly dominated by two hyperparameters: confidence threshold ? (Cf. Equation <ref type="formula" target="#formula_17">12</ref>) and maximum consistency loss weight ? ??? (Cf. Equation <ref type="formula" target="#formula_18">13</ref>).</p><p>We first analyze the effects of ? and ? ??? on GRAND+'s classification performance. Specifically, we adjust the values of ? and ? ??? separately with other hyperparameters fixed, and observe how GRAND+'s accuracy changes on test set. Figure <ref type="figure" target="#fig_3">2</ref> illustrates the results on Pubmed dataset. From Figure <ref type="figure" target="#fig_3">2</ref> (a), it can be seen that the accuracy is significantly improved as ? ??? increases from 0 to 0.8. When ? ??? is greater than 0.8, the accuracy tends to be stable. This indicates that the consistency loss could really contribute to GRAND+'s performance. From Figure <ref type="figure" target="#fig_3">2</ref> (b), we observe model's performance benefits from the enlargement of ? when ? is less than 0.7, which highlights the significance of the confidence mechanism. If ? is set too large (i.e., &gt; 0.7), the performance will degrade because too much unlabeled samples are ignored in this case, weakening the effects of consistency regularization. Figure <ref type="figure" target="#fig_3">2</ref> demonstrates the confidence-aware consistency loss could significantly improve model's performance. We further study its benefits to generalization capability by analysing the crossentropy losses on training set and validation set. Here we measure model's generalization capability with its generalization gap <ref type="bibr" target="#b15">[16]</ref>the gap between training loss and validation loss. A smaller generalization gap means the model has a better generalization capability. Figure <ref type="figure">3</ref> reports the training and validation losses of GRAND+ (A) on Pubmed. We can observe the generalization gap is rather large when we do not use consistency loss (? ??? = 0) during training, indicating a severe over-fitting issue. And the gap becomes smaller when we change ? ??? to 1.0. When we set both ? ??? and ? to proper values (i.e., ? ??? = 1.0, ? = 0.6), the generalization gap further decreases. These observations demonstrate the proposed consistency training and confidence mechanism indeed contribute to GRAND+'s generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Analysis</head><p>Threshold ? ??? and Neighborhood Size ?. GRAND+ uses GF-Push and top-? sparsification to approximate multiple row vectors of ? to perform mini-batch random propagation (Cf. Section 3.4). The approximation error of this process is mainly influenced by two hyperparameters-threshold ? ??? of GFPush and maximum neighborhood size ? for sparsification. We conduct detailed experiments to better understand the effects of ? and ? ??? on model's accuracy and running time. Figure <ref type="figure" target="#fig_7">4</ref> illustrates the corresponding results of GRAND+ (S) w.r.t. different values of ? and ? ??? on MAG-Scholar-C. As we can see, both the accuracy and running time increase when ? ??? becomes smaller, which is coincident with the conclusion of Theorem 1. While ? has an opposite effect-the accuracy and running time are enlarged with the increase of ?. Interestingly, as ? decreases from 128 to 32, the running time is cut in half with only ? 2% performance drop in accuracy. This demonstrates the effectiveness of the top-? sparsification strategy, which could achieve significant acceleration at little cost of accuracy. Propagation Order ? . We study the influence of propagation order ? on GRAND+ when using different propagation matrices. Figure <ref type="figure" target="#fig_10">5</ref> presents the classification performance and running time of three GRAND+ variants on MAG-Scholar-C w.r.t. different values of ? . As we can see, when ? = 2, GRAND+ (S) achieves better      accuracy and faster running time than GRAND+ (P) and GRAND+ (A). However, as ? increases, the accuracy of GRAND+ (S) drops dramatically because of over-smoothing issue, while GRAND+ (P) and GRAND+ (A) do not suffer from this problem and benefit from a larger propagation order. On the other hand, increasing ? will enlarge models' running time. In real applications, we can flexibly adjust the propagation matrix and the value of ? to make desired efficiency and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose GRAND+, a scalable and high-performance GNN framework for graph-based semi-supervised learning. The advantages of GRAND+ include both the scalability and generalization capability while the existing state-of-the-art solutions typically feature only one of the two. To this effect, we follow the consistency regularization principle of GRAND in achieving the generalization performance, while significantly extend it to achieve scalability and retain and even exceed the flexibility and generalization capability of GRAND. To achieve these, GRAND+ utilizes a generalized mixed-order matrix for feature propagation, and uses our approximation method generalized forward push (GFPush) to calculate it efficiently. In addition, GRAND+ adopts a new confidence-aware consistency loss to achieve better consistency training. Extensive experiments show that GRAND+ not only gets the best performance on benchmark datasets, but also achieves performance and efficiency superiority over existing scalable GNNs on datasets with millions of nodes. In the future, we would like to explore more accurate approximation methods to accelerate GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Implementation Note</head><p>A.1.1 Running environment. The experiments are conducted on a single Linux server with Intel(R) Xeon(R) CPU Gold 6420 @ 2.60GHz, 376G RAM and 10 NVIDIA GeForce RTX 3090TI-24GB. The Python version is 3.8.5.</p><p>A.1.2 Implementation details. We implement GFPush with C++, and use OpenMP to perform parallelization. We use Pytorch to implement the training process of GRAND+, and use pybind <ref type="foot" target="#foot_1">5</ref> to create Python binding for approximation module. In GRAND+ and other baselines, we use BatchNorm <ref type="bibr" target="#b14">[15]</ref> and gradient clipping <ref type="bibr" target="#b23">[24]</ref> to stabilize the model training, and adopt Adam <ref type="bibr" target="#b17">[18]</ref> for optimization.</p><p>A.1.3 Dataset details. There are totally 7 datasets used in this paper, that is, Cora, Citeseer, Pubmed, AMiner-CS, Reddit, Amazon2M and MAG-Scholar-C. Our preprocessing scripts for Cora, Citeseer and Pubmed are implemented with reference to the codes of Planetoid <ref type="foot" target="#foot_2">6</ref> . Following the experimental setup used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, we run 100 trials with random seeds for the results on Cora, Citeseer and Pubmed reported in Section 4.2. AMiner-CS is constructed by Feng et al. <ref type="bibr" target="#b11">[12]</ref> based on the AMiner citation network <ref type="bibr" target="#b26">[27]</ref>. In AMiner-CS, each node represents a paper, the edges are citation relations, labels are research topics of papers. Reddit is published by Hamilton et al. <ref type="bibr" target="#b13">[14]</ref>, in which each node represents a post in the Reddit community, a graph link represents the two posts have been commented by the same user. The task is to predict the category of each post. Amazon2M is published by Chiang et al. <ref type="bibr" target="#b9">[10]</ref>, where each node is a product, each edge denotes the two products are purchased together, labels represent the categories of products. MAG-Scholar-C is constructed by Bojchevski <ref type="bibr" target="#b4">[5]</ref> based on Microsoft Academic Graph (MAG) <ref type="bibr" target="#b25">[26]</ref>, in which nodes refer to papers, edges represent citation relations among papers and features are bag-of-words of paper abstracts. For AMiner-CS, Reddit, Amazon2M and MAG-Scholar-C, we use 20?#classes for training, 30?#classes nodes for validation and the remaining nodes for test. For Aminer, Reddit and MAG-Scholar-C, we randomly sample the same number of nodes for each class-20 nodes per class for training and 30 nodes per class for validation. For Amazon2M, we uniformly sample all the training and validation nodes from the whole datasets, as the node counts of some classes are less than 20. For these datasets, we report the average results of 10 trails with random splits.</p><p>A.1.4 Hyperparameter Selection. For results in Table <ref type="table" target="#tab_4">2</ref>, we adjust hyperparameters of GRAND+ on validation set, and use the best configuration for prediction, and the results of other baseline methods are taken from previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. For results in Table <ref type="table" target="#tab_5">3</ref>-5, we conduct detailed hyperparameter search for both GRAND+ and other GNN baselines (i.e., FastGCN, GraphSAINT, SGC, GBP and PPRGo). For each search, we run 3 experiments with random seeds, and select the hyperparameter configuration which gets the best average accuracy on validation set. Then we train model with the selected configuration.  The hyperparameter selection for GRAND+ consists of two stages: We first conduct search for basic hyperparameters of neural network. Specifically, we search learning rate ?? from {10 -2 , 10 -3 , 10 -4 }, weight decay rate ?? from {0, 10 -5 , 10 -3 , 10 -2 }, number of hidden layer ? ? from {1,2} and dimension of hidden layer ? ? from {32, 64, 128, 256, 512, 1024}.</p><p>In the second stage, we fix these basic hyperparameters as best configurations and search the following specific hyperparameters: DropNode rate ?, augmentation times per batch ?, threshold ? ??? in GFPush, maximum neighborhood size ?, propagation order ? , confidence threshold ?, maximum consistency loss weight ? ??? , size of unlabeled subset |? ? | and consistency loss function D. To reduce searching cost, we keep some hyperparameters fixed. Specifically, we fix ? = 0.5, ? = 2 and ? = 2 #classes across all datasets. We set |? ? | = |? | for Cora, Pubmed and Citeseer, and set |? ? | = 10000 for other datasets. We also provide an analysis for the effect of |? ? | in Appendix A.3. We adopt ?? divergence as the consistency loss function for AMiner-CS, Reddit and Amazon2M, and use ? 2 distance for other datasets. This is because ? 2 distance is easily to suffer from gradient vanishing problem when dealing with datasets with a large number of classes. We then conduct hyperparameter selection for ? ??? , ?, ? and ? ??? . Specifically, we search ? ??? from {10 -5 , 10 -6 , 10 -7 }, ? from {16, 32, 64, 128}, ? from {2, 4, 6, 8, 10, 20} and ? ??? from {0.5, 0.8, 1.0, 1.2, 1.5}. The selected best hyperparameter configurations of GRAND+ are reported in Table <ref type="table" target="#tab_6">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Theorem Proofs</head><p>To prove Theorem 1, we first introduce the following lemma: Lemma 1. For any reserve vector q (?) , residue vector r (?) and random walk transition vector P ? ? = ( D -1 A) ? ? (0 ? ? ? ? ), we have:</p><formula xml:id="formula_21">P ? ? = q (?) + ? ?? ?=1 (P ? ) T ? r (?-? )<label>(15)</label></formula><p>Proof. We prove the Lemma by induction. For brevity, we use R H S (?) to denote the right hand side of Equation <ref type="formula" target="#formula_21">15</ref>. In Algorithm 1, q (?) and r (?) are initialized as ? 0 for 1 ? ? ? ? , r (0) and q (0) are initialized as e (? ) . Thus, Equation 15 holds at the algorithm beginning based on the following facts: R H S (0) = e (? ) = P 0 ? , R H S (?) = (P ? ) T ? e (? ) = P ? ? , 1 ? ? ? ? . Then we assume Equation 15 holds at beginning of the ? ? -th iteration, we will show that the equation is still correct after a push operation on node ?. We have three cases with different values of ?:</p><p>1) When ? &lt; ? ? , the push operation does not change q (?) and r (?-? ) , 1 ? ? ? ?. Thus Equation 15 holds for ? &lt; ? ? .</p><p>2) When ? = ? ? , the push operation decrements r (?-1) by r</p><formula xml:id="formula_22">(?-1) ? ? e (?)</formula><p>and increments q (?) by ??N? r ? P ? = P ? ? + ? 0 = P ? ? .</p><p>Thus Equation <ref type="formula" target="#formula_21">15</ref>holds for ? = ? ? + 1.</p><p>3) When ? &gt; ? ? , the push operation will decrease r (? ? ) by r (? ? ) ?</p><p>? e (?)   and increase r (? ? +1) by ??N? r Proof. Let V ? be the set of nodes to be pushed in step ?. When the push operation is performed on ? ? V ? , the value of ? r (?-1) ? 1 will be decreased by at least ? ??? ? d ? . Since ? r (?-1) ? 1 ? 1, we must have</p><formula xml:id="formula_23">??V? d ? ? ? ??? ? 1, thus: ?? ??V? d ? ? 1/? ??? . (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>Time Complexity. For the push operation on ? in step ?, we need to perform d ? times of updates for r (?) . So the total time of push operations in step ? is bounded by ??V? d ? . Therefore, based on Equation <ref type="formula" target="#formula_23">16</ref>, the time complexity of each step is bounded by O (1/? ??? ) and the total time complexity of Algorithm 1 has a O (? /? ??? ) bound.</p><p>Memory Complexity. When the ?-th step iteration finishes, the count of non-zero elements of r (?) is no more than ??V? d ? , as the push operation on ? only performs d ? times of updates for r (?) . Thus the count of non-zero elements of q (?) is also less than ??V? d ? . According to Equation <ref type="formula" target="#formula_23">16</ref>, we can conclude that ? ? has at most ? /? ??? non-zero elements. In implementation, all the vectors are stored as sparse structures. Thus Algorithm 1 has a memory complexity of O (? /? ??? ).</p><p>Error Bound. According to Lemma 1, we can conclude the following equations: </p><formula xml:id="formula_25">P (?) ? -q (?)</formula><p>According to Equation <ref type="formula">17</ref>, we can conclude that P (?) ? q (?)  Analysis for the size of ? ? . In GRAND+, a subset of unlabeled nodes ? ? are sampled from ? for consistency regularization. To this end, we need to pre-compute the sparsified approximation ? ? of row vector ? ? for each node ? ? ? ? . Here we empirically analyze how the size of ? ? affects the classification accuracy (Acc), running time (RT) and approximation time (AT) of GRAND+. Table <ref type="table" target="#tab_8">5</ref> presents the results of GRAND+ (S) when we vary |? ? | from 0 to 10 5 on AMiner-CS, Reddit and Amazon2M. We have the two observations: First, as |? ? | changes from 0 (meaning the consistency loss degenerates to 0) to 10 3 , the classification performances are improved significantly with little changes on running time, which indicates the consistency regularization serves as an economic way for improving GRAND+'s generalization performance under semisupervised setting. Second, when |? ? | exceeds 10 4 , the increase rate of the accuracy will slow down, while the running time and approximation time increase more faster. This observation indicates the sampling procedure on unlabeled nodes is important for ensuring model's efficiency, which also enables us to explicitly control the trade-off between effectiveness and efficiency of GRAND+ through the sampling size |? ? |.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of GRAND+. (a) GRAND+ adopts Generalized Forward Push (GFPush) and Top-k sparsification to approximate the corresponding rows of propagation matrix ? for nodes in ? ? ? ? . (b) The obtained sparsified row approximations are then used to perform mini-batch random propagation to generate augmentations for nodes in the batch. (c) Finally, the calculated feature augmentations are fed into an MLP to conduct confidence-aware consistency training, which employs both supervised loss L ??? and confidence-aware consistency loss L ??? for model optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is feature vector of node ?. At each training step, we generate ? augmented feature vectors {X (?) ? |1 ? ? ? ? } by repeating this procedure for ? times. Let ? = |? ? | + |? ? | denote the batch size. Then the time complexity of each batch is bounded by O (? ? ? ? ? ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effects of ? ??? and ? on Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 Figure 3 :</head><label>63</label><figDesc>Figure 3: Training and Validation Losses on Pubmed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: GRAND+ w.r.t. ? and ? ??? on MAG-Scholar-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of propagation order ? on MAG-Scholar-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>8 MAG</head><label>8</label><figDesc>lrwr ? ? ? ? ? ??? ? ? ? ??? Cora GRAND+ (P) 10 -2 10 -3 2 64 10 -7 32 20 1.5 GRAND+ (A) 10 -2 10 -3 2 64 10 -7 32 4 1.5 GRAND+ (S) 10 -2 10 -3 2 64 10 -7 32 2 ) 10 -3 10 -5 2 1024 10 -6 64 6 0.8 GRAND+ (A) 10 -3 10 -5 2 1024 10 -6 64 4 0.8 GRAND+ (S) 10 -3 10 -5 2 1024 10 -6 32 2 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(Theorem 1 .</head><label>1</label><figDesc>? ? ) ? /d ? ? e(?) . Thus we have R H S(?) = P ? ? + (P ?-? ? -1 ) T ?? ??N? r (? ? ) ? /d ? ? e (?) -(P ?-? ? ) T (r (? ? ) ? ? e (?) ) = P ? ? + (P ?-? ? -1 ) T ? ?? ??N? r (? ? ) ? /d ? ? e (?) -(r (? ? ) ? ? P ? ) T = P ? ? + (P ?-? ? -1 ) T ? ? 0 = P ? ? .Thus Equation 15 holds for ? &gt; ? ? + 1.Hence the induction holds, and the lemma is proved. ?Then, we could prove Theorem 1 as following. Algorithm 1 has O (? /? ??? ) time complexity and O (? /? ??? ) memory complexity, and returns ? ? as an approximation of ? ? with the ? 1 error bound: ? ? ? -? ? ? 1 ? ? ? (2|?| + |? |) ? ? ??? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 . ( 17 )d</head><label>117</label><figDesc>) T ? r (?-? )After algorithm termination, we have 0 ? r(?) ? ? d ? ? ? ??? for all ? ? ? . ? ? ? ??? = (2|? | + |? |) ? ? ??? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 ??</head><label>1</label><figDesc>? ? (2 |? | + |? |) ? ? ??? . Further more, we have:? ? -? ? 1 ? ? ? ? (2|? | + |? |) ? ? ??? (19)As for 0 ? ? ? and ? ?=0 ? ? = 1, hence:? ?? ?=0 ? ? ? ? ? (2 |? | + |? |) ? ? ??? ? ? ? (2|? | + |? |) ? ? ??? ,(20)which indicates ? ? ? -? ? ? 1 ? ? ? (2|? | + |? |) ? ? ??? . ? A.3 Additional Experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref> for ? = 1 : ? do 4 while there exists node ? with r ? is the neighborhood set of ? in graph ?. */ ? ? ? ?=0 ? ? ? q (?) .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(?-1) ?</cell><cell>&gt; d ? ? ? ??? do</cell></row><row><cell>5</cell><cell cols="3">for each ? ? N ? do</cell></row><row><cell cols="4">/* N 6 r (?) ? ? r ? + r (?)</cell><cell>(?-1) ?</cell><cell>/d ? .</cell></row><row><cell>7</cell><cell></cell><cell cols="2">q ? ? r (?) ? . (?)</cell></row><row><cell>8</cell><cell cols="2">end</cell><cell></cell></row><row><cell>9</cell><cell>r</cell><cell>(?-1) ?</cell><cell>? 0.</cell></row><row><cell></cell><cell cols="4">/* Perform a push operation on ?.</cell><cell>*/</cell></row><row><cell>10</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell>11 end</cell><cell></cell><cell></cell><cell></cell></row><row><cell>12 ?</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><ref type="bibr" target="#b12">13</ref> </p>return ? ? .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Graph ?, feature matrix X ? R |? |?? ? , labeled node set ?, unlabeled node set ? and observed labels Y ? ? R |?|?? . Output : Classification probabilities ?(??? ) . 1 Sample a subset of unlabeled nodes ? ? from ? . 2 for ? ? ? ? ? ? do 3 ? ? ? GFPush(?, ?). Sample a batch of labeled nodes ? ? ? ? and a batch of unlabeled nodes ? ? ? ? ? . ? ? ? ? ? do</figDesc><table><row><cell cols="3">Algorithm 2: GRAND+</cell><cell></cell></row><row><cell>4</cell><cell>Obtain ? ? (? )</cell><cell cols="3">by applying top-? sparsification on ? ? .</cell></row><row><cell></cell><cell cols="4">/* Approximating row vectors with pallalization.</cell><cell>*/</cell></row><row><cell>5 end</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">6 for ? = 0 : ? do</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell cols="2">for ? = 1 : ? do</cell><cell></cell></row><row><cell>10</cell><cell cols="3">(?) ? Generate augmented feature vector X</cell><cell>with Equation 7.</cell></row><row><cell>11</cell><cell cols="2">Predict class distribution with ?(?) ?</cell><cell cols="2">= MLP(X ? (?)</cell><cell>, ?).</cell></row><row><cell>12</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>end</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">/* Stop training with early-stopping.</cell><cell></cell><cell>*/</cell></row><row><cell>16 end</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">17 Infer classification probabilities</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p>Input :</p>8 for ? ? 14</p>Compute L ??? via Equation 10 and L ??? via Equation</p>12</p>.</p>15</p>Update the parameters ? by mini-batch gradients descending:</p>? = ? -? ? ? ( L ??? + ? L ??? ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="2">Edges Classes</cell><cell>Features</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell></row><row><cell>AMiner-CS</cell><cell>593,486</cell><cell>6,217,004</cell><cell>18</cell><cell>100</cell></row><row><cell>Reddit</cell><cell>232,965</cell><cell>11,606,919</cell><cell>41</cell><cell>602</cell></row><row><cell>Amazon2M</cell><cell>2,449,029</cell><cell>61,859,140</cell><cell>47</cell><cell>100</cell></row><row><cell cols="3">MAG-Scholar-C 10,541,560 265,219,994</cell><cell cols="2">8 2,784,240</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Classification Accuracy (%) on Benchmarks.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell></cell><cell>GCN</cell><cell>81.5 ? 0.6</cell><cell>71.3 ? 0.4</cell><cell>79.1 ? 0.4</cell></row><row><cell></cell><cell>GAT</cell><cell>83.0 ? 0.7</cell><cell>72.5 ? 0.7</cell><cell>79.0 ? 0.3</cell></row><row><cell>Full-batch</cell><cell>APPNP</cell><cell>84.1 ? 0.3</cell><cell>71.6 ? 0.5</cell><cell>79.7 ? 0.3</cell></row><row><cell>GNNs</cell><cell>GCNII</cell><cell>85.5 ? 0.5</cell><cell>73.4 ? 0.6</cell><cell>80.3 ? 0.4</cell></row><row><cell></cell><cell>GRAND</cell><cell>85.4 ? 0.4</cell><cell>75.4 ? 0.4</cell><cell>82.7 ? 0.6</cell></row><row><cell></cell><cell>FastGCN</cell><cell>81.4 ? 0.5</cell><cell>68.8 ? 0.9</cell><cell>77.6 ? 0.5</cell></row><row><cell></cell><cell cols="2">GraphSAINT 81.3 ? 0.4</cell><cell>70.5 ? 0.4</cell><cell>78.2 ? 0.8</cell></row><row><cell>Scalable</cell><cell>SGC</cell><cell>81.0 ? 0.1</cell><cell>71.8 ? 0.1</cell><cell>79.0 ? 0.1</cell></row><row><cell>GNNs</cell><cell>GBP</cell><cell>83.9 ? 0.7</cell><cell>72.9 ? 0.5</cell><cell>80.6 ? 0.4</cell></row><row><cell></cell><cell>PPRGo</cell><cell>82.4 ? 0.2</cell><cell>71.3 ? 0.3</cell><cell>80.0 ? 0.4</cell></row><row><cell>Our</cell><cell cols="4">GRAND+ (P) 85.8 ? 0.4 75.6 ? 0.4 84.5 ? 1.1</cell></row><row><cell>Methods</cell><cell cols="2">GRAND+ (A) 85.5 ? 0.4</cell><cell cols="2">75.5 ? 0.4 85.0 ? 0.6</cell></row><row><cell></cell><cell>GRAND+ (S)</cell><cell>85.0 ? 0.5</cell><cell>74.4 ? 0.5</cell><cell>84.2 ? 0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%) and Running Time (s) on Large Graphs.</figDesc><table><row><cell></cell><cell cols="2">AMiner-CS</cell><cell>Reddit</cell><cell></cell><cell cols="2">Amazon2M</cell><cell cols="2">MAG.</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>RT</cell><cell>Acc</cell><cell>RT</cell><cell>Acc</cell><cell>RT</cell><cell>Acc</cell><cell>RT</cell></row><row><cell>GRAND</cell><cell cols="3">53.1?1.1 750 OOM</cell><cell>-</cell><cell>OOM</cell><cell>-</cell><cell>OOM</cell><cell>-</cell></row><row><cell>FastGCN</cell><cell cols="8">48.9?1.6 69 89.6?0.6 158 72.9?1.0 239 64.3?5.6 4220</cell></row><row><cell cols="9">GraphSAINT 51.8?1.3 39 92.1?0.5 39 75.9?1.3 189 75.0?1.7 6009</cell></row><row><cell>SGC</cell><cell cols="6">50.2?1.2 9 92.5?0.2 31 74.9?0.5 69</cell><cell>-</cell><cell>&gt;24h</cell></row><row><cell>GBP</cell><cell cols="6">52.7?1.7 21 88.7?1.1 370 70.1?0.9 280</cell><cell>-</cell><cell>&gt;24h</cell></row><row><cell>PPRGo</cell><cell cols="8">51.2?1.4 11 91.3?0.2 233 67.6?0.5 160 72.9?1.1 434</cell></row><row><cell cols="9">GRAND+ (P) 53.9?1.8 17 93.3?0.2 183 75.6?0.7 188 77.6?1.2 653</cell></row><row><cell cols="9">GRAND+ (A) 54.2?1.7 14 93.5?0.2 174 75.9?0.7 136 80.0?1.1 737</cell></row><row><cell cols="9">GRAND+ (S) 54.2?1.6 10 92.8?0.2 62 76.2?0.6 80 77.8?0.9 483</cell></row><row><cell cols="9">the trials. The results are demonstrated in Table 2. It can be ob-</cell></row><row><cell cols="9">served that the best GRAND+ variant consistently outperforms all</cell></row><row><cell cols="9">baselines across the three datasets. Notably, GRAND+ (A) improves</cell></row><row><cell cols="9">upon GRAND by a margin of 2.3% (absolute difference) on Pubmed.</cell></row><row><cell cols="9">The improvements of GRAND+ (P) over GRAND on Cora (85.8?0.4</cell></row><row><cell cols="9">vs. 85.4?0.4) and Citeseer (75.6?0.4 vs. 75.4?0.4) are also statisti-</cell></row><row><cell cols="9">cally significant (p-value ? 0.01 by a t-test). These results suggest</cell></row><row><cell cols="8">the strong generalization performance achieved by GRAND+.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameter configuration of GRAND+.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Effects of unlabeled subset size (|? ? |).</figDesc><table><row><cell></cell><cell>Aminer</cell><cell></cell><cell></cell><cell>Reddit</cell><cell></cell><cell cols="2">Amazon2M</cell><cell></cell></row><row><cell>|U'|</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Acc (%) RT (s) AT (ms) Acc (%) RT (s) AT (ms) Acc (%) RT (s) AT (ms)</cell></row><row><cell>0 51.1 ? 1.4</cell><cell>10</cell><cell>149</cell><cell>92.3 ? 0.2</cell><cell>53</cell><cell>717</cell><cell>75.0 ? 0.7</cell><cell>63</cell><cell>2356</cell></row><row><cell>10 3 53.6 ? 1.6</cell><cell>9</cell><cell>153</cell><cell>92.6 ? 1.2</cell><cell>58</cell><cell>882</cell><cell>75.2 ? 0.5</cell><cell>62</cell><cell>2630</cell></row><row><cell>10 4 54.2 ? 1.6</cell><cell>10</cell><cell>250</cell><cell>92.8 ? 0.2</cell><cell>62</cell><cell>2407</cell><cell>76.1 ? 0.6</cell><cell>69</cell><cell>3649</cell></row><row><cell>10 5 54.4 ? 1.2</cell><cell>13</cell><cell>1121</cell><cell>92.9 ? 0.2</cell><cell>78</cell><cell cols="2">17670 76.3 ? 0.7</cell><cell>86</cell><cell>14250</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For a matrix M ? R ??? , we use M ? ? R ? to denote its ?-th row vector and let M(?, ?) represent the element of the ?-th row and the ?-th column.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://github.com/pybind/pybind11</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/kimiyoung/planetoid</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The work is supported by the <rs type="funder">NSFC</rs> for <rs type="grantName">Distinguished Young Scholar</rs> (<rs type="grantNumber">61825602</rs>) and <rs type="institution">Tsinghua-Bosch Joint ML Center</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qucTtGf">
					<idno type="grant-number">61825602</idno>
					<orgName type="grant-name">Distinguished Young Scholar</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local graph partitioning using pagerank vectors</title>
		<author>
			<persName><forename type="first">Reid</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS&apos;06</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Bidirectional Probability Estimation in Markov Models</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lofgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MixMatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5050" to="5060" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9780262033589.001.0001</idno>
		<ptr target="https://doi.org/10.7551/mitpress/9780262033589.001.0001" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable Graph Neural Networks via Bidirectional Propagation</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIKM</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graph Random Neural Network for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1025" to="1035" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Shirish Nitish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tak</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Sohn</forename><surname>Kihyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berthelot</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chun-Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zizhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlini</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffel</forename><surname>Colin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence. NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05485</idno>
		<title level="m">Diffusion improves graph learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Optimizing generalized pagerank methods for seed-expansion community detection</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10881</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<idno>AAAI&apos;18</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In WSDM</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An Overview of Microsoft Academic Service (MAS) and Applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<publisher>WWW (Companion Volume</publisher>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<idno>KDD&apos;08</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas N Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
