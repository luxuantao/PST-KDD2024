<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On-Demand Deep Model Compression for Mobile Devices: A Usage-Driven Model Selection Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sicong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
							<email>yingyan.lin@rice.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
							<email>zzhou@tik.ee.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiming</forename><surname>Nan</surname></persName>
							<email>nankaiming@stu.xidian.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
							<email>liuhui@xidian.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhao</forename><forename type="middle">2018</forename><surname>Du</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Xidian University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">MobiSys&apos;18</orgName>
								<address>
									<addrLine>June 10-15</addrLine>
									<postCode>2018</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On-Demand Deep Model Compression for Mobile Devices: A Usage-Driven Model Selection Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78D8633525414EE84745AE97101D587F</idno>
					<idno type="DOI">10.1145/3210240.3210337</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>model compression</term>
					<term>deep reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research has demonstrated the potential of deploying deep neural networks (DNNs) on resource-constrained mobile platforms by trimming down the network complexity using different compression techniques. The current practice only investigate stand-alone compression schemes even though each compression technique may be well suited only for certain types of DNN layers. Also, these compression techniques are optimized merely for the inference accuracy of DNNs, without explicitly considering other applicationdriven system performance (e.g. latency and energy cost) and the varying resource availabilities across platforms (e.g. storage and processing capability). In this paper, we explore the desirable tradeoff between performance and resource constraints by user-specified needs, from a holistic system-level viewpoint. Specifically, we develop a usage-driven selection framework, referred to as AdaDeep, to automatically select a combination of compression techniques for a given DNN, that will lead to an optimal balance between user-specified performance goals and resource constraints. With an extensive evaluation on five public datasets and across twelve mobile devices, experimental results show that AdaDeep enables up to 9.8× latency reduction, 4.3× energy efficiency improvement, and 38× storage reduction in DNNs while incurring negligible accuracy loss. AdaDeep also uncovers multiple effective combinations of compression techniques unexplored in existing literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There is a growing trend to bring machine learning, especially deep neural networks (DNNs) powered intelligence to mobile devices. Many smartphones and handheld devices are integrated with intelligent user interfaces and applications such as hand-input recognition (e.g. iType <ref type="bibr" target="#b30">[30]</ref>), speech-based assistants (e.g. Siri), face recognition enabled phone-unlock (e.g. FaceID). New development frameworks targeted at mobile devices have also been launched (e.g. Tensor-Flow Lite) to encourage novel DNN-based mobile applications. In addition to smartphones, DNNs are also expected to execute locally on a wider range of mobile and IoT devices, such as wearables <ref type="bibr" target="#b34">[34]</ref> (e.g. Fitbit wristbands) and smart home infrastructure (e.g. Amazon Echo). The diverse applications and the various mobile platforms raise a challenge for DNN developers and users: How to generate the DNN that meet the application performance requirements on the target resource-constrained mobile platforms?</p><p>Generating DNNs for mobile platforms is non-trivial because many successful DNNs are computationally intensive while mobile devices are usually limited in computation, storage and power. For example, LeNet <ref type="bibr" target="#b29">[29]</ref>, a popular DNN for digit classification, has 60k weight and 341k multiply-accumulate operations (MACs) per image. AlexNet <ref type="bibr" target="#b23">[23]</ref>, one of the most famous DNNs for image classification, requires 61M weights and 724M MACs to process a single image. It can become prohibitive to download applications powered by those DNNs to local devices. These DNN-based applications also drain the battery easily if executed frequently.</p><p>In view of those challenges, DNN compression has been widely investigated to reduce the precision of weights and the number of operations during or after DNN training so as to shrink the computation and the size of the original DNNs while remaining the desired accuracy <ref type="bibr" target="#b41">[41]</ref>. Various DNN compression techniques have been proposed, including weight compression <ref type="bibr" target="#b6">[6]</ref> [16] <ref type="bibr" target="#b25">[25]</ref>, convolution decomposition <ref type="bibr" target="#b7">[7]</ref> [18] <ref type="bibr" target="#b32">[32]</ref>, and special layer architectures <ref type="bibr" target="#b20">[20]</ref>  <ref type="bibr" target="#b31">[31]</ref>. However, there are two major problems in existing DNN compression techniques.</p><p>• Most DNN compression techniques aim to provide a one-forall solution without considering the application performance requirements and the platform resource constraints. A single compression technique may not suffice to meet the diverse user demands on the generated DNNs. DNN compression techniques should be selected on-demand, i.e., adapt to the requirements and constraints on accuracy, latency, storage, and energy imposed by developers and the target platform. • Most combinations of DNN compression techniques are manually selected while the selection criteria remain a black-box to end developers. An automatic compression framework that allow user-defined criteria will benefit the development of DNN-powered mobile applications.</p><p>In this paper, we propose AdaDeep, a framework that automatically selects a combination of DNN compression techniques to adapt to user-specified performance requirements and platformimposed resource constraints on accuracy, latency, storage and energy consumption. We define DNN compression techniques as a new high-level hyper-parameter of DNNs. To model various user demands, we formulate the tuning of DNN compression as a constrained hyperparameter optimization problem. Due to the large numbers of combinations of DNN compression techniques and the varying resource constraints, it is intractable to obtain a closedform solution to the optimization problem. Inspired by the emerging trend to automate the engineering process of deep model architectures <ref type="bibr" target="#b50">[50]</ref>  <ref type="bibr" target="#b51">[51]</ref>, AdaDeep designs a reinforcement learning based optimizer to automatically and effectively solve the constrained optimization problem. We implement AdaDeep with TensorFlow <ref type="bibr" target="#b14">[14]</ref> and evaluate its performance over five different public benchmark datasets for DNNs on twelve different mobile devices. Evaluations show that AdaDeep enables a reduction of 2.1× -38× in storage, 1.1×-19.8× in latency, 1.5×-4.3× in energy consumption, and 1.1×-9.8× in computational cost, with a negligible accuracy loss (&lt; 2.1%) for various datasets, tasks, and mobile platforms.</p><p>The main contributions of this work are summarized as follows.</p><p>• To the best of our knowledge, this is the first work that integrates DNN compression into a hyperparameter tuning framework that aims to balance multiple user-defined requirements and platform resource requirements. In the rest of this paper, we present an overview of AdaDeep in Section 2, formulate the user demands in terms of different metrics in Section 3, present the automatic optimizer in Section 4, and evaluate the performance of AdaDeep in Section 5. Finally we review related work in Section 6 and conclude this work in Section 7.</p><note type="other">•</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW</head><p>This section presents an overview of AdaDeep. From a systemlevel viewpoint, AdaDeep automatically generates the most suitable compressed DNNs that meet the performance requirements and resource constraints imposed by end developers and the target mobile or embedded platforms.</p><p>AdaDeep consists of three functional blocks: DNN initialization, user demand formulation, and on-demand optimization (see Figure <ref type="figure" target="#fig_1">1</ref>). The DNN initialization block selects an initial DNN model for the on-demand optimization block from a pool of state-of-the-art DNN models (see Section 5.1). The user demand formulation block calculates performance and resource constraints imposed by users (see Section 3), which are then input into the on-demand optimization block as the optimization goals and constraints. The on-demand optimization block takes the initial DNN model and the optimization constraints to automatically search for an optimal combination of DNN compression techniques that maximizes the system performance while satisfying the resource constraints (see <ref type="bibr">Section 4)</ref>.</p><p>Mathematically, AdaDeep aims to solve the following constrained optimization problem.</p><formula xml:id="formula_0">arдmax J s ∈J al l μ 1 N (A -A min ) + μ 2 N (E max -E) s.t. T ≤ T bдt , S ≤ S bдt ,<label>(1)</label></formula><p>where A, E, T and S denote the measured accuracy, energy cost, latency and storage of a given DNN running on a specific mobile platform. User demands are expressed as a set of constraints on accuracy, energy, latency and storage. Specifically, A min and E max are the minimal accuracy and maximal energy acceptable by the user.</p><p>The two constraints are combined by coefficients μ 1 and μ 2 . N (x) is a normalization process, i.e., N (x) = (xx min )/(x maxx min ) to transform accuracy and energy to the same scale. We denote T bдt and S bдt as the latency budget and the storage budget imposed by the target mobile platform. The accuracy A, energy cost E, latency T and storage S are determined by the DNN architecture as well as the target mobile platform. These variables can be tuned by applying different combinations of DNN compression techniques. The goal of AdaDeep is to select the best combination of compression techniques J s from the set of all possible combinations J all that satisfies the performance requirements and resource constraints. We maximize the accuracy A because it is the most important performance metric for a DNN. We minimize E but only constrain S within a threshold because mobile devices are usually batterypowered and many applications require continuous inference from deep models. That is, energy effeciency is in general more important than storage for mobile applications. We do not minimize T but limit it within a threshold because the T of many DNNs is acceptable (in the order of millisecond <ref type="bibr" target="#b26">[26]</ref>).</p><p>Technically, AdaDeep faces two challenges.</p><p>• It is non-trivial to derive the accuracy A, energy cost E, latency T and storage S of a DNN. For example, there is still no universal consensus on the estimation models on energy consumption of DNNs on mobile devices. In Section 3, AdaDeep proposes a systematic way to calculate these variables and associates them to the parameters of a DNN and the given mobile platform. We apply the state-of-the-art estimation models and modify them to suite the software/hardware implementation considered in our work. Evaluations show that the models can achieve the same ranking as the measured one on the actual mobile device. • It is intractable to obtain a closed-form solution to Eq. <ref type="bibr" target="#b1">(1)</ref>.</p><p>AdaDeep employs deep reinforcement learning (DRL) based optimiation to solve the problem (see Section 4). Although reinforcement learning is a well-known optimization technique, its combination with deep learning (i.e., DRL) and its applications in automatic deep neural network architecture optimization is emerging <ref type="bibr" target="#b50">[50]</ref>. We follow this trend and apply a novel DRL structure in the context of automatic DNN model compression for mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">USER DEMAND FORMULATION</head><p>This section describes how we formulate the user demand metrics, including accuracy A, energy cost E, latency T and storage S, in terms of DNN parameters and platform resource constraints. Such a systematic formulation enables AdaDeep to predict the most suitable compressed DNNs by user needs, before being deployed into various mobile devices. Accuracy A. The inference accuracy is defined as</p><formula xml:id="formula_1">A = prob( di = d i ), i ∈ D mb (2)</formula><p>where di and d i denote the classifier decision and the true label, respectively, and D mb stands for the sample set in the corresponding mini-batch. Storage S. We calculate the storage needed to run a DNN using the total number of bits associated with weights and activations:</p><formula xml:id="formula_2">S = S f + S p = |X| B a + |W| B w (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where S f and S p denote the storage requirement for the activations and weights, X and W are the index sets of all activations and weights in the network, and B a and B w denote the precision of activations and weights, respectively. For example, B a = B w = 32 bits in TensorFlow <ref type="bibr" target="#b14">[14]</ref>.</p><p>Computational Cost C. We model the computational cost C of a DNN as the total number of multiply-accumulate (MAC) operations in the DNNs. For example, for a fixed-point convolution operation, the total number of MACs is a function of the weight and activation precision as well as the size of the involved weight and activation vectors <ref type="bibr" target="#b45">[45]</ref>.</p><p>Latency T . The inference latency of a DNN executed in mobile devices strongly depends on the system architecture and memory hierarchy of the given device. We referred to the latency model in <ref type="bibr" target="#b45">[45]</ref> which has been verified in hardware implementations. Specifically, the latency T is derived from a synchronous dataflow model, and is a function of the batch size, the storage and processing capability of the deployed device, as well as the complexity of the algorithms, i.e., DNNs.</p><p>Energy Consumption E. The energy consumption of evaluating DNNs include computation cost E c and memory access cost E m . The former can be formulated as the total energy cost of the total MACs, i.e., E c = ε 1 C, where ε 1 and C denote the energy cost per MAC operation and the total number of MACs, respectively. The latter depends on the storage scheme when executing DNNs on the given mobile device. We assume a memory scheme in which all the weights and activations are stored in a Cache and DRAM memory, respectively, as such a scheme has been shown to enable fast inference execution <ref type="bibr" target="#b8">[8]</ref> [48] <ref type="bibr" target="#b47">[47]</ref>. Hence E can be modeled as:</p><formula xml:id="formula_4">E = E c + E m = ε 1 C + ε 2 S p + ε 3 S f (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where ε 2 and ε 3 denote the energy cost per bit when accessing the Cache and DRAM memory, respectively. To obtain the energy consumption, we refer to a energy model from state-of-the-art hardware implementation of DNNs in <ref type="bibr" target="#b48">[48]</ref>, where the energy cost of accessing the Cache and DRAM memory normalized to that of a MAC operation is claimed to be 6 and 200, respectively. Accordingly:</p><formula xml:id="formula_6">E = ε 1 • C + 6 • ε 1 • S p + 200 • ε 1 • S f (5)</formula><p>where ε 1 is measured to be 52.8 pJ for mobile devices. Summary. The user demand metrics (accuracy A, storage S, latency T and energy cost E) can be formulated with parameters of DNNs (e.g. the number of MAC operations C, the index sets of all activations X and weights W) and platform-dependent parameters (e.g. the energy cost per bit). The parameters of DNNs are tunable via various DNN compression techniques. Different mobile platforms may differ in platform parameters and resource constraints. Hence it is desirable to automatically select appropriate compression techniques to optimize the performance requirements and resource constraints for each application and mobile platform.</p><p>Note that it is difficult to precisely model certain user demand metrics e.g. energy consumption since they are tightly coupled with the underlying hardware and may change from device to device. However, the ranking of the estimated costs of the DNNs derived by the models above is consistent with the ranking of the actual costs of these DNNs measured on mobile devices. As will be introduced in the next section, the proposed AdaDeep framework is generic and more advanced metric estimation models can be easily integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ON-DEMAND OPTIMIZATION USING DEEP REINFORCEMENT LEARNING</head><p>We leverage deep reinforcement learning (DRL) to solve the optimization in Eq.( <ref type="formula" target="#formula_0">1</ref>). Specifically, a DQN is employed to learn the automatic agent for selection of hyper-parameters as well as compression techniques, to maximize the performance benefits (i.e., A </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basics for DRL and DQN</head><p>Reinforcement learning refers to a machine learning paradigm that learns an optimal policy, by trail and error, for sequential decision making problems <ref type="bibr" target="#b35">[35]</ref>. The environment is typically formulated as a Markov decision process (MDP) and the reinforcement learning has been recently combined with deep learning, also known as deep reinforcement learning (DRL) to handle complex input, action and rewards to learn an agent.</p><p>In the literature of MDP and DRL, a policy π refers to a specific mapping from the state o to the action a. A reward function R(o, o , s) returns the gain when transitioning to state o after taking action a in state o. Given a state o, an action a and a policy π , the actionvalue (a.k.a. the Q function) of the pair (o, a) under π is defined by the action-value, which defines the expected future discounted reward for taking action a in state o and then following policy π thereafter. A deep Q-network (DQN) iteratively improves its Qfunction estimate by taking actions in the environment, observing the reward and next state, and updating the estimate. DQN has the proven capability to find the optimal policy for any finite MDP <ref type="bibr" target="#b35">[35]</ref>. Once a DQN is learned, the optimal policy for each state o can be decided by selecting a with the highest Q-value. Table <ref type="table" target="#tab_0">1</ref> explains the contextual definitions of DQN terms in our compression problem.</p><p>We propose to adopt DQN for automatic DNN model compression in AdaDeep for the following reasons.</p><p>• DQN can implement automatic decision based on the dynamically detected performance and cost metrics. • The DQN-controller, which is also a neural network architecture, is suited for the feed-forward and back-propagation when training the target regular DNN. The DQN's output is a decision signal that controls which compressed models are selected and combined. In other words, the regular DNN and the DQN can be trained jointly end-to-end <ref type="bibr" target="#b33">[33]</ref>. • DRL is suited for non-linear and non-differentiable optimization. Also, within the framework of DQN, we can add some more new compression techniques by simply adding branch sub-networks (optional actions), and figure out the function of the complex optimization problem's input and results by the DQN mapping. Therefore a DQN-based optimizer provide both capability and flexibility in DNN compression.</p><p>To apply DQN in DNN compression, we need to (i) carefully design a reward function to utilize DQN to solve a constrained optimization problem; and (ii) design a DQN structure with tractable computation complexity. In this work we propose a novel DQN based optimizer for Eq.( <ref type="formula" target="#formula_0">1</ref>), which separates the reward of performance gain and constraint satisfaction into two streams by two parallel parts in the dueling DQN structure <ref type="bibr" target="#b46">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Design of Reward Function in AdaDeep</head><p>To define the reward function R to optimize Eq.( <ref type="formula" target="#formula_0">1</ref>), a common approach is to use the Lagrangian Multiplier function <ref type="bibr" target="#b21">[21]</ref> to first convert the constrained formulation into an unconstrained one:</p><formula xml:id="formula_7">R = [μ 1 Norm(A -A min ) + μ 2 Norm(E max -E) + μ 3 Norm(T bдt - C P ) + μ 4 Norm(S Cache -S p )]<label>(6)</label></formula><p>where μ 1 , μ 2 , μ 3 and μ 4 are the Lagrangian multipliers. It merges the objective (e.g. accuracy, energy) and the constraint satisfaction (how well the latency and storage usages meet budget requirement). However, maximizing Eq.( <ref type="formula" target="#formula_7">6</ref>) rather than Eq.( <ref type="formula" target="#formula_0">1</ref>) can cause ambiguity, in the sense that the following two situations may lead to the same objective values and are thus indistinguishable: (i) poor accuracy and energy performance, with low latency/storage usage; and (ii) high accuracy and energy performance, with high latency/storage usage. Such ambiguity can easily result in a compressed DNN that exceeds the latency/storage usage defined by end developers. To avoid such ambiguity, we define two loss functions for the objective gain and for the constraint satisfaction, respectively. We adopt a dueling DQN architecture <ref type="bibr" target="#b46">[46]</ref>, which separates the stateaction value function and the state-action advantage function (see Figure <ref type="figure" target="#fig_3">3</ref>) into two parallel streams. The two stream share the convolutional (conv) layers with parameters ω which learn the representations of state, but are followed by two separate columns to generate state-action objective gain value G with weight parameter β, and the state-action constraint satisfaction value H with weight parameter η, respectively. The two columns are then aggregated to output an single state-action value Q. We define a novel Q value: The network G and H comes with their corresponding reward functions R 1 and R 2 ,:</p><formula xml:id="formula_8">Q(o, a; ω, β, η) = G(o, a; ω, β) + H (o, a; ω, η) (7)</formula><formula xml:id="formula_9">R 1 = μ 1 Norm(A -A min ) -μ 2 Norm(E max -E) R 2 = μ 3 Norm(T bдt - C P ) + μ 4 Norm(S Cache -S p )<label>(8)</label></formula><p>After taking an action, we can observe the reward R 1 for G, and R 2 for H. Their interaction and balance guide the selection process. </p><formula xml:id="formula_10">Q tдt i = R1+R2+γQ(o , arдmaxQ(o , a ; ω i , β i , η i ); ω, β, η); 8</formula><p>Perform greedy descent iteratively to tune DQN's ω on loss of random mini-batches replay:</p><formula xml:id="formula_11">L(ω) = E (o,a, R,o )∼Λ (Q tдt i -Q(o i , a i ; ω i , β i , η i )) 2 ; Every num steps reset Q tдt = Q; 9 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Design of DRL Optimizer in AdaDeep</head><p>The proposed DRL optimizer based on DQN is outline in Algorithm 1. We select random action with probability ϵ and select the action with largest Q value by 1ϵ probability (ϵ = 0.001 by default). To build a DQN with weight parameters ω, β and η, we optimize the following loss function at iteration i to update</p><formula xml:id="formula_12">Q(o, a; ω i , β i , η i ). L(ω i ) = E (o,a, R,o )∼Λ [(Q tдt i -Q(o, a; ω i , β i , η i )) 2 ]<label>(9)</label></formula><p>with the frozen Q value learned by target network <ref type="bibr" target="#b43">[43]</ref>:</p><formula xml:id="formula_13">Q tдt i = R 1 + R 2 + γQ(o , arg max Q(o , a ; ω i , β i , η i ); ω, β, η) (10)</formula><p>We adopt the standard DQN training techniques <ref type="bibr" target="#b46">[46]</ref> and use the update rule of SARSA <ref type="bibr" target="#b44">[44]</ref> with the assumption that future rewards are discounted by a factor γ <ref type="bibr" target="#b35">[35]</ref> of the default value 0.01. The DQN is further trained with random samples from reply memory Λ to increase the efficiency of experience replaying. Note that since DRL-based optimization is still heuristic, the proposed optimizer in AdaDeep cannot theoretically guarantee a global optimal solution. However, as we will show in the evaluations, the DRL-based optimizer outperforms exhaustive or greedy approaches in terms of the performance of the compressed DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>This section presents the evaluations of AdaDeep across various recognition tasks and mobile platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>We first present the settings for our evaluation.</p><p>Implementation. We implement AdaDeep with TensorFlow <ref type="bibr" target="#b14">[14]</ref> in Python. The compressed DNNs generated by AdaDeep are then loaded into the target platforms and evaluated as Android projects executed in Java. Specifically, AdaDeep selects an initial DNN from a pool of three state-of-the-art DNN models, including LeNet <ref type="bibr" target="#b29">[29]</ref>, AlexNet <ref type="bibr" target="#b23">[23]</ref>, and VGG <ref type="bibr" target="#b37">[37]</ref>, according to the size of samples in D t . For example, LeNet is selected when the sample size is smaller than 28×28, otherwise AlexNet or VGG is chosen. Standard training techniques, such as stochastic gradient descent (SGD) and Adam <ref type="bibr" target="#b22">[22]</ref>, are used to obtain weights for the DNNs.</p><p>Evaluation applications and datasets. To evaluate the performance of the proposed AdaDeep, five commonly used mobile applications are considered, for which the corresponding benchmark datasets are summarized in Table <ref type="table" target="#tab_2">2</ref>. Specifically, AdaDeep is evaluated for hand-written digit recognition (D 1 : MNIST <ref type="bibr" target="#b28">[28]</ref>), image classification (D 2 : Cifar10 <ref type="bibr" target="#b24">[24]</ref> and D 3 : ImageNet <ref type="bibr" target="#b9">[9]</ref>), audio sensing application (D 4 : UbiSound <ref type="bibr" target="#b36">[36]</ref>), and activity recognition (D 5 : Har <ref type="bibr" target="#b42">[42]</ref>). According to the sample size (see Table <ref type="table" target="#tab_2">2</ref>), LeNet <ref type="bibr" target="#b29">[29]</ref> is selected as the initial DNN for D 1 , D 2 , D 4 and D 5 , while AlexNet <ref type="bibr" target="#b23">[23]</ref> and VGG-16 <ref type="bibr" target="#b37">[37]</ref> are chosen for D 3 .</p><p>Mobile platforms for evaluation. We evaluate AdaDeep on twelve commonly used mobile platforms, including six smartphones, two wearable devices, two development boards and two smart home devices, which are equipped with varied processors, storage and battery capacity as elaborated in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Layer Compression Technique Benchmark</head><p>In our experiment, the first step is to study the performance differences of the state-of-the-art DNN compression techniques in terms of user demand metrics, i.e., accuracy A, storage S, latency T , and energy cost E. The outcomes of this study indicates the need to suitably combine compression techniques for different user demands on performance and cost constraints, and will also serve as baselines for evaluating AdaDeep.  </p><formula xml:id="formula_14">(W 1f , W 2 , W 3 , W 1c ), convolution decomposition (C 1 , C 2 , C 3 )</formula><p>, and special architecture layers (L 1 , L 2 , L 3 ), to a 13-layer AlexNet (input, conv 1 , pool 1 , conv 2 , pool 2 , conv 3 , conv 4 , conv 5 , pool 3 , fc 1 , fc 2 , fc 3 and output) <ref type="bibr" target="#b23">[23]</ref> and compare their performance evaluated on CIFAR-10 dataset (D 2 ) <ref type="bibr" target="#b24">[24]</ref> on a RedMi 3S smartphone (Device 1 in Table <ref type="table" target="#tab_3">3</ref>). The details of the compression techniques are as follows.</p><p>• W 1f : insert a fully-connected (fc) layer between fc i and fc (i+1) layers using the singular value decomposition (SVD) based weight matrix factorization <ref type="bibr" target="#b25">[25]</ref>. The number of neurons k in the inserted layer is set as k = m/12, where m is the number of neurons in fc i . • W 2 : insert a fc layer between fc i and fc (i+1) using sparsecoding, another matrix factorization method <ref type="bibr" target="#b6">[6]</ref>. The k-basis dictionary used in W 2 is set as k = m/6, where m is the number of neurons in fc i . • W 3 : prune fc 1 and fc 2 using the magnitude based weight pruning strategy proposed in <ref type="bibr" target="#b16">[16]</ref>. It removes unimportant weights whose magnitudes are below a threshold (i.e., 0.001). • L 3 : replace the traditional fc layers, fc i and fc i+1 , with a global average pooling layer <ref type="bibr" target="#b31">[31]</ref>. It generates one feature map for each category in the last conv layer. The feature map is then fed into the softmax layer. • W 1c : insert a conv layer between conv i and pool i using SVD based weight factorization <ref type="bibr" target="#b25">[25]</ref>. The numbers of neurons k in the inserted layer by SVD is set as k = m/12, where m is the number of neurons in conv i . • C 1 : decompose conv i using convolution kernel sparse decomposition <ref type="bibr" target="#b32">[32]</ref>. It replaces a conv layer using a two-stage decomposition based on principle component analysis.</p><p>• C 2 : decompose conv i with depth-wise separable convolution <ref type="bibr" target="#b18">[18]</ref> and we set the width multiplier α = 0.5. It is a key technique of Google's MobileNet <ref type="bibr" target="#b18">[18]</ref>, which decomposes the standard conv into a depth-wise convolution and a 1 × 1 point-wise convolution. • C 3 : decompose conv i using the sparse random technique <ref type="bibr" target="#b7">[7]</ref> and we set the sparsity coefficient θ = 0.75. The technique replaces the dense connections of a small number of channels with sparse connections between a large number of channels for convolutions. Different from C 2 , it randomly applies dropout across spatial dimensions at conv layers. • L 1 : replace conv i by a Fire layer <ref type="bibr" target="#b20">[20]</ref>. A Fire layer is composed of a 1 × 1 conv layer and a conv layer with a mix of 1 × 1 and 3 × 3 conv filters. It decreases the sizes of input channels and filters. • L 2 : replace conv i by a micro multi-layer perceptron embedded with multiple small kernel conv layers (Mlpconv) <ref type="bibr" target="#b31">[31]</ref>.</p><p>It approximates a nonlinear function to enhance the abstraction of conv layers with small (e.g. 1 × 1) conv filters.</p><p>The parameters (i.e., k in W 1f , W 1c and W 2 , the depth multiplier α in C 2 , the sparse random multiplier θ in C 3 ) are empirically optimized by comparing the performance improvement on the layer where the compression technique is applied. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, compression techniques W 1f , W 2 , W 3 and L 3 can be applied to the fc layers (fc 1 , fc 2 and fc 3 ), while W 1c , C 1 , C 2 , C 3 , L 1 and L 2 are employed to compress the conv layers (conv 2 , conv 3 , conv 4 and conv 5 ). For each layer compression technique, we load the compressed DNN on Device 1 to process the test data 10 times, and obtain the mean and variance of the inference performance and resource utilization cost, considering the varied workload of the device at different test times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance of Single Compression Technique.</head><p>To illustrate the performance of different compression techniques, we compare their compressed DNNs in terms of the evaluation metrics (A, S p , S f , T and E), over both the initial layer that they are applied to (see Figure <ref type="figure" target="#fig_6">5</ref>) and the entire initial network, i.e., AlexNet (see Figure <ref type="figure" target="#fig_7">6</ref>). First, we can see that overall these mainstream compression techniques are quite effective in trimming down the complexity of the initial network, with a certain accuracy loss (0.3% -10.2%) or accuracy gain (0.5% -2.4%). For example, the compression techniques W 3 and L 3 reduce S p by about 150 -203MB, while W 1c , C 1 , C 2 , C 3 , L 1 and L 2 reduce S p to be less than 10MB. Second, as expected, compressing the fc layers (W 1f , W 2 , W 3 , and L 3 ) results in a higher S p reduction, while compressing the conv layers (W 1c , C 1 , C 2 , C 3 , L 1 or L 2 ) lead to a larger C reduction. This is due to the common observation in DNNs that the conv layers consume dominant computational cost while the fc layers account for most of the storage cost. Third, most of the considered compression techniques affect the S f of both a certain layers and the AlexNet model only in the order of KB, thus we only consider S p for the storage cost in following experiments. Fourth, a higher reduction on S p corresponds to a better energy efficiency in this experiment, indicating that the energy cost of fetching weights, i.e., memory access, dominates in the considered AlexNet model.</p><p>Summary. The performance of different categories of compression techniques on the same DNN varies. Within the same category of compression techniques, the performance also differs. There is no a single compression technique that achieves the best A, S, T and E. To achieve optimal overall performance on different mobile platforms and in different applications, it is necessary to combine different compression techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Performance of Blindly Combined Compression Techniques.</head><p>In this experiment, we compare the performance when blindly combining two compression techniques, tested on a RedMi 3S smartphone (Device 1) using the AlexNet model and CIFA-10 dataset (D 1 ). Specifically, one of the four techniques to compress the fc layers fc 1 and fc 2 (i.e., W f , W 2 , W 3 or L 3 ) is combined with one of the six techniques to compress the conv layer conv 2 (i.e., W 1c , C 1 , C 2 , C 3 , L 1 or L 2 ), leading to a total of 24 combinations. Among them, the W 1c + W 2 , L 1 + L 3 and L 2 + L 3 combinations have been introduced in the prior works named SparseSep <ref type="bibr" target="#b6">[6]</ref>, SqueezeNet <ref type="bibr" target="#b20">[20]</ref> and NIN <ref type="bibr" target="#b31">[31]</ref>, respectively. Table <ref type="table">4</ref> summarizes the results. First, the compressed AlexNet using the W3 technique achieves the best overall performance. In particular, it achieves a detection accuracy of 79.9% and requires a parameter storage of 6.09MB, an energy cost of 30.72mJ , and a detection latency of 189ms. Second, compared with the compressed model using W3, some combinations of layer compression techniques, e.g. C 2 +W 3 and C 2 +L 3 , reduce more than 48mJ of the energy cost E, decrease the latency by about 103ms, and dramatically cut down the storage requirement S p by more than 18MB, while incurring only 2.4% accuracy loss. On the other hand, some combinations, e.g. W 1c +L 3 and C 3 +W 3 , incur over 28% accuracy loss, and might perform worse than a single compression technique. Third, the combination of L 1 +W 3 achieves the best balance between system performances and resource constraints.</p><p>Summary. Some combinations of two layer compression techniques can dramatically reduce the resource consumption of DNNs than using a single technique. Others may lead to performance degradation. Furthermore, the search space grows exponentially when combining more than two techniques. These results demonstrate the need for an automatic optimizer to select the proper combination of compression techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance of AdaDeep</head><p>In this subsection, we first evaluate the performance of AdaDeep's core block DRL optimizer, and then test performance of AdaDeep over five different tasks and on twelve different mobile platforms. Furthermore, to show the flexibility of AdaDeep in adjusting the optimization objectives based on user demand, we show some examples of the choices on the scaling coefficients in Eq. ( <ref type="formula" target="#formula_9">8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Performance of the proposed DRL optimizer.</head><p>This experiment is to evaluate the advantage of the proposed DRL optimizer when searching for the optimal compression combination. To do so, we compress [LeNet, MNIST] and [AlexNet, CIFAR-10] using the DRL optimizer and two baseline optimization schemes and evaluate the resulted DNNs on a RedMi 3S snartphone (Device 1). The accuracy loss (%) and the cost reduction (×) are normalized over the compressed DNNs using the W 3 technique.</p><p>• Exhaustive optimizer: This scheme exhaustively test the performance of all combinations of two compression techniques (similar to Section 5.2.3), and select the best trade-off on the validation dataset of MNIST, i.e., the one that yields the largest reward value defined by Eq. ( <ref type="formula" target="#formula_7">6</ref>). The selected one is L 2 +L 3 , i.e., Fixed, in both the cases of LeNet on MNIST and AlexNet on CIFAR-10. • Greedy optimizer: It loads the DNN layer by layer and selects the compression technique that has the largest reward value defined by Eq. ( <ref type="formula" target="#formula_7">6</ref>), in which both μ 1 and μ 2 are set to be 0.5. Also, when T or S violate the budget T bдt or S bдt , the optimization terminates. • DRL optimizer: It compresses the DNN using the DRL optimizer as described in Section 4. We set the scaling coefficients in Eq. ( <ref type="formula" target="#formula_9">8</ref>) to be μ 1 = 0.6 and μ 2 = 0.4 considering that the battery capacity in RedMi 3S is relatively large and thus the energy consumption is of lower priority, and we set μ 3 = 0.5 and μ 4 = 0.5 in Eq. ( <ref type="formula" target="#formula_9">8</ref>) because their corresponding constraints (i.e., C and S p ) are equally important. The same as in the Greedy search within this subsection. Table <ref type="table" target="#tab_4">5</ref> summarizes the best performance achieved by the above three optimizers. We can see that the networks generated by our  DRL optimizer achieve the best overall performance in terms of storage, latency, and energy consumption, while incurring negligible accuracy loss (0.1% or 2.1%), compared to those generated by the two baseline optimizers. In particular, compared with the DNN compressed by W 3 , the best DNN from the Greedy optimizer only reduces the storage size S p by 4.6× and 2.2× in the cases of [LeNet, MNIST] and [AlexNet, CIFAR-10], respectively. In contrast, the best DNN from the Exhaustive optimizer, i.e., Fixed, can reduces S p by 23.9× and 3.5×, respectively, while that generated by the DRL optimizer achieves a maximum reduction of 28.5× and 4.6× on S p , respectively, in the two cases. Second, the network from the DRL optimizer is also the most effective in reducing the latency (&gt; 2.3×) in both experiments, while those from the two baseline optimizers may result in an increased latency in some cases. For example, the network from the Greedy optimizer increases T by 0.6× in the [LeNet, MNIST] experiment and the one from the Exhaustive optimizer introduces an 0.7× extra latency in the [AlexNet, CIFAR-10] experiment. Third, when comparing the energy cost, Fixed is the least energy-efficient (reduce the energy consumption by only 1.1× over the DNN compressed by W 3 ), while those from both the DRL and Greedy optimizers achieve an energy reduction of 1.8× to 2.8×, respectively. Meanwhile, the accuracy loss of the networks from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: Performance of combining two compression techniques to compress both the fc layers and the conv layers, evaluated on a RedMi 3S smartphone (Device 1) using the AlexNet model and CIFAR-10 dataset (D 1 ).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measured accuracy &amp; cost Compression technique</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Measured accuracy &amp; cost A(%) S p (MB) T (ms) E(mJ )</head><p>A . Also, we recognize that it is impossible to compare the performance of the network generated by AdaDeep with that of the DNN compressed by the optimal combination among all possible combinations, because it is not practical to test all possible combinations of the considered compression techniques and identify the optimal one. Summary. DRL optimizer outperforms the other two schemes in terms of the storage size, latency, and energy consumption while incurring negligible accuracy in diverse recognition tasks. This is because the run-time performance metrics (A, S, T and E) and the resource constraints (S and T ) are systematically included in the reward value and adaptively feedback to the DRL decision process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">AdaDeep over Different</head><p>Tasks. In this experiment, AdaDeep is evaluated on all the five tasks/datasets (see Table <ref type="table" target="#tab_2">2</ref>) using a RedMi 3S smartphone (Device 1). We set the scaling coefficients in Eq. ( <ref type="formula" target="#formula_9">8</ref>) to be the same as those for the DRL optimizer in § 5.3.1, i.e., μ 1 = 0.6 and μ 2 = 0.4, μ 3 = 0.5 and μ 4 = 0.5. In addition, we assume a Cache storage budget of 2 MB and a latency budget of 10 ms.</p><p>Performance. Table <ref type="table" target="#tab_5">6</ref> compares the performance of the best DNNs generated by AdaDeep on the five tasks in terms of accuracy loss, storage S p , computation C (total number of MACs), latency T and energy cost E, normalized over the DNNs compressed using W 3 . Compared with their initial DNNs, DNNs generated by AdaDeep can achieve a reduction of 1.8× -38× in S p , 0.8× -3.3× in C, 0.8× -19.8× in T , and 1.1× -4.3× in E, with a negligible accuracy loss (&lt; 1%) or even accuracy gain (&lt; 4.9%).</p><p>Summary. For different compressed DNNs, tasks, and datasets, the combination of compression techniques found by AdaDeep also differs. Specifically, the combination that achieves the best performance while satisfying the resource constraints is C 3 +W 3 for Task 1 (on MNIST initialized using LeNet), L 1 +W 3 for Task 2 (on Cifar10 initialized using AlexNet), L 2 +C 1 +L 3 for Task 3 (on Ima-geNet initialized using AlexNet), L 2 +C 2 +L 3 for Task 4 (on ImageNet initialized using VGG), C 3 +L 3 for Task 5 (on Ubisound initialized using LeNet), and L 1 +W 3 for Task 6 (on Har initialized using LeNet), respectively. We can see that although the combination of compression techniques found by AdaDeep cannot always outperforms a single compression techniquein in all metrics, it achieves a better overall performance in terms of the five metrics according to the specific user demands. As shown in Table <ref type="table" target="#tab_3">3</ref>, different devices have different resource constraints, which lead to different performance and budget demands and thus require different coefficients μ 1 ∼ μ 4 in Eq. ( <ref type="formula" target="#formula_9">8</ref>). Specifically, we empirically optimize μ1 ∼ μ4 for different devices to be:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">AdaDeep over</head><formula xml:id="formula_15">μ 2 = max { 4000-E bat t ery 4000 , 0.6}, μ 1 = 1 -μ 2 , μ 4 = max { 8-S Cache</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>, 0.6}, and μ 3 = 1μ 4 .</p><p>Performance. Table <ref type="table" target="#tab_6">7</ref> summarizes the generated compression combinations by AdaDeep and the performance of the corresponding compressed DNNs. For the twelve different resource constraints, DNNs generated by AdaDeep, which are initiated with the same DNN model, can reduce the parameter size by 3.4× -28.1×, computation cost by 1.6× -6.8×, latency by 1.1× -3.1× and energy cost by 1.1× -9.8×, respectively, while incurring a negligible accuracy loss (≤ 2.1%). The optimal combinations of compression techniques found by AdaDeep differ from device to device. Furthermore, AdaDeep finds some combinations that work the best for a given mobile platform yet have not been proposed by previous works (e.g. Summary. Overall, AdaDeep can automatically select the proper combinations of compression techniques that meet diverse demands on accuracy and resource constraints within 3.5 to 15 hours. We find that the optimal compression strategy differs over tasks and across mobile devices, and there is no one-fit-all compression technique for all tasks and mobile devices. AdaDeep is able to adaptively select the best compression strategy given diverse user demands. It also uncovers some combinations of compression techniques not proposed in previous works. Also, the sensitivity of the performance metrics to different resources may vary for different choices of the scaling coefficients (μ 1 ∼ μ 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Our work is closely related to the following research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Automatic Hyperparameter Optimization</head><p>Hyperparameters of DNNs, such as the number of layers and neurons, the size of filters and the model architecture, are crucial to the inference accuracy of DNNs. Common hyperparameter tuning techniques can be categorized into parallel search, such as grid search <ref type="bibr" target="#b5">[5]</ref> and random search <ref type="bibr" target="#b4">[4]</ref>, and sequential search, such as Bayesian optimization <ref type="bibr" target="#b38">[38]</ref>. The grid and random search approaches search blindly and thus are usually time-consuming. Bayesian approaches <ref type="bibr" target="#b39">[39]</ref> [11] <ref type="bibr" target="#b40">[40]</ref> automatically optimize the hyperparameters, but can still be slow due to the intrinsically sequential operations.</p><p>In general, DNNs need to be compressed either during or after training to be deployed on resource-constrained mobile devices, due to their high complexity. Selecting compression techniques can be viewed as a hyperparameter tuning process. Inspired by state-of-the-art automatic hyperparameter optimization techniques, AdaDeep is the first work to treat compression techniques as a new hyperparameter to be tuned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">DNN Compression</head><p>The success and popularity of machine learning in mobile and IoT applications <ref type="bibr" target="#b12">[12]</ref>[19] <ref type="bibr" target="#b27">[27]</ref>[49] has stimulated the adoption of more powered DNNs in mobile and embedded devices. Compression is a commonly employed technique to trim down the complexity of DNNs, which can be performed by reducing the weight precision, or the number of operations, or both <ref type="bibr" target="#b41">[41]</ref>. Various DNN compression techniques have been proposed, including weight compression <ref type="bibr" target="#b6">[6]</ref> [16] <ref type="bibr" target="#b25">[25]</ref>, convolution decomposition <ref type="bibr" target="#b7">[7]</ref> [18] <ref type="bibr" target="#b32">[32]</ref>, and compact architectures <ref type="bibr" target="#b20">[20]</ref>  <ref type="bibr" target="#b31">[31]</ref>. However, existing compression techniques investigate a one-for-all scheme, e.g. how to reduce DNN complexity using one compression technique, and do not consider the various resource constraints across different deployment platforms.</p><p>AdaDeep enables an automatic selection of the best combination of different compression techniques that balance the applicationdriven system performance and the platform-imposed resource constraints. Specifically, AdaDeep supports automatic selection from three categories of mainstream DNN compression techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Run-time DNN Optimization</head><p>Orthogonal to DNN compression, DNNs can also be optimized at run-time to reduce their resource utilization and unnecessary overhead on energy, latency, storage or computation. MCDNN <ref type="bibr" target="#b17">[17]</ref> pre-evaluates a set of compressed models with different execution overhead and selects one for each DNN that maximizes the accuracy given total cost constrains of multi-programmed DNNs. However, it only presents two cost reduction algorithms. LEO <ref type="bibr" target="#b13">[13]</ref> designs a low power unit resource scheduler to maximize the energy efficiency for the unique workload of different tasks on heterogenous computation resources. DeepX <ref type="bibr" target="#b25">[25]</ref> designs a set of resource control algorithms to decompose DNNs into different unit-blocks for efficient execution on heterogeneous computation resources. EIE <ref type="bibr" target="#b15">[15]</ref> is a dedicated accelerator to execute sparse NN. The above run-time optimization techniques can be applied on top of the compressed DNN generated by AdaDeep to further improve the efficiency of DNN execution on mobile devices. For example, the current version of AdaDeep only leverages the CPU on mobile platforms for DNN execution. The scheduler proposed in <ref type="bibr" target="#b13">[13]</ref> and <ref type="bibr" target="#b25">[25]</ref> can be combined when extending AdaDeep to mobile platforms with heterogenous resources. With proper hardware support, the sparse NN output by AdaDeep can also be executed faster using the accelerator in <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Automatic Control Techniques using DRL</head><p>Deep reinforcement learning (DRL) is widely applied in automaticplay games to learn actions at different states that maximize a given reward function <ref type="bibr" target="#b35">[35]</ref>. For example, Mnih et al. <ref type="bibr" target="#b35">[35]</ref> propose to learn control policies from complex sensory inputs using a deep Qnetwork (DQN). Liu et al. <ref type="bibr" target="#b33">[33]</ref> leverage DQN to dynamically select parts of a NN to execute according to different input resolution so as to improve computational efficiency of multi-objective optimization problems. Achiam et al. <ref type="bibr" target="#b1">[1]</ref> solve the constrained optimization problem with DRL by replacing the objective and constraints with approximate surrogate, i.e., lower bound on policy divergence. However, the required operation of inverting the divergence matrix is in general impractically expensive. Bello et al. <ref type="bibr" target="#b3">[3]</ref> present a framework to tackle the combinatorial optimization of sequential problems with DRL and recurrent DNN.</p><p>To the best of our knowledge, AdaDeep is the first work to leverage DQN for DNN compression optimization, considering both application-driven system performance and platform constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Automatic DNN Architecture Optimization</head><p>An emerging topic of interest for the deep learning community is to automate the engineering process of deep model architectures: using recurrent networks and reinforcement learning to generate the model descriptions of deep models <ref type="bibr" target="#b50">[50]</ref>, or by transferring architectural building blocks to construct scalable architectures on larger datasets <ref type="bibr" target="#b51">[51]</ref>. Those methods are purely data-driven, with deep architectures composed with the goal to maximize the expected accuracy on a validation set. Lately, a handful of exploratory works have emerged to correlate the model composition with domain knowledge. For example, Andreas et al. <ref type="bibr" target="#b2">[2]</ref> constructed and learned modular networks, which composed collections of jointlytrained neural "modules" into deep networks for question answering, to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. Devin et al. <ref type="bibr" target="#b10">[10]</ref> proposed a similar modular network by decomposing robotic policies into task-specific and robot-specific modules, to facilitate multi-task and multi-robot policy transfer. However, none of those previous efforts have correlated their efforts with DNN compression and energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper presents AdaDeep, an automatic optimization framework that selects a combination of compression techniques that balance diverse user-specified performance goals and device-imposed resource constraints. We systematically formulate the goals and constraints on accuracy, latency, storage and energy into a unified optimization problem, and leverage a deep reinforcement learning based strategy to effectively find the feasible combination of compression techniques. Evaluations on five widely used datasets and twelve different mobile devices show that there is no one-fit-all compression technique that meets the specific performance goals and resource constraints. It also figures out some combinations of compression techniques unexplored in previous DNN compression research. AdaDeep is the first work that models DNN compression as a new hyperparameter for automatic tuning. In the future, we plan to investigate fully automatic hyper-parameter tuning to optimize DNNs suited for mobile and embedded platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We propose a reinforcement learning based optimizer to automatically select the best combination of DNN compression techniques. AdaDeep extends the automation of deep model architecture tuning to include DNN compression. • Experiments show that the DNNs generated by AdaDeep achieve comparable performance to existing compression techniques under various user demands (datasets, tasks, and mobile platforms). AdaDeep also uncovers some combinations of compression techniques suitable for mobile platforms that have not been proposed in previous DNN compression works [6][20][31].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Block diagram of AdaDeep. Users e.g. DNN application developers, submit their system performance requirements and the resource constraints of the target platform to AdaDeep. Then AdaDeep automatically generates a DNN that balances these requirements and constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed DRL optimizer for Eq.(1). It takes performance requirements and cost constraints as input, automatically selects compression techniques as well as other hyperparamters, and outputs an optimally compressed neural network. C, Mp and Q stand for the conv layer, the maxpooling layer and the DQN based optimizer, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of dueling DQN adopted by AdaDeep. The rewards of performance gain G and constraint satisfaction H are separated to keep the computational load of the optimization tractable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 2 . 1</head><label>21</label><figDesc>Benchmark Settings. We apply ten mainstream compression techniques from three categories, i.e., weight compression MobiSys'18, June 10-15, 2018, Munich, Germany S.Liu et al.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of the locations that different layer compression techniques are applied to AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of different layer compression techniques minus by the initial layer that they are applied to, in terms of accuracy A, storage (S p , S f ), computational cost C, latency T , and energy cost E. The compression techniques W 1f , W 2 , W 3 , and L 3 are applied to fc 1 and fc 2 , while W 1c , C 1 , C 2 , C 3 , L 1 and L 2 are applied to conv 2 . The Y-axis denotes the accuracy loss (%) over the initial AlexNet and the cost reduction over the initial layer that they are applied to.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of different layer compression techniques normalized over the entire AlexNet in terms of accuracy A, storage (S p , S f ), computational cost C, latency T , and energy cost E. The compression techniques W 1f , W 2 , W 3 , and L 3 are applied to fc 1 and fc 2 , while W 1c , C 1 , C 2 , C 3 , L 1 and L 2 are applied to conv 2 . The Y-axis denotes the accuracy loss (%) and the cost reduction over the original entire AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Different Mobile Devices. This experiment evaluates AdaDeep across twelve different mobile devices (see Table 3) using LeNet and UbiSound (D 4 ) as the initial DNN and evaluation dataset, respectively. The performance achieved by the initial DNN is as follows: A = 95.1%, S p = 25.2 MB, C = 28, 324, 864, T = 31 ms, and E = 4.3 mJ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>C 1 +W 3 for Device 1, C 2 +W 3 for Devices 3, 4 and 5, C 3 +W 3 for Device 11). The training process of AdaDeep includes three intertwined phases: training the regularized DNN, re-training (such as in L 1 , L 2 , L 3 ) or fine-tuning (such as in W 3 ) DNN for compression, and training the DQN optimizer. Because the training time of the regularized DNN is standard, we only quantify the total training time required by the DNN compression and the DQN based selection on different tasks, which is 3 hours on [MNNIST, LeNet], 10 hours on [CIFAR-10, LeNet], 6.5 hours on [CIFAR-10, AlexNet], 3.5 hours on [Ubisound, LeNet], 2 hours on [Har, LeNet], and 15 hours on [ImaeNet, AlexNet], respectively, using two HP Z400 workstations with two GEFORCE GTX 1060 GPU cards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : DQN terms explained in the context of DNN compression.</head><label>1</label><figDesc></figDesc><table><row><cell>DQN Terms</cell><cell>Contextual Meanings for DNN compression</cell></row><row><cell>State o∼Os</cell><cell>Input feature size to DRL</cell></row><row><cell>Action a∼As</cell><cell>Selectable combinations of compression techniques</cell></row><row><cell>Reward function R</cell><cell>Optimization gain G &amp; constraints satisfaction H</cell></row><row><cell>Q value = γ R</cell><cell>Potential optimization gain &amp; constraints satisfaction</cell></row><row><cell cols="2">Training loss function Difference between true Q value with the estimated Q value of DQN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Summary of the applications and corresponding datasets for evaluating AdaDeep</head><label>2</label><figDesc>.</figDesc><table><row><cell cols="2">No. Task</cell><cell>Dataset</cell><cell>Description</cell><cell>Input Size</cell></row><row><cell>D 1</cell><cell>Digit</cell><cell>MNIST [28]</cell><cell>55, 000 images, 10 classes</cell><cell>(28, 28, 1)</cell></row><row><cell>D 2</cell><cell>Image</cell><cell cols="2">CIFAR-10 [24] 60, 000 images, 10 classes</cell><cell>(32, 32, 3),</cell></row><row><cell>D 3</cell><cell>Image</cell><cell>ImageNet [9]</cell><cell>a small version of ImageNet, 65, 000 images, 5 classes</cell><cell>(120, 120, , 3)</cell></row><row><cell>D 4</cell><cell>Audio</cell><cell cols="2">UbiSound [36] 7, 500 audio clips in wav, 9 classes</cell><cell>(40, 40, 1)</cell></row><row><cell>D 5</cell><cell cols="2">Activity Har [42]</cell><cell cols="2">10, 000 records of accelerometer and gyroscope, 7 classes (33, 17, 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Summary of the varied resource constraints of the mobile platforms for evaluating AdaDeep</head><label>3</label><figDesc>.</figDesc><table><row><cell>Type</cell><cell>Device</cell><cell>Processor</cell><cell cols="2">DRAM Cache</cell><cell>Battery</cell></row><row><cell></cell><cell>1. Xiaomi Redmi 3S</cell><cell>Qualcomm 430</cell><cell>3 GB</cell><cell cols="2">L 2 -Cache(2 MB) 4100 mAh</cell></row><row><cell></cell><cell>2. Xiaomi Mi 5S</cell><cell>Qualcomm B21</cell><cell>4 GB</cell><cell cols="2">L 2 -Cache(1 MB) 3200 mAh</cell></row><row><cell>Smart</cell><cell>3. Xiaomi Mi 6</cell><cell>Qualcomm 835</cell><cell>6 GB</cell><cell cols="2">L 2 -Cache(2 MB) 3350 mAh</cell></row><row><cell>phones</cell><cell>4. Huawei pra-al00</cell><cell>HiSilicon kirin655</cell><cell>3GB</cell><cell cols="2">L 2 -Cache(2 MB) 3000 mAh</cell></row><row><cell></cell><cell>5. Samsung note5</cell><cell>samsung exynos7420</cell><cell>4 GB</cell><cell cols="2">L 2 -Cache(2 MB) 3000 mAh</cell></row><row><cell></cell><cell>6. HuaweiP9</cell><cell>HiSilicon kirin955</cell><cell>3 GB</cell><cell cols="2">L 2 -Cache(4 MB) 3000 mAh</cell></row><row><cell>Wearable</cell><cell>7. Sony watch SW 3</cell><cell>Quad-core cortexA7</cell><cell cols="3">512 MB L 2 -Cache(1 MB) 420 mAh</cell></row><row><cell>devices</cell><cell cols="5">8. Huawei watchH2P Snapdragon wear2100 768 MB L 2 -Cache(1 MB) 420 mAh</cell></row><row><cell>Boards</cell><cell>9. firefly-rk3999 10. firefly-rk3288</cell><cell cols="2">Duad-core cortexA72 2 GB Duad-core cortexA17 2 GB</cell><cell cols="2">L 2 -Cache(2 MB) power-plug L 2 -Cache(8 MB) power-plug</cell></row><row><cell>Smart</cell><cell>11. Xiaomi box 3S</cell><cell>Amlogic S905</cell><cell>2 GB</cell><cell cols="2">L 2 -Cache(2 MB) power-plug</cell></row><row><cell>home</cell><cell>12. Huawei box</cell><cell>Hisilicon hi3798M</cell><cell>2 GB</cell><cell>L</cell></row></table><note><p>2 -Cache(1 MB) power-plug</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 : Performance of the best DNN generated by the DRL and baseline optimizers and tested on a RedMi 3S smartphone (Device 1) using LeNet on MNIST (D1) and AlexNet on CIFAR-10 (D2). The accuracy loss % and the cost reduction (×) are normalized over the corresponding DNN compressed using W 3 . Optimizer Compared to the compressed LeNet Compared to the compressed AlexNet</head><label>5</label><figDesc></figDesc><table><row><cell>(%) S p (MB) T (ms) E(mJ )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 : Performance of AdaDeep evaluated on different tasks/datasets using a RedMi 3S smartphone (Device 1), normalized over the corresponding DNNs compressed using W 3 . The compression techniques marked by '*' are the combinations that have not been proposed in related studies.</head><label>6</label><figDesc></figDesc><table><row><cell>Task</cell><cell>Compression techniques</cell><cell cols="3">Compare to the DNN compressed using W 3 technique S p C T E A loss</cell></row><row><cell>1.MNIST (LeNet)</cell><cell>* C 3 + W 3</cell><cell cols="2">1.8× 1.5× 1.8× 1.3×</cell><cell>-2.5%</cell></row><row><cell>2.CIFAR-10 (AlexNet)</cell><cell>L 1 + W 3</cell><cell cols="2">4.6× 3.1× 2.3× 1.8×</cell><cell>-4.9%</cell></row><row><cell>3.ImageNet (AlexNet)</cell><cell>* L 2 + C 1 + L 3</cell><cell cols="2">18.2× 2.1× 3.7× 1.2×</cell><cell>-1.2%</cell></row><row><cell>4.ImageNet (VGG)</cell><cell>* L 2 + C 2 + L 3</cell><cell>38×</cell><cell>3.3× 19.8× 4.3×</cell><cell>0.1%</cell></row><row><cell>5.Ubisound (LeNet)</cell><cell>* C 3 + L 3</cell><cell cols="2">3.2× 1.9× 1.6× 1.1×</cell><cell>0.4%</cell></row><row><cell>6.Har (LeNet)</cell><cell>L 1 + W 3</cell><cell cols="2">2.1× 0.8× 0.8× 1.5×</cell><cell>-2.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 : Performance of AdaDeep on different devices using the UbiSound dataset (D 4 ), normalized over the corresponding initial DNNs. The compression techniques marked by '*' are the combinations that have not been proposed in related studies.</head><label>7</label><figDesc></figDesc><table><row><cell>Device</cell><cell>Compression techniques</cell><cell>S p</cell><cell>Compare to initial DNN C T E</cell><cell>A loss</cell></row><row><cell>1. Xiaomi Redmi 3S</cell><cell>C 1 +W 3</cell><cell cols="3">12.1× 2.1× 1.6× 1.1× 0.9 %</cell></row><row><cell>2. Xiaomi Mi 5S</cell><cell>C 2 +L 3</cell><cell cols="3">27.1× 3.6× 2.1× 1.2× 1.8 %</cell></row><row><cell>3. Xiaomi Mi 6</cell><cell>* C 2 +W 3</cell><cell cols="3">13.1× 5.6× 1.9× 1.6× 1.1%</cell></row><row><cell>4. Huawei pra-al00</cell><cell>* C 2 +W 3</cell><cell cols="3">12.7× 6.8× 1.4× 1.8× 1.2%</cell></row><row><cell>5. Samsung note5</cell><cell>* C 2 +W 3</cell><cell cols="3">12.8× 4.1× 1.6× 1.8× 1.2%</cell></row><row><cell>6. Huawei iP9</cell><cell>C 1 +W 3</cell><cell cols="3">13.0× 1.6× 1.6× 1.7× 0.9%</cell></row><row><cell>7. Sony watch SW 3</cell><cell>C 2 +W 2</cell><cell cols="3">6.4× 2.1× 1.5× 9.8× 1.6%</cell></row><row><cell>8. Huawei watchH2P</cell><cell>L 2 +L 3</cell><cell cols="3">27.8× 3.6× 3.1× 8.3× 2.1%</cell></row><row><cell>9. firefly-rk3999</cell><cell>L 1 +W 3</cell><cell cols="3">13.2× 5.6× 2.6× 1.2× 1.8%</cell></row><row><cell>10. firefly-rk3288</cell><cell>C 2 +W 1f</cell><cell cols="3">3.4× 4.8× 1.1× 1.3× 0.7%</cell></row><row><cell>11. Xiaomi box 3S</cell><cell>* C 3 +W 3</cell><cell cols="3">14.1× 4.1× 1.4× 1.1× 1.2%</cell></row><row><cell>12. Huawei box</cell><cell>L 1 +L 3</cell><cell cols="3">28.1× 1.6× 2.8× 1.2× 1.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Prof. Junzhao Du is the corresponding author.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We are grateful for our shepherd Professor Nic Lane (University of Oxford) and the anonymous reviewers for their valuable comments and suggestions, Professor Lin Zhong (Rice University) for his useful feedback on an early version of this paper, Xin Wang and Yuheng Wei (Xidian University) for their help on implementing some of the baseline techniques, and Prof. Zhangyang Wang  (Texas A&amp;M University) for his useful discussion on Section 4. This work is supported in part by National Key Research &amp; Development Program of China #2018Y FB1003605, Natural Science Foundation of China (NSFC) #61472312, and Natural Science Foundation (NSF) Award #1611295.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Germany</forename><forename type="middle">S</forename><surname>Munich</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<title level="m">Figure 2 shows the DRL optimizer designed for AdaDeep</title>
		<imprint>
			<date type="published" when="2018">June 10-15, 2018</date>
		</imprint>
	</monogr>
	<note>MobiSys&apos;18</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<title level="m">Constrained Policy Optimization. Proceedings of ICML</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.027992</idno>
		<title level="m">Deep compositional question answering with neural module networks</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>James S Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparsification and separation of deep learning layers for constrained resource inference on wearables</title>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SenSys</title>
		<meeting>SenSys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06257</idno>
		<title level="m">The power of sparsity in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning modular neural network policies for multi-task and multi-robot transfer</title>
		<author>
			<persName><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICRA</title>
		<meeting>ICRA</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation</title>
		<author>
			<persName><forename type="first">Biyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jillian</forename><surname>Co</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SenSys</title>
		<meeting>SenSys</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LEO: Scheduling sensor inference algorithms across heterogeneous mobile processors and network resources</title>
		<author>
			<persName><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><forename type="middle">K</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Rachuri</surname></persName>
		</author>
		<author>
			<persName><surname>Mascolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobiCom</title>
		<meeting>MobiCom</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://goo.gl/j7HAZJ" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EIE: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MCDNN: An Approximation-Based Execution Framework for Deep Stream Processing Under Resource Constraints</title>
		<author>
			<persName><forename type="first">Seungyeop</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthai</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobiSys</title>
		<meeting>MobiSys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building Mobile GPU Deep Learning Models for Continuous Vision Applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Loc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngki</forename><surname>Krishna Balan</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MobiSys</title>
		<meeting>MobiSys</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="186" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Forrest N Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Lagrange multiplier approach to variational problems and applications</title>
		<author>
			<persName><forename type="first">Kazufumi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Kunisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam:A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nair</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinton</forename><surname>Geoffrey</surname></persName>
		</author>
		<ptr target="https://goo.gl/hXmru5" />
		<title level="m">The CIFAR-10 dataset</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Forlivesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorena</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Qendro</surname></persName>
		</author>
		<author>
			<persName><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPSN</title>
		<meeting>IPSN</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can deep learning revolutionize mobile sensing?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HotMobile</title>
		<meeting>HotMobile</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepEar: robust smartphone audio sensing in unconstrained acoustic environments using deep learning</title>
		<author>
			<persName><forename type="first">Petko</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorena</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><surname>Qendro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UbiComp</title>
		<meeting>UbiComp</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://goo.gl/t6gTEy" />
		<title level="m">The MNIST database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yan</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://goo.gl/APBzd5" />
	</analytic>
	<monogr>
		<title level="j">LeNet</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">iType: Using eye gaze to enhance typing privacy</title>
		<author>
			<persName><forename type="first">Zhenjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasant</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INFOCOM</title>
		<meeting>INFOCOM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution</title>
		<author>
			<persName><forename type="first">Lanlan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">iType: Using eye gaze to enhance typing privacy</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INFOCOM</title>
		<meeting>INFOCOM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS Workshops</title>
		<meeting>NIPS Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">UbiEar: Bringing Location-independent Sound Awareness to the Hard-of-hearing People with Smartphones</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Sicong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zimu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Junzhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangguan</forename><surname>Longfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of IMWUT</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mr</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian optimization with robust Bayesian neural networks</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09039</idno>
		<title level="m">Efficient processing of deep neural networks: A tutorial and survey</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><surname>Uci</surname></persName>
		</author>
		<ptr target="https://goo.gl/m5bRo1" />
		<title level="m">Dataset for Human Activity Recognition</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A theoretical and empirical analysis of Expected Sarsa</title>
		<author>
			<persName><forename type="first">Harm</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ADPRL</title>
		<meeting>ADPRL</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latency-driven design for FPGA-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stylianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos-Savvas</forename><surname>Venieris</surname></persName>
		</author>
		<author>
			<persName><surname>Bouganis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of FPL</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<title level="m">Dueling network architectures for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Mengwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumay</forename><surname>Pushp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03073</idno>
		<title level="m">Enabling Cooperative Inference of Deep Learning on Wearables and Smartphones</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Designing energy-efficient convolutional neural networks using energy-aware pruning</title>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Design and Implementation of a CSI-Based Ubiquitous Smoking Detection System</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3781" to="3793" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning Transferable Architectures for Scalable Image Recognition</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
