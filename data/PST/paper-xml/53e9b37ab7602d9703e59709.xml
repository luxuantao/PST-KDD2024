<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual humans: thirty years of research, what next?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-11-24">24 November 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nadia</forename><surname>Magnenat-Thalmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">D. Thalmann Vrlab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Miralab</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">D. Thalmann Vrlab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Virtual humans: thirty years of research, what next?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-11-24">24 November 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">C2F811DFBF5CF010E1512D809BEA5D56</idno>
					<idno type="DOI">10.1007/s00371-005-0363-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Virtual human</term>
					<term>Motion modeling</term>
					<term>Crowd modeling</term>
					<term>Cloth and hair simulation</term>
					<term>Believable behavior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present research results and future challenges in creating realistic and believable Virtual Humans. To realize these modeling goals, real-time realistic representation is essential, but we also need interactive and perceptive Virtual Humans to populate the Virtual Worlds. Three levels of modeling should be considered to create these believable Virtual Humans: 1) realistic appearance modeling, 2) realistic, smooth and flexible motion modeling, and 3) realistic high-level behaviors modeling. At first, the issues of creating virtual humans with better skeleton and realistic deformable bodies are illustrated. To give a level of believable behavior, challenges are laid on generating on the fly flexible motion and complex behaviours of Virtual Humans inside their environments using a realistic perception of the environment. Interactivity and group behaviours are also important parameters to create believable Virtual Humans which have challenges in creating believable relationship between real and virtual humans based on emotion and personality, and simulating realistic and believable behaviors of groups and crowds. Finally, issues in generating realistic virtual clothed and haired people are presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the beginning of eighties, we could see short movies and demos involving high quality animation sequences involving very realistic virtual humans. We may cite Adam Powers, the Juggler produced by Triple I or "sexy robot" from Robert Abel. Twenty years later, can we say that there was an incredible progress in virtual humans? This is a very key question and we can answer it in different ways.</p><p>The rendering of the juggler and sexy robot were very nice at that time. Although some aspects including textures are better today, it is very hard to tell that the animation at current stage is any better than that concerning the motion. Both the juggler and sexy robot are realistically animated using typical motion captures, which are still well used for current movies like Final Fantasy or even to build databases of motion for video games.</p><p>So, if rendering and motion are rather similar to twenty years ago, does this mean that the research in the area is dead? Surely not, because, there have been, in fact, considerable advances. <ref type="bibr" target="#b0">1)</ref> In appearance, recent virtual humans have real clothes and hair, which was not possible in the eighties 2) The juggler, for example, was nice, but the body was rigid. Today's virtual humans have deformable bodies: it is possible to generate automatic bodies with various measurements, a very important aspect for virtual fashion, for example. 3) Video games, VR systems, and real-time systems require adaptive and spontaneous motion, which is hard to accomplish with motion capture. It needs to have sophisticated motion control methods and models. 4) Creating interesting games and VR systems with virtual characters requires a high-level behavior of these characters.</p><p>So, the research in cloth and hair modeling, body deformation, motion control, and behavioural animation has made considerable progress in the last decade. However, we are still in the infancy age and a lot of further developments are needed.</p><p>Let us try to consider a virtual human as it should be modeled in a complete and exhaustive way. We may consider three levels of modeling. 1) Appearance modeling. This means that visually the virtual humans should look like real people, this includes face and body shapes of real humans, skin textures, nice hairstyle, and realistic clothes. 2) Realistic, smooth, and flexible motion in any situation. This is a challenge, as flexibility is obtained by creating parameter-based models. However, no model can provide smooth and realistic motion as obtained by motion capture. 3) Realistic high-level behaviour. This is the main challenge, can we expect to model the manner in which humans behave. This is what A.I. and agent technology have been trying for many years. Unfortunately, we cannot expect better results in animation.</p><p>We have also to consider that these levels are not independent and can even also be in conflict each other. For example, clothes and hair are an important aspect in the visual appearance of the virtual humans, but they are also most of the time in motion. Their animation has to be clearly considered. More fundamentally, body motion is not only the result of a skeleton motion, but the deformation of the body itself is an important part and makes the difference with a typical robot. We need to represent these body deformations using sophisticated deformation algorithms and many approaches were proposed in the very early days. We can observe today even a trend to base such algorithms on an anatomy-based approaches derived from medical research.</p><p>Let us consider a very simple example. A virtual human is supposed to enter into a kitchen, look for a glass, find one in a cupboard, grasp it and bring it out of the kitchen. How can one make such a sequence?</p><p>First approach: we just ask a real person to do the job and record it using an optical or magnetic motion capture system; this is the easiest way, very similar to the 80's juggler, and still the way most of today's computer-based movies are done. Now imagine that we decide to replace the glass by a plate. Then the sequence of grasping the object has to be changed, because we do not grasp a glass and a plate in the same way. And what if we replace the human by a child who cannot reach the cupboard? And what if the door of the kitchen is closed? And what if there is a table in front of the cupboard? Each new condition imposes an adaptation of the animation that would probably require the entire process to be restarted.</p><p>Second approach: we use a series of motion controllers including a walking engine, a grasping engine, inverse kinematics. This should solve some problems like changing the glass into a plate, or changing the location of the object, but not the problem of the table obstacle.</p><p>Third approach: we use a task planner with an obstacle avoidance algorithm; this will allow us to solve our problem of the table in front of the cupboard.</p><p>However, will the sequence produced by the most sophisticated approach be better than the original motion capture based sequence? Unfortunately, the answer is no, and even the results will be clearly worse. This is exactly the point. We can make more intelligent virtual humans but they look poorly animated, which gives the wrong impression to people who just watch sequences that after 30 years computer animation has made no progress and it has even regressed.</p><p>What is the future of virtual humans? We may think about virtual worlds inhabited by a virtual human society, where virtual humans will co-operate, negotiate, make friends, communicate, group and ungroup, depending on their likes, moods, emotions, goals, fears, etc. However, the computational models of such interaction and corresponding groups are very hard to develop or even impossible. Behaviour should emerge as a result of a multi-agent system sharing a common environment, in our case, sharing a virtual environment. For example in a panic situation, we should not model the group behaviour as an entity, because each human reacts differently depending, for example, on its level of fear. If we model the individual entity, there will be groups of different behaviours (not programmed explicitly) as result of the interaction of simple individual behaviours. Virtual humans should also have some natural needs like hunger, tiredness, etc., which guide selection of their behaviours. In order to behave in a believable way, these agents also have to act in accordance with their surrounding environment, be able to react to its changes, to the other agents and also to the actions of real humans interacting with the virtual world.</p><p>One important point in the simulation is the believability of the individual virtual humans. They should behave as real humans and have capabilities such as perception, language understanding and generation, emotions, goaldriven behaviour, reactivity to the environment including with other virtual humans, memory, inference, appearance of thought and personalities, interpersonal interactions, social skills and possibly others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Further great challenges in virtual humans</head><p>Real-time realistic virtual humans will be essential in the future, and we will also need interactive perceptive virtual humans to populate the virtual worlds. The ultimate objective in creating realistic and believable virtual actors is to build intelligent autonomous virtual humans with adaptation, perception and memory. These actors should be able to act freely and emotionally. Ideally, they should be conscious and unpredictable. However, how far are we from such an ideal situation?</p><p>The conception of such virtual humans is an immense challenge as it requires solving many problems in various areas. In this section, we will study six major challenges. 1. Creating virtual humans with a better skeleton and realistic deformable bodies. 2. Developing methods to generate on-the-fly flexible motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generating complex behaviours of virtual humans in-</head><p>side their environments using a realistic perception of the environment. 4. Creating believable relationships between real and virtual humans based on emotion and personality. 5. Simulating realistic and believable behaviors of groups and crowds. 6. Generating realistic virtual clothed and haired people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Creating better anatomical bodies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling the skeleton</head><p>The first computerized human models were created 20 years ago by airplane and car manufacturers. The main idea was to simulate a very simple articulated structure for studying problems of ergonomics. Ergonomic analy- Boeman is built as a 23-joint figure with variable link lengths. Sammie (System for Aiding Man Machine Interaction Evaluation) was designed in 1972 at the University of Nottingham for general ergonometric design and analysis. This was, so far, the best parameterized human model and it presents a choice of physical types: slim, fat, muscled, etc. The vision system was very developed and complex objects can be manipulated by Sammie, based on 21 rigid links with 17 joints. Buford was developed at Rockwell International in Downey, California to find reach and clearance areas around a model positioned by the operator. These early models are described in more detail in the first chapter of the Handbook of Virtual Humans <ref type="bibr" target="#b0">[1]</ref>.</p><p>Until now, most animation systems used a simple articulated body (a skeleton) made of segments and joints to model the motion of the human body. The use of dynamic algorithms improved the fluidity of the motion. However, there was a severe limitation of the use of such techniques.</p><p>They assumed a simple skeleton (see Fig. <ref type="figure" target="#fig_0">1</ref>), but the human skeleton is very different from this structure. As a result, dynamics-based motions were more appropriate for robot-like structures than for human characters. One question arose: should we continue to develop very complex algorithms for controlling the motion of primitive structures that do not correspond to the human skeleton?</p><p>The skeleton determines the general shape of the body. It is made up of bones connected by joints that allow relative motion between different parts of the skeleton. Different classes of human joints can be distinguished according to the type of motion they allow. Hinge and pivot joints allow rotations about a single transverse or longitudinal axis.</p><p>The ball-and-socket joint allows rotational movement in all directions; a bone with a rounded end (ball) fits in a cuplike cavity (socket) of another bone. Ellipsoidal and saddle joints are similar to the ball-and-socket, except that no axial rotation is possible due to the different structure of the articulating ends.</p><p>The revolute joint allows a rotation about an arbitrarily fixed axis. The natural parameterization is the rotation angle with respect to a reference configuration. For example, Aubel and Thalmann <ref type="bibr" target="#b1">[2]</ref> model both hinge and pivot joints with the revolute type. The knee joint performs two successive rotations about two orthogonal axes. It is not equivalent to two independent revolute joints located at the same point, because the twist limits are functionally coupled with the extension. In practice, the range of axial motion becomes more and more restricted as the leg approaches full extension, as in reality.</p><p>The swing joint allows all rotational motions except axial rotation. Both saddle and ellipsoidal joints are therefore modeled by swing joints in our system. In terms of parameterization and limits, a swing joint is strictly equivalent to a ball-and-socket joint whose axial motion is restricted altogether.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Body deformations</head><p>Modeling and deformation of 3-D characters, and especially human bodies, during the animation process is an important but difficult problem. Researchers have devoted significant efforts to the representation and deformation of the human body's shape. Broadly, we can classify their models into two categories: the surface model and the multilayered model. The surface model <ref type="bibr" target="#b2">[3]</ref> is conceptually simple, containing a skeleton and an outer skin layer. The envelope is composed of planar or curved patches. One problem with this model is that it requires the tedious input of the significant points or vertices that define the surface. Another problem is that it is hard to control the realistic evolution of the surface across joints. Surface singularities or anomalies can easily be produced.</p><p>The multilayered model <ref type="bibr" target="#b3">[4]</ref> contains a skeleton layer, intermediate layers that simulate the physical behavior of muscle, bone, fat tissue, etc., and a skin layer. Since the overall appearance of a human body is very much influenced by its internal muscle structures, the multilayered model is the most promising for realistic human animation. The key advantage of the layered methodology is that once the layered character is constructed, only the underlying skeleton needs be scripted for an animation; consistent yet expressive shape deformations are generated automatically. Henne <ref type="bibr" target="#b4">[5]</ref> represents the skin by a mesh of bicubic surface patches, whose control points deform at joints in accordance with several constraints such as elasticity, area preservation, and implicit repulsive force fields that mimic bones and muscles. Deformations that occur away from joints are ignored by this approach.</p><p>The question is still remains as to how to model or capture the multilayered model. Yoshomito <ref type="bibr" target="#b5">[6]</ref> showed that implicit formulations like metaballs provide efficient ways of creating beautiful virtual humans at a reduced storage cost. Thalmann et al. <ref type="bibr" target="#b6">[7]</ref> extended this approach by combining implicit surfaces and B-spline patches. Ellipsoids and ellipsoidal metaballs represent the gross shape of bone, muscle, and fat tissue. The motion/deformation of each primitive with respect to underlying joints is specified via a graphical interface. A skin of fixed topology is extracted by casting rays from the skeleton segments in a star-shaped manner and using the intersection points as control points of B-spline patches.</p><p>More recent work aims at mimicking more closely the actual anatomy of humans or animals. Wilhelms <ref type="bibr" target="#b7">[8]</ref> developed an interactive tool for designing and animating monkeys and cats. In her system, ellipsoids or triangular meshes represent bones and muscle. Each muscle is a generalized cylinder made up of a certain number of cross-sections that consist, in turn, of a certain number of points. The muscles show a relative incompressibility when deformed. In their work on anatomically modeling the human musculature, Scheepers et al. <ref type="bibr" target="#b8">[9]</ref> stressed the role of underlying components (muscles, tendons, etc.) on the form. They use three volume-preserving geometric primitives for three different types of muscles. Isometric contraction is handled by introducing scaling factors and tension parameters. The skin is obtained by fitting bicubic patches to an implicit surface created from the geometric primitives. Porcher-Nedel and Thalmann <ref type="bibr" target="#b9">[10]</ref> introduced the idea of abstracting muscles by an action line (a polyline in practice), representing the force produced by the muscle on the bones, and a surface mesh deformed by an equivalent mass-spring mesh. In order to smooth out mesh discontinuities, they employ special springs, termed angular springs, which tend to restore the initial curvature of the surface at each vertex. However, angular springs cannot deal with local inversions of the curvature. Aubel and Thalmann <ref type="bibr" target="#b10">[11]</ref> also use an action line and a muscle mesh. The action line, represented by a polyline with any number of vertices, is moved for each posture using a predefined behavior and a simple, physically based simulation (see Fig. <ref type="figure" target="#fig_1">2</ref>). It is then used as a skeleton for the surface mesh, and the deformations are produced in the usual way. Seo et al. <ref type="bibr" target="#b11">[12]</ref> propose a very fast method of deformation for MPEG-4-based applications.</p><p>4 Generating on-the-fly flexible motion control 4.1 A need for motion control methods Human motion has always been difficult to simulate with an acceptable level of physical realism and/or believability. This is especially the case when strong update rate requirements are imposed as in interaction context. Results from neurophysiology suggest that many basic human skills could be seen as motion patterns managed at such a low level as the spinal cord while the central brain is only in charge of modulating it according to the current context <ref type="bibr" target="#b12">[13]</ref>. Such findings are very stimulating for real-time human animation, as relatively easy methods for motion capture are now available. Numerous approaches aiming at re-using and generalizing this raw motion material at low cost have been proposed <ref type="bibr" target="#b13">[14]</ref>. Among them, statistical and probabilistic methods aim at encapsulating the structure of the motion implicitly, with only minimal normalization of the data <ref type="bibr" target="#b14">[15]</ref>. Additional classification stages can map the motion database onto some high-level qualitative dimension. At the exploitation stage, specifying a value along that dimension leads to the low cost production of a new motion with the desired level of that qualitative property. The major limitation of this family of approaches comes from its black box nature and the unavoidable coupling between the various high-level qualities desired. Adding new samples to the motion database may improve some aspects while perturbating other dimensions. An alternate computer animation technique is procedural modeling that strives to establish explicit analytic formulation linking the high-level qualitative dimension to the animation parameters (i.e. the skeleton joint values).</p><p>In any case, there is a need for a motion control method behind. It specifies how a virtual human is animated and may be characterized according to the type of information it privileged in animating the character. For example, in a keyframe system for an articulated body, the privileged information to be manipulated is the angle. In a forward dynamics-based system, the privileged information is a set of forces and torques; of course, in solving the dynamic equations, joint angles are also obtained in this system, but we consider these as derived information. In fact, any motion control method will eventually have to deal with geometric information (typically joint angles), but only geometric motion control methods explicitly privilege this information at the level of animation control. Typically, motion is defined in terms of coordinates, angles and other shape characteristics. A physical motion control method uses physical characteristics and laws as a basis for calculating motion. Privileged information for these motion control methods include physical characteristics such as mass, moments of inertia, and stiffness. The physical laws involved are mainly those of mechanics, and more particularly dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retargeted motion versus modeled motion</head><p>To create this flexible virtual humans with individualities, there are mainly two approaches.</p><p>The first approach consists in recording the motion (see Fig. <ref type="figure">3</ref>) using motion capture systems (magnetic or optical), then to try to alterate such a motion to create this individuality. This process is tedious and there is no reliable method at this stage. Even if it is fairly easy to correct Fig. <ref type="figure">3</ref>. Motion capture session one posture by modifying its angular parameters (with an inverse kinematics engine, for instance), it becomes a difficult task to perform this over the whole motion sequence while ensuring that some spatial constraints are respected over a certain time range, and that no discontinuities arise. When one tries to adapt a captured motion to a different character, the constraints are usually violated, leading to problems such as the feet going into the ground or a hand unable to reach an object that the character should grab. The problem of adaptation and adjustment is usually referred to as the motion retargeting problem.</p><p>Witkin and Popovic <ref type="bibr" target="#b15">[16]</ref> proposed a technique for editing motions, by modifying the motion curves through warping functions and produced some of the first interesting results. In a more recent paper <ref type="bibr" target="#b16">[17]</ref>, they have extended their method to handle physical elements, such as mass and gravity, and also described how to animate characters with different numbers of degrees of freedom. Their algorithm is based on the reduction of the character to an abstract character, which is much simpler and only contains the degrees of freedom that are useful for a particular animation. The edition and modification are then computed on this simplified character and mapped again onto the end user skeleton. Bruderlin and Williams <ref type="bibr" target="#b17">[18]</ref> have described some basic facilities to change the animation, by modifying the motion parameter curves. The user can define a particular posture at time t, and the system is then responsible for smoothly blending the motion around t. They also introduced the notion of the motion displacement map, which is an offset added to each motion curve. The motion retargeting problem term was brought up by Michael Gleicher <ref type="bibr" target="#b19">[19]</ref>. He designed a space-time constraints solver, into which every constraint is added, leading to a big optimization problem. He mainly focused on optimizing his solver, to avoid enormous computation time, and achieved very good results. Bindiganavale and Badler <ref type="bibr" target="#b20">[20]</ref> also addressed the motion retargeting problem, introducing new elements: using the zero-crossing of the second derivative to detect significant changes in the motion, visual attention tracking (and the way to handle the gaze direction) and applying inverse kinematics to enforce constraints, by defining six sub-chains (the two arms and legs, the spine and the neck). Finally, Lee and Shin <ref type="bibr" target="#b21">[21]</ref> used in their system a coarse-to-fine hierarchy of B-splines to interpolate the solutions computed by their inverse kinematics solver. They also reduced the complexity of the IK problem by analytically handling the degrees of freedom for the four human limbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Creating computational models</head><p>The second approach consists in creating computational models that are controlled by a few parameters. One of the major problems is to find such models and to compose them to create complex motion. Such models can be created for walking or grasping objects, but also for groups and crowds. Let us take the example of walking, where models have been proposed by many researchers. Walking has global and specific characteristics. From a global point of view, every human walk has comparable joint angle variations. However, at a close-up, we notice that individual walk characteristics are overlaid to the global walking gait.</p><p>For example, Boulic et al. proposed the walking engine described in <ref type="bibr" target="#b22">[22]</ref> which has been extended in the context of a European project on virtual human modeling <ref type="bibr" target="#b23">[23]</ref>. Their contribution consists in integrating the walking engine as a specialized action in the animation framework. Walking is defined as a motion where the center of gravity alternatively balances from the right to the left side. It has the following characteristics.</p><p>• At any time, at least one foot is in contact with the floor, the 'single support' duration (ds). • There exists a short instant during the walk cycle, where both feet are in contact with the floor, the 'double support' duration (dds). • It is a periodic motion that has to be normalized in order to adapt to different anatomies.</p><p>The joint angle variations are synthesized by a set of periodic motions, which we briefly mention here.</p><p>• Sinus functions with varying amplitudes and frequencies for the humanoid's global translations (vertical, lateral and frontal) and the humanoid's pelvic motions (forward/backward, left/right and torsion). • Periodic functions based on control points and interpolating Hermite splines. They are applied to the hip flexion, knee flexion, ankle flexion, chest torsion, shoulder flexion and elbow flexion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generating modeled motion engines</head><p>One promising way is the development of a methodology to generate generic real-time motion. This is the approach developed by Glardon et al. <ref type="bibr" target="#b24">[24]</ref> for the simulation of human locomotion. Any motion engine needs input data on which it will create new motion by interpolation and extrapolation. The first step consists of getting through motion capture a database of walking and running motions at various speed and height jumps (without initial speed). In this approach, the principal component analysis (PCA) method is used to represent the motion capture data in a new, smaller space. As the first PCs (principal components) contain the most variance of the data, an original methodology is used to extract essential parameters of a motion. This method decomposes the PCA in a hierarchical structure of sub-PCA spaces. At each level of the hierarchy, an important parameter (personification, type of motion, speed) of a motion is extracted and a related function is elaborated, allowing not only motion interpolation but also extrapolation. Moreover, effort achieved concerning the possibility not only to qualify a parameter but also to quantify it, improves this method in comparison to other similar works. Generic animation, applicable to any kind of human size, is another important aspect of such research. The method cannot only normalize a generated motion, it may animate humans with a large range of different size. In order to have a complete locomotion engine, transitions between different motions should be handled. The PCA structure offers the capability of performing transition from a specific movement to another one in a very smooth and efficient way. Indeed, as coefficient vectors represent a motion, interpolation between these vectors is sufficient to generate intermediate motion, without computing into the joint angle space. Glardon et al. applied these new concepts on a home-made motion database composed of walking and running motion on a treadmill, at different specific speeds. They developed engine proposing real-time variation of high-level parameters (style, speed, type of motion, human size), generating a smooth animation (see Fig. <ref type="figure" target="#fig_2">4</ref>). Although virtual models of humans continue to improve both in real-time and non-real-time applications, controlling and animating them realistically still remains a difficult task. In nature there exists no motionless character, while in computer animation we often encounter cases where no planned actions, such as waiting for another actor finishing his/her part, is implemented as a stop/frozen animation. We identify many situations where a flexible idle motion generator can help: from synchronization of speech/body animation duration, to dynamic creation of stand still variations in between two active plays. Additionally, as every person has a unique way of moving and standing, an individualized motor control system is required to transfer this property to virtual human motions. From now on, we will call the motions that occur in waiting/idle states between animation clips idle motions. These motions include changing balance because of fatigue, small variations in body posture caused by small muscle contractions or eye blinking. Apart from generat-ing random movements for separate joints, another possibility is to take captured animation files as a basis for generating idle motions. This results in idle motions that affect all joints, but unfortunately, they are very repetitive and inflexible. Secondly, this approach generates transition problems between the animation clips. Figure <ref type="figure" target="#fig_3">5</ref> shows an example. Egges et al. <ref type="bibr" target="#b25">[25]</ref> propose a novel animation approach based on principal component analysis that allows generating two layers of subtle motions: small posture variations and personalized change of balance. Such a motion generator is needed in many cases when one attempts to create an animation sequence out of a set of existing clips. In nature there exists no motionless character, while in computer animation we often encounter cases where no planned actions, such as waiting for another actor finishing his/her part, is implemented as a stop/frozen animation. They identify many situations where a flexible idle motion generator can help: from synchronisation of speech/body animation duration, to dynamic creation of stand still variations in between two active plays. Their approach overcomes the limitations of using a small set of existing clips as a basis for synthesizing idle motions, such as unnatural repetition of movements and difficulties to insert idle motions into an animation without breaking its continuity. A realistic animation is obtained by blending small posture variations with personalized balance shifting animations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generating complex behaviours of virtual humans inside their environments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Behavioural models</head><p>By behavioral animation we refer to the techniques applied to the synthesis of autonomous animation of virtual characters. Autonomy is one of the most priced goals in the field of character animation. We generally consider autonomy as the quality or state of being self-governing. Perception of the elements in the environment is essential, as it gives the agent the awareness of what is changing around it. It is indeed the most important element that one should simulate before going further. Most common perceptions include (but are not limited to) simulated visual and auditory feedback. Adaptation and intelligence then define how the agent is capable of reasoning about what it perceives, especially when unpredictable events happen.</p><p>On the other hand, when predictable elements are showing up again, it is necessary to have a memory capability, so that similar behaviour can be selected again. Lastly, emotion instantaneously adds realism by defining affective relationships between agents.</p><p>The subtle and spontaneous gestures are part of lowlevel behaviors, and they are essential to communicate any message and convey emotions. These ideas are just beginning to be explored by the scientific community, some studies focus on non-human characters, like dogs or other virtual animals <ref type="bibr" target="#b26">[26]</ref>, while others put special attention on human-like facial gestures.</p><p>Continuous animation (low level behavior) is the result of proprioception (this includes motivations, internal "mental" or emotional states, etc.), and reactions to external stimuli.</p><p>The low-level behavioral animation is an open research avenue. Creating virtual characters able to select the action to perform in an automatic way is not new. State of the art autonomous characters and virtual humans are able to perform a variety of tasks, and have some level of autonomy in their decision making, but still they don't look like real people. One of the problems is that the virtual humans do not display the subtle gestures and mannerisms that characterize a real human being: facial expressions, body language, etc.</p><p>Several studies and implementations have been done with the objective of giving a virtual character the ability to perceive its virtual environment and react to it by means of executing adequate tasks <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>, creating the illusion that the synthetic character is alive: it moves by itself and achieves relatively complex tasks. The autonomous virtual human explores an unknown environment constructed on mental models as well as a "cognitive map" based on this exploration. Navigation is carried out in two ways: globally (pre-learned model of the virtual environment, few changes and the search for performance with a path planning algorithm) and locally (direct acquisition of the virtual environment.) A 3D geometrical model in the form of a grid is implemented with the help of an octree combined with the approach proposed by Noser <ref type="bibr" target="#b29">[29]</ref> and Kuffner <ref type="bibr" target="#b30">[30]</ref>. Nevertheless, this approach requires a filtering of irrelevant information from the variable positions of the virtual sensors, Elfes <ref type="bibr" target="#b31">[31]</ref>. A virtual human is situated in a virtual environment (VE) equipped with sensors for vision, audition and touch, informing it of the external VE and its internal state. A virtual human possesses effectors, which allow it to exert an influence on the VE and a control architecture, which coordinates its perceptions and actions. The behaviour of a virtual human can be qualified as being adaptive as long as the control architecture allows it to maintain its variables in their validity zone. Learning methodologies can modify the organization of the control architecture. We will then integrate them in the further pursuit of our research. Conde and Thalmann propose an artificial life environment framework equips a virtual human with the main virtual sensors, based on an original approach inspired by neuroscience in the form of a small nervous system with a simplified control architecture to optimise the management of its virtual sensors as well as the virtual perception part. The processes of filtering, selection and simplification are carried out after obtaining the sensorial information; this approach allows us to obtain some persistence in the form of a "cognitive map" and also serves as a pre-learning framework to activate learning methods of low and high level concerning the behaviour of a virtual human.</p><p>The objective pursued is to allow the virtual human to explore virtual environments until then unknown and to construct mental structures and models, cognitive maps or plans from this exploration. Once its representation has been constructed, this knowledge can be passed on to other virtual humans. The autonomous virtual human perceives objects and other virtual humans with the help of its virtual environment, which provides the information concerning the nature and position of the latter. The behavioural model to decide which actions the Virtual human should take such as walking, handling an object, etc., and then uses this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Perception and sensors</head><p>The Conde and Thalmann <ref type="bibr" target="#b32">[32]</ref> approach is based on the multi-sensory integration of the standard theory of neuroscience, where signals of a single object coming from distinct sensory systems are combined. The acquisition steps of signals, filtering, selection and simplification intervening before proprioception, active and predictive perception are integrated into virtual sensors and a virtual environment. We will focus on two aspects: 1) the assignment problem: determining which sensory stimuli belong to the same virtual object, and 2) the sensory recoding problem: recoding signals in a common format before combining them. We have developed three novel methodologies to map the information coming from the virtual sensors of vision, audition and touch, as well as that of the virtual environment in the form of a "cognitive map".</p><p>An autonomous virtual agent (AVA) situated in a virtual environment (VE) is equipped with sensors for vision, audition and touch that inform it of the external VE and its internal state. An AVA possesses effectors to let it exert an influence on the VE and control architecture to coordinate its perceptions and actions. The behaviour of an AVA is adaptive as long as the control architecture allows it to maintain its variables in their validity zone.</p><p>Artificial life environment (ALifeE) based on an original approach inspired by neuroscience equips an AVA with the main virtual sensors in the form of a small nervous system <ref type="bibr" target="#b0">[1]</ref>. The control architecture is kept simple to optimise the management of the AVA's virtual sensors and perception. The processes of filtering, selection and simplification are carried out after obtaining the sensorial information. This approach allows us to achieve some persistence in the form of a "cognitive map".</p><p>The "mental processes" of an AVA can be simulated. Behavioural animation includes the techniques applied to make an AVA intelligent and autonomous, to react to its VE and to make decisions based on its perceptive system, its short-term memory and long-term reasoning. Intelligence is the ability to plan and carry out the tasks based on the model of the current state of the VE.</p><p>Our objective is to permit the AVA to explore unknown VEs and to construct mental structures and models, cognitive maps or plans from this exploration. Once its representation has been created, the knowledge can be communicated to other AVAs. Each AVA perceives objects and other AVAs with the help of its VE, which provides information concerning their nature and positions. The behavioural model decides which action the AVA should take (such as walking or handling an object) and then uses the knowledge.</p><p>In a VE, an AVA requires a combination of perception and action to behave in an autonomous way. The perception system provides a uniform interface to various techniques in the field of virtual perception, including synthetic vision, audition and touch. In usual approaches, different behaviours implement their own perception mechanisms, which lead to computation duplication when multiple behaviours are involved. Basically, an AVA maintains a set of perception pipelines, each corresponding to a particular type of virtual sensor. A pipeline is composed of filters that coordinate themselves in order to extract relevant information from the data sent by the associated virtual sensor.</p><p>Figure <ref type="figure">6</ref> shows a schematic representation of such an artificial life organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">A case study: interacting with objects</head><p>One of the challenge in simulating human behaviour is to correctly model interactions between an object and a virtual human agent (hereafter just referred to as an agent), it appears in most applications of computer animation and simulation. Such applications encompass several domains, for example, virtual autonomous agents living and working in virtual environments, human factors analysis, training, education, virtual prototyping, and simulation-based Fig. <ref type="figure">6</ref>. A schematic representation of our ALifeE. Virtual vision discovers the VE, constructs the different types of perception and updates the AVA's cognitive map to obtain a multi-perceptive mapping. Then the control architecture uses both the "cognitive maps" and the "memory model" to interact with the learning, development, and control processes of the AVA (virtual human controller) design. A good overview of such areas is presented by Badler <ref type="bibr" target="#b33">[33]</ref>. An example of an application using agentobject interactions is presented by Johnson et al. <ref type="bibr" target="#b34">[34]</ref>, whose purpose is to train equipment usage in a populated virtual environment.</p><p>Commonly, simulation systems perform agent-object interactions for specific tasks. Such an approach is simple and direct, but most of the time, the core of the system needs to be updated whenever one needs to consider another class of objects.</p><p>To overcome such difficulties, a natural way is to include within the object description, more useful information than only intrinsic object properties. Some proposed systems already use this kind of approach. In particular, the object specific reasoning <ref type="bibr" target="#b35">[35]</ref> creates a relational table to inform object purpose and, for each object graspable site, the appropriate hand shape and grasp approach direction. This set of information may be sufficient to perform a grasping task, but more information is needed to perform different types of interactions.</p><p>Another interesting way is to model general agentobject interactions based on objects containing interaction information of various kinds: intrinsic object properties, information on how to interact with it, object behaviors, and also expected agent behaviors. The smart object approach, introduced by Kallmann and Thalmann <ref type="bibr" target="#b36">[36]</ref> extends the idea of having a database of interaction information. For each object modeled, we include the functionality of its moving parts and detailed commands describing each desired interaction, by means of a dedicated script language. A feature modeling approach <ref type="bibr" target="#b37">[37]</ref> is used to include all desired information in objects. A graphical interface program permits the user to interactively specify different features in the object, and save them as a script file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Creating realistic groups and crowds of virtual people</head><p>Another challenge in virtual humans is the simulation of larger numbers of virtual human agents for interactive virtual environments such as virtual reality training systems <ref type="bibr" target="#b38">[38]</ref>. In recent years, virtual crowds have become a more and more common element of our cinematic experience. Whether it was a photo-realistic crowd of digital passengers in the Titanic, a legion of animated ants in AntZ, or a rendered army of droids in Star Wars, computer graphics (CG) generated crowds of characters added significantly to the impact of the movies employing them, allowing us to visualize scenes not possible only some years earlier. Crowds of CG human and non-human characters help to overcome prohibiting costs and complexities of working with large numbers of extras, stand as virtual stuntmen in dangerous shots and integrate well with the virtual scenes. Several works in different fields have been exploring issues connected to the domain of crowd simulations. In his pioneer work, Reynolds <ref type="bibr" target="#b39">[39]</ref> described a distributed behavioral model for simulating aggregate motion of a flock of birds. Bouvier and Guilloteau <ref type="bibr" target="#b40">[40]</ref> used a combination of particle systems and transition networks to model human crowds in visualization of urban spaces. Brogan and Hodgins <ref type="bibr" target="#b42">[41]</ref> simulated group behaviors for systems with significant dynamics. Aubel and Thalmann <ref type="bibr" target="#b43">[42]</ref> introduced dynamically generated impostors to render virtual humans. Tecchia et al. <ref type="bibr" target="#b44">[43]</ref> proposed an image-based method for real-time rendering of animated crowds in virtual cities. McPhail et al. <ref type="bibr" target="#b45">[44]</ref> studied individual and collective actions in temporary gatherings. Still <ref type="bibr" target="#b46">[45]</ref> used mobile cellular automata for simulation and analysis of crowd evacuations. However, only a few works tried to explore more general crowd models integrating several sub-components, such as collision avoidance, path-planning, higher-level behaviors, interaction or rendering.</p><p>With current consumer-grade personal computers it is possible to display 3D virtual scenes with thousands of animated individual entities at interactive frame rates using different techniques as animated impostors <ref type="bibr" target="#b47">[46]</ref>. When increasing the number of involved individuals it is becoming more difficult to create unique and varied content of scenarios with large numbers of entities. If we want to create or modify features of every single individual one by one, it will soon become too laborious. If, on the other hand, we apply a set of features (either uniform, or patterned) to many individuals at once, it could create unwanted ar-tifacts on a larger scale, resulting in an "army-like" appearance with too uniform, or periodic distributions of individuals or characteristics.</p><p>The use of random distributions can alleviate such problems; however, it can be very difficult to capture the desired constraints into a set of mathematical equations, especially considering integration into common art production pipelines.</p><p>The challenge is how to create complex scenes resembling a variety-rich look of the real world. Bottom-up approaches, such as local rule based flocking <ref type="bibr" target="#b39">[39]</ref> can create such complexity, however they are difficult to control if we want to achieve particular end configurations (how to set local rules to get a global result). In recent work, Anderson et al. <ref type="bibr" target="#b48">[47]</ref> achieved interesting results for a particular case of flocking animation. Nevertheless, the algorithm can get very costly when increasing the number of entities and simulation time.</p><p>Major 3D content creation packages used by the media industry now offer tools to improve working with a large number of virtual characters. The production of massively populated scenes is still in the majority of cases a lengthy and manual-intervention intensive process, operating mostly in a non-real-time mode.</p><p>In previous works, crowds have been considered as already formed units with more or less uniform behavior placed in particular environments corresponding only to the limited purpose of the simulation, e.g. pedestrians just fleeing from burning building <ref type="bibr" target="#b49">[48]</ref> or a marching crowd during a demonstration, the crowd assigning people to particular groups and determining their behaviors. Collective behavior emerges from interaction of individuals, crowds are dynamically assembled and disassembled and over the time they change their behavior. Musse and Thalmann take inspiration from the field of sociology. McPhail <ref type="bibr" target="#b50">[49]</ref> argues that members of a gathering do not act as a whole, but rather that subsections of the number assembled react in similar way, which, when observed externally appears as a collective behavior. In our system, a crowd is modeled as collection of individuals, which react to the environment, other agents and real human participants of the simulation and can have very different behaviors both for one agent in different situations and for many agents in the same situation.</p><p>Ulicny and Thalmann <ref type="bibr" target="#b51">[50]</ref> implemented our emergent crowd system to reproduce simple simulation consists of group of autonomous virtual human agents existing in dynamic virtual 3D environment. In order to behave in believable way, these agents have to act in accordance with their surrounding environment, be able to react to its changes, to the other agents and also to the actions of real humans interacting with the virtual world. Agents contain a set of internal attributes corresponding to various psychological or physiological states (e.g. level of curiosity, tiredness, etc.), a set of higher-level complex behaviors and a set of rules determining selection of these behav-iors. Events provide a way of agents' interaction with their environment, other agents or human participants of the simulation. Each agent is able to receive events from the environment objects, other agents or user interface. Combinations of different received events and different levels of agent's attributes produce both changes of its internal attributes and change of the overt behavior.</p><p>The behavior model is based on a combination of rules <ref type="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b54">52]</ref> and finite state machines <ref type="bibr" target="#b55">[53]</ref> for controlling the agent's behavior using a layered approach. The first layer deals with the selection of higher-level complex behavior appropriate to the agent's situation, the second layer implements these behaviors using low-level actions provided by the virtual human <ref type="bibr" target="#b56">[54]</ref>. At the higher level, rules select complex behaviors (such as fleeing) according to the agent's state (constituted by attributes) and the state of the virtual environment (conveyed by events). In rules we specify for who (e.g. which particular agent, or agents in a particular group) and when the rule is applicable (e.g. at defined time, after receiving event or when some attribute reached specified value), and what is the consequence of rule firing (e.g. change of agent's highlevel behavior or attribute). An example of such rule is:</p><formula xml:id="formula_0">FOR ALL WHEN EVENT = in_danger_area AND ATTRIBUTE fear &gt; 50% THEN BEHAVIOR FLEE</formula><p>The variety of the reactions to the same situation is achieved by different agents having different values of the attributes (at the beginning through different initializations, later because of their different histories), which consequently leads to different rules being triggered.</p><p>At the middle level, high-level behaviors are implemented using hierarchical finite state machines. Each behavior is realized by one FSM that drives selection of the low-level actions for virtual human (like movement of location, playing a short animation sequence), manages connections with the environment (like path queries, or event sending) and also can call other FSMs to delegate subtasks such as path following. There are two types of high-level behaviors. First, we can specify scripted behavior which is more precise, but less autonomous and with less environment coupling by using explicit sequences of low-level actions. Or second, we can let agents perform autonomously complex behaviors with feedback from the environment. Examples of such autonomous behaviors are wandering, fleeing, neutralizing the threat, or requesting and providing help. Both types can be mixed as needed. Figure <ref type="figure" target="#fig_4">7</ref> shows an example.</p><p>At the lowest level, motion control is also organized into layers. As the result of higher-level behavior, agent's behavior FSM decides (or is told directly by the rule) that the agent wants to move to particular location. This request is forwarded to the path-finding layer, which constructs sequence of waypoints that need to be passed to get to the location. Finally, it is the responsibility of the Ulicny et al. <ref type="bibr" target="#b57">[55]</ref> propose an approach to give full creative power to designers using metaphors of artistic tools, operating on a two-dimensional canvas familiar from image manipulation programs working in WYSI-WYG (what you see is what you get) mode, with a realtime view of the authored scene. The advantages of immediate feedback, intuitive interface and familiarity allows us to better express the artist's vision and can also lead to an increase in productivity. The goal is to create a system that would allow authoring of freely navigable real-time 3D scenes, composed of a large number of varied animated individuals in a virtual environment. The authoring should be simple and intuitive, usable by non-programmers. They take inspiration from the domain of image and word processing, where most of the applications use WYSIWYG approaches with immediate feedback of the resulting manipulations. The idea is simple: the designer manipulates virtual tools, working in a two-dimensional screen space, with a mouse and a keyboard. These tools then affect the corresponding objects in a three-dimensional world space (see Fig. <ref type="figure" target="#fig_0">1</ref>). Different tools have different visualizations and perform different effects on the scene including creation and deletion of crowd members, changing of their appearances, triggering of various animations, setting of higher-level behavioral parameters, setting waypoints for displacement of the crowd, or sending of events to the behavior subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Believable interaction between real and virtual humans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Facial communication</head><p>In 1971, Parke produced a representation of the head and face at the University of Utah, and three years later ad-vances in good enough parametric models to produce a much more realistic face <ref type="bibr" target="#b58">[56]</ref>. Nowadays, a lot of interest from both industry and research exists for virtual environments (VEs) and embodied conversational agents (ECAs). A lot of new techniques are being developed to improve the simulation in general, to add more visual detail and make the interaction between human and VE/ECA more natural. Believability is a measure to help to determine how well these different techniques are working. Believability represents the 'outsider' point of view. It is thus a very powerful evaluation tool, since we can use it to make evaluations of different techniques/methods while the evaluations are independent from the underlying techniques. This allows us to compare different approaches and give a meaningful indication of their quality on the level of believability. In this section, we will focus especially on believability and ECAs. Since ECAs are generally modeled after humans (even cartoon characters), one important aspect of their believability is how well an ECA succeeds in acting like a human.</p><p>We believe that the key to believable ECAs is the definition of their personality and their emotions. Although quite some research has been done to describe the influence of emotion and personality on ECAs, the results until now are not very convincing. We see several reasons for this.</p><p>-Psychological models of emotion/personality are not finalized. The exact structure of our emotions/personality is not certain, nor is the way in which emotions and personality interact with our perception, behaviour and expression. -When one wants to simulate emotions/personality computationally, one tends to take the model the most suitable for a computational simulation. However, this model is not necessarily the best representation for emotions/personality. -Even if there exists a perfect emotion/personality model, it is very difficult to distinguish the resulting behaviour from the emotion that is behind it. Also, other issues interfere with our impression of how emotional an ECA really is, such as its appearance, the surroundings, its capabilities, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Emotion, personality and perception</head><p>In emotion simulation research so far, appraisal is popularly done by a system based on the OCC model <ref type="bibr" target="#b59">[57]</ref>. This model specifies how events, agents and objects from the universe are used to elicit an emotional response depending on a set of parameters: the goals, standards and attitudes of the subject. Since the emotional response is generated from a cognitive point of view, this type of appraisal is called cognitive appraisal and it corresponds closely with Lazarus' emotion theory (not taking into account the physiological response). When one wants to develop a computational model of appraisal, not all of the above mentioned scenarios are suitable to take as a basis, especially those scenarios where arousal plays a crucial role in the determination of the emotional response (ECAs do not yet have a physiology). This raises the question if it is possible to develop a computational model of appraisal that has a high believability. On the level of personality, one could consider the goals, standards and attitudes of the OCC model as a domain dependent 'personality'. However, personality can also be modeled in a more abstract, domain-independent way <ref type="bibr" target="#b60">[58,</ref><ref type="bibr" target="#b61">59]</ref>. Egges et al. <ref type="bibr" target="#b62">[60]</ref> discuss how a link between multidimensional personality models and the OCC appraisal model can be established.</p><p>The effect of personality and emotion on agent behavior has been researched quite extensively, whether it concerns a general influence on behavior <ref type="bibr" target="#b63">[61]</ref>, or a more traditional planning based method <ref type="bibr" target="#b64">[62]</ref>. Also, rule-based models <ref type="bibr" target="#b65">[63]</ref>, probabilistic models <ref type="bibr" target="#b66">[64,</ref><ref type="bibr" target="#b67">65]</ref> and fuzzy logic systems <ref type="bibr" target="#b68">[66]</ref> have been developed. In the case of real human beings, there are still many questions regarding how emotion influences our behavior, but in the field of Neuroscience, work has been done that partly describes the relationship between emotions and the brain <ref type="bibr" target="#b69">[67,</ref><ref type="bibr" target="#b70">68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Emotion, personality and expression</head><p>The expression of emotions (see Fig. <ref type="figure" target="#fig_5">8</ref>) has been widely researched, and the most well known research is the work done by Ekman <ref type="bibr" target="#b71">[69]</ref>. Not only will personality and emotion have an effect on expressions by the face or body; also physiological changes can be measured according to different emotions. Furthermore, emotions and personality have an important effect on speech <ref type="bibr" target="#b72">[70]</ref>. In the following two sections, we will concentrate on the relationship between emotions, personality and face/body animation. Also, we will give some examples on how to improve the believability of an ECA using emotions and personality.</p><p>When communicating with an ECA, the dialogue itself is only a small part of the interaction that is actually going on. In order to simulate human behavior, all the non-verbal elements of interaction should be taken into account. An ECA can be defined by the following parts.</p><p>• Appearance (face model, age, race, etc.).</p><p>• Behaviour (choice of non-verbal behaviour, accompanying speech). • Expressiveness of movement (amplitude, tempo, etc.).</p><p>Other information can also be important, like the cultural background or the context. Facial animation synchronized with speech can be improved by different factors such as nonverbal actions, speech intonation, facial expression consistent with the speech and the context, and also facial expressions between speech sequences. All this information helps to increase the believability of facial animation for ECAs. The main problem is to determine when and how this kind of non-verbal behavior should be expressed. Finally, one of the most important points for increasing believability of facial and body animation is the synchronization between verbal and non-verbal expressions <ref type="bibr" target="#b74">[71,</ref><ref type="bibr" target="#b75">72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Generating realistic virtual clothed and haired people</head><p>Even when virtual humans have nice bodies and faces, and behave realistically, they still need a realistic appearance in terms of hair, skin, and clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Cloth animation</head><p>Dressing a virtual body involves designing complex garment shapes, as well as an advanced simulation system able to detect and to handle multiple collisions generated between the cloth and the body. Very accurate methods are used when simulating complex garments for haute couture for instance, whereas less accurate but interactive ones are used for design and manipulation.</p><p>Along with the evolution of cloth simulation techniques, focus was primarily aimed to address realism through the accurate reproduction of the mechanical features of fabric materials. The early models, developed a decade ago, had to accommodate very limited computational power and display device, and therefore were geometrical models that were only meant to reproduce the geometrical features of deforming cloth <ref type="bibr" target="#b76">[73]</ref>. Then, real mechanical simulation took over, with accurate cloth models simulating the main mechanical properties of fabric. While some models, mostly intended for computer graphics, aimed at simulating the complex garments used for dressing virtual characters, <ref type="bibr" target="#b77">[74]</ref>, <ref type="bibr" target="#b78">[75]</ref>, other studies focused on the accurate reproduction of mechanical behavior, using particle systems <ref type="bibr" target="#b79">[76]</ref> or finite elements <ref type="bibr" target="#b80">[77]</ref>. Despite all these developments, all these techniques remain dependent on high computational requirements, limiting their application along the new trends toward highly interactive and real-time applications brought by the highly spreading multimedia technologies. While highly accurate methods such as finite elements are not suitable for such applications, developments are now focusing toward approximate models, that can render, using minimal computation, approximate, but realistic results in a very robust way.</p><p>The most popular way for modeling the mechanics of clothes are spring-mass particle systems. They are composed of 3D punctual masses, connected to their neighbors by springs. The mechanical behaviors of clothes are simulated by computing forces applied on each mass given their position and velocity. There are different ways to complete the numerical integration.</p><p>Explicit integration computes the next state of clothes at time t + h by evaluating the forces at time t such as the Runge-Kutta family methods. Explicit integrations are fast methods to compute (for first-order ones), but they show instability for large time-steps. Another family is implicit integration or backward integration: it uses the predictive derivative for the next timestep and then deduces the state of clothes for the current one. This method allows a larger timestep, while conserving numerical stability <ref type="bibr" target="#b81">[78]</ref>. Obviously, despite their stability, implicit methods are not necessarily accurate, particularly for large timesteps. Actually, many dynamical effects may disappear in these approximations such as animated wrinkles. In the case of explicit methods, accurate computation of the motion of every particle requires a lot of computational resources, but the result will be very accurate through the use of high-order methods.</p><p>There are hybrid solutions, which take advantage of both integration methods: stiff elements are handled with the time-consuming implicit method, while the others are integrated explicitly <ref type="bibr" target="#b82">[79]</ref>. The fashionizer platform approach <ref type="bibr" target="#b84">[80]</ref> combines the flexibility obtained in <ref type="bibr" target="#b85">[81]</ref> with simulation speeds aimed in <ref type="bibr" target="#b86">[82]</ref>, which are restricted to regular meshes. Furthermore, a suited implicit integration method has been associated to this model to maximize simulation timesteps and computation speeds without trading away mechanical stability.</p><p>Another family of non-physically based methods tries to give the look of real clothes without the expensive cost of physical ones. These methods are well-suited for realtime issues. Kang et al. <ref type="bibr" target="#b87">[83]</ref> improved the visual quality of very low detailed (in terms of polygons) garments by tessellating the polygons with cubic spline curves and succeeded in simulating wrinkles (see Fig. <ref type="figure">9</ref>). Unfortunately, it exhibits undesirable results with a high number of poly-Fig. <ref type="figure">9</ref>. Simulation using spline curves <ref type="bibr" target="#b87">[83]</ref> and real-time simulation of fabrics by <ref type="bibr" target="#b92">[87]</ref> gons that is essential for simulating complex garments. Another interesting approach tries to take advantage of a bump or displacement map technique coupled to a particle system for generating global movement to simulate fast the cloth. However, it requires a lot of designer interaction <ref type="bibr" target="#b88">[84]</ref>.</p><p>Pure geometric methods aim at providing faster results, using geometric primitives such as catenary's curves or surfaces (Fig. <ref type="figure" target="#fig_3">5</ref>), but they are not appropriate for cloth movement reproduction <ref type="bibr" target="#b89">[85]</ref>. Statistical-based approaches have been introduced very recently and show promising results in a wide range of uses. The main idea is to construct a simulator that will learn cloth deformations through a set of examples, to increase the speed of the computation <ref type="bibr" target="#b91">[86,</ref><ref type="bibr" target="#b92">87]</ref>. A very interesting method <ref type="bibr" target="#b93">[88,</ref><ref type="bibr" target="#b94">89]</ref> achieves real-time simulation. It is based on a segmentation of clothes into three areas and a hybrid simulation using both data-driven and physically-based simulation and was used in the LIFEPLUS project <ref type="bibr" target="#b95">[90]</ref> (see Fig. <ref type="figure" target="#fig_7">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Hair animation</head><p>One of the many challenges in simulating believable virtual humans and animals has been to produce realistic looking hair. Hair simulation can be thought of as having three different subtasks: shape modeling, dynamics and rendering. We can even classify the hair modeling attempts based on their underlying models, namely explicit hair models, cluster hair models and volumetric textures. Hair rendering and shape modeling of fur-like short hair is becoming increasingly available to animators. However, shape modeling and dynamics of long hair has been difficult. The difficulties stem from the shear number of hair strands, their geometric intricacies and associated complex physical interactions.</p><p>Explicit models are tedious for shape modeling and numerically intensive for dynamics, however they are intuitive and close to reality. They are especially suitable for the dynamics of long hair. Daldegan et al.   <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> control the position and orientation of hair strand. As their dynamic models were for straight hair, they could not be applied in animating hairstyles. Anjyo et al.</p><p>[93] modeled hair with a simplified cantilever beam and used a one-dimensional projective differential equation of angular momentum to animate hair. Although the algorithm dealt with both hairstyling and hair animation, it had several limitations; the original hairstyle could not be recovered after subsequent animation and the movement of the head was not accounted for. Further, the approximation in collision detection could generate unnatural results when the algorithm was used to animate long hair. To reduce computations, Daldegan et al.</p><p>[91] used a wisp model. Recently, Lee et al. <ref type="bibr">[94]</ref> developed on Anjyo's work to add some details to model hairstyles. Appreciably, none of the previous attempts considered hair-hair and hair-air interactions. Even individual hair dynamics was grossly approximated to suit available computational power. However, in recent years the computing power has grown considerably. Supercomputing power of the past is becoming increasingly available to animator's workstations.</p><p>Hair-hair interaction is probably the most difficult problem in achieving visually pleasing hair dynamics. So far, no good models have been developed in this regard. Though there are many advances in collision detection and response [95], they are simply unsuitable for the problem at hand, because of shear number complexity of hair. <ref type="bibr">Hadap et al. [96]</ref> take a radical approach by considering hair as a continuum. The continuum assumption states that the physical properties of a medium such as pressure, density and temperature are defined at each and every point in the specified region. Fluid dynamics regards liquids and gasses as a continuum and even elastic theory regards solids as such, ignoring the fact that they are still composed of individual molecules. Indeed, the assumption is quite realistic at a certain length scale of the observation, but at smaller length scales the assumption may not be reasonable. One might argue that hair-hair spacing is not at all comparable to inter molecular distances to consider hair as a continuum. However, individual hair-hair interaction is of no interest to us apart from its end effect. Hence, we treat the size of individual hair and hair-hair distance as being much smaller than the overall volume of hair, justifying the continuum assumption. As we develop the model further, it will be apparent that the above assumption is not just about approximat-Fig. <ref type="figure" target="#fig_0">11</ref>. Various hairstyles animated by our real-time simulation system ing the complex hair-hair interaction. An individual hair is surrounded by air, as it moves it generates a boundary layer of air. This influences many other hair strands in motion. This aerodynamic form of friction is comparable to hair-hair contact friction. In addition, there are electrostatic forces to take part in the dynamics. It is not feasible to model these complex multiple forms of interactions. This inspires us to consider the dynamics of single hair interacting with other surrounding hairs in a global manner through the continuum assumption. That way, we hope to have a sound model for an otherwise very complex phenomenon. Figure <ref type="figure" target="#fig_0">11</ref> shows examples.</p><p>As we start considering hair as a continuum, we look at the properties of such a medium, namely the hair medium. There are two possibilities: hair medium could be considered as a solid or a liquid, depending on how it behaves under shearing forces. Under shearing stresses, solids deform till they generate counter stresses. If the shearing stresses are removed, the solids exhibit the ability of retaining their original shape. The liquids are not able to withstand any shearing stresses. Under the influence of the shearing stresses they continue to deform indefinitely and they do not have any shape memory. In the case of hair, if we apply a lateral shearing motion it acts like a liquid. At the same time, length wise, it acts as a solid. Thus there is a duality in the behaviour of hair as a continuum.</p><p>However, from an animation point of view, we cannot treat hair solely as a continuum, unless the viewpoint is far enough and individual hair movement is not perceived. Thus, we have to retain the individual character of hair as well, while considering hair as a continuum. We split hair dynamics into two parts.</p><p>• Hair-hair, hair-body and hair-air interactions, which are modeled using continuum dynamics, and more precisely fluid dynamics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Should virtual humans become complex like real humans? This is an important question. In fact, this is contextdependent. As a guide to find information on the web: we just want an efficient agent. However, we are not interested in dealing with a virtual human that has problem with his children or complains about his/her headaches. However, in a simulation of behavior in the case of emergency (see Fig. <ref type="figure" target="#fig_8">12</ref>), we want believable humans: panic, stress, and leadership are the key issue. It is essential to take into account psychological aspects. Finally, the goal of the research effort in virtual humans is to develop believable autonomous virtual humans in virtual environments. In this context, believability means that the behavior of a virtual human has to be indistinguishable from that of a real human. For instance, an actor inhabiting a typical virtual environment composed of objects and other actors, only concerned with performing a given task without reacting to objects or actors it encounters or to what these actors are doing would not look believable. The behavior of these actors would be more that of a robot than that of a human. To be believable, an actor has to be affected by what takes place around it and needs to engage in social behaviors with other actors. Therefore, the behavioral model of an actor needs to be versatile enough to allow a change of behavior, the emotional state of the actor must be reflected and must affect its behavior, the interpersonal relationships with the other actors must be taken into account and possibly bring the actor to engage in social interactions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A very simple skeleton is shown on the left and the HANIM hierarchy on the right</figDesc><graphic coords="3,52.46,460.92,311.94,235.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Action lines of the pectoral muscle during shoulder abduction. Right: the use of two attractive force fields (solid and wireframe ellipsoids)</figDesc><graphic coords="5,52.46,65.39,311.47,96.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Postures at identical moment in the walk cycle, with two different skeleton sizes</figDesc><graphic coords="7,50.30,304.91,240.56,106.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Virtual humans waiting at a tram stop in Geneva</figDesc><graphic coords="7,303.98,156.82,241.18,170.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Real-time crowd at a dance party collision-avoidance layer to move agent between waypoints correcting trajectory in order to avoid collisions.Ulicny et al.<ref type="bibr" target="#b57">[55]</ref> propose an approach to give full creative power to designers using metaphors of artistic tools, operating on a two-dimensional canvas familiar from image manipulation programs working in WYSI-WYG (what you see is what you get) mode, with a realtime view of the authored scene. The advantages of immediate feedback, intuitive interface and familiarity allows us to better express the artist's vision and can also lead to an increase in productivity. The goal is to create a system that would allow authoring of freely navigable real-time 3D scenes, composed of a large number of varied animated individuals in a virtual environment. The authoring should be simple and intuitive, usable by non-programmers. They take inspiration from the domain of image and word processing, where most of the applications use WYSIWYG approaches with immediate feedback of the resulting manipulations. The idea is simple: the designer manipulates virtual tools, working in a two-dimensional screen space, with a mouse and a keyboard. These tools then affect the corresponding objects in a three-dimensional world space (see Fig.1). Different tools have different visualizations and perform different effects on the scene including creation and deletion of crowd members, changing of their appearances, triggering of various animations, setting of higher-level behavioral parameters, setting waypoints for displacement of the crowd, or sending of events to the behavior subsystem.</figDesc><graphic coords="11,312.50,65.92,224.17,167.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Various expressions for the virtual human</figDesc><graphic coords="13,51.74,65.76,238.55,222.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[91] and Rosenblum et al. [92] used a mass-spring-hinge model to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Real-time simulation of a wearing cloth by<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> </figDesc><graphic coords="14,326.66,248.67,195.32,295.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Crowd in virtual park: a) before emergency, b) after gas leak</figDesc><graphic coords="16,52.46,65.79,283.14,212.54" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<title level="m">Handbook of Virtual Humans</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MuscleBuilder: a modeling tool for human anatomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Sci Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="595" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Direction of Synthetic Actors in the film Rendez-vous à Montreal</title>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9" to="19" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Layered construction for deformable animated characters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;89 Proceedings)</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A constraint-based skin model for human figure animation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-06">June (1990</date>
			<pubPlace>Santa Cruz</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ballerinas generated by a personal computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshimito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Visualizat Comput Animation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="85" to="90" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast realistic human body deformations for animation and vr applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chauvineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics International&apos;96</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Pohang; Korea</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Animals with anatomy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilhelms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput Graph Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anatomy-based modeling of the human musculature</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scheeppers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;97 Pro-ceedings)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anatomic modeling of deformable human bodies</title>
		<author>
			<persName><forename type="first">L</forename><surname>Porcher Nedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="306" to="321" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive modeling of the human musculature</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Animation</title>
		<meeting>Computer Animation<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive modeling of mpeg-4 deformable human body models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cordier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Philippon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Deform&apos;2000, Workshop on Virtual Humans by IFIP Working Group</title>
		<meeting>Deform&apos;2000, Workshop on Virtual Humans by IFIP Working Group<address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
		<respStmt>
			<orgName>Computer Graphics and Virtual Worlds</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Brain&apos;s Sense of Movement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berthoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBN</title>
		<imprint>
			<biblScope unit="volume">067</biblScope>
			<biblScope unit="page" from="400" to="9800" />
			<date type="published" when="2002">2002</date>
			<publisher>Harward University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Verbs and adverbs: multidimensional motion interpolation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bodenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput Graph Appl</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<idno>TR-2000-14 MERL</idno>
		<title level="m">Style machines. Proc. of SIGGRAPH&apos;00</title>
		<meeting><address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<title level="m">Motion warping. Proceedings of SIGGRAPH 95</title>
		<meeting><address><addrLine>Los Angeles; Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-08">August 1995. 1995</date>
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Physically based motion transformation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 99</title>
		<meeting>SIGGRAPH 99<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motion signal processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruderlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Cook</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m">Conference Proceedings, Annual Conference Series, held in Los Angeles</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08">August. 1995</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retargeting motion to new characters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Con-ference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998-07">July 1998. 1998</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling and Motion Capture Techniques for Virtual Environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bindiganavale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg New York</pubPlace>
		</imprint>
	</monogr>
	<note>Motion abstraction and mapping with spatial constraints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jehee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 99, held in Los Angeles</title>
		<meeting>SIGGRAPH 99, held in Los Angeles</meeting>
		<imprint>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A global human walking model with real-time kinematics personification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="344" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The HUMANOID environment for interactive animation of multiple deformable human characters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Capin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moccozet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Molet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lintermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics &apos;95</title>
		<meeting>Eurographics &apos;95<address><addrLine>Maastricht</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-08">August 1995. 1995</date>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PCA-based walking engine using motion capture data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Glardon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Graphics International</title>
		<meeting>Computer Graphics International</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="page" from="292" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Example-based idle motion synthesis in a real-time application</title>
		<author>
			<persName><forename type="first">A</forename><surname>Egges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-12">December (2004</date>
			<publisher>CAPTECH Workshop</publisher>
			<biblScope unit="page" from="13" to="19" />
			<pubPlace>Zermatt, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object persistence for synthetic creatures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Isla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS)</title>
		<meeting>International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July (2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-level direction of autonomous creatures for real-time virtual environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Galyean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 95</title>
		<meeting>SIGGRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Artificial fishes: physics, locomotion, perception, behavior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;94</title>
		<meeting>SIGGRAPH&apos;94</meeting>
		<imprint>
			<date type="published" when="1994-07">July. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Navigation for digital actors based on synthetic vision, memory and learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Renault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast synthetic vision, memory, and learning models for virtual humans</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Kuffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Latombre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Animation, IEEE</title>
		<meeting>Computer Animation, IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occupancy grid: a stochastic spatial representation for active robot perception. 6th Conf</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elfes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Uncertainty in AI</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An artificial life environment for autonomous virtual agents with multi-sensorial and multi-perceptive features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Anim Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Virtual humans for animation, ergonomics, and simulation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Badler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-06">June (1997</date>
			<pubPlace>Motion, Puerto Rico</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Steve: an animated pedagogical agent for procedural training in virtual environments</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sigart Bull</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Connecting planning and acting via object-specific reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Levison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer &amp; Information Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A behavioral interface to simulate agent-object interactions in real-time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kallmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Computer Animation</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<date type="published" when="1991">1991</date>
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Parametric and feature-based CAD/CAM</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mäntylä</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crowd simulation for interactive virtual environments and VR training systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Workshop on Animation and Simulation&apos;01</title>
		<meeting>Eurographics Workshop on Animation and Simulation&apos;01<address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flocks, herds and schools: a, distributed behavioral model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH&apos;87</title>
		<meeting>SIGGRAPH&apos;87</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Crowd simulation in immersive space management</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guilloteau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Workshop on Virtual Environments and Scientific Visualization &apos;96</title>
		<meeting>Eurographics Workshop on Virtual Environments and Scientific Visualization &apos;96</meeting>
		<imprint>
			<biblScope unit="page" from="104" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Berlin Heidelberg New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Group behaviors for systems with significant dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auton Robots</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MuscleBuilder: a modeling tool for human anatomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Sci Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="585" to="595" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time rendering of densely populated urban environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Rendering Workshop</title>
		<meeting>Eurographics Rendering Workshop</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simulating individual and collective actions in temporary gatherings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcphail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Sci Comput Rev</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Crowd Dynamics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Still</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Warwick University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Real-time display of virtual humans: level of details and impostors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Circuits Syst Video Technol, Special Issue on 3D Video Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="227" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Constrained animation of flocks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Computer Animation</title>
		<meeting>ACM Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Simulating dynamical features of escape panic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vicsek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">407</biblScope>
			<biblScope unit="page" from="487" to="490" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">The Myth of Maddening Crowd</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcphail</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Aldine De Gruyter</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards interactive real-time crowd behavior simulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Graph Forum</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="775" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Barr</surname></persName>
		</author>
		<title level="m">Modeling with time and events in computer animation. Proc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m">Eurographics&apos;92</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell Publishers</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<title level="m">The Soar papers: research on artificial intelligence</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">HCSM: framework for behavior and scenario control in virtual environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kearney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Modeling Comput Simul</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="267" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Integration of motion control techniques for virtual human and avatar real-time animation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Becheiraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Emering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VRST &apos;97</title>
		<meeting>VRST &apos;97<address><addrLine>New Yokr</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Crowdbrush: interactive authoring of real-time crowd scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ulicny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Heras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH/Eurographics Symposium on Computer Animation&apos;04</title>
		<meeting>ACM SIGGRAPH/Eurographics Symposium on Computer Animation&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A parametric model for human faces</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Parke</surname></persName>
		</author>
		<idno>UTEC-CSc-75-04</idno>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
		<respStmt>
			<orgName>University of Utah</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ortony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Clore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collins</surname></persName>
		</author>
		<title level="m">The Cognitive Structure of Emotions</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Biological dimensions of personality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Eysenck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Personality: Theory and Research</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Pervin</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Guilford Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="244" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Normal personality assessment in clinical practice: the NEO personality inventory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Assess</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5" to="13" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generic personality and emotion simulation for conversational agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Egges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Anim Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A step towards irrationality: Using emotion to change belief</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Joint Conference on Autonomous Agents and Multi-Agent Systems</title>
		<meeting>the 1st International Joint Conference on Autonomous Agents and Multi-Agent Systems<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July (2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">How emotions and personality effect the utility of alternative decisions: a terrorist target selection case study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tenth Conference On Computer Generated Forces and Behavioral Representation</title>
		<imprint>
			<date type="published" when="2001-05">May (2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Integrating models of personality and emotions into lifelike characters</title>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gebhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Workshop on Affect in Interactions. Towards a New Generation of Interfaces</title>
		<meeting>International Workshop on Affect in Interactions. Towards a New Generation of Interfaces</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Emotion and personality in a conversational character</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Embodied Conversational Characters</title>
		<meeting>the Workshop on Embodied Conversational Characters</meeting>
		<imprint>
			<publisher>October</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="119" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Behavioural programming of autonomous characters based on probabilistic automata and personality</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chittaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Anim Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="319" to="326" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A pet with evolving emotional intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>El-Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ioerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Autonomous Agents&apos;99</title>
		<meeting>Autonomous Agents&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hirst</surname></persName>
		</author>
		<title level="m">Mind and Brain: Dialogues in Cognitive Neuroscience</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Approach withdrawal and cerebral asymmetry: emotional expression and brain physiology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Saron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Senulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Personal Soc Psychol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="330" to="341" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Expression and the nature of emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approaches to emotion</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</editor>
		<meeting><address><addrLine>Lawrence Erlbaum, Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="319" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Emotion expression in speech and music</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Music, Language, Speech, and Brain</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Nord</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Carlson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">London</forename><surname>Macmillan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Computational model of believable conversational agents</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication in MAS: Background, Current Trends and Future</title>
		<editor>
			<persName><forename type="first">M.-P</forename><surname>Huget</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="300" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Autonomous speaker agent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Smid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CASA 2004, 17th International Conference on Computer Animation and Agent</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The synthesis of cloth objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH&apos;86</title>
		<meeting>SIGGRAPH&apos;86</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dressing animated synthetic actors with complex deformable clothes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;92 Proceedings)</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Versatile and efficient techniques for simulating cloth and other deformable objects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courchesne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;95 Proceedings)</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Predicting the drape of woven cloth using interacting particles</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;94 Proceedings)</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Finite-element modeling and control of flexible fabric parts, computer graphics in textiles and apparel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Eischen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Clapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Large steps in cloth simulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1998, ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH 1998, ACM SIGGRAPH<address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Implicit-explicit schemes for fast animation with particles systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzmuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Workshop on Computer Animation and Simulation</title>
		<meeting>the Eurographics Workshop on Computer Animation and Simulation</meeting>
		<imprint>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Berlin Heidelberg New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Developing simulation techniques for an interactive clothing system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<idno>VSMM&apos;97</idno>
	</analytic>
	<monogr>
		<title level="m">Virtual Systems and Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Versatile and efficient techniques for simulating cloth and other deformable objects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courchesne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH&apos;95 Proceedings)</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A fast, flexible, particle-system model for cloth draping. Computer Graphics in Textiles and Apparel</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Strasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications)</title>
		<imprint>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="1996-09">September (1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Bilayered approximate integration for rapid and plausible animation of virtual cloth with realistic wrinkles</title>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Animation</title>
		<meeting>Computer Animation<address><addrLine>Geneva, Switzerland; June</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page">203</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Animating wrinkles on clothes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bangarter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;99</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">GEOFFa geometrical editor for fold formation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Grimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science: Image Analysis Applications and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1024</biblScope>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Berlin Heidelberg New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Neuroanimator: fast neural network emulation and control of physics-based models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings, Annual Conference Series, ACM SIGGRAPH</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Precomputing interactive dynamic deformable scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<meeting><address><addrLine>Reading</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Real-time animation of dressed virtual human</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cordier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>Blackwell Publishers</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Automatic modeling of virtual humans and body clothing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cordier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>December</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
	<note>J Comput Sci Technol</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Mixing virtual and real scenes in the site of ancient Pompeii</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papagiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schertenleib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stoddart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J CAVW</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
