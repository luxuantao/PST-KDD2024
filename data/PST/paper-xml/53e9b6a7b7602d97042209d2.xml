<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency Tree: A Novel Saliency Detection Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Olivier</forename><surname>Le Meur</surname></persName>
							<email>olemeur@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">Z</forename><surname>Frakes</surname></persName>
						</author>
						<author>
							<persName><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institut de Recherche en Informatique et Systèmes Aléatoires</orgName>
								<address>
									<postCode>35042</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Information Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">National Institute of Applied Sciences of Rennes</orgName>
								<orgName type="institution">European University of Brittany</orgName>
								<address>
									<postCode>35708</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Rennes 1</orgName>
								<address>
									<postCode>35042</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Saliency Tree: A Novel Saliency Detection Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7C2F1C1E6A3A999CC9B80A5897DBB058</idno>
					<idno type="DOI">10.1109/TIP.2014.2307434</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Saliency tree</term>
					<term>saliency detection</term>
					<term>saliency model</term>
					<term>saliency map</term>
					<term>regional saliency measure</term>
					<term>region merging</term>
					<term>salient node selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel saliency detection framework termed as saliency tree. For effective saliency measurement, the original image is first simplified using adaptive color quantization and region segmentation to partition the image into a set of primitive regions. Then, three measures, i.e., global contrast, spatial sparsity, and object prior are integrated with regional similarities to generate the initial regional saliency for each primitive region. Next, a saliency-directed region merging approach with dynamic scale control scheme is proposed to generate the saliency tree, in which each leaf node represents a primitive region and each non-leaf node represents a nonprimitive region generated during the region merging process. Finally, by exploiting a regional center-surround scheme based node selection criterion, a systematic saliency tree analysis including salient node selection, regional saliency adjustment and selection is performed to obtain final regional saliency measures and to derive the high-quality pixel-wise saliency map. Extensive experimental results on five datasets with pixel-wise ground truths demonstrate that the proposed saliency tree model consistently outperforms the state-of-the-art saliency models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ability to effortlessly identify salient objects even in a complex scene by exploiting the inherent visual attention mechanism. With the goal both to achieve a comparable saliency detection performance of HVS and to facilitate different saliency-based applications such as those mentioned above, a number of computational saliency models have been proposed in the past decades, and a recent benchmark for saliency models on saliency detection performance is reported in <ref type="bibr" target="#b12">[13]</ref>.</p><p>The early research on saliency model is motivated by simulating the visual attention mechanism of HVS, through which only the significant portion of the scene projected onto the retina is thoroughly processed by human brain for semantic understanding. Based on the biologically plausible visual attention architecture <ref type="bibr" target="#b13">[14]</ref> and the feature integration theory <ref type="bibr" target="#b14">[15]</ref>, Itti et al. proposed a well-known saliency model <ref type="bibr" target="#b15">[16]</ref>, which first computes feature maps of luminance, color and orientation using a center-surround operator across different scales, and then performs normalization and summation to generate the saliency map. Salient regions showing high local contrast with their surrounding regions in terms of any of the three features are highlighted in the saliency map.</p><p>Since then, the center-surround scheme has been widely exploited in a variety of saliency models, due to its clear interpretation of visual attention mechanism and its concise computation form. The centre-surround scheme is implemented using a number of features including local contrasts of color, texture and shape features <ref type="bibr" target="#b16">[17]</ref>, oriented subband decomposition based energy <ref type="bibr" target="#b17">[18]</ref>, ordinal signatures of edge and color orientation histograms <ref type="bibr" target="#b18">[19]</ref>, Kullback-Leibler (KL) divergence between histograms of filter responses <ref type="bibr" target="#b19">[20]</ref>, local regression kernel based self-resemblance <ref type="bibr" target="#b20">[21]</ref>, and earth mover's distance (EMD) between the weighted histograms <ref type="bibr" target="#b21">[22]</ref>. The selection of surrounding region is the key factor to suitably evaluate the saliency of the center pixel/region. Rather than using a fixed shape such as rectangle or circular region, the surrounding region is selected as the whole region of the blurred image in the frequency-tuned saliency model <ref type="bibr" target="#b22">[23]</ref>, and the maximum symmetric region in <ref type="bibr" target="#b23">[24]</ref>. Besides, the center-surround differences are evaluated on the basis of segmented regions using several region-based features to generate the region-level saliency map in <ref type="bibr" target="#b24">[25]</ref>. However, it is nontrivial to determine a suitable scale or to integrate multiple scales for surrounding regions, which can adapt well to salient objects and background with various scales and shapes for reasonable saliency evaluation.</p><p>Besides the widely exploited center-surround scheme, there are various formulations for measuring saliency based on different theories and principles such as information theory, frequency domain analysis, graph theory and supervised learning. Based on information theory, the rarity represented using self-information of local image features <ref type="bibr" target="#b25">[26]</ref>, the complexity represented using local entropy <ref type="bibr" target="#b26">[27]</ref>, and the average transferring information represented using entropy rate <ref type="bibr" target="#b27">[28]</ref> are exploited to measure saliency. Using frequency domain analysis methods, the spectral residual of the amplitude spectrum of Fourier transform <ref type="bibr" target="#b28">[29]</ref>, the phase spectrum of quaternion Fourier transform <ref type="bibr" target="#b9">[10]</ref>, and contrast sensitivity function in the frequency domain <ref type="bibr" target="#b29">[30]</ref> are exploited to generate the saliency map. Based on graph theory, random walks on the weighted graph constructed at pixel level <ref type="bibr" target="#b30">[31]</ref> and block level <ref type="bibr" target="#b31">[32]</ref>, and the stochastic graph model constructed on the basis of region segmentation <ref type="bibr" target="#b32">[33]</ref> are exploited to generate saliency maps at different levels. Using supervised learning methods, a set of features including multi-scale contrast, center-surround histogram and color spatial distribution are integrated to generate the saliency map under the framework of conditional random field <ref type="bibr" target="#b33">[34]</ref>, and region feature vectors are mapped to saliency scores, which are fused across multiple levels to generate the saliency map <ref type="bibr" target="#b34">[35]</ref>. Using support vector machine, eye tracking data is used to train the saliency model for selection of salient/non-salient pixel samples and feature extraction <ref type="bibr" target="#b35">[36]</ref>.</p><p>Recently, the global information of the image has been incorporated into saliency models with different forms. In the context-aware saliency model <ref type="bibr" target="#b36">[37]</ref>, the global uniqueness of color feature and some visual organization rules are combined with the local center-surround difference to generate the saliency map. In <ref type="bibr" target="#b37">[38]</ref>, the global color distribution represented using Gaussian mixture models (GMM), and both local and global orientation distribution are utilized to selectively generate the saliency map. In <ref type="bibr" target="#b38">[39]</ref>, GMMs are used to explicitly construct salient object/background model, and for each pixel, the ratio of posterior probability of object model to background model is calculated as the saliency measure.</p><p>Furthermore, in some saliency models <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b45">[46]</ref>, the image is partitioned into regions using either image segmentation methods or pixel clustering methods, and the global information is effectively incorporated at region level. Statistical models such as Gaussian model <ref type="bibr" target="#b39">[40]</ref> and kernel density estimation based nonparametric model <ref type="bibr" target="#b40">[41]</ref> are used to represent each region, and both color and spatial saliency measures of such statistical models are evaluated and integrated to measure the pixel's saliency. Using different formulations, global contrast and spatially weighted regional contrast <ref type="bibr" target="#b41">[42]</ref>, color compactness of over-segmented regions <ref type="bibr" target="#b42">[43]</ref>, distinctiveness and compactness of regional histograms <ref type="bibr" target="#b43">[44]</ref>, global contrast and spatial sparsity of superpixels <ref type="bibr" target="#b44">[45]</ref>, and two contrast measures for rating global uniqueness and spatial distribution of colors in the saliency filter <ref type="bibr" target="#b45">[46]</ref> are exploited to generate saliency maps with well-defined boundaries. In the recently proposed hierarchical saliency model <ref type="bibr" target="#b46">[47]</ref>, saliency cues are calculated on three image layers with different scales of segmented regions, and then hierarchical inference is exploited to fuse them into a single saliency map.</p><p>Besides, some recent saliency models also exploit object/background priors and cues at different levels for a better saliency detection performance. For example, generic objectness measure <ref type="bibr" target="#b0">[1]</ref> and object-level closed shape prior are effectively incorporated into saliency models presented in <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b48">[49]</ref>, respectively. Under the framework of low-rank matrix recovery, center prior, color prior and learnt transform prior <ref type="bibr" target="#b49">[50]</ref> as well as region segmentation based object prior <ref type="bibr" target="#b50">[51]</ref> are exploited for saliency detection. In the geodesic saliency model <ref type="bibr" target="#b51">[52]</ref>, background priors are exploited to formulate the saliency of patch/superpixel as the length of its shortest path to image borders. In the Bayesian saliency model <ref type="bibr" target="#b52">[53]</ref>, convex hull analysis on interest points and Laplacian sparse subspace clustering on superpixels are used as low-level and mid-level cues, respectively, to infer pixel's Bayesian saliency.</p><p>It should be noted that saliency detection performance has been progressively enhanced with the emerging saliency models, especially those recent models proposed in <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b52">[53]</ref>. They can highlight salient object regions more completely with well-defined boundaries, and suppress background regions more effectively compared to previous saliency models. However, these state-of-the-art saliency models are still insufficient to effectively handle some complicated images with low contrast between objects and background, heterogeneous objects and cluttered background.</p><p>With the main motivation to improve the overall saliency detection performance and especially enhance the applicability on complicated images, we propose saliency tree as a novel saliency model in this paper. Our main contributions are fourfold. First, the proposed saliency tree model enables a hierarchical representation of saliency, which is different from the existing saliency models. Note that the recent model <ref type="bibr" target="#b46">[47]</ref> exploits hierarchical inference for fusing multi-layer saliency cues, and takes the advantage of hierarchical saliency detection for improving the performance. The proposed model is considerably different from <ref type="bibr" target="#b46">[47]</ref> in the complete framework of saliency tree generation and analysis, which selects the most suitable region representation by exploiting the hierarchy of tree structure, to effectively improve the saliency detection performance. Second, on the basis of our previous work <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b50">[51]</ref>, we integrate three measures, i.e., global contrast, spatial sparsity and object prior, at region level to reasonably initialize regional saliency measures. Third, we propose a saliency-directed region merging approach with dynamic scale control scheme for saliency tree generation, which can preserve meaningful regions at different scales. Finally, we propose a systematic procedure of saliency tree analysis including regional center-surround scheme based node selection criterion, salient node selection, regional saliency adjustment and selection to generate high-quality regional saliency map and to derive the final pixel-wise saliency map. Both subjective observations and objective evaluations demonstrate that the proposed saliency tree model achieves a consistently higher saliency detection performance on five datasets compared to the state-of-the-art saliency models.</p><p>The flowchart of the proposed saliency tree model is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, and the following four sections from Section II to V describe image simplification, regional saliency measurement, saliency tree generation and saliency tree analysis, respectively. Extensive experimental results and analysis are presented in Section VI, and conclusions are given in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. IMAGE SIMPLIFICATION</head><p>Natural image generally contains thousands of pixels with a variety of colors. In order to effectively measure saliency, two simplification operations, i.e., adaptive color quantization and region segmentation, are performed on the original image to represent it using a reduced number of colors and regions. Before the following two simplification operations, the original color image is transformed into the Lab color space, in which the luminance channel and the two chrominance channels are well decorrelated.</p><p>For adaptive color quantization, each color channel is first uniformly divided into q bins, and the quantization step is defined as</p><formula xml:id="formula_0">δ z = z max -z min q (1)</formula><p>where z denotes each channel L, a or b, and z max and z min denote the maximum and the minimum, of the channel z, respectively. The parameter q is set to a moderate value, 16, which is generally sufficient for color quantization of natural images. Then a color quantization table Q with q × q × q entries is generated based on the colors of all pixels in the image. The quantized color of each entry, qc k , is calculated as the mean color of those pixels falling into the k th entry of Q. Finally, by removing those entries with zero value, Q is updated to have a total of m (usually m q × q × q) entries. For region segmentation, we choose the gPb-owt-ucm method <ref type="bibr" target="#b53">[54]</ref>, which exploits the globalized probability of boundary (gPb) based contour detector and the oriented watershed transform (owt) to generate the real-valued ultrametric contour map (UCM). By thresholding the UCM, a set of closed boundaries are retained to form a boundary map, which can be converted into a region segmentation result.</p><p>Specifically, the thresholding operation is first performed on the UCM, which is normalized into the range of [0, 1], by increasing the threshold from 0 to 1 with an interval of 0.01, to obtain the boundary map when the corresponding region number decreases just below τ (let t denote the number of actually generated regions). Then for each region smaller than αM/t, where M is the total number of pixels in the image and α is the coefficient for controlling removal of small regions, the pixels belonging to the weakest part of its boundary, in terms of UCM values, are set to zero in the UCM for elimination of this small region. The two parameters, τ and α are set to 200 and 0.2, respectively, to obtain an over-segmentation result with reasonable region sizes. After the above removal of small regions, the remaining regions R i (i = 1, . . . , n) are termed as primitive regions, which are appropriate for regional saliency measurement and are used as the basis for generating the saliency tree in the following sections.</p><p>For the example image in Fig. <ref type="figure" target="#fig_1">2</ref>(a), its UCM is shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>), in which strong boundaries are darker than weak boundaries. The primitive region segmentation result is shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>), in which each primitive region is represented using the mean color of the region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REGIONAL SALIENCY MEASUREMENT</head><p>Based on the image simplification result, regional similarity is measured based on regional histograms. Then initial regional saliency for each primitive region is evaluated by integrating three measures, i.e., global contrast, spatial sparsity and object prior. We observed from a variety of natural images that salient objects are generally different from background regions, and are surrounded by background regions, which usually touch image borders. Specifically, the above three measures are evaluated based on the following three aspects:</p><p>1) Salient object regions usually show contrast with background regions;</p><p>2) Spatial distribution of salient object colors is sparser than background colors, which usually scatter over the whole image;</p><p>3) Background regions generally have a higher ratio of connectivity with image borders than salient object regions.</p><p>The following five subsections will detail the whole process of regional saliency measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regional Similarity</head><p>For each primitive region R i (i = 1, . . . , n), its regional histogram H i is calculated using the quantized colors of all pixels in R i , and then normalized to have m k=1 H i (k) = 1. The regional similarity between each pair of primitive regions, R i and R j , is defined as The color similarity between R i and R j is defined based on the chi-square distance between H i and H j as follows:</p><formula xml:id="formula_1">Si m(R i , R j ) = Si m c (R i , R j ) • Si m d (R i , R j ) (2)</formula><formula xml:id="formula_2">Si m c (R i , R j ) = exp - 1 2 m k=1 H i (k) -H j (k) 2 H i (k) + H j (k)<label>(3)</label></formula><p>The spatial similarity between R i and R j is defined as</p><formula xml:id="formula_3">Si m d (R i , R j ) = 1 - µ i -µ j 2 d (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where d denotes the diagonal length of the image, and the spatial center position of R i is defined as</p><formula xml:id="formula_5">µ i = p∈R i x p |R i |<label>(5)</label></formula><p>where x p denotes the spatial coordinates of each pixel p, and</p><formula xml:id="formula_6">|R i | denotes the number of pixels in R i . Both Si m c (R i , R j ) and Si m d (R i , R j ) fall into the normal- ized range of [0, 1]. Si m(R i , R j</formula><p>) is evaluated higher when the color distributions of R i and R j are similar and the spatial distance between them is shorter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Contrast</head><p>The global contrast of each primitive region R i is measured using the weighted color differences with all the other regions as follows: <ref type="bibr" target="#b5">(6)</ref> where mc i (resp. mc j ) denotes the mean color of R i (resp. R j ). The weight R j • Si m d (R i , R j ) indicates that those regions, which are larger and spatially closer to R i , have a relatively larger contribution to the evaluation of global contrast of R i . Then the normalized global contrast measure for R i is calculated as follows:</p><formula xml:id="formula_7">GC(R i ) = n j =1 R j • Si m d (R i , R j ) • mc i -mc j 2</formula><formula xml:id="formula_8">N GC(R i ) = GC(R i ) -GC min GC max -GC min (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where GC max and GC min are the maximum and the minimum, respectively, in the global contrast measures of all primitive regions.</p><p>Based on a reasonable assumption that region pairs with a high regional similarity should be evaluated with similar values on the global contrast measures, the regional similarities are used to refine the normalized global contrast measures as follows:</p><formula xml:id="formula_10">RGC(R i ) = n j =1 Si m(R i , R j ) • N GC(R j ) n j =1 Si m(R i , R j ) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Sparsity</head><p>For each primitive region R i , the spatial spread of its color distribution is defined as follows:</p><formula xml:id="formula_11">SS(R i ) = n j =1 Si m(R i , R j ) • D(R j ) n j =1 Si m(R i , R j ) (9)</formula><p>where D(R j ) denotes the Euclidean spatial distance from the center position of R j to the image center position. Then an inverse normalization operation is performed on the spatial spread measures to obtain the normalized spatial sparsity measures as follows:</p><formula xml:id="formula_12">N SS(R i ) = SS max -SS(R i ) SS max -SS min<label>(10)</label></formula><p>where SS max and SS min are the maximum and the minimum, respectively, in the spatial spread measures of all primitive regions. Similarly, the normalized spatial sparsity measures are refined as follows:</p><formula xml:id="formula_13">RSS(R i ) = n j =1 Si m(R i , R j ) • N SS(R j ) n j =1 Si m(R i , R j ) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Object Prior</head><p>The connectivity ratio between each region and image borders can be used to indicate the prior probability belonging to a salient object, because salient objects usually do not connect with image borders or connect with image borders less than background regions in a variety of images. Following our previous work <ref type="bibr" target="#b50">[51]</ref>, it is suitable to evaluate the object prior on the basis of a coarse region segmentation, so as to obtain more uniform object prior values for those homogenous background regions, which connect with image borders and are less partitioned in the coarse segmentation result. For this purpose, the UCM is thresholded using a relatively higher value, 0.25, to obtain a coarse segmentation result with n c regions, and the object prior for each region j ( j = 1, . . . , n c ) is defined as follows:</p><formula xml:id="formula_14">O P( j ) = exp -λ j ∩ B ∂ j (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where B denotes the image borders, and ∂ j denotes the perimeter of region j . The coefficient λ is set to 2.0 for a moderate attenuation effect on object priors of those regions touching image borders. Then for each primitive region R i (i = 1, . . . , n), its object prior is assigned as follows:</p><formula xml:id="formula_16">O P(R i ) = O P( j ), ∀R i ⊆ j (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>E. Initial Regional Saliency By integrating the aforementioned three measures using the multiplication operation, the initial regional saliency measure for each primitive region R i is defined as follows:</p><formula xml:id="formula_18">S I (R i ) = RGC(R i ) • RSS(R i ) • O P(R i ) (14)</formula><p>The initial regional saliency measures of all primitive regions are normalized into the range of [0, 1] for the latter use in the saliency tree generation. For the example image in Fig. <ref type="figure" target="#fig_1">2</ref>(a), which has a human object with heterogeneous regions and a cluttered background, the regional global contrast map and spatial sparsity map are shown in Fig. <ref type="figure" target="#fig_1">2(d</ref>) and (e), respectively. On the basis of coarse region segmentation in Fig. <ref type="figure" target="#fig_1">2</ref>(f), the generated object prior map is shown in Fig. <ref type="figure" target="#fig_1">2(g</ref>). Note that for such a complicated image in Fig. <ref type="figure" target="#fig_1">2</ref>(a), either Fig. <ref type="figure" target="#fig_1">2(d)</ref> or (e) reasonably but partly highlights salient object regions and suppresses background regions to some extent. Fig. <ref type="figure" target="#fig_1">2</ref>(g) moderately suppresses a large part of background regions, but those background regions without touching image borders cannot be suppressed.</p><p>By integrating the three maps in Fig. <ref type="figure" target="#fig_1">2(d)</ref>, (e), and (g), the initial regional saliency map shown in Fig. <ref type="figure" target="#fig_1">2(h</ref>) can highlight salient object regions and suppress background regions more effectively than any of the three maps, but it is still insufficient to suppress some background regions. To effectively improve the saliency detection performance on such complicated images, we propose the following saliency tree generation in Sec. IV and saliency tree analysis in Sec. V.</p><p>For reference, the saliency maps generated using the stateof-the-art saliency models for this image are shown from column (c) to (m) in the bottom row of Fig. <ref type="figure" target="#fig_3">7</ref>. We can observe that such a complicated image is difficult for the stateof-the-art saliency models to generate high-quality saliency maps, while the above initial regional saliency map visually outperforms most saliency maps generated using the state-ofthe-art saliency models. Besides, in Sec. VI-B, a performance analysis on five datasets is used to objectively evaluate the contribution of each measure, i.e., global contrast, spatial sparsity and object prior, to the initial regional saliency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SALIENCY TREE GENERATION</head><p>Starting from the primitive regions with their initial regional saliency measures, a saliency-directed region merging approach is proposed to generate the saliency tree, which is a binary partition tree <ref type="bibr" target="#b54">[55]</ref> with saliency measures. Specifically, the region merging sequence is recorded by exploiting the structure of binary partition tree, in which each node is assigned with regional saliency measure. Each primitive region is represented by a leaf node in the saliency tree, and each non-primitive region, which is generated during the region merging process, is represented by a non-leaf node in the saliency tree. The following two subsections first describe the merging criterion and merging order exploited in the region merging process, and then detail the saliency-directed region merging approach for saliency tree generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Merging Criterion and Merging Order</head><p>In order to direct the region merging process, the merging criterion for each pair of adjacent regions, R i and R j , is evaluated based on color similarity and saliency similarity between them as follows:</p><formula xml:id="formula_19">Mrg(R i , R j ) = Si m c (R i , R j ) • Si m s (R i , R j ) (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>where the saliency similarity is defined as</p><formula xml:id="formula_21">Si m s (R i , R j ) = 1 -S I (R i ) -S I (R j ) (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>It is obvious that the merging criterion Mrg(R i , R j ) achieves a higher value when R i and R j show similar color distributions and similar regional saliency measures. Since region merging is always performed on adjacent regions, the merging criterion for each non-adjacent region pair is set to zero, for clarity of the following description.</p><p>Based on the above defined merging criterion, the region merging process is iteratively performed based on the determined merging order. Specifically, at each merging step, one pair or multiple pairs of adjacent regions are selected to merge by checking the following two conditions (a) and (b). Without loss of generality, assume there are a total of t regions at the beginning of the current merging step.</p><p>(a) The out-of-scale regions, which are too small in view of the region number at the current merging step, are first searched to merge with a higher priority. The set of out-ofscale regions is denoted as</p><formula xml:id="formula_23">= {R i } , ∀ |R i | ≤ β M/t,</formula><p>where the coefficient β is set to a moderate value, 0.2, for selection of small regions. In case that is not empty, for each region R i in , its most similar region R j is selected to constitute a region pair (R i , R j ) for merging, i.e.,</p><formula xml:id="formula_24">R j = arg max k=1...t Mrg(R i , R k ), ∀R i ∈<label>(17)</label></formula><p>The condition (a) is exploited to dynamically control the region scale dependent on the current region number t. If more than one pair of regions are selected based on the condition (a), these region pairs are merged simultaneously. However, in case that is empty (usually at the latter merging steps), the condition (b) will be used to select one region pair to merge.</p><p>(b) The region pair (R i , R j ) with the highest merging criterion, which is actually the general condition to determine the merging order in conventional region merging algorithms, is selected as follows:</p><formula xml:id="formula_25">(R i , R j ) = arg max k 1 =1...t, k 2 =1...t Mrg(R k 1 , R k 2 ) (<label>18</label></formula><formula xml:id="formula_26">)</formula><p>When a region pair (R i , R j ) is used to merge into a new region R k , its regional histogram H k and initial regional saliency measure S I (R k ) are calculated as follows:</p><formula xml:id="formula_27">H k = |R i | • H i + R j • H j |R i | + R j (<label>19</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">S I (R k ) = |R i | • S I (R i ) + R j • S I (R j ) |R i | + R j (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>where</p><formula xml:id="formula_31">|R i | • S I (R i ) is termed as the saliency gross in R i .</formula><p>Algorithm 1 Pseudo Code of Saliency-Directed Region Merging Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Saliency-Directed Region Merging</head><p>Based on the above defined merging criterion and merging order, the proposed saliency-directed region merging approach for saliency tree generation is summarized in Algorithm 1. Starting from n primitive regions, the total times of merging two regions into a new region is n -1 during the whole region merging process. Therefore, in the saliency tree, there are n leaf nodes representing primitive regions R i (i = 1, . . . , n) and n -1 non-leaf nodes representing during the region merging process the generated regions R i (i = n + 1, . . . , 2n -1), which are termed as non-primitive regions. The root node in the saliency tree represents the complete image region.</p><p>For the example in Fig. <ref type="figure" target="#fig_1">2</ref>, the saliency tree generated using the proposed saliency-directed region merging approach is shown in Fig. <ref type="figure" target="#fig_2">3(a)</ref>, which only shows the nodes at the top 8 levels and some meaningful regions generated during the region merging process for a clear display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SALIENCY TREE ANALYSIS</head><p>Saliency tree provides for an image a hierarchical saliency representation, which can be fully exploited to generate high-quality regional saliency map and pixel-wise saliency map. In the following, a systematic saliency tree analysis including the definition of node selection criterion, salient node selection, regional saliency adjustment and selection, and pixel-wise saliency map derivation, will be described in the following four subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Node Selection Criterion</head><p>Node selection criterion is defined based on a regional center-surround scheme, in which the region with certain saliency gross and higher saliency difference from its regional surround is considered as more salient. For each region R i (i = 1, . . . , 2n -1) represented by each node N i in the saliency tree, its regional surround C i is defined as the set of primitive regions adjacent to R i . An example of the region R a and its regional surround C a is shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>. The saliency measure of each regional surround C i is defined as follows:</p><formula xml:id="formula_32">S C (C i ) = R j ∈C i log R j • S I (R j ) R j ∈C i log R j (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>where R j is each primitive region covered in C i . The logarithm of region area is used as the weight to reasonably attenuate the contribution of large-sized regions, which have a higher percentage of pixels far away from the boundary of R i , and to make the contribution comparable among the surrounding primitive regions with variable areas.</p><p>Based on the regional center-surround scheme, the node selection criterion for the node N i representing R i is then defined as follows:</p><formula xml:id="formula_34">SC(N i ) = [S I (R i ) -S C (C i )] • |R i | • S I (R i ) (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>where the former term represents the saliency difference between R i and C i , and the latter term |R i | • S I (R i ) representing the saliency gross in R i is introduced to assign a higher node selection criterion for reasonable-sized regions with a higher saliency difference from their surrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Salient Node Selection</head><p>From the saliency tree, in which all nodes are now assigned with node selection criteria, different sets of salient nodes are selected at a series of saliency visibility levels V ( = 1, . . . , ζ ), which determine the minimum saliency gross contained in the correspondingly selected regions. In our implementation, the number of total levels ζ is set to 10, and the saliency visibility levels are set from 1% to 10%, with an interval of 1%, of the total saliency gross of the image, i.e., n i=1 |R i | • S I (R i ). The purpose of salient node selection is to preserve a set of regions, which are considered as salient and the most representative at a certain saliency visibility level, and will be used for regional saliency adjustment and selection in Sec. V-C.</p><p>The proposed salient node selection procedure is summarized in Algorithm 2. Using the salient node Algorithm 2 Pseudo Code of Salient Node Selection Procedure selection procedure on the saliency tree in Fig. <ref type="figure" target="#fig_2">3</ref>(a), the region selection results are output at seven saliency visibility levels, i.e., V 1 ~V6 and V 10 , and shown in the top row of Fig. <ref type="figure">4</ref>. We can observe that the region, which represents the salient object more completely, is selected at different saliency visibility levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Regional Saliency Adjustment and Selection</head><p>For each region selection result , the remaining areas (white areas in the top row of Fig. <ref type="figure">4</ref>) uncovered by any region in are labeled using the connected component analysis to generate one or multiple regions, which constitute a complementary region set . Then the primitive regions, which are covered by each region in (resp. ), constitute the region set (resp.</p><p>). The two sets, and , are complementary to each other on the basis of primitive regions.</p><p>The regions in and constitute a partition of the image at each saliency visibility level V . As shown in the top row of Fig. <ref type="figure">4</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>(a), salient object regions as well as other meaningful background regions, which are generated in the saliency tree, can be more completely preserved in such a partition. Therefore, we can exploit the object prior evaluated on the basis of regions in and to adjust the initial regional saliency measures, in order to highlight salient object regions and suppress background regions more effectively.</p><p>Specifically, for each region k ∈ ∪ , its object prior O P ( k ) is calculated using Eq. ( <ref type="formula" target="#formula_14">12</ref>). Then for each primitive region R i ∈ ∪ , its object prior is assigned as follows:</p><formula xml:id="formula_36">O P (R i ) = O P ( k ), ∀R i ⊆ k and k ∈ ∪<label>(23)</label></formula><p>and then its regional saliency measure at the saliency visibility level V is adjusted as follows:</p><formula xml:id="formula_37">AS (R i ) = O P (R i ) • S I (R i ) (24)</formula><p>From a set of the adjusted regional saliency measures at different saliency visibility levels, the optimal set is selected as the one that can maximize the saliency difference between regions in and regions in , since such an adjustment of regional saliency measures shows that the corresponding region selection result is more confident and rational for regional saliency measurement. Specifically, the optimal set of the adjusted regional saliency measures, AS * , is selected Fig. <ref type="figure">4</ref>. (better viewed in color) Illustration of regional saliency adjustment and selection, and pixel-wise saliency map derivation. Top row: seven region selection results (each selected region is shown using the region's mean color, and the unselected regions are shown using white areas); bottom row: regional saliency maps with saliency difference measures, final regional saliency map (marked with the red border) and pixel-wise saliency map (marked with the yellow border). by maximizing the following saliency difference measure:</p><formula xml:id="formula_38">AS * = arg max ⎡ ⎢ ⎢ ⎣ R i ∈ |R i |• AS (R i ) R i ∈ |R i | - R j ∈ R j • AS (R j ) R j ∈ R j ⎤ ⎥ ⎥ ⎦<label>(25)</label></formula><p>and is used as the final regional saliency measures, i.e.,</p><formula xml:id="formula_39">S F (R i ) = AS * (R i ) for each primitive region R i .</formula><p>For the region selection results shown in the top row of Fig. <ref type="figure">4</ref>, the correspondingly adjusted regional saliency maps with the saliency difference measures are shown in the bottom row of Fig. <ref type="figure">4</ref>. The adjusted regional saliency map at the saliency visibility level V 10 is selected as the final regional saliency map, in which the salient object is highlighted and background regions are suppressed more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pixel-Wise Saliency Map Derivation</head><p>Finally, a pixel-wise saliency map is derived based on final regional saliency measures of primitive regions. For each pixel p ∈ R i , its local neighborhood p includes the primitive region R i and the adjacent primitive regions of R i . The saliency measure of the pixel p is defined as the weighted sum of final regional saliency measures of its neighboring primitive regions, i.e.,</p><formula xml:id="formula_40">S P ( p) = R j ∈ p ω j • S F (R j ) R j ∈ p ω j (<label>26</label></formula><formula xml:id="formula_41">)</formula><p>where the weight ω j is defined as follows:</p><formula xml:id="formula_42">ω j = H j (b p ), i f R j = R i H j (b p ) • exp -x p -μ j 2 x p -μ i 2 , other wise (<label>27</label></formula><formula xml:id="formula_43">)</formula><p>where b p denotes the entry number for the quantized color of the pixel p in the color quantization table Q. Using Eq. ( <ref type="formula" target="#formula_42">27</ref>), a higher weight is given to the primitive region, which is closer to p and has a higher probability of the pixel's color in its regional histogram. It is reasonable that those primitive regions showing a higher color similarity with p and a shorter distance to p have a higher contribution to the saliency of p.</p><p>As shown in the rightmost column of Fig. <ref type="figure">4</ref>, the derived pixel-wise saliency map better highlights the complete salient object region with well-defined boundaries, which are more natural and smoother compared to the final regional saliency map. The pixel-wise saliency map is also shown in the rightmost column of the bottom row in Fig. <ref type="figure" target="#fig_3">7</ref>, for a visual comparison with saliency maps generated using the state-ofthe-art saliency models, and we can see that the quality of our pixel-wise saliency map is better than other saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>To evaluate the performance of the proposed saliency tree (ST) model, we performed extensive experiments on the following five datasets and made a comparison with eleven state-of-the-art saliency models. We used all datasets in the benchmark <ref type="bibr" target="#b12">[13]</ref> on saliency models, i.e., ASD, MSRA, SED and SOD, and one recently introduced dataset, PASCAL-1500 <ref type="bibr" target="#b50">[51]</ref>. We choose the top five models that achieve the best performance in the benchmark <ref type="bibr" target="#b12">[13]</ref>, i.e., region contrast (RC) <ref type="bibr" target="#b41">[42]</ref>, kernel density estimation (KDE) <ref type="bibr" target="#b40">[41]</ref>, context and shape prior (CS) <ref type="bibr" target="#b48">[49]</ref>, fusion of saliency and generic objectness (SVO) <ref type="bibr" target="#b47">[48]</ref>, and context-aware (CA) <ref type="bibr" target="#b36">[37]</ref> model, and six recently proposed saliency models including low-rank matrix recovery (LR) <ref type="bibr" target="#b49">[50]</ref>, saliency filter (SF) <ref type="bibr" target="#b45">[46]</ref>, regional histogram (RH) <ref type="bibr" target="#b43">[44]</ref>, Bayesian saliency (BS) <ref type="bibr" target="#b52">[53]</ref>, segmentation driven low-rank matrix recovery (SLR) <ref type="bibr" target="#b50">[51]</ref>, and hierarchical saliency (HS) <ref type="bibr" target="#b46">[47]</ref>. Note that we use more informative abbreviations, KDE, CS and CA to replace the corresponding abbreviations, LiuICIP, CBsal and Goferman, respectively, used in the benchmark <ref type="bibr" target="#b12">[13]</ref>. We used the executables or source codes with default parameter settings provided by the authors for the eleven saliency models. For a fair comparison, all saliency maps generated using different saliency models are normalized into the same range of [0, 255] with the full resolution of original images. The results of the proposed ST model are available at http://people.irisa.fr/Olivier.Le_Meur/shivpro/index.html. Fig. <ref type="figure">5</ref>. (better viewed in color) ROC curves generated using global contrast maps, spatial sparsity maps, object prior maps, initial regional saliency maps, final regional saliency maps and pixel-wise saliency maps of MSRA dataset.</p><p>In the following, Sec. VI-A introduces the five datasets, and Sec. VI-B analyzes the performance and contribution of different parts in the proposed ST model. The comparison of saliency detection performance with the eleven state-ofthe-art saliency models, both subjectively and objectively, are presented in Sec. VI-C and VI-D, respectively. Some failure cases are analyzed in Sec. VI-E, and the complexity issue of ST model is discussed in Sec. VI-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The five datasets used in the following experiments are described as follows:</p><p>1) MSRA <ref type="bibr" target="#b33">[34]</ref> dataset contains 5,000 images from the image set B of Microsoft Research Asia salient object database. The pixel-wise binary masks of salient objects <ref type="bibr" target="#b34">[35]</ref> are provided as the ground truths. There is a large variation among images including natural scenes, animals, indoor, outdoor, etc.</p><p>2) ASD <ref type="bibr" target="#b22">[23]</ref> dataset contains 1,000 images selected from the above MSRA dataset with pixel-wise binary ground truths for salient objects. Note that ASD is the most commonly used dataset for evaluation of saliency detection performance in the recent years, but the images in this dataset are relatively simpler than the other four datasets.</p><p>3) SED <ref type="bibr" target="#b55">[56]</ref> dataset contains 100 images with one salient object and the other 100 images with two salient objects. Pixel-wise ground truth annotations for salient objects in all 200 images are provided. 4) SOD <ref type="bibr" target="#b56">[57]</ref> dataset contains 300 images from the Berkeley segmentation dataset (BSD) <ref type="bibr" target="#b57">[58]</ref>, for which salient object boundaries are marked by seven users. A unique binary ground truth for each image is generated using the marked boundaries which receive a majority of user votes. This dataset contains many images with different natural scenes making it challenging for saliency detection.</p><p>5) PASCAL-1500 <ref type="bibr" target="#b50">[51]</ref> dataset contains 1500 real-world images from PASCAL VOC 2012 segmentation challenging <ref type="bibr" target="#b58">[59]</ref>, in which only images intuitively deemed to have Fig. <ref type="figure">6</ref>. AUC values achieved using global contrast maps, spatial sparsity maps, object prior maps, initial regional saliency maps, final regional saliency maps and pixel-wise saliency maps of all the five datasets.</p><p>salient objects are selected. The binary ground truths for evaluation of saliency detection performance are adapted from the pixel-wise annotated segmentation ground truths in PASCAL VOC, by labeling object pixels as "1" and other pixels as "0". In PASCAL-1500, many images contain multiple objects with various locations and scales, and/or highly cluttered background, which make this dataset also challenging for saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Analysis</head><p>The proposed ST model first generates global contrast map, spatial sparsity map, object prior map and initial regional saliency map in Sec. III, and then generates final regional saliency map and pixel-wise saliency map in Sec. V. In order to objectively evaluate the contribution of different parts in the proposed ST model to the saliency detection performance, we adopted the commonly used receiver operating characteristic (ROC) curve, which plots the true positive rate (TPR) against the false positive rate (FPR) and presents a robust evaluation of saliency detection performance. Specifically, the above mentioned six classes of maps generated using the ST model are first normalized into the same range of [0, 255]. Then thresholding operations using a series of fixed integers from 0 to 255 are performed on each map to obtain 256 binary salient object masks, and a set of TPR and FPR values are calculated by comparing to the corresponding binary ground truth. Finally, for each class of map, at each threshold, the TPR/FPR values of all images in the dataset are averaged, and the ROC curve for each class of map plots the 256 average TPR values against the 256 average FPR values.</p><p>Fig. <ref type="figure">5</ref> only shows the ROC curves on the largest dataset, i.e., MSRA, due to the page limit. As shown in Fig. <ref type="figure">5</ref>, the ROC curve for initial regional saliency map is higher than the three ROC curves for global contrast map, spatial sparsity map and object prior map. This demonstrates the complementary effect of global contrast, spatial sparsity and object prior for a reasonable estimate of initial regional saliency. Furthermore, compared to the ROC curve for initial regional saliency map, the ROC curve for final regional saliency map is elevated and the ROC curve for pixel-wise saliency map is further elevated. This demonstrates the contribution of saliency tree analysis  for improving the saliency detection performance. We also observed similar trends of such ROC curves on the other four datasets.</p><p>For a more intuitive evaluation, we calculated the area under each ROC curve (AUC) on all the five datasets as a quantitative metric. As shown in Fig. <ref type="figure">6</ref>, the AUC values clearly  demonstrate the effectiveness of initial regional saliency measurement and the contribution of saliency tree analysis to improve the saliency detection performance on all the five datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Subjective Evaluation</head><p>Some saliency maps generated using the proposed ST model and the eleven state-of-the-art saliency models on the five datasets are shown in Figs. 7-11 for a subjective comparison. We can observe that most saliency models can effectively handle images with relatively simple background and homogenous objects, such as the top two examples in Fig. <ref type="figure" target="#fig_4">8</ref>, to generate high-quality saliency maps. However, for some complicated images containing heterogeneous objects (such as human objects, vehicles in Figs. <ref type="figure" target="#fig_3">7</ref> and<ref type="figure" target="#fig_7">11</ref>, and buildings in Fig. <ref type="figure" target="#fig_6">10</ref>), showing a low contrast between objects and background (such as row 6 and 7 in Fig. <ref type="figure" target="#fig_3">7</ref>, the bottom two  rows in Fig. <ref type="figure" target="#fig_4">8</ref>, row 6 and 9 in Fig. <ref type="figure" target="#fig_7">11</ref>), and having a cluttered background (such as row 1, 4 and 10 in Fig. <ref type="figure" target="#fig_3">7</ref>, row 7 in Fig. <ref type="figure" target="#fig_4">8</ref>, row 2 in Fig. <ref type="figure" target="#fig_5">9</ref>, row 1 and 4 in Fig. <ref type="figure" target="#fig_6">10</ref>, and the top five rows in Fig. <ref type="figure" target="#fig_7">11</ref>), the proposed ST model can suppress background regions and highlight the complete salient object regions with well-defined boundaries more effectively than the other saliency models. Thanks to the use of tree structure and the systematic saliency tree analysis process, ST model can better handle the problems of heterogeneous objects, cluttered background and low contrast between object and background more effectively compared to other saliency models.</p><p>Besides, ST model can highlight both large-scale salient objects (such as row 2 in Fig. <ref type="figure" target="#fig_5">9</ref>, row 3 in Fig. <ref type="figure" target="#fig_6">10</ref>, and the bottom row in Fig. <ref type="figure" target="#fig_7">11</ref>) and tiny-scale salient object (such as row 6 in Fig. <ref type="figure" target="#fig_6">10</ref>) more effectively compared to other saliency models, due that the regional center-surround scheme used in ST model flexibly addresses the issue of object scale compared to single or several fixed scales used in other saliency models. Note that the issue of multiple objects itself is not challenging for most saliency models in case that multiple objects are well contrasted with the background (such as row 5 and 6 in Fig. <ref type="figure" target="#fig_5">9</ref>). However, if images containing multiple objects are coupled with the above mentioned problems of heterogeneous objects, cluttered background, low contrast and object scale (such as row 5 and 8 in Fig. <ref type="figure" target="#fig_3">7</ref>, the bottom row in Fig. <ref type="figure" target="#fig_6">10</ref>, and row 7 in Fig. <ref type="figure" target="#fig_7">11</ref>), ST model is more effective to highlight multiple salient objects with well-defined boundaries.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Evaluation</head><p>In order to objectively evaluate the saliency detection performance of different saliency models, ROC curves are generated for each saliency model on each dataset using the same method in Sec. VI-B. Similarly as the generation process of ROC curves, we also generate for each saliency model on each dataset the precision-recall (PR) curve, which plots the precision measure against the recall measure to characterize the saliency detection performance. Due to the page limit, only the ROC curves and PR curves on the largest dataset, i.e., MSRA, are shown in Figs. <ref type="figure" target="#fig_8">12</ref> and<ref type="figure" target="#fig_9">13</ref>, respectively. We can see from Figs. 12 and 13 that both ROC curve and PR curve of ST model are the highest one, which demonstrates the better saliency detection performance of ST model.</p><p>For a quantitative and intuitive comparison, Fig. <ref type="figure" target="#fig_10">14</ref> shows the AUC values, which are calculated for ROC curves of all saliency models on all the five datasets, and objectively demonstrates that ST model consistently outperforms all the other saliency models on all the five datasets. Besides, we can observe from Fig. <ref type="figure" target="#fig_10">14</ref> that for each saliency model, the highest and the lowest AUC value are consistently achieved on ASD dataset and SOD dataset, respectively. This indicates that the widely used ASD dataset is relatively simpler for the state-of-the-art saliency models (note that 10 models achieve an AUC value higher than 0.95), while SOD dataset, which is originally designed for evaluation of image segmentation performance and contains a variety of natural scenes, are the most challenging for saliency detection.</p><p>In order to evaluate the quality of saliency maps and the applicability for salient object detection and segmentation more explicitly, we performed adaptive thresholding operation on each saliency map using the well-known Otsu's method <ref type="bibr" target="#b59">[60]</ref>, which is simple yet effective, to obtain a binary salient object mask. We calculate the measures of precision and recall by comparing each binary salient object mask with the corresponding binary ground truth, and then calculate F-measure, which is the harmonic mean of precision and recall, to evaluate the overall performance as follows:</p><formula xml:id="formula_44">F γ = (1 + γ ) • precision • r ecall γ • precision + r ecall (<label>28</label></formula><formula xml:id="formula_45">)</formula><p>where the coefficient γ is set to 1 indicating the equal importance of precision and recall. For each dataset, the average F-measure on all saliency maps generated using each saliency model is calculated and shown in Fig. <ref type="figure" target="#fig_11">15</ref>. We can see from Fig. <ref type="figure" target="#fig_11">15</ref> that on all the five datasets, ST model consistently achieves the highest F-measure, which objectively demonstrates the overall better quality of saliency maps generated using ST model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Failure Cases and Analysis</head><p>As shown in the previous two subsections, the proposed ST model outperforms the state-of-the-art saliency models on both subjective and objective evaluation. However, some difficult images are still challenging for ST model as well as other state-of-the-art saliency models. If an image contains a part of background regions, which are visually salient against the major part of background, such as rows 1-3 in Fig. <ref type="figure" target="#fig_12">16</ref>, it is difficult to suppress such visually salient background regions. In addition, if a part of salient object shows a very similar color with its nearby background regions in a cluttered scene, such as rows 3-6 in Fig. <ref type="figure" target="#fig_12">16</ref>, the salient object cannot be completely highlighted or/and the nearby background regions are erroneously highlighted in the generated saliency maps. The proposed ST model as well as the state-of-the-art saliency models are still not effective to handle such difficult cases mentioned above. It should be noted that some class-specific knowledge about human object and vehicles (such as motorcycle and train) can be incorporated into saliency models to improve the saliency detection performance on such images in the row 1, 3 and 5 of Fig. <ref type="figure" target="#fig_12">16</ref>, for specific applications such as detection of human objects and vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complexity Analysis</head><p>We implemented the proposed ST model using Matlab R2012b, and used the source code of the gPb-owt-ucm method <ref type="bibr" target="#b53">[54]</ref>, which is written mostly in C++ and coordinated by Matlab scripts, to generate the UCM for region segmentation. Our experiments are performed on a laptop with Intel Core i7-3720QM 2.6GHz CPU and 8GB RAM.  <ref type="table" target="#tab_0">I</ref>. Note that in the current implementation, the resizing factor for eigenvector computation in the gPb is set to 0.5 to reduce the time complexity and memory consumption. Even so, the gPb still occupies a large amount of time and a large memory. In contrast, all the own components of ST model (excluding UCM generation) only take 2.084 seconds in total, and the memory consumption is also lower.</p><p>Therefore, in order to make the proposed ST model more practical for applications with runtime requirements, the computational efficiency of gPb, which is the bottleneck of runtime, should be improved with the highest priority. Fortunately, as reported in <ref type="bibr" target="#b60">[61]</ref>, the gPb method can be effectively parallelized and accelerated using a GPU implementation, which can process the image with a resolution of 0.15 Megapixel in the BSD dataset (note that all 300 images in the SOD dataset are from the BSD dataset as mentioned in Sec. VI-A) within 1.8 seconds on a NVidia GTX 280 GPU. The three own components of ST model can also be parallelized using a GPU implementation. Specifically, regional saliency measurement can be parallelized on the basis of primitive region; salient node selection, regional saliency adjustment and selection can be parallelized on the basis of saliency visibility level; pixel-wise saliency map derivation can be parallelized on the basis of pixel. We believe that a parallel GPU implementation of ST model will substantially improve the computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have presented saliency tree as a novel saliency detection framework, which provides a hierarchical representation of saliency for generating high-quality regional and pixel-wise saliency maps. Initial regional saliency is measured by integrating global contrast, spatial sparsity and object prior of primitive regions to build a reasonable basis for generating the saliency tree. Then saliency-directed region merging, regional center-surround scheme, salient node selection, regional saliency adjustment and selection, and pixel-wise saliency map derivation are proposed and systematically integrated into a complete saliency tree model. Both subjective and objective evaluations on five datasets demonstrate that saliency tree achieves a consistently higher saliency detection performance compared to the state-of-theart saliency models, and especially enhances the applicability on complicated images.</p><p>In our future work, we will extend the current saliency tree model with the incorporation of motion fields and inter-frame spatiotemporal correlations for effective saliency detection in videos. Specifically, saliency detection using the current saliency tree model is only performed on some key frames, which are selected on the basis of video shot with a constraint of maximum interval. Then a regional motion trajectory based temporal saliency measure will be designed to modulate the current final regional saliency measure for each primitive region in the key frame, as its spatiotemporal saliency measure. Finally, we will investigate an inter-frame regional saliency propagation scheme using motion fields, to estimate for each non-key frame its region partition and the spatiotemporal saliency measures of regions, based on the results available in the preceding and the following key frame. The pixel-wise saliency map derivation method can be adapted for estimating pixel's saliency from its spatiotemporal neighboring regions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart of the proposed saliency tree model.</figDesc><graphic coords="3,71.99,58.49,204.50,254.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of region segmentation and regional saliency measurement. (a) original image; (b) ultrametric contour map; (c) primitive region segmentation result; (d) global contrast map; (e) spatial sparsity map; (f) coarse region segmentation result; (g) object prior map; (h) initial regional saliency map.</figDesc><graphic coords="4,71.03,58.37,335.66,90.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Example of saliency tree; (b) (better viewed in color) Illustration of regional surround.</figDesc><graphic coords="7,423.35,60.65,111.14,150.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of saliency detection on MSRA dataset. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="10,82.07,195.89,137.78,138.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples of saliency detection on ASD dataset. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="10,77.63,500.93,140.90,131.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Examples of saliency detection on SED dataset. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="11,77.63,192.41,140.90,135.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Examples of saliency detection on SOD dataset. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="11,77.15,482.33,140.90,131.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Examples of saliency detection on PASCAL-1500 dataset. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="12,77.15,197.57,176.06,104.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (better viewed in color) ROC curves of different saliency models on MSRA dataset.</figDesc><graphic coords="12,56.99,353.69,234.74,171.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (better viewed in color) PR curves of different saliency models on MSRA dataset.</figDesc><graphic coords="12,319.55,353.93,235.22,182.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. AUC values achieved using different saliency models on all the five datasets.</figDesc><graphic coords="13,80.51,58.37,450.26,139.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. F-measures achieved using different saliency models on all the five datasets.</figDesc><graphic coords="13,94.43,221.69,422.90,134.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Some failure examples. (a) images (IM), (b) ground truths (GT) and (c)-(n) saliency maps generated using different models.</figDesc><graphic coords="13,83.15,509.45,273.98,65.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I AVERAGE</head><label>I</label><figDesc>PROCESSING TIME AND MEMORY CONSUMPTION OF EACH COMPONENT IN THE PROPOSED ST MODEL PER IMAGE IN THE SOD DATASET The average processing time and memory consumption of each component in proposed ST model per image in the SOD dataset (all images have a resolution of either 481 × 321 or 321 × 481, equivalent to about 0.15 Megapixel) are shown in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Zhi Liu and Wenbin Zou contributed equally to this paper. The authors would like to thank the anonymous reviewers and the associate editor for their valuable comments, which have greatly helped us to make improvements. The authors would also like to thank Ms. Shuhua Luo and Dr. Wanlei Zhao for their help on performing experiments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61171144, in part by the Shanghai Natural Science Foundation under Grant 11ZR1413000, in part by the Innovation Program of Shanghai Municipal Education Commission under Grant 12ZZ086, in part by the Key (Key grant) Project of Chinese Ministry of Education under Grant 212053, and in part by a Marie Curie International Incoming Fellowship within the 7th European Community Framework Programme under Grant 299202. The associate editor coordinating the review of this manuscript and approving it for publication was Prof.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Region diversity maximization for salient object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="215" to="218" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of visual attention objects in color images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified spectral-domain approach for saliency detection and its application to automatic object segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised salient object segmentation based on kernel density estimation and two-phase graph cut</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1275" to="1289" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retargeting images and video for preserving information saliency</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nienhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="80" to="88" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Seam carving for media retargeting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency density maximization for efficient visual objects discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1822" to="1834" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual attention guided bit allocation in video compression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention-driven image interpretation with application to image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1604" to="1621" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="414" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: Towards the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relevance of a feed-forward model of visual attention for goal-oriented and free-viewing tasks</title>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chevet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency detection and its applications in static and dynamic scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="446" to="456" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The discriminant centersurround hypothesis for bottom-up saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A visual-attention model using earth Mover&apos;s distance based saliency measurement and nonlinear feature combination</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="314" to="328" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency detection using maximum symmetric surround</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="2653" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and robust generation of feature maps for region-based visual attention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mertsching</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="633" to="644" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SUN: A Bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Saliency, scale and image description</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="105" />
			<date type="published" when="2001-11">Nov. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measuring visual saliency by site entropy rate</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2368" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A coherent computational approach to model bottom-up visual attention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="802" to="817" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006-12">Dec. 2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random walks on graphs for salient object detection in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3232" to="3242" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Esaliency (extended saliency): Meaningful attention using stochastic image modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="708" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2376" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Salient region detection by modeling distributions of color and orientation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="892" to="905" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An adaptive computational model for salient object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient saliency detection based on Gaussian models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonparametric saliency detection using kernel density estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Salient object detection through over-segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICME</title>
		<meeting>IEEE ICME</meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency detection using regional histograms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Lett</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="700" to="702" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Superpixel-based saliency detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE WIAMIS</title>
		<meeting>IEEE WIAMIS</meeting>
		<imprint>
			<date type="published" when="2013-07">Jul. 2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fusing generic objectness and visual saliency for salient object detection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic salient object segmentation based on context and shape prior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Segmentation driven lowrank matrix recovery for saliency detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kpalma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ronsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013-09">Sep. 2013</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bayesian saliency via low and mid level cues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Binary partition tree as an efficient representation for image processing, segmentation, and information retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="561" to="576" />
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Design and perceptual validation of performance measures for salient object segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE POCV</title>
		<meeting>IEEE POCV</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2001-07">Jul. 2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01">Jan. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient, high-quality image contour detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="2381" to="2388" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
