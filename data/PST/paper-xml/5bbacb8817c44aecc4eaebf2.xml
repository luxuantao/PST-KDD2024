<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manifold-based Constraint Laplacian Score for multi-label feature selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Rui</forename><surname>Huang</surname></persName>
							<email>huangr@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangling</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Communication and Information Engineering</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manifold-based Constraint Laplacian Score for multi-label feature selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2808E7F7FE03A15AB2FCC6EC755A812D</idno>
					<idno type="DOI">10.1016/j.patrec.2018.08.021</idno>
					<note type="submission">Received date: 27 December 2017 Revised date: 26 June 2018 Accepted date: 12 August 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pattern Recognition Letters</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, multi-label learning has been increasingly applied to many domains such as document classification, protein function prediction, and image annotation. In various multi-label applications, a common and important task is to classify the multi-label data in which each instance is associated with multiple labels simultaneously. Like single-label classification, multi-label classification also suffers from the curse of dimensionality in which the classification accuracy degrades in high dimensional space for a limited number of training samples. Feature selection (FS), which selects an optimal subset of features from the original feature subset according to some criterion, is an effective way to solve the problem. There are many single-label FS methods, but multi-label FS and classification are still open issues.</p><p>According to whether and how a learning algorithm is involved in FS procedure, existing multi-label FS methods can be generally divided into three categories: filter, wrapper and embedded.</p><p>Most multi-label FS methods available are based on the filter model where the selection is independent of the learning algorithm that will be used in the final classification. In Avg.CHI, Chi square statistics between all features and labels are calculated, and the importance score of each feature is obtained by averaging the statistics associated with it [1]. In [2], Lee et al. proposed a mutual information method by maximizing the dependency between selected features and labels. A score function based on the measure of the dependency between the features and labels is proposed [3], which is transformed</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 1 Highlights  Using manifold learning to transform original logical label space to Euclidean label space  The similarity between samples is constrained by the similarity of corresponding numerical labels.  The final selection criterion integrates the influence of both the supervision information and local properties of the data.</p><p>continuously to solve the feature selection as numerical optimization problem. The above algorithms do not consider label correlations. Recent studies have shown that taking label correlations into account can help to select more discriminative features. Sub-Feature Uncovering with Sparsity (SFUS) combines sparse feature selection and shared subspace uncovering to exploit the correlations between different labels <ref type="bibr" target="#b3">[4]</ref>. Spolaor et al. <ref type="bibr" target="#b4">[5]</ref> augmented label set by constructing new labels based on relations between the original labels. Label pairwise comparison transformation with chi-square statistics (PCT-CHI2) converts each original multi-label sample into multiple samples with same feature vectors and different label vectors <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b6">[7]</ref>, the label similarity between different samples is used as a soft constraint which is imposed in laplacian score, and the constrained laplacian score is applicable to multi-label FS.</p><p>In wrapper model, the learning algorithm acts as the evaluation metric for each feature subset generated in FS procedure, e.g. Mlnb which incorporates multi-label naïve bayes classifier into FS <ref type="bibr" target="#b7">[8]</ref>. Different from wrapper model, embedded model integrates the subset selection into the training process, e.g. convex semi-supervised multi-label feature selection (CSFS) combining feature selection and label propagation into a single framework <ref type="bibr" target="#b8">[9]</ref>. There are some FS methods without considering label correlations. In <ref type="bibr" target="#b9">[10]</ref>, Gharroudi et al. proposed wrapper multi-label feature selection method named BRRF (Binary Relevance Random Forest) in which binary relevance random forest is used. In <ref type="bibr" target="#b10">[11]</ref>, multi-label k-nearest neighbor method (MLKNN) and non-dominated sorting genetic algorithm (NSGA-II) are used for feature subset evaluation and search, respectively. Generally speaking, classifier-dependent FS methods can achieve good performance, but they are time consuming.</p><p>In the paper, a new filter FS method, manifold-based constraint laplacian score (MCLS), is proposed for multi-label</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>In recent years, multi-label learning has been increasingly applied to various application areas. As an important pre-processing technique for multilabel learning, multi-label feature selection selects meaningful features to improve classification performance. In this paper, a feature selection method named manifold-based constraint laplacian score (MCLS) is presented. In MCLS, manifold learning is used to transform logical label space to Euclidean label space, and the similarity between samples is constrained by the corresponding numerical labels. The final selection criterion integrates the influence of both the supervision information and local properties of the data. Experimental results demonstrate the effectiveness of the proposed method. 2017 Elsevier Ltd. All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 2 learning. In MCLS, the original logical labels are first transformed into the numerical ones via manifold learning. The numerical labels can indicate different importance of labels relevant to the same sample and help to exploit the label correlations. Then, the similarity between the numerical labels is imposed as constraint on the affinity between the corresponding samples. The similarity is calculated based on a new affinity matrix with the property of scale invariance proposed in <ref type="bibr" target="#b11">[12]</ref>. Finally, the importance of each feature is determined by jointly considering numerical labels and local properties of the data.</p><p>The paper is organized as follows. In Section 2, we introduce the related work including laplacian score, constraint laplacian score, and semi-supervised constraint score. Our method is detailed in Section 3. In Section 4, some experimental results and corresponding analysis are provided. Finally, conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Laplacian score <ref type="bibr" target="#b12">[13]</ref>, constraint score <ref type="bibr" target="#b13">[14]</ref> and semisupervised constraint score <ref type="bibr" target="#b14">[15]</ref> are three single-label FS methods that are closely related to our paper. In this section, we will briefly introduce them. As one of popular unsupervised FS methods, laplacian score <ref type="bibr" target="#b12">[13]</ref> evaluates the importance of each feature according to its power of locality preserving. Let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>  KNN i</head><p>x denote the set of K-nearest neighbors of i</p><p>x . The similarity of samples i x and j x is calculated by:</p><formula xml:id="formula_0">    2 exp if KNN or otherwis 2 e KNN 0 ij i j j i ij e S t                x x x x<label>(1)</label></formula><p>where ‖ ‖ denotes the distance between i x and j x , and parameter t should be tuned to control the spread of neighbors. Then, the score of the d-th feature is defined as:</p><formula xml:id="formula_1">LS = T dd d T dd L D  ff ff<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">  diag DS  1 , L D S ,   1,1, ,1 T  1 , ˆd dd T D D  f1 ff 11 .</formula><p>Constraint score is a supervised version of laplacian score <ref type="bibr" target="#b13">[14]</ref>. In the method, some supervision information is given in the form of pairwise must-link constraints </p><formula xml:id="formula_3">M ij C i j j ij i S S M C               xx xx (3)</formula><p>Then, the score of the d-th feature is calculated as follows:</p><formula xml:id="formula_4">CS = TM dd d TC dd L L  ff ff (4) where   diag MM DS  1 ,   diag C C DS  1 , M M M L D S , C C C L D S .</formula><p>In <ref type="bibr" target="#b14">[15]</ref>, a semi-supervised constraint score is defined as a simple product between laplacian score processed with the unlabeled data and constraint score processed with the labeled data. The score of the d-th feature is calculated as follows:</p><formula xml:id="formula_5">SCS T T M d d d d d T T C d d d d LL DL  f f f f f f f f<label>(5)</label></formula><p>This score considers both the local properties of unlabeled data and the discriminating power of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Manifold-based Constraint Laplacian Score (MCLS)</head><p>MCLS mainly involves three steps: firstly transforming the logical label space to the numerical label space based on manifold learning; secondly, defining three similarity matrices for laplacian graph; and finally, formulating the proposed MCLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Label space transformation</head><p>In multi-label learning, a training set can be denoted by</p><formula xml:id="formula_6">    ,1 ii iN  xy .</formula><p>Here, is a sample and</p><formula xml:id="formula_7">  1, 1 q i  y</formula><p>is the corresponding logical label vector where 1 and -1 represent the relevant and irrelevant to the sample respectively. The logical labels cannot reveal the relative importance of the corresponding label. Hou etc. <ref type="bibr" target="#b15">[16]</ref> proposed to transform the logical labels to numerical labels using manifold learning in order to get more semantic information. According to smoothness assumption, the local topological structure can be transferred from the feature space to the label space. Therefore, the topological structure in the feature space is first computed, and the weight matrix between instance data is obtained. The weight matrix is calculated by minimizing</p><formula xml:id="formula_8">  2 1 2 N j i i j i j i     Γ xx<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">0 j i   unless j x is one of i x "s K-nearest neighbors.</formula><p>Then, with the transferred topological structure, the label manifold can be obtained by minimizing</p><formula xml:id="formula_10">  2 1 2 N j i i j i j i       Z z z<label>(7)</label></formula><p>where is the numerical label. Compared with the logical label i y , i z can indicate different importance of labels relevant to the same sample and help to exploit the label correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">New similarity matrices</head><p>In constraint score, label information is used as two kinds of pairwise constraints to guide graph construction. However, these constraints don"t apply to multi-label data. In MCLS, the similarity between numerical labels is imposed as constraint on the affinity between the corresponding samples. On the other</p><formula xml:id="formula_11">A C C E P T E D M A N U S C R I P T 3</formula><p>hand, Gaussian function is usually applied to calculate the similarity matrix like Eq. <ref type="bibr" target="#b0">(1)</ref>. But the affinities are not scale invariant and is related to the value of parameter t which needs to be carefully tuned. Inspired by the affinity matrix proposed in <ref type="bibr" target="#b11">[12]</ref>, we define a new constraint affinity between i x and j x as follows:</p><p>( </p><formula xml:id="formula_12">1) (1) (2) (2) , 1 , 1 (1) (1) (2) (2) , 1 , 1 11 ˆ0 i K ij i K ij KK ij i K ih i K ih hh</formula><formula xml:id="formula_13">2 ij i j</formula><p>e  zz, and K is the number of nearest neighbors. The similarity between i x and j x depends on the distance between them as well as the affinity between their labels. The latter can be considered as a connection strength between i x and j x influenced by label correlations.</p><p>Among the affinity matrix, the higher similarity between i x and j</p><p>x corresponds to a larger ˆij S .</p><p>In addition, the local properties of the data are examined. A "good" feature should be the one that can preserve the local structure of the data. Specifically, the feature should keep two samples close to each other if they are nearest neighbors; on the other hand, the feature should keep two samples away from each other if they are not nearest neighbors. We define two adjacent matrices as follows:</p><formula xml:id="formula_14">        1 KNN or KNN ˆ0 1 KNN and KNN ˆif otherwise if otherw s 0 ie i j j i M ij i j j i C ij S S               x x x x x x x x<label>(9)</label></formula><p>Eq. ( <ref type="formula" target="#formula_14">9</ref>) looks like Eq. ( <ref type="formula">3</ref>), but only local proximity instead of supervision information is explored. In fact, it is hard to state that samples with multiple labels belong to the must-link or cannotlink constraint sets which are designed for single-label learning.</p><p>MCLS is composed with two items. The first item is laplacian score with modified similarity matrix, and the second item is unsupervised constraint score in which no supervision information is used. For the d-th feature, the score is defined as follows:</p><formula xml:id="formula_15">ˆMCLS ˆT T M d d d d d T T C d d d d LL DL  f f f f f f f f<label>(10)</label></formula><p>where . The whole procedure of MCLS is summarized in Table <ref type="table">1</ref>.</p><formula xml:id="formula_16">  diag DS  1 , L D S  , M M M L D S  with   diag MM DS  1 , Ĉ C C L D S with   diag CC DS  1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and ˆd</head><formula xml:id="formula_17">Table 1 MCLS Algorithm MCLS Input Data set X, Multi-label set Y, parameter K Output MCLS 1.</formula><p>Transform the logical labels to numerical ones according to Eq.( <ref type="formula" target="#formula_8">6</ref>) and (7); 2. Calculate affinity matrix according to Eq. ( <ref type="formula">8</ref>) and then compute the matrices D and L ; 3. Calculate affinity matrices according to Eq. ( <ref type="formula" target="#formula_14">9</ref>) and then compute the ˆM L and ˆC L ;</p><p>4. for d = 1 : D Compute MCLS of the i-th feature according to Eq. ( <ref type="formula" target="#formula_15">10</ref>); end 5. Rank the features according to MCLS in ascending order.</p><p>Inspired by laplacian score and constraint score, MCLS takes a form similar to SCS in <ref type="bibr" target="#b14">[15]</ref>, but it is not simply a multi-label version of SCS, and they are different in the following three ways: 1) Label correlations are explored through numerical labels which are transformed from the original logical ones by using manifold learning; 2) The modified similarity matrix in laplacian score is scale invariant with only one parameter (namely the number of nearest neighbors) to be set, and constrained by the similarity between numerical labels; 3) Unsupervised constraint score is used to evaluate the locality preserving abilities of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment 4.1. Data sets</head><p>In order to evaluate the performance of the proposed algorithm, feature selection and classification experiments are conducted on five datasets (Yeast, Recreation, Education, Flags, and Scene) which are partly from http://mulan.sourceforge.net/datasets-mlc.html. Among them, dataset Yeast includes micro-array expression data and phylogenetic profiles which contains 2417 samples. The dimension of Yeast is 103 and the number of labels is 14. Dataset Flags is used in image domain <ref type="bibr" target="#b16">[17]</ref> and the original data can be found at the UCI repository. The dataset has 194 samples. The numbers of features and labels are 19 and 7 respectively. Dataset Scene belongs to image domain and has 2407 samples. Datasets Recreation and Education have been collected from the "yahoo.com". Both datasets involve in text categorization and contain 5000 samples. Table <ref type="table" target="#tab_1">2</ref> not only lists the details of the datasets, but also gives the cardinalities of feature subsets to be generated by FS methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment settings</head><p>We use six evaluation metrics widely-used in multi-label learning, i.e., Ranking Loss, One Error, Coverage, Average Precision, Macro-averaging F1 (MacF1) and Micro-averaging F1 (MicF1). Except for Average Precision, MacF1 and MicF1, the smaller the values the better performance. In the experiments, these metrics are averaged over 10-fold crossvalidation. For MCLS, the parameter K is set to 5.</p><p>We evaluate the performance of the proposed method MCLS through comparing the following methods:</p><p>1) All features (All-fea): The original data is used as a baseline in the experiments.</p><p>2) Pairwise comparison transformation method combined with chi-square statistics (PCT-CHI2) <ref type="bibr" target="#b5">[6]</ref>: A new label transformation strategy based on ranking loss is proposed and combined with chi-square statistics.</p><p>3) Convex semi-supervised multi-label feature selection (CSFS) <ref type="bibr" target="#b8">[9]</ref>: It does not need graph construction and eigendecomposition, and thus can be readily applied to large-scale dataset.</p><p>4) Sub-feature uncovering with sparsity (SFUS) <ref type="bibr" target="#b3">[4]</ref>: It can uncover the shared subspace of original features by jointly selecting the most relevant features based on a sparsity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Avg.CHI [3]: It calculates 2</head><p> statistics between all features and all labels, and thus obtains the importance of each feature by averaging the values.</p><p>In the experiments, a novel multi-label learning approach named MLFE (Multi-label Learning with Feature-induced labeling information Enrichment) <ref type="bibr" target="#b17">[18]</ref> is used for classification. In the method, the structural information modeled by sparse reconstruction in feature space is conveyed to facilitate generating enriched labeling information in output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment Results</head><p>Fig. <ref type="figure" target="#fig_4">1~5</ref> present the six evaluation metrics vary with regard to the number of selected features on five data sets. The cardinalities of feature subsets are listed in Table <ref type="table" target="#tab_1">2</ref>. From the figures, we can observe that: 1) for datasets Yeast, Education and Recreation, all the FS methods generally obtain better performances with more feature being used. In particular, the proposed MCLS outperforms other FS methods. For datasets Yeast and Education, MCLS shows better performance than All-fea in metrics of Ranking Loss, One Error, Average Precision, MacF1 and MicF1, and metrics of Ranking Loss, Coverage and MacF1, respectively; 2) for dataset Flags, all the FS methods except CSFS outperform All-fea in metrics of Ranking Loss, One Error, Average Precision, MacF1 and MicF1. At the same time, they get better performances when using less features. Among them, MCLS performs best; 3) for dataset Scene, SFUS shows better performance when less features used, but MCLS achieves the best metric values.    Tables 3~7 list the best metric values obtained by the different FS methods. To sort the performance of the FS methods, the average rank is calculated on each dataset. The best performing method on each metric gets the rank of 1, the second best ranks 2 and so on. The average rank is obtained by averaging over all the ranks <ref type="bibr" target="#b18">[19]</ref>. The best results are highlighted in bold. From the tables, we can conclude that MCLS outperforms other FS methods. For dataset Yeast, CSFS generally performs worst. SFUS is in the second place. PCT-CHI2 and Avg.CHI provide comparable results. For dataset Education, PCT-CHI2 ranks between MCLS and Avg.CHI. CSFS performs better than SFUS. For dataset Recreation, SFUS shows the worst results. Avg.CHI and CSFS perform better than PCT-CHI2. For datasets Flags and Scene, according to the metric values, the five methods can be sorted as: MCLS &gt; PCT-CHI2 &gt; SFUS &gt; CSFS &gt; Avg.CHI and MCLS &gt; SFUS &gt; CSFS &gt; PCT-CHI2 &gt; Avg.CHI, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In the paper, we propose a new filter FS method (MCLS) for multi-label learning. MCLS is inspired by laplacian score and constraint score. The selection criterion combines the influence of both the supervision information and local properties of the data. In MCLS, manifold learning is applied to transform the original logical labels into the numerical ones which can reveal different importance of labels relevant to the same sample. The similarity between the numerical labels is imposed as constraint on the affinity between the corresponding samples. The similarity is calculated based on an affinity matrix with the property of scale invariance. Finally, the importance of each feature is determined by jointly considering numerical labels and local properties of the data. Experimental results demonstrate that MCLS outperforms four existing multi-label FS methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>belonging to the same class} and pairwise cannot-link constraints {( )| , belonging to the different class}.Based on the two pairwise constraints, two graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Performance comparison on Yeast</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Performance comparison on Scene</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Datasets and their subsets" cardinalities</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell>Samples</cell><cell>Features</cell><cell>Labels</cell><cell>Number of selected features</cell></row><row><cell>Yeast</cell><cell>biology</cell><cell>2417</cell><cell>103</cell><cell>14</cell><cell>{10, 20, 30, 40, 50, 60, 70, 80, 90}</cell></row><row><cell>Education</cell><cell>text</cell><cell>5000</cell><cell>550</cell><cell>33</cell><cell>{100, 200, 300, 400, 500}</cell></row><row><cell>Recreation</cell><cell>text</cell><cell>5000</cell><cell>606</cell><cell>22</cell><cell>{100, 200, 300, 400, 500}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Six metric values on Yeast</figDesc><table><row><cell></cell><cell cols="2">Ranking Loss One Error</cell><cell>Coverage</cell><cell cols="2">Average Precision MacF1</cell><cell>MicF1</cell><cell>Average Rank</cell></row><row><cell>PCT-CHI2</cell><cell>0.1804</cell><cell>0.2521</cell><cell>6.4545</cell><cell>0.7465</cell><cell>0.3972</cell><cell>0.6373</cell><cell>3.50</cell></row><row><cell>CSFS</cell><cell>0.1892</cell><cell>0.2769</cell><cell>6.5248</cell><cell>0.7279</cell><cell>0.4073</cell><cell>0.6173</cell><cell>4.50</cell></row><row><cell>SFUS</cell><cell>0.1761</cell><cell>0.2528</cell><cell>6.4438</cell><cell>0.7509</cell><cell>0.4034</cell><cell>0.6299</cell><cell>2.83</cell></row><row><cell>Avg.CHI</cell><cell>0.1801</cell><cell>0.2603</cell><cell>6.4421</cell><cell>0.7425</cell><cell>0.4002</cell><cell>0.6389</cell><cell>3.17</cell></row><row><cell>MCLS</cell><cell>0.1589</cell><cell>0.2033</cell><cell>6.1867</cell><cell>0.7817</cell><cell>0.4559</cell><cell>0.6554</cell><cell>1.00</cell></row><row><cell cols="3">Table 4 Six metric values on Education</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ranking Loss One Error</cell><cell>Coverage</cell><cell cols="2">Average Precision MacF1</cell><cell>MicF1</cell><cell>Average Rank</cell></row><row><cell>PCT-CHI2</cell><cell>0.1041</cell><cell>0.5175</cell><cell>4.6800</cell><cell>0.5922</cell><cell>0.4777</cell><cell>0.3852</cell><cell>2.33</cell></row><row><cell>CSFS</cell><cell>0.1210</cell><cell>0.5990</cell><cell>5.0595</cell><cell>0.5333</cell><cell>0.4306</cell><cell>0.2954</cell><cell>4.17</cell></row><row><cell>SFUS</cell><cell>0.1216</cell><cell>0.6910</cell><cell>5.0390</cell><cell>0.4715</cell><cell>0.3949</cell><cell>0.2084</cell><cell>4.83</cell></row><row><cell>Avg.CHI</cell><cell>0.1027</cell><cell>0.5250</cell><cell>4.6385</cell><cell>0.5880</cell><cell>0.4657</cell><cell>0.3795</cell><cell>2.67</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research is supported by Shanghai Natural Science Fund (16ZR1411100), China.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Feature selection for multi-label classification using multivariate mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="357" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization approach for feature selection in multi-label classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Web image annotation via subspace-sparsity collaborated feature selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1021" to="1030" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A systematic review of multi-label feature selection and a new method based on label construction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Spolaôr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label feature selection algorithm based on label pairwise ranking comparison transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1210" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Soft-constrained laplacian score for semi-supervised multi-label feature selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alalga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benabdeslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taleb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="75" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature selection for multilabel naive bayes classification</title>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3218" to="3229" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convex formulation for semi-supervised multi-label feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1171" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Comparison of multilabel feature selection methods using the random forest paradigm</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gharroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elghazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aussem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="95" to="106" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multi-label feature selection algorithm based on multi-objective optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The constrained laplacian rank algorithm for graph-based clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1969" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constraint score: a new filter method for feature selection with pairwise constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1440" to="1451" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constraint scores for semi-supervised feature selection: a comparative study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalakech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Macaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hamad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="656" to="665" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Label manifold learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1680" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A genetic algorithm for optimizing the label ordering in multi-label classifier chains</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plastino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature-induced labeling information enrichment for multi-label learning</title>
		<author>
			<persName><forename type="first">Q.-W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4446" to="4453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparison of ranking methods for classification algorithm selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th European Conference on Machine Learning (ECML2000</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="75" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
