<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 MULTI-HOP ATTENTION GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><surname>' (,+ * + * , '</surname></persName>
						</author>
						<author>
							<persName><surname>' ,,) '</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Magna</forename><surname>Block</surname></persName>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 MULTI-HOP ATTENTION GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention mechanism in graph neural networks (GNNs) led to state-of-theart performance on many graph representation learning task. Currently, at every layer, attention is computed between connected pairs of nodes and depends solely on the representation of the two nodes. However, such attention mechanism does not account for nodes that are not directly connected but provide important network context, which could lead to improved predictive performance. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into attention computation, enabling long-range interactions at every layer of the GNN. To compute attention between nodes that are not directly connected, MAGNA diffuses the attention scores across the network, which increases the "receptive field" for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. This helps MAGNA capture large-scale structural information in every layer, and learn more informative attention. Experimental results on node classification as well as the knowledge graph completion benchmarks show that MAGNA achieves stateof-the-art results: MAGNA achieves up to 5.7% relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains the best performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The introduction of the self-attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b39">Vaswani et al., 2017)</ref>, has pushed the state-of-the-art in many domains including graph presentation learning <ref type="bibr" target="#b28">(Radford et al., 2019;</ref><ref type="bibr" target="#b9">Devlin et al., 2019;</ref><ref type="bibr" target="#b21">Liu et al., 2019a;</ref><ref type="bibr" target="#b18">Lan et al., 2019)</ref>. Graph Attention Network (GAT) <ref type="bibr" target="#b40">(Veličković et al., 2018)</ref> and related models <ref type="bibr" target="#b19">(Li et al., 2018;</ref><ref type="bibr" target="#b41">Wang et al., 2019a;</ref><ref type="bibr" target="#b22">Liu et al., 2019b;</ref><ref type="bibr" target="#b26">Oono &amp; Suzuki, 2020)</ref> developed attention mechanism for Graph Neural Networks (GNNs), which compute attention scores between nodes connected by an edge, allowing the model to attend to messages of node's direct neighbors according to their attention scores. However, such attention computation on pairs of nodes connected by edges implies that a node can only attend to its immediate neighbors to compute its (next layer) representation. This implies that receptive field of a single GNN layer is restricted to one-hop network neighborhoods. Although stacking multiple GATs could in principle enlarge the receptive field and learn non-neighboring interactions, such deep GAT architectures suffer from the oversmoothing problem <ref type="bibr" target="#b41">(Wang et al., 2019a;</ref><ref type="bibr" target="#b22">Liu et al., 2019b;</ref><ref type="bibr" target="#b26">Oono &amp; Suzuki, 2020)</ref> and do not perform well in practice. Furthermore, edge attention weights in the single GAT layer are based solely on representations of the two nodes at the edge endpoints, and do not depend on their graph neighborhood context. In other words, the one-hop attention mechanism in GATs limits their ability to explore the relationship between the broader graph structure and the attention weights. While previous works <ref type="bibr" target="#b47">(Xu et al., 2018;</ref><ref type="bibr" target="#b17">Klicpera et al., 2019b)</ref> have shown advantages in performing multi-hop message-passing in a single layer, these approaches are not graph-attention based. Therefore, incorporating multi-hop neighboring context into the attention computation in graph neural networks had not been explored. Here we present Multi-hop Attention Graph Neural Network (MAGNA), an effective and efficient multi-hop self-attention mechanism for graph structured data. MAGNA uses a novel graph attention diffusion layer (Figure <ref type="figure" target="#fig_0">1</ref>), where we first compute attention weights on edges (represented by solid arrows), and then compute self-attention weights (dotted arrows) between disconnected pairs of nodes through an attention diffusion process using the attention weights on the edges.</p><p>Our model has two main advantages: 1) MAGNA captures long-range interactions between nodes that are not directly connected but may be multiple hops away. Thus the model enables effective long-range message passing, from important nodes multiple hops away. 2) The attention computation in MAGNA is context-dependent. The attention value in GATs <ref type="bibr" target="#b40">(Veličković et al., 2018)</ref> only depends on node representations of the previous layer, and is zero between disconnected pairs of nodes. In contrast, for any pair of nodes within a chosen multi-hop neighborhood, MAGNA computes attention by aggregating the attention scores over all the possible paths (length ≥ 1) connecting the two nodes. Theoretically we demonstrate that MAGNA places a Personalized Page Rank (PPR) prior on the attention values. We further apply spectral graph analysis to show that MAGNA has the capability of emphasizing on large-scale graph structure and lowering high-frequency noise in graphs. Specifically, MAGNA enlarges the lower Laplacian eigen-values, which correspond to the large-scale structure in the graph, and suppresses the higher Laplacian eigen-values which correspond to more noisy and fine-grained information in the graph. We experiment on standard datasets for semisupervised node classification as well as knowledge graph completion.</p><p>Experiments show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7% relative error reduction over previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains better performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion, MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four metrics, with the largest gain of 7.1% in the metric of Hit at 1. Furthermore, we show that MAGNA with just 3 layers and 6 hop wide attention per layer significantly out-performs GAT with 18 layers, even though both architectures have the same receptive field. Moreover, our ablation study reveals the synergistic effect of the essential components of MAGNA, including layer normalization and multi-hop diffused attention. We further observe that compared to GAT, the attention values learned by MAGNA have higher diversity, indicating the ability to better pay attention to important nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MULTI-HOP ATTENTION GRAPH NEURAL NETWORK (MAGNA)</head><p>We first discuss the background and then explain Multi-hop Attention Graph Neural Network's novel multi-hop attention diffusion module and its overall model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARIES</head><p>Let G = (V, E) be a given graph, where V is the set of N n nodes, E ⊆ V × V is the set of N e edges connecting M pairs of nodes in V. Each node v ∈ V and each edge e ∈ E are associated with their type mapping functions: φ : V → T and ψ : E → R. Here T and R denote the sets of node types (labels) and edge/relation types. Our framework supports learning on heterogeneous graphs with multiple elements in R. A general Graph Neural Network (GNN) approach learns an embedding that maps nodes and/or edge types into a continuous vector space. Let X ∈ R Nn×dn and R ∈ R Nr×dr be the node embedding and edge/relation type embedding, where N n = |V|, N r = |R|, d n and d r represent the embedding dimension of node and edge/relation types, each row x i = X[i :] represents the embedding of node v i (1 ≤ i ≤ N n ), and r j = R[j :] represents the embedding of relation r j (1 ≤ j ≤ N r ). MAGNA builds on GNNs, while bringing together the benefits of Graph Attention and Diffusion techniques.</p><p>The core of MAGNA is Multi-hop Attention Diffusion, a principled way to learn attention between any pair of nodes in a scalable way, taking into Here v i , v j , v p , v q ∈ V are nodes in the graph. account the graph structure and enabling multihop context-dependent attention directly.</p><p>The key challenge here is how to allow for flexible but scalable context-dependent multi-hop attention, where any node can influence embedding of any other node in a single GNN layer (even if they are far away in the underlying network). Simply learning attention scores over all node pairs is infeasible and would lead to overfitting and poor generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MULTI-HOP ATTENTION DIFFUSION</head><p>We first introduce attention diffusion to compute the multi-hop attention directly, which operates on the MAGNA's attention scores at each layer. The input to the attention diffusion operator is a set of triples (v i , r k , v j ), where v i , v j are nodes and r k is the edge type. MAGNA first computes the attention scores on all edges. The attention diffusion module then computes the attention values between pairs of nodes that are not directly connected by an edge, based on the edge attention scores, via a diffusion process. The attention diffusion module can then be used as a component in MAGNA architecture, which we will further elaborate in Section 2.3. Edge Attention Computation. At each layer l, a vector message is computed for each triple (v i , r k , v j ). To compute the representation of v j at layer l + 1, all messages from triples incident to v j are aggregated into a single message, which is then used to update v l+1 j . In the first stage, the attention score s of an edge (v i , r k , v j ) is computed by the following:</p><formula xml:id="formula_0">s (l) i,k,j = LeakyReLU(v (l) a tanh(W (l) h h (l) i W (l) t h (l) j W (l) r r k ))<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">W (l) h , W (l) t ∈ R d (l) ×d (l) , W (l) r ∈ R d (l) ×dr and v (l) a ∈ R 1×3d (l)</formula><p>are the trainable weights shared by l-th layer. h</p><formula xml:id="formula_2">(l) i ∈ R d (l)</formula><p>represents the embedding of node i at l-th layer, and h</p><formula xml:id="formula_3">(0) i = x i . r k is the trainable relation embedding of the k-th relation type r k (1 ≤ k ≤ N r )</formula><p>, and a b denotes concatenation of embedding vectors a and b. For graphs with no relation type, we treat as a degenerate categorical distribution with 1 category<ref type="foot" target="#foot_0">1</ref> .</p><p>Applying Eq. 1 on each edge of the graph G, we obtain an attention score matrix S (l) :</p><formula xml:id="formula_4">S (l) i,j = s (l) i,k,j , if (v i , r k , v j ) appears in G −∞, otherwise<label>(2)</label></formula><p>Subsequently we obtain the attention matrix A (l) by performing row-wised softmax over the score matrix S (l) :</p><formula xml:id="formula_5">A (l) = softmax(S (l) ). A (l)</formula><p>ij denotes the attention value at layer l when aggregating message from node j to node i. Attention Diffusion for Multi-hop Neighbors. In the second stage, we further enable attention between nodes that are not directly connected in the network. We achieve this via the following attention diffusion procedure. The procedure computes the attention scores of multi-hop neighbors via graph diffusion based on the powers of the 1-hop attention matrix A:</p><formula xml:id="formula_6">A = ∞ i=0 θ i A i where ∞ i=0 θ i = 1 and θ i &gt; 0 (3)</formula><p>where θ i is the attention decay factor and θ i &gt; θ i+1 . The powers of attention matrix, A i , give us the number of relation paths from node h to node t of length up to i, increasing the receptive field of the attention (Figure <ref type="figure" target="#fig_0">1</ref>). Importantly, the mechanism allows the attention between two nodes to not only depend on their previous layer representations, but also taking into account of the paths between the nodes, effectively creating attention shortcuts between nodes that are not directly connected (Figure <ref type="figure" target="#fig_0">1</ref>). Attention through each path is also weighted differently, depending on θ and the path length.</p><p>In our implementation we utilize the geometric distribution θ i = α(1 − α) i , where α ∈ (0, 1]. The choice is based on the inductive bias that nodes further away should be weighted less in message aggregation, and nodes with different relation path lengths to the target node are sequentially weighted in an independent manner. In addition, notice that if we define θ 0 = α ∈ (0, 1], A 0 = I, then Eq. 3 gives the Personalized Page Rank (PPR) procedure on the graph with the attention matrix A and teleport probability α. Hence the diffused attention weights, A ij , can be seen as the influence of node j to node i. We further elaborate the significance of this observation in Section 4.3.</p><p>We can also view A ij as the attention value of node j to i since</p><formula xml:id="formula_7">Nn j=1 A ij = 1. 2</formula><p>We then define the graph attention diffusion based feature aggregation as AttDiffusion(G, H (l) , Θ) = AH (l) , (4) where Θ is the set of parameters for computing attention. Thanks to the diffusion process defined in Eq. 3, MAGNA uses the same number of parameters as if we were only computing attention between nodes connected via edges. This ensures runtime efficiency as well as good model generalization. Approximate Computation for Attention Diffusion. For large graphs computing the exact attention diffusion matrix A using Eq. 3 may be prohibitively expensive, due to computing the powers of the attention matrix <ref type="bibr" target="#b16">(Klicpera et al., 2019a)</ref>. To resolve this bottleneck, we proceed as follows: Let H (l) be the input entity embedding of the l-th layer (H (0) = X) and θ i = α(1 − α) i . Since MAGNA only requires aggregation via AH (l) , we can approximate AH (l) by defining a sequence Z (K) which converges to the true value of AH (l) (Proposition 1) as K → ∞:</p><formula xml:id="formula_8">Z (0) = H (l) , Z (k+1) = (1 − α)AZ (k) + αZ (0) , where 0 ≤ k &lt; K (5) Proposition 1. lim K→∞ Z (K) = AH (l)</formula><p>In the Appendix we give the proof which relies on the expansion of Eq. 5.</p><p>Using the above approximation, the complexity of attention computation with diffusion is still O(|E|), with a constant factor corresponding to the number of hops K. In practice, we find that choosing the values of K such that 3 ≤ K ≤ 10 results in good model performance. Many realworld graphs exhibit small-world property, in which case even a smaller value of K is sufficient.</p><p>For graphs with larger diameter, we choose larger K, and lower the value of α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DIRECT MULTI-HOP ATTENTION BASED GNN ARCHITECTURE</head><p>Figure <ref type="figure">2</ref> provides an architecture overview of the MAGNA Block that can be stacked multiple times.</p><p>Multi-head Graph Attention Diffusion Layer. Multi-head attention <ref type="bibr" target="#b39">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b40">Veličković et al., 2018)</ref> is used to allow the model to jointly attend to information from different representation sub-spaces at different viewpoints. In Eq. 6, the attention diffusion for each head i is computed separately with Eq. 4, and aggregated:</p><formula xml:id="formula_9">Ĥ(l) = MultiHead(G, H(l) ) = M i=1 head i W o , where head i = AttDiffusion(G, H(l) , Θ i ), H(l) = LayerNorm(H (l) ),<label>(6)</label></formula><p>where denotes concatenation and Θ i are the parameters in Eq. 1 for the i-th head (1 ≤ i ≤ M ), and W o represents a parameter matrix. Since we calculate the attention diffusion in a recursive way in Eq. 5, we add layer normalization which helpful to stabilize the recurrent computation procedure <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. Deep Aggregation. Moreover our MAGNA block contains a fully connected feed-forward sublayer, which consists of a two-layer feed-forward network. We also add the layer normalization and residual connection in both sub-layers, allowing for a more expressive aggregation step for each block <ref type="bibr" target="#b46">(Xiong et al., 2020)</ref>:</p><formula xml:id="formula_10">Ĥ(l+1) = Ĥ(l) + H (l) H (l+1) = W (l) 2 ReLU W (l) 1 LayerNorm( Ĥ(l+1) ) + Ĥ(l+1)<label>(7)</label></formula><p>MAGNA generalizes GAT. MAGNA extends GAT via the diffusion process. The feature aggregation in GAT is H (l+1) = σ(AH (l) W (l) ), where σ represents the activation function. We can divide GAT layer into two components as follows:</p><formula xml:id="formula_11">H (l+1) = σ (2) (AH (2) W (l)<label>(1)</label></formula><p>).</p><p>In component (1), MAGNA removes the restriction of attending to direct neighbors, without requiring additional parameters as A is induced from A. For component (2) MAGNA uses layer normalization and deep aggregation which achieves significant gains according to ablation studies in Table <ref type="table" target="#tab_1">1</ref>. Compared to the "shallow" activation function elu in GAT, we can view deep aggregation (i.e., two-layer MLP) as a learnable deep activation function as two layer MLP can approximate many different functions <ref type="bibr" target="#b27">(Pinkus, 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF GRAPH ATTENTION DIFFUSION</head><p>In this section, we investigate the benefits of MAGNA from the viewpoint of discrete signal processing on graphs <ref type="bibr" target="#b29">(Sandryhaila &amp; Moura, 2013)</ref>. Our first result demonstrates that MAGNA can better capture large-scale structural information. Our second result explores the relation between MAGNA and Personalized PageRank (PPR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SPECTRAL PROPERTIES OF GRAPH ATTENTION DIFFUSION</head><p>We view the attention matrix A of GAT, and A of MAGNA as weighted adjacency matrices, and apply Graph Fourier transform and spectral analysis (details in Appendix) to show the effect of MAGNA as a graph low-pass filter, being able to more effectively capture large-scale structure in graphs. By Eq. 3, the sum of each row of either A or A is 1. Hence the normalized graph Laplacians are Lsym = I − A and L sym = I − A for A and A respectively. We can get the following proposition:</p><p>Proposition 2. Let λg i and λ g i be the i-th eigeinvalues of Lsym and L sym .</p><formula xml:id="formula_13">λg i λ g i = 1 − α 1−(1−α)(1−λ g i ) λ g i = 1 α 1−α + λ g i .<label>(9)</label></formula><p>Refer to Appendix for the proof. We additionally have λ g i ∈ [0, 2] (proved by <ref type="bibr" target="#b25">(Ng et al., 2002)</ref>). Eq. 9 shows that when λ g i is small such that α 1−α + λ g i &lt; 1, then λg i &gt; λ g i , whereas for large λ g i , λg</p><p>i &lt; λ g i . This relation indicates that the use of A increases smaller eigenvalues and decreases larger eigenvalues<ref type="foot" target="#foot_3">3</ref> . See Section 4.3 for its empirical evidence. The low-pass effect increases with smaller α. The eigenvalues of the low-frequency signals describe the large-scale structure in the graph <ref type="bibr" target="#b25">(Ng et al., 2002)</ref> and have been shown to be crucial in graph tasks <ref type="bibr" target="#b17">(Klicpera et al., 2019b)</ref>. As λ g i ∈ [0, 2] (Ng et al., 2002) and α 1−α &gt; 0, the reciprocal format in Eq. 9 will amplify the ratio of lower eigenvalues to the sum of all eigenvalues. In contrast, high eigenvalues corresponding to noise are suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PERSONALIZED PAGERANK MEETS GRAPH ATTENTION DIFFUSION</head><p>We can also view the attention matrix A as a random walk matrix on graph G since Nn j=1 A i,j = 1 and A i,j &gt; 0. If we perform Personalized PageRank (PPR).with parameter α ∈ (0, 1] on G with transition matrix A, the fully Personalized PageRank <ref type="bibr" target="#b23">(Lofgren, 2015)</ref> is defined as:</p><p>A ppr = α(I − (1 − α)A) −1 (10) Using the power series expansion for the matrix inverse, we obtain</p><formula xml:id="formula_14">A ppr = α ∞ i=0 (1 − α) i A i = ∞ i=0 α(1 − α) i A i (11)</formula><p>Comparing to the diffusion Equation 3 with θ i = α(1 − α) i , we have the following proposition. Proposition 3. Graph attention diffusion defines a personalized page rank with parameter α ∈ (0, 1] on G with transition matrix A, i.e., A = A ppr .</p><p>The parameter α in MAGNA is equivalent to the teleport probability of PPR. PPR provides a good relevance score between nodes in a weighted graph (the weights from the attention matrix A). In  <ref type="bibr" target="#b51">(Zhuang &amp; Ma, 2018)</ref> 83.5 72.6 80.0 JKNet <ref type="bibr" target="#b47">(Xu et al., 2018)</ref> 81.1 69.8 78.1 LGCN <ref type="bibr" target="#b11">(Gao et al., 2018)</ref> 83.3 ± 0.5 73.0 ± 0.6 79.5 ± 0.2 Diffusion-GCN <ref type="bibr" target="#b17">(Klicpera et al., 2019b)</ref> 83.6 ± 0.2 73.4 ± 0.3 79.6 ± 0.4 APPNP <ref type="bibr" target="#b16">(Klicpera et al., 2019a)</ref> 84.3 ± 0.2 71.1 ± 0.4 79.7 ± 0.3 g-U-Nets <ref type="bibr" target="#b10">(Gao &amp; Ji, 2019)</ref> 84.4 ± 0.6 73.2 ± 0.5 79.6 ± 0.2 GAT <ref type="bibr" target="#b40">(Veličković et al., 2018)</ref> 83.0 ± 0.7 72.5 ± 0.7 79.0 ± 0.3 Abl.</p><p>No summary, MAGNA places a PPR prior over node pairwise attention scores: the diffused attention between node i and j depends on the attention scores on the edges of all paths between i and j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate MAGNA on two classical tasks<ref type="foot" target="#foot_4">4</ref> . ( <ref type="formula" target="#formula_0">1</ref>) On node classification we achieve an average of 5.7% relative error reduction;</p><p>(2) On knowledge graph completion we achieve 7.1% relative improvement in the Hit at 1 metric. <ref type="foot" target="#foot_5">5</ref> We compare with numbers reported by baseline papers when available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TASK 1: NODE CLASSIFICATION</head><p>Datasets. We employ four benchmark datasets for node classification: (1) standard citation network benchmarks Cora, Citeseer and Pubmed <ref type="bibr" target="#b31">(Sen et al., 2008;</ref><ref type="bibr" target="#b15">Kipf &amp; Welling, 2016)</ref>; and (2) a benchmark dataset ogbn-arxiv on 170k nodes and 1.2m edges from the Open Graph Benchmark (Weihua Hu, 2020). We follow the standard data splits for all datasets. Further information about these datasets is summarized in the Appendix.</p><p>Baselines. We compare against a comprehensive suite of state-of-the-art GNN methods including: GCNs (Kipf &amp; Welling, 2016), Chebyshev filter based GCNs <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>, Du-alGCN <ref type="bibr" target="#b51">(Zhuang &amp; Ma, 2018)</ref>, JKNet <ref type="bibr" target="#b47">(Xu et al., 2018)</ref>, LGCN <ref type="bibr" target="#b11">(Gao et al., 2018)</ref>, Diffusion-GCN <ref type="bibr" target="#b17">(Klicpera et al., 2019b)</ref>, APPNP <ref type="bibr" target="#b16">(Klicpera et al., 2019a)</ref>, Graph U-Nets (g-U-Nets) <ref type="bibr" target="#b10">(Gao &amp; Ji, 2019)</ref>, and GAT <ref type="bibr" target="#b40">(Veličković et al., 2018)</ref>. Experimental Setup. For datasets Cora, Citeseer and Pubmed, we use 6 MAGNA blocks with hidden dimension 512 and 8 attention heads. For the large-scale ogbn-arxiv dataset, we use 2 MAGNA blocks with hidden dimension 128 and 8 attention heads. Refer to Appendix for detailed description of all hyper-parameters and evaluation settings. Results. We report node classification accuracies on the benchmarks. Results are summarized in Tables <ref type="table" target="#tab_3">1 and 2</ref>. MAGNA improves over all methods and achieves the new state-of-the-art on all datasets.</p><p>Table <ref type="table">3</ref>: KG Completion on WN18RR and FB15k-237. MAGNA achieves state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models WN18RR</head><p>FB15k-237 MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10 TransE <ref type="bibr" target="#b5">(Bordes et al., 2013)</ref> 3384 Ablation study. We report (Table <ref type="table" target="#tab_1">1</ref>) the model performance after removing each component of MAGNA (layer normalization, attention diffusion and deep aggregation feed forward layers) from every layer of MAGNA. Note that the model is equivalent to GAT without these three components. We observe that both diffusion and layer normalization play a crucial role in improving the node classification performance for all datasets. While layer normalization alone does not benefit GNNs, its use in conjunction with the attention diffusion module significantly boosts MAGNA's performance. Since MAGNA computes many attention values, layer normalization is crucial in ensuring training stability <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. Meanwhile, we also remove both layer normalization and deep aggregation feed forward layer, and only keep the attention diffusion layer (see the next-to-last row of Table <ref type="table" target="#tab_1">1</ref>). Comparing to GAT, attention diffusion allows multi-hop attention in each layer still benefits the performance of node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TASK 2: KNOWLEDGE GRAPH COMPLETION</head><p>Datasets. We evaluate MAGNA on standard benchmark knowledge graphs: WN18RR <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref> and FB15K-237 <ref type="bibr" target="#b37">(Toutanova &amp; Chen, 2015)</ref>. See the statistics of these KGs in Appendix.</p><p>Baselines. We compare MAGNA with state-of-the-art baselines, including (1) translational distance based models: TransE <ref type="bibr" target="#b5">(Bordes et al., 2013)</ref> and its latest extension RotatE <ref type="bibr" target="#b34">(Sun et al., 2019)</ref>, OTE <ref type="bibr" target="#b35">(Tang et al., 2020)</ref> and ROTH <ref type="bibr" target="#b6">(Chami et al., 2020)</ref>; (2) semantic matching based models: ComplEx <ref type="bibr" target="#b38">(Trouillon et al., 2016)</ref>, QuatE <ref type="bibr" target="#b50">(Zhang et al., 2019)</ref>, CoKE <ref type="bibr" target="#b42">(Wang et al., 2019b)</ref>, ConvE <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref>, DistMult <ref type="bibr" target="#b48">(Yang et al., 2015)</ref> and TuckER <ref type="bibr" target="#b2">(Balazevic et al., 2019)</ref>;</p><p>(3) GNN-based models: R-GCN <ref type="bibr" target="#b30">(Schlichtkrull et al., 2018)</ref>, SACN <ref type="bibr" target="#b32">(Shang et al., 2019)</ref> and A2N <ref type="bibr" target="#b3">(Bansal et al., 2019)</ref>.</p><p>Training procedure. We use the standard training procedure used in previous KG embedding models <ref type="bibr" target="#b2">(Balazevic et al., 2019;</ref><ref type="bibr" target="#b8">Dettmers et al., 2018)</ref> (Appendix for details). We follow an encoderdecoder framework: The encoder applies the proposed MAGNA model to compute the entity embeddings. The decoder then makes link prediction given the embeddings, and existing decoders in prior models can be applied. To show the power of MAGNA, we employ the DistMult decoder <ref type="bibr" target="#b48">(Yang et al., 2015)</ref>, a simple decoder without extra parameters.</p><p>Evaluation. We use the standard split for the benchmarks, and the standard testing procedure of predicting tail (head) entity given the head (tail) entity and relation type. We exactly follow the evaluation used by all previous works, namely the Mean Reciprocal Rank (MRR), Mean Rank (MR), and hit rate at K (H@K). See Appendix for a detailed description of this standard setup.</p><p>Results. MAGNA achieves new state-of-the-art in knowledge graph completion on all four metrics (Table <ref type="table">3</ref>). MAGNA compares favourably to both the most recent shallow embedding methods (QuatE), and deep embedding methods (SACN). Note that with the same decoder (DistMult), MAGNA using its own embeddings achieves drastic improvements over using the corresponding DistMult embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MAGNA MODEL ANALYSIS</head><p>Here we present (1) the spectral analysis results, (2) effect of the hyper-parameters on MAGNA performance, and (3) attention distribution analysis to show the strengths of MAGNA.</p><p>Spectral Analysis: Why MAGNA works for node classification? We compute the eigenvalues of the graph Laplacian of the attention matrix A, λg i , and compare to that of the diffused matrix A, λ g i . Figure <ref type="figure" target="#fig_1">3</ref> (a) shows the ratio λg i /λ g i on the Cora dataset. Low eigenvalues corresponding to largescale structure in the graph are amplified (up to a factor of 8), while high eigenvalues corresponding to eigenvectors with noisy information are suppressed <ref type="bibr" target="#b17">(Klicpera et al., 2019b)</ref>. MAGNA Model Depth. Here we conduct experiments by varying the number of GCN, GAT and our MAGNA layers to be 3, 6, 12, 18 and 24 for node classification on Cora. Results in Figure <ref type="figure" target="#fig_1">3</ref> (b) show that both deep GCN and deep GAT (even with residual connection) suffer from degrading performance, due to the over-smoothing problem <ref type="bibr" target="#b19">(Li et al., 2018;</ref><ref type="bibr" target="#b41">Wang et al., 2019a)</ref>. In contrast, the MAGNA model achieves consistent best results even with 18 layers, making deep MAGNA model robust and expressive. Notice that GAT with 18 layers cannot out-perform MAGNA with 3 layers and K=6 hops, although they have the same receptive field. Effect of K and α. Figures <ref type="figure" target="#fig_1">3 (c</ref>) and (d) report the effect of hop number K and teleport probability α on model performance. We observe significant increase in performance when considering multihop neighbors information (K &gt; 1). However, increasing the hop number K has a diminishing returns, for K ≥ 6. Moreover, we find that the optimal K is correlated with the largest node average shortest path distance (e.g., 5.27 for Cora). This provides a guideline for choosing the best K. We also observe that the accuracy drops significantly for larger α &gt; 0.25. This is because small α increases the low-pass effect (Figure <ref type="figure" target="#fig_1">3</ref>  We first define a discrepancy metric over the attention matrix A for node v as ∆ i =</p><p>A [i,:] −Ui degree(vi) <ref type="bibr" target="#b33">(Shanthamallu et al., 2020)</ref>, where U i is the uniform distribution score for the node v i . ∆ i gives a measure of how much the learnt attention deviates from an uninformative uniform distribution. Large ∆ i indicates more meaningful attention scores. Fig. <ref type="figure">4</ref> shows the distribution of the discrepancy metric of the attention matrix of the 1st head w.r.t. the first layer of MAGNA and GAT. Observe that attention scores learned in MAGNA have much larger discrepancy. This shows that MAGNA is more powerful than GAT in distinguishing important and non-important nodes and assigns attention scores accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Our proposed MAGNA belongs to the family of Graph Neural Network (GNN) models <ref type="bibr" target="#b4">(Battaglia et al., 2018;</ref><ref type="bibr" target="#b44">Wu et al., 2020;</ref><ref type="bibr" target="#b15">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017)</ref>, while taking advantage of graph attention and diffusion techniques. Graph Attention Neural Networks (GATs) generalize attention operation to graph data. GATs allow for assigning different importance to nodes of the same neighborhood at the feature aggregation step <ref type="bibr" target="#b40">(Veličković et al., 2018)</ref>. Based on such framework, different attention-based GNNs have been proposed, including GaAN <ref type="bibr" target="#b49">(Zhang et al., 2018)</ref>, <ref type="bibr">AGNN (Thekumparampil et al., 2018)</ref>, GeniePath <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref>. However, these models only consider direct neighbors for each layer of feature aggregation, and suffer from over-smoothing when they go deep <ref type="bibr" target="#b41">(Wang et al., 2019a)</ref>.</p><p>Diffusion based Graph Neural Network. Recently Graph Diffusion Convolution (GDC) <ref type="bibr" target="#b17">(Klicpera et al., 2019b;</ref><ref type="bibr">a)</ref> proposes to aggregate information from a larger (multi-hop) neighborhood at each layer, by sparsifying a generalized form of graph diffusion. This idea was also explored in <ref type="bibr" target="#b20">(Liao et al., 2019;</ref><ref type="bibr" target="#b24">Luan et al., 2019;</ref><ref type="bibr" target="#b45">Xhonneux et al., 2019;</ref><ref type="bibr" target="#b16">Klicpera et al., 2019a)</ref> for multi-scale deep Graph Convolutional Networks. However, these methods do not incorporate attention mechanisms which proves to have a significant gain in model performance, and do not make use of edge embeddings (e.g., Knowledge graph) <ref type="bibr" target="#b17">(Klicpera et al., 2019b)</ref>. Our approach defines a novel multi-hop context-dependent self-attention GNN which resolves the over-smoothing issue of GAT architectures <ref type="bibr" target="#b41">(Wang et al., 2019a)</ref>. EdgeNets <ref type="bibr" target="#b14">(Isufi et al., 2020</ref>) also extends attention mechanism for multi-hop information aggregation, but it needs more parameters to compute the attention scores of multi-hop neighbors. In contrast, our method infers the attention scores of multi-hop neighbors based on the one-hop neighbor attention scores via graph diffusion, and thus not only more parameter efficiency but show better spectral property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed Multi-hop Attention Graph Neural Network (MAGNA), which brings together benefits of graph attention and diffusion techniques in a single layer through attention diffusion, layer normalization and deep aggregation. MAGNA enables context-dependent attention between any pair of nodes in the graph in a single layer, enhances large-scale structural information, and learns more informative attention distribution. MAGNA improves over all state-of-the-art methods on the standard tasks of node classification and knowledge graph completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-hop attention diffusion. Consider making a prediction at nodes A and D. Left: A single GAT layer only computes attention scores α between directly connected pairs of nodes (i.e., edges) and thus α D,C = 0. Furthermore, the attention α A,B between A and B only depends on their node representations. Right: A single MAGNA layer is able to: (1) capture the information of two-hop neighbor C to D via the diffused multi-hop attention α D,C ; And, (2) enhance graph structure learning by considering all paths between nodes via diffused attention based on powers of graph adjacency matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of MAGNA. (a) Influence of MAGNA on Laplacian eigenvalues. (b) Effect of depth on performance. (c) Effect of hop number K on performance. (d) Effect of teleport probability α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4: Attention weights on Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Inputs: Initial node and relation embeddings</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Output: Final node embedding ! &amp;</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Layer Norm &amp; Add</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feed Forward</cell><cell></cell><cell></cell></row><row><cell>L×</cell><cell>Layer Norm &amp; Add</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Aggregation with</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Multi-head Attention between Nodes Diffused Attention</cell><cell>* (</cell><cell>' (,)</cell><cell>* )</cell></row><row><cell></cell><cell>! " , $</cell><cell></cell><cell></cell></row><row><cell cols="3">Figure 2: MAGNA Architecture.</cell><cell cols="2">Each</cell></row><row><cell cols="5">MAGNA block consists of attention computa-</cell></row><row><cell cols="5">tion, attention diffusion, layer normalization,</cell></row><row><cell cols="5">feed forward layers, and 2 residual connec-</cell></row><row><cell cols="5">tions for each block. MAGNA blocks can be</cell></row><row><cell cols="5">stacked to constitute a deep model. As illus-</cell></row><row><cell cols="5">trated on the right, context-dependent attention</cell></row><row><cell cols="5">is achieved via the attention diffusion process.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy on Cora, Citeseer, Pubmed. MAGNA achieves state-of-theart.</figDesc><table><row><cell></cell><cell>Models</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell></cell><cell>GCN (Kipf &amp; Welling, 2016)</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell></row><row><cell></cell><cell>Chebyshev (Defferrard et al., 2016)</cell><cell>81.2</cell><cell>69.8</cell><cell>74.4</cell></row><row><cell>Baselines</cell><cell>DualGCN</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy on the OGB Arxiv dataset.</figDesc><table><row><cell></cell><cell>GCN</cell><cell>GraphSAGE</cell><cell>Node2vec</cell><cell>JKNet</cell><cell>GaAN</cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell>(Kipf &amp; Welling, 2016)</cell><cell>(Hamilton et al., 2017)</cell><cell>(Grover &amp; Leskovec, 2016)</cell><cell>(Xu et al., 2018)</cell><cell>(Zhang et al., 2018)</cell><cell>MLP</cell><cell>MAGNA</cell></row><row><cell>ogbn-arxiv</cell><cell>71.74 ± 0.29</cell><cell>71.49 ± 0.27</cell><cell>70.07 ± 0.13</cell><cell>72.19 ± 0.21</cell><cell>71.97 ± 0.24</cell><cell cols="2">55.50 ± 0.23 72.76 ± 0.14</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this case, we can view that there is only one "pseudo" relation type (category), i.e., Nr = 1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Obtained by the attention definition A (l) = softmax(S (l) ) and Eq.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">The eigenvalues of A and A correspond to the same eigenvectors, as shown in Proposition 2 in Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">All datasets used are public, and the code will be released at the time of publication.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Please see the definitions of these two tasks in Appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A2n: Attending to neighbors for knowledge graph inference</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lowdimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Under review as a conference paper at ICLR 2021</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Elvin</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07620</idno>
		<title level="m">Edgenets: Edge varying graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient Algorithms for Personalized PageRank</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lofgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multiscale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximation theory of the mlp model in neural networks</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta numerica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>OpenAI Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Graph fourier transform</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><surname>José</surname></persName>
		</author>
		<author>
			<persName><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">AI magazine</biblScope>
			<biblScope unit="page" from="93" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end structureaware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A regularized attention mechanism for graph attention networks</title>
		<author>
			<persName><forename type="first">Uday</forename><surname>Shankar Shanthamallu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaraman</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Spanias</surname></persName>
		</author>
		<editor>ICASSP</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orthogonal relation transforms with graph context modeling for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Kiran K Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVSC-WS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-WS</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02168</idno>
		<title level="m">Coke: Contextualized knowledge graph embedding</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Louis-Pascal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">Continuous graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
