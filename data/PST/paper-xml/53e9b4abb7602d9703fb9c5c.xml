<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">So Who Won? Dynamic Max Discovery with the Crowd</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Guo</surname></persName>
							<email>sdguo@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Parameswaran</surname></persName>
							<email>adityagp@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
							<email>hector@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">So Who Won? Dynamic Max Discovery with the Crowd</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A176459E26A6F8D0D425F1874259606</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Information Filtering Crowdsourcing</term>
					<term>Human Computation</term>
					<term>Max</term>
					<term>Voting</term>
					<term>Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a crowdsourcing database system that may cleanse, populate, or filter its data by using human workers. Just like a conventional DB system, such a crowdsourcing DB system requires data manipulation functions such as select, aggregate, maximum, average, and so on, except that now it must rely on human operators (that for example compare two objects) with very different latency, cost and accuracy characteristics. In this paper, we focus on one such function, maximum, that finds the highest ranked object or tuple in a set. In particularm we study two problems: given a set of votes (pairwise comparisons among objects), how do we select the maximum? And how do we improve our estimate by requesting additional votes? We show that in a crowdsourcing DB system, the optimal solution to both problems is NP-Hard. We then provide heuristic functions to select the maximum given evidence, and to select additional votes. We experimentally evaluate our functions to highlight their strengths and weaknesses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A crowdsourcing database system uses people to perform data cleansing, collection or filtering tasks that are difficult for computers to perform. For example, suppose that a large collection of maps is being loaded into the database, and we would like to add data fields to identify "features" of interest such as washed out roads, dangerous curves, intersections with dirt roads or tracks not on the maps, accidents, possible shelter, and so on. Such features are very hard for image analysis software to identify, but could be identified relatively easily by people who live in the area, who visited the area recently, or who are shown satellite images of the area. A crowdsourcing database system issues tasks to people (e.g., tag features on this map, compare the quality of one map as compared to another), collects the answers, and verifies the answers (e.g., by asking other people to check identified features). We focus on crowd-sourcing systems where people are paid for their work, although in others they volunteer their work (e.g., in the Christmas Bird Count, http://birds.audubon.org/christmas-bird-count, volunteers identify birds across the US), while in other systems the tasks are presented as games that people do for fun (e.g., gwap.com). Amazon's Mechanical Turk (mturk.com) can be used by the database system to find human workers and perform the tasks, although there are many other companies now offering services in this space (CrowdFlower crowdflower.com, uTest utest.com, Microtask microtask.com, Tagasauris tagasauris.com).</p><p>Just like a conventional database system, a crowdsourcing database system will need to perform data processing functions like selects and aggregates, except that now these functions may involve interacting with humans. For example, to add a field "average movie rating" to movie tuples involves an aggregate over the user inputs. Selecting "horror movies" may involve asking humans what movies are "horror movies." The underlying operations (e.g., ask a human if a given movie is a "horror movie" or ask a human which of two cameras is best) have very different latency, cost and accuracy characteristics than in a traditional database system. Thus, we need to develop effective strategies for performing such fundamental data processing functions.</p><p>In particular, in this paper we focus on the Max (Maximum) function: The database has a set of objects (e.g., maps, photographs, Facebook profiles), where conceptually each object has an intrinsic "quality" measure (e.g., how useful is a map for a specific humanitarian mission, how well does a photo describe a given restaurant, how likely is it that a given Facebook profile is the actual profile of Lady Gaga). Of the set of objects, we want to find the one with the largest quality measure. While there are many possible underlying types of human operators, in this paper we focus on a pairwise operator: a human is asked to compare two objects and returns the one object he thinks is of higher quality. We call this type of pairwise comparison a vote.</p><p>If we ask two humans to compare the same pair of objects they may give us different answers, either because they makes mistakes or because their notion of quality is subjective. Either way, the crowdsourcing algorithm may need to submit the same vote to multiple humans to increase the likelihood that its final answer is correct (i.e. that the reported max is indeed the object with the highest quality measure). Of course, executing more votes increases the cost of the algorithm, either in running time and/or in monetary compensation given to the humans for their work.</p><p>There are two types of algorithms for the Max Problem: structured and unstructured. With a structured approach, a regular pattern of votes is set up in advance, as in a tournament. For example, if we have 8 objects to consider, we can first compare 1 to 2, 3 to 4, 5 to 6 and 7 to 8. After we get all results, we compare the 1-2 winner to the 3-4 winner and the 5-6 winner to the 7-8 winner. In the third stage, we compare the two winners to obtain the overall winner, which is declared the max. If we are concerned about voting errors, we can repeat each vote an odd number of times and use the consensus result. For instance, three humans can be asked to do the 1-2 comparison, and the winner of this comparison is the object that wins in 2 or 3 of the individual votes.</p><p>While structured approaches are very effective in predictable environments (such as in a sports tournament), they are much harder to implement in a crowdsourcing database system, where humans may simply not respond to a vote, or may take an unacceptably long time to respond. In our 8-object example, for instance, after asking for the first 4 votes, and waiting for 10 minutes, we may have only the answers to the 1-2 and 5-6 comparisons. We could then re-issue the 3-4 and 7-8 comparisons and just wait, but perhaps we should also try comparing the winner of 1-2 with the winner of 5-6 (which was not in our original plan).</p><p>The point is that even if we start with a structured plan in mind, because of incomplete votes we will likely be faced with an unstructured scenario: some subset of the possible votes have completed (some with varying numbers of repetitions), and we have to answer one or both of the following questions:</p><p>• Judgment Problem: what is our current best estimate for the overall max winner? • Next Votes Problem: if we want to invoke more votes, which are the most effective ones to invoke, given the current standing of results? In this paper we focus precisely on these two problems, in an unstructured setting that is much more likely to occur in a crowdsourcing database system. Both of these problems are quite challenging because there may be many objects in the database, and because there are many possible votes to invoke. An additional challenge is contradictory evidence. For instance, say we have three objects, and one vote told us 1 was better than 2, another vote told us that 2 was better than 3, and a third one told us that 3 was better and 1. What is the most likely max in a scenario like this one where evidence is in conflict? Should we just ignore "conflicting" evidence, but how exactly do we do this? Yet another challenge is the lack of evidence for some objects. For example, say our evidence is that 1 is better than objects 2, 3 and 4. However, there are two additional objects, 5 and 6, for which there is no data. If we can invoke one more vote, should we compare the current favorite, object 1, against another object to verify that it is the max, or should we at least try comparing 5 and 6, for which we have no information?</p><p>The Judgment Problem draws its roots from the historical paired comparisons problem, wherein the goal is to find the best ranking of objects when noisy evidence is provided <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b11">11]</ref>. The problem is also related to the Winner Determination problem in the economic and social choice literature <ref type="bibr">[6]</ref>, wherein the goal is to find the best object via a voting rule: either by finding a "good" ranking of objects and then returning the best object(s) in that ranking, or by scoring each object and returning the best scoring object(s). As we will see in Section 2, our solution to the Judgment Problem differs from both of these approaches. As far as we know, no counterpart of the Next Votes problem exists in the literature. We survey related work in more detail in Section 4.</p><p>In summary, our contributions are as follows:</p><p>• We formalize the Max Problem for a crowdsourcing database system, with its two subproblems, the Judgment and the Next Votes problems. • We propose a Maximum Likelihood (ML) formulation of the Judgment Problem, which finds the object that is probabilistically the most likely to be the maximum. We show that computing the Maximum Likelihood object is NP-Hard, while computation of the probabilities involved is #P-Hard.</p><p>To the best of our knowledge, our ML formulation is the first formal definition and analysis of the Judgment Problem. • We propose and evaluate four different heuristics for the Judgment Problem, some of which are adapted from solutions for sorting with noisy comparisons. For small problem settings, we compare the heuristic solutions to those provided by ML. When there is only a small number of votes available, we show that one of our methods, a novel algorithm based on PageRank, is the best heuristic. • We provide the first formal definition of the Next Votes Problem, and again, propose a formulation based on ML. We show that selecting optimal additional votes is NP-Hard, while computation of the probabilities involved is #P-Hard. • We propose four novel heuristics for the Next Votes Problem. We experimentally evaluate the heuristics, and when feasible, compare them to the ML formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">JUDGMENT PROBLEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setup</head><p>Objects and Permutations: We are given a set O of n objects {o1, ..., on}, where each object oi is associated with a latent quality ci, with no two c's being the same. If ci &gt; cj , we say that oi is greater than oj . Let π denote a permutation function, e.g., a bijection from N to N , where N = {1, ..., n}. We use π(i) to denote the rank, or index, of object oi in permutation π, and π -1 (i) to denote the object index of the ith position in permutation π. If π(i) &lt; π(j), we say that oi is ranked higher than oj in permutation π. Since no two objects have the same quality, there exists a true permutation π * such that for any pair</p><formula xml:id="formula_0">(i, j), if π * (i) &lt; π * (j), then c π * (i) &gt; c π * (j)</formula><p>. Note that throughout this paper, we use the terms permutation and ranking interchangeably.</p><p>Voting: We wish to develop an algorithm to find the maximum (greatest) object in set O, i.e., to find π * -1 (1). The only type of operation or information available to an algorithm is a pairwise vote: in a vote, a human worker is shown two objects oi and oj , and is asked to indicate the greater object. We assume that all workers vote correctly with probability p (0.5 &lt; p ≤ 1), where p is a quantity indicating average worker accuracy. We also assume p is unaffected by worker identities, object values, or worker behavior. In other words, each vote can be viewed as a independent Bernoulli trial with success probability p. In general, the value p is not available to the algorithm, but may be used for algorithm evaluation. However, for reference we do study two algorithms where p is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goals:</head><p>No matter how the algorithm decides to issue vote requests to workers, at the end it must select what it thinks is the maximum object based on the evidence, i.e., based on the votes completed so far. We start by focusing on this Judgment Problem, which we define as follows: PROBLEM 1 (JUDGMENT). Given W , predict the maximum object in O, π * -1 (1).</p><p>In Section 3, we then address the other important problem, i.e., how to request additional votes (in case the algorithm decides it is not done yet). In general, a solution to the Judgment Problem is based upon a scoring function s. The scoring function first computes a score s(i) for each object oi, with the score s(i) representing the "confidence" that object oi is the true maximum. As we will see, for some strategies scores are actual probabilities, for others they are heuristic estimates. Then, the strategy selects the object with the largest score as its answer. Representation: We represent the evidence obtained as an n × n vote matrix W , with wij being the number of votes for oj being greater than oi. Note that wii = 0 for all i. No other assumptions are made about the structure of matrix W . The evidence can also be viewed as a directed weighted graph Gv = (V, A), with the vertices being the objects and the arcs representing the vote outcomes. For each pair (i, j), if wij &gt; 0, arc (i, j) with weight wij is present in A. For example, Figure <ref type="figure" target="#fig_0">1</ref> displays a sample vote matrix and equivalent graph representation. In this example, object 1 is called A, object 2 is B, and so on. For instance, there is an arc from vertex (object) B to vertex C with weight 2 because w2,3 = 2, and there is a reverse arc from C to B with weight 1 because w3,2 = 1. If there are no votes (wij = 0, as from B to A), we can either say that there is no arc, or that the arc has weight 0.</p><formula xml:id="formula_1">W = 0 B @ 0 2 0 0 0 0 2 3 0 1 0 1 0 0 1 0 1 C A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Maximum Likelihood</head><p>Preliminaries: We first present a Maximum Likelihood (ML) formulation of the Judgment Problem. We directly compute the object that has the highest probability of being the maximum object in O, given vote matrix W . Assuming that average worker accuracy p is known, the ML formulation we present is the optimal feasible solution to the Judgment Problem.</p><p>Let π be a random variable over the set of all n! possible permutations, where we assume a-priori that each permutation is equally likely to be observed. We denote the probability of a given permutation π d given the vote matrix W as P (π = π d |W ). For the ease of exposition, we adopt the shorthand P (π d |W ) instead of writing P (π = π d |W ). To derive the formula for P (π d |W ), we first apply Bayes' theorem,</p><formula xml:id="formula_2">P (π d |W ) = P (W |π d )P (π d ) P (W ) = P (W |π d )P (π d ) X j P (W |πj)P (πj)<label>(1)</label></formula><p>From our assumption that the prior probabilities of all permutations are equal, P (π d ) = 1 n! . Now consider P (W |π d ). Given a permutation π d , for each unordered pair {i, j}, the probability fπ d (i, j) of observing wij and wji is the binomial distribution probability mass function (p.m.f.):</p><formula xml:id="formula_3">fπ d (i, j) = ( `wij +w ji w ij ´pw ji (1 -p) w ij if π d (i) &lt; π d (j) `wij +w ji w ji ´pw ij (1 -p) w ji if π d (j) &lt; π d (i)</formula><p>Note that if both wij and wji are equal to 0, then fπ d (i, j) = 1. Now, given a permutation π d , observing the votes involving an unordered pair {i, j} is conditionally independent of observing the votes involving any other unordered pair. Using this fact, P (W |π d ), the probability of observing all votes given a permutation π d is simply:</p><formula xml:id="formula_4">P (W |π d ) = Y i,j:i&lt;j fπ d (i, j)<label>(2)</label></formula><p>Since we know the values of both p and W , we can derive a formula for P (π d |W ) in Equation <ref type="formula" target="#formula_2">1</ref>. In particular, the most likely permutation(s), is simply:</p><formula xml:id="formula_5">arg max d P (π d |W )<label>(3)</label></formula><p>The permutations optimizing Equation 3 are also known as Kemeny permutations or Kemeny rankings <ref type="bibr" target="#b9">[9]</ref>. For example, consider the matrix W of Figure <ref type="figure" target="#fig_0">1</ref>. We do not show the computations here, but it turns out that the two most probable permutations of the objects are (D, C, B, A) and (C, D, B, A), with all other permutations having lower probability. This result roughly matches our intuition, since object A was never voted to be greater than any of the other objects, and C and D have more votes in favor over B.</p><p>We can derive the formula for the probability that a given object oj has a given rank k. Let π -1 (i) denote the position of object i in the permutation associated with random variable π. We are interested in the probability P (π -1 (k) = j|W ). Since the event (π = π d ) is disjoint for different permutations π d , we have:</p><formula xml:id="formula_6">P (π -1 (k) = j|W ) = X d:π -1 d (k)=j P (π d |W )</formula><p>Substituting for P (π d |W ) using Equation <ref type="formula" target="#formula_2">1</ref>and simplifying, we have:</p><formula xml:id="formula_7">P (π -1 (k) = j|W ) = X d:π -1 d (k)=j P (W |π d ) X l P (W |π l )<label>(4)</label></formula><p>Since we are interested in the object oj with the highest probability of being rank 1, e.g., P (π -1 (1) = j|W ), we now have the Maximum Likelihood formulation to the Judgment Problem: ML FORMULATION 1 (JUDGMENT). Given W and p, determine: arg maxj P (π -1 (1) = j|W ).</p><p>In the example graph of Figure <ref type="figure" target="#fig_0">1</ref>, while C and D both have Kemeny permutations where they are the greatest objects, D is the more likely max over a large range of p values. For instance, for p = 0.75, P (π -1 (1) = C|W ) = 0.36 while P (π -1 (1) = D|W ) = 0.54. This also matches our intuition, since C has one vote where it is less than B, while D is never voted to be less than either A or B. Maximum Likelihood Strategy: Equation 4 implies that we only need to compute P (W |π d ) for each possible permutation π d , using Equation 2, in order to determine P (π -1 (k) = j|W ) for all values j and k. In other words, by doing a single pass through all permutations, we can compute the probability that any object oj has a rank k, given the vote matrix W .</p><p>We call this exhaustive computation of probabilities the Maximum Likelihood Strategy and use it as a baseline in our experiments. Note that the ML strategy is the optimal feasible solution to the Judgment Problem. In the extended technical report <ref type="bibr" target="#b15">[15]</ref>, we provide pseudocode for the computation of the scoring function based on our ML formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computational Complexity</head><p>Hardness of the Judgment Problem: In Section 2.2, we presented a formulation for the Judgment Problem based on ML for finding the object most likely to be the max (maximum) object in O. Unfortunately, the strategy based on that formulation was computationally infeasible, as it required computation across all n! permutations of the objects in O. We now show that the optimal solution to the problem of finding the maximum object is in fact NP-Hard using a reduction from the problem of determining Kemeny winners <ref type="bibr" target="#b17">[17]</ref>. <ref type="bibr">(Hudry et al. [17]</ref> actually show that determining Slater winners in tournaments is NP-Hard, but their proof also holds for Kemeny winners. We will describe the Kemeny winner problem below.) Our results and proof are novel. THEOREM 1. (Hardness of the Judgment Problem) Finding the maximum object given evidence is NP-Hard.</p><p>PROOF. We first describe the Kemeny winner problem. In this proof, we use an alternate (but equivalent) view of a directed weighted graph like Figure <ref type="figure" target="#fig_0">1</ref>. In particular, we view weighted arcs as multiple arcs. For instance, if there is an arc from vertex A to B with weight 3, we can instead view it as 3 separate edges from A to B. We use this alternate representation in our proof.</p><p>An arc i → j respects a permutation if the permutation has oj ranked higher than oi (and does not if the permutation has oi ranked higher than oj ). A Kemeny permutation is simply a permutation of the vertices (objects), such that the number of arcs that do not respect the permutation is minimum. There may be many such permutations, but there always is at least one such permutation. The starting vertex (rank 1 object) in any of these permutations is a Kemeny winner. It can be shown that finding a Kemeny winner is NP-Hard (using a reduction from the feedback arc set problem, similar to the proof in Hudry et al. <ref type="bibr" target="#b17">[17]</ref>).</p><p>We now reduce the Kemeny winner determination problem to one of finding the maximum object. Consider a directed weighted graph G, where we wish to find a Kemeny winner. We show that with a suitable probability p, which we set, the maximum object (i.e., the solution to the Judgment Problem) in G is a Kemeny winner. As before, the probability that a certain object oj is the maximum object is the right hand side of Equation <ref type="formula" target="#formula_7">4</ref>with k set to 1. The denominator can be ignored since it is a constant for all j. We set worker accuracy p to be very close to 1. In particular, we choose a value p such that 1-p p &lt; 1 n! . Now, consider all permutations π d that are not Kemeny permutations. In this case, it can be shown that X d:π d is not Kemeny P (W |π d ) &lt; P (W |πs) for any Kemeny permutation πs. Thus, the object oj that maximizes Equation 4 (for k = 1) has to be one that is a Kemeny winner.</p><p>To see why</p><formula xml:id="formula_8">X d:π d is not Kemeny P (W |π d ) &lt; P(W |πs)</formula><p>for a Kemeny permutation πs, notice that the left hand side is at most n! × P (W |π d ) where π d is the permutation (not Kemeny) that has the least number of arcs that do not respect the permutation. Note that P (W |π d ) is at most P (W |πs)× 1-p p , since this permutation has at least one more mistake as compared to any Kemeny permutation.</p><p>Therefore, we have shown that, for a suitable p, the maximum object in G is a Kemeny winner. Thus, we have a reduction from the Kemeny winner problem to the Judgement problem. Since finding a Kemeny winner is NP-Hard, this implies that finding the maximum object in G is NP-Hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#P-Hardness of Probability Computations:</head><p>In addition to being NP-Hard to find the max object, we can show that evaluating the numerator of the right hand side of Equation <ref type="formula" target="#formula_7">4</ref>(with k = 1) is #P-Hard, in other words: computing</p><formula xml:id="formula_9">P (π -1 (1) = j, W ) is #P-Hard.</formula><p>We use a reduction from the problem of counting the number of linear extensions in a directed acyclic graph (DAG), which is known to be #P-Hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THEOREM 2. (#P-Hardness of Probability Computation</head><formula xml:id="formula_10">) Com- puting P (π -1 (1) = j, W ) is #P-Hard.</formula><p>PROOF. A linear extension is a permutation of the vertices, such that all arcs in the graph respect the permutation (i.e., a linear extension is the same as a Kemeny permutation for a DAG).</p><p>Consider a DAG G = (V, A). We add an additional vertex x such that there is an arc from each of the vertices in G to x, giving a new graph G = (V , A ). We now show that computing P (π -1 (1) = x, W ) in G can be used to compute the number of linear extensions in G. Notice that:</p><formula xml:id="formula_11">P (π -1 (1) = x, W ) = |A | X i=0 aip i (1 -p) |A |-i = p |A | × |A | X i=0 ai( 1 -p p ) |A |-i (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>where ai is the number of permutations where there are i arcs that respect the permutation. Clearly, the number that we wish to determine is a |A | , since that is the number of permutations that correspond to linear extensions. Equation 5 is a polynomial of degree |A | in  <ref type="formula" target="#formula_11">5</ref>to create a set of |A | + 1 equations involving the ai coefficients. We may then derive the value of a |A| using Lagrange's interpolation formula.</p><p>Since vertex x is the only maximum vertex in G , by computing P (π -1 (1) = x, W ) in G , we count the number of linear extensions in DAG G. Since counting the number of linear extensions in a DAG is #P-Hard, this implies that the computation of</p><formula xml:id="formula_13">P (π -1 (1) = x, W ) in G is #P-Hard, which implies that the com- putation of P (π -1 (k) = j, W ) for directed graph Gv (associated with vote matrix W ) is #P-Hard.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Heuristic Strategies</head><p>The ML scoring function is computationally inefficient and also requires prior knowledge of p, the average worker accuracy, which is not available to us in real-world scenarios. We next investigate the performance and efficiency of four heuristic strategies, each of which runs in polynomial time. The heuristics we present, excluding the Indegree heuristic, do not require explicit knowledge of the worker accuracy p. Indegree Strategy: The first heuristic we consider is an Indegree scoring function proposed by Coppersmith et al. <ref type="bibr" target="#b10">[10]</ref> to approximate the optimal feedback arc set in a directed weighted graph where arc weights lij, lji satisfy lij + lji = 1 for each pair of vertices i and j.We can transform the vote matrix W to a graph where this Indegree scoring function can be directly applied. The idea is to construct a complete graph between all objects where arc weights lji are equal to P (π(i) &lt; π(j)|wij , wji), where lji reflects the probability that oi is greater than oj given the local evidence wij and wji. For details, see the extended technical report <ref type="bibr" target="#b15">[15]</ref>.</p><p>The Indegree scoring function computes the score of object oj as: s(j) = P i lij . Intuitively, vertices with higher scores correspond to objects which have compared favorably to other objects, and hence should be ranked higher. The predicted ranking has been shown to be a constant factor approximation to the feedback arc set for directed graphs where all arcs (i, j) are present and lij + lji = 1 <ref type="bibr" target="#b10">[10]</ref>. The running time of this heuristic is dominated by the time to do the final sort of the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Strategy:</head><p>The Indegree heuristic is simple to compute, but only takes into account local evidence. That is, the score of object oi only depends on the votes that include oi directly. We now consider a Local scoring function, adapted from a heuristic proposed by David <ref type="bibr" target="#b12">[12]</ref>, which considers evidence two steps away from oi. This method was originally proposed to rank objects in incomplete tournaments with ties. We adapted the scoring function to our setting, where there can be multiple comparisons between objects, and there are no ties in comparisons.</p><p>This heuristic is based on the notion of wins and losses, defined as follows: wins(i) = P j wji and losses(i) = P i wij . For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, vertex B has 3 wins and 5 losses.</p><p>The score s(i) has three components. The first is simply wins(i)losses(i), reflecting the net number of votes in favor of oi. For vertex B, this first component would be 3 -5 = -2. Since this first component does not reflect the "strength" of the objects oi was compared against, we next add a "reward": for each oj such that wji &gt; wij (i has net wins over j), we add wins(j) to the score of oi. In our example, B only has net wins over A, so we reward B with wins(A) (which in this case is zero). On the other hand, since C beat out B, then C gets a reward of wins(B) = 3 added to its score. Finally, we "penalize" s(i) by subtracting losses(j) for each oj that overall beat oi. In our example, we subtract from s(B) both losses(C) = 2 and losses(D) = 1. Thus, the final score s(B) is -2 plus the reward minus the penalty, i.e., s(B) = -2 + 0 -3 = -5.</p><p>More formally, score s(i) is defined as follows:</p><formula xml:id="formula_14">s(i) = wins(i) -losses(i) + X j [1(wji &gt; wij )wins(j)] - X j [1(wij &gt; wji)losses(j)]<label>(6)</label></formula><p>Having computed s(•), we sort all objects by decreasing order of s. The resulting permutation is our predicted ranking, with the vertex having largest s being our predicted maximum object.</p><p>PageRank Strategy: Both the Indegree and Local heuristics use only information one or two steps away to make inferences about the objects of O. We next consider a global heuristic scoring function inspired by the PageRank <ref type="bibr" target="#b26">[26]</ref> algorithm. The general idea behind using a PageRank-like procedure is to utilize the votes in W as a way for objects to transfer "strength" between each other. The use of PageRank to predict the maximum has been previously considered <ref type="bibr" target="#b5">[5]</ref> in the literature. Our contribution is a modified PageRank to predict the maximum object in O, which in particular, can handle directed cycles in the directed graph representing W .</p><p>Consider again the directed graph Gv representing the votes of W (Figure <ref type="figure" target="#fig_0">1</ref> is an example). Let d + (i) to denote the outdegree of vertex i in Gv, e.g.</p><formula xml:id="formula_15">d + (i) = P j wij. If d + (i) = 0, we say that i is a sink vertex.</formula><p>Let prt(i) represent the PageRank of vertex i in iteration t. We initialize each vertex to have the same initial PageRank, e.g., pr0(•) = 1 n . In each iteration t + 1, we apply the following update equation to each vertex i:</p><formula xml:id="formula_16">prt+1(i) = X j wji d + (j) prt(j)<label>(7)</label></formula><p>For each iteration, each vertex j transfers all its PageRank (from the previous iteration) proportionally to the other vertices i whom workers have indicated may be greater than j, where the proportion of j's PageRank transferred to i is equal to</p><formula xml:id="formula_17">w ji d + (j) .</formula><p>Intuitively, prt(i) can be thought as a proxy for the probability that object oi is the maximum object in O (during iteration t).</p><p>What happens to the PageRank vector after performing many update iterations using Equation <ref type="formula" target="#formula_16">7</ref>? Considering the strongly connected components (SCCs) of Gv, let us define a terminal SCC to be a SCC whose vertices do not have arcs transitioning out of the SCC. After a sufficient number of iterations, the PageRank probability mass in Gv becomes concentrated in the terminal SCCs of Gv, with all other vertices outside of these SCCs having zero PageRank <ref type="bibr" target="#b4">[4]</ref>. In the context of our problem, these terminal SCCs can be thought of as sets of objects which are ambiguous to order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy 1 PageRank</head><p>Require: n objects, vote matrix W , K iterations Ensure:</p><formula xml:id="formula_18">ans = predicted maximum object construct Gv = (V, A) from W compute d + [•] for each vertex {compute all outdegrees} for i : 1 . . . n do if d + [i] == 0 then w ii ← 1 end if end for pr 0 [•] ← 1 n {pr 0 is the PageRank vector in iteration 0} for k : 1 . . . K do for i : 1 . . . n do for j : 1 . . . n, j = i do pr k [i] ← pr k [i] + w ji d + [j] pr k-1 [j] end for end for end for compute period[•] of each vertex using final iterations of pr[•] for i : 1 . . . n do s[i] ← 0 {s[•] is a vector storing average PageRank} for j : 0 . . . period[i] -1 do s[i] ← s[i] + pr K-j [i] end for s[i] ← s[i] period[i] end for ans ← argmax i s[i]</formula><p>Our proposed PageRank algorithm is described in Strategy 1. How is our strategy different from the standard PageRank algorithm? The original PageRank update equation is:</p><formula xml:id="formula_19">prt+1(i) = 1 -d n + d X j wji d + (j) prt(j))</formula><p>Comparing the original equation and Equation <ref type="formula" target="#formula_16">7</ref>, the primary difference is that we use a damping factor d = 1, e.g. we remove jump probabilities. PageRank was designed to model the behavior of a random surfer traversing the web, while for the problem of ranking objects, we do not need to model a random jump vector.</p><p>A second difference between our modified PageRank and the original PageRank is that prior to performing any update iterations, for each sink vertex i, we set wii equal to 1 in W . In our setting, sinks correspond to objects which may be the maximum object (e.g., no worker voted that oi is less than another object). By setting wii to 1 initially, from one iteration to the next, the PageRank in sink i remains in sink i. This allows PageRank to accumulate in sinks. Contrast this with the standard PageRank methodology, where when a random surfer reaches a sink, it is assumed that (s)he transitions to all other vertices with equal probability.</p><p>Finally, a caveat to our PageRank strategy is that the PageRank vector (pr(•) in Strategy 1) may not converge for some vertices in terminal SCCs. To handle the oscillating PageRank in terminal SCCs, we execute our PageRank update equation (Equation <ref type="formula" target="#formula_16">7</ref>) for a large number of iterations, denoted as K in Strategy 1. Then, we examine the final iterations, say final 10%, of the PageRank vector to empirically determine the period of each vertex, where we define the period as the number of iterations for the PageRank value of a vertex to return to its current value. In practice, we find that running PageRank for K iterations, where K = O(n), is sufficient to detect the period of nearly all vertices in terminal SCCs. For example, consider a graph among 3 objects A, B, C with 3 arcs: (A, B), (B, C), and (C, B). All vertices initially have 1  3 PageRank probability. After 1 iteration, the PageRank vector is (0, 2  3 , 1 3 ).</p><p>After 2 iterations, the PageRank vector is (0, 1 3 ,<ref type="foot" target="#foot_0">2</ref> 3 ). And so on. In this example, object B and C each have periods of 2.</p><p>With the periods computed for each vertex, we compute an average PageRank value for each vertex over its period. This average PageRank is used as the scoring function s(•) for this strategy. After the termination of PageRank, we sort the vertices by decreasing order of s(•), and predict that the vertex with maximum average PageRank corresponds to the maximum object in O. Note that our PageRank heuristic is primarily intended to predict a maximum object, not to predict a ranking of all objects (as many objects will end up with no PageRank). However, for completeness, when evaluating PageRank in later experiments, we still do consider the predicted ranking induced by PageRank. The details of our implementation are displayed in Strategy 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Strategy:</head><p>We next propose an Iterative heuristic strategy to determine the maximum object in O. The general framework is the following:</p><p>1. Place all objects in a set.</p><p>2. Rank the objects in the set by a scoring metric.</p><p>3. Remove the lower ranked objects from the set. 4. Repeat steps 3 and 4 until only one object remains.</p><p>There are two parameters we can vary in this framework: the scoring metric and the number of objects eliminated each iteration. Let us define the dif (i) metric of object oi to be equal to wins(i)losses(i). An implementation of the Iterative strategy using the dif metric is displayed in Strategy 2. In our particular implementation, we emphasize computational efficiency and remove half of the remaining objects each iteration. The Iterative strategy relies upon the elimination of lower ranked objects before reranking higher ranked objects. With each iteration, as more objects are removed, the dif s of the higher ranked objects separate from the dif s of the lower ranked objects. Basically, by removing lower ranked objects, the strategy is able to more accurately rank the remaining set of objects. The strategy can be thought of as iteratively narrowing in on the maximum object.</p><p>It is important to note that other scoring metrics can be used with this Iterative strategy as well. For example, by iteratively ranking with the Local heuristic, we were able to achieve (slightly) better performance than the simple dif metric. Our method is similar to the Greedy Order algorithm proposed by Cohen et al. <ref type="bibr" target="#b7">[7]</ref>, who considered a problem related to feedback arc set. Our strategy differs in that it is more general (e.g., it can utilize multiple metrics), and our strategy can be optimized (e.g., if we eliminate half of the objects each iteration, we require only a logarithmic number of sorts, as opposed to a linear number).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Experiments</head><p>In this section, we experimentally compare our heuristic strategies: Indegree (DEG), Local (LOC), PageRank, (PR), and Iterative (ITR). We also compare them with the Maximum Likelihood (ML) Strategy, which we consider the best possible way to select the maximum. However, since ML is computationally very expensive, we only do this comparison on a small scenario. For our experiments, we synthetically generate problem instances, varying : n (the number of objects in O), v (the number of votes we sample for W ), and p (average worker accuracy). We prefer to use synthetic data, since it lets us study a wide spectrum of scenarios, with highly reliable or unreliable workers, and with many or few votes.</p><p>In our base experiments, we vary the number of sampled votes v, from 0 to 5n(n -1) and vary worker accuracy p from 0.55 to 0.95. As a point of reference, we refer to n(n-1)</p><formula xml:id="formula_20">2 votes as v = 1x</formula><p>Edge Coverage, e.g. each pair of objects is sampled approximately once. So 5n(n -1) votes is equivalent to v = 10x Edge Coverage in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy 2 Iterative</head><p>Require: n objects, vote matrix W Ensure: ans = predicted maximum object</p><formula xml:id="formula_21">dif [•] ← 0 {dif [•] is the scoring metric} for i : 1 . . . n do for j : 1 . . . n, j = i do dif [j] ← dif [j] + w ij ; dif [i] ← dif [i] -w ij end for end for initialize set Q {which stores objects} for i : 1 . . . n do Q ← Q ∪ i end for while |Q| &gt; 1 do sort objects in Q by dif [•] for r : ( |Q| 2 + 1) . . . |Q| do remove object i (with rank r) from Q for j : j ∈ Q do if w ij &gt; 0 then dif [j] ← dif [j] -w ij ; dif [i] ← dif [i] + w ij end if if w ji &gt; 0 then dif [i] ← dif [i] -w ji ; dif [j] ← dif [j]</formula><p>+ w ji end if end for end for end while ans ← S <ref type="bibr" target="#b1">[1]</ref> {S <ref type="bibr" target="#b1">[1]</ref> is the final object in S} Each data point (given n, p, v values) in our results graphs is obtained from 5,000 runs. Each run proceeds as follows: We initialize W as an n × n null matrix and begin with an arbitrary true permutation π * of the objects in O. Let U denote the set of all tuples (i, j) where i = j. We randomly sample v tuples from U with replacement. After sampling a tuple (i, j), we simulate the human worker's comparison of objects oi and oj . If π * (i) &lt; π * (j), with probability p, we increment wji, and with probability 1p, we increment wij . If π * (j) &lt; π * (i), with probability p, we increment wij , and with probability 1p, we increment wji.</p><p>For each generated matrix W in a run, we apply each of our heuristic strategies to obtain predicted rankings of the objects in O. Comparing the predicted ranking with π * we record a "yes" if the predicted maximum agrees with the true maximum. After all runs have completed, we compute Precision at 1 (P@1), the fraction of "yes" cases over the number of runs. For results with other evaluation metrics, see the extended technical report <ref type="bibr" target="#b15">[15]</ref>.</p><p>As a first experiment, we consider the prediction performance of Maximum Likelihood (ML) and the four heuristics for a set of 5 objects with p = 0.75, displayed in Figure <ref type="figure">2(a)</ref>. We choose a small set of objects, so that ML can be computed. We find that as the number of votes sampled increases, the P@1 of all heuristics (excluding PageRank) increase in a concave manner, approaching a value of 0.9 for 10x Edge Coverage (e.g., if 5n(n -1) votes are uniformly sampled, the heuristics can predict the maximum object 90% of the time, even though average worker accuracy is 0.75).</p><p>ML has better performance than all the four heuristics.</p><p>As expected, ML performs the best in Figure <ref type="figure">2</ref>(a), but recall that ML requires explicit knowledge of p, and it is computationally very expensive. Still, the ML curve is useful, since it tells us how far the heuristics are from the optimal feasible solution (ML). Also, note that PageRank (PR) performs poorly, indicating that PageRank is poor when the number of objects is small. Iterative is the best of the four heuristics when the number of votes sampled is n(n-1)  For a larger experiment, we consider the problem of prediction for n = 100 objects in Figure <ref type="figure">2(b)(c)(d)</ref>. ML is necessarily omitted from this experiment. Looking at the graphs, we first note that the Iterative (ITR) heuristic performs significantly better than the other heuristics, particularly when p = 0.55 or p = 0.75. This is best demonstrated by Figure <ref type="figure">2(c)</ref>, which shows that for p = 0.75 and 10x Edge Coverage, the Iterative heuristic has a P@1 of over 0.9, whereas the second best heuristic, Indegree (DEG), only has a P@1 of approximately 0.5. Looking at the middle graph again, note how the performance gap between the Iterative heuristic and the other heuristics widens as the Edge Coverage increases from 1x to 5x. The strength of the Iterative strategy comes from its ability to leverage the large number of redundant votes, in order to iteratively prune out lower-ranked objects until there is a predicted maximum. The strategy is robust even when worker accuracy is low. When average worker accuracy is high, Figure <ref type="figure">2</ref>(d), the Iterative heuristic still is the heuristic of choice, although the performance gap between the Iterative and Indegree or Local (LOC) heuristics decreases to a minimal amount, as the number of votes sampled becomes very large.</p><p>PageRank is a poor heuristic when worker accuracy is low. However, when worker accuracy is reasonable, PageRank is quite effective, even when the number of votes is low. We next focus upon the performance of the PageRank (PR) heuristic. For p = 0.75 and p = 0.95, the PageRank heuristic's prediction curve crosses the prediction curves for the Indegree (DEG) and Local (LOC) heuristics. This is an indication that the PageRank heuristic is quite effective when the number of votes is low, but is unable to utilize the information from additional votes when the number of votes is large. We also observe the poor performance of PageRank when p = 0.55, in Figure <ref type="figure">2(b)</ref>, indicating that PageRank is not a suitable heuristic when worker accuracy is low.</p><p>Over various worker accuracies, Iterative is the best heuristic, followed by PageRank, Local and Indegree.</p><p>From the prior experiments, we see that prediction performance for each strategy varies greatly with respect to the average worker accuracy p.We next directly investigate prediction performance versus worker accuracy for a fixed 1x Edge Coverage in Figure <ref type="figure">3</ref>(left). We find that for this fixed Edge Coverage, the Iterative (ITR) strategy performs the best, followed by PageRank (PR), then the Local (LOC) and Indegree (DEG) heuristics. As expected, prediction performance increases with worker accuracy across all strategies. In particular, note the large slope of the Iterative and PageRank prediction curves, as compared to the Local and Indegree prediction curves, which are near identical.</p><p>PageRank is the best of the four heuristics when there are few votes and worker accuracy is high.</p><p>All experiments considered thus far examine prediction when the number of votes is an order of magnitude larger than the number of objects. For a more difficult scenario, we examine prediction performance when the number of votes is approximately the same as the number of objects. Figure <ref type="figure">3</ref>(right) displays P@1 for 100 objects when the number of votes is varied from 20 to 200 and p = 0.95. We observe that PageRank (PR) has the highest prediction performance among the four heuristics. Conducting several other experiments, we find that, so long as worker accuracy is high, PageRank facilitates good prediction, even when the number of votes is low relative to the number of objects. This fact will prove useful when we consider the problem of selecting which additional votes to request, given an initial sparse vote graph.</p><p>From our experiments, we conclude that Iterative (ITR) is the strategy of choice when evaluating a large number of votes (relative to the number of objects), whereas PageRank is the preferred heuristic when evaluating a small number of votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NEXT VOTES PROBLEM</head><p>We now consider the second half of the Max Problem, the Next Votes Problem. Beginning with an initial vote matrix W , if we wish to submit additional vote requests to a crowdsourcing marketplace, which additional votes (i.e., comparisons between pairs of objects) should be requested to augment our existing vote matrix W , and improve our prediction of the maximum object? In particular, we assume that we are given a vote budget of b additional votes that may be requested. There are two ways in which we can use this vote budget: (a) an adaptive strategy, where we submit some initial votes, get some responses, then submit some more, get more responses, and so on, or (b) a one-shot strategy, where we submit all votes at once. In this paper, we consider a one-shot strategy with a vote budget of b. This strategy is more relevant in a crowdsourcing setting since the latency of crowdsourcing is high. Once the responses for these vote requests are received, we assume that the entire evidence thus far is our new vote matrix W . Note that we can iteratively submit batches of votes to improve our prediction of the maximum object. As before, we assume that the response to each vote is i.i.d. correct with probability p. We define the Next Votes Problem as follows: PROBLEM 2 (NEXT VOTES). Given b, W , select b additional votes and predict the maximum object in O, π * -1 (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Maximum Likelihood</head><p>We first present a Maximum Likelihood (ML) formulation of the selection of votes for the Next Votes Problem; we directly compute the multiset of votes which most improves the prediction of the maximum object in O. Assuming that average worker accuracy p is known, the ML vote selection formulation we present is the optimal feasible solution to the Next Votes Problem. Beforing presenting the ML formulation, we first provide some definitions needed for the Next Votes Problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vote and Answer Multisets:</head><p>We represent a potential vote (comparison) between objects oi and oj as a unordered pair {oi, oj }. Given a vote budget b, all possible multisets Q of b votes are allowed (note that repetition of votes is allowed). For a potential vote {oi, oj }, we define an answer to be a tuple ({oi, oj}, ox), where the first element of the tuple is an unordered pair, and the second element is one of the objects in the pair indicating the human worker's answer (e.g., x = i if the worker states that oi is greater than oj , or x = j otherwise).</p><p>For each vote multiset Q, we define an answer multiset a of Q to be a multiset of answer tuples, where there is a one-to-one mapping from each unordered pair in Q to an answer tuple in a. Note that each vote is answered (independently) with probability p. As an example, if Q = {{oi, oj}, {o k , o l }}, a possible answer multiset a that could be received from the workers is {({oi, oj }, oi), ({o k , o l }, o k )}. Note that for a multiset of b votes, there are 2 b possible answer multisets. Let A(Q) denote the multiset of all possible answer multisets of Q.</p><p>Having defined vote and answer multisets, we next consider the probability of receiving an answer multiset given W , then explain how to compute the confidence of the maximum object having received an answer multiset, before finally presenting the ML vote selection strategy. Probabilities of Multisets and Confidences: Suppose that we submitted vote multiset Q and received answer multiset a from the crowdsourcing marketplace. Let P (a|W ) denote the probability of observing an answer multiset a for Q, given initial vote matrix W . We have the following:</p><formula xml:id="formula_22">P (a|W ) = P (a ∧ W ) P (W ) (<label>8</label></formula><formula xml:id="formula_23">)</formula><p>where a∧W is the new vote matrix formed by combining the votes of a and W .</p><p>Our estimate for how well we are able to predict the maximum object in O is then the probability of the maximum object, given the votes of our new vote matrix, i.e., a ∧ W . We denote this value by Pmax(a ∧ W ), i.e., this value is our confidence in the maximum object. The computation, based upon Equation <ref type="formula" target="#formula_7">4</ref>, is the following:</p><formula xml:id="formula_24">Pmax(a ∧ W ) = max i P (π -1 (1) = i|a ∧ W )</formula><p>This simplifies to give:</p><formula xml:id="formula_25">Pmax(a ∧ W ) = maxi P (π -1 (1) = i, a ∧ W ) P (a ∧ W )<label>(9)</label></formula><p>Maximum Likelihood Strategy: We can now define the Maximum Likelihood formulation of the Next Votes Problem. We wish to find the multiset Q of b votes such that, on average over all possible answer multisets for Q (and weighted by the probability of those answer multisets), our confidence in the prediction of the maximum object is greatest.</p><p>In other words, we want to find the multiset that maximizes:</p><formula xml:id="formula_26">X a∈A(Q) P (a|W ) × Pmax(a ∧ W )</formula><p>which, on using Equations 8 and 9, simplifies to:</p><formula xml:id="formula_27">1 P (W ) × X a∈A(Q) max i P (π -1 (1) = i, a ∧ W ) Since P (W ) is a constant, independent of Q, we have: ML FORMULATION 2 (NEXT VOTES). Given b, W , find the vote multiset Q, |Q| = b, that maximizes X a∈A(Q) max i P (π -1 (1) = i, a ∧ W )<label>(10)</label></formula><p>Let score(Q) be the value in Equation <ref type="formula" target="#formula_27">10</ref>. We now have an exhaustive strategy to determine the best multiset Q: compute score(•) for all possible multisets of size b, and then choose the multiset with the highest score. Although this strategy is the optimal feasible solution to the Next Votes Problem, it is also computationally infeasible, since a single iteration of ML itself requires enumeration of all n! permutations of the objects in O. Additionally, knowledge of worker accuracy p is required for ML vote selection. This leads us to develop our own vote selection and evaluation framework enabling more efficient heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational Complexity</head><p>As in the Judgment Problem, the Next Votes Problem also out to be NP-Hard, while the computation of the probabilities involved also turns out to be #P-Hard. While the proofs use reductions from similar problems, the details are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardness of the Next Votes Problem:</head><p>We first show that the ML formulation for the Next Votes Problem is NP-Hard, implying that finding the optimal set of next votes to request is intractable. We are given a graph G where we wish to find a Kemeny winner. We add an extra vertex v to this graph to create a new graph, G , where v does not have any incoming or outgoing arcs. By definition, v is a Kemeny winner in G , since trivially, v can be placed anywhere in the permutation without changing the number of arcs that are respected. Therefore, there are at least two Kemeny winners in G . Recall, however, that our goal is to return a Kemeny winner in G , not in G. Now, consider the solution to the Next Votes problem on G , where an additional vote is requested. As in the proof of Theorem 1, we set p to be very close to 1. It can be shown that the solution to the Next Votes problem on G consists of two vertices, such that they are both Kemeny winners on G . (Note that v may be one of the vertices, but at least one more vertex is returned.) These vertices (if they are not v) are also Kemeny winners in G. Thus, the Kemeny winner determination problem on G can be reduced to the Next Votes (with one vote) problem on G . The details of this proof are available in the extended technical report <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THEOREM 3 (HARDNESS OF NEXT VOTES). Finding the vote multiset</head><formula xml:id="formula_28">Q that maximizes P a∈A(Q) maxi P (π -1 (1) = i, a∧W ) is NP-Hard,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#P-Hardness of Probability Computations:</head><p>We next show that computing Equation 10 is #P-Hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THEOREM 4 (#P-HARDNESS OF NEXT VOTES). Computing</head><formula xml:id="formula_29">P a∈A(Q) maxi P (π -1 (1) = i, a ∧ W ) is #P-Hard, even for a Q with a single vote.</formula><p>PROOF. (Sketch) Our proof uses a reduction from the #P-Hard problem of counting linear extensions in a DAG. Consider a DAG G = (V, A). We now add two additional vertices, x and y, such that there is an arc from each of the vertices in G to x and to y giving a new graph G = (V , A ). Consider the computation of P a∈A(Q) maxi P (π -1 (1) = i, a∧ W ) for Q = {{x, y}} for G , which simplifies to: maxi P π:i wins P (W ∧ (x &gt; y)|π) + maxi P π:i wins P (W ∧ (y &gt; x)|π). The first of these two terms is maximized when x is the maximum, and the second term is maximized when y is the maximum. Both terms are identical, since x and y are identical, so we focus on only one of the terms. Let F (p) = P π:x wins P (W ∧ x &gt; y|π). Using a calculation similar to that used to derive Equation <ref type="formula" target="#formula_11">5</ref>, we have:</p><formula xml:id="formula_30">F (p) = p |A | × P |A | i=0 ai( 1-p p ) |A |-i .</formula><p>We are interested in a |A | , the number of permutations that correspond to linear extensions. Once again, by repeating the trick in Theorem 2, we may use multiple values for p to generate different graphs G , and use the probability computation to derive many equations F (p) corresponding to different p, and then derive the value of a |A | using Lagrange's interpolation.</p><p>Therefore, counting the number of linear extensions in G can be reduced to a polynomial number of instances of computing the probability expression corresponding to the Next Votes problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Selection and Evaluation of Additional Votes</head><p>We next present a general framework to select and evaluate additional votes for the Next Votes Problem. Our approach is the following:</p><p>1. score all objects with a scoring function s using initial vote matrix W 2. select a batch of b votes to request 3. evaluate the new matrix W (initial votes in W and additional b votes) with a scoring function f to predict the maximum object in O. This framework is displayed in more detail in Algorithm 3. In Step 1, we use a scoring function s(•) to score each object, and in Step 3, we use a scoring function f (•) to evaluate the new matrix W to predict the maximum object in O. We briefly discuss the choice of these scoring functions when presenting experimental results later in Section 3.4. For now, we assume the use of a scoring function in Step 1 which scores objects proportional to the probability that they are the maximum object in O. It is important to note that our general framework assumes no knowledge of worker accuracy p, unlike in ML vote selection. We next focus our attention upon how to select b additional votes (Step 2). Heuristic Vote Selection Strategies: How should we select pairs of objects for human workers to compare, when given a vote budget of b votes? Since ML vote selection is computationally infeasible, we consider four efficient polynomial-time vote selection strategies: Paired, Max, Greedy, and Complete Tournament strategies. For ease of explanation, we use the graph in Figure <ref type="figure" target="#fig_4">4</ref> as an example. Before executing a vote selection strategy, we assume that each object has been scored by a scoring function in Step 1 of the framework, denoted by s[•] in Algorithm 3. As a running example to explain our strategies, we assume that our PageRank heuristic (Section 2.4) is used as the scoring function in Step 1: object A has score 0.5, objects B and E each have score 0.25, and objects C, D, and F have score 0. Without loss of generality, assume that the final rank order of the objects, before next vote selection, is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 General Vote Selection Framework</head><formula xml:id="formula_31">(A, B, E, C, D, F ).</formula><p>The first strategy we consider is Paired vote selection (PAIR). In this strategy, pairs of objects are selected greedily, such that no object is included in more than one of the selected pairs. For example, with a budget of b = 2, the strategy asks human workers to compare the rank 1 and rank 2 objects, and the rank 3 and rank 4 objects, where rank is determined by the scoring function from</p><p>Step 1 in Algorithm 3. The idea behind this strategy is to restrict each object to be involved in at most one of the additional votes, thus distributing the b votes among the largest possible set of objects. This can be anticipated to perform well when there are many objects with similar scores, e.g., when there are many objects in the initial vote graph Gv which have equally high chances of being the maximum object. Considering the example in Figure <ref type="figure" target="#fig_4">4</ref>, for b = 2, this strategy requests the votes (A, B) and (E, C).</p><p>The second strategy we consider is Max vote selection (MAX). In this strategy, human workers are asked to compare the top-ranked object against other objects greedily. For example, with a budget of b = 2, this strategy asks human workers to compare the rank 1 and rank 2 objects, and the rank 1 and rank 3 objects, where rank is determined by the scoring function in Step 1 in Algorithm 3. Considering again the example in Figure <ref type="figure" target="#fig_4">4</ref>, for b = 2, this strategy requests the votes (A, B) and (A, E).</p><p>The third strategy we consider is Greedy vote selection (GREEDY). In this strategy, all possible comparisons (unordered object pairs) are weighted by the product of the scores of the two objects, where the scores are determined in Step 1 of Algorithm 3. In other words, a distribution is constructed across all possible object pairs, with higher weights assigned to object pairs involving high scoring objects (which are more likely to be the maximum object in O). After weighting all possible object pairs, this strategy submits the b highest weight pairs for human comparison. Considering the example in Figure <ref type="figure" target="#fig_4">4</ref>, object pairs (A, B) and (A, E) has weight 0.125, (B, E) has weight 0.0625, and all other pairs have weight 0. For a budget b = 2, this strategy requests the votes (A, B) and (A, E).</p><p>The fourth strategy we consider is Complete Tournament vote selection (COMPLETE). In this strategy, we construct a single roundrobin tournament among the K objects with the highest scores from Step 1 of Algorithm 3, where K is the largest number such that</p><formula xml:id="formula_32">K * (K+1) 2 ≤ b.</formula><p>In a single round-robin tournament, each of the K objects is compared against every other exactly once. For the remaining r = b -K * (K+1) 2 votes, we consider all object pairs containing the (K + 1)st (largest scoring) object and one of the first K objects, and weight each of these K object pairs by the product of the scores of the two objects (as we did with Greedy vote selection). We then select the r object pairs with highest weight.</p><p>The idea behind the Complete Tournament strategy is that a roundrobin tournament will likely determine the largest object among the set of K objects. If the set of K objects contains the true max, this strategy can be anticipated to perform well. Regarding the selection of the remaining votes, the strategy can be thought of as augment- ing the K object tournament to become an incomplete K +1 object tournament, where the remaining votes are selected greedily to best determine if the (K +1)st object can possibly be the maximum object in O. Considering the example in Figure <ref type="figure" target="#fig_4">4</ref>, for b = 2, there is a 2-object tournament among objects A and B and vote (A, B) is requested. Then, for the remaining vote, the strategy greedily scores object pairs which contain both the next highest ranked object not in the tournament, object E, and one of the initial 2 objects. Object pair (A, E) will be scored 0.125 and (B, E) will be scored 0.0625, so the second vote requested is (A, E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments</head><p>Which of our four vote selection heuristics (PAIR, MAX, GREEDY, or COMPLETE) is the best strategy? We now describe a set of experiments measuring the prediction performance of our heuristics for various sets of parameters. When evaluating our vote selection strategies, we utilized a uniform vote sampling procedure, described previously in Section 2.5, to generate an initial vote matrix W . Then, in Step 1 of our vote selection framework (Algorithm 3), we adopted our PageRank heuristic (Section 2.4) as our scoring function s(•) to score each object in O. In Step 2, we executed each of our vote selection strategies using these scores. In Step 3, we used our PageRank heuristic as our scoring function f (•) to score each object in the new matrix W (composed of both the initial votes in vote matrix W and the b requested additional votes), and generate final predictions for the maximum object in O. Note that we performed several experiments contrasting prediction performance of PageRank versus other possible scoring functions and found PageRank to be superior to the other functions. Hence, we selected PageRank as the scoring function for both Step 1 and Step 3 of our vote selection framework.</p><p>• ML vote selection outperforms heuristic strategies when results are evaluated with ML scoring. • However, when ML vote selection is evaluated with PageRank (e.g., like the heuristics), prediction performances of all methods are similar.</p><p>For a first experiment, we compare the prediction performance (Precision at 1) of our four vote selection heuristics (and random initial vote selection (RAND)) against the "optimal" strategy, i.e., the Maximum Likelihood (ML) vote selection procedure described in Section 3.1. Recall that ML can be used in two places: when selecting additional votes (as in Section 3.1), and when predicting the max given the initial plus additional votes (e.g., ML evaluation in Section 2.2). We use ML-ML to refer to using ML for both tasks, this gives the best possible strategy. To gain additional insights, we also consider ML-PR, a strategy where ML is used to select the additional votes, and PageRank is used to select the winner. Since ML is computationally very expensive, for this experiment we consider a small problem: select one additional vote given a set of 50 (2.5x Our experimental results are displayed in Figure <ref type="figure" target="#fig_6">5</ref>. First, as expected, ML-ML has the best performance. Clearly, ML-ML is doing a better job at selecting the additional vote and in selecting the winner. Of course, keep in mind that ML-ML is not feasible in most scenarios, and it also requires knowledge of the worker accuracy p. Nevertheless, the gap between ML-ML and the other strategies indicates there is potential room for future improvement beyond the heuristics we have developed.</p><p>Second, we observe in Figure <ref type="figure" target="#fig_6">5</ref> that all other strategies, including ML-PR, perform similarly. The relative performance of ML-PR indicates that the gain achieved by ML-ML is due to its better prediction of the winner, as opposed to its choice for the next vote. In hindsight, this result is not surprising, since the selection of a single vote cannot be expected to have a large impact. (We will observe larger impacts when we select multiple additional votes.) The results also demonstrate that our vote selection heuristics show promise, since they seem to be doing equally well as ML, and since they often perform slightly better than RAND, at least for the selection of a single next vote.</p><p>To evaluate our heuristics in larger scenarios, we conducted a series of experiments, and the results of some of those are summarized in Figures <ref type="figure">6,</ref><ref type="figure">7</ref>(left) and 8. To begin, we summarize some of the general trends that can be observed in these figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General observations regarding all strategies:</head><p>• As the number of additional votes increases, prediction performance increases. • As the number of additional votes increases, the gain from additional votes decreases (though the decrease is not very dramatic). • As worker accuracy increases, prediction performance increases. • As worker accuracy increases, the gain from additional votes increases.</p><p>We only explain the graph in Figure <ref type="figure">6</ref>(right), since the others are self-explanatory. In this graph, the vertical axis shows the incremental P@1 gain at k additional votes, defined as (P@1 with k additional votes -P@1 with k -1 additional votes) / (P@1 with 0 additional votes). As we can see, the information provided by  additional votes is more valuable when there are fewer initial votes (second bullet above).</p><p>The Complete Tournament and Greedy strategies are significantly better than the Max and Paired strategies.</p><p>We can also use Figures <ref type="figure">6,</ref><ref type="figure">7</ref>(left) and 8 to compare our heuristics. First, notice that the difference between heuristics can be very significant. For instance, in Figure <ref type="figure" target="#fig_8">8</ref>(left) we see that the Paired (PAIR) strategy provides a 0.7x P@1 gain for 5 additional votes (100 initial votes, p = 0.95), while the Complete Tournament (COMPLETE) strategy provides a 1.5x P@1 gain, where we measure P@1 gain as (P@1 with k votes -P@1 with 0 votes) / (P@1 with 0 votes). Second, we observe that the Complete Tournament and Greedy (GREEDY) vote selection strategies consistently outperform the Max (MAX) and Paired strategies in all scenarios.In particular, in Figure <ref type="figure">6</ref>(left), we observe that the prediction performances of the Complete Tournament and Greedy strategies steadily improve with additional votes, while the Max and Paired strategies taper off. This indicates that when a larger vote budget b is available for additional votes, the additional votes will be better utilized by the more sophisticated strategies (Complete Tournament and Greedy) as compared to the simpler strategies (Max and Paired).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given only votes between objects of the same type:</head><p>• The value of additional votes is greater when it is more difficult to predict the maximum object. • The Complete Tournament strategy is the best strategy.</p><p>In our scenarios so far, the Complete Tournament and Greedy strategies perform similarly. To differentiate between the two, we explored different ways in which the initial votes could be generated. (Recall that up to this point we have been randomly selecting the pairs of objects that are compared by the initial votes.) We next discuss one of these possible different vote generation schemes. Suppose that our objects are of different types (e.g., softcover books, hardcover books, e-books, etc.), and for some reason initial votes between objects of the same type are much more likely than across types. For example, it is more likely that two e-books have been compared, rather than one e-book and one hard-cover book. (The situation is analogous to sporting events, where intraleague games are more likely than inter-league games.)</p><p>For our experiment, we consider an extreme instance where there are no initial votes involving objects of different types. In particular, we divide our set O of n objects into k disjoint object types. When votes are sampled for the initial vote matrix W , sampling of votes is only permitted between objects of the same type. Keep in mind that predicting the maximum object in O is more difficult when there are more object types because each object type will likely have a leader (greatest object), each of these leaders will have on average similar probabilities of being the maximum object (since object type groups are likely of similar size), and the initial vote matrix W provides no information regarding comparisons between these leaders.</p><p>We perform experiments for different values of k (e.g., different numbers of initial object types), Figure <ref type="figure" target="#fig_9">9</ref> displays Precision at 1 gain relative to a 0 additional votes baseline for k = 1 and k = 20. See the extended technical report <ref type="bibr" target="#b15">[15]</ref> for comprehensive results. We observe that the P@1 gain increases for the Complete Tournament and Greedy strategies as k increases, implying that the value of additional votes is greater when it is more difficult to predict the maximum object from the initial vote matrix. That is, in the harder problem instances (larger k), the additional votes play a more critical role in comparing the object type leaders. More importantly, the Complete Tournament strategy outperforms the Greedy strategy (and the others too) in this more challenging scenario.</p><p>Finally, we conduct a more in-depth study of the Complete Tournament vote selection strategy and examine the benefit of vote redundancy. Given a limited budget, should the Complete Tournament strategy select fewer top objects and propose more redundant votes, or should it select more objects and ask fewer votes per pair? For instance, the Complete Tournmament strategy could select the top three objects and submit four votes for each pair, for a total of 12 additional votes. Or it could select the top 4 objects, and for each of the possible 6 comparisons, request 2 votes (for the same 12 total additional votes). What is the best approach?</p><p>Figure <ref type="figure">7</ref>(right) displays the Precision at 1 of the Complete Tournament strategy for 10 additional votes, where the votes are uniformly and randomly distributed among the 5, 10, or 20 objects in O with highest score (as provided in Step 1 of Algorithm 3). We find that distributing the 10 votes among 5 objects, where each object is compared against every other, leads to the best prediction performance. That is, we do not observe any benefit for distributing votes among a larger set objects when using the Complete Tournament strategy. The strategy performs well only when additional votes provide the ability to rank the objects in a set. Assuming that votes are distributed randomly among object pairs, Complete Tournament strategy is able to order the set only when most objects in the are compared against each other. Note that Figure <ref type="figure">7</ref>(right) is only an illustration of the interaction between the number of top objects selected, and the redundancy of votes. The results will vary depending upon worker accuracy and the vote budget b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>As far as we know, we are the first to address the Next Votes Problem, and there is no relevant literature that directly addresses this problem. Thus, in this section, we review work related to the Judgment Problem. The algorithms and heuristics we presented for the Judgment Problem are primarily drawn from three diverse topic areas: paired comparisons, social choice, and ranking.</p><p>The Judgment Problem has its roots in the paired comparisons problem, first studied by statisticians decades ago <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b11">11]</ref>. In the paired comparisons problem, given a set of pairwise observations regarding a set of objects, it is desired to obtain a ranking of the objects. In contrast, in the Judgment Problem, we are interested in predicting the maximum object.</p><p>The Judgment Problem also draws upon classical work in the economic and social choice literature regarding Winner Determination in elections <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b31">31]</ref>. Numerous voting rules have been used (Borda, Condorcet, Dodgson, etc.) to determine winners in elec-tions <ref type="bibr" target="#b28">[28]</ref>. The voting rules most closely related to our work are the Kemeny rule <ref type="bibr" target="#b19">[19]</ref> and Slater rule <ref type="bibr" target="#b29">[29]</ref>. A Kemeny permutation minimizes the total number of pairwise inconsistencies among all votes, whereas a Slater permutation minimizes the total number of pairwise inconsistencies in the majority-vote graph <ref type="bibr">[6]</ref>. An object is considered a Kemeny winner or Slater winner if it is the greatest object in a Kemeny permutation or Slater permutation.</p><p>We believe our ML formulation is more principled than these voting rules, since ML aggregates information across all possible permutations. For example, in the graph of Figure <ref type="figure" target="#fig_0">1</ref>, while C and D are both admissible solutions for the Kemeny rule, ML returns D as an answer, since D has almost one and a half more times the probability of being the maximum object compared to C. No prior work about the Judgment Problem, to our knowledge, uses the same approach as our ML formulation.</p><p>In the recent social choice literature, the research most closely related to ours has been work by Conitzer et al. regarding Kemeny permutations <ref type="bibr" target="#b9">[9]</ref>. Conitzer has studied various voting rules and determined for which of them there exist voter error models where the rules are ML estimators <ref type="bibr" target="#b8">[8]</ref>. In our study, we focused upon the opposite question: for a specific voter error model, we presented both Maximum Likelihood, as well as heuristic solutions, to predict the winner.</p><p>Our work is also related to research in the theory community regarding ranking in the presence of errors <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b1">1]</ref> and noisy computation <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b2">2]</ref>. Both Kenyon et al. and Ailon et al. present randomized polynomial-time algorithms for feedback arc set in tournament graphs. Their algorithms are intended to approximate the optimal permutation, whereas we seek to predict the optimal winner. Feige et al. and Ajtai et al. present algorithms to solve a variety of problems, including the Max Problem, but their scenarios involve different comparison models or error models than ours.</p><p>More generally, in the last several years, there has been a significant amount of work regarding crowdsourcing systems, both inside <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b16">16]</ref> and outside <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> the database community. Of note is recent work by Tamuz et al. <ref type="bibr" target="#b30">[30]</ref> on a crowdsourcing system that learns a similarity matrix across objects, while adaptively requesting votes. Not as much work has been done regarding general crowdsourcing algorithms <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27]</ref>. Instead, most algorithmic work in crowdsourcing has focused upon quality control <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In a conventional database system, finding the maximum element in a set is a relatively simple procedure. It is thus somewhat surprising that in a crowdsourcing database system, finding the maximum is quite challenging, and there are many issues to consider. The main reason for the complexity, as we have seen, is that our underlying comparison operation may give an incorrect answer, or it may even not complete. Thus, we need to decide which is the "most likely" max (Judgment Problem), and which additional votes to request to improve our answer (Next Votes Problem).</p><p>Our results show that solving either one of these problems optimally is very hard, but fortunately we have proposed effective heuristics that do well. There is a gap between the optimal solution and what our heuristics find (as seen for example in Figure <ref type="figure" target="#fig_6">5</ref>), but we believe that it will be very hard to close this gap without incurring high computational costs. Among the heuristics, we observed significant differences in their predictive ability, indicating that it is very important to carefully select a good heuristic. Our results indicate that in many cases (but not all) our proposed PageRank heuristic is the best for the Judgment Problem, while the Complete Tournament heuristic is the best for the Next Votes Problem.</p><p>Our results are based on a relatively simple model where object comparisons are pairwise, and worker errors are independent. Of course, in a real crowdsourced database system these assumptions may not hold. Yet we believe it is important to know that even with the simple model, the optimal strategies for the Judgment Problem and Next Votes Problem are NP-Hard. Furthermore, our heuristics can be used even in more complex scenarios, since they do not depend on the evaluation model. Even though they can be used when the model assumptions do not hold, we believe it is important to understand how the heuristics perform in the more tractable scenario we have considered here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: How should these objects be ranked? Vote matrix (left) and equivalent graph representation (right). Arc weights indicate number of votes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Precision at 1 (P@1) versus Edge Coverage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>even for a single vote.PROOF. (Sketch) Our proof for the Next Votes problem uses a reduction from the same NP-Hard problem described in Section 2.3, i.e., determining Kemeny winners.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: How should we select additional votes to request?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Require: n</head><label></label><figDesc>objects, vote matrix W , budget b Ensure: ans = predicted maximum object compute score s[•] for all objects using function s {Step 1} initialize multiset Q {of votes to request} sort all objects by s[•], store object indices in index[•] select b votes for Q using a vote selection strategy {Step 2} submit batch Q update W with new votes from workers compute final score f [•] for all objects using function f {Step 3} ans ← argmax i f [i]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision at 1 versus number of initial votes. 1 additional vote, 7 objects, p=0.75.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Precision at 1 versus number of additional votes (left). Incremental Gain (P@1) of each vote relative to a 0 additional votes baseline (right). 100 objects, p=0.95, 200 initial votes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Gain (P@1) relative to a 0 additional votes baseline vs number of initial votes. 100 objects, p=0.95. 5 add. votes (left), 15 add. votes (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Objects are divided into k initial object types. Gain (P@1) relative to a 0 additional votes baseline vs number of initial votes. 100 objects, p=0.95, 15 additional votes. 1 type (left), 20 types (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1-p  p , thus, we may simply choose |A | + 1 different values of1-p  </figDesc><table /><note><p>p , generate |A | +1 different graphs G , and use the probability computation in Equation</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>, e.g. 1x Edge Coverage.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This research has been supported by NSF grant IIS-1009916.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregating inconsistent information: ranking and clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sorting and selection with imprecise comparisons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automata, Languages and Programming</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crowdsourcing for relevance evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">PageRank as a function of the damping factor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PageRank as a weak tournament solution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WINE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A short introduction to computational social choice</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chevaleyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOFSEM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to order things</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="270" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Common voting rules as maximum likelihood estimators</title>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved bounds for computing kemeny rankings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ordering by weighted number of wins gives a good ranking for weighted tournaments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coppersmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The method of paired comparisons</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranking from unbalanced paired-comparison data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="436" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing with noisy information</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1001" to="1018" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CrowdDB: answering queries with crowdsourcing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">So who won? Dynamic max discovery with the crowd</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="http://ilpubs.stanford.edu:8090/1032/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Turkalytics: analytics for human computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heymann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the complexity of Slater&apos;s problems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="216" to="221" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quality management on amazon mechanical turk</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Human Computation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mathematics without numbers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kemeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the method of paired comparisons</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1940">1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to rank with few errors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kenyon-Mathieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowdsourcing user studies with mechanical turk</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TurKit: human computation algorithms on mechanical turk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-powered sorts and joins</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Choosing from a tournament</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Choice and Welfare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="271" to="291" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-assisted graph search: it&apos;s okay to ask questions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Basic geometry of voting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inconsistencies in a schedule of paired comparisons</title>
		<author>
			<persName><forename type="first">P</forename><surname>Slater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptively learning the crowd kernel</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tamuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal voting rules</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econ. Perspectives</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
