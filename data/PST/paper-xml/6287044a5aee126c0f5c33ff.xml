<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Reliable Multimodal Stress Detection under Distribution Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Foltyn</surname></persName>
							<email>andreas.foltyn@iis.fraunhofer.de</email>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Deuschel</surname></persName>
							<email>jessica.deuschel@iis.fraunhofer.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IIS</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Fraunhofer IIS</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Reliable Multimodal Stress Detection under Distribution Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3461615.3486570</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>stress estimation</term>
					<term>multimodal</term>
					<term>uncertainty estimation</term>
					<term>calibration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recognition of stress is an important issue from a health care perspective as well as in the human-computer interaction context. With the help of multimodal sensors, stress can be detected relatively well under laboratory conditions. However, when models are used in the real world, shifts in the data distribution can occur, often leading to performance degradation. It is therefore desirable that models in these scenarios are at least able to accurately capture this uncertainty and thus know what they do not know. This work aims to investigate how synthetic shifts in the data distribution can affect the reliability of a multimodal stress detection model in terms of calibration and uncertainty quantification. We compare a baseline with three known approaches that aim to improve reliability of uncertainty estimates. Our results show that all methods we tested improve the calibration. However, calibration generally deteriorates and spreads with stronger shifts for all approaches. They perform especially poorly for shifts in highly relevant modalities. Overall, we conclude that in the conducted experiments the investigated methods are not sufficiently reliable under distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Supervised learning by classification; Neural networks; • Human-centered computing → HCI theory, concepts and models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Affective state estimation plays an important role in making systems more empathic and thus improving human-computer interaction (HCI). From a health perspective, the detection of harmful long-term conditions can also be significant, as is the case with stress <ref type="bibr" target="#b30">[31]</ref>. These states are often expressed in various ways in humans and can thus be captured with different sensors, e.g. ECG and respiration rate. Current systems for estimating states such as emotions, stress, or cognitive load generally work more robustly when these modalities are combined. Although it has been shown that machine learning models can discriminate affective states to some degree when the data is recorded in a similar environment <ref type="bibr" target="#b3">[4]</ref>, there is a lack of research on the robustness of multimodal models when the data distribution of the inputs changes. These distribution shifts often violate the underlying assumption that the data is independently and identically distributed (i.i.d.) which can lead to a collapse of the performance during deployment. Shifts can occur in various ways, e.g. signal noise, missing modalities, subpopulation shifts <ref type="bibr" target="#b28">[29]</ref> or spurious correlations within the training data <ref type="bibr" target="#b2">[3]</ref>. To deal with this challenge, a system should not merely rely on the discrete classification outputs, but also consider the probability distribution over the classes. For example, with very uncertain predictions, a system could ask the user for feedback. However, to trust these uncertainty estimates, they must be well-calibrated and reliable. This means that the predicted probability of correctness, i.e. the confidence outputs of the model, should match its empirical probability of correctness, i.e. the actual accuracy. In order to improve the calibration of machine learning models, different approaches were presented in the literature which can generally be divided into three perspectives <ref type="bibr" target="#b6">[7]</ref>. Regularization methods modify the training process in order to improve calibration. This includes the use of data augmentation techniques like Mixup <ref type="bibr" target="#b35">[36]</ref>. Post-Hoc approaches try to adjust the calibration after the training by applying a recalibration process using a validation dataset under the assumption that its data distribution is similar to the distribution of the data for inference <ref type="bibr" target="#b7">[8]</ref>. The last perspective considers structural changes of the model, e.g. Bayesian neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref> or deep ensembles <ref type="bibr" target="#b19">[20]</ref>. Large-scale studies <ref type="bibr" target="#b26">[27]</ref> comparing these approaches show that while the calibration performs well within the training distribution, it deteriorates as the data distribution shifts. It is important to further explore this behavior on shifted data, especially for multimodal and sequential datasets. So far the main focus of the research examining distributional shifts with respect to the reliability of uncertainty estimates is mostly on unimodal data. They range from theoretical investigations on simple regression tasks on tabular data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> to image classification tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b16">17]</ref>. For physiological sensory data most literature focuses on the robustness of the classification performance. For example, for stress detection Hovsepian et al. <ref type="bibr" target="#b14">[15]</ref> investigated the robustness of a model that was developed in laboratory conditions and then also evaluated in the field. Consequently, in this paper we investigate the reliablity of uncertainty estimates of a multimodal model for stress detection under distribution shifts. To this end, we first create a synthetic shift for an existing multimodal dataset. To do this, we use the WESAD <ref type="bibr" target="#b29">[30]</ref> dataset and generate perturbations in each modality at inference time. We then explore three approaches, in addition to a baseline deep neural network, that can be used to improve uncertainty estimates: deep ensembles, Mixup, and Focal Loss. We present a detailed analysis of the behavior of these approaches over different shift intensities. The contribution of this work consists therefore especially in the systematic investigation of distribution shifts for multimodal physiological data with respect to the reliability of the uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACHES</head><p>We compare three approaches commonly used to improve the reliability of uncertainty estimates, in addition to a simple baseline model. In doing so, we want to cover three different perspectives: Structural changes, modification of the loss function and regularization. In the following we give a formal description of the setting and all methods. Let (x, y) be a data-label pair from an input space x ∈ X ⊆ R d and label space y ∈ Y ⊆ K = {0, 1} for a binary problem. Further, let f θ (x) ∈ R k be the embedding of the data point x using a neural network architecture with weights θ and let P θ (y|x) be the probability for predicting label y.</p><p>Deep ensembles. From the structural point of view, we apply deep ensembles, proposed by <ref type="bibr" target="#b19">[20]</ref>, which have shown great success in terms of calibration on data shifts <ref type="bibr" target="#b26">[27]</ref>. As training a deep neural network is a highly stochastic process, deep ensembles are created to consider several perspectives: Multiple models f θ with θ ∈ Θ = {θ 1 , . . . , θ n } are trained with the same architecture but different parameter initializations. The confidence values output by the network are then averaged, thus P Θ (y|x) = 1 |Θ| θ ∈Θ P θ (y|x). Ideally, each member f θ of the ensemble converges towards a different local minimum of the loss landscape, such that the resulting predictive distribution p Θ (y|x) expresses the uncertainty. We investigate both three-and five-fold ensembles.</p><p>Focal Loss. Considering the loss-perspective, Mukhoti and Kulharia et al. <ref type="bibr" target="#b25">[26]</ref> have pointed out that the Focal Loss can improve calibration. Contrary to cross-entropy, Focal Loss weights each sample of a batch by the individual classification error of the model. Mathematically it is defined by <ref type="bibr" target="#b20">[21]</ref> as</p><formula xml:id="formula_0">L = −(1 − P θ (y|x)) γ log P θ (y|x)<label>(1)</label></formula><p>with hyperparamter γ &gt; 0, which we find using a hyperparameter search. As argued and empirically shown by <ref type="bibr" target="#b25">[26]</ref>, Focal Loss leads to a less peaked and therefore less over-confident prediction compared with the usual confidence output obtained by training with crossentropy loss.</p><p>Manifold Mixup. In terms of regularization, we use Manifold Mixup <ref type="bibr" target="#b33">[34]</ref>. Mixup is an augmentation technique that creates a soft, weighted interpolation</p><formula xml:id="formula_1">z = γ z 1 + (1 − γ )z 2 y = γy 1 + (1 − γ )y 2<label>(2)</label></formula><p>of two random samples z 1 , z 2 and their one-hot labels y 1 , y 2 within each batch, with interpolation weight γ ∼ Beta(α, α), (γ ∈ [0, 1]) and parameter α ≥ 0. Mixup is originally created for computer vision to mix two samples on the input level. Since we work with time-series data we apply Mixup on the embedding space, thus z j = f θ (x j ), j ∈ {0, 1}. Training with these linear interpolations and the soft labels yields a smooth decision boundary in the embedding space which has shown to improve the calibration of the uncertainty estimates obtained from the outputs <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP 3.1 Data</head><p>For our analysis, we use the multimodal dataset WESAD <ref type="bibr" target="#b29">[30]</ref>, in which physiological data were recorded from 15 individuals, each of whom going through three phases: a rest phase (20 minutes), an entertainment phase (392 seconds), and a stress phase (about 10 minutes). Stress was induced using the Trier Social Stress Test <ref type="bibr" target="#b15">[16]</ref>. The recorded sensors include BVP, ECG, EDA, EMG, respiration, body temperature, and three-axis acceleration. For our experiments we do not include BVP and acceleration. Furthermore, we use only the baseline and stress phase, resulting in a binary annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distribution Shifts</head><p>Several naturally occurring shifts could be used for the the underlying multimodal sensor setup, e.g. sensor-specific artifacts caused by movements, misplacement <ref type="bibr" target="#b27">[28]</ref> or changes in the subpopulation, e.g. gender or age. For simplicity, we focus on general synthetically generated shifts applicable to all time series data. For this purpose, we use two types of corruptions, each applied to one of the five available modalities in the test set. Our first shift consists of random Gaussian noise parameterized with the mean and standard deviation of the input signal. The second shift represents missing data which is realized by replacing parts of the signal by zeros. The shifts are applied both sequentially and randomly with an increasing intensity expressed by various percentages of affected data p ∈ {0.25, 0.5, 0.75, 1}. In case of sequential application, Gaussian noise or zeros are applied at the first p percent of the signal. With higher intensity levels the share of the signal on which noise is applied increases. At the last level the shift is applied on the entire signal of one modality each, which means that the entire signal of one modality at a time is missing. For the case of a random application of the shift, random positions of a signal become noisy or are replaced by zeros. Note that both sequential and random shifts are the same on the highest level of intensity, i.e. in both cases the entire signal is noisy or missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture</head><p>Our main focus is the investigation of end-to-end machine learning models under distribution shifts. Hence, we will not use any manual feature extraction using expert knowledge. As an architecture for the feature extraction we use an adaptation of the ResNet <ref type="bibr" target="#b10">[11]</ref> architecture for time series data by Wang et al. <ref type="bibr" target="#b34">[35]</ref>. For each modality, a 128 dimensional feature representation is learned. These are then concatenated and fused into a final fully-connected layer before applying a sigmoid activation function. Thus we rely on a simple intermediate fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Protocol</head><p>To make a more robust statement about the effectiveness of the approaches, we use a nested cross validation protocol. We divide the dataset subject-wise into 5 folds to avoid data leakage. 4 folds are used as a training set and the last is our test set. All reported metrics are averaged over these folds. The biosignals are divided into 20 seconds long non-overlapping sections and normalized using standard scaling. We use the full training dataset to find the normalization parameters, which are also applied to the test data. While personalized normalization may yield better results, adapting to new users during deployment may not be possible. We train all models for 25 epochs using the RAdam <ref type="bibr" target="#b22">[23]</ref> optimizer. To allow for a better comparison of the approaches, we use a hyperparameter search using a 3-fold cross validation on the training fold. The Tree-Structured Parzen Estimator <ref type="bibr" target="#b0">[1]</ref> with 20 trials is used for finding the optimal hyperparameter combination. With the best hyperparameter setting, we train the model on the complete training data with 5 random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation</head><p>We evaluate the methods with respect to the classification performance and reliability of uncertainty estimates. In order to measure the classification performance we consider the AUROC <ref type="bibr" target="#b9">[10]</ref>. To measure calibration, we report the Calibration Error defined as follows <ref type="bibr" target="#b18">[19]</ref>. Let д : f (X ) → K be the classifier. For our binary task д is the sigmoid function. The calibration error is</p><formula xml:id="formula_2">CE(д(f θ (X ))) = E |д(f θ (X )) − E [Y | д(f θ (X )) ]| 2 1 2 .</formula><p>(3)</p><p>For application we use the definition as given by <ref type="bibr" target="#b8">[9]</ref>, which approximates Eq. 3 by partitioning K ∈ [0, 1] into 15 equally-spaced bins and estimating the calibration error by calculating the weighted absolute difference between the average of д(f θ (X )) and Y in each bin. The last metric is motivated by the fact that in some situations we have the possibility to get user feedback, i.e. the system should be able to ask the user about the ground truth in situations where it is very uncertain. Consequently, in an optimal system, examples with higher uncertainty should have a higher error rate than examples with lower uncertainty. To quantify this behavior we use the rejection area ratio defined as RR = AR unc AR orc <ref type="bibr" target="#b24">[25]</ref>. For this, predictions in descending order of an uncertainty measure are replaced step by step by more ground truth labels. Since we have a binary output we use the variance of the Bernoulli distribution as an uncertainty measure. This gives us an uncertainty curve with decreasing error rate over the number of replaced samples. AR orc describes the area between the perfect curve, i.e. by replacing the uncertainty measures with error rates, and a random curve. AR unc describes the area between a random curve and the uncertainty curve. An in-depth explanation can be found in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>We show the impact of different corruptions on the test set with different levels of intensity in Figure <ref type="figure">2</ref>. Each boxplot summarizes all corruptions applied to the test set. It includes 4 types of corruptions applied to the 5 modalities respectively. Thus, each plot includes 20 data points averaged over all folds. First, we note that the median AUROC, CE and RR worsen as the distribution shifts while the variance increases. Considering the median AUROC, it is possible to give a clear ranking to the different methods: baseline, Mixup, Focal Loss, 3-fold and as best method 5-fold ensemble. In terms of variance this is not as easy. Overall, we can see that the more corruption there is in the data, the less the performance of a single model can be trusted, as it may vary between 0.82 and 0.91 in the case of the baseline method for intensity 4. For the other methods the variance is smaller than for the baseline, especially Focal Loss gives the most consistent results. However we can see considerable outliers for all methods that will be investigated below. In terms of calibration error and RR, our results indicate that all methods outperform the baseline at the median. The only exception is the Focal Loss which shows a poor performance in terms of RR, however the best in terms of CE. Furthermore, the results show that the ensemble methods overall outperform all others at the median, which is clearly visible for the RR. Also Mixup shows a considerable improvement over the baseline such that it is possible to conclude that these three methods can improve the calibration both in terms of CE and RR.</p><p>As our results show, some outliers are present for all methods and metrics. Most outliers tend to be in the direction of poorer performance. We have investigated the outliers and noticed that they are mainly caused by specific modalities, namely respiration and ECG. Therefore, we further investigate the spreads dependent on the modalities. The results can be seen in Figure <ref type="figure" target="#fig_1">3</ref>   with the results given in <ref type="bibr" target="#b29">[30]</ref>. Interestingly, the error decreases for more corruption or more missings in EDA or EMG. To investigate this behavior we analyzed the influence of EDA and EMG on the AUROC-performance. It decreases only minimally (less than 1%) with increasing distortion in both signals, and therefore they do not seem to have much contribution to the final classification performance in this specific experimental setup. We hypothesize that the distortions lower the confidence level overall which, together with only a minimal decrease in the AUROC, could result in better calibration, since most models have rather overconfident uncertainty estimates. However, it would be interesting to do further research on the dependencies among the modalities in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we compared methods for improving the calibration and uncertainty quantification of deep neural networks for the task of multimodal stress detection under synthetically generated distribution shifts. Our results indicate that across all investigated methods the predictive performance, calibration, and uncertainty estimates deteriorate and vary more with increasingly strong shifts. At the median, most methods, especially ensembles and Mixup, improve upon our baseline in all metrics, thus these are the most promising candidates for a well-calibrated uncertainty estimation.</p><p>Investigating the variance, however, we noticed that strong shifts in highly relevant modalities (ECG and respiration) lead to a particularly poor calibration performance over all methods. Thus, there are strong modality-dependent instabilities regarding the calibration of the model and even though all modalities exceeded the baseline in these modalities, none of the methods could provide a sufficiently reliable and calibrated uncertainty estimation for these critical shifts. Consequently, future work should investigate this challenge more deeply in order to avoid making the calibration of the entire model too dependent on individual modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the training and evaluation setting. For training the original, clean signal of every modality is embedded and fused. We apply different approaches during training, each depicted at their perspective respectively. For evaluation the signals are shifted one by one by two different shifts: zero out/missings and noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Calibration error over all intensity levels for a 5fold ensemble by modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>for a 5-fold ensemble and the CE. For the other methods we have seen the same CE changes more than when the shift increases for temperature. The curves also indicate which modalities have been most important for the classification and the calibration. Respiration and ECG have the most influence of the used modalities which aligns</figDesc><table><row><cell>Clean Approach Focal Ensemble-3 Intensity 1 Intensity 2 Intensity 3 Intensity 4 Corruption Level Ensemble-5 Clean Intensity 1 Intensity 2 Intensity 3 Intensity 4 Corruption Level Approach Baseline Focal MixUp Ensemble-3 Ensemble-5 Clean Intensity 1 Intensity 2 Intensity 3 Intensity 4 Corruption Level Approach Baseline Focal Mixup Ensemble 3 Ensemble 5 Figure 2: AUROC, Calibration Error and Rejection Ratio un-0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 AUROC ( ) 0.175 0.200 0.225 0.250 0.275 0.300 0.325 0.350 0.375 Calibration Error ( ) 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 Rejection Ratio ( ) der different levels of corruptions. behavior such that the plot is representative for all methods. The plot shows that different modalities have different influence on the CE: e.g. when the degree of shift increases on the respiration modality, Clean Mixup 0.18 Baseline 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 Calibration Error ( ) Modality ECG EDA Respiration Intensity 1 Intensity 2 Intensity 3 Intensity 4 Corruption Level Temperature EMG</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the Center for Analytics -Data -Applications (ADA-Center) within the framework of "BAYERN DIGITAL II" (20-3410-2-9-8).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for Hyper-Parameter Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>ArXiv abs/1505.05424</idno>
		<title level="m">Weight Uncertainty in Neural Networks</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Deaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Hormozdiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Mincu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akinori</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Nielson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">G</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harini</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Vladymyrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadlowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taedong</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<idno>ArXiv abs/2011.03395</idno>
		<title level="m">Underspecification Presents Challenges for Credibility in Modern Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can we ditch feature engineering? end-to-end deep learning for affect recognition from physiological sensor data</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Dzieżyc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Gjoreski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemysław</forename><surname>Kazienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Saganowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matjaž</forename><surname>Gams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">6535</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking importance weighting for deep learning under distribution shift</title>
		<author>
			<persName><forename type="first">Tongtong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04662</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>ArXiv abs/1506.02142</idno>
		<title level="m">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Gawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedrique</forename><surname>Rovile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Njieutcheu</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsin</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongseo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Humt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kruspe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">A Survey of Uncertainty in Deep Neural Networks</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On Calibration of Modern Neural Networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>ArXiv abs/1706.04599</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (ROC) curve</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016. 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16241</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Hovsepian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Absi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tom P Kamarck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<title level="m">cStress: towards a gold standard for continuous stress assessment in the mobile environment. Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The &apos;Trier Social Stress Test&apos;-a tool for investigating psychobiological stress responses in a laboratory setting</title>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Kirschbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl-Martin</forename><surname>Pirke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">H</forename><surname>Hellhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychobiology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shiori Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Wei Hua Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno>ArXiv abs/2012.07421</idno>
		<title level="m">WILDS: A Benchmark of in-the-Wild Distribution Shifts</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4485" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10155</idno>
		<title level="m">Verified uncertainty calibration</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balaji Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stable Adversarial Learning under Distributional Shifts</title>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8662" to="8670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>ArXiv abs/1908.03265</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Simple Baseline for Bayesian Uncertainty in Deep Learning</title>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Uncertainty estimation in deep learning with application to spoken language assessment</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="95" to="97" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jishnu</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amartya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Dokania</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09437</idno>
		<title level="m">Calibrating deep neural networks using focal loss</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Can you trust your model&apos;s uncertainty? Evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02530</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Rangaraj M Rangayyan</surname></persName>
		</author>
		<title level="m">Biomedical signal analysis</title>
				<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Shibani Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
		<title level="m">BREEDS: Benchmarks for Subpopulation Shift. arXiv: Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introducing wesad, a multimodal dataset for wearable stress and affect detection</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Attila</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Duerichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claus</forename><surname>Marberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Van Laerhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on multimodal interaction</title>
				<meeting>the 20th ACM international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="400" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stress without distress</title>
		<author>
			<persName><forename type="first">Hans</forename><surname>Selye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychopathology of human adaptation</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fairness violations and mitigation under covariate shift</title>
		<author>
			<persName><forename type="first">Harvineet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwali</forename><surname>Mhasawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumi</forename><surname>Chunara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><surname>Michalak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11001</idno>
		<title level="m">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Time series classification from scratch with deep neural networks: A strong baseline</title>
		<author>
			<persName><forename type="first">Zhiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN</title>
				<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>ArXiv abs/1710.09412</idno>
		<title level="m">mixup: Beyond Empirical Risk Minimization</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
