<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning to Rank with Pairwise Policy Gradient</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<email>junxu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Arti cial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Arti cial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeng</forename><surname>Wei</surname></persName>
							<email>weizeng@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
							<email>longxia@yorku.ca</email>
							<affiliation key="aff3">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<email>yanyanlan@ict.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
							<email>yindawei@acm.org</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science &amp; Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Arti cial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement Learning to Rank with Pairwise Policy Gradient</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3397271.3401148</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Learning to rank</term>
					<term>reinforcement learning</term>
					<term>policy gradient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper concerns reinforcement learning (RL) of the document ranking models for information retrieval (IR). One branch of the RL approaches to ranking formalize the process of ranking with Markov decision process (MDP) and determine the model parameters with policy gradient. Though preliminary success has been shown, these approaches are still far from achieving their full potentials. Existing policy gradient methods directly utilize the absolute performance scores (returns) of the sampled document lists in its gradient estimations, which may cause two limitations: 1) fail to re ect the relative goodness of documents within the same query, which usually is close to the nature of IR ranking; 2) generate high variance gradient estimations, resulting in slow learning speed and low ranking accuracy. To deal with the issues, we propose a novel policy gradient algorithm in which the gradients are determined using pairwise comparisons of two document lists sampled within the same query. The algorithm, referred to as Pairwise Policy Gradient (PPG), repeatedly samples pairs of document lists, estimates the gradients with pairwise comparisons, and nally updates the model parameters. Theoretical analysis shows that PPG makes an unbiased and low variance gradient estimations. Experimental results have demonstrated performance gains over the state-of-the-art baselines in search result diversi cation and text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Learning to rank; • Computing methodologies → Reinforcement learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning to rank, a family of machine learning techniques where the training objective is to provide the right ranking order of documents for a given query, has played a vital role in the eld of information retrieval (IR) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. In recent years, reinforcement learning (RL) <ref type="bibr" target="#b30">[31]</ref> techniques have been applied to the task and a number of RL models have been developed <ref type="bibr">[10, 12, 14, 22, 26, 39-41, 43, 44, 46]</ref>, referred to as "reinforcement learning to rank" in this paper. One branch of the research formulates the process of constructing a document list for a query as sequential decision making and models it with Markov decision process (MDP). For example, in <ref type="bibr" target="#b34">[35]</ref>, the document ranking in search result diversi cation is modeled as an MDP in which each time step corresponds to a ranking position and each action corresponds to choosing a document for the position. Given a set of labeled training data, policy gradient <ref type="bibr" target="#b30">[31]</ref> is utilized to learn the MDP parameters.</p><p>Despite the apparent successes, there remain limitations in these methods. Speci cally, at each of its training iteration, the policy gradient algorithm (e.g., REINFORCE <ref type="bibr" target="#b30">[31]</ref>) rst samples a document list as its training instance. Then, the gradient is estimated according to the list, weighted by the absolute performance score of the list (i.e., returns). In this way, if the performance score is high, the model parameters are updated with a high probability of repeating the document selection in the future visits to similar state.</p><p>When being applied to document ranking in IR, estimating gradients in this way has two limitations. First, it ignores the relative ordering nature of IR ranking. It has been widely observed that predicting relative orders of documents is close to the nature of ranking. This observation has motivated the success of pairwise learning to rank, and clearly shows that ranking cares more about the relative goodness of a document compared to others, rather than its absolute relevance score. Moreover, in IR ranking the comparisons should only be conducted between the documents within the same query. Existing policy gradient algorithms, however, sample one document list per iteration and thus only focus on the absolute scores of the chosen actions (selection of documents). The relative orders of the documents within each query are not considered.</p><p>Second, it estimates the gradients with high variance. From the viewpoint of RL, directly leveraging the absolute performance scores (also called returns) as the gradient weights leads to high variance gradient estimations. A popular solution in traditional RL is comparing each absolute performance score to a state baseline, resulting in a relative performance score as the weight. In IR ranking, however, estimating a reasonable baseline function is very di cult, due to the extremely huge and separated state space.</p><p>In this paper, we aim to develop a new policy gradient algorithm that can fully take the relative ordering nature of IR ranking into consideration and estimate gradients with low variance. Inspired by pairwise learning to rank, we propose to develop a novel policy gradient algorithm using intra-query pairwise comparisons, referred to as Pairwise Policy Gradient (PPG). PPG utilizes MDPs as its ranking models. In its learning procedure, PPG repeats the process of sampling a pair of document lists starting from the same state (and thus within the same query), comparing these two lists in terms of their performance scores and gradient directions, and nally update the MDP parameters with the comparison results.</p><p>Theoretical analysis shows that the gradients calculated in PPG are unbiased estimations and have low variance (under some conditions), assuring it has good theoretical convergence properties and can produce fast learning. We also show that the proposed PPG is a natural generalization of REINFORCE, and it can be considered as a variation of REINFORCE with baseline.</p><p>PPG o ers several advantages: ease in the implementation, theoretical soundness, e ciency in training, and high accuracy in the ranking. Empirically, we have implemented PPG for two IR ranking tasks: search result diversi cation and text retrieval, through con guring the MDP states, actions, rewards, state transitions, and policies accordingly. In our experiments, we found that PPG outperformed all of the state-of-the-art baselines in terms of popularly used evaluation measures (e.g., α-NDCG <ref type="bibr" target="#b6">[7]</ref> in search result diversi cation and NDCG <ref type="bibr" target="#b14">[15]</ref> in text retrieval) in both of the ranking tasks, indicating the e ectiveness of PPG for document ranking. Empirical analysis showed that PPG converged faster and had lower estimation variance than REINFORCE, veri ed the conclusions drawn in the theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing learning to rank studies can be categorized into pointwise approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>, pairwise approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref>, and listwise approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>. Speci cally, the pairwise methods consider the preference pairs composed of two documents with di erent relevance levels under the same query and construct classi er. However, the non-di erentiable ranking metrics make it hard to optimize the evaluation measures directly.</p><p>In recent years, reinforcement learning has been applied in the IR ranking.Radlinski et al. <ref type="bibr" target="#b26">[27]</ref> proposed two online learning bandit algorithms to learn a diverse ranking of the documents based on users clicking behaviors. Yue and Joachims <ref type="bibr" target="#b38">[39]</ref> formalized the interactively optimizing of information retrieval systems as a dueling bandit problem, called Dueling Bandit Gradient Descent (DBGD). In <ref type="bibr" target="#b12">[13]</ref>, DBGD was further improved so that the click data can be used to judge the user preference of the document rankings. In <ref type="bibr" target="#b16">[17]</ref>, a cascading bandits model is proposed to identify the K most attractive document for users.MDP has also shown its e ectiveness in learning to rank. In <ref type="bibr" target="#b39">[40]</ref>, the process of document ranking is formalized with MDP and solved with the classic policy gradient algorithm of REINFORCE. Similar MDP con gurations are used to model the sequential document selection process in search result diversi cation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref> and multi-page search <ref type="bibr" target="#b40">[41]</ref>. The query change model <ref type="bibr" target="#b36">[37]</ref> formalizes the problem of session search as an MDP. The win-win search framework models session search as a dual-agent stochastic game, on the basis of partially observed Markov decision process (POMDP) <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a log-based document re-ranking algorithm is proposed, also based on POMDP. DRM <ref type="bibr" target="#b23">[24]</ref> makes use of deep RL to deal with the complex ranking problems in which both the user preferred document order and display position order for result presentation is considered.</p><p>RL has also been widely used for the IR tasks beyond document ranking. For example, Shi et al. <ref type="bibr" target="#b29">[30]</ref> use reinforcement learning for commodity search. Li et al. <ref type="bibr" target="#b18">[19]</ref> model collaborative ltering with bandit. Shani et al. <ref type="bibr" target="#b28">[29]</ref> design an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. Lu and Yang <ref type="bibr" target="#b20">[21]</ref> propose POMDP-Rec, a neural-optimized POMDP algorithm, for building a collaborative ltering recommender system. In <ref type="bibr" target="#b44">[44]</ref>, the recommendation is modeled with MDP and deep Q-learning is used to conduct the optimization. In <ref type="bibr" target="#b31">[32]</ref>, the process of ranking is modeled with generative adversarial networks (GANs) and solved with reinforcement learning, called IRGAN. IRGAN can be used for both document ranking and item recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REINFORCEMENT LEARNING TO RANK</head><formula xml:id="formula_0">Given N labeled training queries {(Q (n) , X (n) , Y (n) )} N n=1</formula><p>, where</p><formula xml:id="formula_1">X (n) = {d (n) 1 , • • • , d (n) M n } and Y (n) = { (n) 1 , • • • , (n) M n</formula><p>} are the sets of candidate documents and labels associated with query Q (n) , respectively. The ranking models aim at constructing a document list by putting the relevant documents to the top of the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MDP formulation of ranking</head><p>MDP is a powerful mathematical model used to describe an interaction system between the environment and the agent. The MDP model has been used to model the process of document ranking. Formally, the MDP formulation of the document ranking considers the construction of a document list as sequential decision making. In the MDP, each time step t (t = 0, 1, • • • ) corresponds to a position in the document list. Speci cally, the action a t ∈ A(s t ) denotes to select the document d m(a t ) from the candidates and move it to the rank t + 1, and the policy π (a|s; θ ) de nes a probabilistic distribution over the available candidate documents. A(s t ), m(a t ), and θ denote the set of available actions, index of the document corresponding to a t , and the model parameters, respectively.</p><p>Construction of a document list with the MDP can be described as: given a user query Q and the set of retrieved M documents</p><formula xml:id="formula_2">X = {d 1 , • • • , d M }, the MDP state is initialized as s 0 = S(Q, X ),</formula><p>where S is the state initialization function. At each of the time steps t = 0, • • • , M − 1, the agent observes the state s t , calculates the policy π (•|s t ; θ ), and chooses an action a t which selects the document d m(a t ) from the document set and places it to the rank t +1. The user feedbacks the reward R(s t , a t ) to measure the selected document. Moving to the next time step t + 1, the state becomes s t +1 = T (s t , a t ), where T is the state transition function. The process is repeated until all of the M documents are ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning with policy gradient</head><p>RL methods are widely used to determine the MDP model parameters θ (may consist of the parameters in the functions of policy π , state initialization S, and the state transition T etc.). With the labeled queries, the MDP environment can emit an immediate numerical reward r t +1 = R(s t , a t )<ref type="foot" target="#foot_0">1</ref> for each issued action a t . With the rewards, the policy gradient algorithms such as REINFORCE can be employed to learn the parameters θ . Speci cally, the policy gradient algorithms employ stochastic gradient ascent iterations for conducting the optimization. At each iteration, a document list (episode</p><formula xml:id="formula_3">) τ = (S 0 , A 0 , R 1 , • • • , S M −1 , A M −1 , R M ) ∼ π (•|•; θ ) is sampled,</formula><p>where S t , A t , and R t +1 are the observed state, the chosen action, and the received reward, respectively. Then, at each t, the gradient of the parameters θ is estimated as:</p><formula xml:id="formula_4">∆θ = G t ∇ log π (A t |S t ; θ ),<label>(1)</label></formula><p>where G t is the long-term return starting from t: G t = M −t k=1 γ k−1 R t +k , where γ ∈ (0, 1] is the discount factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Example applications: search result diversi cation and text retrieval</head><p>The MDP formulation of ranking has been applied to the ranking tasks of search result diversi cation and text retrieval and promising results have been presented <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. The con gurations of the MDPs, including the de nitions of time step, state, state transition, reward, policy, and the gradient of policy, are shown in Table <ref type="table">1</ref>.</p><p>Please refer to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Problem Analysis</head><p>Though signi cant progresses have been achieved, the MDP formulation of document ranking still has limitations. More speci cally, the gradient in Equation (1) consists of two terms: the gradient ∇ log π (A t |S t ; θ ) which points to the direction that most increases the probability of repeating the action A t on future visits to the state S t , and the long term return G t which lets the parameters move most in the directions that favor actions that yield the highest return.</p><p>When being directly applied in IR ranking, the gradient setting has two limitations: (1) fails to consider the intra-query relative ordering nature of IR ranking, and (2) estimates the gradients with high variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.1</head><p>The intra-query relative ordering nature. The aim of document ranking in IR is to come up with optimal orderings of documents retrieved by queries. Compared with traditional machine learning tasks such as classi cation or regression, document ranking is unique in that: (1) ranking does not care much about the exact relevance score that each document gets, but cares more about the relative goodness of a document compared to others; (2) the existence of queries poses a further restriction on the comparisons: only the documents retrieved by the same query can be compared with each other. The cross-query comparisons are not meaningful because the two documents will not appear in the same list. Existing policy gradient approaches, however, fail to capture both of these two important properties in IR ranking.</p><p>(1) At each rank position t, the gradient weight G t in Equation ( <ref type="formula" target="#formula_4">1</ref>) is calculated as the absolute goodness of the chosen document d m(A t ) , calculated based on a sampled document list. The goodness DCG@5 = 1 DCG@5 = 1.95</p><p>Figure <ref type="figure">1</ref>: DCG@5 of the optimal document list for a di cult query q 1 and a suboptimal document list for an easy query q 2 . The shaded icons denote the relevant documents.</p><p>of choosing other documents at the same position t (and at the same state) is not taken into consideration. Thus, with these gradients and weights, the policy gradient still cares about the absolute document scores, not the relative document orders.</p><p>(2) When being applied to document ranking <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>, the nal gradients are the linear combinations of the gradient directions at all positions of all sampled document lists. The combination weights (G t 's) are usually set as the absolute performance scores based on DCG or α-DCG, without considering which query they come from. In another word, all of the queries are equally treated. However, it have been observed in IR that there exists high variance in performance among the queries <ref type="bibr" target="#b5">[6]</ref>. Some queries are in nature more di cult than others. The reasons could be, for example, the query may have very few relevant documents available in its candidates, or there exists a big semantic gap between the query and the relevant documents. The performance of the optimal ranking for a di cult query may be lower than the performance of a suboptimal ranking for an easy query, as the examples shown in Figure <ref type="figure">1</ref>. Since the easy queries always generate document lists with high-performance scores while policy gradient treats them equally, the trained ranking model may be biased to easy queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">High variance gradient estimation.</head><p>In policy gradient, it has been widely observed that directly using returns as the gradient weights leads to high variance gradient estimation <ref type="bibr" target="#b30">[31]</ref>. As the solutions for traditional RL tasks, policy gradient with baseline and actor-critic have been proposed for reducing the estimation variance. In these methods, the absolute return of each action is compared to the baseline of the state. Thus, the weights become relative returns. Intuitively, in some states, all actions have high values and high baselines are needed to di erentiate the higher valued actions from the less highly valued ones; in other states all actions have low values and low baselines are appropriate. Usually, the baseline is estimated using the historically estimated episodes.</p><p>In IR ranking, however, it is very di cult (if not impossible) to estimate reasonable baselines because the state space is huge and separated. In the MDP model for ranking, the ranked document lists are usually involved in the state. Suppose that a query is associated with M documents (on average M ≈ 150 in OHSUMED), the number of possible document lists per query is M!. The size of the state space makes it impossible to sample enough data to estimate the baseline values. Moreover, in training N di erent training queries are provided. Since each query retrieves its own set of documents for ranking, the whole state space is separated into N disjoint subsets. The document lists sampled in one query cannot be used to learn the baselines in another query.</p><p>Table <ref type="table">1</ref>: MDP con gurations for search result diversi cation and text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDP con guration</head><p>search result diversi cation <ref type="bibr" target="#b34">[35]</ref> text retrieval <ref type="bibr" target="#b39">[40]</ref> state at time t :</p><formula xml:id="formula_5">s t s t = [Z t , X t , h t ]</formula><p>, where Z t is ranked doc list, h t is latent vector</p><formula xml:id="formula_6">s t = [t, X t ] initial state s 0 s 0 = S(Q, X ) = [∅, X, σ (V q q)],</formula><p>where q is query embedding</p><formula xml:id="formula_7">s 0 = S(Q, X ) = [0, X ],</formula><p>where X contains all docs state transition T(s t , a t ) T(s t ,</p><formula xml:id="formula_8">a t ) = [Z t ∪ {d m(at ) }, X t \ {d m(at ) }, σ (Vx m(at ) + Wh t )], T(s t , a t ) = [t + 1, X t \ {d m(at ) }]</formula><p>where x m(at ) is doc embedding, and</p><formula xml:id="formula_9">V, W are parameters reward R(s t , a t ) R(s t , a t ) = α -DCG[t+1]−α -DCG[t], where α -DCG[0] = 0 R(s t , a t ) = 2 m(a t ) − 1 t = 0 (2 m(a t ) − 1)/log 2 (t + 1) t &gt; 0 policy π (a t |s t ; θ ) π (a t |s t ) = exp x T m(a t ) Uht a∈A(s t ) exp x T m(a) Uht , π (a t |s t ; θ ) = exp θ T ϕ(Q,d m(a t ) ) a∈A(s t ) exp θ T ϕ(Q,d m(a) )</formula><p>, where U is the parameter matrix where ϕ(Q, d m(at ) ) is ranking feature vector Gradient ∇ log π (a |s) refer to <ref type="bibr" target="#b34">[35]</ref> for details</p><formula xml:id="formula_10">ϕ(Q, d m(at ) ) − a∈A(st ) π (a |s t ; θ )ϕ(Q, d m(a) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR APPROACH: PPG</head><p>To deal with the issues and inspired by the intra-query preference pair generation mechanism in pairwise learning to rank, in this section we propose to combine the intra-query pairwise comparisons with conventional policy gradient algorithm of REINFORCE, achieving a new algorithm tailored for document ranking, referred to as pairwise policy gradient (PPG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Policy gradient with pairwise comparison</head><p>4.1.1 Gradient representation with pairwise comparisons. First, we show theoretically that the gradient of the ranking objective can be represented in the form of pairwise comparisons. Formally, the goal of the learning algorithm is to maximize the expected long term return (performance of the list) from the beginning:</p><formula xml:id="formula_11">(θ ) = π (s 0 ) = E τ ∼π [G 0 ],<label>(2)</label></formula><p>where π (s 0 ) is the expected return starting from t = 0, τ = {s 0 , a 0 , r 1 ,</p><formula xml:id="formula_12">• • • , s M −1 , a M −1 , r M } (corresponds to the document list of {d m(a 0 ) , • • • , d m(a M −1 ) })</formula><p>is the episode sampled according to current policy π , and G 0 is the long-term return of the episode starting from time step t = 0:</p><formula xml:id="formula_13">G 0 = M k =1 γ k −1 r k .</formula><p>The policy gradient method calculate the gradient based on the Policy Gradient Theorem which can be represented as ( <ref type="bibr" target="#b30">[31]</ref>, Chapter 13.2):</p><formula xml:id="formula_14">∇ (θ ) ∝ s µ(s) a q π (s, a) • ∇π (a|s; θ )<label>(3)</label></formula><p>where µ(s) is on-policy distribution over the state s under π , q π (•, •) is the Q-function whose value is the discounted total reward expected after performing the action in the state and then following π , and</p><formula xml:id="formula_15">∇π (•|•; θ ) is the partial derivatives of π (•|•; θ ) w.r.t. θ .</formula><p>In retrieval scenario, the search engine constructs the ranking list given a query, which can re ect the preference of each document. Just as the equation 3 shown, the traditional policy gradient methods measure the sampled action by the absolute accumulative rewards gained in future. However, the distributions of the candidate documents various over di erent queries. The absolute reward may lead to unfairness to di erent queries.</p><p>Inspired by the pairwise ranking methods, which optimize the rank model based on the preference document pairs. We consider the search engine samples two action at once to construct the "preference pair". We modify the policy gradient method to utilize the di erence of rewards gained by these two document to optimize the rank model and prove that the gradient of (θ ) can be expressed in the form of pairwise comparisons, as shown in Theorem 4.1.</p><p>T 4.1. The gradient of (θ ) in Equation ( <ref type="formula" target="#formula_11">2</ref>) can be represented as Firstly, the search engine interacts with the user based on current policy π (a|s, θ ), then the sampled state satis es S t ∼ µ(s)</p><formula xml:id="formula_16">∇ (θ ) ∝ s µ(s) a b (q π (s, a) − q π (s, b)) • (</formula><formula xml:id="formula_17">∇ (θ ) ∝ E π a b (q π (S t , a) − q π (S t , b)) • (π (b |S t ; θ )∇π (a|S t ; θ ) − π (a|S t ; θ )∇π (b |S t ; θ ))</formula><p>Then we utilize the multipliers π (a t |S t , θ ) and π (b t |S t , θ ), and sample the action A t and B t follow the current policy. Thus</p><formula xml:id="formula_18">∇ (θ ) ∝ E π a b π (a|S t ; θ )π (b|S t ; θ ) • (q π (S t , a) − q π (S t , b)) • ∇π (a|S t ; θ ) π (a|S t ; θ ) − ∇π (b |S t ; θ ) π (b |S t ; θ ) = E π q π (S t , A t ) − q π (S t , B t ) • ∇ log π (A t |S t ; θ ) − ∇ log π (B t |S t ; θ ) ,<label>(4)</label></formula><p>Session 3B: Learning to Rank SIGIR '20, July 25-30, 2020, Virtual Event, China</p><p>Algorithm 1 Pairwise Policy Gradient (PPG) for t = 0 to M − 1 do 7:</p><formula xml:id="formula_19">INPUT: Training set D = {(Q (n) , X (n) , Y (n) )} N n=1 ,</formula><formula xml:id="formula_20">τ A = S, A t , R A t +1 , S A t +1 , • • • , S A M −1 , A M −1 , R A M ∼ π (θ ) 8: G A ← M −t k =1 γ k −1 R A t +k {long term return of τ A } 9: τ B = S, B t , R B t +1 , S B t +1 , • • • , S B M −1 , B M −1 , R B M ∼ π (θ )</formula><p>10: where the state-action value function q π (S t , A t ) is the expected discount reward:</p><formula xml:id="formula_21">G B ← M −t k =1 γ k −1 R B t +k {long term return of τ B } 11: ∆θ ← ∆θ + (G A − G B ) • ∇ log π (A t |S t ; θ ) − ∇ log π (B t |S t ; θ ) {according to Equation (5)} 12: S ← S A t +1 G A ≥ G B S B t +1</formula><formula xml:id="formula_22">q π (S t , A t ) = E[R A t + γ R A t +1 + • • • + γ (T −t ) R A T |S t , A t ] = E[G A t |S t , A t ]</formula><p>. q π (S t , B t ) can be calculated in a similar way. In Equation ( <ref type="formula" target="#formula_18">4</ref>), the rst equation holds because the overall gradient is a sum over states weighted by how often the states occur under the target policy π . Thus we can get rid of µ(s) because the state will be encountered in these proportions following the π ; the second equation introduces a weighting π (a|S t ; θ )π (b |S t ; θ ) without changing the equality by multiplying and dividing it; and the third equation replaces a by action A t and b by action B t , both sampled following the policy π (•|S t ; θ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.3</head><p>Learning with stochastic gradient ascent. Using the gradient in Equation ( <ref type="formula" target="#formula_18">4</ref>) to instantiate the generic stochastic gradient ascent, we get the gradient at each position t:</p><formula xml:id="formula_23">∆θ = (G A t − G B t ) ∇ log π (A t |S t ; θ ) − ∇ log π (B t |S t ; θ ) . (<label>5</label></formula><formula xml:id="formula_24">)</formula><p>The equation indicates that the gradient of the policy parameters can be estimated with pairwise comparisons: given a query and starting from S t , the algorithm samples two episodes τ A and τ B . The unbiased gradient of the policy, then, can be calculated based on the pairwise comparisons between the two lists. Algorithm 1 shows the proposed pairwise policy gradient (PPG) algorithm. The process of sampling the episodes and choosing the next states in Algorithm 1 is illustrated in Figure <ref type="figure">2</ref>. We can see that PPG successfully introduces intra-query pairwise comparisons into policy gradient and captures the intra-query relative ordering nature of document ranking. Note that the two lists in a pair start from the same state (under a query), which assures only intra-query comparisons are allowed in PPG.</p><p>The time complexity of the PPG training algorithm is of O(T N M (M + 1)), where T is the number of training iterations, N is the</p><formula xml:id="formula_25">S 0 S A 1 S A 2 … S A M-1 R A 1 R A 2 S B 1 S B 2 … S B M-1 R B 1 R B 2 S A 2 … S A M-1 S B 2 … S B M-1 R A M R B M G 0 A G 1 B G 1 A … … t = 0 t = 1 t = 2 R A 2 R A M R B 2 R B M ! " ! # ! " ! # G 0 B Figure 2:</formula><p>Illustration of the sampling process in PPG. The highlighted arrows and circles shows the chosen actions and the resulting next states. Given the initial state S 0 and ranking position t = 0, PPG samples two episodes τ A and τ B , and conducts the intra-query pairwise comparison. Then, at t = 1, the system follows the winner episode one step (τ B in the example because G A 0 &lt; G B 0 ) and moves to a new state S B 1 . Again, starting from S B 1 , PPG samples two new episodes for another comparison. The process continues until t = M − 1. Note that for an M-size candidate set, PPG will sample 2M + 2(M − 1)</p><formula xml:id="formula_26">+ • • • + 2 = M • (M + 1) times.</formula><p>number of queries in training data, and M is the average number of labeled documents per query.  <ref type="formula" target="#formula_18">4</ref>) derives an unbiased Monte Carlo sampling. Therefore Algorithm 1 makes an unbiased estimation of ∇ (θ ), guaranteeing that it will converge asymptotically to an optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussions</head><p>Moreover, the intra-query pairwise comparisons have the ability to estimate the gradients with low variance (under some conditions), as shown in the following Theorem 4.2. T 4.2. Given state s ∼ µ π where µ π is is on-policy distribution under π , and given two actions a ∼ π (•|s; θ ) and b ∼ π (•|s; θ ). Considering the following two representations of the gradient:</p><formula xml:id="formula_27">g 1 = (q π (s, a) − q π (s, b)) • (∇ log π (a|s; θ ) − ∇ log π (b|s; θ )) , g 2 =q π (s, a) • ∇ log π (a|s; θ ) + q π (s, b) • ∇ log π (b |s; θ ).</formula><p>The variances of g 1 and g 2 satisfy</p><formula xml:id="formula_28">Var(g 1 ) ≤ Var(g 2 ), if q π (•, •) ≥ 0 and E µ π , π (q(s, a) − q(s, b))• q(s, b) ∇ log π (a|s; θ ) 2 − q(s, a) ∇ log π (b |s; θ ) 2 ≥ 0,</formula><p>where Var(g) = tr(cov(g, g)) is the trace of the covariance matrix, and 'tr' and 'cov' denote trace and covariance, respectively.</p><p>Proof of the theorem is given in Section A.2. Note that g 1 is the gradient used in PPG and g 2 equals to the gradient used in REINFORCE<ref type="foot" target="#foot_1">2</ref> . Theorem 4.2 indicates that PPG has the ability to reduce variance in gradient estimation when the conditions being satisi ed. In stochastic optimization, the estimation with low variance usually leads to fast in learning and accurate in ranking.</p><p>The rst condition q π (•, •) ≥ 0 can be satis ed naturally in IR, because the commonly used IR evaluation measures such as DCG and α-DCG are nonnegative and increases monotonically with the ranking positions. The second condition can be explained as follows: under the π and µ π , the state-action value and the norm of gradient (multiplied by another state-action value) are positively correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Relation with pairwise learning to rank. PPG is inspired by the intra-query pairwise comparison mechanism in pairwise learning to rank. In that sense, they share a number of common advantages such as capturing the relative ordering nature of ranking and avoiding cross-query comparisons. However, PPG also has several striking di erences from pairwise learning to rank.</p><p>First, the criterion for "preference" are di erent. In pairwise learning to rank, a document is preferred because it has a higher relevance label. In PPG, however, an action (choosing a document) is preferred because selecting the document at current state will get higher long term return. In this sense, PPG looks at the future e ects and current state when evaluating a document, which is a more reasonable criteria to get optimal rankings.</p><p>Second, the times for generating the pairs are di erent. In pairwise learning to rank, the algorithms generate all of the pairs before the learning starts. PPG, on the other hand, generate the pairs at each of the training iterations with an "online" manner. Therefore, it is possible for PPG to dynamically generate the most valuable pairs at each of the training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Relation with REINFORCE and REINFORCE with baseline.</head><p>PPG is derived under the policy gradient framework and it is a nature generalization of REINFORCE. Comparing the estimated gradients by PPG (Equation ( <ref type="formula" target="#formula_23">5</ref>) and line 11 of Algorithm 1) and that of by REINFORCE (Equation ( <ref type="formula" target="#formula_4">1</ref>)), we can see that PPG automatically degenerates to REINFORCE by setting τ A (or τ B ) to an empty episode. For example, by setting τ B to empty, the gradient of PPG becomes the gradient of REINFORCE:</p><formula xml:id="formula_29">∆θ =(G A t − G B t )(∇ log π (A t |S t ; θ ) − ∇ log π (B t |S t ; θ )) =(G A t − 0)(∇ log π (A t |S t ; θ ) − 0) = G A t ∇ log π (A t |S t ; θ ).</formula><p>PPG can also be considered as a variation of REINFORCE with baseline. REINFORCE with baseline compares the action value G t to the baseline value of S t and its gradient can be written as:</p><formula xml:id="formula_30">∆θ = (G t − b(S t ))∇ log π (A t |S t ; θ ),</formula><p>where b(S t ) is the base line function that does not vary with action. From this viewpoint, the gradient of PPG can be decomposed as the sum of two gradients with baselines:</p><formula xml:id="formula_31">∆θ =(G A t − G B t ) (∇ log π (A t |S t ; θ ) − ∇ log π (B t |S t ; θ )) =(G A t − G B t )∇ log π (A t |S t ; θ ) + (G B t − G A t )∇ log π (B t |S t ; θ ). The rst term (G A t − G B t )∇ log π (A t |S t ; θ )</formula><p>is the gradient estimated based on τ A , using G B t as the baseline; and second term</p><formula xml:id="formula_32">(G B t − G A t )∇ log π (B t |S t ; θ )</formula><p>is the gradient estimated based on τ B , using G A t as the baseline. Since policy gradient with baseline can reduce the variance, it is intuitive that PPG can also produce low variance estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conducted experiments to test the performances of PPG. The source code of PPG can be found at https://github.com/wzeng316/ PPG-Rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments on search result diversi cation</head><p>We tested the performances of PPG on the ranking task for search result diversi cation. Speci cally, following the practices in <ref type="bibr" target="#b34">[35]</ref>, the experiments were conducted on the combination of four TREC benchmark datasets (TREC Web Track 2009 ∼ 2012). The dataset consists of 200 queries and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC annotators. The document relevance labels are made at the subtopic level and the labels are binary. The candidate documents were retrieved from the ClueWeb09 Category B data collection, which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal were applied to the documents as preprocessing. The queries were randomly split into ve folds and we conducted 5-fold cross-validation experiments. The results reported were the averages over the ve trials.</p><p>We compared the proposed PPG with state-of-the-art baselines in search result diversi cation, including the heuristic methods of MMR <ref type="bibr" target="#b4">[5]</ref>, xQuAD <ref type="bibr" target="#b27">[28]</ref>, and PM-2 <ref type="bibr" target="#b8">[9]</ref>; and the learning methods of SVM-DIV <ref type="bibr" target="#b37">[38]</ref>, R-LTR <ref type="bibr" target="#b45">[45]</ref>, PAMM <ref type="bibr" target="#b32">[33]</ref>, NTN-DIV <ref type="bibr" target="#b33">[34]</ref>, and MDP-DIV <ref type="bibr" target="#b34">[35]</ref>.</p><p>In MDP-DIV and PPG, the reward is calculated based on α-DCG and the discount factor γ = 1. The evaluation metrics include α-NDCG, S-recall, and ERR-IA, at the positions of 5 and 10.</p><p>Table <ref type="table" target="#tab_1">2</ref> reports the ranking accuracies of PPG and all of the baseline methods. Boldface indicates the highest scores in all runs. From the results, we can see that PPG outperformed all baselines in terms of all evaluation metrics, demostrating the e ectiveness of PPG in the task of search result diversi cation. We conducted signi cant tests (t-test) on the improvements of PPG over the best baseline MDP-DIV. The results showed that most of the improvements were signi cant (p-value &lt; 0.05 and denoted with ' * ').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments on text retrieval</head><p>We also conducted experiments on the task of text retrieval. The experiments were conducted on three traditional learning to rank benchmark datasets: OHUSMED, MQ2008 <ref type="bibr" target="#b24">[25]</ref>, and MSLR-Web10K. Each dataset consists of queries, corresponding retrieved documents and human judged labels, and the statistics over the three dataset  <ref type="table" target="#tab_2">3</ref>. In all of the experiments, we conducted 5fold cross-validation experiments on these datasets. The results reported were the average over the ve folds. For OHUSMED and MQ2008 dataset, we used the provided standard features and the linear score function. Speci cally, for MSLR-Web10K dataset, we normalize each provided feature by the mean/standard deviation. We compared the proposed PPG to the traditional learning to rank baselines, including RankSVM <ref type="bibr" target="#b15">[16]</ref>, RankNet <ref type="bibr" target="#b1">[2]</ref>, ListNet <ref type="bibr" target="#b3">[4]</ref>, AdaRank <ref type="bibr" target="#b35">[36]</ref>, and MDPRank <ref type="bibr" target="#b39">[40]</ref>.</p><p>In both MDPRank and PPG, the rewards are calculated based on DCG and the discount factor γ = 1. As for evaluation measures, NDCG at the positions of 1, 3, 5 and 10 were used for evaluation. Table <ref type="table" target="#tab_3">4</ref>, Table <ref type="table" target="#tab_4">5</ref>, and Table <ref type="table" target="#tab_5">6</ref> report the performances of PPG and the baselines on OHSUMED, MQ2008 and MSLR-Web10K, respectively. Boldface indicates the highest scores among all runs. From the results, we can see that PPG outperformed all the baselines, including the traditional learning to rank methods and reinforcement learning-based method, on both datasets in terms of all of the evaluation measures. The results showed the e ectiveness of PPG for the task of text retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of the experimental results</head><p>In this section, we conducted experiments to investigate how PPG works and outperformed the baselines, using the results on the rst fold of the combined TREC Web Track data as example.</p><p>5.3.1 Convergence. Theorem 4.1 and Theorem 4.2 show that PPG makes unbiased and low variance estimations, guaranteeing to converge to an optimum fast. We conducted experiments to verify the theoretical conclusions. Speci cally, we compared the convergence curves of PPG with the baseline method of MDP-DIV and MDPRank (using REINFORCE in its learning). Figure <ref type="figure" target="#fig_4">3</ref> illustrates the learning curves where the x-axis is number of training epochs and y-axis is the average rewards received. The curves in Figure <ref type="figure" target="#fig_4">3</ref> show that PPG converged much faster than MDPRank and MDP-DIV. Moreover, PPG converged to a better optimum. The results veri ed that: (1) the unbiased estimations made PPG to converge to an optimum; (2) the low variance estimations made PPG to converge fast.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.2</head><p>Reducing the variance. Theorem 4.2 shows that PPG can produce low variance estimations, under some conditions. We conducted experiments to directly compare the variances of the estimations by PPG and MDP-DIV. Speci cally, we trained diverse ranking models with PPG and MDP-DIV. At each iteration, the gradient vectors calculated on all of the sampled document lists were recorded and the variance (trace of the covariance matrix) was calculated. Figure <ref type="figure" target="#fig_5">4</ref> illustrated the variance curves by PPG and MDP-DIV. We can see that the variance curve by PPG is much lower than that of by MDP-DIV at all of the training epochs. The results veri ed the conclusion drawn in Theorem 4.2 on real applications. It also indicated that the conditions in Theorem 4.2 is reasonable and can be well satis ed in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.3.3</head><p>Improving di icult queries. The analysis in Section 3.4 shows that the document ranking models trained by REINFORCE may be biased to easy queries, because of the high variance in performance among queries. We conducted experiments to show whether the bias can be alleviated in PPG.</p><p>Speci cally, we made statistics on the number of relevant documents per query in the training data. The queries are clustered into di erent groups based on percentage of relevant documents among the candidates. Intuitively, more number of relevant documents makes the query easier. Figure <ref type="figure" target="#fig_6">5</ref> shows the distribution of the query groups. In the gure, for example, '0% ∼ 20%' is the group of queries  whose percentage of relevance documents per query are between 0% and 20%. We can see that the numbers of relevant documents per query really vary from query to query, indicating the high variance in performance among the queries in real data.</p><p>Next we evaluated the accuracies of PPG and MDP-DIV in terms of NDCG@10 for each of the query group. The results are reported in Figure <ref type="figure" target="#fig_7">6</ref>. We found that the average NDCG@10 of PPG over the groups is higher than MDP-DIV (except the fth group). Furthermore, it is interesting to see that PPG performs particularly better than MDP-DIV for queries with small numbers of relevant documents (e.g., 0% ∼ 20%, 20% ∼ 40%, 40% ∼ 60%, and 60% ∼ 80%). The results indicate that PPG successfully avoids creating a model biased toward easy queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed a novel reinforcement learning algorithm tailored for document ranking in IR, called Pairwise Policy Gradient (PPG). In its learning, PPG estimates the gradients based on the intraquery comparisons between pairs of sampled document lists. PPG addressed the two issues in conventional policy gradient: ignoring intra-query relative ordering nature of IR ranking and generating high variance gradient estimations. Theoretical analysis showed that PPG makes unbiased and low variance estimations, leading to a correct and fast learning. Experimental results showed that PPG outperformed the state-of-the-art baselines on search result diversi cation and text retrieval. Empirical analysis also showed that PPG converged fast to an optimum and really reduced the estimation variances. A.2 Proof of Theorem 4.2 P . The di erence of these two variance is Var(g 1 ) − Var(g 2 ) =tr(cov(g 1 , g 1 )) − tr(cov(g 2 ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><formula xml:id="formula_33">=E µ π , π g 1 − E µ π , π [g 1 ] 2 − E µ π , π g 2 − E µ π , π [g 2 ] 2 =E µ π , π g 1 2 − E µ π , π g 2 2 =E µ π , π g 1 2 − g 2 2</formula><p>The third equation stands because E µ π , π [g 1 ] = E µ π , π [g 2 ]. Note that according to Theorem 4.1 and the policy gradient theorem (Chapter 13.2 of <ref type="bibr" target="#b30">[31]</ref>), both g 1 and g 2 are unbiased estimation. According to the de nitions of g 1 and g 2 , and denoting q a = q π (s, a), q b = q π (s, b), π a = π (a|s; θ ), and π b = π (b |s; θ ) for simplicity:</p><formula xml:id="formula_34">g 1 2 − g 2 2 = (q a − q b ) • ∇ log π a − ∇ log π b 2 − q a • ∇ log π a + q b • ∇ log π b 2 =q 2 a ∇ log π b 2 − 2q 2 a [∇ log π a ] T ∇ log π b + q 2 b ∇ log π a 2 − 2q 2 b [∇ log π a ] T ∇ log π b − 2q a q b ∇ log π a 2 − 2q a q b ∇ log π b 2 + 2q a q b [∇ log π a ] T ∇ log π b .</formula><p>For any a ∼ π (•|s; θ ),</p><formula xml:id="formula_35">E µ π , π [∇ log π a ] =E µ π , π ∇π a π a = E µ π a π a ∇π a π a =E µ π a ∇π a = E µ π ∇ a π a = E µ π [∇1] = 0.</formula><p>and a and b are independent random variables, it is easy to know E µ π , π q 2 a [∇ log π a ] T ∇ log π b =[E µ π , π q 2 a ∇ log π a ] T E µ π , π ∇ log π b =[E µ π , π q 2 a ∇ log π a ] T 0 = 0, and similarly</p><formula xml:id="formula_36">E µ π , π q 2 b [∇ log π a ] T ∇ log π b =0.</formula><p>Therefore, Var(g 1 )−Var(g</p><formula xml:id="formula_37">2 ) = E µ π , π [ g 1 2 − g 2 2 ] = E µ π , π −2(q 2 a + q 2 b )[∇ log π a ] T ∇ log π b + E µ π , π [q 2 b ∇ log π a 2 + q 2 a ∇ log π b 2 − 2q a q b ∇ log π a 2 − 2q a q b ∇ log π b 2 + 2q a q b [∇ log π a ] T ∇ log π b ] = E µ π , π [q 2 b ∇ log π a 2 + q 2 a ∇ log π b 2 − 2q a q b ∇ log π a 2 − 2q a q b ∇ log π b 2 + 2q a q b [∇ log π a ] T ∇ log π b ] = E µ π , π (q a − q b )(q a ∇ log π b 2 − q b ∇ log π a 2 ) − E µ π , π q a q b ∇ log π a − ∇ log π b 2 ≤ E µ π , π (q a − q b )(q a ∇ log π b 2 − q b ∇ log π a 2 ),</formula><p>where the last inequation stands because q a ≥ 0, q b ≥ 0 and ∇ log π a − ∇ log π b 2 ≥ 0. Thus, we have Var(g 1 ) ≤ Var(g 2 ),</p><p>if E µ π , π (q a − q b )(q b ∇ log π a 2 − q a ∇ log π b 2 ) ≥ 0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>π (b |s; θ )∇π (a|s; θ ) − π (a|s; θ )∇π (b|s; θ )) , In Theorem 4.1, the gradient of the policy is represented in the form of pairwise comparisons between actions a and b. The proof of Theorem 4.1 can be found in Section A.1. 4.1.2 Gradient estimation with Monte Carlo sampling. Theorem 4.1 gives a new analytic expression for ∇ (θ ). Following the practices in deriving the REINFORCE algorithm, we estimate ∇ (θ ) with Monte Carlo sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>repeat 3 :← 0 4 : 5 :</head><label>345</label><figDesc>learning rate η, discount factor γ , and reward function R 1: Initialize θ ← random values 2: ∆θ for all (Q, X, Y ) ∈ D do Initial state S = S(Q, X ){Init state with Q } 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>θ</head><label></label><figDesc>← θ + η∆θ 16: until converge 17: return θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 2 . 1</head><label>21</label><figDesc>Bias and variance of the gradients. Theorem 4.1 shows the form of the gradient ∇ (θ ) and Equation (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of PPG and MDP-DIV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variance curves of PPG and MDP-DIV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of training queries w.r.t. di erent percentages of relevant documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison in terms of α-NDCG@10 for di erent query groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This work was supported by National Natural Science Foundation of China (61872338, 61832017, and 61773362), Beijing Academy of Articial Intelligence (BAAI2019ZD0305 and BAAI2020ZJ0303), Beijing Outstanding Young Scientist Program (BJJWZYJH012019100020098), the Youth Innovation Promotion Association CAS (2016102), Fundamental Research Funds for the Central Universities, and Research Funds of Renmin University of China (2018030246). A PROOF OF THE THEOREMS A.1 Proof of Theorem 4.1 P . According to policy gradient theorem([31], Chapter 13.2),∇ (θ ) ∝ s µ(s) a q π (s, a) • ∇π (a|s; w),where a is the action. Calculating it by sampling two times∇ (θ ) ∝ s µ(s) a q π (s, a) • ∇π (a|s; θ ) + b q π (s, b) • ∇π (b |s; θ ) .Note that a (s)∇π (a|s; θ ) = (s)∇ a π (a|s; θ ) = (s)∇1 = 0, where(s) = a q π (s, a)π (a|s; θ ) = b q π (s, b)π (b|s; θ ) is value function. The term can be added to ∇ without changing its value: s, a) • ∇π (a|s; θ ) − b q π (s, b)π (b |s; θ ) • a ∇π (a|s; θ ) − a q π (s, a)π (a|s; θ ) • b ∇π (b|s; θ ) + b q π (s, b) • ∇π (b|s; θ ) . Since the policy is a distribution over the actions, we have b π (b|s; θ ) = a π (a|s; θ ) = 1, and the gradient can be written as: π (s, a)π (b|s; θ )∇π (a|s; θ ) − a b q π (s, a)π (a|s; θ )∇π (b |s; θ ) − a b q π (s, b)π (b |s; θ )∇π (a|s; θ ) + a b q π (s, b)π (a|s; θ )∇π (b|s; θ ) = s µ(s) a b q π (s, a) − q π (s, b) • π (b|s; θ )∇π (a|s; θ ) − π (a|s; θ )∇π (b |s; θ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on TREC web track datasets</figDesc><table><row><cell>Method</cell><cell cols="6">α-NDCG@5 α-NDCG@10 S-rescall@5 S-rescall@10 ERR-IA@5 ERR-IA@10</cell></row><row><cell>MMR</cell><cell>0.2753</cell><cell>0.2979</cell><cell>0.4388</cell><cell>0.5151</cell><cell>0.2005</cell><cell>0.2309</cell></row><row><cell>xQuAD</cell><cell>0.3165</cell><cell>0.3941</cell><cell>0.4933</cell><cell>0.6043</cell><cell>0.2314</cell><cell>0.2890</cell></row><row><cell>PM-2</cell><cell>0.3047</cell><cell>0.3730</cell><cell>0.4910</cell><cell>0.6012</cell><cell>0.2298</cell><cell>0.2814</cell></row><row><cell>SVM-DIV</cell><cell>0.3030</cell><cell>0.3730</cell><cell>0.4910</cell><cell>0.6012</cell><cell>0.2298</cell><cell>0.2814</cell></row><row><cell>R-LTR</cell><cell>0.3498</cell><cell>0.4132</cell><cell>0.5397</cell><cell>0.6511</cell><cell>0.2521</cell><cell>0.3011</cell></row><row><cell>PAMM</cell><cell>0.3712</cell><cell>0.4327</cell><cell>0.5561</cell><cell>0.6612</cell><cell>0.2619</cell><cell>0.3029</cell></row><row><cell>NTN-DIV</cell><cell>0.3962</cell><cell>0.4577</cell><cell>0.5817</cell><cell>0.6872</cell><cell>0.2773</cell><cell>0.3285</cell></row><row><cell>MDP-DIV</cell><cell>0.4493</cell><cell>0.4924</cell><cell>0.5718</cell><cell>0.6826</cell><cell>0.3485</cell><cell>0.3712</cell></row><row><cell>PPG</cell><cell>0.4799  *</cell><cell>0.5122  *</cell><cell>0.6099  *</cell><cell>0.6928</cell><cell>0.3727  *</cell><cell>0.3914  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of L2R datasets</figDesc><table><row><cell>Method</cell><cell cols="4">#Query #LabeledDoc #Feature # LabelLevel</cell></row><row><cell>OHSUMED</cell><cell>106</cell><cell>16,140</cell><cell>45</cell><cell>3</cell></row><row><cell>MQ2008</cell><cell>784</cell><cell>9,360</cell><cell>46</cell><cell>3</cell></row><row><cell cols="2">MSLR-Web10K 10,000</cell><cell>1,200,192</cell><cell>136</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison on LETOR OHSUMED.</figDesc><table><row><cell>Method</cell><cell cols="4">NDCG@1 NDCG@3 NDCG@5 NDCG@10</cell></row><row><cell>RankSVM</cell><cell>0.4958</cell><cell>0.4958</cell><cell>0.4958</cell><cell>0.4140</cell></row><row><cell>RankNet</cell><cell>0.4785</cell><cell>0.4516</cell><cell>0.4464</cell><cell>0.4367</cell></row><row><cell>ListNet</cell><cell>0.5326</cell><cell>0.4732</cell><cell>0.4432</cell><cell>0.4410</cell></row><row><cell>AdaRank</cell><cell>0.4790</cell><cell>0.3730</cell><cell>0.4673</cell><cell>0.4496</cell></row><row><cell>MDPRank</cell><cell>0.5743</cell><cell>0.5045</cell><cell>0.4784</cell><cell>0.4558</cell></row><row><cell>PPG</cell><cell>0.5771</cell><cell>0.5218</cell><cell>0.4911</cell><cell>0.4664</cell></row><row><cell cols="2">are shown in Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on LETOR MQ2008.</figDesc><table><row><cell>Method</cell><cell cols="4">NDCG@1 NDCG@3 NDCG@5 NDCG@10</cell></row><row><cell>RankSVM</cell><cell>0.3626</cell><cell>0.4286</cell><cell>0.4695</cell><cell>0.2279</cell></row><row><cell>RankNet</cell><cell>0.3202</cell><cell>0.3984</cell><cell>0.4408</cell><cell>0.2094</cell></row><row><cell>ListNet</cell><cell>0.3754</cell><cell>0.4324</cell><cell>0.4747</cell><cell>0.2303</cell></row><row><cell>AdaRank</cell><cell>0.3826</cell><cell>0.4420</cell><cell>0.4821</cell><cell>0.2307</cell></row><row><cell>MDPRank</cell><cell>0.3827</cell><cell>0.4420</cell><cell>0.4881</cell><cell>0.2327</cell></row><row><cell>PPG</cell><cell>0.3877</cell><cell>0.4511</cell><cell>0.4910</cell><cell>0.2455</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on MSLR-WEB10K.</figDesc><table><row><cell>Method</cell><cell cols="4">NDCG@1 NDCG@3 NDCG@5 NDCG@10</cell></row><row><cell>RankSVM</cell><cell>0.3447</cell><cell>0.3589</cell><cell>0.3687</cell><cell>0.3881</cell></row><row><cell>RankNet</cell><cell>0.3768</cell><cell>0.3862</cell><cell>0.3942</cell><cell>0.4105</cell></row><row><cell>ListNet</cell><cell>0.3878</cell><cell>0.3879</cell><cell>0.3969</cell><cell>0.4135</cell></row><row><cell>AdaRank</cell><cell>0.3437</cell><cell>0.3272</cell><cell>0.3337</cell><cell>0.3475</cell></row><row><cell>MDPRank</cell><cell>0.4182</cell><cell>0.4052</cell><cell>0.4082</cell><cell>0.4189</cell></row><row><cell>PPG</cell><cell>0.4230</cell><cell>0.4104</cell><cell>0.4144</cell><cell>0.4254</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The reward can be calculated, for example, on the basis of IR evaluation measures of DCG, α -DCG, S-recall etc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Note that for making fair comparisons, both g 1 and g 2 are based on two sampled actions a and b. g 2 is an estimation of the original REINFORCE gradient q π (s, a) • ∇ log π (a |s; θ ) based on two sampled actions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session 3B: Learning to Rank SIGIR '20, July 25-30, 2020, Virtual Event, China</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning (ICML &apos;05</title>
				<meeting>the 22nd International Conference on Machine Learning (ICML &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
	<note>Learning to Rank Using Gradient Descent</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting Ranking SVM to Document Retrieval</title>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;06)</title>
				<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to Rank: From Pairwise Approach to Listwise Approach</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning (ICML &apos;07)</title>
				<meeting>the 24th International Conference on Machine Learning (ICML &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)</title>
				<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating the Query Di culty for Information Retrieval</title>
		<author>
			<persName><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Yom-Tov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="89" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novelty and Diversity in Information Retrieval Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)</title>
				<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pranking with Ranking</title>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)</title>
				<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference (WWW &apos;18)</title>
				<meeting>the 2018 World Wide Web Conference (WWW &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1939" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp;#38</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
	<note>Development in Information Retrieval (SIGIR &apos;18</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="63" to="90" />
			<date type="published" when="2013-02-01">2013. 01 Feb 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Balancing Exploration and Exploitation in Listwise and Pairwise Online Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="63" to="90" />
			<date type="published" when="2013-02">2013. Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application</title>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anxiang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th SIGKDD (KDD &apos;18)</title>
				<meeting>the 24th SIGKDD (KDD &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002-10">2002. Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing Search Engines Using Clickthrough Data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;02)</title>
				<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cascading Bandits: Learning to Rank in the Cascade Model</title>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Branislav Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Ashkan</surname></persName>
		</author>
		<idno>CoRR abs/1502.02763</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Rank for Information Retrieval and Natural Language Processing</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="121" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative Filtering Bandits</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009-03">2009. March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Partially Observable Markov Decision Process for Recommender Systems</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR abs/1608.07793</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Win-win Search: Dual-agent Stochastic Game in Session Search</title>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative Models for Information Retrieval</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;04)</title>
				<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ranking for Relevance and Display Preferences in Complex Presentation Layouts</title>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="346" to="374" />
			<date type="published" when="2010-06">Jun Xu, and Hang Li. 2010. Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Diverse Rankings with Multi-armed Bandits</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="784" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Diverse Rankings with Multi-armed Bandits</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="784" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting Query Reformulations for Web Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web</title>
				<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An MDP-Based Recommender System</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-12">2005. Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jing-Cheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anxiang</forename><surname>Zeng</surname></persName>
		</author>
		<idno>CoRR abs/1805.10000</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement Learning: An Introduction</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adapting Markov Decision Process for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AdaRank: A Boosting Algorithm for Information Retrieval</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;07</title>
				<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Query Change Model: Modeling Session Search As a Markov Decision Process</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyi</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2015-05">2015. May 2015</date>
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Predicting Diverse Subsets Using Structural SVMs</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML &apos;09)</title>
				<meeting>the 26th Annual International Conference on Machine Learning (ICML &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reinforcement Learning to Rank with Markov Decision Process</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi Page Search with Reinforcement Learning to Rank</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR &apos;18</title>
				<meeting>the 2018 ACM SIGIR International Conference on Theory of Information Retrieval (ICTIR &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="175" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A POMDP Model for Contentfree Document Re-ranking</title>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1139" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for Page-wise Recommendations</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems (RecSys &apos;18)</title>
				<meeting>the 12th ACM Conference on Recommender Systems (RecSys &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3240323.3240374</idno>
		<ptr target="https://doi.org/10.1145/3240323.3240374" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="95" to="103" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining (KDD &apos;18)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;#38; Data Mining (KDD &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1040" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning for Search Result Diversi cation</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;19)</title>
				<meeting>the 25th annual ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
