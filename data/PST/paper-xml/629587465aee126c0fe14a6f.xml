<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Setting of Semi-supervised Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weikai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Setting of Semi-supervised Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>{li-za19</term>
					<term>dm18</term>
					<term>liwk19</term>
					<term>zhwang19</term>
					<term>zengzy19</term>
					<term>cyk20}@mails.tsinghua.edu.cn</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We argue that the present setting of semisupervised learning on graphs may result in unfair comparisons, due to its potential risk of overtuning hyper-parameters for models. In this paper, we highlight the significant influence of tuning hyper-parameters, which leverages the label information in the validation set to improve the performance. To explore the limit of over-tuning hyperparameters, we propose ValidUtil, an approach to fully utilize the label information in the validation set through an extra group of hyper-parameters. With ValidUtil, even GCN can easily get high accuracy of 85.8% on Cora. To avoid over-tuning, we merge the training set and the validation set and construct an i.i.d. graph benchmark (IGB) consisting of 4 datasets. Each dataset contains 100 i.i.d. graphs sampled from a large graph to reduce the evaluation variance. Our experiments suggest that IGB is a more stable benchmark than previous datasets for semisupervised learning on graphs. 1 * indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b3">[Gori et al., 2005;</ref><ref type="bibr" target="#b6">Kipf and Welling, 2016]</ref> have emerged as a heated field in machine learning in recent years. Among the widely known GNN models <ref type="bibr" target="#b6">[Kipf and Welling, 2016;</ref><ref type="bibr" target="#b3">Hamilton et al., 2017;</ref><ref type="bibr">Velickovic et al., 2018;</ref><ref type="bibr" target="#b1">Feng et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020]</ref>, which one is the best? Most GNN papers demonstrate their performance on the task of semi-supervised learning on graphs following GCN, where a widely-used benchmark includes <ref type="bibr">Cora, CiteSeer, and PubMed [Sen et al., 2008]</ref>.</p><p>This benchmark is competitive but unstable. For example, the accuracy of <ref type="bibr">GCNII [Chen et al., 2020]</ref> (SOTA method) on Cora is 85.5% with a dropout rate of 0.6, but it will drop to 79.0% if we slightly increase the dropout rate to 0.75. In con-trast, the reported accuracy of GCN is about 81.5%. <ref type="foot" target="#foot_1">2</ref> Previous researchers attributed this kind of instability to the small size of the graphs and put forward larger benchmarks, e.g., OGB <ref type="bibr" target="#b4">[Hu et al., 2020]</ref> and HGB <ref type="bibr" target="#b7">[Lv et al., 2021]</ref>. However, to the best of our knowledge, few works challenge the setting of semi-supervised learning on graphs.</p><p>A similar predicament of the unstable performance also exists in few-shot natural language understanding, where Zheng et al. <ref type="bibr">[2021]</ref> recently found that models were over-fitting the validation set via hyper-parameters. Since the labels in the validation set are even more than those in the training set, the searched value of hyper-parameters becomes vitally important. Inspired by this finding, we hypothesize that the same reason could also, to some extent, account for the instability of semi-supervised learning on graphs, since the size of the validation set is also usually much larger than that of the training set (e.g., 140 training samples vs. 500 validation samples in Cora) <ref type="bibr" target="#b8">[Yang et al., 2016]</ref>. Meanwhile, recent GNNs show a trend of owning more hyper-parameters. While GCN has very few hyper-parameters, GAT needs the accuracy on the validation set to determine its structures (e.g., the number of attention heads and the existence of a residual connection). PPNP <ref type="bibr">[Klicpera et al., 2019a]</ref> further requires a global diffusion radius and a teleport probability ?. GDC <ref type="bibr">[Klicpera et al., 2019b]</ref> searches a diffusion radius and a threshold for sparsification as hyper-parameters on the validation set, which is improved in ADC <ref type="bibr" target="#b11">[Zhao et al., 2021]</ref> by replacing the grid-search with gradient-based optimization for layerwise and channel-wise diffusion radii. This evolving path of GNNs suggests that GNN models utilize the validation set to a greater and greater extent and that the more explicit utilization improves and stabilizes the training. This benchmark leads to an unfair comparison favoring models with larger sizes of hyper-parameters.</p><p>The same phenomenon still exists in OGB <ref type="bibr" target="#b4">[Hu et al., 2020]</ref>, even though its percentage of validation set is much smaller than that of Cora. The participants <ref type="bibr">[Wang et al., 2021]</ref> find that directly merging validation set into the training set can significantly increase the performance, which is then only allowed on the collab dataset according to the updated OGB rules. Moreover, C&amp;S <ref type="bibr" target="#b5">[Huang et al., 2020]</ref> in-corporates the labels in the validation set during label propagation and obtains a great improvement. All of them indicate that simply reducing the validation set ratio or to increase the graphs' size is not a satisfying solution to the problem.</p><p>The findings urge us to rethink the setting of the semisupervised learning on graphs and the meaning of validation set. On the one hand, the original motivation of introducing validation set is to optimize the hyper-parameters, which cannot be directly optimized by usual methods, e.g., stochastic gradient descent (SGD). On the other hand, we have only two kinds of samples in real-world applications, labeled and unlabeled. We have to split out a part of the labeled data as the validation set to search for the best hyper-parameters. This means it is a disadvantage instead of a merit to own a large set of hyper-parameters because they use a great quantity of labeled data as a validation set and result in a smaller training set providing real-world information. However, under the current setting, the extra and informative validation set encourages the model to equip itself with more hyperparameters to fully utilize the labels in the validation set, which deviates from the real-world scenarios. In this work, we name the problem of "using hyper-parameters to fit validation labels" as over-tuning.</p><p>Present work. In this paper, we analyze the influence of the size of validation set and propose ValidUtil, a method to make any GNN fully utilize the label information in the validation set. ValidUtil explores the limit of over-tuning hyperparameters. To avoid meaningless optimization towards the utilization of validation set and increase the stability of GNN benchmarks, we construct the i.i.d. graph benchmark (IGB) with the following two improvements:</p><p>? Unify the training and validation set. In IGB, the graphs have no pre-defined ratio of training and validation set but only labeled data and unlabeled data. Different models can freely split the labeled data into training and validation set based on the number of hyper-parameters. In this way, the over-tuning of hyper-parameters is discouraged.</p><p>? Multiple i.i.d. graphs and diverse domains. We have 4 datasets consisting of a co-authorship network, a social network, a knowledge graph, and a photo sharing network. In each dataset, we sample 100 subgraphs using a modified Random-Walk method. The sampled graphs are approximately i.i.d, while each gives a reliable evaluation of GNN performance. Therefore, we can obtain more stable metrics by averaging performances over them.</p><p>2 The Risk of Over-tuning of Semi-supervised Learning on Graphs 2.1 Semi-Supervised Learning on Graphs Definition. Given an undirected graph G = (V, E), where the node set V contains n nodes {v 1 , ..., v n } and E is the edge set. Each node v i is associated with a feature vector x i and a class label y i . We denote the set of node labels as Y.</p><p>In the task of semi-supervised node classification on graphs (transductive), only a small part of node labels Y L ? Y are given, and the rest label set</p><formula xml:id="formula_0">Y U = Y -Y L needs to be predicted. Usually |Y L | |Y U |.</formula><p>Here, we briefly introduce three widely used citation networks (i.e., Cora, CiteSeer, PubMed) for the analysis in this section. In these datasets, node features are bag-of-words representations of documents. Each dataset is a connected graph constructed based on the citation links between documents. Each dataset uses 20 training samples per class as labeled data in the semi-supervised setting. Table <ref type="table" target="#tab_0">1</ref> shows the statistics of the three datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">An Analysis of Over-tuning in Current GNNs</head><p>In this section, we will investigate the over-tuning phenomenon in current GNNs. As discussed above, the hyperparameters act as a tool to utilize the labels in the validation set. Therefore, the models' performance should improve as we increase the size of the validation set. We select five representative GNNs, GCN, GAT, APPNP, GDC-GCN, and ADC, and exhibit their accuracy on Cora with different sizes of validation set. The search scope of the hyper-parameters includes learning rate, hidden size, early stopping iteration, number of layers, the dropout rate, the diffusion radius of APPNP and GDC, the sparsification threshold of GDC, etc. We use grid search to find the best hyper-parameters for each model. Details about the search scopes are shown in the released codes.</p><p>We run the experiments on the public split <ref type="bibr" target="#b8">[Yang et al., 2016]</ref> of Cora, ranging the size of validation set from 10 to 500 by hiding a part of the labels. For each validation size, we report the accuarcy on the test set after training the model with the best searched hyper-parameters. The results are demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows that the GNN models have a clear trend that the performance is usually better with a larger validation set. Since the validation set can only affect the model via the hyper-parameters, we can conclude that the model benefits from the validation labels with the help of hyper-parameters. The accuracy improvement is up to 1% ? 3% if we increase the size of validation set from 100 to 500, which is significant enough to suggest that the over-tuning already exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ValidUtil: Exploring the Limits of Over-tuning</head><p>Although the analysis above shows that over-tuning influences GNN models' performance, we wonder to what extent the influence could achieve. If the importance of validation labels is much smaller than that of model structures, the previous benchmarks will still be a proper choice for GNN benchmarks. If not, we should rethink and re-design the evaluation pipeline and datasets for semi-supervised learning on graphs.</p><p>The most intuitive method to fully utilize the validation labels is to merge the validation set into the training set. However, this operation is universally acknowledged as invalid and is forbidden since it is a kind of data leakage. In this section, we put forward ValidUtil, a technique to mimic this operation via searching hyper-parameters. ValidUtil is not really a method to improve GNNs, but more like a "reduction to absurdity". The full pipeline of ValidUtil is defined as follows and in Algorithm 1:</p><p>1. Add hyper-parameters. For any given model, we add t extra hyper-parameters Y T = {? 1 , ..., ?t }, which refers to "pseudo-labels of the t nodes in the validation set". These hyper-parameters affect the model training in a way that they act as a known label for the corresponding validation node; that is to say, the model will be trained on an augmented training set Y L ? Y T . And since the labels of the validation set given to the model are not ground-truth labels, this is not a data leakage process.</p><p>2. Alternately optimize hyper-parameters. The most common search method for hyper-parameters is grid search, but it is time-consuming when there are many hyper-parameters. Thus, we alternately search the best value of each ?i individually. In the beginning, each ?i is initialized as the predicted label y i of the GNN without ValidUtil. We will search and fix the best ?i one by one, following the method described in the next paragraph.</p><p>We set the dropout rate as 0 when searching for the best pseudo-labels.</p><p>3. Search the best pseudo-label for each validation node. To search for the best value of ?i , we enumerate all possible values of ?i while keeping the other hyperparameters unchanged. For each possible value, we train a GNN model using labels Y L ? Y T and select the best value according to the accuracy of the validation set. This is the standard process of searching a hyperparameter.</p><p>4. Train the final model based on the best pseudo-labels.  report the results on the test set. Analysis of Effectiveness. The key reason for the effectiveness of ValidUtil is that in most cases, we can obtain the true label y i for validation node v i in step 3, which makes the final training equivalent to the training on the union of training and validation set. If the model is over-parameterized, it is powerful enough to overfit the predicted label of v i as the true label y i after sufficient training<ref type="foot" target="#foot_2">3</ref> . Then in most cases, the highest accuracy is reached if and only if ?i = y i . We find that most GNNs models are powerful enough to overfit the pseudo-labels on Cora, Citeseer, and PubMed in practice. We demonstrate the performance of ValidUtil plus three GNN models, GCN, PPNP, and MixHop, in Figure <ref type="figure" target="#fig_2">2</ref>. We find that even only 20 ? 60 hyper-parameters from ValidUtil can bring about a leap in performance for some models. When we add hyper-parameters for all the 500 nodes in the validation set, PPNP can achieve an accuracy much better than that of the sota methods in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Remark. Although ValidUtil works purely by utilizing the validation labels, it is totally valid under the current setting. If we treat the GNN+ValidUtil as a black-box model, the training process is quite normal. ValidUtil actually utilizes the labels with low efficiency, because each hyper-parameter can only learn the information of one node -but this is enough to verify our hypothesis. The current setting cannot prevent the validation labels from "leaking" during hyperparameters tuning. We believe that there exist some more efficient ways to define influential hyper-parameters. These hyper-parameters could be entangled with the features or model structures, and they can acquire information from multiple validation labels. According to Figure <ref type="figure" target="#fig_0">1</ref>, such influential hyper-parameters might already exist in some models and cannot be easily detected. Therefore, it is urgent to construct a new benchmark for semi-supervised learning on graphs to avoid over-tuning and fairly and robustly compare GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IGB: An Independent and Identically</head><p>Distributed Graph Benchmark</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Our new benchmark has two aims: avoiding over-tuning and being more robust.</p><p>To avoid over-tuning, we propose a new setting where there are only two sets of nodes, labeled and unlabeled. The model can use the labeled set in any way to train the best model and evaluate its performance on the unlabeled (test) set. If we need to search hyper-parameters, we can split out a part of the labeled nodes as the validation set. The over-tuning problem is eliminated because the validation labels are already exposed. This setting is closer to real-world scenarios and enables fair comparison between models with different sizes of hyper-parameters. To easily migrate the GNNs to this new setting, we will introduce a simple and powerful method to create validation set in section 3.2.</p><p>To construct a more robust benchmark, we expect models' performances to be stable for different random seeds. One of the most common methods in machine learning to reduce the variance of evaluation results is to test repeatedly and report the average performance. To achieve that, we expect to test a model's performance on multiple i.i.d graphs. However, how can we get multiple i.i.d. Cora-like graphs to evaluate the results?</p><p>If we think about the construction of the citation networks such as Cora and Citeseer, we will find that the papers are crawled down by spiders from the Internet, which means these networks can be seen as sampled from the large realworld citation network. A similar assumption is already used in previous works <ref type="bibr" target="#b9">[Yang et al., 2020]</ref> that the real-world graph data are sampled from a large underlying graph. To acquire i.i.d. graphs, we could sample "again" from an established graph. With appropriate sampling strategies, we can construct a group of i.i.d. graphs. The details about sampling are introduced in section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Pipeline of Evaluation</head><p>To solve the over-tuning problem, we have to update the pipeline of the task of semi-supervised learning on graphs. Following the real-world scenarios, we only split the nodes in a graph into two sets, labeled and unlabeled (with a ratio of 1:4 by default in IGB). The model can use the labeled set in any way to train the best model, and evaluate its performance on the unlabeled (test) set. A recommended method is as follows:</p><p>1. Divide the labeled set into training and validation sets.<ref type="foot" target="#foot_3">4</ref> 2. Find the best hyper-parameters using grid search on the training and validation sets from the first step.</p><p>3. Train the model with the best hyper-parameters on the full labeled nodes.</p><p>4. Test the performance of the model from the third step on the unlabeled (test) sets.</p><p>5. Repeat the above steps on each graph in a dataset and report the average accuracy.</p><p>The first two steps aims to find the best hyper-parameters for the GNN model. We believe that this approach is suitable for many GNN models to get satisfying hyper-parameters. If there are other reasonable methods to decide the best hyperparameters with the labeled set, they will also be encouraged to replace the first two steps in this pipeline. In this way, we can avoid over-tuning by directly exposing all the label information in the validation set later in the third step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>IGB consists of four datasets: AMiner <ref type="bibr" target="#b8">[Tang et al., 2008]</ref>,</p><p>Facebook <ref type="bibr" target="#b7">[Rozemberczki et al., 2019]</ref>, NELL <ref type="bibr" target="#b8">[Yang et al., 2016]</ref>, and Flickr <ref type="bibr" target="#b10">[Zeng et al., 2019]</ref>. Each dataset contains 100 undirected connected graphs, sampled from the original large graph according to the random walk method in section 3.4. We also report the average node overlap rate, the ratio of common nodes to the total size of nodes for a pair of sampled graphs. The coverage rate is defined as the ratio of the union of the 100 sampled graph to the original large graph. Lower overlap rate and higher coverage rate are preferred. The statistics of the datasets are reported in Table <ref type="table" target="#tab_3">3</ref>. AMiner. The AMiner dataset is a co-authorship graph extracted from the AMiner system <ref type="bibr" target="#b8">[Tang et al., 2008]</ref>. Nodes represent authors, while edges mean co-authorship in at least one paper. Node features indicate the venues in which the author has publications. Specifically, each feature has 3,883 dimensions, and each dimension is 0 or 1, representing whether or not an author has had publications in the corresponding venue. Node labels represent the authors' main research fields.</p><p>Facebook. The Facebook dataset is the Facebook Page-Page dataset from the paper <ref type="bibr" target="#b7">[Rozemberczki et al., 2019]</ref>. It is a graph of official Facebook pages. Nodes are official Facebook pages, while edges are mutual likes between the pages. Node features are extracted from the page descriptions. Node labels are one of the following 4 categories defined by Facebook: politician, governmental organization, television show, and company.</p><p>NELL. The NELL dataset is a knowledge graph dataset generated from the NELL knowledge graph <ref type="bibr" target="#b0">[Carlson et al., 2010]</ref>. Nodes represent entities, and edges represent relationships between two entities. Each node initially has a 61,278dimension feature, a binary bag-of-words representation of entity descriptions. We only reserve the features of the most frequent 10,000 words for efficiency.</p><p>Flickr. The Flickr dataset is a graph of photos uploaded to the Flickr website. Each node represents a photo, and an edge means two photos share some properties in common, such as from the same location or the same gallery. 500dimension node features are bag-of-words representations of photos. The labels are one of 7 classes developed by Zeng et al. from 81 original tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sampling Algorithm</head><p>The simplest way to make the subgraph's node label distribution similar to that of the original graph is vertex sampling. However, it does not meet our expectations because it generates unconnected subgraphs. To obtain nearly i.i.d. subgraphs for our benchmark, we must carefully design the sampling strategy and principles. Specifically, we expect the sampling strategy to have the following properties:</p><p>1. The sampled subgraph is a connected graph.</p><p>2. The distribution of the subgraph's node labels is close to that of the original graph. 3. The distribution of the subgraph's edge categories (edge category is defined by the combination of its two endpoints' labels) is close to that of the original graph.</p><p>The first property can be well satisfied by the Random-Walk (RW) algorithm. When performing RW on an undirected graph G = (V, E), we start sampling from node u = n 0 , and the following nodes can be selected by the transition possibility:</p><formula xml:id="formula_1">P u,v = 1 du , if (u, v) ? E, 0,</formula><p>otherwise,</p><p>where P u,v is the transition possibility from node u to v, and d u is the degree of node u.</p><p>We retreat to reject sampling-like methods to guarantee the second and third properties. Here we introduce the Kullback-Leibler divergence (KL divergence) as a metric to measure the difference between two different distributions. Aiming to get 100 subgraphs with relatively low KL divergence of node labels' distribution ("Node KL") and edge categories' distribution ("Edge KL"), we set a pre-defined threshold to decide whether to accept a sampled subgraph or not. The comparison of the results before and after adding the threshold is shown in Table <ref type="table" target="#tab_4">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Benchmarking Results</head><p>We evaluate 7 representative GNNs on IGB: Grand <ref type="bibr" target="#b1">[Feng et al., 2020]</ref>, <ref type="bibr">GCNII [Chen et al., 2020]</ref>, APPNP [Klicpera  <ref type="table" target="#tab_5">5</ref>.</p><p>To evaluate the GNN model, we first define a search scope for each hyper-parameter. The scope is carefully chosen in order to include the best values on all the datasets. For each model, the best hyper-parameters in its original paper and the best hyper-parameters reported by <ref type="bibr">CogDL [Cen et al., 2021]</ref> are usually included in the search scopes. After that, we use our IGB benchmark to evaluate each model under the setting introduced in the section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The Stability of IGB</head><p>We verify the stability of IGB in two ways. Firstly, we verify its stability when evaluating models on different graphs, as each IGB dataset contains 100 nearly i.i.d. graphs. Specifically, we compare the variances of the accuracies on 100 AMiner's subgraphs (IGB style) and on 100 Cora's random data splits (Cora style). The result shown in Figure <ref type="figure" target="#fig_3">3</ref> strongly suggests that the evaluation on IGB is more stable than Cora style, even though each AMiner graph uses random data split. Secondly, we focus on IGB's stability when evaluating models with different random seeds. In a stable benchmark, the ranks of different models should not easily change when changing the random seed. To verify this, we use the "inversion number" 5 of the ranking as a metric. Specifically, we evaluate the seven models using ten different random seeds, providing ten ranking sequences S i (i ? [1, 10]). On each ranking sequence, we sort models according to their accuracy. The ranking sequence of the first seed is used as a reference sequence S 1 = {m 1 , m 2 ...m 7 }, where m i is a GNN model. 5 The definition of "Inversion" can be seen in Wikipedia.</p><p>We call m j &gt; m k if and only if m j ranks higher than m j on S 1 . For another sequence S i , i = 1, if m j &gt; m k , but m j ranks lower than m k on S i , we call (j, k) an inversion pair in S i . "Inversion number" is the number of inversion pairs in all sequences S i , i = 1. Therefore, a high "inversion number" indicates a high instability of evaluation with different seeds. The result is reported in Table <ref type="table" target="#tab_6">6</ref>. IGB has a significantly smaller inversion number than Cora, CiteSeer, and PubMed, demonstrating its strong stability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Is limiting the number of hyper-parameters a good way to solve the over-tuning problems? In section 2, we illustrate the power of over-tuning, where the improvements basically correlate with the number of hyper-parameters. However, if we set a hard limit for the number of hyper-parameters, complicated optimizers with many hyper-parameters, e.g. Adam [Kingma and Ba, 2014], will not be encouraged due to this limit. The models will also be encouraged to investigate more influential hyper-parameters to utilize the labels in the validation set under the limited budget of hyper-parameters. Therefore, the most fundamental solution is to change the evaluation setting as IGB.</p><p>What is the best GNN? In the results of IGB, GCNII performs the best. However, the performance differs in different datasets. For example, the sota method on Citeseer, GRAND, performs badly on NELL and thus gets a low average score, because NELL is a knowledge graph, whose distribution is quite different from that of citation networks. Will a GNN be good at all kinds of graphs, or do we need to design different GNNs for different categories of graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we revisit the setting of semi-supervised learning on graphs, identify the over-tuning problem, and prove its significance via the experiments of ValidUtil. To solve it, we propose a new benchmark, IGB, with a more reasonable evaluation pipeline. To further increase evaluation stability, we propose a sampling algorithm based on RW. GNNs are evaluated on the new benchmark, and the results demonstrate a stable performance rank. We expect that IGB can benefit the graph learning community by stabilizing the evolving path of GNNs in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The accuracy of models with different size of the validation set on Cora. The test accuracy is the average of 20 runs with different random seeds.</figDesc><graphic url="image-1.png" coords="3,67.29,55.14,216.41,165.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>M with Y L ? Y T . 9: Acc l = the accuracy of M on validation set. for 18: Train a new GNN M with Y L ? Y T . 19: return the accuracy of M on test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The test accuracy of ValidUtil plus GCN, GAT, MixHop and PPNP on Cora, Citeseer, and PubMed. The horizontal axis means the number of new hyper-parameters from ValidUtil, where 0 is equivalent to the original GNN without ValidUtil.</figDesc><graphic url="image-2.png" coords="4,54.00,54.00,504.01,134.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy of GNNs on AMiner and Cora. The blue area is the fluctuation range of the test accuracy. The results are based on 100 runs on AMiner's subgraphs or Cora's random splits.</figDesc><graphic url="image-3.png" coords="6,54.00,431.37,243.00,89.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Cora / CiteSeer / PubMed datasets.</figDesc><table><row><cell cols="2">Dataset Nodes Edges</cell><cell>Split</cell><cell cols="2">Classes Features</cell></row><row><cell>Cora</cell><cell cols="2">2,708 5,429 140 / 500 / 1,000</cell><cell>7</cell><cell>1,433</cell></row><row><cell cols="3">CiteSeer 3,327 4,732 120 / 500 / 1,000</cell><cell>6</cell><cell>3,703</cell></row><row><cell cols="3">PubMed 19,717 44,338 60 / 500 / 1,000</cell><cell>3</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ValidUtil Input Graph G with nodes in training/validation/test set. We denote their labels as Y L /Y valid /Y test . There are k classes. Output Accuracy of the GNN model on test set. 1: Train a GNN model M with Y L . 2: Predict the labels of validation set with M as Y P = {y 1 , ..., y t }. 3: Add t extra hyper-parameters Y T = {? 1 , ..., ?t }. 4: Initialize Y T = Y P .</figDesc><table /><note><p>After the pseudo-labels are determined, we can train on Y L ? Y T , and use ordinary grid search to determine other hyper-parameters, including the dropout rate, and Algorithm 1 5: for i from 1 to t do 6:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between ValidUtil and the sota methods on Cora, Citeseer, and PubMed.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer PubMed</cell></row><row><cell>GCNII (sota Cora)</cell><cell>85.5</cell><cell>73.4</cell><cell>80.3</cell></row><row><cell cols="2">GRAND (sota Citeseer) 85.4</cell><cell>75.4</cell><cell>82.7</cell></row><row><cell>SAIL (sota PubMed)</cell><cell>84.6</cell><cell>74.2</cell><cell>83.8</cell></row><row><cell>GCN + ValidUtil</cell><cell>85.8</cell><cell>76.0</cell><cell>83.8</cell></row><row><cell>MixHop + ValidUtil</cell><cell>84.9</cell><cell>75.5</cell><cell>84.2</cell></row><row><cell>PPNP + ValidUtil</cell><cell>85.8</cell><cell>77.3</cell><cell>84.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the datasets in IGB.</figDesc><table><row><cell></cell><cell>AMiner</cell><cell>Facebook</cell><cell>NELL</cell><cell>Flickr</cell></row><row><cell cols="5">Average Nodes 4,485 ? 26 3,475 ? 71 3,540? 68 4,452 ? 31</cell></row><row><cell>Edges</cell><cell>5,000</cell><cell>5,000</cell><cell>5,000</cell><cell>5,000</cell></row><row><cell>Features</cell><cell>3,883</cell><cell>128</cell><cell>10,000</cell><cell>500</cell></row><row><cell>Classes</cell><cell>8</cell><cell>4</cell><cell>164</cell><cell>7</cell></row><row><cell>Original Size</cell><cell>236,017</cell><cell>22,470</cell><cell>63,910</cell><cell>89,250</cell></row><row><cell>Overlap Rate</cell><cell>0.083</cell><cell>0.339</cell><cell>0.146</cell><cell>0.119</cell></row><row><cell>Coverage Rate</cell><cell>0.550</cell><cell>0.958</cell><cell>0.956</cell><cell>0.969</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>KL divergence of the sampling results.</figDesc><table><row><cell></cell><cell>AMiner</cell><cell>Facebook</cell><cell>NELL</cell><cell>Flickr</cell></row><row><cell>Node KL</cell><cell cols="4">0.0186 ? 0.0107 0.1306 ? 0.0427 0.4393 ? 0.1008 0.0060 ? 0.0025</cell></row><row><cell>Edge KL</cell><cell cols="4">0.0189 ? 0.0149 0.0284 ? 0.0121 0.2796 ? 0.0678 0.0046 ? 0.0015</cell></row><row><cell>+Thresholds</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Node KL</cell><cell cols="4">0.0123 ? 0.0024 0.0326 ? 0.0064 0.3184 ? 0.0243 0.0041 ? 0.0006</cell></row><row><cell>Edge KL</cell><cell cols="4">0.0062 ? 0.0008 0.0243 ? 0.0120 0.2068 ? 0.0179 0.0021 ? 0.0003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluation Results of GNNs on IGB.</figDesc><table><row><cell></cell><cell>AMiner</cell><cell>Facebook</cell><cell>NELL</cell><cell>Flickr</cell><cell>Avg</cell></row><row><cell>Grand</cell><cell cols="5">82.5?0.8 88.9?1.0 84.4?1.1 44.3?0.8 75.0</cell></row><row><cell>GCN</cell><cell cols="5">76.5?1.1 87.9?1.0 93.9?0.7 41.9?1.3 75.1</cell></row><row><cell>GAT</cell><cell cols="5">78.8?1.0 88.3?1.2 91.1?1.2 43.1?1.3 75.3</cell></row><row><cell cols="6">GraphSAGE 81.6?0.8 87.2?1.1 94.9?0.6 43.4?0.9 76.8</cell></row><row><cell>APPNP</cell><cell cols="5">87.0?1.0 88.0?1.3 93.0?0.8 44.6?0.9 78.2</cell></row><row><cell>MixHop</cell><cell cols="5">86.1?1.1 89.1?0.9 94.7?0.7 43.5?1.2 78.4</cell></row><row><cell>GCNII</cell><cell cols="5">88.4?0.6 89.5?0.9 91.5?1.0 44.7?0.8 78.5</cell></row><row><cell cols="6">et al., 2019a], GAT [Velickovic et al., 2018], GCN [Kipf</cell></row><row><cell cols="6">and Welling, 2016], GraphSAGE [Hamilton et al., 2017] and</cell></row><row><cell cols="6">MixHop [Abu-El-Haija et al., 2019]. The results are shown</cell></row><row><cell>in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The inversion numbers of the ranking sequences using 10 different random seeds. Smaller inversion number indicates better stability.</figDesc><table><row><cell cols="7">Cora CiteSeer PubMed AMiner Facebook NELL Flickr</cell></row><row><cell>67</cell><cell>45</cell><cell>107</cell><cell>0</cell><cell>9</cell><cell>0</cell><cell>5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code and data are released at https://github.com/THUDM/ IGB/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The experiments can be reproduced using the CogDL package<ref type="bibr" target="#b0">[Cen et al., 2021]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Some weak GNNs, e.g. vanilla GCN and GAT on CiteSeer, cannot well distinguish nodes in a strongly connected graph, and therefore cannot overfit the given labels. We add an additional selfloop for each node for GCN to solve the problem. This trick is already implemented in PyG [Fey and<ref type="bibr" target="#b2">Lenssen, 2019]</ref> by passing improved=True for GCNs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The best ratio may differ from model to model. In practice, we find that 1:1 is an appropriate ratio for most models.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Abu-El-Haija</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00959</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2019. 2019. 2010. 2010. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cogdl: An extensive toolkit for deep learning on graphs</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph random neural network for semisupervised learning on graphs</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Fey</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lenssen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2017. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
	<note>IJCNN</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization</title>
		<imprint>
			<publisher>Kingma and Ba</publisher>
			<date type="published" when="2014">2020. 2020. 2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Welling ;</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Klicpera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semi-supervised classification with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks</title>
		<author>
			<persName><surname>Klicpera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>
			<persName><forename type="first">Galileo</forename><surname>Sen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mustafa</forename><surname>Namata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lise</forename><surname>Bilgic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brian</forename><surname>Getoor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tina</forename><surname>Gallagher</surname></persName>
		</editor>
		<editor>
			<persName><surname>Eliassi-Rad</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1909">2019. 2019. 2021. 2019. CoRR, abs/1909.13021, 2019. 2008. 2008</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
		</imprint>
	</monogr>
	<note>In KDD</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Velickovic et al., 2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio&apos;, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02936</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Salakhudinov</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2018. 2021. 2021. 2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding negative sampling in graph representation learning</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD 2020</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1666" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
		<idno>CoRR, abs/1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive diffusion in graph neural networks</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12742</idno>
	</analytic>
	<monogr>
		<title level="m">Sebastian Ruder, and Zhilin Yang. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding</title>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
