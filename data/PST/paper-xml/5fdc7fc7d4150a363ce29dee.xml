<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Exposure Convolutional Memory Network for accurate estimation of finger kinematics from surface electromyographic signals</title>
				<funder ref="#_BJp6gNq #_WaTURsv">
					<orgName type="full">Shenzhen Basic Research Grants</orgName>
				</funder>
				<funder>
					<orgName type="full">Pioneer Hundred Talents Program of Chinese Academy of Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Xueyuan Blvd</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>1068, 518055</postCode>
									<settlement>Xili Nanshan, Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Yanqihu Campus</orgName>
								<address>
									<addrLine>380 Huaibeizhuangcun, Huairou District</addrLine>
									<postCode>101408</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenfei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Medicine and Biological Information Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<addrLine>195 Chuangxin Road</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hunnan New District</orgName>
								<address>
									<postCode>110016</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Xueyuan Blvd</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>1068, 518055</postCode>
									<settlement>Xili Nanshan, Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Xueyuan Blvd</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>1068, 518055</postCode>
									<settlement>Xili Nanshan, Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Xueyuan Blvd</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>1068, 518055</postCode>
									<settlement>Xili Nanshan, Shenzhen</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Yanqihu Campus</orgName>
								<address>
									<addrLine>380 Huaibeizhuangcun, Huairou District</addrLine>
									<postCode>101408</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dario</forename><surname>Farina</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Bioengineering</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution" key="instit1">Neuromechanics &amp; Rehabilitation Technology Group</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ning</forename><surname>Jiang</surname></persName>
							<email>ning.jiang@uwaterloo.ca</email>
							<affiliation key="aff5">
								<orgName type="department">Department of Systems Design Engineering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Ave. W</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuang</forename><surname>Lin</surname></persName>
							<email>linchuang_78@126.com</email>
							<affiliation key="aff6">
								<orgName type="department">Ganjingzi District</orgName>
								<orgName type="institution">Dalian Maritime University</orgName>
								<address>
									<addrLine>1 Linghai Rode</addrLine>
									<postCode>116026</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Exposure Convolutional Memory Network for accurate estimation of finger kinematics from surface electromyographic signals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>simultaneous</term>
					<term>proportional</term>
					<term>estimation</term>
					<term>finger joint angle</term>
					<term>surface electromyography</term>
					<term>convolutional Long Short-Term Memory network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective. Estimation of finger kinematics is an important function of an intuitive humanmachine interface, such as gesture recognition. Here, we propose a novel deep learning method, named Long Exposure Convolutional Memory Network (LE-ConvMN), and use it to proportionally estimate finger joint angles through surface electromyographic (sEMG) signals. Approach. We use a convolution structure to replace the neuron structure of traditional Long Short-Term Memory (LSTM) networks, and use the long exposure data structure which retains the spatial and temporal information of the electrodes as input. The Ninapro database, which contains continuous finger gestures and corresponding sEMG signals was used to verify the efficiency of the proposed deep learning method. The proposed method was compared with LSTM and Sparse Pseudo-input Gaussian Process (SPGP) on this database to predict the 10 main joint angles on the hand based on sEMG. The correlation coefficient (CC) was evaluated using the three methods on eight healthy subjects, and all the methods adopted the root mean square (RMS) features. Main results. The experimental results showed that the average CC, RMSE, NRMSE of the proposed LE-ConvMN method (0.82?0.03,11.54?1.89,0.12?0.013) was significantly higher than SPGP (0.65?0.05, p? 0.001; 15.51?2.82, p?0.001; 0.16?0.01, p?0.001) and LSTM (0.64?0. 06, p?0.001;  14.77?3.21, p?0.001; 0.15?0.02, p=?0.001). Furthermore, the proposed real-timeestimation method has a computation cost of only approximately 82 ms to output one state of ten joints (average value of 10 tests on TitanV GPU). Significance. The proposed LE-ConvMN method could efficiently estimate the continuous movement of fingers with sEMG, and its performance is significantly superior to two established deep learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Currently, robots are extensively used in industrial, military, commercial, and medical applications for improving the quality of human life. As such, the interactions between individuals and robots have become increasingly frequent. Human-robot interaction has become an integral part of systems such as active prostheses, robot-assisted surgery, drone reconnaissance. Building a user-friendly, intuitive and accurate human-machine collaboration (HMC) or humancomputer interaction (HCI) has drawn much attention in recent years. One of the key challenges in HMC and HCI is the decoding of human movement intentions from biological signals. The surface electromyographic (sEMG) signal, electric manifestation of muscular contractions, is a direct source to extract effective information on movement intent and execution. This signal has been indeed used as a source for decoding human movement intentions for decades <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>.</p><p>The human hand is extremely complex <ref type="bibr" target="#b2">[3]</ref>, embedding a large number of afferents and intrinsic muscles that articulates &gt;20 degrees of freedom (DoFs) <ref type="bibr" target="#b3">[4]</ref>. Using sEMG, estimating intent of fine movements of the hand, such as different hand gestures, is a challenging task <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>.</p><p>For decades, one of the most popular ideas in myoelectric analysis and control has been the use of manually selected features and machine learning algorithms to decode sEMG and recognize motions <ref type="bibr" target="#b6">[7]</ref>. Recently, deep learning algorithms have been used that can automatically select the features, which is more effective for sEMG applications. Most of the related works on sEMG pattern recognition focus on the improvement of classification accuracy and the number of discriminated motions <ref type="bibr" target="#b2">[3]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>. Phinyomark et al. <ref type="bibr" target="#b9">[10]</ref> investigated fifty time-domain and frequency-domain features for classifying ten upper limb motions using sEMG. Krasoulis et al. <ref type="bibr" target="#b10">[11]</ref> used the exp-kernel to solve the force regression and movement classification problems. Zanghieri et al. <ref type="bibr" target="#b7">[8]</ref> proposed Temporal Convolutional Networks (TCNs) for robust gesture recognition. Although these classification algorithms are able to distinguish actions commonly used in daily life, they do not provide a natural and fine-grained control. More recent literature began to shift research focus from discrete movement classification to continuous movement regression. Although the estimation of continuous human motor intention from EMG signal and its application have been investigated <ref type="bibr" target="#b11">[12]</ref>[13] <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b15">[16]</ref>, to the best of our knowledge, the current methods do not satisfy the criteria for accuracy, real-time performance and large number of estimated joint angles, to allow continuous hand gesture recognition.</p><p>EMG-based continuous motion estimation approaches can be categorized as model-based and model-free approaches. Model-based approaches include kinematics models <ref type="bibr" target="#b16">[17]</ref>, musculoskeletal models <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref>, or dynamic models in general. However, the model-based approaches need to build a complex physical model and it is generally difficult to accurately estimate the relation between sEMG and joint angles. The model-based approaches are generally used to determine physical parameters such as position and acceleration. On the other hand, currently, researchers tend to use model-free approaches when facing practical control problem. For example, Lin et al. proposed a simultaneous and proportional multiple degree of freedom (DOF) myoelectric control method for active prostheses using sparse constraint non-negative matrix factorization. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> presented a simultaneous and continuous kinematics estimation method which used a single ANN for 4-DoFs across shoulder and elbow joints. Xiloyannis et al. <ref type="bibr" target="#b20">[21]</ref> used the Gaussian process to estimate hand movements to achieve an accuracy of 0.79. Quivira et al. <ref type="bibr" target="#b21">[22]</ref> proposed a hand pose estimation approach from low-cost sEMG systems using recurrent neural networks (RNN). However, the estimation accuracy and time delay of these methods are still below the needs for clinical translation.</p><p>In order to meet the needs on precision and low-latency, we propose a novel and low-computational-cost approach, which we named Long Exposure Convolutional Memory network (LE-ConvMN). This approach builds the connection between sEMG recordings and hand kinematics. This method was validated on the currently widely used Ninapro dataset and compared with two classical methods, i.e. LSTM <ref type="bibr" target="#b22">[23]</ref> and Sparse Pseudo-input Gaussian Process (SPGP) <ref type="bibr" target="#b20">[21]</ref>. These two compared methods are those with current best performance for multiple joint angle estimation of finger movements <ref type="bibr" target="#b23">[24]</ref>. The experimental results showed that LE-ConvMN has the best estimation performance on both stability and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Ninapro <ref type="bibr" target="#b24">[25]</ref> is currently the repository with the largest number of intact and hand amputated subjects in the literature. In this study, we design experiments on 6 movements shown in Fig. <ref type="figure" target="#fig_1">3</ref> from 8 subjects. Because the training of the deep learning algorithm is time consuming (e.g., LSTM took 33.25 hours), we choose 8 subjects out of the database. However, these 8 subjects are chosen to cover the all subjects' information as much as possible, i.g. the range of the height weights, gender, age and dominant hand are 154cm-187cm, 50kg-90kg, 5-male/3-female, 24-35, 6-right/2-left, respectively. Grasping movement is the most commonly used hand movement in daily life. We have selected these 6 types of grasping movement based on representative shapes and diameters. Hand kinematics was measured using a 22-sensor CyberGlove II data-glove and sEMG is collected by Delsys Trigno Wireless System, which comprises 12 wireless sEMG electrodes. These electrodes sample the raw sEMG signal at a rate of 2 kHz. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, we choose Proximal Interphalangeal point (PIP) and Metacarpophalangeal point (MCP) as estimated joint because they are the main active joints in the grasping movement.</p><p>The hand kinematics movement was collected at 20 Hz and resampled to 2000 Hz to synchronise with the sEMG signals.</p><p>The sEMG signals and hand joint angle signals were divided into fragment sequences of 100-ms duration and sliding steplength of 0.5 ms. To reduce noise and simplify the computation, the root mean square (RMS) value was chosen as feature. After the RMS processing, each movement (containing six trials) was segmented into training dataset (four trials) and testing dataset to perform the train-test experiment on the LE-ConvMN as well as the two contrast neural networks with respect to the hand joint angle movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters for evaluation</head><p>Pearson correlation coefficient is the most commonly used linear correlation coefficient, which can reflect the linear Fig. <ref type="figure">1</ref>. One-step RMS to build 3D inputs which contain more detail information on time and space. The minor processing window contains 12 channels 100ms sEMG and the sliding step-length is 0.5 ms. sEMG is collected from 12 electrodes which 1-8 electrodes equally spaced around the forearm 9 and 10 spaced on the surface of flexor digitorum superficialis and digitorum superficial extenso, 11 and 12 measuring signals from Biceps Brachii and Triceps Brachii. </p><p>Where ? ??? , ? ??? ????? ,? ???? , ? ???? ?????? presents the value of estimated joint angle, the mean value of estimated joint angles, the value of real joint angle and the mean value of real joint angles, respectively. The CC value is between -1 and 1, which can be used to evaluate the performance of the algorithm. The closer CC value is to 1, the more similar the predicted finger trajectory is to that of the actual movement, and the higher accuracy of the estimation can reach. Root mean square error (RMSE) can reflect the deviation between two variables. Here, RMSE is used to evaluate the deviation between the estimated value and the measured value of each joint angle, in degrees( ?). RMSE is calculated as follows:</p><formula xml:id="formula_1">???? = ?? (? ??? -? ???? ) 2 ? ? ?<label>(2)</label></formula><p>RMSE is the numerical deviation between the predicted angle and the actual angle, so it can reflect the performance of the algorithm from the angle amplitude of joint. For the same joint, the smaller RMSE is, the closer the predicted value is to the actual value, the higher accuracy of the estimation can reach.</p><p>Due to the different range of motion of each joint, RMSE is unable to compare the performance of algorithms between different joints. So, we defined Normalized RMSE as a complement to evaluate different joints:</p><formula xml:id="formula_2">????? = ???? ? ??? -? ??? ,<label>(3)</label></formula><p>where, ? ??? represents the maximum value of the actual measured angle of a joint, ? ??? represents the minimum value of the actual measured angle of the joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis</head><p>Due to the limited number of subjects, Friedman test was used to analyze significant differences among subjects with different evaluating methods, and Wilcoxon signed-rank test was used for post hoc comparisons. P value is corrected by Bonferroni correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Pseudo-input Gaussian Process (SPGP)</head><p>For non-linear non-parametric regression and classification, especially in high-dimensional space, the Gaussian process (GP) has shown to have an outstanding performance <ref type="bibr" target="#b20">[21]</ref> <ref type="bibr" target="#b25">[26]</ref>. Assuming a dataset ? = {?, ?}, the input vector ? = {? ? } ?=1 ? and the target value ? = {? ? } ?=1 ? , we aim to find a function relating X and Y <ref type="bibr" target="#b0">[1]</ref>. According to the standard GP, the targets are assumed to obey a Gaussian distribution? ~ ?(0, ? ? + ? ? 2 ?), where ? ? 2 ? present Gaussian noise with zero mean and ? ? 2 variance, and ? ? is the covariance matrix commonly calculated by a kernel function, such as the commonly used </p><formula xml:id="formula_3">?(?|?, ?, ?) = ? (?| ? ? ? (? ? + ? ? 2 ?) -1 ?, ? ?? -? ? ? (? ? + ? ? 2 ?) -1 ? ? + ? 2 ) (4)</formula><p>where ? ? = ?(? ? , ?) and ? ?? = ?(?, ?) , and ? and ? 2 hyperparameters. Unfortunately, when the data set is large, the computational complexity and time of the standard GP would be too large for most practical applications. The complexity of training will be ?(? 3 ) because the dimension of K is ? 2 . As a result, the complexity of the testing will be at least ?(? 2 ). In order to solve these computational problems, Snelson et al.</p><p>proposed SPGP (Sparse Pseudo-input Gaussian Process) where the entire set N in standard GP is replaced by a small number of points M (? ? ?) <ref type="bibr" target="#b1">[2]</ref>. Given the pseudo-input ? ? = {?? ? } ?=1 ? , and the pseudo target ? ? = {? ? ? } ?=1 ? , the distribution of a single point is:</p><formula xml:id="formula_4">?{?|?, ?, ? ? ? } = ?(?|? ? ? ? ? -1 ? ? , ? ?? -? ? ? ? ? -1 ? ? + ? 2 ) (5)</formula><p>where ? ? = ?(?? ? , ?? ? ), and ? ? = ?(?? ? , ?).</p><p>As for the entire data, the complete likelihood is given as:</p><formula xml:id="formula_5">?{?|?, ?, ? ? ? } = ?(?|? ?? ? ? -1 ? ? ?? + ? 2 ?)<label>(6)</label></formula><p>where ? = diag(?), ? ? = ? ?? -? ? ? ? ? -1 ? ? , and ? ?? = ?(? ? , ?? ? ). Then the posterior distribution of ? ? can be found as:</p><formula xml:id="formula_6">?(? ? |?, ? ? )?(? ? |? ? ? ? -1 ? ?? (? + ? 2 ) -1 ?, ? ? ? ? -1 ? ? )<label>(7)</label></formula><p>where ? ? = ? ? + ? ?? (? + ? 2 ?) -1 ? ?? . Now given a new input ? ? , the distribution of the target ? ? is:</p><formula xml:id="formula_7">p(? ? |? ? , ?, ? ? ) = ? ?? ? ?(? ? |? ? , ? ? , ? ? )?(? ? |?, ? ? ) = ?(? ? |? ? , ? ?2 ) (8)</formula><p>where ? ? = ? ?? ? ? -1 ? ?? (? + ? 2 ?) -1 ? and ? ?2 = ? ?? -? ?? (? ? -1 -? ? -1 )? ? + ? 2 .</p><p>The complexity of ? ? is ?(? 2 N) and therefore the complexity in the testing process is ?(? 2 ) . Thus, the complexity and computation time have no association with the size of the data set. In the current work, we utilized the code from the Sheffield Machine Learning Group <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long short-term memory (LSTM)</head><p>Long short-term memory (LSTM) neural network is a type of recurrent neural network (RNN). The basic recurrent circle provides RNN the capacity to bring in the information from previous iterations. Therefore, RNN has been used widely in solving problems with temporal information <ref type="bibr" target="#b26">[27]</ref>. With the growth of data complexity, the short recurrent circle could not meet the demand which led to the proposal of LSTM. The LSTM structure is built to implement long term dependencies. Being one type of RNN, LSTM has been proved more capable on the regression problem or continuous output fitting problem. It controls the information pass from time ? to time ? + 1 by a gate structure that contains a matrix weight. Hence, important information maintains a relatively long term to take effect. LSTM has been employed for natural language processing <ref type="bibr" target="#b27">[28]</ref>, intention recognition <ref type="bibr" target="#b28">[29]</ref>, continuous control <ref type="bibr" target="#b23">[24]</ref>, and many other applications. In previous studies <ref type="bibr" target="#b21">[22]</ref> <ref type="bibr" target="#b29">[30]</ref>, RNN can extract kinematics information efficiently from sEMG signals. In this study, we take advantages of LSTM to implement simultaneous and proportional control as a comparison to our proposed method. The procedure for the LSTM algorithm was implemented as follows.</p><p>Assuming ? = [? 0 , ? 1 , ? ? ? , ? ] is the one-dimensional input vector, the middle-hidden layer state vector ? = [? 0 , ? 1 , ? ? ? , ? ] can be written as:</p><formula xml:id="formula_8">? = ?(? ? ? ? + ? ? ? ?-1 + ? ? ) (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>where ? ? stands for the input weight matrix, ? ? is the hidden weight matrix, ? ? presents the hidden bias matrix and ? stands for the activation function. Then, the output sequence ? = [? 0 , ? 1 , ? ? ? , ? ] is obtained as: ? = ?(? ? ? ? + ? ? ) (10) Similarly, ? ? presents the output weight matrix, ? ? is the output bias and ? stands for the activation function. Weights in the LSTM control the maintenance of information. LSTM includes input gate, forget gate, memory cell and output gate. The structure can be seen in Fig. <ref type="figure" target="#fig_2">4</ref>. A single iteration is expressed as follows:</p><formula xml:id="formula_10">? ? = ?(? ?? ? ? + ? ?? ? ?-1 + ? ? )<label>(11)</label></formula><p>?? = tanh(? ?? ? ? + ? ?? ? ?-1 + ? ? )</p><p>? ? = ?(? ?? ? ? + ? ?? ? ?-1 + ? ? )</p><p>? ? = ?(? ?? ? ? + ? ?? ? ?-1 + ? ? ) ( <ref type="formula">14</ref>)</p><formula xml:id="formula_13">? ? = ? ? ? ? ?-1 + ? ? ? ?? (15) ? ? = ? ? ? ?(? ? )<label>(16)</label></formula><p>where ? ? , ? ? , ? ? , ? ? stand for input gate, output gate, forget gate and the LSTM cell state (which influences the output at the next iteration). The ? terms stand for the weight matrices and their subscripts denote their respective positions (e.g. ? ?? denotes the weight matrix from the input gate to the input), the ? terms stand for bias vectors (? ? is the input gate bias vector). ? and ? represent the activation functions of the hidden layer and the output layer, respectively. The symbol '?' represents element-wise product of the vectors. The three gates are influenced by the last step hidden state and the cell state. Moreover, the cell state is controlled by the forget gate from the last step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Exposure-convolutional Memory Network (LE-ConvMN)</head><p>Although LSTM has been proven to have powerful ability to process temporal data, it has suboptimal performance on spatial data and depends on many parameters. The major drawback of LSTM in handling spatiotemporal data is that it has to flatten the multi-dimensional inputs into a 1D vector before processing and, as a result, all the spatial information embedded in the original input data is lost. </p><p>The advantage of ConvLstm is that the internal state can maintain the time and space dimension of the input data.</p><p>ConvLstm has also good performance on temporal-spatial data. Thus, we propose an extension of ConvLstm, namely Long Exposure-convolutional Memory Network (LE-ConvMN) which is more suitable for processing myoelectric data. The LE-ConvMN determines the future state of a certain cell in the grid by the inputs and past states of its local neighbours. This can be achieved by using a convolution operator in the state-to-state and input-to-state transitions. The differences between the adjacent inputs are not so large in this application, so the full-connection structure in LSTM is not necessary. Full connection will greatly increase the complexity of the model, reduce the training speed and generalization ability. The use of convolutional structures not only reduces redundancy but also extracts combined information from multiple channels more efficiently (twodimensional convolution is adopted).</p><p>In previous work, researchers used to extract features in time and frequency domain within each channel. However, the interaction between channels was less explored. As shown in Fig. <ref type="figure">1</ref>, we convert the raw sEMG into a multi-channel Long exposure EMG sample collection which contains 100 subframes root mean square (RMS) features (1 sub-frame RMS feature is calculated from 100-ms EMG and the sliding steplength is 0.5 ms) rather than using 200 ms EMG signal as one features input. Obviously, comparing to other methods which only takes 1 frame feature input, the long exposure structure can make better usage of the information of EMG in time and space dimensions. Considering the requirement of data sufficiency for deep learning, we aim at a fine balance between presenting information to the network and the realtime performance of the pipeline. This operation is analogous to a photographer getting a richer photo by increasing the exposure times of the camera. Due to the enlargement of feature extraction area, we set the LE-ConvMN layer hidden sizes as 64, 32 ,10 to compress volume and extract features in higher dimension. All convolution operations use 3 * 3 convolution kernels. The stride and padding are 1. Meanwhile, compressing the feature dimension can also effectively prevent overfitting. At the end of the network we added a fully connected layer to reorganize the mapping of features to the target space. The structure of LE-ConvMN is shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>When Long exposure EMG collection (100 sub-frames) is provided as input to LE-ConvMN, features in time and space dimension of EMG sub-frame are extracted in each layer and kept in hidden states until 100 sub-frames of data are extracted. Then the hidden states of this layer will transfer to the next layer. According to our design, the dimension of the hidden state is reduced (64-32-10) layer by layer, which can complete the transition of data from low-dimensional features to highdimensional features. Finally, the fully connected layer will map the high-dimensional features to joint angles. Compared with traditional machine learning methods, LE-ConvMN can better perform the joint training of feature extraction and feature-to-target space mapping. In this study, we verify that adding a full connection layer at the end of the network can make the motion curve smoother.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result</head><p>We built LSTM and LE-ConvMN on the Pytorch framework <ref type="bibr" target="#b31">[32]</ref>, and built SPGP on MATLAB, comparing their performance on continuous proportional estimation of finger movements.</p><p>LSTM and LE-ConvMN were trained on the same GPUs (Titan V). Both models converged after training for around 300 epochs and the average convergence time of LSTM and LE-ConvMN is 33.25 hours and 3.5 hours, respectively. Fig. <ref type="figure" target="#fig_4">6</ref> shows the performance of the three contrast models for the eight experimental subjects and ten channels. We firstly performed Friedman test on the performance (RMSE, NRMSE, CC) of the three methods on the subjects and the channels, and the p-values were all less than 0.05, indicating that the performance of the three methods is significantly different. Afterwards, we performed Wilcoxon signed-rank test on (LE-ConvMN, SPGP) and (LE-ConvMN, LSTM). The p-values on subject-to-subject and joint-to-joint were both less than 0.05, indicating that LE-ConvMN significantly outperformed the other two methods. The average CC, RMSE, NRMSE of the proposed method (0.82?0.03, 11.54?1.89, 0.12?0.01) was significantly higher than SPGP (0.66?0.05, p=0.012; 15.51?2.81, p=0.025; 0.16?0.01, p=0.017 ) and LSTM (0.64?0.06, p=0.012; 14.77?3.21, p=0.025; 0.15?0.02, p=0.025). In our previous work <ref type="bibr" target="#b23">[24]</ref>, we proved that LSTM has high estimation accuracy in a single class of actions, which is consistent with the results of the current study. However, the accuracy of LSTM from our current results is greatly reduced because of the combination of multiple motions, while LE-ConvMN maintained high accuracy. This indicates that our proposed network has stronger generalization ability in intra-class continuous estimation of finger movements than LSTM. Almost all of the LE-ConvMN estimation accuracies among all subjects are greater than 0.8, which indicates that the proposed LE-ConvMN has the capability of generalizing for most subjects. It is evident from Fig. <ref type="figure" target="#fig_4">6</ref>(a) that the performance of LSTM and SPGP was particularly worse than LE-ConvMN on subjects 2, 3, 5. As for the RMSE and NRMSE, LE-ConvMN presents superior performance, especially for subject 3.</p><p>The estimation accuracy with each individual channel is shown in Fig. <ref type="figure" target="#fig_4">6</ref>(b)(d)(f). LE-ConvMN always has the best performance for all ten channels. In Fig. <ref type="figure" target="#fig_4">6</ref>(b), the LE-ConvMN surpasses LSTM and SPGP significantly in accuracies by 10% among all joint angles. Fig. <ref type="figure" target="#fig_4">6(d)</ref> shows that the RMSE of LE-ConvMN is lower than LSTM and SPGP on each joint angle. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>(f), NRMSE also shows superior performance when applying LE-ConvMN. LE-ConvMN also shows high accuracy and stability for channels distinctly more difficult to predict (e.g. channel <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17)</ref>. Especially on joint angle 9, LE-ConvMN provides high accuracies, small variance, low RMSE and low NRMSE while LSTM and SPGP perform distinctly worse for this joint angle than for the others. This suggests that LE-ConvMN has the capability, stability and versatility to be applied on various subjects and joint angles.</p><p>In order to verify the effect of the noise may appear in practice, we add gaussian noise with SNR 10dB to the test data. Experimental results show that the noise lower the accuracy of the LE-ConvMN by 0.3% which proved that the method is robust to this kind of noise.</p><p>The estimated joint angles with the three contrast models are plotted against the measured curves of participant 1 in Fig. <ref type="figure" target="#fig_5">7</ref>. As is shown in Fig. <ref type="figure" target="#fig_5">7</ref>(a), the SPGP model provides accurate trend but the amplitude is incorrect, which leads to the worst performance among the three models. On the other hand, Fig. <ref type="figure" target="#fig_5">7</ref> (b) shows that the predicted curve from the LSTM model estimates the correct trend but presents large error at turning points. Fig. <ref type="figure" target="#fig_5">7 (c</ref>) shows that the predicted curve by LE-ConvMN is the most similar to the measured curve without distortion. As for the processing delay, the average delay of LE-ConvMN is 82ms (input step 50ms + calculation time 32ms.) which is well below the need for real-time requirement in myocontrol (200 ms <ref type="bibr" target="#b32">[33]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The finger motion detection and prediction would greatly influence the performance of an HCI involving hand movements, such as gestures. In this study, the proposed approach of LE-ConvMN was compared with traditional mathematical (SPGP) and machine learning methods (LSTM), when finger movements are estimated from sEMG.</p><p>To accurately describe the estimation performance of each algorithm, correlation coefficients of each algorithm were measured between the actual and predicted finger joint angle. The results shown in Fig. <ref type="figure" target="#fig_5">7</ref> indicate that, among these 8 individual participants, the proposed method not only has significantly higher estimation accuracy but also provides better stability than SPGP and LSTM. Assuming applied in real-world application scenario, it would be difficult for SPGP and LSTM to identify the joint angles because of the heterogeneity of participants (LSTM performs better than SPGP on some participants only). In comparison, the proposed method has the ability to generalize and was superior to both LSTM and SPGP in all participants. As shown in Fig. <ref type="figure" target="#fig_5">7</ref>, LSTM with its unique recurrent structure performs better than SPGP on most joint angles, LE-ConvMN performs the best on each angle due to the powerful structure pattern recognition ability of convolutional kernel, the extraordinary sequence data decoding capability of LSTM, and the specific long exposure data input method.</p><p>The advantage of the proposed estimation approach is further reflected in Fig. <ref type="figure" target="#fig_5">7</ref>. The predicted joint angle curves of SPGP have a relatively long lag in response, especially during intervals of sudden inflections. The LSTM is able to detect the trend of the actual finger motion clearly but there are discrepancies in amplitude, especially when the joint angle is constant. Comparatively, the LE-ConvMN provides more continuous, stable and accurate joint angle curves to match the actual curves.</p><p>At present, the application of deep learning in the field of myoelectric control mostly relies on discrete estimation <ref type="bibr" target="#b7">[8]</ref>  [9] [34] <ref type="bibr" target="#b34">[35]</ref>. Although TCN can achieve good results in discrete estimation <ref type="bibr" target="#b7">[8]</ref>, it cannot be effectively applied to continuous estimates. Researchers are trying to find effective methods to estimate the continuous movement of the hand. For example, Xiloyannis et al. proposed Gaussian Process Autoregression (GPA) for decoding neural information that enables a user to independently control 11 joints of the hand <ref type="bibr" target="#b35">[36]</ref>. Ghaderi et al. applied Generalized Regression Neural Network (GRNN), a non-linear identification approach, to estimate finger kinematics (15 Degrees of Freedom) from sEMG signals <ref type="bibr" target="#b36">[37]</ref>. Comparing with the two methods, LE-ConvMN can estimate more complex actions and provides higher accuracy. Li et al. proposed a time-delay recurrent neural network (TDRNN) method by using sEMG signals to predict the kinematics of hand and wrist but only applied it for estimates of 3 degrees of freedom. Compared with our previous work <ref type="bibr" target="#b23">[24]</ref>, LE-ConvMN does not need to build different models for different actions and has a faster training speed.</p><p>However, there are some limits in this work, such as the finger motions in this study mainly focused on the basic functional grasp motions. We have not tested the performance of LE-ConvMN on predicting other types of finger motions (e.g., "OK", "three", "V" , and pinching). Therefore, the compatibility of the method with other types of finger motions remains to be tested. It has been mathematically proved that multilayer neural network can fit arbitrary functions <ref type="bibr" target="#b37">[38]</ref>, so it is possible to develop a deep learning network to meet the more complexed finger motions, we are considering to integrate more complex finger motions in the future work. Meanwhile, the deviation between the predicted and actual joint angle might be reduced by integrating adaptive type algorithms. Although off-line analysis is the starting point of exploration, there are still some issues (i.e., shift in electrode position, condition, and arm posture) to be solved when apply the method in practice. We have tried to solve the problem of electrode shifting or noise. In our former work, we have used high density electrodes array to simulate electrode shifting, and our preliminary work has been published <ref type="bibr" target="#b38">[39]</ref> <ref type="bibr" target="#b39">[40]</ref>. But in this paper, the Ninapro database uses sparse electrodes, the distance between two sparse electrodes is too far, which is not so easy to simulate the electrode shifting like the actual situation. We have simulated moving the electrode in a large range (equal to the interval width of two neighboring sparse electrodes). This will obviously reduce the accuracy of the LE-ConvMN (more than 10%). In the future work, we will collect the shifted electrode data by our own equipment, which may better evaluate the robustness of the algorithm when meeting electrode shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a LE-ConvMN-based deep learning model to estimate finger motion simultaneously and proportionally. According to the temporal and spatial characteristics of sEMG data and the real-time requirements of practical use, we designed a multi-channel long exposure input and convolution structure to reproduce the gate operation of LSTM. Then we chose two representative algorithms (SPGP and LSTM) for the continuous estimation field to compare them with LE-ConvMN. The experimental results showed that LE-ConvMN achieves higher accuracy, stability and smaller delay than LSTM, and is expected to play an important role in human-computer interaction and human-computer cooperation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. CyberGlove II data-glove. The red dot represents our estimate of 10 angles</figDesc><graphic url="image-2.png" coords="3,89.38,592.80,155.34,114.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. One channel sEMG signal, RMS and joint angle after 6 types of action combinations</figDesc><graphic url="image-3.png" coords="4,115.80,78.95,362.28,249.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The structure of a LSTM cell. where ? ? , ? ? , ? ? , ? ? stand for input gate, output gate, forget gate and the LSTM cell state. ?? stand for alternate cell state.</figDesc><graphic url="image-4.png" coords="5,52.90,420.60,221.60,240.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. ConvLstm cell uses convolution to achieve the calculation of each gate and the number of convolution channels is equal to the number of hidden states. The Structure of ConvMN Block consists of three ConvLstm layers with hidden state 64, 32, 10 and one fully connected layer. Inside the ConvMN Block, the hidden state is delivered layer by layer and the dimensions are continuously reduced, and finally the fully connected layer does feature mapping. The inputs of ConvMN block at time t consists of the LE-sEMG at time t and the hidden state of the third layers of the ConvMN block at time t-1 (10 * 12 * 200). Hidden state is computed by forget gate, input gate and output gate.</figDesc><graphic url="image-5.png" coords="6,52.90,77.80,485.15,172.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Summary of the performance measures of 8 able-bodied subjects. From top to down, are the CC, RMSE, NRMSE, and overshoots indicates the variances.</figDesc><graphic url="image-10.png" coords="8,99.25,480.28,198.95,170.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>(a) (b) (c) shows the predicted and actual values of SPGP, LSTM and LE-ConvMN respectively. 8 subjects, each subject execute 6 grasp movements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-12.png" coords="9,97.18,79.55,393.98,275.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-13.png" coords="9,96.13,411.51,396.10,277.13" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The work is supported by the <rs type="funder">Shenzhen Basic Research Grants</rs> (#<rs type="grantNumber">JCYJ20170413152804728</rs>, #<rs type="grantNumber">JCYJ20180507182508857</rs>), <rs type="funder">Pioneer Hundred Talents Program of Chinese Academy of Sciences</rs> (2017).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BJp6gNq">
					<idno type="grant-number">JCYJ20170413152804728</idno>
				</org>
				<org type="funding" xml:id="_WaTURsv">
					<idno type="grant-number">JCYJ20180507182508857</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author et al</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eine neue elecktrokunsthand</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Grenzgeb. Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">EMG signal classification for human computer interaction: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ibrahimy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Khalifa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Sci. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="501" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Control of Hand Prostheses Using Peripheral Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Micera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carpaneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raspopovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="48" to="68" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Upper Limb</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kapandji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Physiology of the Joints</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">544</biblScope>
			<biblScope unit="page" from="140" to="140" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>Postgrad. Med. J.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimation of finger joint angles from sEMG using a recurrent neural network with time-delayed input vectors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hioki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Rehabilitation Robotics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction of the performance of artificial neural networks in mapping sEMG to finger joint angles via signal pre-investigation techniques</title>
		<author>
			<persName><forename type="first">W</forename><surname>Batayneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abdulhay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alothman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heliyon</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e03669</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for time-series modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>L?ngkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust Real-Time Embedded EMG Recognition Framework Using Temporal Convolutional Networks on a Multicore IoT Processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zanghieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kartsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="256" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>C?t? -Allard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="760" to="771" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EMG feature evaluation for improving myoelectric pattern recognition robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phinyomark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Quaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Serviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tarpin-Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Laurillau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4832" to="4840" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of regression methods for the continuous decoding of finger movement from surface EMG and accelerometry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krasoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nazarpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. IEEE/EMBS Conf. Neural Eng. NER</title>
		<imprint>
			<biblScope unit="page" from="631" to="634" />
			<date type="published" when="2015-07">2015-July. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Myoelectric control of artificial limbsis there a need to change focus?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="148" to="152" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In the Spotlight</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting wrist kinematics from motor unit discharge timings for the control of active prostheses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kapelner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neuroeng. Rehabil</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A review on EMGbased motor intention prediction of continuous human upper limb motion for human-robot collaboration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feleke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The extraction of neural information from the surface EMG for the control of upper-limb prostheses: emerging avenues and challenges</title>
		<author>
			<persName><forename type="first">D</forename><surname>Farina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="809" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting simultaneous and proportional neural control information for multiple-dof prostheses from the surface electromyographic signal</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Englehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1070" to="1080" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kinematic models of the upper limb joints for multibody kinematics optimisation: An overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duprey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moissenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Begon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ch?ze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2017-12">December. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust simultaneous myoelectric control of multiple degrees of freedom in wrist-hand prostheses by real-time neuromusculoskeletal modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durandau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Do?en</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66026</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Myoelectric Control Based on a Generic Musculoskeletal Model: Toward a Multi-User Neural-Machine Interface</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1435" to="1442" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous and continuous estimation of shoulder and elbow kinematics from surface EMG signals</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">MAY</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian Process Autoregression for Simultaneous Proportional Multi-Modal</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xiloyannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gavriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A C</forename><surname>Thomik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1785" to="1801" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translating sEMG signals to continuous hand poses using recurrent neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Quivira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koike-Akino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE EMBS Int. Conf. Biomed. Heal. Informatics, BHI</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="166" to="169" />
			<date type="published" when="2018-03">2018. 2018. March. 2018</date>
		</imprint>
	</monogr>
	<note>Janua</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">sEMG-based continuous estimation of grasp movements by long-short term memory network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101774</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Electromyography data for robotic hand prostheses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atzori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous and simultaneous estimation of finger kinematics using inputs from an EMG-to-muscle activation model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ngeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neuroeng. Rehabil</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparative study of RNN for outlier detection in data mining</title>
		<author>
			<persName><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. -IEEE Int. Conf. Data Mining, ICDM</title>
		<meeting>-IEEE Int. Conf. Data Mining, ICDM</meeting>
		<imprint>
			<date type="published" when="2002">May 2014. 2002</date>
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>in Thirteenth annual conference of the international speech communication association</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EEG-based motion intention recognition via multi-task RNNs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Int. Conf. Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
	<note>SDM 2018</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EMG-Based Estimation of Limb Movement Using Deep Learning With Recurrent Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Organs</title>
		<imprint>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="issue">00</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<title level="m">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</title>
		<imprint>
			<date type="published" when="2015-06">June, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to PyTorch BT -Deep Learning with Python: A Hands-on Introduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ketkar</surname></persName>
		</author>
		<editor>N. Ketkar</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Apress</publisher>
			<biblScope unit="page" from="195" to="208" />
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Robust, Real-Time Control Scheme for Multifunction Myoelectric Control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Englehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hudgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="848" to="854" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Fully Embedded Adaptive Real-Time Hand Gesture Classifier Leveraging HD-sEMG and Deep Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boukadoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campeau-Lecours</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boukadoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campeau-Lecours</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Fully Embed. Adapt. Real-Time Hand Gesture Classif</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="243" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Biomed. Circuits Syst.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">sEMG-based hand-gesture classification using a generative flow model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors (Switzerland)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gaussian Process Autoregression for Simultaneous Proportional Multi-Modal Prosthetic Control with Natural Hand Kinematics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xiloyannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gavriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A C</forename><surname>Thomik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1785" to="1801" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hand kinematics estimation to control prosthetic devices: A nonlinear approach for simultaneous and proportional estimation of 15 DoFs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghaderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karimimehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Andani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Marateb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nd Iran. Conf. Biomed. Eng. ICBME</title>
		<imprint>
			<date type="published" when="2015">2015 22. 2015. 2016</date>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial Correlation of High Density EMG Signals Provides Features Robust to Electrode Number and Shift in Pattern Recognition for Myocontrol</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Negro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DFNN-based gesture recognition with the shift and damage of the HD-sEMG electrodes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Robot. Biomimetics</title>
		<imprint>
			<biblScope unit="page" from="1275" to="1279" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
