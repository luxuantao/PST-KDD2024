<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rear-View Vehicle Detection and Tracking by Combining Multiple Parts for Complex Urban Surveillance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Tian</surname></persName>
							<email>bin.tian@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
							<email>ye.li@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
							<email>bo.li@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Laboratory of Management and Control for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rear-View Vehicle Detection and Tracking by Combining Multiple Parts for Complex Urban Surveillance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D9CD7F6FD959BC4638545D090DE0E13</idno>
					<idno type="DOI">10.1109/TITS.2013.2283302</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic surveillance is an important topic in intelligent transportation systems. Robust vehicle detection and tracking is one challenging problem for complex urban traffic surveillance. This paper proposes a rear-view vehicle detection and tracking method based on multiple vehicle salient parts using a stationary camera. We show that spatial modeling of these vehicle parts is crucial for overall performance. First, the vehicle is treated as an object composed of multiple salient parts, including the license plate and rear lamps. These parts are localized using their distinctive color, texture, and region feature. Furthermore, the detected parts are treated as graph nodes to construct a probabilistic graph using a Markov random field model. After that, the marginal posterior of each part is inferred using loopy belief propagation to get final vehicle detection. Finally, the vehicles' trajectories are estimated using a Kalman filter, and a tracking-based detection technique is realized. Experiments in practical urban scenarios are carried out under various weather conditions. It can be shown that our method adapts to partial occlusion and various lighting conditions. Experiments also show that our method can achieve real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head><p>Terms-Kalman filter (KF), Markov random field (MRF), part-based object detection, tracking, vehicle detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH rapid development of urbanization, traffic conges- tion, incident, and violation pose great challenges for traffic management systems. Vision, as an information collection access of real-world environments, has attracted much attention in intelligent transportation systems (ITSs). Computer vision techniques are mainly used to collect traffic parameters and analyze traffic behaviors for traffic surveillance. In [1], an overview of the background, concepts, basic methods, major issues, and current applications of parallel transportation man-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>agement systems (PTMSs) was proposed. Video processing results can provide various information derived from the actual traffic world for PTMSs.</p><p>Reliable and robust vehicle detection is a fundamental component for traffic surveillance. There are still some issues for vehicle detection in ITSs. Various vehicle appearances and poses make it difficult to train a unified detection model. Complex urban environments, bad weather, illumination changes, and poor/strong lighting conditions will degrade the detection performance dramatically. In particular, for traffic congestion, vehicles are occluded by each other so that separate vehicles will easily merge into a single vehicle. The parameter learning for the vehicle detection is also a critical issue. A detection method with complex parameters is usually not practical. The advances of machine learning techniques can be used to learn the parameters. In <ref type="bibr" target="#b1">[2]</ref>, a weakly supervised approach for object detection was proposed. This method does not need manual collection and labeling of training samples. A boosting algorithm was extended to train samples with probabilistic labels.</p><p>In this paper, we aim to develop a vehicle detection and tracking system particularly for urban traffic surveillance under various environments in China. The system utilizes a rear-view stationary camera to capture the image sequence. In practical traffic scenarios, occlusion between vehicles often occurs; therefore, it is unreasonable to treat the vehicle as a whole. Much research has detected the object by detecting its parts first and measuring their spatial relationships; this is called a partbased model. In our system, we treat the vehicle as an object composed of multiple parts. Unlike other vehicle detection methods, we choose salient vehicle parts, including the license plate and rear lamps, which usually exist on each vehicle. Then, we combine these parts into a vehicle by using a Markov random field (MRF) to model their spatial relationships. We further track the detected vehicles by employing a Kalman filter (KF) to obtain vehicle trajectories. A detection-by-tracking strategy is realized to improve vehicle detection performance. The main contributions of this paper lie in the following. First, novel methods are proposed to localize vehicle parts. We localize the Chinese license plate and the rear lamp using their distinctive color, texture, and region features. Second, we propose a partbased vehicle detection model using the MRF. Detected vehicle parts are combined into a vehicle using an MRF model in which parts are treated as graph nodes. Our method can adapt to partial occlusion and various lighting conditions. The details of the system will be discussed in later sections.</p><p>The remainder of this paper is organized as follows. In Section II, we provide an overview of the latest research related to vehicle detection and tracking. Section III presents how to localize vehicle parts and combine them into a vehicle. Section IV describes the vehicle tracking process and a detection-by-tracking strategy. Section V provides extensive and detailed experiments to analyze the system performance. Section VI concludes this paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In good and stable lighting conditions, moving object detection methods are widely used for vehicle detection in ITSs. These methods can be classified into background modeling, frame differencing, and optical flow <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. They can handle illumination change and apply to multimode and slight background change. However, there are some drawbacks in that they are unable to detect a stationary vehicle and the detected moving object is not necessarily a vehicle. Therefore, much research utilizes the visual features of the vehicle to detect it in a still image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Features such as Gabor, color, edge, and corner are usually used to represent the vehicle. Then, they are fed into a deterministic classifier and a generative model to identify vehicles. In addition, researchers usually employ a twostep method, including hypothesis generation and hypothesis verification <ref type="bibr" target="#b6">[7]</ref>, to locate the vehicle. This method works well during the sunny daytime but may fail during poor lighting conditions such as nighttime.</p><p>Many recent studies on part-based models have been developed to recognize vehicles. According to a human cognitive study <ref type="bibr" target="#b7">[8]</ref>, a vehicle is considered to be composed of a window, a roof, wheels, and other parts. These parts are usually learned and detected by using their appearance, edge, and shape features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. After part detection, the spatial relationship, motion cues, and multiple models are usually used to detect vehicles. Winn et al. <ref type="bibr" target="#b10">[11]</ref> decomposed an object into several local regions to detect them. The relationships between them were used to improve detection performance by a layout conditional random field model. Hoiem et al. <ref type="bibr" target="#b11">[12]</ref> further expanded this method by using 3-D models to mark the learning samples. In addition, vehicle parts can be selected and learned automatically in a deformable part-based model <ref type="bibr" target="#b12">[13]</ref>. Niknejad et al. <ref type="bibr" target="#b13">[14]</ref> employed this model composed of five components, including the front, back, side, front truncated, and back truncated. Each component contained a root filter and six part filters, which were learned using a latent support vector machine and a histogram of oriented gradients features. To use the part-based model in parameter transfer learning, Xu and Sun <ref type="bibr" target="#b14">[15]</ref> proposed a basic learning framework named part-based transfer learning (PBTL). All the complex tasks are regarded as a collection of constituent parts, and each task can be divided into several parts, respectively. Transfer learning between two complex tasks can be accomplished by subtransfer learning tasks between their parts. To avoid negative transfer and improve the effectiveness of transfer learning, Sun et al. <ref type="bibr" target="#b15">[16]</ref> extended PBTL into an effective learning framework named multisource PBTL. By this, it is possible to focus on the parts that contribute more to the target task. The inference method is critical for a probabilistic graph model. In <ref type="bibr" target="#b16">[17]</ref>, Sun gave a comprehensive introduction of loopy belief propagation (LBP). The pros and cons of LBP were listed, and the improved methods were analyzed in detail with a sufficient survey of the literatures.</p><p>Vehicle detection at nighttime is a dramatic challenge for traffic surveillance. Generally, vehicle headlights and taillights are used to represent the vehicle <ref type="bibr" target="#b17">[18]</ref>. Robert <ref type="bibr" target="#b17">[18]</ref> detected bright blobs as candidate headlights. To filter out false detection, they assumed that two headlights were aligned horizontally. Inspired by <ref type="bibr" target="#b17">[18]</ref>, Wang et al. <ref type="bibr" target="#b18">[19]</ref> proposed a two-layer nighttime detection method. In the first layer, the headlight detection process was the same as in <ref type="bibr" target="#b17">[18]</ref>. In the second layer, Haar features based on the AdaBoost cascade method were employed to recognize vehicle frontal views. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> modeled the reflection intensity map, the reflection suppressed map, and image intensity into an MRF model to distinguish light pixels from reflection pixels. For taillight detection, its color information is mostly utilized. O'Malley et al. <ref type="bibr" target="#b20">[21]</ref> proposed a system to detect and track vehicle taillight pairs. Taillight candidates were extracted with a hue-saturation-value (HSV) color threshold and paired by their symmetry features. To detect nighttime brake lights, Chen et al. <ref type="bibr" target="#b21">[22]</ref> modeled taillights using the Nakagami distribution.</p><p>Vehicle tracking is used to obtain trajectories of moving vehicles, enabling higher level tasks such as traffic incident detection and behavior understanding. A detailed survey about object tracking is shown in <ref type="bibr" target="#b22">[23]</ref>. Vehicle tracking can be classified into four categories: model-based tracking, region-based tracking, deformable template-based tracking, and featurebased tracking. Various filtering algorithms are used in tracking tasks, such as the Bayesian filter, the KF, and the particle filter <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Object tracking based on the mean-shift algorithm is an appearance-based tracking method and performs well in tracking moving objects even in dense traffic <ref type="bibr" target="#b23">[24]</ref>. However, this method needs manual initialization of a target model. In <ref type="bibr" target="#b24">[25]</ref>, kinematic variables including position, speed, and size of the vehicle were estimated with a projective KF to initialize the mean-shift tracker.</p><p>Different from the aforementioned methods, we treat the vehicle as an object composed of multiple salient parts, including the license plate and rear lamps. With the advance of the MRF model, the visual features and structure of the parts are modeled into probability distributions. Hence, the method proposed in this paper can adapt to partial occlusions and various lighting conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PART-BASED VEHICLE DETECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline of Vehicle Detection</head><p>For practical urban traffic surveillance, reliable and robust vehicle detection is a fundamental component. Occlusions between vehicles occurs frequently so that it is unreasonable to treat the vehicle as a whole. In this paper, we treat the vehicle as an object composed of multiple salient parts, including the license plate and rear lamps, which usually exist on each vehicle. These parts are localized using their distinctive color, texture, and region features. Then, an MRF model is constructed to model the spatial relationships among these parts to infer and localize the vehicle. Even if some parts are occluded, vehicles can be correctly detected. Meanwhile, our method can cope with several weather and lighting conditions. An illustration of the vehicle detection pipeline is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. License Plate Localization</head><p>License plate localization is a critical technology in ITS applications for traffic management and analysis. In this section, we localize the Chinese license plates with a coarseto-fine strategy. It has its unique texture features that are distinguished from the other vehicle parts. The Chinese license plates include a blue background/white character plate, a yellow background/black character plate, a white background/blackand-red character plate, and a black background and white character plate.</p><p>Our license plate localization process is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Here, we take a white-blue pair as an example. This type of plate is the most common in China. To get the width of the plate for each image coordinate, we utilized the calibration toolbox in the OpenCV library for traffic scene calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Deriving Plate Color Converting Model:</head><p>We mainly utilized the color difference between plate characters and plate background. The vast majority of plate characters are white or black, whose values are not certain in HSV space. Consequently, we chose the red-green-blue (RGB) color space. A database of 50 license plate images were created for the blue-white pair. The color distributions of plate pixels are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. As observed, the plate background and plate characters have some unique characteristics. For the plate background pixel, its value of the blue channel is far greater than those of the other two channels. Moreover, the values of the green and red channels are both relatively small. For the plate character pixel, the values of all three channels are relatively  large. The characteristics were analyzed to find the appropriate color conversion for the plate.</p><p>According to these observations, we converted the image into a specific color space as in</p><formula xml:id="formula_0">C x, y = B x, y -min{R x, y , G x, y } (1)</formula><p>where C x, y is the converted color of pixel (x, y), and R x, y , G x, y , B x, y are the red, green, and blue channel values. The converted color image is shown in Fig. <ref type="figure" target="#fig_2">3</ref>(b), which is also called the plate color image. The conversion can enhance the difference between the plate background and characters. Furthermore, it suppresses the values of the nonplate pixels.</p><p>Comparing with a grayscale image, the gradient of the plate region is significantly enhanced, whereas the gradient of the other regions is suppressed. For the yellow-black pair, a similar process was taken to obtain the corresponding plate color image. For the white-black pair and black-white pair, we did not perform color conversion and use the grayscale image immediately.</p><p>2) Plate Hypothesis Score Calculation: After color conversion, the image gradient was calculated for the plate color image. We calculated the image gradient by computing the difference between the maximum and minimum of the neighboring pixels, as calculated in</p><formula xml:id="formula_1">Grad x, y = max{Nb x, y } -min{Nb x, y } (2)</formula><p>where Grad x, y is the gradient of the pixel (x y) and Nb x, y is the neighboring pixels. Then, we scanned the gradient image with a sliding window to compute the average gradient. Using the previous calibration result, we could get the plate size and scan with a plate-size window, as in</p><formula xml:id="formula_2">S x, y = x+pw/2 x =x-pw/2 y+ph/2 y =y-ph/2 Grad x , y /(pw * ph) (3)</formula><p>where S x, y is the plate score at pixel (x, y). pw and ph are the plate width and height, respectively. Considering the processing speed, we scanned the gradient sparsely. As a result, we obtained a score image, as shown in Fig. <ref type="figure" target="#fig_4">3(c</ref>).</p><p>Finally, we used a nonmaxima suppression (NMS) <ref type="bibr" target="#b25">[26]</ref> strategy to seek local maximums in the score image. If a local maximum is greater than a predefined threshold α, it is recognized as a candidate plate region.</p><p>3) Cascade Plate Refining: Except for the plate regions, there may be some plate-like regions on the vehicle. They have strong gradient and meet the color feature, which may lead to false detection. As we observed, the gradient of a plate region is not only strong but also evenly distributed. Consequently, we further used the consistency of textures on the gradient image to improve algorithm efficiency. We chose texture consistency for texture measurement as computed in</p><formula xml:id="formula_3">U = L-1 i=0 p 2 (z i ) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where U is the consistency of an image region. L indicates the possible gray levels. p(z) is the histogram of the gray level in an image region. The consistency of the real plate region is relatively consistent (0.7 in this paper). We scanned the candidate plate regions to obtain refined localization results, as shown in Fig. <ref type="figure" target="#fig_4">3(d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vehicle Rear-Lamp Localization</head><p>A vehicle rear lamp is an obvious feature of the vehicle. According to the Chinese national standard, the color of the vehicle rear lamp is red and falls within a specified range. We adopted the multithreshold segmentation and connected component analysis to extract as many candidate rear lamps as possible. In this section, we focus on candidate rear-lamp localization without pairing them. The localization process is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>1) Deriving the Color Converting Model: Similar to license plate localization, we operated rear-lamp localization under the RGB color space. As observed, a rear-lamp pixel satisfies some unique properties. The value of the red channel is large. The values of the green and blue channels are small. The differences between the values of the green and blue channels are small. The characteristics were analyzed to find the appropriate color conversion for the rear lamp. The conversion can suppress the values of the nonrear-lamp pixels. The RGB image was converted into a color grayscale image, which is called the rear-lamp color image, as calculated in</p><formula xml:id="formula_5">C x, y = R x, y -max{G x, y , B x, y }-2 * G x, y -B x, y<label>(5)</label></formula><p>where C x, y is the converted color of pixel (x, y), whereas R x, y , G x, y , B x, y are the red, green, and blue channel values. The converted rear-lamp color image is shown in Fig. <ref type="figure" target="#fig_3">4</ref>(b).</p><p>2) Rear-Lamp Candidate Generation by Multithreshold Segmentation: Motivated by the maximally stable extremal region detector <ref type="bibr" target="#b26">[27]</ref>, we adopted a multithreshold segmentation method to segment the rear-lamp color image. To adapt to relatively dark and bright lamps, we used three thresholds to execute binarization. Three binary images were obtained, as shown in Fig. <ref type="figure" target="#fig_3">4(c)-(e)</ref>.</p><p>Finally, a connected component analysis was used to extract candidate rear lamps on the binary images as follows.</p><p>Step 1) Get the connected regions by multithreshold segmentation:</p><formula xml:id="formula_6">Q 1 , Q 2 , Q 3 (Q i ⊂ Q i+1 ).</formula><p>Step 2) Select the rear-lamp regions with appropriate areas in the threshold ascending order, i.e., Area </p><formula xml:id="formula_7">(Q i ) ∈ [γ 1 , γ 2 ].<label>Step</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Vehicle Detection Using MRF</head><p>Earlier, we localized the vehicle parts, including the license plate and rear lamps. Then, it became a challenge how to efficiently combine these parts into a vehicle. In this section, to make better use of the relationships among vehicle parts, we adopted an MRF model to localize the vehicle.</p><p>1) Probability Model Representation: At first, we constructed an MRF graph model and defined its model parameters. Vehicle parts were treated as the graph nodes, and the relationships among them were the graph edges. First, we selected one detected license plate as a graph node in the current frame. Then, neighboring vehicle rear lamps were added into the graph if they were close enough to the license plate. In Section IV-C, we will mention that tracking results are used to improve the detection results by adding predictions of vehicle location into the graph. </p><formula xml:id="formula_8">F i . Order f = {f 1 , f 2 , . . . , f n } is a configuration of F . f i belongs to Q = {1, 2, 3, 0}.</formula><p>In our MRF graph, there are four types of nodes:</p><p>• q = 1, license plate node;</p><p>• q = 2, left rear-lamp node;</p><p>• q = 3, right rear-lamp node;</p><p>• q = 0, false detection node.</p><p>Our MRF model is a pairwise model with the distribution in</p><formula xml:id="formula_9">P (F ) = 1 Z c∈C ψ c (f c ) = 1 Z i∈V ϕ(f i ) node potential (i,j)∈E φ(f i , f j ) edge potential<label>(6)</label></formula><formula xml:id="formula_10">Z = F i∈V ϕ(f i ) (i, j)∈E φ(f i , f j ) (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where Z is a partition function. ϕ(f i ) is the node potential, representing detection confidence of each node. φ(f i , f j ) is the edge potential, representing the relationship between each pair of nodes. The model is shown in Fig. <ref type="figure" target="#fig_5">5(b</ref>). Then, we will determine the node potentials and edge potentials. a) Node potentials: The node potential ϕ(f i = p) indicates the probability of node i to be part p without considering other nodes. It depends on the scores of part detectors S i . To integrate the discriminative detectors into the MRF model, it is necessary to give a probabilistic meaning to the outputs. We introduce the sigmoid function to normalize the scores of the vehicle parts. The final node potential is</p><formula xml:id="formula_12">ϕ(f i = p) = ⎧ ⎨ ⎩ 1 1+exp{A p * S i +B p } , Node i is detected as p 1- 1 1+exp{A p * S i +B p } , p= 0 λ, otherwise.<label>(8)</label></formula><p>The probability of the node to be false detection is obtained by computing the complementary probability of the detection. If node i is not detected as q, its probability is set to be a small constant λ (10 -2 in this paper). The parameters A p and B p for each part were learned by the sigmoid fitting method introduced in <ref type="bibr" target="#b27">[28]</ref>. For the rear lamps, there are no discriminative features, except their color. Therefore, its detection score was set to be a constant 0.8 by careful selection and cross validation. The corresponding sigmoid parameters were set to be A p = -1 and B p = 0.</p><p>b) Edge potentials: The edge potential φ(f i = p, f j = q) evaluates the compatibility between the neighboring nodes. It represents the relationship between the node pairs connected by an edge. A vehicle model is defined to model the edge potentials, as shown in Fig. <ref type="figure" target="#fig_6">6</ref>. In the model, there are two main spatial relationships between vehicle parts, i.e., the relationship between the plate and a rear lamp, and the other one between both rear lamps as follows:</p><p>1) The distance between vehicle parts, i.e., d(LP i , LR j ) and d(LR j , RR k ); 2) The angle between vehicle parts, i.e., tan -1 (d x (LP i , LR j )/ d y (LP i , LR j )) and tan -1 (d x (LR j , RR k )/d y (LR j , RR k )). Since the poses of vehicles on the road vary greatly, we chose the Gaussian to model the spatial relationship between parts. In addition, since a variety of types of vehicles exists, the relationships between nodes might be multimodal. Based on the two considerations, the Gaussian mixture model (GMM) was selected to model the part relationships. The edge potential is</p><formula xml:id="formula_13">φ(f i = p, f j = q) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ M k=1 α k p k (x|θ k ),</formula><p>(p = 0 and q = 0)</p><formula xml:id="formula_14">and p = q 1 2 1 - M k=1 α k p k (x|θ k ) , (p = 0 or q = 0)</formula><p>and p = q λ, p = q.</p><p>(9)</p><p>The relationships between the part nodes are represented by the GMM. Because the left and right rear lamps are horizontally symmetrical on both sides of the license plate, we learned the relationship model of the plate and the left rear lamp and obtained that of the plate and the right rear lamp by symmetry. The relationship between the false detection node and the part node (p = 0 or q = 0) is difficult to be represented by a probability distribution; hence, they are modeled using the complementary probabilities of part relationships. In this paper, the probability of the pairwise part relationship is M k=1 α k p k (x|θ k ), and the complementary probability of the node pair with a false detection node is supposed to be (1/2)(1 -M k=1 α k p k (x|θ k )). For the case where two nodes share the same labels (p = q), the probability takes a small constant λ (10 -2 in this paper).</p><p>For the parameter learning process, we used the expectationmaximization (EM) algorithm to estimate the parameters of the GMM using the maximum-likelihood estimation. The weights α k , mean, and covariance θ k for each Gaussian component were the parameters for the GMM. The vehicle parts were carefully labeled artificially. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, the relationship between the plate and the rear lamp is modeled to be a GMM with two components. The relationship between both rear lamps is modeled to be one Gaussian component.</p><p>2) Inference: Node potentials ϕ(f i ) of variables and edge potentials φ(f i , f j ) between the graph nodes determine the probabilistic assumptions of the MRF model. Here, we perform probabilistic inference to estimate each f i ∈ F , to satisfy the maximum a posteriori (MAP) in</p><formula xml:id="formula_15">Q * = arg max P (F ) = arg max i∈V ϕ(f i ) (i, j)∈E φ(f i , f j ). (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Pearl et al. <ref type="bibr" target="#b28">[29]</ref> proposed belief propagation inference, which is a probabilistic inference method. For the acyclic graph, it can provide the exact solution. For a loopy graph, it can provide an approximate solution, which is called LBP. Here, we initialize all messages m ij (f j ) to 1. After certain updating steps, all of the messages tend to converge. Then, the marginal probability of each variable, which is called beliefs, are read out. Beliefs are normalized so that they approximate the marginal probability. After vehicle inference, optimal labels are assigned for all of the MRF nodes, as shown in Fig. <ref type="figure" target="#fig_5">5(c</ref>). The vehicle detection results are shown in Fig. <ref type="figure" target="#fig_5">5(d)</ref>. During the vehicle detection process, if more than two parts are correctly inferred, we recognize it as a vehicle. In case some parts are not detected or are wrongly detected, our method can still localize them, as in Fig. <ref type="figure" target="#fig_9">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VEHICLE TRACKING</head><p>Vehicle detection has been described in the earlier sections. In this section, detected vehicles are tracked to obtain vehicle trajectories. A simple detection-by-tracking strategy was used to improve vehicle detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Outline of the Tracking Process</head><p>Vehicle tracking is used to predict vehicle positions in subsequent frames, match vehicles between adjacent frames, and ultimately obtain the trajectory of the vehicle. In our system, we used the KF to track vehicles. In statistics, the KF <ref type="bibr" target="#b29">[30]</ref> is a minimum variance estimation of linear movement. Each KF corresponds to a tracked object, which is called a tracker. It estimates the true values of observations with noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KF</head><p>In our system, we treated the center of the vehicle license plate and the speed of the vehicle as the state vector, which is shown as</p><formula xml:id="formula_17">x = [p x , p y , s x , s y ] t</formula><p>where p x and p y are x and y coordinates of the vehicle, respectively. s x and s y are the speeds of the x-axis and y-axis directions. KF is a recursive estimation method and can be split into two steps, i.e., prediction and update.</p><p>1) Prediction: In the prediction step, we predicted state vector x and state error covariance matrix P at the current time k, as in where x k-1 is the state of previous time k -1; F is the state transition matrix; x k is the state of current time; P k-1 and P k are error covariance matrices of the previous time k -1 and current time k, respectively; and Q is the process noise covariance matrix.</p><formula xml:id="formula_18">xk = F x k-1 Pk = F P k-1 F t + Q (11)</formula><p>2) Update: We selected the closest vehicle around the prediction location. If the distance between them was below the threshold, it was considered the observation y k . If an observation was unavailable, the update step would be skipped, and multiple prediction steps were performed. To perform the update, the Kalman gain K k is computed in</p><formula xml:id="formula_19">K k = P k H t (H P k H t + R) -1 (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where H is the measurement matrix and R is the measurement noise covariance matrix. Then, we updated the state vector and the error covariance matrix</p><formula xml:id="formula_21">x k = xk + K k (y k -H xk ) P k = (I -K k H) Pk . (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>After vehicle tracking, the vehicles corresponded between adjacent frames and vehicle trajectories were estimated, as shown in Fig. <ref type="figure" target="#fig_11">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Improved Detection by Tracking</head><p>The camera view is for rear-facing vehicles. When the vehicle enters the camera field of view, the vehicle parts can be seen without any occlusion. With the vehicle moving forward, the camera perspective is lower, which may cause severe occlusion of the license plate. Coupled with lower resolution in the distance, the plate is easily lost. However, the rear lamps are usually localized. We wish to detect the vehicle robustly in this case.</p><p>We added the prediction of the tracker into the MRF graph as a license plate node. In case that the plate was not localized, as shown in Fig. <ref type="figure" target="#fig_8">8</ref>(a), the MRF model was constructed using the predictions and rear lamps. The vehicle parts are then correctly inferred, as in Fig. <ref type="figure" target="#fig_8">8(b</ref>). Finally, according to the node marginal probabilities, we used the NMS method to eliminate repeated inferences. In addition, in poor lighting conditions, the license plate is difficult to see, particularly due to the limited scope of the supplemental lighting equipment at night. On the other hand, rear lamps are a more prominent vehicle feature. This technique performs more effectively at night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>To test and evaluate our method, we applied it to the urban traffic road for vehicle detection and tracking. We selected a busy traffic intersection in China as the actual testing environment. The intersection included four lanes and one crosswalk. We collected image sequences using a high-resolution chargecoupled device camera.</p><p>We selected six scenarios including sunny daytime, rainy daytime, cloudy, dusk, nighttime, and rainy nighttime, as shown in Fig. <ref type="figure" target="#fig_9">9</ref>. Each data set is a continuous video recording. The frame rate is 8.3 frames/s, except for the nighttime data set, which is 11.1 frames/s. For each data set, there were sedan vehicles, sports utility vehicles (SUVs), and buses, and we labeled them with ground-truth rectangles manually. In each environment, vehicles were occluded at different levels, and their pose transformations ranged from about -45 • to 45 • . Traffic congestion was divided into three levels: heavy traffic corresponding to frequent vehicle occlusion; moderate traffic, which means usual vehicle adhesion; and slight traffic with no vehicle occlusion. In particular, in the night condition, stroboscopic supplemental lighting equipment was used. Detailed information for all data sets is listed in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance</head><p>Recall and precision of our method were evaluated for both the vehicle and its parts detection performance, as shown in Table <ref type="table" target="#tab_1">II</ref>. The performance of the part detection is highly related to that of the vehicle detection. We tried to ensure high recall rates of the vehicle parts since they determine the upper limits of the vehicle detection rate. As in Table <ref type="table" target="#tab_1">II</ref>, the average recall rate of the license plate is higher than 98%, and the recall  rate of the rear lamp is higher than 89%. The final detection performance of the vehicle is highly superior to those of the parts. Our method achieved an average vehicle recall rate of 95% with an average precision rate of 92% for all the scenarios. Fig. <ref type="figure" target="#fig_9">9</ref> shows some detection results of the vehicle and its parts in various scenarios. For each scenario, the left side is the part detection result, and the right side is the vehicle detection result. The detected plates are indicated by yellow rectangles, and rear lamps are indicated by red rectangles. Most of the vehicle parts were detected. A few false positives of the rear lamps were caused by red cars in Fig. <ref type="figure" target="#fig_9">9(c</ref>) vehicle reflection on a rainy day and nighttime in Fig. <ref type="figure" target="#fig_9">9(b)</ref>, (e), and (f). However, they did not affect the final vehicle detection because we also utilized the license plate part in the MRF model. The detected vehicles are indicated by green rectangles. As shown in Fig. <ref type="figure" target="#fig_9">9</ref>(a)-(f), the proposed method could detect vehicles in various weather and lighting conditions. Even in some challenging situations, the method can robustly detect vehicles, as shown in Fig. <ref type="figure" target="#fig_10">10</ref>. The main challenges included occlusion, background clutter, different vehicle types, different vehicle poses, and poor lighting. Since our vehicle detection method is based on detection of multiple salient parts and the probability model, it can deal with the challenging cases.</p><p>The receiver operating characteristic (ROC) curve of vehicle detection is drawn by adjusting the scoring thresholds in the license plate localization. We tested all data sets in different scenarios to get the summary ROC curve. As shown in Fig. <ref type="figure" target="#fig_0">11</ref>, the true positive (TP) rate of vehicle detection increased with the false positive (FP) rate at the beginning. When the FP rate of the plate increased, vehicle detection was degraded because of using incorrectly detected plates. Therefore, the TP rate decreased when the FP rate exceeded a certain limit. With the ROC curve, we can choose a relatively good scoring threshold for all scenarios.</p><p>We evaluated the tracking performance using the indicator of mostly tracked trajectories (MT). If more than 80% of a ground-truth trajectory is correctly tracked, it is called an MT. The tracking results are listed in Table <ref type="table" target="#tab_1">II</ref>. It is shown that our tracking method achieved an average tracking rate of 90%. Tracking results are better during nighttime in slight traffic congestion than those during sunny daytime for heavy traffic. Fig. <ref type="figure" target="#fig_11">12</ref> shows the tracking results of an intersection during sunny daytime. The proposed method could robustly detect preceding vehicles that change its direction. It can robustly track the vehicle even with pedestrian occlusion, as shown by   the vehicle labeled as number 16. As discussed in Section IV, we introduced the vehicle predictions of the previous frame for vehicle detection. Therefore, the distant vehicles can be correctly tracked even if their plates are not detected.</p><p>The learning of critical parameters for the vehicle detection algorithm is an important issue in practice. Here, we list the critical parameters of the algorithm and their learning methods, as shown in Table <ref type="table" target="#tab_2">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Cost</head><p>Our algorithm was mainly implemented with C++ and OpenCV libraries. The platform was configured with a 2.5-GHz Intel Core i5 3210M processor and a 4-GB memory device. To evaluate the running time of our algorithm, we divided it into the following steps: 1) license plate localization; 2) rearlamp localization; 3) vehicle inference; and 4) vehicle tracking. Detailed computation time of each step is listed in Table <ref type="table" target="#tab_3">IV</ref>. Our algorithm only processed the region of interest (ROI), which was about two million pixels. The running time per frame was no more than 100 ms, and this satisfied real-time performance on this processing platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detection Error Analysis</head><p>Some of the failure results of the proposed method are shown in Fig. <ref type="figure" target="#fig_12">13</ref>. False negatives tend to occur in the case of vehicle parts missing detection. For example, the license plate of the vehicle is blurred or there is no plate, as in Fig. <ref type="figure" target="#fig_12">13(a)</ref>. Rear lamps may not be detected if they are repainted with other colors, as in Fig. <ref type="figure" target="#fig_12">13(b</ref>). These false negatives can be reduced by only utilizing detected parts. However, this causes a tradeoff by increasing false positives. An example of a false positive is shown in Fig. <ref type="figure" target="#fig_12">13(c</ref>). There are some plate-like and lamplike regions on the vehicle. False positives tend to occur on long vehicles and repainted vehicles. They can be eliminated by introducing more vehicle parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed a rear-view vehicle detection and tracking method based on a high-resolution camera. First, we localized the salient parts of the vehicle, including the license plate and rear lamps. Then, we constructed an MRF model by treating vehicle parts as graph nodes. LBP was used to infer the MRF graph to obtain the vehicle locations. After vehicle detection, we implemented vehicle tracking using KF. We realized a detection-by-tracking technique in which the prediction locations of KF were added into the MRF model as graph nodes. We carried out experiments in practical urban scenarios. Our method could adapt to partial occlusion and various weather conditions. The experiments showed that the proposed method could achieve real-time performance. This method could provide good results even in complex scenarios such as in a busy traffic intersection.</p><p>In the future, we will localize more salient parts, such as the windshield and front cover, aiming to deal with more severe occlusion and vehicle posture variation. The inference process will also be improved for better inference performance according to the comprehensive description in <ref type="bibr" target="#b16">[17]</ref>. In addition, the vehicle detection and tracking process can be integrated into an embedded camera platform as a low-cost implementation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart depicting the vehicle detection pipeline.</figDesc><graphic coords="3,51.23,70.01,222.86,226.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. RGB scatter plot of pixels from a database of license plate images for a blue background/white character plate.</figDesc><graphic coords="3,313.79,254.45,224.06,143.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of the license plate location process. (a) Input image. (b) Converted license plate color image. (c) Score image obtained by gradient statistics. (d) License plate localization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of the rear-lamp localization process. (a) Input image. (b) Converted rear-lamp color image. (c)-(e) Binary images obtained by three thresholds (σ 1 = 20, σ 2 = 40, σ 3 = 80). (f) Rear-lamp localization results.</figDesc><graphic coords="4,307.91,70.13,246.14,127.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 )</head><label>3</label><figDesc>Clear the remaining regions Q k (k &lt; i) and analyze the next connected region to Step 1. An NMS strategy was used to remove overlaid rear lamps. The final localization results are shown in Fig. 4(f).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. MRF graph construction and inference process. (a) Treat vehicle parts with green rectangles as inputs. (b) Construct MRF graphs. Red points and lines denote nodes and links in MRF, respectively. (c) Infer these on the graphs. (d) Vehicle detection results. Detected vehicles are labeled with green rectangles.</figDesc><graphic coords="5,42.23,70.01,504.14,77.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Spatial relationship among vehicle parts.</figDesc><graphic coords="5,302.75,193.97,246.14,60.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Part relationships modeled by GMMs. The left column provides the statistical histograms of the part relationships including distance and angle. The right column shows the learned models of the part relationships. (a) Relationship between the plate and the rear lamp. (b) Relationship between the both rear-lamps.</figDesc><graphic coords="6,44.99,70.49,246.02,213.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example of a detection result improved by tracking. (a) License plate with the dashed rectangle is lost localized. The rear lamps are localized correctly with red rectangles. (b) Vehicle parts are correctly inferred using the prediction of the tracker as a graph node.</figDesc><graphic coords="6,307.91,70.13,246.14,116.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Vehicle detection results for different scenarios. For each scenario, the left side is the vehicle part detection result, and the right side is the vehicle detection result. The license plates are marked with yellow rectangles. The rear lamps are labeled with red rectangles. Green boxes are bounding boxes of the detected vehicles. (a) Sunny daytime. (b) Rainy daytime. (c) Cloudy. (d) Dusk. (e) Nighttime. (f) Rainy nighttime.</figDesc><graphic coords="7,42.23,69.89,504.14,151.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Vehicle detection results in some challenging cases. Green boxes are bounding boxes of the detected vehicles. (a) Occlusion by a pedestrian. (b) Occlusion by a bus. (c) Cluttered background with several pedestrians and bicyclists. (d) Different vehicle types including sedan vehicles, vans, and SUVs. (e) Different vehicle poses. (f) Poor lighting condition during nighttime.</figDesc><graphic coords="8,44.99,209.93,246.02,145.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Tracking example during sunny daytime. The vehicles are marked with green rectangles. Each vehicle is assigned with a yellow number. The trajectories are drawn with yellow lines. (a) Frame 299. (b) Frame 307. (c) Frame 315.</figDesc><graphic coords="9,39.71,70.49,246.14,63.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Failure of detection cases. (a) and (b) Two false negative examples. (c) False positive example.</figDesc><graphic coords="9,302.75,70.37,246.14,91.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Bin</head><label></label><figDesc>Tian received the B.S. degree from Shandong University, Jinan, China, in 2009. He is currently working toward the Ph.D. degree in control theory and control engineering with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China. His research interests include intelligent transportation systems, pattern recognition, and computer vision. Ye Li received the B.S. degree in automation from University of Science and Technology Beijing, Beijing, China, in 2009. She is currently working toward the combined Master's and Doctoral degree in control theory and control engineering with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing. Her research interests include intelligent transportation systems, computer vision, and pattern recognition. Bo Li received the B.S. degree from Southeast University, Nanjing, China, in 2009. She is currently working toward the Ph.D. degree in control theory and control engineering with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China. Her research interests include image processing and computer vision and their applications in intelligent transportation systems. Ding Wen (M'95-SM'99) is a Professor with National University of Defense Technology (NUDT), Changsha, China, and a Senior Advisor with the Research Center for Military Computational Experiments and Parallel Systems Technology, NUDT. He has published extensively and received numerous awards. His main research interests include behavioral operation management, human resource management, management information systems, and intelligent systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>DATA SET DESCRIPTION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SYSTEM</head><label>II</label><figDesc>PERFORMANCE Fig. 11. ROC curve for vehicle detection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CRITICAL</head><label>III</label><figDesc>PARAMETERS OF THE ALGORITHM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>RUN TIME PER FRAME BY PROCESS</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. F.-Y. Wang for his instruction and encouragement.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science of China under Grants 71232006, 61233001, and 61101220 and in part by the Ministry of Transport of China under Grant 2012-364-X18-112. The Associate Editor for this paper was Q. Kong. B. Tian, Y. Li, and B. Li are with the State Key</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel control and management for intelligent transportation systems: Concepts, architectures, and applications</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="630" to="638" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A weakly supervised approach for object detection based on soft-label boosting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Workshop Appl. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="331" to="338" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image analysis and rule-based reasoning for a traffic monitoring system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-road vehicle detection using gabor filters and support vector machines</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Digital Signal Process</title>
		<meeting>IEEE Conf. Digital Signal ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1019" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial surveillance using dynamic bayesian networks</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2152" to="2159" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-based vehicle detection system with consideration of the detecting location</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vehicle-component identification based on multiscale textural couriers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H C</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="681" to="694" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A stochastic graph grammar for compositional object representation and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1297" to="1307" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating appearance and edge features for sedan vehicle detection in the blind-spot area</title>
		<author>
			<persName><forename type="first">B.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-A</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="737" to="747" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The layout consistent random field for recognizing and segmenting partially occluded objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d layoutcrf for multi-view object class recognition and segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On-road multivehicle tracking using deformable object model and particle filter with improved likelihood estimation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Niknejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="748" to="758" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Part-based transfer learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adv. Neural Netw</title>
		<meeting>Int. Conf. Adv. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
	<note>ser. ISNN&apos;11</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transfer learning with part-based ensembles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Multiple Classifier Syst</title>
		<meeting>Multiple Classifier Syst</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="271" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review of deterministic approximate inference techniques for bayesian machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neur. Comput. Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Night-time traffic surveillance: A robust framework for multivehicle detection, classification and tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Adv. Video Signal Based Surv</title>
		<meeting>IEEE Conf. Adv. Video Signal Based Surv</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A two-layer nighttime vehicle detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Digit</title>
		<meeting>Digit</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tracking and pairing vehicle headlight in night scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rear-lamp vehicle detection and tracking in low-exposure color video for night conditions</title>
		<author>
			<persName><forename type="first">R</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glavin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="453" to="462" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nighttime brake-light detection by nakagami imaging</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1627" to="1637" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="577" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vehicle tracking by non-drifting mean-shift using projective kalman filter</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L M</forename><surname>Bouttefroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. IEEE Conf</title>
		<meeting>Int. IEEE Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A non-maxima suppression method for edge detection with sub-pixel accuracy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Devernay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>SophiaAntipolis, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. rR-2724</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<meeting><address><addrLine>San Mateo, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. ASME, J. Basic Eng</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
