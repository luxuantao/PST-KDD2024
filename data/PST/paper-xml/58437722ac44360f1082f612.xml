<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Sparse Representation Based Classification for Face Recognition with Insufficient Labeled Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingbo</forename><surname>Zhao</surname></persName>
							<email>mbzhao4@gmail.com</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>yuille@stat.ucla.edu.</email>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><forename type="middle">Ma</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">M</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Laboratory</orgName>
								<address>
									<postCode>518057</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information Sciences and Technology</orgName>
								<orgName type="institution">Donghua University</orgName>
								<address>
									<postCode>201620</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California at Los Angeles</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Cognitive Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">John Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Sparse Representation Based Classification for Face Recognition with Insufficient Labeled Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">094F2724DF98A2F6DAF5AD3CF0566CEB</idno>
					<idno type="DOI">10.1109/TIP.2017.2675341</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2675341, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2675341, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gallery dictionary learning</term>
					<term>semi-supervised learning</term>
					<term>face recognition</term>
					<term>sparse representation based classification</term>
					<term>single labeled sample per person</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of face recognition when there is only few, or even only a single, labeled examples of the face that we wish to recognize. Moreover, these examples are typically corrupted by nuisance variables, both linear (i.e. additive nuisance variables such as bad lighting, wearing of glasses) and non-linear (i.e. non-additive pixel-wise nuisance variables such as expression changes). The small number of labeled examples means that it is hard to remove these nuisance variables between the training and testing faces to obtain good recognition performance. To address the problem we propose a method called Semi-Supervised Sparse Representation based Classification (S 3 RC). This is based on recent work on sparsity where faces are represented in terms of two dictionaries: a gallery dictionary consisting of one or more examples of each person, and a variation dictionary representing linear nuisance variables (e.g. different lighting conditions, different glasses). The main idea is that (i) we use the variation dictionary to characterize the linear nuisance variables via the sparsity framework, then (ii) prototype face images are estimated as a gallery dictionary via a Gaussian Mixture Model (GMM), with mixed labeled and unlabeled samples in a semi-supervised manner, to deal with the non-linear nuisance variations between labeled and unlabeled samples. We have done experiments with insufficient labeled samples, even when there is only a single labeled sample per person. Our results on the AR, Multi-PIE, CAS-PEAL, and LFW databases demonstrate that the proposed method is able to deliver significantly improved performance over existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the Sparse Representation based Classification (S-RC) method, introduced by Wright et al. <ref type="bibr" target="#b0">[1]</ref>, has received a lot of attention for face recognition <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. In SRC, a sparse coefficient vector was introduced in order to represent the test image by a small number of training images. Then the SRC model was formulated by jointly minimizing the reconstruction error and the 1 -norm on the sparse coefficient vector <ref type="bibr" target="#b0">[1]</ref>. The main advantages of SRC have been pointed out in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>: i) it is simple to use without carefully crafted feature extraction, and ii) it is robust to occlusion and corruption.</p><p>One of the most challenging problems for practical face recognition application is the shortage of labeled samples <ref type="bibr" target="#b6">[7]</ref>. This is due to the high cost of labeling training samples by human effort, and because labeling multiple face instances may be impossible in some cases. For example, for terrorist recognition, there may be only one sample of the terrorist, e.g. his/her ID photo. As a result, nuisance variables (or so called intra-class variance) can exist between the testing images and the limited amount of training images, e.g. the ID photo of the terrorist (the training image) is a standard front-on face with neutral lighting, but the testing images captured from the crime scene can often include bad lighting conditions and/or various occlusions (e.g. the terrorist may wear a hat or sunglasses). In addition, the training and testing images may also vary in expressions (e.g. neutral and smile) or resolution. The SRC methods may fail in these cases because of the insufficiency of the labeled samples to model nuisance variables <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>In order to address the insufficient labeled samples problem, Extended SRC (ESRC) <ref type="bibr" target="#b12">[13]</ref> assumed that a testing image equals a prototype image plus some (linear) variations. For example, a image with sunglasses is assumed to equal to the image without sunglasses plus the sunglasses. Therefore, ESRC introduced two dictionaries: (i) a gallery dictionary containing the prototype of each person (these are the persons to be recognized), and (ii) a variation dictionary which contains nuisance variations that can be shared by different persons (e.g. different persons may wear the same sunglasses). Recent improvements on ESRC can give good results for this problem even when the subject only has a single labeled sample (namely the Single Labeled Sample Per Person problem, i.e. SLSPP) <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>However, various non-linear nuisance variables also exist in human face images, which makes prototype images hard to obtain. In other words, the nuisance variables often occur pixel-wise, which are not additive and cannot shared by different persons. For example, we cannot simply add a specific Fig. <ref type="figure">1</ref>. Comparisons of the gallery dictionaries estimated by SSRC (i.e. the mean of the labeled data) and our method (i.e. one Gaussian centroid of GMM by semi-supervised EM initialized by the labeled data mean) using first 300 Principal Components (PCs, dimensional reduction by PCA). This illustrates that our method can estimate a better gallery dictionary with very few labeled images which contains both linear (i.e. occlusion) and non-linear (i.e. smiling) variations. The gallery from our method is learned by 5 semi-supervised EM iterations.</p><p>variation to a neutral image (i.e. the labeled training image) to get its smile images (i.e. the testing images). Therefore, the limited number of training images may not yield a good prototype to represent the testing images, especially when non-linear variations exist between them. Attempts were to learn the gallery dictionary (i.e. better prototype images) in Superposed SRC (SSRC) <ref type="bibr" target="#b17">[18]</ref>. However, it requires multiple labeled samples per subject, and still used simple linear operations (i.e. averaging the labeled faces w.r.t each subject) to get the gallery dictionary.</p><p>In this paper, we propose a probabilistic framework called Semi-Supervised Sparse Representation based Classification (S 3 RC) to deal with the insufficient labeled sample problem in face recognition, even when there is only one labeled sample per person. Both linear and non-linear variations between the training labeled and the testing samples are considered. We deal with the linear variations by a variation dictionary. After eliminated the linear variation (by simple subtraction), the non-linear variation is addressed by pursuing a better gallery dictionary (i.e. better prototype images) via a Gaussian Mixture Model (GMM). Specifically, in our proposed S 3 RC, the testing samples (without label information) are also exploited to learn a better model (i.e. better prototype images) in a semi-supervised manner to eliminate the non-linear variation between the labeled and unlabeled samples. This is because the labeled samples are insufficient, and exploiting the unlabeled samples ensures that the learned gallery (i.e. the better prototype) can well represent the testing samples and give better results. An illustrative example which compares the prototype image learned from our method and the existing SSRC is given in Fig. <ref type="figure">1</ref>. Clearly from Fig. <ref type="figure">1</ref>, we can see that, with insufficient labeled samples, a better gallery dictionary is learned by S 3 RC that can well address the non-linear variations. Also Figs. 8 and 12 in the later sections show that the learned gallery dictionary of our method can well represent the testing images for better recognition results.</p><p>In brief, since the linear variations can be shared by different persons (e.g. different persons can wear the same sunglasses), therefore, we model the linear variations by a variation dictionary, where the variation dictionary is constructed by a large pre-labeled database which is independent of the training or testing. Then, we rectify the data to eliminate linear variations using the variation dictionary. After that, a GMM is applied to the rectified data, in order to learn a better gallery dictionary that can well represent the testing data which contains nonlinear variation from the labeled training. Specifically, all the images from the same subject are treated as a Gaussian with its Gaussian mean as a better gallery. Then, the GMM is optimized to get the mean of each Gaussian using the semi-supervised Expectation-Maximization (EM) algorithm, initialized from the labeled data, and treating the unknown class assignment of the unlabeled data as the latent variable. Finally, the learned Gaussian means are used as the gallery dictionary for sparse representation based classification. The major contributions of our model are:</p><p>• Our model can deal with both linear and non-linear variations between the labeled training and unlabeled testing samples. • A novel gallery dictionary learning method is proposed which can exploit the unlabeled data to deal with the non-linear variations. • Existing variation dictionary learning methods are complementary to our method, i.e. our method can be applied to other variation dictionary learning method to achieve improved performance. The rest of the paper is organized as follows. We first summarize the notation and terminology in the next subsection. Section II describes background material and related work. SSRC and ESRC are described in Section III. In Section IV, starting with the insufficient training samples problem, we introduce the proposed S 3 RC model, discuss the EM optimization, and then we extend S 3 RC to the SLSPP problem. Extensive simulations have been conducted in Section V, where we show that by using our method as a classifier, further improved performance can be achieved using Deep Convolution Neural Network (DCNN) features. Section VI discusses the experimental results, and is followed by concluding remarks in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summary of notation and terminology</head><p>In this paper, capital bold and lowercase bold symbols are used to represent matrices and vectors, respectively. 1 d ∈ R d×1 denotes the unit column vector, and I is the identity matrix</p><formula xml:id="formula_0">. || • || 1 , || • || 2 , || • || F denote the 1 , 2 ,</formula><p>and Frobenius norms, respectively. â is the estimation of parameter a.</p><p>In the following, we demonstrate the promising performance of our method on two problems with strictly limited labeled data: i) the insufficient uncontrolled gallery samples problem without generic training data, and ii) the SLSPP problem with generic training data. Here, uncontrolled samples are images containing nuisance variables such as different illumination, expression, occlusion, etc. We call these nuisance variables as intra-class variance in the rest of the paper. The generic training dataset is an independent dataset w.r.t the training/testing dataset. It contains multiple samples per person to represent the intra-class variance. In the following, we use the insufficient training samples problem to refer to the former problem, and the SLSPP problem is short for the latter one. We do not distinguish the terms training/gallery/labeled samples, testing/unlabeled samples in the following. But note that the gallery samples and gallery dictionary are not identical. The latter means the learned dictionary for recognition.</p><p>The promising performance of our method is obtained by estimating the prototype of each person as the gallery dictionary, and the prototype is estimated using both labeled and unlabeled data. Here, the prototype means a learned image that represents the discriminative features of all the images from a specific subject. There is only one prototype for each subject. Typically, the prototype can be the neutral image of a specific subject without occlusion and obtained under uniform illumination. Our method learn the prototype by estimating the true centroid for both labeled and unlabeled data of each person, thus we do not distinguish the prototype and true centroid in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The proposed method is a Sparse Representation based Classification (SRC) method. Many research works have been inspired by the original SRC method <ref type="bibr" target="#b0">[1]</ref>. In order to learn a more discriminative dictionary, instead of using the training data itself, Yang et al. introduced the Fisher discrimination criterion to constrain the sparse code in the reconstructed error <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Ma et al. learned another discriminative dictionary by imposing low-rank constraints on it <ref type="bibr" target="#b20">[21]</ref>. Following these approaches, a model unifying <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b20">[21]</ref> was proposed by Li et al. <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Alternatively, Zhang et al. proposed a model to indirectly learn the discriminative dictionary by constraining the coefficient matrix to be low-rank <ref type="bibr" target="#b23">[24]</ref>. Chi and Porikli incorporated SRC and Nearest Subspace Classifier (NSC) into a unified framework, and balanced them by a regularization parameter <ref type="bibr" target="#b24">[25]</ref>. However, this category of methods need sufficient samples of each subject to construct an over-complete dictionary for modeling the variations of the uncontrolled samples <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, and hence is not suitable for the insufficient training samples problem and the SLSPP problem.</p><p>Recently, ESRC was proposed to address the limitations of SRC when the number of samples per class is insufficient to obtain an over-complete dictionary, where a variation dictionary is introduced to represent the linear variation <ref type="bibr" target="#b12">[13]</ref>. Motivated by ESRC, Yang et al. proposed the Sparse Variation Dictionary Learning (SVDL) model to learn the variation dictionary V, more precisely <ref type="bibr" target="#b13">[14]</ref>. In addition to modeling the variation dictionary by a linear illumination model, Zhuang et al. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> also integrated auto-alignment into their method. Gao et al. <ref type="bibr" target="#b25">[26]</ref> extended the ESRC model by dividing the image samples into several patches for recognition. Wei and Wang proposed robust auxiliary dictionary learning to learn the intra-class variation <ref type="bibr" target="#b16">[17]</ref>. The aforementioned methods did not learn a better gallery dictionary to deal with non-linear variation, therefore good prototype images (i.e. the gallery dictionary) were hard to obtain. To address this issue, Deng et al. proposed SSRC to learn the prototype images as the gallery dictionary <ref type="bibr" target="#b17">[18]</ref>. But this uses only simple linear operations to estimate the gallery dictionary, which requires sufficient labeled gallery samples and it is still difficult to model the non-linear variation.</p><p>There are semi-supervised learning (SSL) methods which use sparse/low-rank techniques. For example, Yan and Wang <ref type="bibr" target="#b26">[27]</ref> used sparse representation to construct the weight of the pairwise relationship graph for SSL. He et al. <ref type="bibr" target="#b27">[28]</ref> proposed a nonnegative sparse algorithm to derive the graph weights for graph-based SSL. Besides the sparsity property, Zhuang et al. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> also imposed low-rank constraints to estimate the weight matrix of the pairwise relationship graph for SSL. The main difference between them and our proposed method S 3 RC is that the previous works used sparse/low-rank technologies to learn the weight matrix for graph-based SSL, which are essentially SSL methods. By contrast our method aims at learning a precise gallery dictionary in the ESRC framework, and the gallery dictionary learning was assisted by probabilitybased SSL (GMM), which is essentially a SRC method. Also note that as a general tool, GMM has been used for face recognition for a long time since Wang and Tang <ref type="bibr" target="#b30">[31]</ref>. However, to the best of our knowledge, GMM has not been previously used for gallery dictionary learning in SRC based face recognitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SEMI-SUPERVISED SPARSE REPRESENTATION BASED CLASSIFICATION WITH EM ALGORITHM</head><p>In this section, we present our proposed S 3 RC method in detail. Firstly, we introduce the general SRC formulation with the gallery plus variation framework, in which the linear variation is directly modeled by the variation dictionary. Then, we prove that, after eliminating linear variations of each sample (which we call rectification), the rectified data (both labeled and unlabeled) from one person can be modeled as a Gaussian to learn the non-linear variations. Following this, the whole rectified dataset including both labeled and unlabeled samples are formulated by a GMM. Next, initialized by the labeled data, the semi-supervised EM algorithm is used to learn the mean of each Gaussian as the prototype images. Then, the learned gallery dictionary is used for face recognition by the gallery plus variation framework. After that, we describe the way to apply S 3 RC to the SLSPP problem. Finally, the overall algorithm is summarized.</p><p>We use the gallery plus variation framework to address both linear and non-linear variations. Specifically, the linear variation (such as illumination changes, different occlusions) is modeled by the variation dictionary. After eliminating the linear variation, we address the non-linear variation (e.g. expression changes) between the labeled and unlabeled samples by estimating the centroid (prototype) of each Gaussian of the GMM. Note that GMM learn the class centroid (prototype) by semi-supervised clustering, i.e. we only use the ground truth label as supervised information, the class assignment of the unlabeled data is treated as the latent variable in EM and updated iteratively during learning the class centroid (prototype).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The gallery plus variation framework</head><p>The SRC with gallery plus variation framework has been applied to the face recognition problem as follows. The observed images are considered as a combination of two different sub-signals, i.e. a gallery dictionary P plus a variation dictionary V in the linear additive model <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_1">y = Pα + Vβ + e,<label>(1)</label></formula><p>where α is a sparse vector that selects a limited number of bases from the gallery dictionary P, and β is another sparse vector that selects a limited number of bases from the universal linear variation dictionary V, and e is a small noise. The sparse coefficients α, β can be estimated by solving the following 1 minimization problem:</p><formula xml:id="formula_2">α β = arg min α,β P V α β -y 2 2 + λ α β 1 ,<label>(2)</label></formula><p>where λ is a regularization parameter. Finally, recognition can be conducted by calculating the reconstruction residuals for each class using α (according to each class) and β, i.e. the test sample y is classified to the class with the smallest residual.</p><p>In this process, the linear additive variation (e.g. illumination changes, different occlusions) of human faces can be directly modeled by the variation dictionary, given the fact that the linear additive variation can be shared by different subjects, e.g. different persons may wear the same sunglasses. Let A = [A 1 , ..., A i , ..., A K ] ∈ R D×n denote a set of n labeled images with multiple images per subject (class), where A i ∈ R D×ni is the stacked n i sample vectors of subject i, and D is the data/feature dimension, the (linear) variation dictionary can be constructed by:</p><formula xml:id="formula_3">V = [A - 1 -a * 1 1 T n1 , ..., A - K -a * K 1 T n K ],<label>(3)</label></formula><formula xml:id="formula_4">V = [A 1 -c 1 1 T n1 , ..., A K -c K 1 T n K ],<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">c i = 1 ni A i 1 ni ∈ R D×1</formula><p>is the i-th class centroid of the labeled data. a * i ∈ R D×1 is the prototype of class i that can best represent the discriminative features of all the images from subject i, A - i is the complementary set of a * i according to A i .</p><p>The gallery dictionary P can then be set accordingly using one of the following equations:</p><formula xml:id="formula_6">P = A,<label>(5)</label></formula><formula xml:id="formula_7">P = [c 1 , ..., c K ],<label>(6)</label></formula><p>The aforementioned formulations of the gallery dictionary P and variation dictionary V works well when a large amount of labeled data is available. However, in practical applications such as recognizing a terrorist by his ID photo, the labeled/training data is often limited and the unlabeled/testing images are often taken under severely different conditions from the labeled/training data. Therefore, it is hard to obtain good prototype images to represent the unlabeled/testing images from the labeled/training data only. In order to address the non-linear variation between the labeled and unlabeled samples, in the following we learn a prototype a * i for each class by estimating the true centroid for both the labeled and unlabeled data of each subject, and represent the gallery dictionary P using the learned prototype a * i . (The importance of learning the gallery dictionary is shown in the previous Fig. <ref type="figure">1</ref> and Figs. 8 and 12 in the later sections.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construct the data from each class as a Gaussian after eliminating linear variations</head><p>We rectify the data to eliminate linear variations of each sample (e.g. illumination changes, occlusions), so that the data from one person can be modeled as a Gaussian. This can be achieved by solving Eq. (1) using Eq. ( <ref type="formula" target="#formula_6">5</ref>) or <ref type="bibr" target="#b5">(6)</ref> to represent the gallery dictionary and using Eq. ( <ref type="formula" target="#formula_3">3</ref>) or (4) to represent the variation dictionary:</p><formula xml:id="formula_8">ŷ = y -V β = Pα + e, (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where ŷ is the rectified unlabeled image without linear variation, α and β can be initialized by Eq. ( <ref type="formula" target="#formula_2">2</ref>).</p><p>Then, the problem becomes to find the relationship between the rectified unlabeled data ŷ and its corresponding class centroid a * . Note that the sparse coefficient α is sparse and typically there is only one entry of P that represents each class. For an unlabeled sample, y, Pα actually selects the most significant entry of P, i.e. , it selects the class centroid that is nearest to ŷ.</p><p>However, the "class centroid" selected by Pα cannot be directly used as the initial class centroid for each Gaussian, because the biggest element of the sparse coefficient α typically does not take value 1. In other words, Pα can introduce scaling on the class centroid and additional (small) noise. More specifically, assume that the most significant entry of α is associated with class i, thus we have</p><formula xml:id="formula_10">Pα = P[ , , ..., s (i-th entry), ..., ] T = sa * i + Pα - i = sa * i + e ,<label>(8)</label></formula><p>where the sparse coefficient α = [ , , ..., s (i-th entry), ..., ] T consisting of small values and a significant value s in its i-th entry. a * i is the i-th column of P, e = Pα - i is the summation of the "noise" class centroids selected by αi , in which αi contains only the small values (i.e. 's) is the complementary set of s according to α.</p><p>Recall that the gallery dictionary P has been normalized to have column unit 2 -norm in Eq. ( <ref type="formula" target="#formula_2">2</ref>), therefore, the scale parameter s can be eliminated by normalizing y -V β to have unit 2 -norm:</p><formula xml:id="formula_11">ŷnorm = norm(y -V β) = norm(sa * + e + e) ≈ a * i + e * ,<label>(9)</label></formula><p>where e * is a small noise which is assumed to be a zeromean Gaussian. Since there are insufficient samples from each subject, we assign the Gaussian noise of each class (subject) to be different from each other, i.e. e * i = N (0, Σ i ), so as to estimate the gallery dictionary more precisely. Thus, the normalized ŷ obeys the following distribution:</p><formula xml:id="formula_12">ŷnorm ≈ a * i + e * i ∈ N (a * i , Σ i ).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GMM Formulation</head><p>After modeling the rectified data for each subject as a Gaussian, we construct a GMM for the whole data to estimate the true centroids a * , to address the non-linear variation between the labeled/training and unlabeled/testing samples. Specifically, the unknown assignment for the unlabeled data is used as the latent variable. The detailed formulation is given in the following.</p><p>Let D = {(y 1 , l 1 )..., (y n , l n ), y n+1 , ..., y N } denote a set of images of K classes including both labeled and unlabeled samples, i.e. {(y 1 , l 1 )..., (y n , l n )} are n labeled samples with {y i ∈ A li , i = 1, ..., n}; and {y n+1 , ..., y N } are N -n unlabeled samples. Based on Eq. ( <ref type="formula" target="#formula_12">10</ref>), a GMM can be formulated to model the data and the EM algorithm can be used to more precisely estimate the true class centroids by clustering the normalized rectified images (that exclude the linear variations).</p><p>Firstly, the normalized rectified dataset Dnorm = {(ŷ norm 1 , l 1 )..., (ŷ norm n , l n ), ŷnorm n+1 , ..., ŷnorm N } must be calculated in order to construct the GMM. The calculation of the normalized rectifications includes two parts: i) for the labeled data, the normalized rectifications are the roughly estimated class centroids; ii) for the unlabeled data, the normalized rectifications can be estimated by Eq. ( <ref type="formula" target="#formula_11">9</ref>):</p><formula xml:id="formula_13">ŷnorm i = c li , if i ∈ {1, ..., n}, norm(y i -V βi ), if i ∈ {n + 1, ..., N },<label>(11)</label></formula><p>where c li is the mean of the labeled data of the l i -th subject, i.e. it is the roughly estimated centroid of class l i , V is the variation dictionary, and βi is the sparse coefficient vector estimated by Eq. ( <ref type="formula" target="#formula_2">2</ref>). Following this, the GMM can be constructed as described in <ref type="bibr" target="#b31">[32]</ref>. Specifically, let π j denote the prior probability of class j, i.e. p(j) = π j , and θ be a set of unknown model parameter: θ = {a * j , Σ j , π j , for j = (1, ..., K)}. For the incomplete samples (the unlabeled data), an latent indicator z i,j is introduced to denote their label. That is, z i,j = 1, if ŷnorm i ∈ class j; otherwise z i,j = 0. Therefore, the objective function to optimize θ can be obtained as:</p><formula xml:id="formula_14">θ = arg max θ log p( Dnorm |θ), s.t. K j π j = 1,<label>(12)</label></formula><p>where the log likelihood log p( Dnorm |θ) is:</p><formula xml:id="formula_15">log p( Dnorm |θ) = log N i=1 p(ŷ norm i |θ) zi,j = log n i=1 p(ŷ norm i , l i |θ) N i=n+1 p(ŷ norm i |θ) zi,j = n i=1 log π li N (ŷ norm i |a * li , Σ li ) + N i=n+1 K j=1 z i,j log π j N (ŷ norm i |a * i , Σ i ).<label>(13)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_15">13</ref>) the label of sample i, i.e. l i , was used as a subscript to index the mean and variance, i.e. a * li , Σ li , of the cluster which sample i belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Estimation of the gallery dictionary by semi-supervised EM algorithm</head><p>The EM algorithm is applied to estimate the unknown parameters θ by iteratively calculating the latent indicator (label) of the unlabeled data and maximizing the log likelihood log p( Dnorm |θ) <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b35">[36]</ref>.</p><p>For the EM iterations, a * j , Σ j and π j for j = 1, ..., K are initialized by p j , I, and n i /n, respectively. Here, n i is the number of labeled samples in each class and n is the total number of labeled samples.</p><p>E-Step: this aims at estimating the latent indicator, z i,j , of the unlabeled data using the current estimate of θ. For the labeled data, z i,j is known then it is set to its label, which is the main difference between semi-supervised EM and original EM. (This has already been applied in Eq. ( <ref type="formula" target="#formula_15">13</ref>)):</p><formula xml:id="formula_16">ẑi,j = 1, if i ∈ {1, ..., n} and j = l i , 0, if i ∈ {1, ..., n} and j = l i .<label>(14)</label></formula><p>Equation ( <ref type="formula" target="#formula_16">14</ref>) ensures that the "estimated labels" of the labeled samples are fixed by their true labels. Thus, the labeled data plays a role of anchors, which encourage the EM, applied to the whole dataset, to converge to the true gallery.</p><p>For the unlabeled data, the algorithm uses the expectation of z i,j from the previously estimated model θ old , to give a good estimation of z i,j :</p><formula xml:id="formula_17">ẑi,j = E[z i,j ] = π old j 1 |Σ old j | 1/2 exp (-1 2 ||ŷ norm i -(a * j ) old || 2 Σ old j ) K k=1 π old k 1 |Σ old k | 1/2 exp (-1 2 ||ŷ norm i -(a * k ) old || 2 Σ old k ) , if i ∈ {n + 1, ..., N }.<label>(15)</label></formula><p>M-Step: z i,j in Eq. ( <ref type="formula" target="#formula_15">13</ref>) can be substituted by ẑi,j , so that we can optimize the model parameter θ. By using Maximum-Likelihood Estimation (MLE), the optimized model parameters can be obtained by:</p><formula xml:id="formula_18">Nj = N i=1 ẑi,j , πj = Nj N . (<label>16</label></formula><formula xml:id="formula_19">) â * j = 1 Nj N i=1 ẑi,j ŷnorm i ,<label>(17)</label></formula><formula xml:id="formula_20">Σj = 1 Nj N i=1 ẑi,j (ŷ norm i -â * j )(ŷ norm i -â * j ) T ,<label>(18)</label></formula><p>The E-Step and M-Step iterate until the log likelihood log p( Dnorm |θ) converges.</p><p>Initialization: The EM algorithm needs good initialization to avoid the local minima. To initialize the mean of each Gaussian a * i , it is natural to use the roughly estimated class centroids, i.e. the mean of labeled data, c i (See Fig. <ref type="figure">1</ref> as an example). The variance for each class is initialized by the identity matrix, i.e. Σ i = I, i = 1, ..., K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Classification using the estimated gallery dictionary and SRC</head><p>The estimated â * j is used as the gallery dictionary,</p><formula xml:id="formula_21">P * = [â * 1 , ..., â * K ]</formula><p>. Then P * is used to substitute P in Eq. ( <ref type="formula" target="#formula_2">2</ref>), to estimate the new sparse coefficients, α * and β * . Finally, the residuals for each class k are computed for the final classification by:</p><formula xml:id="formula_22">r k (y) = y -P * V δ k (α * ) β * 2 F ,<label>(19)</label></formula><p>where δ k (α * ) is a vector whose nonzero entries are the entries in α * that are associated with class k. Then the testing sample y is classified into the class with smallest r k (y), i.e. Label(y) = arg min k r k (y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. S 3 RC model for SLSPP problem with generic dataset</head><p>Recently, a lot of researchers have introduced an extra generic dataset for addressing the face recognition problem with SLSPP <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, of which the SRC methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref> have achieved state-of-the-art results.</p><p>Here, the generic dataset can be an independent dataset from the training and testing dataset D. When a generic dataset is given in advance, our model can also be easily applied to the SLSPP problem.</p><p>In the SLSPP problem, the input data has N samples, D = {(y 1 , 1), ..., (y K , K), y K+1 , ..., y N }. From D, the set of the labeled data, T = {y 1 , ..., y K }, is known as the gallery dataset, where there is only one sample for each subject.</p><formula xml:id="formula_23">G = {G 1 , ..., G K g } ∈ R D×N g</formula><p>denotes a labeled generic dataset with N g samples and K g subjects in total. Here, D is the data dimension shared by gallery, generic and testing data, and G i ∈ R D×n g i is the stacked n g i vectors of the samples from class i.</p><p>Due to the limited number of gallery samples, the initial class center is set to the only labeled sample of each class, and the corresponding variation dictionary can be constructed similar to Eq. ( <ref type="formula" target="#formula_4">4</ref>):</p><formula xml:id="formula_24">P = T,<label>(20)</label></formula><formula xml:id="formula_25">V = [G 1 -c g 1 1 T n g 1 , ..., G K -c g 1 1 T n g K g ],<label>(21)</label></formula><p>where c g i is the average of the samples according to i-th subject in generic dataset, i.e. c g i = 1</p><formula xml:id="formula_26">n g i G i 1 n g i .</formula><p>The obtained initial gallery dictionary and variation dictionary can then be applied in our model, as discussed in Section III-B -Section III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Summary of the algorithm</head><p>The overall algorithm is summarized in Algorithm 1. We also gave an illustrated procedures of the proposed method in Fig. <ref type="figure" target="#fig_0">2</ref>. The inputs to the algorithm, besides the regularization parameter λ, are:</p><p>• For the insufficient training samples problem: a set of images including both labeled and unlabeled samples D = {(y 1 , l 1 )..., (y n , l n ), y n+1 , ..., y N }, where y i ∈ R D , i = 1, ..., N are image vectors, and l i , i = 1, ..., n are the labels. We denote n i to be the number of labeled samples of each class. • For the SLSPP problem with generic dataset: the dataset including gallery and testing data D = {(y 1 , 1)..., (y K , K), y K+1 , ..., y N }, in which T = {y 1 , ..., y K } is the gallery set with SLSPP and 1, ..., K are the labels. A labeled generic dataset with N g samples from K g subjects, G = {G 1 , ..., G N g }. To investigate the convergence of the semi-supervised EM, Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the typical performance of change in negative log-likelihood (from a randomly selected run on the AR database), which convergence with less than 5 iterations. A converged example is illustrated in Fig. <ref type="figure">1</ref> (i.e. the rightmost subfigure, obtained by 5 iterations). From both Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_2">3</ref>, it can be observed that the algorithm converges quickly and the discriminative features of each specific subject can be learned in a few steps.</p><p>Note that our algorithm is related to the SRC methods which also used the gallery plus variation framework, such as ESRC <ref type="bibr" target="#b12">[13]</ref>, SSRC <ref type="bibr" target="#b17">[18]</ref>, SVDL <ref type="bibr" target="#b13">[14]</ref> and SILT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Among these method, only SSRC aims to learn the gallery Algorithm 1: Semi-Supervised Sparse Representation based Classification Compute the prototype matrix, P, by Eq. ( <ref type="formula" target="#formula_7">6</ref>) (for the insufficient training samples problem), or by Eq. ( <ref type="formula" target="#formula_24">20</ref>) (for the SLSPP problem). Compute the universal linear variation matrix, V, by Eq. (4) (for the insufficient training samples problem), or by Eq. ( <ref type="formula" target="#formula_25">21</ref>) (for the SLSPP problem). / * V can also be calculated by other variation dict. learning methods. * / Apply dimensional reduction (e.g. PCA) on the whole dataset as well as P and V, and then normalize them to have column unit 2 norm. Solve the Sparse Representation problem to estimate α and β for all the unlabeled y by Eq. ( <ref type="formula" target="#formula_2">2</ref>). Rectify the samples to eliminate linear variation and normalize them to have unit 2 -norm by Eq. <ref type="bibr" target="#b10">(11)</ref>. Initialize each Gaussian of the GMM by N (p i , I) for i = 1, ...K, where p i is the i-th column of P. Initialize the prior of GMM, π i = n i /n (for the insufficient training samples problem), or π i = 1/K (for SLSPP problem) for i = 1, ...K. repeat E-Step: Calculate ẑij by Eqs. ( <ref type="formula" target="#formula_16">14</ref>) and ( <ref type="formula" target="#formula_17">15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">M-</head><p>Step: Optimize the model parameter θ = {µ j , Σ j , π j , for j = 1, ..., K} by Eqs. ( <ref type="formula" target="#formula_18">16</ref>)- <ref type="bibr" target="#b17">(18)</ref>. 11 until Eq. ( <ref type="formula" target="#formula_15">13</ref>) converges; 12 Let P * = [μ 1 , ..., μK ], estimate α * , β * by Eq. ( <ref type="formula" target="#formula_2">2</ref>). <ref type="bibr" target="#b12">13</ref> Compute the residual r k (y) by Eq. ( <ref type="formula" target="#formula_22">19</ref>). dictionary, but it needs sufficient label/training data. Also it is not ensured that the learned prototype from SSRC can well represent the testing data due to the possible severe variation between the labeled/training and the testing samples. While ESRC, SVDL and SILT focus to learn a more representative variation dictionary. The variation dictionary learning in these methods are complementary to our proposed method. In other words, we can use them to replace Eqs. ( <ref type="formula" target="#formula_3">3</ref>), ( <ref type="formula" target="#formula_4">4</ref>) or <ref type="bibr" target="#b20">(21)</ref> for better performance, which will be verified in the experiments by using SVDL to construct our variation dictionary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>In this section, our model is tested to verify the performance for both the insufficient training samples problem and the SLSPP problem. Firstly, the performance of the proposed method on the insufficient training samples problem using the AR database <ref type="bibr" target="#b38">[39]</ref> is shown. Then, we test our method on the SLSPP problem on both the Multi-PIE <ref type="bibr" target="#b39">[40]</ref> and the CAS-PEAL <ref type="bibr" target="#b40">[41]</ref> databases, using one facial image with neutral expression from each person as the only labeled gallery sample. Next, in order to further investigate the performance of the proposed semi-supervised gallery dictionary learning, we re-do the SLSPP experiments with one randomly selected image (i.e. uncontrolled image) per person as the only gallery sample. The performance has been evaluated on the Multi-PIE <ref type="bibr" target="#b39">[40]</ref>, and the more challenging LFW <ref type="bibr" target="#b41">[42]</ref> databases. After that, the influence of different amounts of labeled and/or unlabeled data is investigated. Finally, we have illustrated the performance of our method through a practical system with automatic face detection and automatic face alignment.</p><p>For all the experiments, we report the results from both transductive and inductive experimental settings. Specifically, in the transductive setting, the data is partitioned into two parts, i.e. the labeled training and the unlabeled testing. We focus on the performance on the unlabeled/testing data, where we do not distinguish the unlabeled and the testing data. In inductive setting, we split the data into three parts, i.e. the labeled training, the unlabeled training and the testing, where the model is learned by the labeled training and the unlabeled training data, the performance is evaluated on the testing data. In order to provide comparable results, the Homotopy method <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref> was used to solve the<ref type="foot" target="#foot_1">1</ref> minimization problem for all the methods involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance on the insufficient training samples problem</head><p>In the AR database <ref type="bibr" target="#b38">[39]</ref>, there are 126 subjects with 26 images for each of them taken at two different sessions (dates), each session containing 13 images. The variations in the AR database include illumination change, expressions and facial occlusions. In our experiment, a cropped and normalized subset of the AR database that contains 100 subjects with 50 males and 50 females is used. The corresponding 2600 images are cropped to 165 × 120. This subset of AR used in our experiment has been selected and cropped by the data provider <ref type="bibr" target="#b45">[46]</ref> 1 . Figure <ref type="figure" target="#fig_3">4</ref> illustrates one session (13 images) for a specific subject.</p><p>Here we conduct four experiments to investigate the performance of our methods, and the results are reported from both the transductive and the inductive settings. No extra generic dataset is used in either of the experiments. The experimental settings are:</p><p>• Transductive: There are two transductive experiments.</p><p>One is an extension of the experiment in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, by using different amounts of labeled data. In the experiment, 2-13 uncontrolled images (of 26 images in total) of State-of-the-art methods for the insufficient training samples problem are used for comparison, including sparse/dense representation methods SRC <ref type="bibr" target="#b0">[1]</ref> and ProCRC <ref type="bibr" target="#b46">[47]</ref>, low-rank models DLRD SR <ref type="bibr" target="#b20">[21]</ref>, D 2 L 2 R 2 <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, gallery plus variation representation ESRC <ref type="bibr" target="#b12">[13]</ref>, SSRC <ref type="bibr" target="#b17">[18]</ref>, and RADL <ref type="bibr" target="#b16">[17]</ref>. We follow the same settings of other parameters as in <ref type="bibr" target="#b17">[18]</ref>, i.e. first 300 PCs from PCA <ref type="bibr" target="#b47">[48]</ref> have been used and λ is set to 0.005. The results are illustrated by the mean and standard deviation of 20 runs. Particularly, in order to show that the generalizability of the proposed gallery dictionary learning S 3 RC, we also investigate the performance of our method in RADL framework, i.e. we replace the gallery dictionary in RADL by the learned gallery from S 3 RC and keep the remaining part of the RADL model unchanged. We denote it by S 3 RC-RADL.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows that our results, i.e. S 3 RC and S 3 RC-RADL, consistently outperform other counterparts under the same configurations. Moreover, significant improvements in the results are observed when there are few labeled samples per subject. Especially, it is observed that the results of SSRC are less satisfactory comparing with more recent state-of-theart methods RADL and ProCRC, however, its performance is boosted to the second highest by utilizing the proposed gallery dictionary method S 3 RC. For example, the accuracy of S 3 RC is higher by around 10% than SSRC when using 2-4 labeled samples per person. Furthermore, by combining with more recent RADL method, S 3 RC-RADL achieves the best performance. It is also noted that the size of outperformance decreases when more labeled data is used. This is because the class centroids estimated by SSRC (averaging the labeled data according to same label) are less likely to be the true gallery when the number of labeled data are small. Thus, by improving the estimates of the true gallery from the initialization of the averaged labeled data, better results can be obtained by our method. Conversely, if the number of labeled data is sufficiently large, then the averaged labeled data becomes good estimates of the true gallery, which results in less improvement compared with our method.</p><p>The results of Figs. 5a and 5c (i.e. the Left Column) are higher than those of Figs. <ref type="figure" target="#fig_4">5b</ref> and<ref type="figure" target="#fig_4">5d</ref> (i.e. the Right Column) for all methods, because the labeled and unlabeled samples of the Right Column of Fig. <ref type="figure" target="#fig_4">5</ref> are obtained from different sessions. Interestingly, the higher outperformance of S 3 RC can be observed in the more challenging experiment shown in Fig. <ref type="figure" target="#fig_4">5b</ref>. This observation further demonstrates the effectiveness of our proposed semi-supervised gallery dictionary learning method. Also, the results of the transductive experiments (i.e. Figs. <ref type="figure" target="#fig_4">5a</ref> and<ref type="figure" target="#fig_4">5b</ref>) are better than those of the inductive experiments (i.e. Figs. <ref type="figure" target="#fig_4">5c</ref> and<ref type="figure" target="#fig_4">5d</ref>), because the testing samples have been directly used to learn the model in the transductive settings.</p><p>The above results have demonstrated the effectiveness of our method for the insufficient training samples problem. ). The gallery image from a specific subject and its corresponding unlabeled/testing images with a randomly selected illumination are shown in Fig. <ref type="figure">6</ref>.</p><p>The results from classical classifiers NN <ref type="bibr" target="#b47">[48]</ref>, SVM <ref type="bibr" target="#b48">[49]</ref>, sparse/dense representation SRC <ref type="bibr" target="#b0">[1]</ref>, CRC <ref type="bibr" target="#b8">[9]</ref>, ProCRC <ref type="bibr" target="#b46">[47]</ref>, gallery plus variation representation ESRC <ref type="bibr" target="#b12">[13]</ref>, SVDL <ref type="bibr" target="#b13">[14]</ref>, RADL <ref type="bibr" target="#b16">[17]</ref> are chosen for comparison <ref type="foot" target="#foot_2">2</ref> . In order to further investigate the generalizability of our method and to show the power of the gallery dictionary estimation, besides the evaluation on S 3 RC-RADL, we also report the results of S 3 RC using the variation dictionary learned by SVDL (S 3 RC-SVDL), i.e. initializing the first four steps of the Algorithm 1 by SVDL. The parameters were identical to those in <ref type="bibr" target="#b13">[14]</ref>, i.e. the 90 PCA dimension and λ = 0.001. The transductive and inductive experimental settings are: Fig. <ref type="figure">6</ref>. The cropped image instances of one subject from various subsets of the Multi-PIE database.</p><p>• Transductive: Here we use all the images from the corresponding session as unlabeled testing data, the results are summarized in the top subfigure of Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>• Inductive: The bottom subfigure of Fig. <ref type="figure" target="#fig_6">7</ref> summarizes the inductive experimental results, where the 20 images of the corresponding session were partitioned into two parts, i.e. half for unlabeled training and the other half for testing. The inductive results are obtained by averaging 20 replicates. Figure <ref type="figure" target="#fig_6">7</ref> shows that the proposed gallery dictionary learning methods, i.e. S 3 RC, S 3 RC-SVDL, S 3 RC-RADL achieve the top recognition rates compared with the other categories of methods in recognizing all the subsets. In particular, the highest enhancements can be observed for recognition with varying expressions. The reason might be that the samples with different expressions cannot be properly aligned by using only the eye centers, so the gallery dictionary learned by S 3 RC can achieve better alignment with the testing images. This demonstrates that gallery dictionary learning plays an important role in SRC based face recognition. In addition, our inductive results are comparable to the transductive results, which implies we might do not need as many as 20 images as unlabeled training to learn the model on Multi-PIE database. It is also observed that in this challenging SLSPP problem, all the gallery plus variation representation methods (i.e. ESRC, SVDL, RADL) outperform the best sparse/dense representation method (i.e. ProCRC), suggesting that the gallery plus variation representation methods are more suitable for the SLSPP problem. As a generally applicable gallery dictionary learning method to the gallery plus variation framework, our method further improves the performance on this challenging tasks.</p><p>Furthermore, by integrating the variation dictionary learned by SVDL into S 3 RC, S 3 RC-SVDL, S 3 RC-RADL also improves the performance of S 3 RC in most cases. This also demonstrates the generalizablity of our method. These performance enhancements of S 3 RC, S 3 RC-SVDL and S 3 RC-RADL (w.r.t ESRC, SVDL, and RADL) are benefited from using the  unlabeled samples for estimating the true gallery instead of relying on the labeled samples only. When given insufficient labeled samples, the other methods find it is hard to achieve satisfactory recognition rates in some cases.</p><p>The learned gallery is also investigated. We are especially interested in examining the learned gallery when there is a large difference between the input labeled samples and the unlabeled/testing images. Therefore, we use the neutral image as the labeled gallery and randomly choose 10 images with smile (i.e. from S1 Ca051 R2 and S2 Ca051 R2), the learned gallery is shown in Fig. <ref type="figure" target="#fig_7">8</ref>. Figure <ref type="figure" target="#fig_7">8</ref> illustrates that by using the unlabeled training samples with smile, the learned gallery can also possess desirable (non-linear) smile attributes (see the mouth region), which better represents the prototype of the unlabeled/testing images. In fact, the proposed semisupervised gallery dictionary learning method can be regarded as a pixel-wise alignment between the labeled gallery and the unlabeled/testing images. The analysis in this section demonstrates the promising performance of our method for the SLSPP problem on Multi-PIE database.</p><p>2) The CAS-PEAL Database: The CAS-PEAL database <ref type="bibr" target="#b40">[41]</ref> contains 99594 images with different illuminations, facing directions, expressions, accessories, etc. It is considered to be the largest database available that contains occluded images. Note that although the occlusions may commonly occurs on the objects of interests in practice <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, the Multi-PIE database does not contain images with occlusions. Thus, as a complementary experiment, we use all the 434 subjects from the Accessory category for testing, and their corresponding images from the Normal category as gallery. In this experimental setting, there are 1 neutral image, 3 images with hats, and 3 images with glasses/sunglasses for each subject. All the images are cropped to 100 × 82, with the centers of both eyes located at <ref type="bibr" target="#b18">(19,</ref><ref type="bibr" target="#b27">28)</ref> and (63, 28). Figure <ref type="figure" target="#fig_8">9</ref> illustrates the gallery and testing images for a specific subject.</p><p>Among the 434 subjects used, 300 subjects are selected for training and testing, and the remaining 134 subjects are used as generic training data. The first 100 dimension PCs (dimensional reduction by PCA) are used and λ is set to 0.001. We also compare our results with NN <ref type="bibr" target="#b47">[48]</ref>, SVM <ref type="bibr" target="#b48">[49]</ref>, SRC <ref type="bibr" target="#b0">[1]</ref>, CRC <ref type="bibr" target="#b8">[9]</ref>, ProCRC <ref type="bibr" target="#b46">[47]</ref>, ESRC <ref type="bibr" target="#b12">[13]</ref>, SVDL <ref type="bibr" target="#b13">[14]</ref> and RADL <ref type="bibr" target="#b16">[17]</ref>. The results of the transductive and inductive experiments are reported in Table <ref type="table" target="#tab_0">I</ref>. The experimental settings are:</p><p>• Transductive: All the 6 images of the Accessory category shown in Fig. <ref type="figure" target="#fig_8">9</ref> are used for unlabeled/testing. • Inductive: Of all the 6 images, we randomly select 3 images as unlabeled training and the remaining 3 images are used as testing. The inductive results are obtained by averaging 20 replicates. Table <ref type="table" target="#tab_0">I</ref> shows that S<ref type="foot" target="#foot_3">3</ref> RC, S 3 RC-SVDL and S 3 RC-RADL achieve top three recognition rates in the CAS-PEAL database, only except S 3 RC-RADL vs. RADL in the Inductive case. This is the only case that our method slightly inferior than our counterpart among all of our experiments (including the experiments in the previous and following sections). The reason is that in the inductive experiments, there is too few (i.e. 3 samples per subject) unlabeled data to guarantee the generalizibility of them. Also RADL used weighted 2 norm (i.e. 2 norm with projection) to calculate the data error term, which already gains some robustness to the less perfect gallery dictionary. The results in Table I verify the promising performance of our method for SLSPP problem on CAS-PEAL database. C. The performance on the SLSPP problem using uncontrolled image as gallery</p><p>1) The Multi-PIE Database : In order to validate the proposed S 3 RC methods, additional more challenging exper-iments are performed on the Multi-PIE Database, where an uncontrolled image is used as the labeled gallery. Specifically, for each unlabeled/testing subset illustrated in Fig. <ref type="figure">10</ref>, we randomly choose one image per subject from the other subsets (excluding the unlabeled/testing subset) as the labeled gallery. It should be noted that the well controlled gallery, i.e. the neutral images from S1 Ca050 R1, is not used in this section. Both the transductive and the inductive experiments are also reported as the same protocol used in Sect. IV-B1. Fig. <ref type="figure">10</ref>. The illustrations of the data used in Sect. IV-C1. We first randomly select a subset as unlabeled/testing data, then a gallery sample is randomly chosen from other subsets (excluding the unlabeled/testing select).  <ref type="figure" target="#fig_10">11</ref>, in which the proposed S 3 RC is compared with NN, SVM, SRC <ref type="bibr" target="#b0">[1]</ref>, CRC, and Pro-CRC methods 3 . Figure <ref type="figure" target="#fig_10">11</ref> shows that our method consistently outperforms the other outline methods. In fact, although the overall accuracy decreases due to the uncontrolled labeled gallery, all the conclusions made in Sect. IV-B1 are supported and verified by Fig. <ref type="figure" target="#fig_10">11</ref> here.</p><p>We also investigated the learned gallery when a uncontrolled image is used as input labeled gallery. Figure <ref type="figure" target="#fig_11">12</ref> shows the results when using the squint image (i.e. a image from S2 Ca051 R3) as the single labeled input gallery for each subject. It is observed (see eye and mouth regions) that the learned gallery can better represent the testing images (i.e. smile) by the non-linear semi-supervised gallery dictionary learning. The reason is the same as the previous experiments in Fig. <ref type="figure" target="#fig_7">8</ref>, i.e. the proposed semi-supervised method conducts a pixel-wise alignment between the labeled gallery and the unlabeled/testing images so that the non-linear variations between them are well addressed.</p><p>2) The LFW Database: The Labeled Face in the Wild (LFW) <ref type="bibr" target="#b41">[42]</ref> is the latest benchmark database for face recognition, which has been used to test several advanced methods  with dense or deep features for face verification, such as <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b55">[56]</ref>. In this section, our method has been tested on the LFW database to create a more challenging face identification problem.</p><p>Specifically, the face images of the LFW database are collected from the Internet, as long as they can be detected by the Viola-Jones face detector <ref type="bibr" target="#b41">[42]</ref>. As a result, there are more than 13,000 images in the LFW database containing enormous intra-class variations, where controlled (e.g. neutral and facial) faces may not be available. Considering the previous experiments on the Multi-PIE and CAS-PEAL databases dealt with specific kinds of variations separately, as an important extended experiment, the effectiveness of our S 3 RC method can be further validated by its performance on the LFW dataset.</p><p>A pre-aligned database by deep funneling <ref type="bibr" target="#b56">[57]</ref> was used in our experiment. We select a subset of the LFW database containing more than 10 images for each person, with 4324 images from 158 subjects in total. In the experiments, we randomly select 100 subjects for training and testing the model, the remaining 58 subjects are used to construct the variation dictionary. The experimental results for both transductive and inductive learning are reported. The only gallery image is randomly chosen from each subject, then the transductive and the inductive experimental settings are:</p><p>• Transductive: All the remaining images from a specific subject are used for unlabeled/testing. The results are obtained by averaging 20 replicates due to the randomly selected subjects. • Inductive: For each subject, we randomly select half of the remaining images as unlabeled training and the other half are used for testing. The results are obtained by averaging 50 replicates due to the randomly selected subjects and random unlabeled-testing split.</p><p>The results are shown in Table <ref type="table" target="#tab_1">II</ref>, where we use NN, SVM, SRC <ref type="bibr" target="#b0">[1]</ref>, CRC, and ProCRC for comparison. First, we have tested our method using simple features obtained from an unsupervised dimensional reduction by PCA. The results in the left two columns of Table <ref type="table" target="#tab_1">II</ref> show that, although none of the methods achieve a satisfactory performance, our method, based on the semi-supervised gallery dictionary, still significantly outperforms the baseline methods.</p><p>Nowadays, Deep Convolution Neural Network (DCNN) based methods have achieved state-of-the-art performance on the LFW database <ref type="bibr" target="#b51">[52]</ref>- <ref type="bibr" target="#b55">[56]</ref>. It is noticed that the DCNN methods often use basic classifiers to do the classification, such as softmax, linear SVM or 2 distance. Recently, it is showed that by coupling with the deep-learned CNN features, the SRC methods can achieve significantly improved results <ref type="bibr" target="#b46">[47]</ref>. Motivated by this, we also aim to verify that by utilizing the same deep-learned features, our method (i.e. our classifier) is able to further improve the results obtained by the basic classifiers.</p><p>Specifically, we utilize a recent and public DCNN model named VGG-face <ref type="bibr" target="#b51">[52]</ref> to extract the 4096-dimensional features, then our method, as well as the baseline methods, are implied to perform the classification. The results, shown in the left two-column of Table <ref type="table" target="#tab_1">II</ref>, demonstrate the significantly improved results from the proposed methods using the DCNN features, whereas such an investigation cannot be observed by comparing other SRC methods, e.g. SRC, CRC, ESRC, with the basic NN classifier. It is noted that the original classifier used in <ref type="bibr" target="#b51">[52]</ref> is the 2 distance in face verification, which is equivalent to KNN (K = 1) in face identification with SLSPP. Therefore, the results in Table <ref type="table" target="#tab_1">II</ref> demonstrate that with the state-of-the-art DCNN features, the performance on the LFW database can be further boosted by using the proposed semisupervised gallery dictionary learning method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis of the influence of different amounts of labeled and/or unlabeled data</head><p>The impact of different amounts of unlabeled data in S 3 RC is analyzed on different amounts of labeled data using AR database. In this experiment, we first randomly choose a session, and then select 1-13 unlabeled data for each subject from that session to investigate the influence of different amounts of unlabeled data. 2, 4 and 6 labeled samples of each subject are randomly chosen from the other session. For comparison, we also illustrate the results of SRC, ESRC, SSRC with the same configurations. The results are shown in Fig. <ref type="figure" target="#fig_12">13</ref>, which is obtained by averaging 20 runs.</p><p>It can be observed from Fig. <ref type="figure" target="#fig_12">13</ref> that: i) our method is effective (i.e. can outperform the state-of-the-art) even when there is only 1 unlabeled sample per subject; ii) when more unlabeled samples are used, we observe significant increased accuracy from our method, while the accuracies of the stateof-the-art methods do not change much, because unlabeled information is not considered in these methods; iii) the better performance of our method compared with the alternatives is not affected by different amounts of labeled data used.</p><p>Furthermore, we are also interested in illustrating the learned galleries. Compared with the experiments on the AR database stated above, the experiments similar to those on the Multi-PIE database in Sect.IV-B1 are more suitable to our purpose. It is because in the above AR experiments, the influence of randomly selected gallery and unlabeled/testing samples can be eliminated by averaging the RecRate of multiple replicates. However, the learned galleries from multiple replicates cannot be averaged for illustration. Therefore, the only (fixed) labeled gallery sample per subject and the similarity between the unlabeled/testing samples in the Multi-PIE database enable to alleviate such influence. In addition, the large difference between the labeled gallery and the unlabeled/testing samples is more suitable to illustrate the effectiveness of the proposed semi-supervised gallery learning method.</p><p>Specifically, the same neutral image from Sect.IV-B1 is used as the only labeled gallery sample per subject. Images from S1 Ca051 R2 are used as the unlabeled/testing images. We randomly choose 1-10 testing images to do the experiments, each trail is used to draw the learned galleries as each subfigure of Fig. <ref type="figure" target="#fig_14">15</ref>.</p><p>Figure <ref type="figure" target="#fig_14">15</ref> shows that with more unlabeled training samples, the gallery samples learned by our proposed S 3 RC method can better represent the unlabeled data (i.e. smile, see the mouth region). In fact, we note that the proposed semisupervised learning method can be regarded as nonlinear pixel-wise/local alignments, e.g. to align the neutral gallery and the smiling unlabeled/testing faces in Fig. <ref type="figure" target="#fig_14">15</ref>, therefore enabling a better representation of the unlabeled/testing data to achieve improved performance over the existing linear and global SRC methods (e.g. SRC, ESRC, SSRC, etc. ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The performance of our method with different alignments</head><p>In the previous experiments, the performance of our method is investigated using face images that are aligned manually by eye centers. Here, we show the results using different  alignments in a fully automatic pipeline. That is, for a given image, we use the Viola-Jones detector <ref type="bibr" target="#b57">[58]</ref> for face detection, Misalignment-Robust Representation (MRR) <ref type="bibr" target="#b58">[59]</ref> for face alignment, and the proposed S 3 RC for recognition. Note that the aim of this section is to prove that i) the better performance of S 3 RC is not affected by different alignments, and ii) S 3 RC can be integrated into a fully automatic pipeline for practical use. MRR is chosen for alignment, because the code is available online <ref type="foot" target="#foot_4">4</ref> . Researchers can also use other alignment techniques (e.g. SILT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, DSRC <ref type="bibr" target="#b1">[2]</ref> by aligning the gallery first, then align the query data to the well aligned gallery; or TIPCA <ref type="bibr" target="#b59">[60]</ref>), but the comparison of different alignment methods is beyond the scope of this paper.</p><p>For MRR alignment, we use the default settings of the demo codes, and only change the input data. That is, the well aligned neutral images of the first 50 subjects of Multi-PIE database (as provided in the MRR demo codes) are used as the gallery to align the whole Multi-PIE database of 337 subjects, λ of MRR is set to 0.05, the number of selected candidates is set to 8 and the output aligned data are cropped into 60 × 48 images. An example of detected and aligned result for a specific subject on the original image is shown in Fig. <ref type="figure" target="#fig_13">14</ref> The classification results using Viola-Jones for detection and MRR for alignment are shown in Table <ref type="table" target="#tab_2">III</ref> for some subsets of Multi-PIE database with transductive experimental settings. The classification parameters are identical with them in Sections 3.2. Table III clearly shows that S 3 RC and S 3 RC-SVDL (i.e. S 3 RC plus the variation dictionary learned by SVDL) achieve the two highest accuracies. By using the same aligned data as input for all the methods, we show that the strong performance of S 3 RC is due to the utilization of the unlabeled information, no matter whether the alignment is obtained manually or by automatic MRR. V. DISCUSSION A semi-supervised sparse representation based classification method is proposed in this paper. By exploiting the unlabeled data, it can well address both linear and non-linear variations between the labeled/training and unlabeled/testing samples. This is particularly useful when the amount of labeled data is limited. Specifically, we first use the gallery plus variation model to estimate the rectified unlabeled samples excluding linear variations. After that, the rectified labeled and unlabeled samples are used to learn a GMM using EM clustering to address the non-linear variations between them. These rectified labeled data is also used as the initial mean of the Gaussians in the EM optimization. Finally, the query samples are classified by SRC with the precise gallery dictionary estimated by EM.</p><p>The proposed method is flexible, in which the gallery dictionary learning method is complementary to existing methods which focus on learning the (linear) variation dictionary, such as ESRC <ref type="bibr" target="#b12">[13]</ref>, SVDL <ref type="bibr" target="#b13">[14]</ref>, SILT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, RADL <ref type="bibr" target="#b16">[17]</ref> etc. For the ESRC, SVDL and RADL methods, we have shown by experiments that the combination of the proposed gallery dictionary learning and ESRC (namely S 3 RC), SVDL (namely S 3 RC-SVDL) and RADL (namely S 3 RC-RADL) achieve significantly improved performance on all the tasks.</p><p>It is also noted by coupling with state-of-the-art DCNN features, our method, as a better classifier, can further improve the recognition performance. This has been verified by using VGG-face <ref type="bibr" target="#b51">[52]</ref> features on LFW database, where our method outperforms the other baselines by 1.98%. While less improvement is observed by feeding DCNN feature to other classifiers, including SRC, CRC, ProCRC and ESRC, when comparing with the basic nearest neighbor classifier.</p><p>Moreover, our method can be combined with SRC methods that incorporate auto-alignment, such as SILT <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, MRR <ref type="bibr" target="#b58">[59]</ref>, and DSRC <ref type="bibr" target="#b1">[2]</ref>. Note that these methods will degrade to SRC after the alignment (except SILT, while by which the learned illumination dictionary can also be utilized in S 3 RC by the same approach as S 3 RC-SVDL). Thus, these methods can be used to align the images first, then S 3 RC can be applied for classification utilizing the unlabeled information. In practical face recognition tasks, our method can be used following on automatic pipeline of face detection (e.g. Viola-Jones detector <ref type="bibr" target="#b57">[58]</ref>), followed by alignment (e.g. one of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>), and then S 3 RC classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a semi-supervised gallery dictionary learning method called S 3 RC, which improves the SRC based face recognition by modeling both linear and non-linear variation between the labeled/training and unlabeled/testing samples, and leveraging the unlabeled data to learn a more precise gallery dictionary. These better characterize the discriminative features of each subject. Through extensive simulations, we can draw the following conclusions: i) S 3 RC can deliver significantly improved results for both the insufficient training samples problem and the SLSPP problems. ii) Our method can be combined with state-of-the-art method that focus on learning the (linear) variation dictionary, so that we can obtain further improved the results (e.g. see ESRC v.s. S 3 RC, SVDL v.s. S 3 RC-SVDL, and RADL v.s. S 3 RC-RADL). iii) The promising performance of S 3 RC is robust to different face alignment methods. A future direction is to use SSL methods other than GMM to better estimate the gallery dictionary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The illustration of the procedures of the proposed semi-supervised gallery dictionary learning method.</figDesc><graphic coords="6,124.33,56.07,359.85,205.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b13">14</ref> Output: Label(y) = arg min k r k (y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The typical performance of the change in negative log-likelihood (from a randomly selected trial on the AR database), which convergence with less than 5 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The cropped image instances of one subject from the AR database (one complete session).</figDesc><graphic coords="8,59.77,56.07,225.95,85.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The results (Recognition Rate) from the AR database. We used the first 300 PCs (dimensional reduction by PCA) and λ is set to 0.005 (as identical of [18]). Each value was obtained from 20 runs. The Left and Right Columns denote experiments with Combined and Separated Session, the Top and Bottom Rows represent the transductive and inductive settings, respectively. Zoom-in figures are provided in subfigs (a), (c), and (d).</figDesc><graphic coords="8,316.51,56.07,238.49,212.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B</head><label></label><figDesc>. The performance on the SLSPP problem using facial image with neutral expression as gallery 1) The Multi-PIE Database : The large-scale Multi-PIE database [40] consists of images of four sessions (dates) with variations of pose, expression, and illumination. For each subject in each session, there are 20 illuminations, with indices from 0 to 19, per pose per expression. In our experiments, all the images are cropped to the size of 100 × 82. Since the data provider did not label the eye centers of each image in advance, we average the 4 labeled points of each eye ball (Points 38, 39, 41, 42 for the left eye and Points 44, 45, 47, 48 for the right eye) as the eye center, then crop them by locating the two eye centers at (19, 28) and (63, 28) of the 100 × 82 images. This experiment is a reproduction of the experiment on the SLSPP problem using the Multi-PIE database in [14]. Specifically, the images with illumination 7 from the first 100 subjects (among all 249 subjects) in Session 1 of the facial image with neutral expression (Session 1, Camera 051, Recording 1. S1 Ca051 R1 for short) are used as gallery. The remaining images under various illuminations of the other 149 subjects in S1 Ca051 R1 are used as the generic training data. For the testing data, we use the images of the first 100 subjects from other subsets of the Multi-PIE database, i.e. the image subsets that are with different illuminations (S2 Ca051 R1, S3 Ca051 R1, S4 Ca051 R1), different illuminations and poses (S1 Ca050 R1, S2 Ca050 R1, S3 Ca050 R1, S4 Ca050 R1), different illuminations and expressions (S1 Ca051 R2, S2 Ca051 R2, S2 Ca051 R3, S3 Ca051 R2), different illuminations, expressions and poses (S1 Ca050 R2, S1 Ca140 R2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The results (Recognition Rate, %) from the Multi-PIE database with controlled single gallery sample per person, Top: RecRate for transductive experiments, Bottom: RecRate for inductive experiments. The bars from Left to Right are: NN, SVM, SRC, CRC, ProCRC, ESRC, SVDL, S 3 RC, S 3 RC-SVDL, and S 3 RC-RADL.</figDesc><graphic coords="10,85.77,306.39,436.96,125.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Illustrations of the learned gallery samples when there is non-linear variation between the (input) labeled gallery and the unlabeled/testing samples. The labeled gallery is controlled facial image with neutral expression under standard illumination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The cropped image instances for one subject from the Normal and the Accessory categories of the CAS-PEAL database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>•</head><label></label><figDesc>Transductive: Here we use all the images from the corresponding session as unlabeled testing data, the results are summarized in top subfigure of Fig. 11. • Inductive: Bottom subfigure of Fig. 11 summarizes the inductive experimental results, where the 20 images of the corresponding session are partitioned into two parts, i.e. half for unlabeled training and the other half for testing. The inductive results are obtained by averaging 20 replicates. The results are shown in Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The results (Recognition Rate, %) from the Multi-PIE database with uncontrolled single gallery sample per person, Top: RecRate for transductive experiments, Bottom: RecRate for inductive experiments. The bars from Left to Right are: NN, SVM, SRC, CRC, ProCRC, ESRC, S 3 RC.</figDesc><graphic coords="12,60.07,56.07,488.36,196.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Illustrations of the learned gallery samples when there is non-linear variation between the (input) labeled gallery and the unlabeled/testing samples. The labeled gallery is uncontrolled (i.e. the squint image).</figDesc><graphic coords="12,85.77,294.98,436.95,142.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The analysis of the impact of different amounts of unlabeled data from 1-13 on AR database. Three different amounts of labeled data are chosen for analysis, i.e. 2, 4 and 6 labeled samples per subject, as shown in Left, Middle, Right subfigures, respectively. The results are obtained by averaging 20 runs, number of PCs is 300 (dimensional reduction by PCA) and λ is 0.005.</figDesc><graphic coords="13,48.96,601.27,251.05,69.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The examples of detected and aligned faces. (a) An example of detected faces (by Viola-Jones detector, green dash box) and aligned faces (by MRR, red solid box) on the original image. (b) The cropped detected faces. (c) The cropped aligned faces.</figDesc><graphic coords="14,59.77,228.73,225.94,133.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The illustrations of the learned gallery with different amount (1-10 per subject) of unlabeled training samples. The reconstructed learned galleries are illustrated in the Red Box, with 1-10 unlabeled samples per subject from Left to Right, then Top to Bottom. Better representation can be observed with more unlabeled training samples (i.e. smile, see the mouth region).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a). Other detected and aligned face examples are shown in Figs. 14(b) and 14(c), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>RESULTS (RECOGNITION RATE, %) FROM THE CAS-PEAL DATABASE ON THE Normal AND Accessory SUBSETS, FOR BOTH transductive AND inductive EXPERIMENTS. IN THE BRACKETS, WE SHOW THE IMPROVEMENT OF S 3 RC w.r.t ESRC, S 3 RC-SVDL w.r.t SVDL AND S 3 RC-RADL w.r.t RADL, SINCE THE SAME VARIATION DICTIONARY HAS BEEN USED IN THESE THREE PAIRS. WE USED THE FIRST 100 PCS (DIMENSIONAL REDUCTION BY PCA) AND λ IS SET TO 0.001.</figDesc><table><row><cell>Method</cell><cell>Transductive</cell><cell>Inductive</cell></row><row><cell>NN</cell><cell>41.00</cell><cell>41.08</cell></row><row><cell>SVM</cell><cell>41.00</cell><cell>41.08</cell></row><row><cell>SRC</cell><cell>57.22</cell><cell>56.70</cell></row><row><cell>CRC</cell><cell>54.56</cell><cell>53.84</cell></row><row><cell>ProCRC</cell><cell>54.89</cell><cell>54.67</cell></row><row><cell>ESRC</cell><cell>71.78</cell><cell>71.76</cell></row><row><cell>SVDL</cell><cell>69.67</cell><cell>69.74</cell></row><row><cell>RADL</cell><cell>75.27</cell><cell>73.78</cell></row><row><cell>S 3 RC</cell><cell cols="2">75.39 (↑3.61) 74.66 (↑2.90)</cell></row><row><cell>S 3 RC-SVDL</cell><cell cols="2">72.06 (↑2.39) 71.79 (↑2.05)</cell></row><row><cell>S 3 RC-RADL</cell><cell>75.89 (↑0.62)</cell><cell>72.56 (↓1.23)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>RESULTS (RECOGNITION RATE, %) FROM THE LFW DATABASE, FOR BOTH transductive AND inductive EXPERIMENTS WITH SIMPLE PCA FEATURES AND DEEP LEARNED FEATURE BY [52]. IN THE BRACKETS, WE SHOW THE IMPROVEMENT OF S 3 RC w.r.t ESRC. WE USED THE FIRST 100 PCS (DIMENSIONAL REDUCTION BY PCA) AND THE DEEP LEARNED FEATURES BY [52] IS OF 4096 DIMENSIONS. λ IS SET TO 0.001.</figDesc><table><row><cell>Method</cell><cell cols="2">PCA fea. (100)</cell><cell cols="2">DCNN fea. by [52] (4096)</cell></row><row><cell></cell><cell>Transductive</cell><cell>Inductive</cell><cell>Transductive</cell><cell>Inductive</cell></row><row><cell>NN</cell><cell>5.57</cell><cell>5.82</cell><cell>89.28</cell><cell>90.19</cell></row><row><cell>SVM</cell><cell>5.37</cell><cell>5.82</cell><cell>89.28</cell><cell>90.19</cell></row><row><cell>SRC</cell><cell>10.92</cell><cell>11.13</cell><cell>89.50</cell><cell>90.23</cell></row><row><cell>CRC</cell><cell>10.47</cell><cell>10.69</cell><cell>89.18</cell><cell>89.86</cell></row><row><cell>ProCRC</cell><cell>10.77</cell><cell>10.99</cell><cell>90.85</cell><cell>90.13</cell></row><row><cell>ESRC</cell><cell>15.51</cell><cell>16.23</cell><cell>90.58</cell><cell>90.73</cell></row><row><cell>S 3 RC</cell><cell>17.99 (↑2.48)</cell><cell>17.90 (↑1.67)</cell><cell>92.55 (↑1.98)</cell><cell>92.57 (↑1.84)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>RESULTS (RECOGNITION RATE, %) FOR THE MULTI-PIE DATABASE USING VIOLA-JONES FOR DETECTION AND MRR FOR ALIGNMENT. IN THE BRACKETS, WE SHOW THE IMPROVEMENT OF S 3 RC w.r.t ESRC AND S 3 RC-SVDL w.r.t SVDL, SINCE THE SAME VARIATION DICTIONARY HAS BEEN USED IN BOTH PAIRS. WE USED THE FIRST 90 PCS (DIMENSIONAL REDUCTION BY PCA) AND λ IS SET TO 0.001.</figDesc><table><row><cell>Method</cell><cell>S2 Ca051 R1</cell><cell>S3 Ca051 R1</cell><cell>S4 Ca051 R1</cell></row><row><cell>SRC</cell><cell>55.75</cell><cell>51.47</cell><cell>53.64</cell></row><row><cell>ESRC</cell><cell>86.78</cell><cell>85.07</cell><cell>86.17</cell></row><row><cell>SVDL</cell><cell>91.78</cell><cell>89.30</cell><cell>89.68</cell></row><row><cell>S 3 RC</cell><cell>95.75 (↑8.97)</cell><cell>94.58 (↑9.51)</cell><cell>93.96 (↑7.79)</cell></row><row><cell>S 3 RC-SVDL</cell><cell>97.74 (↑5.96)</cell><cell>95.28 (↑5.98)</cell><cell>95.32 (↑5.64)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The data can be downloaded at http://cbcsl.ece.ohio-state.edu/ protected-dir/AR warp zip.zip upon authorization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Note that the DLRD SR<ref type="bibr" target="#b20">[21]</ref> and D 2 L 2 R 2<ref type="bibr" target="#b21">[22]</ref>,<ref type="bibr" target="#b22">[23]</ref> methods, which we compared in the insufficient training samples problem, are less suitable for comparison here due to the SLSPP problem. It is because in order to learn a low-rank (sub-)dictionary for each subject, both of them assume low-rank property of the gallery dictionary, which requires multiple gallery samples per subject. SSRC also requires multiple gallery samples per subject to learn the gallery dictionary and thus is less suitable for comparison either.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Note that the SVDL and RADL method is less suitable to the SLSPP problem with uncontrolled image as gallery. It is because the variation dictionary learning of SVDL or RADL requires reference images for all the subjects in the generic training data, where the reference images should have the same type of variation as the gallery. However, such information cannot be inferred due to the uncontrolled gallery images used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>MRR codes can be downloaded at http://www4.comp.polyu.edu.hk/ ∼ cslzhang/code/MRR eccv12.zip.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2675341, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. XX, XXXX</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors would like to thank Weichao Qiu for giving feedbacks on the manuscript. This work was supported by the National Natural Science Foundation of China (No. 61503288, 61601112) and the NSF award CCF-1317376.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward a practical face recognition system: Robust alignment and illumination by sparse representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast l1-minimization algorithms and an application in robust face recognition: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1849" to="1852" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust sparse coding for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-rigid visible and infrared face registration via regularized gaussian fields criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="772" to="784" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition from a single image per person: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1725" to="1745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Is face recognition really a compressive sensing problem</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: Which helps face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale patch based collaborative representation for face recognition with margin distribution optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Shiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face image superresolution through locality-induced support regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="168" to="183" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Srlsp: A face image super-resolution algorithm using smooth regression with local structure prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extended SRC: Undersampled face recognition via intraclass variant dictionary</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1864" to="1870" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse variation dictionary learning for face recognition with a single training sample per person</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Singlesample face recognition with image corruption and misalignment via sparse illumination transfer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse illumination learning and transfer for single-sample face recognition with image corruption and misalignment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Undersampled face recognition via robust auxiliary dictionary learning</title>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1722" to="1734" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">In defense of sparsity based face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fisher discrimination dictionary learning for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse representation based fisher discrimination dictionary learning for image classification</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse representation for face recognition based on discriminative low-rank dictionary learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative dictionary learning with lowrank regularization for face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning low-rank and discriminative dictionary for image classification</title>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="814" to="823" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning structured low-rank representations for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Connecting the dots in multi-class classification: From nearest subspace to collaborative representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3602" to="3609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neither global nor local: Regularized patch-based representation for single sample per person face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by sparse representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonnegative sparse coding for discriminative semi-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2849" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-negative low rank and sparse graph for semi-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Constructing a non-negative low-rank and sparse graph with dataadaptive features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian face recognition based on gaussian mixture models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Morgan &amp; Claypool</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust feature matching for remote sensing image registration via locally linear transforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6469" to="6481" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A robust and outlieradaptive method for non-rigid point registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="388" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3519" to="3532" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-rigid point set registration by preserving global and local structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive discriminant learning for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2497" to="2509" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive generic learning for face recognition from a single sample per person</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The AR face database</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVC, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-PIE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The cas-peal large-scale chinese face database and baseline evaluations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="161" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="page" from="7" to="49" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic updating for l1 minimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of selected topics in signal processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fast solution of l1-norm minimization problems when the solution may be sparse</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsaig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4789" to="4812" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A new approach to variable selection in least squares problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Presnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turlach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA journal of numerical analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A probabilistic collaborative representation based approach for pattern classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Libsvm : a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Symmetry non-rigid structure from motion for category-specific object structure estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Exploiting symmetry and/or Manhattan properties for 3D object structure estimation from single and multiple images</title>
		<idno type="arXiv">arXiv:1607.07129</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision</title>
		<meeting>the British Machine Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2892" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to align from scratch</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient misalignment-robust representation for real-time face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="850" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transform-invariant pca: A unified approach to fully automatic face alignment, representation, and recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1275" to="1284" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Currently, he is a computer vision researcher in Tencent AI Lab. He was a visiting student with the Department of Statistics</title>
	</analytic>
	<monogr>
		<title level="m">Jiayi Ma received the B.S. degree from the Department of Mathematics, and the Ph.D. Degree from the School of Automation</title>
		<title level="s">Yuan Gao received the B.S. degree in biomedical engineering and the M.S. degree in pattern recognition and intelligent systems from the Huazhong University of Science and Technology</title>
		<meeting><address><addrLine>Wuhan, China; Kowloon, Hong Kong; University of California, Los Angeles; Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2009 and 2012. 2016. 2015. 2008 and 2014</date>
		</imprint>
		<respStmt>
			<orgName>City University of Hong Kong ; Huazhong University of Science and Technology ; University of California, Los Angeles</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer vision, pattern recognition, and machine learning. respectively. From 2012 to 2013, he was an exchange student with the Department of Statistics. He is now an</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
