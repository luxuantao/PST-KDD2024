<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sharing Clusters Among Related Groups: Hierarchical Dirichlet Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yee</forename><surname>Whye Teh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Statistics</orgName>
								<orgName type="institution">University of California at Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
							<email>beal@cs.toronto.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
							<email>blei@cs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sharing Clusters Among Related Groups: Hierarchical Dirichlet Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6EFD532349978C8AA0278B12F90899EF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most significant conceptual and practical tools in the Bayesian paradigm is the notion of a hierarchical model. Building on the notion that a parameter is a random variable, hierarchical models have applications to a variety of forms of grouped or relational data and to general problems involving "multi-task learning" or "learning to learn." A simple and classical example is the Gaussian means problem, in which a grand mean µ 0 is drawn from some distribution, a set of K means are then drawn independently from a Gaussian with mean µ 0 , and data are subsequently drawn independently from K Gaussian distributions with these means. The posterior distribution based on these data couples the means, such that posterior estimates of the means are shrunk towards each other. The estimates "share statistical strength," a notion that can be made precise within both the Bayesian and the frequentist paradigms.</p><p>Here we consider the application of hierarchical Bayesian ideas to a problem in "multi-task learning" in which the "tasks" are clustering problems, and our goal is to share clusters among multiple, related clustering problems. We are motivated by the task of discovering topics in document corpora <ref type="bibr" target="#b0">[1]</ref>. A topic (i.e., a cluster) is a distribution across words while documents are viewed as distributions across topics. We want to discover topics that are common across multiple documents in the same corpus, as well as across multiple corpora.</p><p>Our work is based on a tool from nonparametric Bayesian analysis known as the Dirichlet process (DP) mixture model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Skirting technical definitions for now, "nonparametric" can be understood simply as implying that the number of clusters is open-ended. Indeed, at each step of generating data points, a DP mixture model can either assign the data point to a previously-generated cluster or can start a new cluster. The number of clusters is a random variable whose mean grows at rate logarithmic in the number of data points.</p><p>Extending the DP mixture model framework to the setting of multiple related clustering problems, we will be able to make the (realistic) assumption that we do not know the number of clusters a priori in any of the problems, nor do we know how clusters should be shared among the problems.</p><p>When generating a new cluster, a DP mixture model selects the parameters for the cluster (e.g., in the case of Gaussian mixtures, the mean and covariance matrix) from a distribution G 0 -the base distribution. So as to allow any possible parameter value, the distribution G 0 is often assumed to be a smooth distribution (i.e., non-atomic). Unfortunately, if we now wish to extend DP mixtures to groups of clustering problems, the assumption that G 0 is smooth conflicts with the goal of sharing clusters among groups. That is, even if each group shares the same underlying base distribution G 0 , the smoothness of G 0 implies that they will generate distinct cluster parameters (with probability one). We will show that this problem can be resolved by taking a hierarchical Bayesian approach. We present a notion of a hierarchical Dirichlet process (HDP) in which the base distribution G 0 for a set of DPs is itself a draw from a DP. This turns out to provide an elegant and simple solution to the problem of sharing clusters among multiple clustering problems.</p><p>The paper is organized as follows. In Section 2, we provide the basic technical definition of DPs and discuss related representations involving stick-breaking processes and Chinese restaurant processes. Section 3 then introduces the HDP, motivated by the requirement of a more powerful formalism for the grouped data setting. As for the DP, we present analogous stick-breaking and Chinese restaurant representations for the HDP. We present empirical results on a number of text corpora in Section 5, demonstrating various aspects of the HDP including its nonparametric nature, hierarchical nature, and the ease with which the framework can be applied to other realms such as hidden Markov models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dirichlet Processes</head><p>The Dirichlet process (DP) and the DP mixture model are mainstays of nonparametric Bayesian statistics (see, e.g., <ref type="bibr" target="#b2">[3]</ref>). They have also begun to be seen in applications in machine learning (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>). In this section we give a brief overview with an eye towards generalization to HDPs. We begin with the definition of DPs <ref type="bibr" target="#b3">[4]</ref>. Let (Θ, B) be a measurable space, with G 0 a probability measure on the space, and let α 0 be a positive real number. A Dirichlet process is the distribution of a random probability measure G over (Θ, B) such that, for any finite partition (A 1 , . . . , A r ) of Θ, the random vector (G(A 1 ), . . . , G(A r )) is distributed as a finite-dimensional Dirichlet distribution:</p><formula xml:id="formula_0">(G(A 1 ), . . . , G(A r )) ∼ Dir α 0 G 0 (A 1 ), . . . , α 0 G 0 (A r ) .<label>(1)</label></formula><p>We write G ∼ DP(α 0 , G 0 ) if G is a random probability measure distributed according to a DP. We call G 0 the base measure of G, and α 0 the concentration parameter.</p><p>The DP can be used in the mixture model setting in the following way. Consider a set of data, x = (x 1 , . . . , x n ), assumed exchangeable. Given a draw G ∼ DP(α 0 , G 0 ), independently draw n latent factors from G: φ i ∼ G. Then, for each i = 1, . . . , n, draw x i ∼ F (φ i ), for a distribution F . This setup is referred to as a DP mixture model.</p><p>If the factors φ i were all distinct, then this setup would yield an (uninteresting) mixture model with n components. In fact, the DP exhibits an important clustering property, such that the draws φ i are generally not distinct. Rather, the number of distinct values grows as O(log n), and it is this that defines the random number of mixture components.</p><p>There are several perspectives on the DP that help to understand this clustering property.</p><p>In this paper we will refer to two: the Chinese restaurant process (CRP), and the stickbreaking process. The CRP is a distribution on partitions that directly captures the clustering of draws from a DP via a metaphor in which customers share tables in a Chinese restaurant <ref type="bibr" target="#b4">[5]</ref>. As we will see in Section 4, the CRP refers to properties of the joint distribution of the factors {φ i }. The stick-breaking process, on the other hand, refers to properties of G, and directly reveals its discrete nature <ref type="bibr" target="#b5">[6]</ref>. For k = 1, 2 . . ., let:</p><formula xml:id="formula_1">θ k ∼ G 0 β k ∼ Beta(1, α 0 ) β k = β k k-1 l=1 (1 -β k ).<label>(2)</label></formula><p>Then with probability one the random measure defined by G = ∞ k=1 β k δ θ k is a sample from DP(α 0 , G 0 ). The construction for β 1 , β 2 , . . . in (2) can be understood as taking a stick of unit length, and repeatedly breaking off segments of length β k . The stick-breaking construction shows that DP mixture models can be viewed as mixture models with a countably infinite number of components. To see this, identify each θ k as the parameter of the k th mixture component, with mixing proportion given by β k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Dirichlet Processes</head><p>We will introduce the hierarchical Dirichlet process (HDP) in this section. First we describe the general setting in which the HDP is most useful-that of grouped data. We assume that we have J groups of data, each consisting of n j data points (x j1 , . . . , x jnj ). We assume that the data points in each group are exchangeable, and are to be modeled with a mixture model. While each mixture model has mixing proportions specific to the group, we require that the different groups share the same set of mixture components. The idea is that while different groups have different characteristics given by a different combination of mixing proportions, using the same set of mixture components allows statistical strength to be shared across groups, and allows generalization to new groups.</p><p>The HDP is a nonparametric prior which allows the mixture models to share components. It is a distribution over a set of random probability measures over (Θ, B): one probability measure G j for each group j, and a global probability measure G 0 . The global measure G 0 is distributed as DP(γ, H), with H the base measure and γ the concentration parameter, while each G j is conditionally independent given G 0 , with distribution G j ∼ DP(α 0 , G 0 ). To complete the description of the HDP mixture model, we associate each x ji with a factor φ ji , with distributions given by F (φ ji ) and G j respectively. The overall model is given in Figure <ref type="figure">1</ref> left, with conditional distributions:</p><formula xml:id="formula_2">G 0 | γ, H ∼ DP(γ, H) G j | α, G 0 ∼ DP(α 0 , G 0 ) (3) φ ji | G j ∼ G j x ji | φ ji ∼ F (φ ji ) .<label>(4)</label></formula><p>The stick-breaking construction <ref type="bibr" target="#b1">(2)</ref> shows that a draw of G 0 can be expressed as a weighted sum of point masses:</p><formula xml:id="formula_3">G 0 = ∞ k=1 β k δ θ k .</formula><p>This fact that G 0 is atomic plays an important role in ensuring that mixture components are shared across different groups. Since G 0 is the base distribution for the individual G j 's, (2) again shows that the atoms of the individual G j are samples from G 0 . In particular, since G 0 places non-zero mass only on the atoms θ = (θ k ) ∞ k=1 , the atoms of G j must also come from θ, hence we may write:</p><formula xml:id="formula_4">G 0 = ∞ k=1 β k δ θ k G j = ∞ k=1 π jk δ θ k .<label>(5)</label></formula><p>Identifying θ k as the parameters of the k th mixture component, we see that each submodel corresponding to distinct groups share the same set of mixture components, but have differing mixing proportions, π j = (π jk ) ∞ k=1 . Finally, it is useful to explicitly describe the relationships between the mixing proportions β and (π j ) J j=1 . Details are provided in <ref type="bibr" target="#b9">[10]</ref>. Note that the weights π j are conditionally independent given β since each G j is independent given G 0 . Applying (1) to finite partitions of θ, we get π j ∼ DP(α 0 , β), where we interpret β and π j as probability measures over the positive integers. Hence β is simply the putative mixing proportion over the groups. We may in fact obtain an explicit stick-breaking construction for the π j 's as well. Applying (1) to partitions ({1, . . . , k -1}, {k}, {k + 1, . . .}) of positive integers, we have:</p><formula xml:id="formula_5">G 2 G 1 G 3 x 1 φ 1i n 1i x2i n2 φ 2i x n3 3i 3i φ H γ 0 α 0 α 0 α G 0</formula><formula xml:id="formula_6">π jk ∼ Beta α 0 β k , α 0 1 - k l=1 β l π jk = π jk k-1 l=1 (1 -π jl ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Chinese Restaurant Franchise</head><p>We describe an alternative view of the HDP based directly upon the distribution a HDP induces on the samples φ ji , where we marginalize out G 0 and G j 's. This view directly leads to an efficient Gibbs sampler for HDP mixture models, which is detailed in the appendix. Consider, for one group j, the distribution of φ j1 , . . . , φ jnj as we marginalize out G j . Recall that since G j ∼ DP(α 0 , G 0 ) we can describe this distribution by describing how to generate φ j1 , . . . , φ jnj using the CRP. Imagine n j customers (each corresponds to a φ ji ) at a Chinese restaurant with an unbounded number of tables. The first customer sits at the first table. A subsequent customer sits at an occupied table with probability proportional to the number of customers already there, or at the next unoccupied table with probability proportional to α 0 . Suppose customer i sat at table t ji . The conditional distributions are:</p><formula xml:id="formula_7">t ji | t j1 , . . . , t ji-1 , α 0 ∼ t njt P t n jt +α0 δ t + α0 P t n jt +α0 δ t new ,<label>(7)</label></formula><p>where n jt is the number of customers currently at table t. Once all customers have sat down the seating plan corresponds to a partition of φ j1 , . . . , φ jnj . This is an exchangeable process in that the probability of a partition does not depend on the order in which customers sit down. Now we associate with table t a draw ψ jt from G 0 , and assign φ ji = ψ jtji .</p><p>Performing this process independently for each group j, we have now integrated out all the G j 's, and have an assignment of each φ ji to a sample ψ jtji from G 0 , with the partition structures given by CRPs. Notice now that all ψ jt 's are simply i.i.d. draws from G 0 , which is again distributed according to DP(γ, H), so we may apply the same CRP partitioning process to the ψ jt 's. Let the customer associated with ψ jt sit at table k jt . We have: Finally we associate with table k a draw θ k from H and assign ψ jt = θ kjt . This completes the generative process for the φ ji 's, where we marginalize out G 0 and G j 's. We call this generative process the Chinese restaurant franchise (CRF). The metaphor is as follows: we have J restaurants, each with n j customers (φ ji 's), who sit at tables (ψ jt 's). Now each table is served a dish (θ k 's) from a menu common to all restaurants. The customers are sociable, prefering large tables with many customers present, and also prefer popular dishes.</p><formula xml:id="formula_8">k jt | k 11 , . . . , k 1n1 , k 21 , . . . , k jt-1 , γ ∼ k m k P k m jk +γ δ k + γ P k m k +α0 δ k new . (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We describe 3 experiments in this section to highlight the various aspects of the HDP: its nonparametric nature; its hierarchical nature; and the ease with which we can apply the framework to other models, specifically the HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nematode biology abstracts.</head><p>To demonstrate the strength of the nonparametric approach as exemplified by the HDP mixture, we compared it against latent Dirichlet allocation (LDA), which is a parametric model similar in structure to the HDP <ref type="bibr" target="#b0">[1]</ref>. In particular, we applied both models to a corpus of nematode biology abstracts 1 , evaluating the perplexity of both models on held out abstracts. Here abstracts correspond to groups, words correspond to observations, and topics correspond to mixture components, and exchangeability correspond to the typical bag-of-words assumption. In order to study specifically the nonparametric nature of the HDP, we used the same experimental setup for both models 2 , except that in LDA we had to vary the number of topics used between 10 and 120, while the HDP obtained posterior samples over this automatically.</p><p>The results are shown in Figure <ref type="figure" target="#fig_0">2</ref>. LDA performs best using between 50 and 80 topics, while the HDP performed just as well as these. Further, the posterior over the number of topics used by HDP is consistent with this range. Notice however that the HDP infers the number of topics automatically, while LDA requires some method of model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIPS sections.</head><p>We applied HDP mixture models to a dataset of NIPS 1-12 papers organized into sections 3 . To highlight the transfer of learning achievable with the HDP, we 1 Available at http://elegans.swmed.edu/wli/cgcbib. There are 5838 abstracts in total. After removing standard stop words and words appearing less than 10 times, we are left with 476441 words in total and a vocabulary size of 5699. 2 In both models, we used a symmetric Dirichlet distribution with weights of 0.5 for the prior H over topic distributions, while the concentration parameters are integrated out using a vague gamma prior. Gibbs sampling using the CRF is used, while the concentration parameters are sampled using a method described in <ref type="bibr" target="#b9">[10]</ref>. This also applies to the NIPS sections experiment on next page. 3 To ensure we are dealing with informative words in the documents, we culled stop words as well show improvements to the modeling of a section when the model is also given documents from another section. Our test section is always the VS (vision sciences) section, while the additional section is varied across the other eight. The training set always consist of 80 documents from the other section (so that larger sections like AA (algorithms and architecures) do not get an unfair advantage), plus between 0 and 80 documents from VS. There are 47 test documents, which are held fixed as we vary over the other section and the number N of training VS documents. We compared 3 different models for this task. The first model (M1) simply ignores documents from the additional section, and uses a HDP to model the VS documents. It serves as a baseline. The second model (M2) uses a HDP mixture model, with one group per document, but lumping together training documents from both sections. The third model (M3) takes a hierarchical approach and models each section separately using a HDP mixture model, and places another DP prior over the common base distributions for both submodels 4 .</p><p>As we see in Figure <ref type="figure" target="#fig_1">3</ref> left, the more hierarchical approach of M3 performs best, with perplexity decreasing drastically with modest values of N , while M1 does worst for small N . However with increasing N , M1 improves until it is competitive with M3 but M2 does worst. This is because M2 lumps all the documents together, so is not able to differentiate between the sections, as a result the influence of documents from the other section is unduly strong. This result confirms that the hierarchical approach to the transfer-of-learning problem is a useful one, as it allows useful information to be transfered to a new task (here the modeling of a new section), without the data from the previous tasks overwhelming those in the new task.</p><p>We also looked at the performance of the M3 model on VS documents given specific other sections. This is shown in Figure <ref type="figure" target="#fig_1">3</ref> right. As expected, the performance is worst given LT (learning theory), and improves as we move to AA and AP (applications). In Table <ref type="table" target="#tab_1">1</ref> we show the topics pertinent to VS discovered by the M3 model. First we trained the model on all documents from the other section. Then, keeping the assignments of words to topics fixed in the other section, we introduced VS documents and the model decides to reuse some topics from the other section, as well as create new ones. The topics reused by VS documents confirm to our expectations of the overlap between VS and other sections.</p><p>as words occurring more than 4000 or less than 50 times in the documents. As sections differ over the years, we assigned by hand the various sections to one of 9 prototypical sections: CS, NS, LT, AA, IM, SP, VS, AP and CN. 4 Though we have only described the 2 layer HDP the 3 layer extension is straightforward. In fact on our website http://www.cs.berkeley.edu/˜ywteh/research/npbayes we have an implementation of the general case where DPs are coupled hierarchically in a tree-structured model.  <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b9">[10]</ref> we show that the HDP framework can be applied to obtain a cleaner formulation of the iHMM, providing effective new inference algorithms and potentially hierarchical extensions. In fact the original iHMM paper <ref type="bibr" target="#b10">[11]</ref> served as inspiration for this work and first coined the term "hierarchical Dirichlet processes"-though their model is not hierarchical in the Bayesian sense, involving priors upon priors, but is rather a set of coupled urn models similar to the CRF. Here we report experimental comparisons of the iHMM against other approaches on sentences taken from Lewis Carroll's Alice's Adventures in Wonderland. [12] models with numbers of states ranging from 1 to 30 were trained multiple times on 20 sentences of average length 51 symbols (27 distinct symbols, consisting of 26 letters and ' '), and tested on 40 sequences of average length 100. Figure <ref type="figure" target="#fig_2">4</ref> shows the perplexity of test sentences. For VB, the predictive probability is intractable to compute, so the modal setting of parameters was used. Both MAP and VB models were given optimal settings of the hyperparameters found in the iHMM. We see that the iHMM has a lower perlexity than every model size for ML, MAP, and VB, and obtains this with one countably infinite model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML, MAP, and variational</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have described the hierarchical Dirichlet process, a hierarchical, nonparametric model for clustering problems involving multiple groups of data. HDP mixture models are able to automatically determine the appropriate number of mixture components needed, and exhibit sharing of statistical strength across groups by having components shared across groups. We have described the HDP as a distribution over distributions, using both the stick-breaking construction and the Chinese restaurant franchise. In <ref type="bibr" target="#b9">[10]</ref> we also describe a fourth perspective based on the infinite limit of finite mixture models, and give detail for how the HDP can be applied to the iHMM. Direct extensions of the model include use of nonparametric priors other than the DP, building higher level hierarchies as in our NIPS experiment, as well as hierarchical extensions to the iHMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Gibbs Sampling in the CRF</head><p>The CRF is defined by the variables t = (t ji ), k = (k jt ), and θ = (θ k ). We describe an inference procedure for the HDP mixture model based on Gibbs sampling t, k and θ given data items x. For the full derivation see <ref type="bibr" target="#b9">[10]</ref>. Let f (•|θ) and h be the density functions for F (θ) and H respectively, n -i jt be the number of t ji 's equal to t except t ji , and m -jt k be the number of k j t 's equal to k except k jt . The conditional probability for t ji given the other variables is proportional to the product of a prior and likelihood term. The prior term is given by <ref type="bibr" target="#b6">(7)</ref> where, by exchangeability, we can take t ji to be the last one assigned. The likelihood is given by f (x ji |θ kjt ) where for t = t new we may sample k jt new using <ref type="bibr" target="#b7">(8)</ref>, and θ k new ∼ H. The distribution is then:</p><formula xml:id="formula_9">p(t ji = t | t\t ji , k, θ, x) ∝ α 0 f (x ji |θ kjt ) if t = t new n -i jt f (x ji |θ kjt ) if t currently used.<label>(9)</label></formula><p>Similarly the conditional distributions for k jt and θ k are:</p><formula xml:id="formula_10">p(k jt = k | t, k\k jt , θ, x) ∝ γ i:tji =t f (x ji |θ k ) if k = k new m -t k i:tji=t f (x ji |θ k ) if k currently used. (10) p(θ k | t, k, θ\θ k , x) ∝ h(θ k ) ji:kjt ji =k f (x ji |θ k )<label>(11)</label></formula><p>where θ k new ∼ H. If H is conjugate to F (•) we have the option of integrating out θ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: comparison of LDA and HDP mixture. Results are averaged over 10 runs, with error bars being 1 standard error. Right: histogram of the number of topics the HDP mixture used over 100 posterior samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: perplexity of test VS documents given training documents from VS and another section for 3 different models. Curves shown are averaged over the other sections and 5 runs. Right: perplexity of test VS documents given LT, AA and AP documents respectively, using M3, averaged over 5 runs. In both, the error bars are 1 standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing iHMM (horizontal line) versus ML, MAP and VB trained HMMs. Error bars are 1 standard error (those for iHMM too small to see).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Left: graphical model of an example HDP mixture model with 3 groups. Corresponding to each DP node we also plot a sample draw from the DP using the stick-breaking construction. Right: an instantiation of the CRF representation for the 3 group HDP. Each of the 3 restaurants has customers sitting around tables, and each table is served a dish (which corresponds to customers in the Chinese restaurant for the global DP).</figDesc><table><row><cell></cell><cell cols="2">global</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ψ 13 ψ 11</cell><cell>ψ 31 ψ 22 1 θ</cell><cell cols="2">ψ 23 2 θ</cell><cell>ψ 32</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ψ 21 3 θ ψ 24 ψ 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>φ 13 φ 14 φ 12 φ 15 φ 16 11 φ ψ 11 φ 18 φ 17 13 ψ ψ 12</cell><cell>φ 22 φ 21 φ 27 ψ 21 φ 28 ψ 24</cell><cell>φ 25 φ ψ 22 φ 23 24</cell><cell cols="2">ψ 23 φ 26</cell><cell>φ 31 ψ 31</cell><cell>32 φ φ 35 φ 36</cell><cell>φ 34 φ 33</cell><cell>ψ 32</cell></row><row><cell>group j=1</cell><cell cols="2">group j=2</cell><cell></cell><cell></cell><cell cols="3">group j=3</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Topics shared between VS and the other sections. Shown are the two topics with most numbers of VS words, but also with significant numbers of words from the other section. The infinite hidden Markov model (iHMM) is a nonparametric model for sequential data where the number of hidden states is open-ended and inferred from data</figDesc><table><row><cell></cell><cell>NS</cell><cell>LT</cell><cell>AA</cell><cell>IM</cell><cell>SP</cell><cell>AP</cell><cell>CN</cell></row><row><cell>task</cell><cell>cells cell</cell><cell>signal layer</cell><cell>algorithms test</cell><cell>processing</cell><cell>visual images</cell><cell>approach</cell><cell>ii tree pomdp</cell></row><row><cell>representation</cell><cell>activity</cell><cell>gaussian cells</cell><cell>approach</cell><cell>pattern</cell><cell>video language</cell><cell>based trained</cell><cell>observable</cell></row><row><cell>pattern</cell><cell>response</cell><cell>fig nonlinearity</cell><cell>methods based</cell><cell>approach</cell><cell>image pixel</cell><cell>test layer</cell><cell>strategy class</cell></row><row><cell>processing</cell><cell>neuron visual</cell><cell>nonlinear rate</cell><cell>point problems</cell><cell>architecture</cell><cell>acoustic delta</cell><cell>features table</cell><cell>stochastic</cell></row><row><cell>trained</cell><cell>patterns</cell><cell>eq cell</cell><cell>form large</cell><cell>single shows</cell><cell>lowpass flow</cell><cell>classification</cell><cell>history</cell></row><row><cell>representations</cell><cell>pattern single</cell><cell></cell><cell>paper</cell><cell>simple based</cell><cell></cell><cell>rate paper</cell><cell>strategies</cell></row><row><cell>three process</cell><cell>fig</cell><cell></cell><cell></cell><cell>large control</cell><cell></cell><cell></cell><cell>density</cell></row><row><cell>unit patterns</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>examples</cell><cell>visual cells</cell><cell>large examples</cell><cell>distance</cell><cell>motion visual</cell><cell>signals</cell><cell>image images</cell><cell>policy optimal</cell></row><row><cell>concept</cell><cell>cortical</cell><cell>form point see</cell><cell>tangent image</cell><cell>velocity flow</cell><cell>separation</cell><cell>face similarity</cell><cell>reinforcement</cell></row><row><cell>similarity</cell><cell>orientation</cell><cell>parameter</cell><cell>images</cell><cell>target chip eye</cell><cell>signal sources</cell><cell>pixel visual</cell><cell>control action</cell></row><row><cell>bayesian</cell><cell>receptive</cell><cell>consider</cell><cell>transformation</cell><cell>smooth</cell><cell>source matrix</cell><cell>database</cell><cell>states actions</cell></row><row><cell>hypotheses</cell><cell>contrast spatial</cell><cell>random small</cell><cell>transformations</cell><cell>direction optical</cell><cell>blind mixing</cell><cell>matching facial</cell><cell>step problems</cell></row><row><cell>generalization</cell><cell>cortex stimulus</cell><cell>optimal</cell><cell>pattern vectors</cell><cell></cell><cell>gradient eq</cell><cell>examples</cell><cell>goal</cell></row><row><cell>numbers</cell><cell>tuning</cell><cell></cell><cell>convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>positive classes</cell><cell></cell><cell></cell><cell>simard</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hypothesis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Alice in Wonderland.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating mixture of Dirichlet process models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Maceachern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="223" to="238" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exchangeability and related topics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">École d&apos;été de probabilités de Saint-Flour XIII-1983</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="1" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A constructive definition of Dirichlet priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The infinite Gaussian mixture model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested Chinese restaurant process</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>653</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The infinite hidden Markov model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Variational Algorithms for Approximate Bayesian Inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Gatsby Unit, University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
