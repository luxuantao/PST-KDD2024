<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal learning for facial expression recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Youmei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shijie</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal learning for facial expression recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4C4F4AB01839A244384D8765B80A7D1</idno>
					<idno type="DOI">10.1016/j.patcog.2015.04.012</idno>
					<note type="submission">Received date: 3 October 2014 Revised date: 12 February 2015 Accepted date: 10 April 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Multimodal learning</term>
					<term>Facial expression recognition</term>
					<term>Texture</term>
					<term>Landmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, multimodal learning for facial expression recognition (FER) is proposed. The multimodal learning method makes the first attempt to learn the joint representation by considering the texture and landmark modality of facial images, which are complementary with each other. In order to learn the representation of each modality and the correlation and interaction between different modalities, the structured regularization (SR) is employed to enforce and learn the modality-specific sparsity and density of each modality, respectively. By introducing SR, the comprehensiveness of the facial expression is fully taken into consideration, which can not only handle the subtle expression but also perform robustly to different input of facial images. With the proposed multimodal learning network, the joint representation learning from multimodal inputs will be more suitable for FER. Experimental results on the CK+ and NVIE databases demonstrate the superiority of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There exist a number of FER approaches in the past years. Generally speaking, the current methods can be broadly classified into two categories based on the availability of the data for recognition. The first category can be regarded as texture-based methods <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b5">[6]</ref>. Texture modality for FER represents the facial image information, which displays face expression in pixel space. As such, texture-related features are extracted from the pixel value, which is capable of capturing detailed and subtle information of facial expression. On the other hand, the features are very sensitive to the image changes, such as luminance and masking effects. Furthermore, the texture-related features correlate very closely to each individual for FER. The other category is the landmark-based methods <ref type="bibr" target="#b1">[2]</ref> [25] <ref type="bibr" target="#b30">[31]</ref>. Landmark indicates face key points, the corresponding movements of which can help capture the facial expression. However, the landmark movements cannot efficiently capture the subtle changes, which may not able to distinguish the expressions with similar landmark information.</p><p>If the texture modality (facial image) is available, facial features are extracted from the images which are further fed into classifiers for recognition. The method in <ref type="bibr" target="#b4">[5]</ref> firstly convolves the video clip with Gabor motion energy (GME) filter in a filter bank for feature extraction. In order to make the problem close to reality, the first six frames are employed for feature extraction. Afterwards, support vector machine (SVM) is employed to train the features for expression recognition. Similarly, SVM is also employed in <ref type="bibr" target="#b3">[4]</ref> for FER. Prior of being fed into SVM, non-negative matrix factorization (NMF) <ref type="bibr" target="#b13">[14]</ref> is performed by minimizing a cost function. Firstly, local patches are extracted from each facial image, based on which the NMF is performed to reconstruct a sparse and part-based representation of the patches. Then SVM comes in handy to perform classification. Moreover, Yang et al. <ref type="bibr" target="#b5">[6]</ref> proposed to represent the dynamics of facial expression for recognition. Haar-like features are employed for the sake of simplicity and effectiveness.</p><p>The K-means clustering method is employed to generate the temporal pattern models of the expressions, and the Adaboost learning is employed as the classifier for FER.</p><p>If the landmark modality (face key point) is available, features can be extracted from the landmarks for FER. Similar to texture-based methods, most landmark-based methods extract handcrafted features from input landmark before performing recognition. In <ref type="bibr" target="#b24">[25]</ref>, Perveen et al. proposed to search the bounding boxes which help compute facial characteristic points (FCP). The facial animation parameters, such as the openness of eyes, width of eyes and height of eyebrows are then evaluated via referring to the FCPs.</p><p>With these animation parameters, the expression can be further recognized by employing the Gini Index <ref type="bibr" target="#b27">[28]</ref>. More recently, Lorincz et al. <ref type="bibr" target="#b1">[2]</ref> did a pioneering work on extracting features from the landmark in 3D space for FER. Only the landmark information is incorporated in 3D constrained local model (CLM). Such process makes the proposed FER robust against head pose variations. Additionally, they use either dynamic time warping (DTW) or global alignment (GA) kernel algorithm to deal with multiframes considering the spatio-temporal attribute of facial expression, and the landmark is tracked by using 3D CLM. Afterward, the Euclidean distance is calculated to build matrix, where the nearest correlation matrix is found with kernel, and the gram matrix by DTW kernel or global alignment kernel is further employed for SVM training. Finally, in order to minimize the classification error, the best parameters are searched for both kernels. With such processes, state-of-the-arts FER performance was obtained. He et al. <ref type="bibr" target="#b30">[31]</ref> conducted spontaneous facial expression recognition based on landmarks. First, they normalized the sequences according to the pupil s coordinates. Afterwards, they labeled landmarks on the onset and apex images manually and tracked landmarks on the whole sequences. The features depicting the point distance variation are extracted and the hidden Markov model (HMM) is employed to recognize the facial expression.</p><p>Although tremendous progresses of FER have been made in the past few decades, the problem remains with great challenges. Mostly, all previous work treats the texture or landmark modality independently, where only the texture or landmark modality is employed for FER. It has been demonstrated that each single modality is useful for FER. However, one single modality alone cannot help obtain the details of facial expression variation while avoiding the extraneous affections. The texture modality captures the detailed changes of the face information, which will be helpful for recognizing the subtle facial expression. However, external variations, such as the lightning condition and masking effect, will significantly affect the texture features, which will make the textural-based FER very sensitive. On the contrary, the landmark modality presents more robust property to the external affections. However, the landmark modality just simply outlines the shapes and contours of the face which is lack of sufficient detailed information. In this case, the landmark modality cannot accurately distinguish the subtle facial expression, specifically for the two expressions with similar landmark information. Texture and landmark modalities seem to be complementary to each other. Therefore, how to integrate the two modalities to improve the performance of the FER system remains an open question. The two modalities are of great difference, where the texture modality mostly describes the facial detailed expression, specifically the facial image content, and the landmark modality describes the positions of face key points.</p><p>Nowadays, some algorithms were proposed to address the representation learning for multiple modalities. In <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>, multimodal deep belief network (DBN) <ref type="bibr" target="#b0">[1]</ref> is developed for learning the joint representations from the input multiple modalities. In <ref type="bibr" target="#b7">[8]</ref>, the video and audio inputs were employed to learn a bimodal DBN. In order to further discover the correlations among the two modalities, both modalities are presented during feature learning but only a single modality is used for supervised training, which means that the deep autoencoder is trained to reconstruct both modalities when given only one modality (video or audio input). In <ref type="bibr" target="#b9">[10]</ref>, the multimodal DBN is trained to learn the joint representation of the multimodal data, specifically the text and image modalities. Firstly, two DBNs are trained for image and text respectively. To form a multimodal DBN, the two trained DBNs are combined by learning a joint RBM on top of them. In <ref type="bibr" target="#b10">[11]</ref>, Deep Boltzmann Machine (DBM) is employed to train each modality. In order to form a multimodal DBM, the two trained DBMs are combined by adding an additional layer of binary hidden units on top of them. From the work in <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b10">[11]</ref>, it is possible for the model to find representations such that some hidden unites are tuned only for one modality while others are tuned only for the other modalities <ref type="bibr" target="#b7">[8]</ref>.</p><p>Besides, there exists another defect with previous methods on FER. As aforementioned, the previous FER methods can be regarded as a type of two-step methods. Firstly, handcrafted features from texture or landmark modality are extracted, which are expected to represent the expression. Subsequently, the classifiers, such as SVM or Adaboost, or employed for training on the extracted features for FER.</p><p>Therefore, in such cases, features are the key components of the whole FER system. If the features can accurately depict the expression and are of great discriminations to different expressions, the classifier can recognize the expressions well. However, all the features are tuned by hand and thus can hardly ensure the classifier to distinguish the expression well. Therefore, it would be better to have feature extraction and classification assembled together to be globally optimized for FER.</p><p>In this paper, we make the first attempt to employ different modalities and assemble the feature representation and classification together for FER. Specifically, the facial texture and landmark modalities are combined together to benefit from the inherent properties of the two different modalities. A joint representation for FER is learned from the texture and landmark modalities. In order to ensure that the two modalities interact with each other for the joint representation, a structured regularization method is employed for each modality to control the connection tightness of representations. FER is then performed based on the learnt representation. With such multimodal learning process, the proposed FER method can not only ensure the robustness of the system to time resolution of the expressions but also make the method robust against head pose variations. Additionally, the multimodal learning combines feature extraction and classification together and thus avoids the cumbersome task of features handcrafting.</p><p>The rest of this paper is organized as follows. In Section II, our proposed multimodal learning method is introduced. Experimental results are given and discussed in Section III. Finally, Section IV concludes the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multimodal Learning Architecture</head><p>The proposed multimodal learning architecture is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, which takes different numbers and types of modalities as inputs and outputs the final classification results. The proposed multimodal learning architecture not only considers each modality property but also accounts for the interactions of different modalities. The proposed multimodal learning architecture is built by stacking several layers together and feeding the hidden representation of the k th layer as the input into the (k + 1) th layer.</p><p>The multimodal learning architecture in Fig. <ref type="figure" target="#fig_0">1</ref> can be formulated as:</p><formula xml:id="formula_0">L = Ψ(f k (f (k-1) ...f 2 (f SR 1 (x 1 , x 2 , ..., x m ))))<label>(1)</label></formula><p>Eq. ( <ref type="formula" target="#formula_0">1</ref>) represents the global function of the proposed multimodal learning method. L is the output class label of the multimodal learning network. f SR 1 (•) is the function that firstly maps the visual input layer to the first hidden layer. As our method targets at a multimodal learning network, f SR 1 (•) is an auto-encoder (AE) with the structured regularization (SR), which enforces the modality-specific sparsity and density of each modality. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>(a), AE is a simple learning circuit aiming to transform inputs into outputs with the least possible amount of distortion, where z i is the reconstructed signal of x i . It can be observed that AE treats each input node of different modalities equally, where the contributions of different modalities to the hidden nodes cannot be well learned. However, different modalities may contribute differently to the specific classification task, as demonstrated in Section III. To overcome this limitation and fully exploit the contributions of different modalities, AE with SR is employed, which allows the network to distinguish different modalities for individual treatments. function f : R d → R dh :</p><formula xml:id="formula_1">y i = f (x i ) = σ(W x i + b),<label>(2)</label></formula><p>where y i represents the encoder output and x i represents the input of the encoder. W ∈ R d×dh and b are the mapping weight and encoder bias, respectively. σ denotes a non-linear function, which can employ sigmoid, tanh, and rectified linear unit (ReLU) function. With this non-linear mapping process, AE can present strong feature learning capabilities <ref type="bibr" target="#b11">[12]</ref>.</p><p>In order to obtain the encoder parameters, the following optimization problem needs to be solved by minimizing the reconstruction error introduced from AE:</p><formula xml:id="formula_2">min W,b,c l(x, W, b, c) ,<label>(3)</label></formula><p>where c is the decoder bias, l(x, W, b, c) denotes the loss function to capture the reconstruction error.</p><p>There are some alternatives to define the loss functions, such as the squared error or Kullback-Leibler divergence (KLD) while the feature values lie in [0, 1]. Taking the squared error as the reconstruction error, l(x, W, b, c), Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be further represented as:</p><formula xml:id="formula_3">l(x, W, b, c) = 1 2n n i=1 z i -x i 2 2 ,<label>(4)</label></formula><formula xml:id="formula_4">y i = σ(W x i + b),<label>(5)</label></formula><formula xml:id="formula_5">µ i = y i y i y i ,<label>(6)</label></formula><formula xml:id="formula_6">z i = W µ i + c. (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where y i is the obtained hidden representations through the feedforward encoder, Eq. ( <ref type="formula" target="#formula_6">7</ref>) represents the decoder with the bias c, and z i is the reconstructed signal through performing a round of feedforward encoder and backward decoder. In order to reduce the effect of filter scale, the L 2 -normalization is normally performed on all hidden nodes of the encoder level as expressed in Eq. ( <ref type="formula" target="#formula_5">6</ref>).</p><p>As aforementioned, if training each modality separately and learning a joint representation (e.g. RBM)</p><p>on top of them, it is possible for the model to find representations such that some hidden unites are tuned only for one modality while others are tuned only for the other modalities <ref type="bibr" target="#b7">[8]</ref>. Similarly, if we simply employ AE in Eq. ( <ref type="formula" target="#formula_1">2</ref>) to map the multimodal inputs into the hidden nodes, the network is to connect all nodes of visible layer to nodes of the hidden layer, which means that all the different modality features are treated equally. Ignoring the specific properties of different modalities, AE will be trained to the form that some hidden nodes are strongly connected with some individual modality inputs while weakly connected to other modalities. As such, the correlations between different modalities cannot be well learned and represented. Therefore, to overcome this limitation, we employed the structured regularization (SR) <ref type="bibr" target="#b16">[17]</ref> [18], which allows the network to distinguish different modalities for individual treatment. Also the modality-specific sparsity and modality-specific density of the features from different modalities are enforced and further learned. SR is employed in the layer with multimodal inputs of Fig. <ref type="figure" target="#fig_0">1</ref> to distinguish and learn the representation from different multimodal inputs.</p><p>2) Structured Regularization (SR): As aforementioned, the SR function is employed for AE with multimodal inputs inspired by <ref type="bibr">[17] [18]</ref>. Suppose S r,i as an K × N modality binary matrix, where K denotes the number of modalities and N indicates the number of units in corresponding modality. For SR, each modality will be used as a regularization group separately for each hidden unit, applied in a manner similar to the group regularization, compared with the traditional regularization that treats each input unit equally and ignores the relationship and correlation between different modalities. SR is defined as:</p><formula xml:id="formula_8">SR(W [1] ) = M j=1 K k=1 N i=1 S r,i |(W [1] i,j ) P | 1 p<label>(8)</label></formula><p>where M denotes the total number of the hidden units. K is the total number of the modalities. N</p><p>indicates the total number of the input units in each modality. The regularization can be viewed as the summation of the corresponding Minkowski distance. For p ≥ 1, the Minkowski distance is a metric as a result of the Minkowski inequality. When p &lt; 1, the Minkowski distance violates the triangle inequality. In the limiting case of p reaching infinity, the regularization will be changed to the summation of Chebyshev distance:</p><formula xml:id="formula_9">SR(W [1] ) = M j=1 K k=1 max i (S k,i |W [1] i,j |)<label>(9)</label></formula><p>which only penalizes the maximum weight from each input unit to each hidden unit. In order to prevent over-constraining, the regularization function is modified to penalize nonzero weight maxima for each modality for each hidden unit without additional penalty for larger values of these maxima. The regularization function in Eq. ( <ref type="formula" target="#formula_9">9</ref>) are further modified as:</p><formula xml:id="formula_10">SR(W [1] ) = M j=1 K k=1 B (max i (S k,i |W [1] i,j |)) &gt; 0 (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where B indicates a boolean function that takes a value of 1 if its variable is true, and 0 otherwise. The regularization function in Eq. ( <ref type="formula" target="#formula_10">10</ref>) performs a direct penalty on the number of modalities used for each weight, without further constraining the weights of modes with nonzero maxima.</p><p>By integrating SR into the multimodal AE training as in <ref type="bibr" target="#b16">[17]</ref>, the objective function can be further represented as:</p><formula xml:id="formula_12">W [1] * = arg min W [1]</formula><p>n [1]   i=1</p><formula xml:id="formula_13">||z [1] i -x [1] i || 2 2 + α • SR(W [1] )<label>(11)</label></formula><p>where</p><formula xml:id="formula_14">z [1] i = k [1] j=1 µ [1] j W [1] i,j .<label>(12)</label></formula><p>where µ <ref type="bibr" target="#b0">[1]</ref> j is the hidden node generated by the encoder of the multimodal AE, while z</p><formula xml:id="formula_15">[1] i is signal</formula><p>reconstructed by the decoder from µ</p><p>[1] j . n [1] is the number of the input nodes including all the modality features, and k [1] is the number of the hidden nodes of the multimodal AE. W <ref type="bibr" target="#b0">[1]</ref> i,j is the corresponding weights of the multimodal AE by introducing SR. α is the parameter to balance the error and the regularization terms, which is experimentally set to 3 × e -4 in practice. Fig. <ref type="figure" target="#fig_1">2(b</ref>) illustrates the structure of AE with SR to demonstrate how SR with AE works for the multimodal inputs. By integrating SR into AE, the connection between the visual input layer and the first hidden layer is learned. As Eq. <ref type="bibr" target="#b9">(10)</ref> shows, to minimize SR(W [1] ), the zero number of W A softmax layer is added on top of the multimodal learning architecture, which takes the learned joint representation as inputs and outputs the classification results for each facial expression. For each expression, the softmax layer will determine whether the given inputs, including the texture and landmark modalities, will result in the specific expression or not. Consequently, the number of output nodes in the classification layer (top layer) is two. We can employ the introduced network including SR in the multimodal AE to build the network. The depth of the network depends on the problem and the number of the training samples. As aforementioned, insufficient training samples will incur overfitting with high probabilities, for the specific FER case, due to the constraint of the training sample number, only one hidden layer is employed, of which the network structure is I -H -C. Fig. <ref type="figure" target="#fig_2">3</ref> shows the structure of our network succinctly. Take the experiment conducted on the CK+ database as an example, C is defined as two to distinguish whether the inputs is the latent facial expression we aim to recognize. I is defined as the size of the data from all the multimodal inputs, which is set as 4040 in this paper. H denotes the size of the hidden layer nodes. As facial expressions affect the eyes and mouth significantly in each frame, the patches covering eyes and mouth are extracted and resized into 16 × 16 and 16 × 10 respectively. Afterwards, these corresponding patches will be concatenated as individual vectors, respectively. Supposing that F is the number of frames imported to the network, the size of eye and mouth modalities become 256×F and 160×F by temporally concatenating the modality vector from each frame. Furthermore, the displacements of the landmarks provide more persuasive representation than static coordinates. Besides, we suppose that features in different direction contribute differently to FER. As a result, the displacements of the landmarks in X and Y direction are separated as different modalities. As there are 68 marked face key points in every frame, temporally concatenating the landmark displacement results in a vector with size of 68 × F for each direction. By considering the texture and landmark modalities together, the vector size is 4040, with F equals to 5, which is fed into the network for further training. The output of each hidden nodes is generated by a sigmoid function σ(a) = 1/(1 + exp(-a)) of the weighted input:</p><formula xml:id="formula_16">h [1] j = σ I i=1 x i W [1] i,j<label>(13)</label></formula><formula xml:id="formula_17">p(o|x; Θ) = σ H i=1 h [1] i W [2] i<label>(14)</label></formula><p>where o denotes the output nodes which indicate whether the expression exists or not, and Θ indicates all the parameters in the network, specifically W [1] and W [2] .</p><p>The object of the learning network is to realize the non-linear mapping function for the FER. The inference can be realized by the following function:</p><formula xml:id="formula_18">ô = arg max o p(o|x; Θ)<label>(15)</label></formula><p>As mentioned before, each facial expression will be treated separately, for each of which we construct a network for the classification. Consequently, Eq. ( <ref type="formula" target="#formula_18">15</ref>) will help distinguish whether the multimodal input is the facial expression that we aim to recognize.</p><p>In order to make the inference, we need to obtain the parameters of the constructed network, specifically the parameters of the two layers, respectively. For the parameters W [1] in the multimodal layer, AE is first pretrained to obtain the initialized parameters. Specifically, the parameters of multimodal layer are firstly 1: Initialize W [1] and W [2] randomly;</p><p>2: Pretraining: W [1] is pretrained based on W [1] * = arg min W [1] n [1]  i=1 ||z [1] ) to learn the connections between the visual input layer and the contributions of different modalities to the hidden nodes on the benefit of SR; 3: Finetuning: Θ is updated according to Θ * = arg max Θ 2 t=1 logP ( L = L|x; Θ) -βSR(W [1] ) to strengthen the recognition capability of the network; 4: Record Θ for the multimodal learning network.</p><formula xml:id="formula_19">[1] i -x [1] i || 2 2 + α • SR(W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T EST IN G :</head><p>Input: Θ, X test Output: Ltest 1: Generate the output class labels Ltest of X test based on Θ according to Eqs. ( <ref type="formula" target="#formula_16">13</ref>) -( <ref type="formula" target="#formula_17">14</ref>); 2: Output the labels of facial expressions Ltest . fine-tuning process needs to be further performed to make the network more suitable for FER. Thereby, a log-likelihood function is employed as the object function for further training the parameters W [2] in the softmax layers and fine-tuning the parameters W [1] in the multimodal layer:</p><formula xml:id="formula_20">Θ * = arg max Θ 2 t=1 logP ( L = L|x; Θ) -βSR(W [1] )<label>(16)</label></formula><p>where, L represents the label of the inputs and L represents the outputs of the network. For the parameter training, traditional back-prorogation (BP) <ref type="bibr" target="#b25">[26]</ref> is employed to fine-tune parameters of the constructed deep network. This algorithm is first proposed by Rumelhart and McCelland, the essence of which is to minimize the mean squared error between actual output and desired output based on gradient descent. BP algorithm is especially powerful because it can extract regular knowledge from input data and memory on the weights in the network automatically <ref type="bibr" target="#b16">[17]</ref>. Simultaneously, it can improve generalization performance of the learning system, which is fabulous when used in FER.</p><p>Furthermore, in order to prevent over-fitting in training neural network, drop-out is introduced. Typically the outputs of neurons are set to zero with a probability of p in the training stage and multiplied with 1 -p in the test stage. By randomly masking out the neurons, dropout is an efficient approximation of training many different networks with shared weights. In our experiments, we applied the dropout to all the layers and the probability is set as p = 0.2.</p><p>We summarize our proposed multimodal learning for FER as in Algorithm 1. X train is the training sample which contains both textures and landmarks of facial expression. And L train denotes its corresponding labels. Based on the training samples, the parameters Θ of the multimodal learning network, specifically W [1] and W [2] are trained and learned. For testing, when imported the testing sample X test to the trained network, the output class label Ltest is generated based on the learned parameters Θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>In order to evaluate the effectiveness of the proposed method, Cohn-Kanade Extended Dataset (CK+) <ref type="bibr" target="#b14">[15]</ref> and the natural visible and infrared facial expression (NVIE) database <ref type="bibr" target="#b29">[30]</ref> are employed for experimental results. Firstly, the detailed information of the database are introduced. Afterwards, we will present how to process the input data to obtain the multimodal inputs for the proposed multimodal FER, including training and testing. Finally, experimental results are provided to demonstrate the effectiveness of the proposed multimodal method, as well as the performance comparison of the multimodal inputs and unimodal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database</head><p>The Cohn-Kanade Extended Dataset (CK+) <ref type="bibr" target="#b14">[15]</ref>  68 face key points are detected by AAM <ref type="bibr" target="#b23">[24]</ref> for each frame, which are regarded as the facial landmark.</p><p>In this paper, six emotions are selected for FER testing and the inventory of each expression used in this experiment is shown in Table <ref type="table" target="#tab_0">I</ref>. When imported to the recognition system, the samples of the certain expression are set as positive with the rest as negative. Obviously, the positive samples of expression "Fear" and "Sad" are less than others. As the luminance information is more important for FER, the color frame are converted into gray ones to only preserve the luminance components before further processing.</p><p>The natural visible and infrared facial expression (NVIE) database is newly developed for expression analysis. This database includes two sub-databases, that are posed database consisting of apex images and spontaneous database containing images and landmarks from onset to apex images. As the posed database with only apex frame could not meet our requirement, we did not take the posed one into consideration. For the spontaneous database, the facial images were recorded by DZ-GX25M camera with resolution 704 × 480 under three different conditions: illumination from left, front and right. There are 105 subjects under front illumination, 111 subjects under left illumination and 112 subjects under right illumination, respectively A total of 28 landmarks are located and tracked on each image. Different from the CK+ database, the labels of samples in the NVIE database are assigned values from 0 to 2 to every expression. The larger of the value, the more likely the sample belongs to that expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Processing</head><p>As aforementioned, the databases contain both texture and landmark modalities for each facial image.</p><p>These two modalities reflect different properties of the facial expression, which should be considered together for FER. As introduced in <ref type="bibr" target="#b12">[13]</ref>, the preprocessing of the data is critical to learning process. In the following, the texture and landmark modalities of the facial image will be first process respectively before being fed into the multimodal FER system. 2) Landmark Modality: The generation of facial expression is a dynamic process. Therefore, for the landmark modality, movements of the landmarks between the current frame and the previous one in video flow provide more insightful representation of facial expression than static landmarks. Additionally, we are not sure whether the head positions in the images from different people remain unchanged or not. As a result, we calculate the different value between current frame and previous one as the movements of landmarks. Assuming X i t+1 and Y i t+1 are the ith X and Y coordinate in the current frame respectively, X i t and Y i t are the ith X and Y coordinate in the previous frame, the landmark movements can be calculated as:</p><formula xml:id="formula_21">∆X i t = X i t+1 -X i t ∆Y i t = Y i t+1 -Y i t<label>(17)</label></formula><p>Please note that the first frame of the input sequence has no previous frame for reference, which only serves as reference and is excluded from the landmark modality for FER. After obtaining the movements from for each frame, the movements are concatenated as the input of the landmark modality, which results in the size of the landmark input modality as 68 × 2 = 136 for the CK+ database and 28 × 2 = 56</p><p>for the NVIE database.</p><p>3) Modality-specific Normalization: As the extracted vectors are from two different modalities, a normalization method under the incentive of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b26">[27]</ref> is employed to make the network robust to illumination and contrast variations. In addition, the normalization process is vital to the network training.</p><p>The procedure of normalization could be summarized as follows. Firstly, the mean value of the texture and landmark from one frame is obtained. Then the difference between real and the mean value is calculated to remove the individual difference for texture and landmark modalities. Finally, the standard deviation is divided to make the data to be normally distributed. Supposing P j as the j th pixel value of stretched patch (texture modality) or the j th coordinate of landmark modality in the row vector, J as the number of pixels in one patch or the number of landmark modality in one frame, the normalized result Pj of the input data is obtained by:</p><formula xml:id="formula_22">µ = J j=1 P j N σ = J j=1 [P j -µ] 2 Pj = P j -µ σ + C (<label>18</label></formula><formula xml:id="formula_23">)</formula><p>where C is a constant avoiding the numerator be divided by zero. The normalization makes the network robust to illumination and contrast variation as demonstrated by <ref type="bibr" target="#b19">[20]</ref>. Fig. <ref type="figure" target="#fig_6">6</ref> shows the intensity of input data before and after normalization in histogram. It can be observed that the values of the input data before normalization are mostly around 1. After pre-processing, the input data subjects to normal distribution approximately, which tends to be more suitable for network training <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Temporal Cascading:</head><p>The importance of facial dynamics in FER has been established in many vision experiments <ref type="bibr">[16] [19]</ref>. As stated in <ref type="bibr" target="#b4">[5]</ref>, facial dynamics is about motion among frames, rather than static patterns. Additionally, the inputs fed to the network is a row vector conventionally, which makes that the cascading the multi-frame in the same video together becomes an essential work. After integrating the texture and landmark modalities from different frames in the same sequence as one row, the multimodal input data for the network is prepared. The corresponding input data and labels can be obtained for further training. As long as the training is finished, the feed forward network with learned parameters can be employed to recognize facial expressions from texture and landmark modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multimodal Learning FER Results</head><p>The experimental settings are as follows. For each facial expression in the two databases, 2/3 of the whole samples are randomly selected to form the training set, with the rest as testing samples.</p><p>The network was trained and tested for five times, with the average experimental results as the network's performance. The receiver operating characteristic (ROC) curves are employed as the criterion to evaluate the performance, which is more general and reliable than recognition accuracy <ref type="bibr" target="#b5">[6]</ref> for evaluating the FER system. The X-coordinate of the ROC curve is F P/N , where N represents the number of negative samples and F P (false positive) as the number of samples incorrectly labeled as belonging to the positive class.</p><p>Analogically, the Y-coordinate is T P/P , where P indicates the positive samples and T P (true positive)</p><p>presents the number of samples correctly labeled as belonging to P . To draw a complete ROC curve, the threshold value ranges from 0 to 1 with 0.01 as the step size to obtain the curve. As aforementioned, there are two units in the output layer. The value of only one unit is employed for ROC curve generation.</p><p>If the value is larger than the threshold, the unit is set to 1, and 0 otherwise. The area under the ROC curve (AUC) is digital representation of the performance. Obviously, the larger the AUC, the better the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison to Prior Study on FER :</head><p>In order to efficiently assess the performance of our algorithm, we compare it with existing state-of-the-arts FER algorithms. We first use the first six frames of every labeled sequence as inputs. As the first frame only serves as reference, only the texture and landmark modalities from the rest five frames are extracted for training and testing. The performance is compared with the recent work done by Lorincz et al. <ref type="bibr" target="#b1">[2]</ref>, which achieved start-of-the-art performance. The corresponding results are illustrated in Table <ref type="table" target="#tab_1">II</ref>. It is noteworthy here that the experimental results from <ref type="bibr" target="#b1">[2]</ref> are employed for performance comparison. Fig. <ref type="figure">7</ref> shows the ROC curves of the six expressions.</p><p>Obviously, the performance of algorithms in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b4">[5]</ref> is inferior to other methods. Although the dynamic characteristics of facial expression have been considered and a spatiotemporal GME filter is employed, the texture modality is only adopted for FER in <ref type="bibr" target="#b4">[5]</ref>. Long et al. <ref type="bibr" target="#b2">[3]</ref> proved that learning  With the first six frames as inputs, our algorithm produced better results than existing algorithms. Since texture describes the face details and landmark outlines the shapes and contours, the proposed method integrates them together to exploit the complementarity of them. However, through the observation of the CK+ database, we find that the expression process is incomplete in the first six frames. In order to improve the recognition performance, we also take six frames which contain the first, the middle four and the last frame of sequence as inputs. The experimental results displayed in Table <ref type="table" target="#tab_1">III</ref> demonstrate that substantial change of expression reduce the recognition difficulty and can generate better recognition results.</p><p>Furthermore, Fig. <ref type="figure">7</ref> indicates that the performance of recognition on emotions "Fear" and "Sad" is inferior than the others, because AUC under the ROC curves of these two expression is less. The reason may be attributed to the extreme lack of training samples of these two emotions referring to Table . I.</p><p>Hence. the equilibrium, correctness and scale of the dataset are crucial for training a successful neural network.</p><p>2) Unimodality vs. Multimodality: The core idea of this paper is to address the integration of the texture and landmark modalities for FER. Therefore, it is necessary to compare the performance with unimodality and multimodality, respectively. Table <ref type="table" target="#tab_0">IV</ref> shows the corresponding experimental results, where the texture and landmark modalities are extracted from the integral multimodality dataset. The average performance of recognition results prove that multimodality is more reliable than unimodality for FER. It can be observed that the texture modality alone as the input data performs worse than the landmark modality. This probably because the texture modality only covers portions of the face while network performance. In order to find the best parameters H and U , we test various combinations of them on the facial expression "Anger". Fig. <ref type="figure">8</ref> (a) reveals the regularity as follows. Once the number of hidden layer is set to more than one, 50 units tend to give the best performance. More units will result in more parameters to be learned. However, the training samples cannot afford a network with too many parameters, which map the inputs from visible layer to hidden layer. On the other hand, if the number of hidden units is too small, it is hard to represent the 4040 input nodes. Our intention is to build a deep network to learn the nonlinear property of the texture and landmark modalities for FER. However, the lack of training data cannot afford a deep architecture for FER. It be observed that in Fig. <ref type="figure">8</ref> (b)</p><p>shows the network with one hidden layer and 100 hidden units performed the best. As the number of hidden layer increased, the performance on FER will be degraded. Hence, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, the settings of one hidden layer and 100 hidden units are adopted for FER in this paper.</p><p>5) The proposed method vs. Other Classifiers: As aforementioned, we integrate texture and landmark modalities together as the input and use multimodal learning method to perform the FER. To demonstrate that the proposed method indeed performs better than other algorithms, we compare the method to other two classifiers, specifically SVM and KNN. We first use the same row vector with 4040 units as the input to the classifiers. Experimental results demonstrate that SVM performs better than KNN. However, both results are not satisfactory. To further prove that AE with SR can not only integrate texture and landmark together but also automatically extract the meaningful features for FER, we import the learned feature of the hidden layer into SVM and KNN, respectively. As shown in Table <ref type="table" target="#tab_1">VII</ref>, after performing the multimodal feature learning, the performances of both SVM and KNN are significantly improved. We can conclude that the multimodal learning can effectively learn the representation from the multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Experiments on spontaneous database:</head><p>There are great differences between posed and spontaneous facial expression. The former is acted intentionally, while the latter is displayed unconsciously by subjects. The posed expressions are captured by asking subjects to perform different expressions in front of a camera, which are usually exaggerated. The spontaneous ones are more natural and different from the posed one both in appearance and timing. The recognition of spontaneous seems to have more profound theoretical and practical significances. However, its expression recognition is thus harder.</p><p>In this experiment, "Happy", "Fear" and "Disgust" are selected as samples to conduct the three-class classification. Fig. <ref type="figure" target="#fig_9">9</ref> illustrates the comparison results of the proposed method and He's method on the NVIE database. For "Disgust", "Fear" and "Happy", the recognition accuracy is measured by the ratio of the correctly recognized specific expression over the total number of the specific expression samples.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multimodal learning architecture for FER</figDesc><graphic coords="6,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of AE without SR (a) and with SR (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The structure of the network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>pretrained, which simply learns features from unlabeled data automatically aiming to transform inputs into outputs with the least possible amount of distortion. With the process of pre-training, the constructed network can effectively avoid the risk of trapping in poor local optima. After the pre-training process, the Algorithm 1 Multimodal Learning for Facial Expression Recognition T RAIN IN G : Input: {X train , L train } Output: Θ, Ltrain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is built by Kanade et al., which is developed for automated facial analysis and has been widely used for testing the performance of FER algorithms. In this dataset, the facial behaviors of 210 adults are recorded using two hardware synchronized Panasonic AG-7500 cameras and participants are 18 to 50 years of age. There are posed and non-posed expressions concurrently in the dataset. The facial expression dynamics of sequences in the CK+ dataset starts from neutral expression and ends on the apex of the expression. Since we need data with labels for training and testing, only posed expressions with explicit labels are selected. There are totally 123 subjects with 593 frontal image sequences in our input data, where 327 sequences are annotated with the emotion labels (1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sad and 7=surprise). Each frame in the sequence is digitized into either 640 × 490 or 640 × 480 pixel arrays with 8-bit gray scale or 24-bit color values, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 )Fig. 4 .Fig. 5 .</head><label>145</label><figDesc>Fig. 4. The structure of our approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Histogram of intensity of input data before and after normalization. (a) before (b)after.</figDesc><graphic coords="18,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>spatiotemporal filters with ICA works better than spatiotemporal Gabor features. Yet the final result relies largely on the handcrafted features. Jeni et al. [4] and Lorincz et al. [2] yield satisfactory results. Jeni et al. [4] removed personal mean texture manually. Only selected portions of the face image are employed, where the overall change of the face is neglected. Conversely, Lorincz et al. [2] used only the landmarks and neglected the texture one, with some important details of face missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. ROC curves of six different emotions</figDesc><graphic coords="27,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison with the method [31] on the NVIE database.</figDesc><graphic coords="28,95.00,3.34,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">NUMBER OF EXPRESSIONS</cell><cell></cell><cell></cell></row><row><cell>Emotion</cell><cell>Anger</cell><cell>Disgust</cell><cell>Fear</cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell></row><row><cell>Number</cell><cell>45</cell><cell>59</cell><cell>25</cell><cell>69</cell><cell>28</cell><cell>83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>TO PRIOR STUDY ON FER (FIRST SIX FRAMES)</figDesc><table><row><cell>Method</cell><cell>Ang.</cell><cell>Dis.</cell><cell>Fea.</cell><cell>Hap.</cell><cell>Sad.</cell><cell>Sur.</cell><cell>Aver.</cell></row><row><cell>Wu [5]</cell><cell cols="7">0.829 0.677 0.667 0.877 0.784 0.879 0.786</cell></row><row><cell>Long [3]</cell><cell cols="7">0.774 0.711 0.692 0.894 0.848 0.891 0.802</cell></row><row><cell>Jeni [4]</cell><cell cols="7">0.817 0.908 0.774 0.938 0.865 0.886 0.865</cell></row><row><cell>DTW [2]</cell><cell cols="7">0.873 0.893 0.793 0.892 0.843 0.909 0.867</cell></row><row><cell>GA [2]</cell><cell cols="7">0.921 0.905 0.887 0.910 0.871 0.930 0.904</cell></row><row><cell cols="8">Proposed algorithm 0.948 0.929 0.890 0.916 0.903 0.930 0.913</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the NSFC Grant No. 61203253 and 61233014, Research Found of Outstanding Young Scientist Award of Shandong Province (BS2013DX023), Independent Innovation Foundation of Shandong University (IIFSDU) 2013TB004, and Program of Key Lab of ICSP MOE China.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yang <ref type="bibr" target="#b5">[6]</ref> 0.973 0.941 0.916 0.991 0.978 0.998 0.966</p><p>Long <ref type="bibr" target="#b2">[3]</ref> 0.933 0.988 0.964 0.993 0.991 0.999 0.978</p><p>Jeni <ref type="bibr" target="#b3">[4]</ref> 0.989 0.998 0.977 0.998 0.994 0.994 0.992 DTW <ref type="bibr" target="#b1">[2]</ref> 0.991 0.994 0.987 0.999 0.995 0.996 0.994 GA <ref type="bibr" target="#b1">[2]</ref> 0.986 0.993 0.986 1.000 0.984 0.997 0.991</p><p>Proposed algorithm 0.995 0.999 0.967 0.999 1.000 1.000 0.993 landmarks can outline the shape and contour of the whole face. Moreover, the detailed change of face information that presented by texture and the global change of face represented by landmark can be viewed as complementary to each other. Consequently, when combined together, they yield the best FER results.</p><p>However, it seems that the recognition of "Happy" is improved by integrating both the texture and landmark modalities. It is easy to recognize "Happy" in this dataset. The texture and landmark modality alone already performs well. However, by integrating them together, the network will be much larger, which requires more training data. In this case, lack of training data can somewhat lead to overfitting, which results in the performance degradation.</p><p>Another issue is that the modality number may affect the FER performance. As shown in Table <ref type="table">V</ref>, FER is first performed with two modalities as inputs, where the left eye, right eye, and mouth are combined together as the texture modality, and the X-displacement and Y-displacement are combined together as  As such, by introducing more related modalities, the FER results will be improved further.</p><p>3) Comparison of the algorithms with and without pretraining : Table <ref type="table">VI</ref> displays the comparison of algorithms with and without pretraining. It is demonstrated that, once pretraining is added to the network, the performance is improved by six percent. BP is based on local gradient descent, and starts usually at some random initial points, which may cause poor local optima. If pretraining is employed to initialize the parameters, the network could be fine-tuned on the pretrained parameters. In this case, the parameters of the network will avoid the risk of getting stuck at local optima. Table <ref type="table">VI</ref> illustrates the FER results with and without pretraining, which demonstrate the necessity of pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) One hidden layer vs. Multiple hidden layers :</head><p>The number of the hidden layers H and the number of the units U in each layer are the hyper-parameters of the network, which is very important for the For "Total accuracy", the recognition accuracy is calculated by the ratio of all correctly recognized samples over all the total number of the samples. It can be observed that for the comparison of the spontaneous FER, the proposed method performs better than He's method. For "Total accuracy", 10% accuracy improvement is obtained by our proposed multimodal learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we presented a multimodal FER algorithm, where the texture and landmark are integrated together to boost the FER performance. In order to avoid handcrafted features, which are cumbersome and time-consuming, the joint representation for FER is learned from the built neural network. By incorporating SR into AE, the proposed network can not only distinguish each modality but also learn the correlation and interaction between the texture and landmark modalities, which are complementary to each other. Various experimental results and comparisons have demonstrated the superiority of the proposed method over existing ones.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lorincz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<title level="m">Emotional Expression Classification using Time-Series Kernels, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="889" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features by Using Independent Component Analysis with Application to Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="126" to="132" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D L</forename><surname>Torre</surname></persName>
		</author>
		<title level="m">Continuous AU Intensity Estimation Using Localized, Sparse Facial Feature Space, IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition Using Gabor Motion Energy Filters</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<title level="m">Facial Expression Recognition Using Encoded Dynamic Features, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Expressionlets on Spatio-Temporal Manifold for Dynamic Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal Deep Learning, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multimodal DBN for Predicting High-Quality Answers in cQA portals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Learning Representations for Multimodal Data with Deep belief Nets, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal Learning with Deep Boltzmann Machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marginalized Denoising Autoencoders for Domain Adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-Negative Matrix Factorization: a Comprehensive Review</title>
		<author>
			<persName><forename type="first">Wanf</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1336" to="1353" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comprehensive Database for Facial Expression Analysis</title>
		<author>
			<persName><forename type="first">Kanade</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietickaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Learning for Detecting Robtic Grasps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Dirty Model for Multi-task Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing Facial Expressions in Image Sequences using Local Parameterized Models of Image Motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="23" to="48" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based Learning Applied to Document Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<title level="m">Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="53" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Control of Facial Expressions of the Humanoid Robot Head ROMAN</title>
		<author>
			<persName><forename type="first">K</forename><surname>Berns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3119" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automated Drowsiness Detection for Improved Driver Safety-Comprehensive Databases for Facial Expression Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ercil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automotive Technologies</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatically Detecting Pain using Facial Actions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Prkachin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression recognition using facial characteristic points and Gini index</title>
		<author>
			<persName><forename type="first">N</forename><surname>Perveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Students Conference on Engineering and Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Three Classes of Deep Learning Architectures and Their Applications: A Tutorial Survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<title level="m">Convolutional Neural Networks for No-reference Image Quality Assessment, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbiased Split Selection for Classification Trees Based on the Gini Index</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Augustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="483" to="501" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-column Deep Neural Network for Traffic Sign Classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A natural visible and infrared facial expression database for expression recognition and emotion inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="682" to="691" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spontaneous facial expression recognition based on feature point tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Image and Graphics (ICIG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="760" to="765" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
