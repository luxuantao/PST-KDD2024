<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Agent Graph Convolutional Reinforcement Learning for Dynamic Electric Vehicle Charging Pricing</title>
				<funder ref="#_kjMMUdQ #_3wFjx3r">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Zw2z6NG #_Tth4tBn">
					<orgName type="full">Foshan HKUST Projects</orgName>
				</funder>
				<funder ref="#_sUJ7DG8">
					<orgName type="full">Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<email>liuh@ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Jindong</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
							<email>yongge@arizona.edu</email>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>xionghui@ust.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">HKUST SHCIRI denotes HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Multi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">HKUST (GZ) &amp; CSE</orgName>
								<orgName type="institution" key="instit1">HKUST SHCIRI &amp; AIT</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Agent Graph Convolutional Reinforcement Learning for Dynamic Electric Vehicle Charging Pricing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539416</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Charging station dynamic pricing</term>
					<term>multi-agent reinforcement learning</term>
					<term>graph neural networks</term>
					<term>graph contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Electric Vehicles (EVs) have been emerging as a promising lowcarbon transport target. While a large number of public charging stations are available, the use of these stations is often imbalanced, causing many problems to Charging Station Operators (CSOs). To this end, in this paper, we propose a Multi-Agent Graph Convolutional Reinforcement Learning (MAGC) framework to enable CSOs to achieve more effective use of these stations by providing dynamic pricing for each of the continuously arising charging requests with optimizing multiple long-term commercial goals. Specifically, we first formulate this charging station request-specific dynamic pricing problem as a mixed competitive-cooperative multi-agent reinforcement learning task, where each charging station is regarded as an agent. Moreover, by modeling the whole charging market as a dynamic heterogeneous graph, we devise a multi-view heterogeneous graph attention networks to integrate complex interplay between agents induced by their diversified relationships. Then, we propose a shared meta generator to generate individual customized dynamic pricing policies for large-scale yet diverse agents based on the extracted meta characteristics. Finally, we design a contrastive heterogeneous graph pooling representation module to learn a condensed yet effective state action representation to facilitate policy learning of large-scale agents. Extensive experiments on two real-world datasets demonstrate the effectiveness of MAGC and empirically show that the overall use of stations can be improved if all the charging stations in a charging market embrace our dynamic pricing policy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Electric Vehicles (EVs) have been recognized as an effective way for reducing carbon emissions and facilitating renewable integration, which is of great importance to advance the United Nations Sustainable Development Goals (SDGs). Although a large number of public charging stations have been built to meet the growing charging demand, we observe an unprofitable dilemma for Charging Station Operators (CSOs), which prevents the further prevalence of EVs. On one hand, the imbalanced charging demands induce an unsatisfactory charging experience for EV drivers <ref type="bibr" target="#b43">[44]</ref>, and lead to low utilization of public charging stations. On the other hand, the under-utilization of charging stations prevents the CSOs from constructing more stations. To unleash the business value of deployed charging stations, the dynamic pricing scheme is promising to be used to incentivize EV drivers' charging behaviors, so that it enables CSOs to rebalance charging demands, improve their charging stations' utilization, and ultimately improve the company turnover.</p><p>Prior studies on Charging Station Dynamic Pricing (CSDP) problem can be mainly categorized into two classes: (1) Traditional Optimization Methods (TOMs) optimize CSDP problem by various traditional optimization approaches, such as dynamic programming <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>, genetic algorithm <ref type="bibr" target="#b19">[20]</ref>, and game theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref>. These methods either require a precise environment model that is difficult for a complex dynamic system, or may incur high computational overhead to make an optimal decision, which is undesirable for real-time decision-making; (2) Reinforcement Learning (RL)-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> introduce model-free RL techniques into CSDP problem, which enforces the agent to automatically learn an optimal dynamic pricing policy by interacting with environment to maximize long-term reward. Compared with TOMs, RL-based methods can learn to efficiently make real-time decisions in dynamic system without relying on the precise environment model, thereby are more appropriate for CSDP problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>. However, prior RL-based methods are only applicable for a small number of charging stations (no more than twenty <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>), whereas there can be thousands of charging stations in a metropolis <ref type="bibr" target="#b43">[44]</ref>. Moreover, existing methods for CSDP mainly focus on developing dynamic pricing schemes with fixed time interval (e.g., adjust the price per hour), which fail to provide fine-grained and real-time pricing decisions for continuously arising charging requests.</p><p>Different from previous studies, in this work, we aim to solve the Charging Station Request-Specific Dynamic Pricing (CSRSDP) problem, which considers a fine-grained and real-time charging market scenario. Figure <ref type="figure" target="#fig_0">1</ref> shows an illustrative example of the CSRSDP problem. On the one side, charging requests may be continuously issued by EV drivers at any time of the day, and the EV selects which station for charging by comparing various decision factors (e.g., price, availability, and distance to the charging station). On the other side, a large number of charging stations operated by different CSOs in a metropolis are responsible for providing real-time and reasonable pricing decisions to affect each EV driver's station selection, in order to maximize their operational effectiveness from a competitive charging market. Particularly, since stations' available charging resources are limited, and EV's charging process may occupy the charging spot for several hours, charging stations' current pricing decisions for a charging request may influence their future pricing decisions and induce a long-term effect on the whole charging market. To develop long-term optimal real-time pricing policies in such a highly dynamic charging market environment, it's intuitive for us to apply RL into CSRSDP problem for optimizing multiple long-term commercial goals. However, due to the large number of charging stations in a metropolis, a centralized RL method that manages the whole system by a single agent would be intractable and induce severe scalability and efficiency issues <ref type="bibr" target="#b43">[44]</ref>, and would be vulnerable for single points of failure <ref type="bibr" target="#b16">[17]</ref>. To this end, by regarding each charging station as an agent, we attempt to introduce Multi-Agent Reinforcement Learning (MARL) for the CSRSDP problem to realize a scalable and efficient distributed multiagent management system for long-term optimal dynamic pricing of large-scale stations.</p><p>However, it is a non-trivial task to apply MARL for effective CSRSDP, because of the following technical challenges. (1) How to formulate CSRSDP as a MARL task? Only few studies have attempted to apply MARL into CSDP problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>, which develop dynamic pricing scheme with fixed time interval. The first challenge is how to formulate CSRSDP, which involves a fine-grained and highly dynamic charging market environment, as a MARL task.</p><p>(2) How to model complex interplay between agents induced by their diversified relationships? In real charging market, the relationships between charging stations (a.k.a. agents) can be competitive or cooperative depends on their CSOs. Moreover, the relationships can be changeless in terms of spatial proximity of charging stations or dynamic due to charging requests issued in different locations. The diversified relationships among charging stations induce complex interplay between agents, which is challenging to model. (3) How to learn customized polices for large-scale yet diverse agents? The inherent charging station's diversity (e.g., electricity cost, charging demands, and surrounding charging stations) arises customized pricing scheme requirement for each agent. However, considering the learning efficiency and scalability of the MARL algorithm, it's challenging to learn individual dynamic pricing policies for each of large-scale agents. (4) How to effectively represent the highdimensional charging market? To stimulate agents to make globally coordinated and rational decisions, it's critical to enable agents to perceive the state of whole charging market during policy learning. However, representation of whole charging market need to involve all of large-scale charging stations throughout a metropolis, which may induce the curse of dimensionality issue <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>, leading to agents' policy learning intractable and significant performance degradation. The last challenge is how to learn a condensed yet effective representation for high-dimensional charging market.</p><p>To tackle above challenges, in this paper, we present a Multi-Agent Graph Convolutional Reinforcement Learning (MAGC) framework for CSRSDP problem. Specifically, we first formulate CSRSDP problem as a mixed competitive-cooperative MARL task. To achieve effective distributed multi-agent management, we devise a multiview heterogeneous graph attention networks to model complex interplay between agents. In particular, we formulate the whole charging market as a dynamic heterogeneous graph, where the longstanding and temporal relationships between agents are constructed from station-centric and request-centric views, respectively. Then, a two-stage graph attentive aggregator is proposed to adaptively integrate competitive and cooperative effects from adjacent agents for supporting agents to make context-aware intelligent dynamic pricing decisions. Moreover, a shared meta generator is proposed to generate individual customized dynamic pricing policies for largescale agents based on the automatically extracted meta characteristics from their historical trajectories. For centralized multi-agent policy learning, we design a contrastive heterogeneous graph pooling representation module to learn a condensed yet semantically rich charging market representation to enable policy learning of large-scale agents to be tractable, wherein a heterogeneous graph pooling approach is devised to distill high-dimensional charging market to a condensed latent state action representation of agents, and a graph contrastive learning objective is proposed as an auxiliary task to facilitate latent state action representation learning.</p><p>Our major contributions can be summarized as follows: (1) We investigate a new CSRSDP problem for fine-grained and real-time operation optimization of large-scale charging stations, and formulate it as a mixed competitive-cooperative MARL task. <ref type="bibr" target="#b1">(2)</ref> We devise a multi-view heterogeneous graph attention networks to model complex interplay between charging stations. (3) We propose a shared meta generator to generate individual customized dynamic pricing policies for large-scale charging stations. (4) We design a contrastive heterogeneous graph pooling representation module to facilitate multi-agent policy learning. ( <ref type="formula">5</ref>) Experiments on two realworld datasets empirically show that MAGC can largely improve CSO's operational effectiveness and even improve the overall social welfare if all CSOs in the charging market embrace MAGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we first introduce some important definitions of this work, then formally formulate the CSRSDP problem.</p><p>Considering a integrated charging platform (e.g., Google Maps and Baidu Maps) that contains ? CSOs ? = {? 1 , ? 2 , . . . , ? ? } and ? charging stations, each CSO ? ? operates a set of ? ? stations ? ? = {? 1 , ? 2 , . . . , ? ? ? }. Charging stations belonging to the same CSO are cooperative, otherwise are competitive. We regard each day as a complete episode, thereby all statistical summation operations are performed on one day. We first define charging request as below.</p><p>Definition 1 (Charging reqest). A charging request ? ? = ?? ? ,? ? ,? ? ? ? ? Q is defined as the ?-th request (i.e., step ?) arising in the charging platform of a day. Specifically, ? ? and ? ? are the location and time of that an EV issues ? ? , and ? ? ? is time when ? ? is finished. A request ? ? is finished if it successfully charges in a station (i.e., charging success) or finally gives up if it queues for too long (i.e., charging failure). All the CSOs, charging stations, and charging requests jointly compose a complex and dynamic charging market. Definition 2 (Charging price). The charging price is defined as the unit price per kilowatt hour (kWh) of a charging station, which is the sum of electricity cost and service fee. Definition 3 (Profit). We define a charging service as the event that a charging request ? ? is successfully serviced (charging success) by a charging station, the profit of this charging service is defined as</p><formula xml:id="formula_0">??? ? ?? = (?? (? ? ) -?? (? ? )) ? ??? (? ? ),</formula><p>where ?? (? ? ), ?? (? ? ) and ??? (? ? ) are charging price, electricity cost and charging electricity (energy) quantity of ? ? , respectively. We further define the overall profits of a CSO ? ? as the profits summation of its all charging services. Definition 4 (Charging service volume (CSV)). The charging service volume of a CSO ? ? is the total number of charging services that its charging stations ? ? serve, which is an important metric to measure the utilization of ? ? .</p><p>Definition 5 (Charging failure rate (CFR)). The charging failure rate of ? ? is defined as the ratio of the number of charging requests who fail to charge in ? ? over the total number of charging requests who go to ? ? for charging. This metric is to measure the imbalance of supplies and demands of ? ? .</p><p>Problem 1 (Charging station reqest-specific dynamic pricing (CSRSDP)). Considering a set of charging requests Q of a day continuously arise in the charging platform, our problem is to enable charging stations ? ? of a CSO to develop real-time pricing decisions for each ? ? ? Q, with the long-term goals to simultaneously maximize the overall profits and CSV, as well as minimize the CFR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we detail our MAGC framework. We first present the MARL formulation on CSRSDP problem. Then we elaborate on the complex agents interplay modeling with individual customized policy generation for distributed multi-agent management (execution) and the high-dimensional charging market representation to facilitate policy learning (training) over large-scale agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the framework overview of MAGC. By regarding each charging station as an agent, CSRSDP is first formulated as a mixed competitive-cooperative MARL task. Then we employ a multi-agent actor-critic method with Decentralized Execution with Centralized Training (DECT) architecture to achieve efficient Distributed Multi-Agent Management (DMAM) and effective Centralized Multi-Agent Policy Learning (CMAPL) over large-scale agents. Specifically, DMAM consists of a Multi-View Heterogeneous Graph Attention Networks (MVHGAT) module to model complex interplay between agents and a shared meta generator module to generate individual customized dynamic pricing policy for each of large-scale agents based on the extracted meta characteristics. In CMAPL, a Contrastive Heterogeneous Graph Pooling Representation (CHGPR) module is designed to learn a condensed yet effective state action representation of agents from whole charging market. Then the state action representation is used by critic to learn coordinated policies of large-scale agents. After CMAPL is finished, only DMAM is required for efficient charging stations dynamic pricing in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MARL Formulation</head><p>We formulate the CSRSDP as a mixed competitive-cooperative MARL task, which consists of a set of elements ?I, O, A, T , ?, ??.</p><p>Agents I = {? ? , ? ? }. By regarding each charging station ? ? ? I as an agent, we formulate two types of agents: the dynamic pricing agents ? ? dynamically offer a real-time charging price for each charging request ? ? , and the fixed pricing agents ? ? offer charging prices based on their real Time-of-Use (TOU) tariff policies, where the charging prices are predefined and vary according to time. Without loss of generality, we select all the charging stations ? ? of a typical commercial CSO ? ? as ? ? , and the stations of the other CSOs as ? ? to keep their original pricing policies.</p><formula xml:id="formula_1">Observation space O = O 1 ? ? ? ? ? O ? , State space S.</formula><p>We denote ? ? ? ? O ? as the observation of agent ? ? at step ? for charging request ? ? , which is the combination of two aspects of information. One is the attributes of agent ? ? itself, including its unique agent identifier, CSO, charging power, the number of available charging spots (supply) at step ?, and electricity cost at step ?. Another is the information provided by the charging platform, including time ? ? at step ?, the geographical distance and estimated time of arrival (ETA) from location ? ? to ? ? , the number of traveling EVs to ? ? , and the predicted number of forthcoming charging requests around ? ? (future demand). Then, we denote joint observation ? ? at step ? as the set of all agents' observations, formalized as ? ? = {? 1 ? , ? 2 ? , . . . , ? ? ? } ? O, and denote ? ? ? S as state at step ?.</p><p>Action space</p><formula xml:id="formula_2">A = A 1 ? ? ? ? ? A ? .</formula><p>The action ? ? ? ? A ? of agent ? ? at step ? is defined as its offered charging price for charging request ? ? . Agent ? ? ? ? ? ? will dynamically generate a real-time action for each ? ? . In contrast, the charging prices of agents ? ? are based on fixed TOU tariff policies, thus their actions for each ? ? are known in advance. Likewise, the joint action ? ? is defined as the set of all agents' actions ? ? = {? 1 ? , ? </p><formula xml:id="formula_3">) : S ? A ? S, wherein ? ? +1 is associated to ? ? +1 .</formula><p>Reward ? . We propose a delayed reward design that the reward ? (? ? , ? ? ) : S ? A ? R at step ? defers to return until charging request ? ? is finished. We simplify ? (? ? , ? ? ) to ? ? in the following. Formally, reward function is formulated as</p><formula xml:id="formula_4">? ? = ? ? ? ? ? ? ? ? ? ? ? + ?? (? ? ) -?? (? ? ), ??????? ?? ? ? ? ? , ? ?????? ?? ? ? 0, ????????? .<label>(1)</label></formula><p>Specifically, to attract more charging requests for charging and improve utilization of ? ? , the environment will return a relatively small positive reward ? ? as long as ? ? goes to charging stations ? ? for charging. If a charging request ? ? finally charges success in ? ? , the environment will further return the unit profit of this charging service as an additional reward to stimulate the balance of charging resources and the improvement of profits. In the other cases, the environment will return a zero reward as penalty. In our MARL formulation, reward is shared by all agents ? ? of the same CSO, and we don't consider reward for ? ? since their policies are fixed. ? ? compete with ? ? , and aim to cooperatively maximize expected return</p><formula xml:id="formula_5">? ? = | Q | ? ? =? ? ? ? -? ? ? ? of all charging request Q in a day, where ? ? [0, 1] is a discount factor.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complex Agents Interplay Modeling</head><p>Agents (charging stations) continually interplay with others in a charging market environment. It's critical to consider agents' interplay for dynamic pricing, because whether an agent can attract an EV driver for charging depends not only on itself but also on the competitiveness of other agents. Effective interplay modeling enhances agents environmental perception for dynamic charging market, which enables them to make context-aware intelligent pricing decisions. However, it's a non-trivial task to model interplay between agents, which has two immediate problems to be addressed:</p><p>(1) Which agents interplay with? There are a large number of agents under different CSOs in a charging market. Considering fully pairwise interplay is computationally expensive and ineffective since not any pair of agents interplay with each other. ( <ref type="formula" target="#formula_6">2</ref>) How do agents interplay? There are a lot of factors affecting an EV driver to select charging station, e.g., charging price, distance, charging spots availability. An agent may lose customers for other agents' integrated advantages in many factors, which jointly induce agents' interplay in a charging market.</p><p>To tackle above problems, MVHGAT exploits competitive and cooperative relationships between agents from multiple views to discover which pair of agents interplay with each other, and employs a two-stage graph attentive aggregator to adaptively integrate influential factors from adjacent agents, enabling agents to effectively model complex interplay in a dynamic charging market. Specifically, we formulate the state of whole charging market at each step ? as a dynamic heterogeneous graph G = ?I, X ? , E, X ? ?, where I = {? ? , ? ? } denotes nodes which consists of all agents, X ? denotes attributes of all nodes, E = {? ? , ? ? } are heterogeneous edges comprising competitive relationship set ? ? and cooperative relationship set ? ? among agents, X ? are attributes of all edges. Note that all operations in this section are assumed at step ? and we omit the subscript ? to ease the presentation.</p><p>To comprehensively represent interplay between agents, we consider each kind of relationship among agents from both the charging station-centric and charging request-centric views, which is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, where the station-centric view expresses long-standing relationships in terms of spatial proximity of charging stations, and request-centric view reveals temporal relationships associated with the location in which charging requests is issued. Thus each edges set is composed of two types of edges, denoted as ? ? = {? ?,? , ? ?,? }. Specifically, we define edge ? ? ? ?,? ? ? ?,? of competitive relationship from station-centric view as</p><formula xml:id="formula_6">? ? ? ?,? = 1, ? (?) ? ? ( ?), ???? (? ? , ? ? ) ? ? ? 0, ????????? ,<label>(2)</label></formula><p>and define edge ? ? ? ?,? ? ? ?,? from request-centric view as</p><formula xml:id="formula_7">? ? ? ?,? = 1, ? (?) ? ? ( ?), ???? (? ? , ? ? ) ? ? ? , ???? (? ? , ? ? ) ? ? ? 0, ????????? ,<label>(3)</label></formula><p>where ? (?) denotes the CSO of ? ? , ???? (?, ?) is geographical distance between two locations, ? ? and ? ? are distance thresholds. The construction of cooperative relationship set ? ? is similar, but alters the condition to ? (?) = ? ( ?). Note there is no self-loop in E.</p><p>Inspired by recent success of Graph Neural Networks (GNNs) <ref type="bibr" target="#b37">[38]</ref> on processing graph data, we propose a two-stage graph attentive aggregator to model complex interplay of agents on graph G. Note that since only agents ? ? implement dynamic pricing, we only need to compute the interplay between ? ? and ? ? , and among ? ? for DMAM. The first stage models interplay based on competitive relationships. Given ? ? ? ? ? , ? ? ? ? ? , to comprehensively quantify mutual influence between them, we compute the adaptive edge weight ? ? ? ?,? by performing a learnable attention operation on all attributes of the connected nodes and edge,</p><formula xml:id="formula_8">? ? ? ?,? = v ? ? Tanh W ? 1 ? ? + W ? 2 ? ? ?? ? + W ? 3 ? (? ? ? ?,? ) , ? ? ? ?,? = exp(? ? ? ?,? ) ? ? {?,? } ? ?N ? ?,? exp(? ?? ?,? )) ,<label>(4)</label></formula><p>where v ? ,W ? 1 ,W ? 2 ,W ? 3 are learnable parameters, ? (?</p><formula xml:id="formula_9">? ? ?,?</formula><p>) is edge attributes, including geographical distance between the connected nodes and a one-hot vector to identify its view (stationcentric or request-centric), N ? ?,? denotes competitive adjacent agents set of ? ? in the corresponding view of edge, ? is the concatenation operation. Then we employ graph convolution operation to adaptively integrate influential factors from competitive adjacent agents, </p><formula xml:id="formula_10">? ? ? = ReLU W ? ? ?? ? ? {?,</formula><formula xml:id="formula_11">? ? ? = ReLU W ? ? ?? ? ? {?,? } ?? ? ?N ? ?,? ? ? ? ?,? ? ? ? ? (? ? ? ?,? ) ,<label>(6)</label></formula><p>where W ? ? are learnable parameters. Finally, we incorporate the two-stage representations into agent's observation to attain the enhanced observation of ? ? ,</p><formula xml:id="formula_12">? ? = ? ? ? ? ? ? ? ? .<label>(7)</label></formula><p>? ? effectively enhances agent's observation for environment, which improves its ability to compete or cooperate with others in the charging market. It is noteworthy that thanks to GNNs' inherently inductive capability <ref type="bibr" target="#b9">[10]</ref>, MVHGAT can still work even if the graph varies due to agents' failure, which guarantees its robustness in a large-scale agent system, and are naturally appropriate for efficient and effective DMAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Individual Customized Policy Generation</head><p>There are thousands of charging stations in a metropolis, and the number is growing year-by-year <ref type="bibr" target="#b43">[44]</ref>. Considering algorithm's learning efficiency and scalability, it's infeasible to learn dedicated policies for each of such large-scale agents. In contrast, learning a generalized policy (i.e., share policy network parameters) for all agents sounds reasonable. However, the diversity of agents comes from the differences of their own attributes (e.g., electricity cost, charging spots availability) and environmental factors (e.g., surrounding charging demands, interplay with other charging stations), which arises customized pricing scheme requirement for each agent. For example, a charging station located in a busy commercial area may have high electricity cost and surrounding charging demands, thus it generally has higher pricing range in comparison with a one located in remote suburb. A completely generalized policy fails to meet the obvious distinction among agents' pricing schemes. To tackle above problem, we propose a shared meta generator to generate individual customized dynamic pricing policies for agents based on their extracted meta characteristics. As the meta generator is shared by all agents, its learnable parameters are independent with the number of agents, which guarantees the learning efficiency and scalability for large-scale agents.</p><p>Intuitively, agent's meta characteristics should include its own inherent attributes and long-term environmental factors, which essentially determines agent's action (charging price) distribution. Therefore, we employ the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">[5]</ref>, an effective variant of recurrent neural networks for long-term sequence modeling, combined with MVHGAT for considering agents' interplay, to automatically extract meta characteristics from agent's historical trajectory, which can be formalized as</p><formula xml:id="formula_13">H ? ? = GRU H ? ? -1 , ? ? ? ?? ? ? -1 ,<label>(8)</label></formula><p>where H ? ? -1 is the output of GRU at previous step ? -1, ? ? ? is the output of MVHGAT (Eq. ( <ref type="formula" target="#formula_12">7</ref>)) at current step ?.</p><p>With meta characteristics H ? ? as input, we introduce the learnable hypernetworks <ref type="bibr" target="#b8">[9]</ref> G to generate agent-specific parameters for the individual customized policy of each agent,</p><formula xml:id="formula_14">W ? ?,? = G ? (H ? ? ), b ? ?,? = G ? (H ? ? ).<label>(9)</label></formula><p>Then each agent ? ? ? ? ? produces action at step ? via the generative individual customized policy (actor)</p><formula xml:id="formula_15">? ? ? , ? ? ? = ? ? ? (? ? ? ) = Sigmoid W ? ?,? ? (? ? ? ) + b ? ?,? ? ?,<label>(10)</label></formula><p>where ? (?) is a learnable transformation function (e.g., neural networks), ? is a coefficient to scale action to real charging price.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">High-Dimensional Charging Market Representation</head><p>To achieve global coordination among agents and avoid the nonstationary environment issue<ref type="foot" target="#foot_0">1</ref> in a multi-agent system, Lowe et al. <ref type="bibr" target="#b22">[23]</ref> adopt a DECT architecture into MARL, where the observations and actions of all agents (i.e., ? ? and ? ? ) are used as state and joint action for the input of a centralized critic to facilitate training process. However, in our CSRSDP problem, directly using joint observation ? ? and joint action ? ? faces two problems:</p><p>(1) First, it fails to model complex interplay between agents, which is important information to comprehensively suggest the state and trend of charging market. (2) Second, the dimension of joint observation and action increases along with the growth of agents number. In a large-scale agent system like ours, the model will suffer from the curse of dimensionality issue <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>. To address above problems, we creatively design a Contrastive Heterogeneous Graph Pooling Representation (CHGPR) module to learn a condensed yet semantically rich representation of whole charging market, which can be used as an effective state action representation to facilitate centralized learning for large-scale agents. In this section, we omit step subscript ? to ease presentation as well.</p><p>3.5.1 Heterogeneous Graph Pooling. Based on prior formulation that formulating the whole charging market as a dynamic heterogeneous graph, we devise a Heterogeneous Graph Pooling (HGP) approach to distill the high-dimensional charging market to a condensed representation, which is illustrated in Figure <ref type="figure" target="#fig_11">7</ref>. Specifically, we first incorporate actions of ? ? into G (by concatenating actions with observations of ? ? ), so that charging prices of all agents are obtained, which makes up a complete charging market, denoted as G ? .</p><p>Then we perform MVHGAT on G ? , which computes the interplay among all agents I, to derive all agents' enhanced observations,</p><formula xml:id="formula_16">? = W ? ? MVHGAT G ? ,<label>(11)</label></formula><p>which models complex interplay of agents into ? = {? ? , ? ? }, where W ? are learnable parameters. Next we adopt a learnable pooling operation to distill essential information of charging market into latent representation, while drop redundant information. We first learn to project ? ? to importance scores for agents ? ? selection,</p><formula xml:id="formula_17">? ? = ? ? p ? ,<label>(12)</label></formula><p>where p ? is a learnable projection vector. After that, we filter agents by selecting the most top-? ? important ones and discarding others based on the learned importance scores ? ? ,</p><formula xml:id="formula_18">? ???? ? , ? ???? ? = Filter (? ? , ? ? , ? ? ) ,<label>(13)</label></formula><p>where ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>???? ?</head><p>denotes the largest top-? ? importance scores. Furthermore, we employ a gate operation to control knowledge retention, wherein top-? ? importance scores are normalized as a gate vector,</p><formula xml:id="formula_19">? ???? ? = ? ???? ? ? Norm ? ???? ? , (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where ? is broadcasted element-wise product, and we select Softmax as Norm function. Note the gate operation also enables gradients to flow into projection vector p ? , which makes p ? learnable via back-propagation <ref type="bibr" target="#b7">[8]</ref>. With ?</p><formula xml:id="formula_21">???? ? = {? 1,???? ? , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, ?</head><p>? ???? ,???? ? } obtained, we apply a permutation-invariant readout operation to derive integrated representation of the dynamic pricing agents,</p><formula xml:id="formula_22">? ? = ? ???? ?? ?=1 ? ?,???? ? ? ???? max ?=1 ? ?,???? ? . (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>Similarly, we can obtain integrated representation ? ? of the fixed pricing agents except with different learnable projection vector and remaining agents number. In this way, the latent representation of whole charging market is derived by</p><formula xml:id="formula_24">? = [? ? ? ? ? ] .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Graph Contrastive</head><p>Learning. An immediate problem is how to train HGP to derive an effective latent representation of charging market. A straightforward method is to update HGP by RL objective. However, RL algorithms optimize agent policy by the feedback reward from environment, which is inherently intractable compared with supervised learning. It's conceivably non-trivial for RL to learn an effective latent representation from high-dimensional input <ref type="bibr" target="#b15">[16]</ref>. Inspired by <ref type="bibr" target="#b13">[14]</ref> that introduces image contrastive learning into RL to learn latent semantic state representation from highdimensional pixels, we propose a Graph Contrastive Learning (GCL) objective as an auxiliary task to facilitate the latent state action representation learning from high-dimensional charging market. Specifically, given a query instance ? ? , a positive instance ? + , and ? -1 negative instances H -= {? 1 -, . . . , ? ?-1 -}, our contrastive learning aims to enforce query instance ? ? to match with ? + more than any of negative one ? ? -? H -so as to learn a discriminating representation for instances. We derive query instance ? ? by performing HGP on subgraph ? ? ? G ? , which is formulated as ? ? = HGP(? ? ).</p><p>(17) We choose the location ? ? of charging request ? ? as a center, and select the nearest top-? ? agents to ? ? and their corresponding edges to make up the subgraph ? ? . It looks like clipping a subarea from the geographical map, which is illustrated in Figure <ref type="figure" target="#fig_12">8</ref>. Likewise, subgraph ? + of positive instance ? + is clipped from the same graph G ? with ? ? but choosing a random center location, and subgraph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>-of ? ? -is randomly clipped from other graph (in the batch). ? + and ? ? -are obtained the same as Eq. ( <ref type="formula">17</ref>). Then we adopt the InfoNCE loss <ref type="bibr" target="#b28">[29]</ref> to optimize GCL objective,</p><formula xml:id="formula_25">? ? = -log exp ? ? ? W ? ? + exp ? ? ? W ? ? + + ?-1 ?=1 exp ? ? ? W ? ? ? - ,<label>(18)</label></formula><p>where we employ a bilinear product to evaluate the similarity of pairwise instances, and W ? are learnable parameters. Objective ? ? will be as an auxiliary task to be jointly optimized with RL objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-Agent Policy Learning</head><p>We improve Multi-Agent Deep Deterministic Policy Gradient (MAD-DPG) <ref type="bibr" target="#b22">[23]</ref>, a well-known multi-agent actor-critic method for continuous action space, by our designed DMAM approach and CHGPR module to achieve effective multi-agent policy learning. Given the joint observation ? ? and dynamic heterogeneous graph G ? at step ?, we update MVHGAT module and meta generator module by maximizing the estimated expected return,</p><formula xml:id="formula_26">? (? ? , ? ? ) = E ? ? ,G ? ?D ? ? (? ? )| ? ? ? =? ? ? (? ? ? ) ,<label>(19)</label></formula><p>where </p><formula xml:id="formula_27">?</formula><formula xml:id="formula_28">? ? = ? ? + ?? ? ? ? (? ? +1 )| ? ? ? +1 =? ? ? ? +1 (? ? ? +1 ) ,<label>(21)</label></formula><p>where ? ? and ? ? are the learnable parameters of critic ? ? and CHGPR module, respectively, and ?</p><formula xml:id="formula_29">? ? ? +1 , ? ? ?</formula><p>? denote target policy (actor) of ? ? and target critic with delayed parameters <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we show experiment results about overall performance, ablation study, effect on different CSO, and effect on different period. Results of parameter sensitivity are shown in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Data description. We conduct experiments on two real-world datasets, i.e., Beijing and Shanghai, which represent two major metropolises in China. The details of datasets and experiment settings are shown in Appendix A.1. We load the real-world data into a CSRSDP simulator, which is shown in Appendix B, for experiments. Implementation details of MAGC can be found in Appendix A.2. Evaluation metrics. We adopt the overall Profits, the CSV, and the CFR (described in Section 2) of all dynamic pricing charging stations ? ? to evaluate the performance. The overall profits and the CSV are averaged by day. Baselines. We compare our approach with the real TOU tariff scheme (TOU), two heuristic baselines (Station-MIN and Request-MIN), and four RL baselines (CDDPG <ref type="bibr" target="#b18">[19]</ref>, IDDPG <ref type="bibr" target="#b18">[19]</ref>, MAD-DPG <ref type="bibr" target="#b22">[23]</ref>, and HAMA <ref type="bibr" target="#b31">[32]</ref>). Details of these baselines are introduced in A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>Table <ref type="table" target="#tab_3">1</ref> reports overall performances of our method and all compared baselines on two datasets in regard to three metrics. Overall, MAGC achieves the best performance among all the baselines, and obtains a significant (69.4%, 80.1%, 64.5%) and (53.9%, 132.8%, 71.7%) improvements beyond the real TOU scheme for (Profits, CSV, CFR) on Beijing and Shanghai, respectively. Moreover, we have several observations. Firstly, all RL-based methods outperform the other baselines, which demonstrates the superiority of applying RL into CSRSDP problem. Secondly, we observe centralized RL method (CDDPG) is notably inferior to MARL methods (IDDPG, MADDPG, HAMA, MAGC) for the reason that CDDPG deeply suffers from the high-dimensional state and action problem during execution induced by large-scale agents. Thirdly, graph-based methods (HAMA, MAGC) surpass other RL-based methods, which verifies the effectiveness by introducing GNNs into our problem. However, HAMA performs worse than MAGC, because it fails to achieve a global agents' coordination during training, and cannot handle the diversity of large-scale agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We evaluate the performance of MAGC and it's four variants on both Beijing and Shanghai for three metrics.    centralized training via joint observation and action (the same as MADDPG); (4) MAGC-C removes the GCL objective. As can be seen in Figure <ref type="figure" target="#fig_5">4</ref>, removing any of components causes remarkable performance degradation. Particularly, we find a large performance degradation on all metrics by removing CHGPR module or GCL objective, which demonstrate their significant effectiveness for centralized large-scale agents policy learning. We also observe removing MVHGAT lead to a notable performance degradation on Profits and CSV, which verifies that modeling stations' interplay is helpful to improve CSO's profits and utilization of stations. By comparing MAGC with MAGC-M, it indicates effect of meta generator to generate individual customized policies for large-scale agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect on Different CSO</head><p>To evaluate the effectiveness of MAGC on different CSOs, we separately deploy MAGC on the charging stations of each CSO, and compare the performance with real TOU tariff scheme. The results on Beijing are reported in Table <ref type="table" target="#tab_4">2</ref>. As can be seen, every CSO can notably improve its profits, CSV, and CFR by adopting MAGC compared with TOU. In particular, the profits by MAGC significantly improve 69.4%, 68.8%, 72.8%, 74% over TOU for each of CSOs, respectively. The above results demonstrate the generalization and effectiveness of MAGC on different CSOs. We further deploy MAGC on all CSOs to test what if all CSOs adopt MAGC for dynamic pricing, and introduce two additional metrics from the perspective of EV drivers' benefits, i.e., Mean Charging Price (MCP) of all charging services and Mean Charging Wait Time (MCWT, including travel time to a charging station and queuing time in it), to evaluate the overall social influence by MAGC. As shown in Table 3, the benefits of both CSOs and EV drivers are improved when all CSOs in a charging market adopting MAGC. This is because MAGC effectively balances the supplies and demands of charging resources in the whole charging market, which largely reduces the CFR of EVs, and improve the overall use of stations, therefore the overall social welfare can be improved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect on Different Period</head><p>To evaluate the effect of MAGC on different time, we split a day into six periods, each of which contains four consecutive hours, then we respectively compute the performance for each period.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> shows the results of MAGC and TOU on different period on Beijing. We can observe MAGC almost achieves a consistent improvement for all metrics at every period compared with TOU.</p><p>The improvement (absolute difference) shows an increasing trend at first and reaches peak at evening (16-20 hour), then it decreases. Such varying trend is related to the number of charging requests, which are higher during the day, and reach the top during evening rush hours. This discovery indicates that the advantages of MAGC enlarge with the growth of charging demands in charging market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Charging Station Dynamic Pricing. Previous studies on Charging Station Dynamic Pricing (CSDP) problem can be mainly categorized as Traditional Optimization Methods (TOMs) and RL-based Methods. TOMs formulate CSDP as a typical optimization problem (with various objectives and different constraints) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>, which are solved by various traditional optimization approaches, such as linear optimization approach <ref type="bibr" target="#b3">[4]</ref>, lyapunov optimization technique <ref type="bibr" target="#b11">[12]</ref>, dynamic programming <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>, genetic algorithm <ref type="bibr" target="#b19">[20]</ref>, queuing theory <ref type="bibr" target="#b44">[45]</ref>, and game theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref>. However, these methods either incur high computational overhead to obtain optimal solution, failing to meet real-time decision-making requirement in our CSRSDP problem, or require a precise model of environment, which is always difficult for complex dynamic system. Recent years, model-free RL have been used into CSDP problem to learn a long-term optimal dynamic pricing scheme in a dynamic environment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. Specifically, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref> employ RL approaches to realize a dynamic pricing scheme for maximizing long-term profits or revenues of charging station, and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> introduce RL approaches for dynamic pricing strategy development to improve the total social welfare. In particular, a few studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref> recently attempt to apply MARL for multiple CSDP management to maximize profits of charging stations. However, these RL-based methods are only applied to manage a small number of charging stations (no more than twenty <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>). In this work, we aim to enable large-scale charging stations to provide a request-specific pricing decision for each of the continuously  arising charging requests, which is a more fine-grained and realtime scenario that hasn't been studied previously, thus the above methods cannot be directly adopted for our CSRSDP problem.</p><p>Multi-Agent Reinforcement Learning. Multi-Agent Reinforcement Learning (MARL) extends typical RL to a multi-agent scenario, where multiple agents interact in a common environment and learn to accomplish a particular task. Studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> directly apply typical RL algorithms into multi-agent system to train agents independently, which fails to coordinate multiple agents and causes non-stationary environment issue <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. To address above problems induced by independent MARL, studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref> employ a Decentralized Execution with Centralized Training (DECT) architecture, where the training process is centralized for enabling agents to learn coordination and the execution is decentralized without requiring the complete information. Alternatively, some works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> develop a communication protocol to enable information transfer between agents to facilitate agents' cooperation. Particularly, a few recent studies successfully apply MARL into intelligent transportation tasks, such as traffic signal control <ref type="bibr" target="#b39">[40]</ref>, ridehailing scheduling <ref type="bibr" target="#b16">[17]</ref>, and EV charging recommendation <ref type="bibr" target="#b43">[44]</ref>. Graph Neural Networks. Graph Neural Networks (GNNs) are powerful techniques for learning from data with graph structures, which have been widely used and demonstrated its effectiveness on node-level and graph-level representation learning tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>. Besides, some studies have applied GNNs into MARL <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> to achieve effective communication or coordination among agents, which are connected to the node-level task. However, we apply GNNs to learn enhanced observation and effective state action representation for large-scale agents, which simultaneously involves both node-level and graph-level representation learning tasks. It is worth mentioning that our CHGPR is connected to <ref type="bibr" target="#b2">[3]</ref>, but we extend it to the dynamic heterogeneous graph for obtaining state action representation in MARL, and further design a GCL objective to facilitate the representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we investigated the charging station dynamic pricing for continuously arising charging requests with long-term goals of simultaneously optimizing the overall profits, CSV, and CFR. By regarding each charging station as an agent, we formulated this problem as a mixed competitive-cooperative MARL task and proposed a graph enhanced MARL framework MAGC to achieve efficient DMAM and effective CMAPL over large-scale agents. Specifically, by modeling the whole charging market as a dynamic heterogeneous graph, we devised a MVHGAT to integrate complex interplay between agents induced by their diversified relationships. Then a shared meta generator was proposed to generate individual customized dynamic pricing policies for large-scale agents based on their extracted meta characteristics. Furthermore, we designed a CHGPR module to learn a condensed yet effective state action representation to facilitate multi-agent policy learning. Extensive experiments on two real-world datasets demonstrated the effectiveness of MAGC and empirically showed that the overall social welfare is improved if all CSOs in a charging market embrace MAGC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>We choose ? ? = 2km, ? ? = 5km in MVHGAT to construct relationships from station-centric and request-centric views, respectively. We separately keep the most top ? ? = 15% important agents for ? ? and ? ? in HGP, select the nearest top ? ? = 80% agents and ? = 32 in GCL, set ? = 2.3 in Eq. ( <ref type="formula" target="#formula_15">10</ref>) and ? = 0.5 in Eq. <ref type="bibr" target="#b19">(20)</ref>. The dimensions of representations ? ? ? and ? ? ? in MVHGAT are 16, and dimensions of H ? ? in meta generator and ? in HGP are 64. We adopt Multi-Layer Perceptron (MLP) to implement hypernetworks and transformation function in the meta generator. The hypernetworks are with Tanh activation function and dimensions <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b15">16)</ref> for hidden layers, and the transformation function is with ReLU and dimensions (64, 64) for hidden and output layers. The critic is also implemented by a MLP with ReLU and dimensions (64, 64) for hidden layers. The replay buffer size is 2000, batch size is 32. The learning rates of MVHGAT, meta generator are set to 0.001, and of critic, CHGPR are 0.01, 0.1, respectively. We select discount factor ? = 0.99, and stochastic gradient descent optimizer with momentum = 0.9 for all RL algorithms. We carefully tune major hyper-parameters of all RL baselines via a grid search strategy. All RL algorithms are trained for 60 iterations and chosen the best iteration by validation set for testing. Detailed settings of simulator can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Baselines</head><p>We compare our approach with the following seven baselines: (1) TOU follows the real time-of-use tariff scheme to offer charging prices. (2) Station-MIN lets each charging station use the cheapest charging price of stations within 2km to itself (i.e., station-centric) as the dynamic prices. (3) Request-MIN lets each charging station use the cheapest price of stations within 5km to the charging request (i.e., request-centric) as the dynamic prices. (4) CDDPG <ref type="bibr" target="#b18">[19]</ref> is a centralized policy gradient approach for continuous action, where all charging stations are controlled by a centralized agent. CDDPG generates real-time prices for all dynamic pricing charging stations based on joint observation of all charging stations. The actor and critic networks are implemented by 3 layers MLP with the same ReLU activation and dimensions (256, 256) and (256, 64), respectively, for hidden layers. The learning rates for actor and critic are set to 1e-4 and 1e-2, respectively. Other hyper-parameter settings are the same as MAGC. ( <ref type="formula">5</ref>) IDDPG <ref type="bibr" target="#b18">[19]</ref> directly applies DDPG into MARL task by training agents independently. The actor and critic networks are implemented by 3 layers MLP with the same ReLU activation and dimensions (64, 64) for hidden layers. The inputs of actor and critic are agent-specific observation and observation action. The learning rates for actor and critic are set to 1e-3 and 1e-2, respectively. Other hyper-parameter settings are the same as MAGC. ( <ref type="formula" target="#formula_11">6</ref>) MADDPG <ref type="bibr" target="#b22">[23]</ref> is an effective MARL algorithm that extents DDPG to mixed competitive-cooperative multi-agent task. The actor and critic networks are implemented by 3 layers MLP with the same ReLU activation and dimensions (64, 64), (256, 64), respectively, for hidden layers. The actor takes action based on agent-specific observation, but the critic can access joint observation and joint action during training. Other hyper-parameter settings are the same as IDDPG. To generalize MADDPG into our large-scale agent system, we share actor and critic networks among all agents. (7) HAMA <ref type="bibr" target="#b31">[32]</ref> is a state-of-the-art MARL algorithm for mixed competitive-cooperative task, which proposes a hierarchical graph attention networks to integrate information of adjacent agents from different groups. To apply HAMA into our problem, we regard the charging stations of the same CSO as a group, and the adjacent agents are defined the same as our MVHGAT. Other hyper-parameter settings are the same as IDDPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Parameter Sensitivity</head><p>We conduct experiments on Beijing to study the hyper-parameters sensitivity of MAGC. Parameters ? ? and ? ? are geographical distance thresholds used to construct relationships in MVHGAT from stationcentric and request-centric views, respectively. Parameter ? ? is the number of remaining agents in HGP, and ? ? is the number of selected agents in GCL.</p><p>As can be seen in Figure <ref type="figure" target="#fig_10">6</ref>(a) and Figure <ref type="figure" target="#fig_10">6</ref>(b), setting ? ? = 2km and ? ? = 5km achieves the best performance on profits. Further increasing or decreasing the distance thresholds leads to performance degradation. This is because a too longer distance threshold introduces extra noises in agents' interplay modeling, whereas a shorter distance threshold limits agents' perception fields. Note that decreasing the distance thresholds to 0 means discarding the relationships of a view, causing notable performance degradation for MAGC, which demonstrates the effectiveness of multi-view relationships construction.</p><p>From Figure <ref type="figure" target="#fig_10">6</ref>(c), we can observe MAGC achieves the best performance on profits when ? ? = 15%. The performance degrades when further decrease or increase ? ? . This is because too few remaining agents are insufficient to represent the whole charging market,   As can be seen in Figure <ref type="figure" target="#fig_10">6</ref>(d), MAGC achieves the best performance on profits when set ? ? = 80%. The performance degrades when further decrease or increase ? ? . On possible reason is that a small ? ? results in too large difference between query and positive instances, which is too hard as an auxiliary task for RL to match them <ref type="bibr" target="#b13">[14]</ref>. However, a too large ? ? makes GCL objective easily be optimized and probably converge to a trivial solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SIMULATOR DESIGN</head><p>Our simulator can be divided into two major components: (1) Demand response model of EVs; (2) Charging market simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Demand Response Model</head><p>In reality, an EV (charging request) will elastically respond various relevant factors to select which station for charging. Following many previous studies that assume an EV behaves rationally to minimize its charging costs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>, we introduce a demand response model,</p><formula xml:id="formula_30">? ? = M (? ? ),<label>(22)</label></formula><p>where ? ? ? R ? is a vector that represents the integrated charging costs for the EV ? ? to charge at each of charging stations. The charging costs are composed of charging price and Charging Wait Time (CWT, including travel time to the charging station and queuing time in it). ? ? will select the lowest-cost charging station ? ? = argmin(? ? ) for charging. Since the charging costs (e.g., CWT) to charge at a station is not determinate until ? ? is finished, it will be very computational expensive to compute ? ? for each ? ? by directly simulating all possible situations. Thus, we train a MLP as the model M to estimate the charging costs. Specifically, we first simulate to generate abundant training samples (X, Y) by letting each EV randomly select a charging station ? ? for charging, where each of X includes issued time of a ? ? , and charging price, supply, future demand, power, the number of traveling EVs of ? ? , and distance, ETA from location of ? ? to ? ? , and each of Y is simulated charging costs that ? ? charges at ? ? . Then model M is trained by taking X as input to fit Y, that is minimizing the following mean squared error loss,</p><formula xml:id="formula_31">? M = (M (X) -Y) 2 . (<label>23</label></formula><formula xml:id="formula_32">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Charging Market Simulation</head><p>We modify and extend an electric vehicle charging recommendation simulator developed by <ref type="bibr" target="#b43">[44]</ref> for our CSRSDP simulation, where the  Initially, the prices, charging powers, basic supplies of charging stations and charging requests of a day from the real-world dataset are loaded into simulator. Then two major processes are simulated.</p><p>(1) Departure and Arrival of EVs. We take a charging station ? ? for explanation. If there are EVs' departure, ? ? will free corresponding number of charging spots. For an arrived EV, if ? ? currently has available charging spots, this EV can successfully charge at ? ? and block one charging spot in a duration obeying a Gaussian distribution associated to the charging power of ? ? , and the environment will return a reward in terms of the reward function Eq. (1) (? ? = 0.1). If ? ? is full, this EV has to queue until there is an available charging spot, or fails to charge when the CWT exceeds a predefined threshold (45 minutes). In implementation, the number of available charging spots can be a negative number, indicating how many EVs are queuing at the station for charging.</p><p>(2) Dynamic Pricing for Charging Requests. Each charging station will offer a real-time charging price in terms of their respective pricing schemes once there is a charging request issued by an EV. Then this EV will select the lowest-cost charging station for charging based on the demand response model M.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of CSRSDP problem. A charging request is issued at 15:01:25, and different CSOs are denoted by different colors.</figDesc><graphic url="image-1.png" coords="2,54.78,86.51,235.45,124.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework overview of MAGC.</figDesc><graphic url="image-2.png" coords="3,328.85,83.69,216.21,193.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diversified relationships among agents. Charging station with different color belongs to different CSO.</figDesc><graphic url="image-3.png" coords="4,325.24,83.69,223.40,122.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>MAGC-G removes the MVHGAT module in DMAM and CHGPR; (2) MAGC-M removes the meta generator, and all of agents use a generalized policy; (3) MAGC-P removes the CHGPR module, and achieves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation studies on two datasets. "B" and "S" denote Beijing and Shanghai, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect on different period.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Effect of ? ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Effect of ? ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Parameter sensitivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An illustration of heterogeneous graph pooling.</figDesc><graphic url="image-4.png" coords="11,318.04,195.45,237.82,110.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Clipping a subgraph by selecting the nearest top-? ? charging stations to the location of the charging request. ETA is computed by an open API 2 , and the number of forthcoming charging requests (i.e., future 15 minutes demands) around stations are predicted in test phase by a well-trained MLP model.Initially, the prices, charging powers, basic supplies of charging stations and charging requests of a day from the real-world dataset are loaded into simulator. Then two major processes are simulated.(1) Departure and Arrival of EVs. We take a charging station ? ? for explanation. If there are EVs' departure, ? ? will free corresponding number of charging spots. For an arrived EV, if ? ? currently has available charging spots, this EV can successfully charge at ? ? and block one charging spot in a duration obeying a Gaussian distribution associated to the charging power of ? ? , and the environment will return a reward in terms of the reward function Eq. (1) (? ? = 0.1). If ? ? is full, this EV has to queue until there is an available charging spot, or fails to charge when the CWT exceeds a predefined threshold (45 minutes). In implementation, the number of available charging spots can be a negative number, indicating how many EVs are queuing at the station for charging.(2) Dynamic Pricing for Charging Requests. Each charging station will offer a real-time charging price in terms of their respective pricing schemes once there is a charging request issued by an EV. Then this EV will select the lowest-cost charging station for charging based on the demand response model M.</figDesc><graphic url="image-5.png" coords="11,322.84,326.60,228.23,72.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>State transition T . With the state ? ? and joint action ? ? at step ?, the environment induces a state transition to next state ? ? +1 at step ? + 1 in accordance with the state transition function T (? ? +1 |? ? , ? ?</figDesc><table /><note><p>2 ? , . . . , ? ? ? } ? A.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? ? are learnable parameters. The second stage models interplay based on cooperative relationships. Given ? ? ? ? ? , ? ? ? ? ? , ? ? ? = [? ? ?? ? ? ], we first employ attention operation to compute adaptive edge weight ? ) of edge, which is similar to Eq. (4) with different learnable parameters. Once edges weights are obtained, we derive integrated representation of cooperative adjacent agents via adaptively propagating information on ? ? by the graph convolution operation,</figDesc><table><row><cell>? }</cell><cell cols="2">?? ? ?N ? ?,?</cell><cell>?</cell><cell>? ? ?,? ? ? ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?,? based</cell></row><row><cell cols="2">on representations ? ? ? and ?</cell><cell cols="3">? ? of connected nodes and attributes</cell></row><row><cell>? ? ?,? ? (?</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p>? </p>? ? ? (? ? ? ?,? ) ,</p><ref type="bibr" target="#b4">(5)</ref> </p>where W</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? and ? ? are learnable parameters of MVHGAT and meta generator, respectively, D is the experience replay buffer containing transition tuples (? ? , ? ? , G ? , ? ? +1 , G ? +1 , ? ? ), ? ? is obtained by Eq. (16) at step ?,and ? ? is a centralized critic shared by all agents to estimate the expected return. As the centralized critic perceives global information through a condensed informative representation of all agents, it well handles both the non-stationary issue induced by fully distributed MARL approaches and the curse of dimensionality issue induced by high-dimensional information of large-scale agents, and simultaneously achieves agents' global coordination. The centralized critic ? ? and CHGPR are updated by jointly minimizing RL objective and GCL objective, ?(? ? , ? ? ) = E ? ? ,? ? ,G ? ,? ? +1 ,G ? +1 ,? ? ?D (? ? (? ? ) -? ? ) 2 +?? ? ,<ref type="bibr" target="#b19">(20)</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Overall performance on Beijing and Shanghai.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Beijing</cell><cell></cell><cell></cell><cell>Shanghai</cell></row><row><cell cols="2">Algorithm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Profits</cell><cell>CSV</cell><cell>CFR</cell><cell>Profits</cell><cell>CSV</cell><cell>CFR</cell></row><row><cell cols="2">TOU</cell><cell></cell><cell>30861</cell><cell>991</cell><cell>20.3%</cell><cell>18537</cell><cell>534</cell><cell>16.6%</cell></row><row><cell cols="3">Station-MIN</cell><cell>37047</cell><cell>1223</cell><cell>17.9%</cell><cell>21690</cell><cell>819</cell><cell>18.5%</cell></row><row><cell cols="3">Request-MIN</cell><cell>33159</cell><cell>1370</cell><cell>16.0%</cell><cell>21469</cell><cell>1185</cell><cell>14.1%</cell></row><row><cell cols="2">CDDPG</cell><cell></cell><cell>40935</cell><cell>1123</cell><cell>15.2%</cell><cell>24062</cell><cell>684</cell><cell>13.0%</cell></row><row><cell cols="2">IDDPG</cell><cell></cell><cell>44706</cell><cell>1306</cell><cell>13.8%</cell><cell>25523</cell><cell>960</cell><cell>11.8%</cell></row><row><cell cols="2">MADDPG</cell><cell></cell><cell>46163</cell><cell>1419</cell><cell>12.2%</cell><cell>25643</cell><cell>1033</cell><cell>10.8%</cell></row><row><cell cols="2">HAMA</cell><cell></cell><cell>47104</cell><cell>1500</cell><cell>11.5%</cell><cell>26302</cell><cell>1075</cell><cell>8.8%</cell></row><row><cell cols="2">MAGC</cell><cell></cell><cell cols="2">52273 1785</cell><cell>7.2%</cell><cell cols="2">28520 1243</cell><cell>4.7%</cell></row><row><cell>4.</cell><cell>BEIJING</cell><cell cols="2">SHANGHAI</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Effect on different CSO using MAGC.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TOU</cell><cell></cell><cell></cell><cell>MAGC</cell><cell></cell></row><row><cell cols="2">Operator # of CS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">Profits CSV CFR Profits CSV CFR</cell></row><row><cell>#1</cell><cell>123</cell><cell cols="6">30861 991 20.3% 52273 1785 7.2%</cell></row><row><cell>#2</cell><cell>446</cell><cell cols="6">40102 1811 18.7% 67677 2380 6.1%</cell></row><row><cell>#3</cell><cell>16</cell><cell>2220</cell><cell>82</cell><cell cols="2">35.8% 4148</cell><cell cols="2">162 29.0%</cell></row><row><cell>#4</cell><cell>11</cell><cell>292</cell><cell>15</cell><cell>64.5%</cell><cell>508</cell><cell>17</cell><cell>39.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effect of all CSOs in a charging market using MAGC.</figDesc><table><row><cell cols="3">Algorithm Profits CSV CFR MCP MCWT</cell></row><row><cell>TOU</cell><cell>73475 2899 20.4% 1.494</cell><cell>18.24</cell></row><row><cell>MAGC</cell><cell>85653 3474 4.6% 1.476</cell><cell>10.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Description</cell><cell>Beijing</cell><cell>Shanghai</cell></row><row><cell>Time scope</cell><cell cols="2">May 18, 2019 -July 01, 2019</cell></row><row><cell># of CSOs</cell><cell>4</cell><cell>4</cell></row><row><cell># of all charging stations</cell><cell>596</cell><cell>367</cell></row><row><cell># of station of CSOs</cell><cell>123 / 446 / 16 / 11</cell><cell>116 / 209 / 24 / 18</cell></row><row><cell># of supply records</cell><cell>38,620,800</cell><cell>23,781,600</cell></row><row><cell># of charging requests</cell><cell>152,889</cell><cell>87,142</cell></row><row><cell cols="3">A SUPPLEMENTARY EXPERIMENTS</cell></row><row><cell cols="2">A.1 Data Description</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>summarizes the statistics of datasets Beijing and Shanghai. Both datasets are collected from one of the largest online map providers<ref type="bibr" target="#b20">[21]</ref> in the world and the scope of them ranges from May 18, 2019 to July 01, 2019. We take the first 28 days' data for training, the following 1 day's data for validation, and the rest 16 days' data for testing. We split each city into a set of 1 ? 1km 2 disjoint grids, then aggregate the number of future 15 minutes charging requests of stations' (nine) surrounding grids as the corresponding stations' future demands. We totally involve 4, 4 CSOs and 596, 367 charging stations in Beijing and Shanghai, respectively. We choose all charging stations of a typical commercial CSO, which has 123 and 116 stations in Beijing and Shanghai, respectively, to develop dynamic pricing policy for the major experiments, and the other charging stations keep their original (TOU) pricing policies. We also evaluate the performance of MAGC on different CSOs for dynamic pricing, which is shown in Section 4.4.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Agents' policies are updated and changing with training progressing, then the environment becomes non-stationary from the perspective of any individual agent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://lbsyun.baidu.com/index.php?title=webapi</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No.<rs type="grantNumber">62102110</rs>, the <rs type="funder">Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</rs> (<rs type="grantNumber">HZQB-KCZYB-2020083</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No.<rs type="grantNumber">61960206008</rs>, and <rs type="funder">Foshan HKUST Projects</rs> (<rs type="grantNumber">FSUST21-FYTRI01A</rs>, <rs type="grantNumber">FSUST21-FYTRI02A</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kjMMUdQ">
					<idno type="grant-number">62102110</idno>
				</org>
				<org type="funding" xml:id="_sUJ7DG8">
					<idno type="grant-number">HZQB-KCZYB-2020083</idno>
				</org>
				<org type="funding" xml:id="_3wFjx3r">
					<idno type="grant-number">61960206008</idno>
				</org>
				<org type="funding" xml:id="_Zw2z6NG">
					<idno type="grant-number">FSUST21-FYTRI01A</idno>
				</org>
				<org type="funding" xml:id="_Tth4tBn">
					<idno type="grant-number">FSUST21-FYTRI02A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic Pricing for Differentiated PEV Charging Services Using Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdalrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy trading with dynamic pricing for electric vehicles in a smart city environment</title>
		<author>
			<persName><forename type="first">Gagangeet</forename><surname>Singh Aujla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName><forename type="first">C?t?lina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Jovanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic price vector formation modelbased automatic demand response strategy for PV-assisted EV charging stations</title>
		<author>
			<persName><forename type="first">Qifang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bri-Mathias</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miadreza</forename><surname>Shafie-Khah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o Ps</forename><surname>Catal?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="page" from="2903" to="2915" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic pricing for electric vehicle extreme fast charging</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="531" to="541" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counterfactual Multi-Agent Policy Gradients</title>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2974" to="2982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph Convolutional Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jiechuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic pricing, scheduling, and energy management for profit maximization in PHEV charging stations</title>
		<author>
			<persName><forename type="first">Yeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongho</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="page" from="1011" to="1026" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5639" to="5650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic pricing and energy management for profit maximization in multiple smart electric vehicle charging stations: A privacy-preserving deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Sangyoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dae-Hyun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="page">117754</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">State representation learning for control: An overview</title>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>D?az-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Franois</forename><surname>Goudou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="379" to="392" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient ridesharing order dispatching with mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Minne</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guobin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="983" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jayesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="764" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Timothy P Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Peak load reduction through dynamic pricing for electric vehicle charging</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Limmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Rodemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical Power &amp; Energy Systems</title>
		<imprint>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hydra: A personalized and context-aware multi-modal transportation recommendation system</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panpan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2314" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-agent game abstraction via graph attention neural network</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7211" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
	<note>OpenAI Pieter Abbeel, and Igor Mordatch</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic dynamic pricing for EV charging stations with renewable integration and energy storage</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yih-Fang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<biblScope unit="page" from="1494" to="1505" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems</title>
		<author>
			<persName><forename type="first">Laetitia</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadine</forename><forename type="middle">Le</forename><surname>Fort-Piat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An online reinforcement learning approach for dynamic pricing of electric vehicle charging stations</title>
		<author>
			<persName><forename type="first">Valeh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirmehdi</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Parlevliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Shahnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="130305" to="130313" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revenue Maximization for Electric Vehicle Charging Service Providers Using Sequential Dynamic Pricing</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Mrkos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton?n</forename><surname>Komenda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Jakob</surname></persName>
		</author>
		<idno>AAMAS. 832-840</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Agent Graph-Attention Communication and Teaming</title>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Paleja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gombolay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="964" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhanced coordinated operations of electric power and transportation networks via EV charging services</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengcheng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Smart Grid</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Xiuli Wang, and Mohammad Shahidehpour</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic with hierarchical graph attention network</title>
		<author>
			<persName><forename type="first">Heechang</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7236" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Agent Deep Reinforcement Learning Method for EV Charging Station Game</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shahidehpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengcheng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ardi</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorian</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristjan</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaan</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reinforcement learning for demand response: A review of algorithms and modeling techniques</title>
		<author>
			<persName><forename type="first">Zolt?n</forename><surname>Jos? R V?zquez-Canteli</surname></persName>
		</author>
		<author>
			<persName><surname>Nagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied energy</title>
		<imprint>
			<biblScope unit="page" from="1072" to="1089" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinforcement learning for real-time pricing and scheduling control in EV charging stations</title>
		<author>
			<persName><forename type="first">Shuoyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzhi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjun Angela</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="page" from="849" to="859" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Colight: Learning network-level cooperation for traffic signal control</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huichu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinshi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chacha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1913" to="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimal pricing for efficient electric vehicle charging station management</title>
		<author>
			<persName><forename type="first">Yanhai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeng</forename><forename type="middle">Chai</forename><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="749" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minne</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5571" to="5580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1856" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimal charging scheduling by pricing for EV charging station with dual charging modes</title>
		<author>
			<persName><forename type="first">Yongmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="page" from="3386" to="3396" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
