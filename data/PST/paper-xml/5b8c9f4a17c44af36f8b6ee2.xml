<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<email>zhangwei.hi@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Huiyou</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Dong</surname></persName>
							<email>ledong@uestc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C918248DDE733E34E5523E9F1255EC3D</idno>
					<idno type="DOI">10.1145/3240508.3240636</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image rain streaks removal has recently witnessed substantial progress due to the development of deep convolutional neural networks. However, existing deep learning based methods either focus on the entrance and exit of the network by decomposing the input image into high and low frequency information and employing residual learning to reduce the mapping range, or focus on the introduction of cascaded learning scheme to decompose the task of rain streaks removal into multi-stages. These methods treat the convolutional neural network as an encapsulated end-to-end mapping module without deepening into the rationality and superiority of neural network design. In this paper, we delve into an effective endto-end neural network structure for stronger feature expression and spatial correlation learning. Specifically, we propose a non-locally enhanced encoder-decoder network framework, which consists of a pooling indices embedded encoder-decoder network to efficiently learn increasingly abstract feature representation for more accurate rain streaks modeling while perfectly preserving the image detail. The proposed encoder-decoder framework is composed of a series of non-locally enhanced dense blocks that are designed to not only fully exploit hierarchical features from all the convolutional layers but also well capture the long-distance dependencies and structural information. Extensive experiments on synthetic and real datasets demonstrate that the proposed method can effectively remove rainstreaks on rainy image of various densities while well preserving the image details, which achieves significant improvements over the recent state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Images with rain streaks are often captured by outdoor surveillance equipments, which may significantly degrade the performance of some existing computer vision systems and may also result in a pool visual experience for some multimedia applications. Automatic rain streaks removal has thus become a crucial research task in the field of computer vision and multimedia, and has been successfully applied in the fields of driverless technology <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref> and content based image editing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>. The research on visual de-raining can be traced back to the last decade. Most of the early research focused on the removal of rain streaks in video sequences captured with static cameras <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>. They mostly attempted to solve the problem by exploiting the temporal correlation in the luminance domain between successive frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39]</ref>. Due to the lack of temporal information, de-raining on single image is more ill-posed, however, it has received widespread research attention due to its greater practicality and challenge <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. Traditional methods on single image de-raining explore certain prior information on physical characteristics of rain streaks and model it as a signal separation problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, or directly regard it as an image filtering problem and solve by resorting to nonlocal mean smoothing <ref type="bibr" target="#b13">[14]</ref>. However, since these models are based on handcrafted low-level feature and fixed a priori rain streaks assumptions, they can only cope with rain drops of specific shapes, scales and density, and can easily lead to the destruction of image details which are similar to rain streaks.</p><p>In recent years, due to the powerful feature representation and end-to-end data inference capabilities, deep convolutional neural networks have been widely applied to single image de-raining and have achieved significant performance improvement. These methods generally model the problem as a pixel-wise image regression process which directly learns to map an input rainy image to its clean version or a negative residual map in an end-to-end mode through a series of convolution, pooling, and non-linear operations, etc. Although considerable progress has been made in comparison with traditional methods, existing deep models still suffer from several limitations. Firstly, most of the deep CNN based models emulate the experience of low-level image processing such as image denoising, super-resolution and filtering, design shallow neural network structure, and maintain a constant feature map resolution during network propagation. As the size of the network receptive field is limited, the pixel value inference of each spatial location only relies on small local surrounding regions, it is usually arduous to remove longer rain streaks (e.g. third row of Fig. <ref type="figure" target="#fig_0">1</ref>). Moreover, due to the ignorance of long-distance spatial context modeling, these models often have difficulty in accurately filling raindrop-removed image content while detecting heavy rain streaks, resulting in an often overly blurred result, especially on texture-rich edges (e.g. first row in Fig. <ref type="figure" target="#fig_0">1</ref>). Although various deep CNN based solutions have been proposed, existing efforts either focus on the entrance of the networks by decomposing the input image into high and low frequency information <ref type="bibr" target="#b5">[6]</ref> or design cascaded learning schemes to decompose the task of rain removal into multi-stages <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. A contextualized dilated network is proposed in <ref type="bibr" target="#b34">[35]</ref> to aggregate context information from three scales of receptive filed for more effective rain streak feature learning. All of these methods use convolutional neural network as an encapsulated end-to-end mapping module without deepening into the rationality and superiority of neural network design towards more effective rain streaks removal.</p><p>Inspired by the adaptive nonlocal means filter <ref type="bibr" target="#b13">[14]</ref> for efficient single-image rain streaks removal, we proposed to incorporate nonlocal operation <ref type="bibr" target="#b31">[32]</ref> to the design of our end-to-end de-raining network framework. The non-local operation computes the feature response at a spatial position as a weighted sum of the features at a specific range of positions in the considered feature maps <ref type="bibr" target="#b31">[32]</ref>. Specifically, we propose a non-locally enhanced encoder-decoder network framework for single-image de-raining. The core architecture of our trainable de-raining engine is a concatenation of an encoder network and a corresponding decoder network. It is designed to be a symmetrical structure and both the encoder and the decoder network are composed of three cascaded non-locally enhanced dense blocks (abbr. NEDB). Each NEDB is designed as a residual learning module which contains a non-local feature map weighting followed by four densely connected convolution layers for hierarchical feature encoding and another convolution layer for residual inference. Moreover, we introduce the pooling striding mechanism in our encoder network to learn increasingly abstract feature representation, which results in a decrease in resolution with the enlargement of receptive filed. We further incorporate pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling in our decoder, which helps to preserve the structure and details in the resulted image.</p><p>In summary, this paper has the following contributions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Single Image De-raining</head><p>Single image de-raining is a challenging and highly ill-posed task.</p><p>Traditional approaches treat single image de-raining as an image decomposition problem, in which they model the rain streaks and rain-free scene lie in two separate sub-spaces. For example, Kang et al. <ref type="bibr" target="#b12">[13]</ref> and Li et al. <ref type="bibr" target="#b23">[24]</ref> rely on morphological component analysis and Gaussian mixture models (GMMs) based dictionary learning, respectively. Yu et al. <ref type="bibr" target="#b24">[25]</ref> distinguishes the dictionaries of rain streak and rain-free scene via discriminative sparse coding. Zhu et al. <ref type="bibr" target="#b40">[41]</ref> further considers rain direction in a joint optimization process. In these methods, although varies prior information of both rain streaks and rain-free images have been extensively exploited, due to the handcraft low-level feature representation and strong prior assumptions, they usually tend to overly smooth the details in rain free scenes. Recently, deep learning based approaches dominate the research of image-to-image mapping for various of computer vision tasks, such as image inpainting <ref type="bibr" target="#b22">[23]</ref>, saliency detection <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, automatic image colorization <ref type="bibr" target="#b37">[38]</ref> and image super-resolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. With the integration of several commonly used advances in network architectures such as residual connections <ref type="bibr" target="#b8">[9]</ref> and dilated convolutions <ref type="bibr" target="#b35">[36]</ref>, recent de-raining methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref> obtain impressive results by building fully convolutional architectures that learns pixel-wise mapping from rainy image to the rain-free version. Although additional considerations have been taken, such as rain  density <ref type="bibr" target="#b36">[37]</ref>, frequency domain knowledge <ref type="bibr" target="#b5">[6]</ref>, etc., they are still weak in removing long rain streaks in complex background scenes as well as distinguishing dense rain streaks with similar image patterns in rain-free ones. This is due to the fact that convolutions in existing CNN-based de-raining neural networks are inherently local operations with small range of spatial receptive field in each computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-local Networks</head><p>Very recently, non-local neural network is proposed to realize the computation of long-range dependencies <ref type="bibr" target="#b31">[32]</ref> for video classification task. In each 2D non-local operation, the response at a position is computed as a weighted sum of the features at all spatial positions. This is the first component of neural network that is able to enlarge the receptive field from neighbor positions to the entire image. Interestingly, such non-local operation is majorly inspired by traditional non-local mean filtering which is also exploited in early single image de-raining method <ref type="bibr" target="#b13">[14]</ref>. This verifies importance of the non-local enhancement in our proposed de-raining CNN engine. As far as we know, our proposed non-locally enhanced CNN architecture is the first piece of work that attempts to incorporate non-local mean calculation into a fully convolutional neural network architecture with correlation propagation for the task of pixel-wise image restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We introduce an end-to-end convolutional neural network for single image de-raining, called non-locally enhanced encoder-decoder network (NLEDN). Our framework contains a fully convolutional encoder-decoder network which has been proven able to learn complex pixel-wise mappings from large amount of input-output image pairs. The overall architecture of the proposed network is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Particularly, in order to exploit the abundant structure cues in rain streak maps and the self-similarities in rainfree nature scenes, we propose the non-locally enhanced dense block (NEDB) as the basic component in our network architecture. We carefully integrate NEDBs with both encoding and decoding layers to enable the computation of long-range spatial dependencies as well as efficient usage of the feature activation of proceeding layers. In the following sections, we introduce each component of the proposed architecture with more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entrance and Exit Layers</head><p>The proposed de-raining NLEDN takes one image with rain-streaks as input in the entrance and outputs its rain-free version in the exit. In this section, we focus on the network structure of the entrance and exit of our entire framework. Shallow Feature Extraction In the very beginning of the whole architecture, we use two convolution layers to extract shallow features of the input rainy image, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Formally, we have</p><formula xml:id="formula_0">F 0 = H 0 (I 0 ),<label>(1)</label></formula><p>where I 0 and H 0 (•) denote the input rainy image and the first shallow feature extraction convolution layer, respectively. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, using the long-range skip-connections which bypass intermediate layers, we link both the input image I 0 and the shallow features F 0 with layers that are close to the exit of the whole network. The benefits of such skip-connections here are two folds: first, they provide long-range information compensation such that raw pixel values and low level feature activation are still available in the very end of the whole architecture; Second, they enable the residual learning that facilitates gradient back-propagation and the pixel-wise prediction. Next, the shallow features F 0 is fed into the second convolution layer H 1 (•) to obtain shallow features F 1 :</p><formula xml:id="formula_1">F 1 = H 1 (F 0 ),<label>(2)</label></formula><p>F 1 is used as the input to the subsequent encoding layers.</p><p>Exit layers As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the input image I 0 and the shallow features F 0 are gradually added to the feature activation of layers near the exit of the whole architecture. Particularly, one tanh layer is adopted as the nonlinear unit of the final convolution layer to obtain a rain map R with pixel value within (-1, 1). The final rain-free image Ŷ can be computed via</p><formula xml:id="formula_2">Ŷ = I 0 + R,<label>(3)</label></formula><p>Noted that the architecture of entrance and exit layers here are not unique, we choose such architecture in order to effectively incorporate our core modules, the non-locally enhanced encoding and decoding layers, which will be elaborated in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-locally Enhanced Encoding and Decoding</head><p>Conventional encoding-decoding networks are widely used in imageto-image translation or other pixel-wise prediction tasks. Here, the proposed architecture can be regarded as an enhanced version with non-local operations and dense connections. To achieve this, a novel enhanced dense block (NEDB) is plugged into each stage of both the encoder and the decoder.</p><p>Non-locally Enhanced Dense Block (NEDB) The detailed architecture of the proposed NEDB is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Specifically, we denote the input feature activation to the NEDB as F n , which has the spatial dimension of H n × W n × C n . The pair-wise function f that calculates the pair-wise relationship is defined as</p><formula xml:id="formula_3">f (F n,i , F n, j ) = θ (F n,i ) T ϕ (F n, j )<label>(4)</label></formula><p>where F n,i , F n, j denote the feature activation F n at position i, j respectively. θ (•) and ϕ (•) are two feature embedding operations with different learned parameters W θ and W ϕ , denoted as θ (F n,i ) = W θ F n,i and ϕ (F n,i ) = W ϕ F n,i . Following <ref type="bibr" target="#b31">[32]</ref>, we define the nonlocal operation in NEDB as</p><formula xml:id="formula_4">y n,i = 1 C (F ) ∀j f (F n,i , F n, j )д(F n, j )<label>(5)</label></formula><p>where д(•) is the unary function that computes the representation of F n while C (F ) is the normalization factor, defined as C (F ) = ∀j f (F n,i , F n, j ). In this way, the feature representation is nonlocally enhanced via considering all positions (∀j) for each location i.</p><p>Next, the non-locally enhanced feature activation is fed to five consecutive convolution layers which are densely connected. Specifically, following <ref type="bibr" target="#b10">[11]</ref>, we adopt direct connections from each layer to all subsequent layers. The architecture is shown in Figure <ref type="figure" target="#fig_2">3</ref>. Hence, the l t h layer receives the feature activations from all preceding layers, D 0 ,...,D l -1 as input:</p><formula xml:id="formula_5">D l = H l ([D 0 , ..., D l -1 ])<label>(6)</label></formula><p>where [D 0 ,...,D l -1 ] denotes the concatenation of the feature activations produced in layer 0,...,l -1. Moreover, to avoid the notorious problem of gradients vanishing/exploding caused by an increase in the number of network layers and connections, we adopt local residual learning in the design of each NEDB. Formally, the final output of the m-th NEDB can be achieved by</p><formula xml:id="formula_6">F m = F (F m-1 ,W m ) + F m-1 ,<label>(7)</label></formula><p>where F (F m-1 ,W m ) represents the residual mapping to be learned in the considered block, which is actually inferred from a concatenation of feature activations from all preceding layers with a 1 × 1 convolution layer, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref> and formally written as</p><formula xml:id="formula_7">F (F m-1 ,W m ) = Conv 1×1 ([D 0 , ..., D L ]) .<label>(8)</label></formula><p>Pooling Indices Guided Decoding As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the encoding part consists of three consecutive NEDBs, each of which is followed by one max-pooling layer with striding that downsamples the feature activation. Symmetrically, another three NEDBs are stacked in the decoding part, each of which is followed by one maxunpooling layer that upsamples the feature activation. Moreover, skip-connections are utilized to link feature activations of encoding layers to their counterpart in the decoding layers. Particularly, we propose to record pooling indices <ref type="bibr" target="#b0">[1]</ref> during encoding for further upsample inferring in the decoding stage. The max-unpooling layer uses the pooling indices computed in the max-pooling step of the corresponding encoding layer to perform non-linear upsampling. Given the recorded pooling index matrix, the output feature map of max-unpooling is calculated by first initializing to the size before the max pooling operation, and then assigning the feature column at each position of the input feature map to the corresponding position (given by the index matrix) and zeroing the remaining positions. The upsampled feature activations are sparse and convolved with subsequent trainable filters to produce dense feature maps. We will show by experiments that such pooling indices guided decoding scheme is more suitable here for rain streaks removal compared with conventional bilinear upsampling. Multi-scale Non-Local Enhancement With the above mentioned max-pooling and max-unpooling operations, the spatial size of intermediate feature activations gradually decreases in the encoding stage while gradually increases during decoding. Therefore, as the non-local operation in NEDB requires to compute pair-wise relations between every two spatial positions of the feature activation map, the computation burden increases dramatically when spatial dimension gets larger. To address this problem and construct a more flexible non-local enhancement across feature activations with different spatial resolution, we implement the non-local operation in a multi-scale manner when building the encoding and decoding layers.</p><p>Specifically, for the feature activation with lowest spatial resolution (e.g. F4 in Figure <ref type="figure" target="#fig_1">2</ref>), the subsequent NEDB directly works on the whole feature activation map to realize a global-level non-local enhancement. For the feature activation with higher spatial resolution, we first divide it into a grid of regions (As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the k × k NEDB indicates how the input feature map is divided before performing region-wise nonlocal operation. e.g. F 1 is divided into a grid of 8 × 8) and then let the subsequent NEDB work on the feature activation inside each region. Accordingly, such region-level nonlocal enhancement is able to prevent unacceptable computational consumption caused by directly working on high resolution feature activation. One the other hand, comparing with conventional local convolution operation which operates inside 3 × 3 or 5 × 5 windows, region-level non-local enhancement is able to retrieve long-range structural cues to facilitate better rain-streaks removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>The loss function is defined as the mean absolute error (MAE) between the resulted rain-free image and its corresponding groundtruth, which is formulated as follow:</p><formula xml:id="formula_8">L = 1 HW C i j k ∥ Ŷi, j,k -Y i, j,k ∥ 1 ,<label>(9)</label></formula><p>where H , W and C denotes the height, width and channel number of the rain-free image. Ŷ and Y denote the predicted rain-free image and ground-truth respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Criteria</head><p>We evaluate the performance of our proposed method (NLEDN) on both synthetic datasets and real data. For synthetic data, we use four benchmark datasets, including the dataset provided by Fu et al. <ref type="bibr" target="#b5">[6]</ref>, denoted as DDN-Data; the dataset synthesized by Zhang et al. <ref type="bibr" target="#b36">[37]</ref>, denoted as DIDMDN-Data; the Rain100L and the Rain100H dataset provided in <ref type="bibr" target="#b34">[35]</ref>. Specifically, the DDN-Data contains 14,000 rainy/clean image pairs, which is synthesized from 1,000 clean images with 14 kinds of different rain-streak orientations and magnitudes. Following Fu et al. <ref type="bibr" target="#b5">[6]</ref>, we select 9,100 pairs of them for training and the remaining 4,900 image pairs for evaluation. The DIDMDN-Data consists of 12,000 image pairs of three rain density levels (i.e. light, medium and heavy). There are roughly 4,000 images per rain-density level in the dataset. The rain-density labels are also provided and are used as extra data for model training in <ref type="bibr" target="#b36">[37]</ref>. Noted that we did not use this extra information in our model. Rain100L is the synthesized dataset selected from BSD200 <ref type="bibr" target="#b25">[26]</ref> with only one type of rain streaks, which consists of 200 image pairs for training and the other 100 images for testing. Compared with Rain100L, Rain100H is more challenging. It is synthesized with five streaks directions and contains 1,800 images for training plus 100 images for testing. As pointed out in <ref type="bibr" target="#b34">[35]</ref>, although some of the synthesized examples in Rain100H are inconsistent with real images, adding these data as training can further enhance the robustness of the network. For real data, we collected some of the images from the Internet and some from the released images of <ref type="bibr" target="#b36">[37]</ref>. We have also taken some real cases using our own cameras for testing. We evaluate the performance of the synthesized data using two metrics, including Peak Signal-to-Noise Ratio (PSNR) <ref type="bibr" target="#b11">[12]</ref> and Structure Similarity Index (SSIM) <ref type="bibr" target="#b32">[33]</ref>. As with existing works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>, we evaluate the results in the luminance channel (i.e. Y channel of YCbCr space), which has the most significant impact on the human visual system. As the rain-free groundtruth are not available on real-world image, we evaluate the performance on real data singly based on visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>Our proposed NLEDN has been implemented on the Pytorch framework, a flexible open source deep learning frame network. During training, we use horizontal flipping for data augmentation and resize the image to have long side smaller than 512. As the network is fully convolutional, and we set the mini-batch size to 1, the size of the input image does not have to be the same. We use adam optimizer to update the parameters of network during training. The learning rate is initially set to 0.0005, and we reduce it by 10% when the training loss stops decreasing, until 0.0001. We use a weight decay of 0.0001 and a momentum of 0.9. Because of the large differences in the size of each datasets, the time spent on training each specific model is different. It takes around 3.5 days to train a model using the training set of DDN-Data or DIDMDN-Data, and it cost around 15 hours for training on Rain100H dataset. For Rain100L dataset, it is much faster and only takes about four hours to complete a whole model training. Therefore, we conduct ablation study on Rain100L dataset in our experiment. However, as our entire model is fully convolutional, the testing process is very efficient, which only takes 1.44 seconds for the trained model to process a testing image with 512 × 512 pixels on a PC with an NVIDIA X GPU and a 3.4GHz Intel processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the-art</head><p>We compare our proposed NLEDN method against five state-ofthe-art single-image de-raining methods, including discriminative sparse coding (DSC) <ref type="bibr" target="#b24">[25]</ref>, GMM-based layer prior (GMM) <ref type="bibr" target="#b23">[24]</ref>, deep detail network (DDN) <ref type="bibr" target="#b5">[6]</ref>, joint rain detection and removal (JORDER) <ref type="bibr" target="#b34">[35]</ref> and density-aware single image de-raining using a multi-stream dense network (DID-MDN) <ref type="bibr" target="#b36">[37]</ref>. The last three are the latest deep learning based methods. As all of these methods use different data in training their models, we trained four versions of our model based on the training set of the four synthesized dataset respectively. Moreover, for fair comparison, we have also fine-tuned the released deep model of the comparison methods on each specific training set before evaluation. When evaluating on real-world data, we test four versions of the models for each deep learning based methods (including our NLEDN) on each image and select the one with best visualization as the result for comparison.</p><p>Quantitative Evaluation. We report a quantitative comparison w.r.t PSNR and SSIM in Table <ref type="table" target="#tab_2">1</ref>. As can be observed, our proposed method (NLEDN) increases the PSNR metric achieved by the existing best-performing algorithms by an average of 1.07db, Dataset Metric DSC <ref type="bibr" target="#b24">[25]</ref> (ICCV'15) GMM <ref type="bibr" target="#b23">[24]</ref> (CVPR'16) DDN <ref type="bibr" target="#b5">[6]</ref> (CVPR'17) JORDER <ref type="bibr" target="#b34">[35]</ref> (CVPR'17) DID-MDN <ref type="bibr" target="#b36">[37]</ref>    2.81db, 1.34db and 4.03db respectively on DDN-Data, DIDMDN-Data, Rain100L and Rain100H. And at the same time, our model improves the SSIM by 2.70%, 4.90%, 0.73% and 7.87% respectively on the above four datasets. We can find that, on average, the rainfree images restored by our NLEDN are consistently closer to the groundtruth than existing state-of-the-art on the synthesized datasets, and the higher SSIM value also indicates that our method can better restore the structural information of an image. Moreover, the more complex the data set, the more significant the performance of our algorithm compared to existing algorithms. Noted that JORDER <ref type="bibr" target="#b34">[35]</ref> use additionally provided rain mask and rainstreak annotation while training their models on Rain100L and Rain100H datasets and DID-MDN <ref type="bibr" target="#b36">[37]</ref> use extra rain density level information while training on their DIDMDN-Data. Nonetheless, our proposed model can still greatly outperform their results without resorting to any additional data. Qualitative Evaluation. Fig. <ref type="figure" target="#fig_4">4</ref> shows visual comparisons of rain-streaks removal results for five synthesized rainy images. The first three examples are synthetic heavy rain cases which are similar to real-world scenes, our models consistently achieves the best visualization results in terms of effectively removing the rain streaks while preserving the image structure details. The latter two are hard examples chosen from the Rain100H dataset, and may be rare in real world. As can be observed, both DDN <ref type="bibr" target="#b5">[6]</ref> and DID-MDN <ref type="bibr" target="#b36">[37]</ref>     are fail to remove rain streaks of these extreme cases though having been fine-tuned on this dataset. Although JORDER <ref type="bibr" target="#b34">[35]</ref> is designed to handle such hard cases, the restored images are over-smoothing and full of artifacts. Our proposed NLEDN generates much cleaner results with promisingly structure details preserving as it is able to fully exploit the long-range spatial context information based on gradually increased size of receptive field and the non-locally weighting scheme. Fig. <ref type="figure" target="#fig_6">5</ref> demonstrates the results of some real images with rain-streaks in various rain densities. As can be observed, our proposed model shows the best visual performance on rain-streaks removal. It is particularly effective in removing long rain-streaks while perfectly preserving the image structural details. Though the existing best-performing DID-MDN <ref type="bibr" target="#b36">[37]</ref> can remove long rain-streaks of high density to some extent, it suffers from artifacts and blurry if observed in a zoom-in view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>As discussed in Section 3, our proposed NLEDN contains two core components towards more effective rain-streaks removal, including the encoder-decoder framework with pooling striding and maxunpooling operation, and the tailored non-locally enhanced dense block with densely connected convolution layers and a non-local feature weighting scheme. To validate the effectiveness and necessity of the internal network design of each of these two modules, we exhaustively compare NLEDN with its five variants trained and tested on the Rain100L dataset. The specific performance changes in terms of PSNR and SSIM are listed in Table <ref type="table" target="#tab_3">2</ref>.  the two recent deep learning based methods DDN <ref type="bibr" target="#b5">[6]</ref> and DID-MDN <ref type="bibr" target="#b36">[37]</ref> but inferior to JORDER <ref type="bibr" target="#b34">[35]</ref>. R b adds dense connection between convolutional layers to R a . As shown in the table, adding dense connection leads to an average increase of 0.59db in terms of PSNR and 0.7% improvement on SSIM. This proves the effectiveness of dense connections. Due to space limitations, in subsequent ablation studies, we add dense connection to each NEDB by default and discuss on the role of other network components.</p><formula xml:id="formula_9">R a R b R c R d R e R f without</formula><p>R c is a simple concatenation of multiple dense blocks with neither non-local weighting nor receptive field controlling (pooling striding). For comparison, the number of blocks is set to the same as that of NLEDN. As shown in Table <ref type="table" target="#tab_3">2</ref>, simply concatenating multiple dense blocks can bring significant performance improvement, which increases the average PSNR by 1.62db while at the same time boosts the SSIM by 1% when compared to its single block version R b . This verifies the effectiveness of deeper feature representation in rain-streaks removal. In our experiments, we find that a cascade of 6 dense blocks leads to the best performance in our validation. Concatenating more than 6 dense blocks even leads to a performance deterioration. R d is directly modified on R c by adding pooling striding and corresponding skip connection between blocks guided by pooling indices. As illustrated in the table, adding a pooling indices based receptive filed controlling scheme further leads to an increase of 0.36db in terms of PSNR and a 0.3% improvement on SSIM. R e and R f focus on the effectiveness of introducing non-local feature weighting to the network design. Firstly, we directly add multi-scale non-local enhancement to R c and observe a performance boost on PSNR from 35.44db to 35.91db and an increase of 0.3% in terms of SSIM. The performance improvement is more significant when adding non-local operation to R d , which forms our full model R f (R f = NLEDN). As shown in the table, the introduction of non-local operation contributes an average of 0.77db and 0.32% improvement in terms of PSNR and SSIM respectively. This verifies the effectiveness and universality of non-local optimization for rainstreaks removal on single images. A visual comparison is provided in Fig. <ref type="figure" target="#fig_8">6</ref>. As can be seen, the model without non-local enhancement suffers from some obvious artifacts and shows powerless for most of the long rain streaks. This further proves the effectiveness of long-distance dependencies modeling in rain-streaks removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have introduced a non-locally enhanced encoderdecoder network framework for rain streaks removal from single images. It is designed as a concatenation of an encoder network followed by a corresponding decoder network, which are both composed of a series of tailored, non-locally enhanced dense blocks (NEDB). The NEDB is designed to not only fully exploit hierarchical features from densely connected convolutional layers but also well capture the long-distance dependencies and structural information by employing a non-locally weighting operation at a specific range of feature maps. Experimental results on both synthetic and real datasets have demonstrated that our proposed method can effectively remove rain-streaks on rainy image of various density while promisingly preserve the image texture similar to the rain streaks, which greatly outperforms the state-of-the-art. In our future research work, we plan to extend the proposed algorithm to a wider range of image restoration tasks, including but not limited to image denoising, image dehazing and image super-resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample examples of single image de-raining results. (a) Input images with rain-streaks. (b) Results of DDN [6] (c) Results of JORDER [35] (d) Our results. The first row is the enlargement of the selected regions of the second row, which shows the advantage of our proposed NLEDN in detail preserving. The third row demonstrates the promising result of our NLEDN in removing long rain streaks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall architecture of our proposed non-locally enhanced encoder-decoder network (NLEDN). As can be observed, the input image and low-level feature activation are linked to the very end of the whole architecture via long-range skip-connections. The core of the whole architecture is a non-locally enhanced encoder-decoder, in which novel non-locally enhanced dense blocks (NEDBs) and pooling indices guided scheme are adopted.</figDesc><graphic coords="3,104.39,274.74,53.16,78.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of our proposed non-locally enhanced dense block (NEDB). Left part shows the multi-scale input via either adopting global-level non-local enhancement which feeds the entire feature map to NEDB or dividing the feature map into a grid of regions to realize region-level non-local enhancement. Here we show by a 2 × 2 grid for convenience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visual comparison of rain-streaks removal results generated from state-of-the-art deep learning based methods (including our NLEDN) on synthesized rainy images. Our model consistently achieves the best visualization results in terms of effectively removing the rain streaks while preserving the image structure details.</figDesc><graphic coords="6,66.52,449.34,77.97,52.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visual comparison of rain-streaks removal results generated from state-of-the-art methods on real-world rainy images. Our model consistently achieves the best visualization results.</figDesc><graphic coords="7,66.45,314.46,94.02,58.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Input (b) w/o non-local weighting (c) w/ non-local weighting (d) Ground Truth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visual comparison of rain-streaks removal results with and without non-local weighting enhancement in our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>R a refers to the result of a very basic baseline which only includes the convolution layers of a single NEDB without dense connection or non-local weighting enhancement, as well as the same entrance and exit layer settings as NLEDN. Actually, it is a degenerate FCN based de-raining model with residual connection. It reaches the PSN R = 33.23db and SSIM = 0.9533 which already outperforms Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of quantitative results in terms of PSNR and SSIM on four synthesized benchmark datasets. The three best performing algorithms are marked in red, blue, and green, respectively. Our proposed NLEDN consistently achieves the best performance. ' * ′ indicates that the method uses additional data (e.g. rain density level, rain mask annotation) provided by the dataset.</figDesc><table><row><cell>(CVPR'18) Our NLEDN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on different components of our proposed non-locally enhanced encoder-decoder network framework.</figDesc><table><row><cell>dense connection?</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>single block?</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>multiple blocks?</cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>pooling striding?</cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell></row><row><cell>pooling indices?</cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell></row><row><cell>non-local operation?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell></row><row><cell>PSNR</cell><cell>33.23</cell><cell>33.82</cell><cell>35.44</cell><cell>35.80</cell><cell>35.91</cell><cell>36.57</cell></row><row><cell>SSIM</cell><cell cols="6">0.9533 0.9596 0.9691 0.9716 0.9720 0.9747</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Corresponding author is Le Dong. This work was supported by the National Natural Science Foundation of China under Grant 61702565 and Grant 61772114, the Science and Technology Planning Project of Guangdong Province under Grant 2017B010116001, Guangdong Natural Science Foundation Project for Research Teams under Grant 2017A030312006, and was also sponsored by CCF-Tencent Open Research Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation</title>
		<author>
			<persName><surname>V Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence PP</title>
		<imprint>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rain or Snow Detection in Image Sequences Through Use of a Histogram of Orientation of Streaks</title>
		<author>
			<persName><forename type="first">Jãľrãľmie</forename><surname>Bossu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hautiãĺre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Philippe</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="348" to="367" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention-Aware Face Hallucination via Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Qingxing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1656" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chiou</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1968">2014. 1968-1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Deep Convolutional Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing Rain from Single Images via a Deep Detail Network</title>
		<author>
			<persName><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1715" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and Removal of Rain from Videos</title>
		<author>
			<persName><forename type="first">Kshitiz</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shree</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When does a camera see rain?</title>
		<author>
			<persName><forename type="first">Kshitiz</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-aware single image rain removal</title>
		<author>
			<persName><surname>De-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Chun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scope of validity of PSNR in image/video quality assessment</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Thu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="800" to="801" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic single-image-based rain streaks removal via image decomposition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1742" to="1755" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single-image deraining using an adaptive nonlocal means filter</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">Young</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="914" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video Deraining and Desnowing Using Temporal Correlation and Low-Rank Matrix Completion</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">Young</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2658" to="2670" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rainy weather recognition from in-vehicle camera images for driver assistance</title>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Kurihata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomokazu</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshito</forename><surname>Mekada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukimasa</forename><surname>Tamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Miyahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium</title>
		<meeting>the IEEE Intelligent Vehicles Symposium</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Instance-level salient object segmentation</title>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flow Guided Recurrent Neural Encoder for Video Salient Object Detection</title>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3243" to="3252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep CNN features</title>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrast-Oriented Deep Neural Networks for Salient Object Detection</title>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Haofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07778</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Context-Aware Semantic Inpainting. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rain Streak Removal Using Layer Priors</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Removing Rain from a Single Image via Discriminative Sparse Coding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3397" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Utilizing local phase information to remove rain from video</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Santhaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast algorithm for rain detection and removal from videos</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-Shot&quot; Super-Resolution using Deep Internal Learning</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting image structural similarity for single image rain removal</title>
		<author>
			<persName><forename type="first">Shao-Hua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Pu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4482" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rain sensor for automatic systems on vehicles</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Vasile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Vasile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Nistor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luige</forename><surname>Vladareanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mihaela Pantazica, Florin Caldararu, Andreea Bonea, Andrei Drumea, and Ioan Plotog</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7821</biblScope>
		</imprint>
	</monogr>
	<note>Advanced Topics in Optoelectronics, Microelectronics, and Nanotechnologies V</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Non-local Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motion robust rain detection and removal from videos</title>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Workshop on Multimedia Signal Processing</title>
		<meeting>IEEE International Workshop on Multimedia Signal Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="170" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Joint Rain Detection and Removal from a Single Image</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Density-aware Single Image De-raining using a Multi-stream Dense Network</title>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic Colorization with Improved Spatial Coherence and Boundary Localization</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan-Bin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="494" to="506" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rain removal in video by combining temporal and chromatic properties</title>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Kheng Leow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teck Khim</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint Bilayer Optimization for Single-Image Rain Streak Removal</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pheng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2545" to="2553" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
