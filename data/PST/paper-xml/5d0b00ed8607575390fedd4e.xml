<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HopSkipJumpAttack: A Query-Efficient Decision-Based Attack</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
							<email>jianbochen@</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
							<email>wainwrig@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Voleon</forename><surname>Group</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HopSkipJumpAttack: A Query-Efficient Decision-Based Attack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/SP40000.2020.00045</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for 2 and ∞ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks. It also achieves competitive performance in attacking several widely-used defense mechanisms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Although deep neural networks have achieved state-of-the-art performance on a variety of tasks, they have been shown to be vulnerable to adversarial examples-that is, maliciously perturbed examples that are almost identical to original samples in human perception, but cause models to make incorrect decisions <ref type="bibr" target="#b0">[1]</ref>. The vulnerability of neural networks to adversarial examples implies a security risk in applications with real-world consequences, such as self-driving cars, robotics, financial services, and criminal justice; in addition, it highlights fundamental differences between human learning and existing machine-based systems. The study of adversarial examples is thus necessary to identify the limitation of current machine learning algorithms, provide a metric for robustness, investigate the potential risk, and suggest ways to improve the robustness of models.</p><p>Recent years have witnessed a flurry of research on the design of new algorithms for generating adversarial examples <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Adversarial examples can be categorized according to at least three different criteria: the similarity metric, the attack goal, and the threat model. Commonly used similarity metrics are p -distances between adversarial and original examples with p ∈ {0, 2, ∞}. The goal of attack is either untargeted or targeted. The goal of an untargeted attack is to perturb the input so as to cause any type of misclassification, whereas the goal of a targeted attack is to alter the decision of the model to a pre-specific target class. Changing the loss function allows for switching between two types of attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Perhaps the most important criterion in practice is the threat model, of which there are two primary types: white-box and black-box. In the white-box setting, an attacker has complete access to the model, including its structure and weights. Under this setting, the generation of adversarial examples is often formulated as an optimization problem, which is solved either via treating misclassification loss as a regularization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> or via tackling the dual as a constrained optimization problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>. In the black-box setting, an attacker can only access outputs of the target model. Based on whether one has access to the full probability or the label of a given input, black-box attacks are further divided into score-based and decision-based. See Figure <ref type="figure" target="#fig_0">1</ref> for an illustration of accessible components of the target model for each of the three threat models. Chen et al. <ref type="bibr" target="#b7">[8]</ref> and Ilyas et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> introduced score-based methods using zeroth-order gradient estimation to craft adversarial examples.</p><p>The most practical threat model is that in which an attacker has access to decisions alone. A widely studied type of the decision-based attack is transfer-based attack. Liu et al. <ref type="bibr" target="#b10">[11]</ref> showed that adversarial examples generated on an ensemble of deep neural networks from a white-box attack can be transferred to an unseen neural network. Papernot et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> proposed to train a substitute model by querying the target model. However, transfer-based attack often requires a carefully-designed substitute model, or even access to part of the training data. Moreover, they can be defended against via training on a data set augmented by adversarial examples from multiple static pre-trained models <ref type="bibr" target="#b16">[17]</ref>. In recent work, Brendel et al. <ref type="bibr" target="#b13">[14]</ref> proposed Boundary Attack, which generates adversarial examples via rejection sampling. While relying neither on training data nor on the assumption of transferability, this attack method achieves comparable performance with state-of-the-art white-box attacks such as C&amp;W attack <ref type="bibr" target="#b5">[6]</ref>. One limitation of Boundary Attack, however, is that it was formulated only for 2 -distance. Moreover, it requires a relatively large number of model queries, rendering it impractical for real-world applications.</p><p>It is more realistic to evaluate the vulnerability of a machine learning system under the decision-based attack with a limited budget of model queries. Online image classification platforms often set a limit on the allowed number of queries within a certain time period. For example, the cloud vision API from Google currently allow 1,800 requests per minute. Query inefficiency thus leads to clock-time inefficiency and prevents an attacker from carrying out large-scale attacks. A system may also be set to recognize the behavior of feeding a large number of similar queries within a small amount of time as a fraud, which will automatically filter out query-inefficient decision-based attacks. Last but not least, a smaller query budget directly implies less cost in evaluation and research. Query-efficient algorithms help save the cost of evaluating the robustness of public platforms, which incur a cost for each query made by the attacker. It also helps facilitate research in adversarial vulnerability, as such a decision-based attack which does not require access to model details may be used as a simple and efficient first step in evaluating new defense mechanisms, as we will see in Section V-B and Appendix C.</p><p>In this paper, we study decision-based attacks under an optimization framework, and propose a novel family of algorithms for generating both targeted and untargeted adversarial examples that are optimized for minimum distance with respect to either the 2 -distance or ∞ distance. The family of algorithms is iterative in nature, with each iteration involving three steps: estimation of the gradient direction, step-size search via geometric progression, and Boundary search via a binary search. Theoretical analysis has been carried out for the optimization framework and the gradient direction estimate, which not only provides insights for choosing hyperparamters, but also motivating essential steps in the proposed algorithms. We refer to the algorithm as HopSkipJumpAttack <ref type="foot" target="#foot_0">1</ref> . In summary, our contributions are the following: Roadmap. In Section II, we describe previous work on decision-based adversarial attacks and their relationship to our algorithm. We also discuss the connection of our algorithm to zeroth-order optimization. In Section III, we propose and analyze a novel iterative algorithm which requires access to the gradient information. Each step carries out a gradient update from the boundary, and then projects back to the boundary again. In Section IV, we introduce a novel asymptotically unbiased gradient-direction estimate at the boundary, and a binary-search procedure to approach the boundary. We also discuss how to control errors with deviation from the boundary. The analysis motivates a decision-based algorithm, HopSkipJumpAttack (Algorithm 2). Experimental results are provided in Section V. We conclude in Section VI with a discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Decision-based attacks</head><p>Most related to our work is the Boundary Attack method introduced by Brendel et al. <ref type="bibr" target="#b13">[14]</ref>. Boundary Attack is an iterative algorithm based on rejective sampling, initialized at an image that lies in the target class. At each step, a perturbation is sampled from a proposal distribution, which reduces the distance of the perturbed image towards the original input. If the perturbed image still lies in the target class, the perturbation is kept. Otherwise, the perturbation is dropped. Boundary Attack achieves performance comparable to state-of-the-art white-box attacks on deep neural networks for image classification. The key obstacle to its practical application is, however, the demand for a large number of model queries. In practice, the required number of model queries for crafting an adversarial example directly determines the level of the threat imposed by a decision-based attack. One source of inefficiency in Boundary Attack is the rejection of perturbations which deviate from the target class. In our algorithm, the perturbations are used for estimation of a gradient direction.</p><p>Several other decision-based attacks have been proposed to improve efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Zeroth-order optimization</head><p>Zeroth-order optimization refers to the problem of optimizing a function f based only on access to function values f (x), as opposed to gradient values ∇f (x). Such problems have been extensively studied in the convex optimization and bandit literatures. Flaxman et al. <ref type="bibr" target="#b17">[18]</ref> studied one-point randomized estimate of gradient for bandit convex optimization. Agarwal et al. <ref type="bibr" target="#b18">[19]</ref> and Nesterov and Spokoiny <ref type="bibr" target="#b19">[20]</ref> demonstrated that faster convergence can be achieved by using two function evaluations for estimating the gradient. Duchi et al. <ref type="bibr" target="#b20">[21]</ref> established optimal rates of convex zeroth-order optimization via mirror descent with two-point gradient estimates. Zerothorder algorithms have been applied to the generation of adversarial examples under the score-based threat model <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>Subsequent work <ref type="bibr" target="#b21">[22]</ref> proposed and analyzed an algorithm based on variance-reduced stochastic gradient estimates.</p><p>We formulate decision-based attack as an optimization problem. A core component of our proposed algorithm is a gradient-direction estimate, the design of which is motivated by zeroth-order optimization. However, the problem of decision-based attack is more challenging than zerothorder optimization, essentially because we only have binary information from output labels of the target model, rather than function values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. AN OPTIMIZATION FRAMEWORK</head><p>In this section, we describe an optimization framework for </p><formula xml:id="formula_0">F c (x).</formula><p>We study adversaries of both the untargeted and targeted varieties. Given some input x , the goal of an untargeted attack is to change the original classifier decision c := C(x ) to any c ∈ [m]\{c }, whereas the goal of a targeted attack is to change the decision to some pre-specified c † ∈ [m]\{c }. Formally, if we define the function S x : R d → R via</p><formula xml:id="formula_1">S x (x ) := ⎧ ⎨ ⎩ max c =c F c (x ) − F c (x ) (Untargeted) F c † (x ) − max c =c † F c (x ) (Targeted)<label>(1)</label></formula><p>then a perturbed image x is a successful attack if and only if S x (x ) &gt; 0. The boundary between successful and unsuccessful perturbed images is</p><formula xml:id="formula_2">bd(S x ) := z ∈ [0, 1] d | S x (z) = 0 .</formula><p>As an indicator of successful perturbation, we introduce the Boolean-valued function</p><formula xml:id="formula_3">φ x : [0, 1] d → {−1, 1} via φ x (x ) := sign (S x (x )) = 1 if S x (x ) &gt; 0, −1 otherwise.</formula><p>This function is accessible in the decision-based setting, as it can be computed by querying the classifier C alone. The goal of an adversarial attack is to generate a perturbed sample x such that φ x (x ) = 1, while keeping x close to the original sample x . This can be formulated as the optimization problem</p><formula xml:id="formula_4">min x d(x , x ) such that φ x (x ) = 1,<label>(2)</label></formula><p>where d is a distance function that quantifies similarity. Standard choices of d studied in past work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> include the usual p -norms, for p ∈ {0, 2, ∞}.</p><p>A. An iterative algorithm for 2 distance</p><p>Consider the case of the optimization problem (2) with the</p><formula xml:id="formula_5">2 -norm d(x, x ) = x − x 2 .</formula><p>We first specify an iterative algorithm that is given access to the gradient ∇S x . Given an initial vector x 0 such that S x (x 0 ) &gt; 0 and a stepsize sequence {ξ t } t≥0 , it performs the update</p><formula xml:id="formula_6">x t+1 = α t x + (1 − α t ) x t + ξ t ∇S x (x t ) ∇S x (x t ) 2 , (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>where ξ t is a positive step size. Here the line search parameter α t ∈ [0, 1] is chosen such that S x (x t+1 ) = 0-that is, so that the next iterate x t+1 lies on the boundary. The motivation for this choice is that our gradient-direction estimate in Section IV is only valid near the boundary.</p><p>We now analyze this algorithm with the assumption that we have access to the gradient of S x in the setting of binary classification. Assume that the function S x is twice differentiable with a locally Lipschitz gradient, meaning that there exists L &gt; 0 such that for all x, y ∈ {z</p><formula xml:id="formula_8">: z − x 2 ≤ x 0 − x 2 }, we have ∇S x (x) − ∇S x (y) 2 ≤ L x − y 2 ,<label>(4)</label></formula><p>In addition, we assume the gradient is bounded away from zero on the boundary: there exists a positive C &gt; 0 such that ∇S x (z) &gt; C for any z ∈ bd(S x ).</p><p>We analyze the behavior of the updates (3) in terms of the angular measure</p><formula xml:id="formula_9">r(x t , x ) := cos ∠ (x t − x , ∇S x (x t )) = x t − x , ∇S x (x t ) x t − x 2 ∇S x (x t ) 2 ,</formula><p>corresponding to the cosine of the angle between x t − x and the gradient ∇S x (x t ). Note that the condition r(x, x ) = 1 holds if and only if x is a stationary point of the optimization (2). The following theorem guarantees that, with a suitable step size, the updates converge to such a stationary point:</p><p>Theorem 1. Under the previously stated conditions on S x , suppose that we compute the updates (3) with step size ξ t = x t − x 2 t −q for some q ∈ 1 2 , 1 . Then there is a universal constant c such that</p><formula xml:id="formula_10">0 ≤ 1 − r(x t , x ) ≤ c t q−1 for t = 1, 2, . . .. (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>In particular, the algorithm converges to a stationary point of problem <ref type="bibr" target="#b1">(2)</ref>.</p><p>Theorem 1 suggests a scheme for choosing the step size in the algorithm that we present in the next section. An experimental evaluation of the proposed scheme is carried out in Appendix B. The proof of the theorem is constructed by establishing the relationship between the objective value d(x t , x ) and r(x t , x ), with a second-order Taylor approximation to the boundary. See Appendix A-A for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extension to ∞ -distance</head><p>We now describe how to extend these updates so as to minimize the ∞ -distance. Consider the 2 -projection of a point x onto the sphere of radius α t centered at x :</p><formula xml:id="formula_12">Π 2 x ,αt (x) := argmin y−x 2 ≤αt y − x 2 = α t x + (1 − α t )x.<label>(6)</label></formula><p>In terms of this operator, our 2 -based update (3) can be rewritten in the equivalent form</p><formula xml:id="formula_13">x t+1 = Π 2 x ,αt x t + ξ t ∇S x (x t ) ∇S x (x t ) 2 . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>This perspective allows us to extend the algorithm to other p -norms for p = 2. For instance, in the case p = ∞, we can define the ∞ -projection operator Π ∞ x ,α . It performs a perpixel clip within a neighborhood of x , such that the ith entry of</p><formula xml:id="formula_15">Π ∞ x ,α (x) is Π ∞ x ,α (x) i := max {min{x i , x i + c} , x i − c}, where c := α x − x ∞ .</formula><p>We propose the ∞ -version of our algorithm by carrying out the following update iteratively:</p><formula xml:id="formula_16">x t+1 = Π ∞ x ,αt x t + ξ t sign(∇S x (x t )) , (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where α t is chosen such that S x (x t+1 ) = 0, and "sign" returns the element-wise sign of a vector. We use the sign of the gradient for faster convergence in practice, similar to previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>.</p><formula xml:id="formula_18">IV. A DECISION-BASED ALGORITHM BASED ON A NOVEL GRADIENT ESTIMATE</formula><p>We now extend our procedures to the decision-based setting, in which we have access only to the Boolean-valued function φ x (x) = sign(S x (x))-that is, the method cannot observe the underlying discriminant function F or its gradient. In this section, we introduce a gradient-direction estimate based on φ x when x t ∈ bd(S x ) (so that S x (x t ) = 0 by definition). We proceed to discuss how to approach the boundary. Then we discuss how to control the error of our estimate with a deviation from the boundary. We will summarize the analysis with a decision-based algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. At the boundary</head><p>Given an iterate x t ∈ bd(S x ) we propose to approximate the direction of the gradient ∇S x (x t ) via the Monte Carlo estimate</p><formula xml:id="formula_19">∇S(x t , δ) := 1 B B b=1 φ x (x t + δu b )u b ,<label>(9)</label></formula><p>where {u b } B b=1 are i.i.d. draws from the uniform distribution over the d-dimensional sphere, and δ is small positive parameter. (The dependence of this estimator on the fixed centering point x is omitted for notational simplicity.)</p><p>The perturbation parameter δ is necessary, but introduces a form of bias in the estimate. Our first result controls this bias, and shows that ∇S(x t , δ) is asymptotically unbiased as δ → 0 + . Theorem 2. For a boundary point x t , suppose that S x has L-Lipschitz gradients in a neighborhood of x t . Then the cosine of the angle between ∇S(x t , δ) and ∇S x (x t ) is bounded as</p><formula xml:id="formula_20">cos ∠ E[ ∇S(x t , δ)], ∇S x (x t ) ≥ 1 − 9L 2 δ 2 d 2 8 ∇S(x t ) 2</formula><p>In particular, we have</p><formula xml:id="formula_21">lim δ→0 cos ∠ E[ ∇S(x t , δ)], ∇S x (x t ) = 1,<label>(11)</label></formula><p>showing that the estimate is asymptotically unbiased as an estimate of direction.</p><p>We remark that Theorem 2 only establishes the asymptotic behavior of the proposed estiamte at the boundary. This also motivates the boundary search step in our algorithm to be discussed in Seciton IV-B. The proof of Theorem 2 starts from dividing the unit sphere into three components: the upper cap along the direction of gradient, the lower cap opposite to the direction of gradient, and the annulus in between. The error from the annulus can be bounded when δ is small. See Appendix A-B for the proof of this theorem. As will be seen in the sequel, the size of perturbation δ should be chosen proportionally to d −1 ; see Section IV-C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Approaching the boundary</head><p>The proposed estimate ( <ref type="formula" target="#formula_19">9</ref>) is only valid at the boundary. We now describe how we approach the boundary via a binary search. Let xt denote the updated sample before the operator Π p x,αt is applied:</p><formula xml:id="formula_22">xt := x t + ξ t v t (x t , δ t ), such that (12) v t (x t , δ t ) = ∇S(x t , δ t )/ ∇S(x t , δ t ) 2 , if p = 2, sign( ∇S(x t , δ t )), if p = ∞,</formula><p>where ∇S will be introduced later in equation ( <ref type="formula">16</ref>), as a variance-reduced version of ∇S, and δ t is the size of perturbation at the t-th step.</p><p>We hope xt is at the opposite side of the boundary to x so that the binary search can be carried out. Therefore, we initialize at x0 at the target side with φ x (x 0 ) = 1, and set</p><formula xml:id="formula_23">x 0 := Π p x,α0 (x 0 )</formula><p>, where α 0 is chosen via a binary search between 0 and 1 to approach the boundary, stopped at x 0 lying on the target side with φ x (x 0 ) = 1. At the t-th iteration, we start at x t lying at the target side φ x (x t ) = 1. The step size is initialized as</p><formula xml:id="formula_24">ξ t := x t − x p / √ t, (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>as suggested by Theorem 1 in the 2 case, and is decreased by half until φ x (x t ) = 1, which we call geometric progression of ξ t . Having found an appropriate xt , we choose the projection radius α t via a binary search between 0 and 1 to approach the boundary, which stops at x t+1 with φ x (x t+1 ) = 1. See Algorithm 1 for the complete binary search, where the binary search threshold θ is set to be some small constant. Set α l = 0 and α u = 1.</p><formula xml:id="formula_26">while |α l − α u | &gt; θ do Set α m ← α l +αu 2 . if φ(Π x,αm (x )) = 1 then Set α u ← α m . else Set α l ← α m . end if end while Output x = Π x,αu (x ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Controlling errors of deviations from the boundary</head><p>Binary search never places x t+1 exactly onto the boundary. We analyze the error of the gradient-direction estimate, and propose two approaches for reducing the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) Appropriate choice of the size of random perturbation:</head><p>First, the size of random perturbation δ t for estimating the gradient direction is chosen as a function of image size d and the binary search threshold θ. This is different from numerical differentiation, where the optimal choice of δ t is at the scale of round-off errors (e.g., <ref type="bibr" target="#b22">[23]</ref>). Below we characterize the error incurred by a large δ t as a function of distance between xt and the boundary, and derive the appropriate choice of ξ t and δ t . In fact, with a Taylor approximation of S x at x t , we have</p><formula xml:id="formula_27">S x (x t + δ t u) = S x (x t ) + δ t ∇S x (x t ), u + O(δ 2 t )</formula><p>. At the boundary S x (x t ) = 0, the error of gradient approximation scales at O(δ 2 t ), which is minimized by reducing δ t to the scale of rooted round-off error. However, the outcome x t of a finite-step binary search lies close to, but not exactly on the boundary. When δ t is small enough such that second-order terms can be omitted, the first-order Taylor approximation implies that φ x (x t +δ t u) = −1 if and only if x t +δ t u lies on the spherical cap C, with</p><formula xml:id="formula_28">C := u | ∇S x (x t ) ∇S x (x t ) 2 , u &lt; −δ −1 t S x (x t ) ∇S x (x t ) 2 .</formula><p>On the other hand, the probability mass of u concentrates on the equator in a high-dimensional sphere, which is characterized by the following inequality <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_29">P(u ∈ C) ≤ 2 c exp{− c 2 2 }, where c = √ d − 2S x (x t ) δ t ∇S x (x t ) 2 . (<label>14</label></formula><formula xml:id="formula_30">)</formula><p>A Taylor expansion of</p><formula xml:id="formula_31">x t at x t := Π 2 ∂ (x t ) yields S x (x t ) = ∇S x (x t ) T (x t − x t ) + O( x t − x t 2 2 ) = ∇S x (x t ) T (x t − x t ) + O( x t − x t 2 2 ).</formula><p>By the Cauchy-Schwarz inequality and the definition of 2projection, we have</p><formula xml:id="formula_32">|∇S x (x t ) T (x t − x t )| ≤ ∇S x (x t ) 2 x t − Π 2 ∂ (x t ) 2 ≤ ∇S x (x t ) 2 θ xt−1 − x p , if p = 2, ∇S x (x t ) 2 θ xt−1 − x p √ d, if p = ∞.</formula><p>This yields</p><formula xml:id="formula_33">c = O( d q θ xt−1 − x p δ t ),</formula><p>where q = 1 − (1/p) is the dual exponent. In order to avoid a loss of accuracy from concentration of measure, we let</p><formula xml:id="formula_34">δ t = d q θ xt−1 − x 2 .</formula><p>To make the approximation error independent of dimension d, we set θ at the scale of d −q−1 , so that δ t is proportional to d −1 , as suggested by Theorem 2. This leads to a logarithmic dependence on dimension for the number of model queries. In practice, we set</p><formula xml:id="formula_35">θ = d −q−1 ; δ t = d −1 xt−1 − x p . (<label>15</label></formula><formula xml:id="formula_36">)</formula><p>b) A baseline for variance reduction in gradient-direction estimation: Another source of error comes from the variance of the estimate, where we characterize variance of a random vector v ∈ R d by the trace of its covariance operator:</p><formula xml:id="formula_37">Var(v) := d i=1</formula><p>Var(v i ). When x t deviates from the boundary and δ t is not exactly zero, there is an uneven distribution of perturbed samples at the two sides of the boundary:</p><formula xml:id="formula_38">|E[φ x (x t + δ t u)]| &gt; 0,</formula><p>as we can see from Equation <ref type="bibr" target="#b13">(14)</ref>. To attempt to control the variance, we introduce a baseline φ x into the estimate:</p><formula xml:id="formula_39">φ x := 1 B B b=1 φ x (x t + δu b ),</formula><p>which yields the following estimate:</p><formula xml:id="formula_40">∇S(x t , δ) := 1 B − 1 B b=1 (φ x (x t + δu b ) − φ x )u b . (16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 HopSkipJumpAttack</head><p>Require: Classifier C, a sample x, constraint p , initial batch size B 0 , iterations T . Ensure: Perturbed image x t .</p><p>Set θ (Equation ( <ref type="formula" target="#formula_35">15</ref>)). Initialize at x0 with φ x (x 0 ) = 1.</p><p>Compute</p><formula xml:id="formula_41">d 0 = x0 − x p . for t in 1, 2, . . . , T − 1 do (Boundary search) x t = BIN-SEARCH(x t−1 , x, θ, φ x , p) (Gradient-direction estimation) Sample B t = B 0 √ t unit vectors u 1 , . . . , u Bt . Set δ t (Equation (15)). Compute v t (x t , δ t ) (Equation (12)). (Step size search) Initialize step size ξ t = x t − x p / √ t. while φ x (x t + ε t v t ) = 0 do ξ t ← ξ t /2. end while Set xt = x t + ξ t v t . Compute d t = xt − x p . end for Output x t = BIN-SEARCH(x t−1 , x, θ, φ x , p).</formula><p>It can be easily observed that this estimate is equal to the previous estimate in expectation, and thus still asymptotically unbiased at the boundary: When x t ∈ bd(S x ), we have</p><formula xml:id="formula_42">cos ∠ E[ ∇S(x t , δ)], ∇S x (x t ) ≥ 1 − 9L 2 δ 2 d 2 8 ∇S(x t ) 2 2 , lim δ→0 cos ∠ E[ ∇S(x t , δ)], ∇S x (x t ) = 1.</formula><p>Moreover, the introduction of the baseline reduces the variance when E[φ x (x t + δu)] deviates from zero. In particular, the following theorem shows that whenever</p><formula xml:id="formula_43">|E[φ x (x t + δu)]| = Ω(B − 1 2</formula><p>), the introduction of a baseline reduces the variance.</p><p>Theorem 3. Defining σ 2 := Var(φ x (x t + δu)u) as the variance of one-point estimate, we have</p><formula xml:id="formula_44">Var( ∇S(x t , δ)) &lt; Var( ∇S(x t , δ))(1 − ψ),</formula><p>where</p><formula xml:id="formula_45">ψ = 2 σ 2 (B − 1) 2BE[φ x (x t + δu)] 2 − 1 − 2B − 1 (B − 1) 2 .</formula><p>See Appendix A-C for the proof. We also present an experimental evaluation of our gradient-direction estimate when the sample deviates from the boundary in Appendix B, where we show our proposed choice of δ t and the introduction of baseline yield a performance gain in estimating gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. HopSkipJumpAttack</head><p>We now combine the above analysis into an iterative algorithm, HopSkipJumpAttack. It is initialized with a sample in the target class for untargeted attack, and with a sample blended with uniform noise that is misclassified for targeted attack. Each iteration of the algorithm has three components. First, the iterate from the last iteration is pushed towards the boundary via a binary search (Algorithm 1). Second, the gradient direction is estimated via Equation <ref type="bibr" target="#b15">(16)</ref>. Third, the updating step size along the gradient direction is initialized as Equation (13) based on Theorem 1, and is decreased via geometric progression until perturbation becomes successful. The next iteration starts with projecting the perturbed sample back to the boundary again. The complete procedure is summarized in Algorithm 2. Figure <ref type="figure" target="#fig_4">2</ref> provides an intuitive visualization of the three steps in 2 . For all experiments, we initialize the batch size at 100 and increase it with √ t linearly, so that the variance of the estimate reduces with t. When the input domain is bounded in practice, a clip is performed at each step by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we carry out experimental analysis of HopSkipJumpAttack. We compare the efficiency of Hop-SkipJumpAttack with several previously proposed decisionbased attacks on image classification tasks. In addition, we evaluate the robustness of three defense mechanisms under our attack method. All experiments were carried out on a Tesla K80 GPU, with code available online. 2 Our algorithm is also 2 See https://github.com/Jianbo-Lab/HSJA/. available on CleverHans <ref type="bibr" target="#b24">[25]</ref> and Foolbox <ref type="bibr" target="#b25">[26]</ref>, which are two popular Python packages to craft adversarial examples for machine learning models.</p><p>A. Efficiency evaluation a) Baselines: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack <ref type="bibr" target="#b13">[14]</ref>, Limited Attack <ref type="bibr" target="#b8">[9]</ref> and Opt Attack <ref type="bibr" target="#b15">[16]</ref>. We use the implementation of the three algorithms with the suggested hyperparameters from the publicly available source code online. Limited Attack is only included under the targeted ∞ setting, as in Ilyas et al. <ref type="bibr" target="#b8">[9]</ref>. b) Data and models: For a comprehensive evaluation of HopSkipJumpAttack, we use a wide range of data and models, with varied image dimensions, data set sizes, complexity levels of task and model structures.</p><p>The experiments are carried out over four image data sets: MNIST, CIFAR-10 [27], CIFAR-100 <ref type="bibr" target="#b26">[27]</ref>, and ImageNet <ref type="bibr" target="#b27">[28]</ref> with the standard train/test split <ref type="bibr" target="#b28">[29]</ref> We also use models of varied structure, from simple to complex. For MNIST, we use a simple convolutional network composed of two convolutional layers followed by a hidden dense layer with 1024 units. Two convolutional layers have 32, 64 filters respectively, each of which is followed by a max-pooling layer. For both CIFAR-10 and CIFAR-100, we train a 20-layer ResNet <ref type="bibr" target="#b30">[31]</ref> and 121-layer DenseNet <ref type="bibr" target="#b31">[32]</ref> respectively, with the canonical network structure <ref type="bibr" target="#b28">[29]</ref>. For ImageNet, we use a pre-trained 50-layer ResNet <ref type="bibr" target="#b30">[31]</ref>. All models achieve close to state-of-the-art accuracy on the respective data set. All pixels are scaled to be in the range [0, 1]. For all experiments, we clip the perturbed image into the input domain [0, 1] for all algorithms by default.</p><p>c) Initialization: For untargeted attack, we initialize all attacks by blending an original image with uniform random noise, and increasing the weight of uniform noise gradually until it is misclassified, a procedure which is available on Foolbox <ref type="bibr" target="#b25">[26]</ref>, as the default initialization of Boundary Attack.</p><p>For targeted attack, the target class is sampled uniformly among the incorrect labels. An image belonging to the target class is randomly sampled from the test set as the initialization.</p><p>The same target class and a common initialization image are used for all attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d) Metrics:</head><p>The first metric is the median p distance between perturbed and original samples over a subset of test images, which was commonly used in previous work, such as Carlini and Wagner <ref type="bibr" target="#b5">[6]</ref>. A version normalized by image dimension was employed by Brendel et al. <ref type="bibr" target="#b13">[14]</ref> for evaluating Boundary Attack. The As an alternative metric, we also plot the success rate at various distance thresholds for both algorithms given a limited budget of model queries. An adversarial example is defined a success if the size of perturbation does not exceed a given distance threshold. The success rate can be directly related to the accuracy of a model on perturbed data under a given Throughout the experiments, we limit the maximum budget of queries per image to 25,000, the setting of practical interest, due to limited computational resources.</p><p>e) Results: Figure <ref type="figure" target="#fig_12">3 and 4</ref> show the median distance (on a log scale) against the queries, with the first and third quartiles used as lower and upper error bars. For Boundary, Opt and HopSkipJumpAttack, Table <ref type="table" target="#tab_2">I</ref> summarizes the median distance when the number of queries is fixed at 1,000, 5,000, and 20,000 across all distance types, data, models and objectives. Figure <ref type="figure" target="#fig_6">5</ref> and 6 show the success rate against the distance threshold. Figure <ref type="figure" target="#fig_3">3</ref>  By comparing the odd and even columns of Figure <ref type="figure" target="#fig_12">3-6</ref>, we can find that targeted HopSkipJumpAttack takes more queries than the untargeted one to achieve a comparable distance. This phenomenon becomes more explicit on CIFAR-100 and ImageNet, which have more classes. With the same number of queries, there is an order-of-magnitude difference in median distance between untargeted and targeted attacks (Figure <ref type="figure" target="#fig_12">3 and 4</ref>). For 2 -optimized HopSkipJumpAttack, while the untargeted version is able to craft adversarial images by perturbing 4 bits per pixel on average within 1,000 queries for 70% − 90% of images in CIFAR-10 and CIFAR-100, the targeted counterpart takes 2,000-5,000 queries. The other attacks fail to achieve a comparable performance even with 25,000 queries. On ImageNet, untargeted 2 -optimized Hop-SkipJumpAttack is able to fool the model with a perturbation of size 6 bits per pixel on average for close to 50% of images with 1, 000 queries; untargeted ∞ -optimized Hop-SkipJumpAttack controls the maximum perturbation across all pixels within 16 bits for 50% images within 1, 000 queries. The targeted Boundary Attack is not able to control the perturbation size to such a small scale until after around 25, 000 queries. On the one hand, the larger query budget requirement results from a strictly more powerful formulation of targeted attack than untargeted attack. On the other hand, this is also because we initialize targeted HopSkipJumpAttack from an arbitrary image in the target class. The algorithm may be trapped in a bad local minimum with such an initialization. Future work can address systematic approaches to better initialization.</p><p>As a comparison between data sets and models, we see that adversarial images often have a larger distance to their corresponding original images on MNIST than on CIFAR-10 and CIFAR-100, which has also been observed in previous work (e.g., <ref type="bibr" target="#b5">[6]</ref>). This might be because it is more difficult to fool a model on simpler tasks. On the other hand, Hop-SkipJumpAttack also converges in a fewer number of queries on MNIST, as is shown in Figure <ref type="figure" target="#fig_3">3</ref>. It does not converge even after 25, 000 queries on ImageNet. We conjecture the query As a comparison with state-of-the-art white-box targeted attacks, C&amp;W attack <ref type="bibr" target="#b5">[6]</ref> achieves an average 2 -distance of 0.33 on CIFAR-10, and BIM <ref type="bibr" target="#b2">[3]</ref> achieves an average ∞distance of 0.014 on CIFAR-10. Targeted HopSkipJumpAttack achieves a comparable distance with 5K-10K model queries on CIFAR-10, without access to model details. On ImageNet, targeted C&amp;W attack and BIM achieve an 2 -distance of 0.96 and an ∞ -distance of 0.01 respectively. Untargeted HopSkipJumpAttack achieves a comparable performance with 10, 000 − 15, 000 queries. The targeted version is not able to perform comparably as targeted white-box attacks when the budget of queries is limited within 25, 000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualized trajectories of HopSkipJumpAttack optimized for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Defense mechanisms under decision-based attacks</head><p>We investigate the robustness of various defense mechanisms under decision-based attacks.</p><p>a) Defense mechanisms: Three defense mechanisms are evaluated: defensive distillation, region-based classification, and adversarial training. Defensive distillation <ref type="bibr" target="#b32">[33]</ref>, a form of gradient masking <ref type="bibr" target="#b12">[13]</ref>, trains a second model to predict the output probabilities of an existing model of the same structure. We use the implementaion provided by Carlini and Wagner <ref type="bibr" target="#b5">[6]</ref> for defensive distillation. The second defense, region-based classification, belongs to a wide family of mechanisms which add test-time randomness to the inputs or the model, causing the gradients to be randomized <ref type="bibr" target="#b33">[34]</ref>. Multiple variants have been proposed to randomize the gradients <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. We adopt the implementation in Cao and Gong <ref type="bibr" target="#b34">[35]</ref> with suggested noise levels. Given a trained base model, region-based classification samples points from the hypercube centered at the input image, predicts the label for each sampled point with the base model, and then takes a majority vote to output the label. Adversarial training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> is known to be one of the most effective defense mechanisms against adversarial perturbation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>We evaluate a publicly available model trained through a robust optimization method proposed by Madry et al. <ref type="bibr" target="#b6">[7]</ref>. We further evaluate our attack method by constructing a non-differentiable model via input binarization followed by a random forest in Appendix C. The evaluation is carried out on MNIST, where defense mechanisms such as adversarial training work most effectively.</p><p>b) Baselines: We compare our algorithm with state-of-the-art attack algorithms that require access to gradients, including C&amp;W Attack <ref type="bibr" target="#b5">[6]</ref>, DeepFool <ref type="bibr" target="#b3">[4]</ref> for minimizing 2 -distance, and FGSM <ref type="bibr" target="#b1">[2]</ref>, and BIM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> for minimizing ∞ -distance.</p><p>For region-based classification, the gradient of the base classifier is taken with respect to the original input.</p><p>We further include methods designed specifically for the defense mechanisms under threat. For defensive distillation, we include the ∞ -optimized C&amp;W Attack <ref type="bibr" target="#b5">[6]</ref>. For regionbased classification, we include backward pass differentiable approximation (BPDA) <ref type="bibr" target="#b33">[34]</ref>, which calculates the gradient of the model at a randomized input to replace the gradient at the original input in C&amp;W Attack and BIM. All of these methods Trajectories on ImageNet assume access to model details or even defense mechanisms, which is a stronger threat model than the one required for decision-based attacks. We also include Boundary Attack as a decision-based baseline.</p><p>For HopSkipJumpAttack and Boundary Attack, we include the success rate at three different scales of query budget: 2K, 10K and 50K, so as to evaluate our method both with limited queries and a sufficient number of queries. We find the convergence of HopSkipJumpAttack becomes unstable on region-based classification, resulting from the difficulty of locating the boundary in the binary search step when uncertainty is increased near the boundary. Thus, we increase the binary search threshold to 0.01 to resolve this issue.</p><p>c) Results: Figure <ref type="figure" target="#fig_11">8</ref> shows the success rate of various attacks at different distance thresholds for the three defense mechanisms. On all of the three defenses, HopSkipJumpAttack demonstrates similar or superior performance compared to state-of-the-art white-box attacks with sufficient model queries. Even with only 1K-2K model queries, it also achieves acceptable performance, although worse than the best whitebox attacks. With sufficient queries, Boundary Attack achieves a comparable performance under the 2 -distance metric. But it is not able to generate any adversarial examples when the number of queries is limited to 1, 000. We think this is because the strength of our batch gradient direction estimate over the random walk step in Boundary Attack becomes more explicit when there is uncertainty or non-smoothness near the decision boundary. We also observe that Boundary Attack does not work in optimizing the ∞ -distance metric for adversarial examples, making it difficult to evaluate defenses designed for ∞ distance, such as adversarial training proposed by Madry et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>On a distilled model, when the ∞ -distance is thresholded at 0.3, a perturbation size proposed by Madry et al. <ref type="bibr" target="#b6">[7]</ref> to measure adversarial robustness, HopSkipJumpAttack achieves success rates of 86% and 99% with 1K and 50K queries respectively. At an 2 -distance of 3.0, the success rate is 91% with 2K queries. HopSkipJumpAttack achieves a comparable performance with C&amp;W attack under both distance metrics with 10K-50K queries. Also, gradient masking <ref type="bibr" target="#b12">[13]</ref> by defensive distillation does not have a large influence on the query efficiency of HopSkipJumpAttack, indicating that the gradient direction estimate is robust under the setting where the model does not have useful gradients for certain white-box attacks.</p><p>On region-based classification, with 2K queries, Hop-SkipJumpAttack achieves success rates of 82% and 93% at the same ∞ -and 2 -distance thresholds respectively. With 10K-50K queries, it is able to achieve a comparable performance to BPDA, a white-box attack tailored to such defense mechanisms. On the other hand, we observe that Hop-SkipJumpAttack converges slightly slower on region-based classification than itself on ordinary models, which is because stochasticity near the boundary may prevent binary search in HopSkipJumpAttack from locating the boundary accurately.</p><p>On an adversarially trained model, HopSkipJumpAttack achieves a success rate of 11.0% with 50K queries when the ∞ -distance is thresholded at 0.  has a success rate of 7.4% at the given distance threshold. The success rate of ∞ -HopSkipJumpAttack transfers to an accuracy of 87.58% on adversarially perturbed data, close to the state-of-the-art performance achieved by white-box attacks. 3 With 1K queries, HopSkipJumpAttack also achieves comparable performance to BIM and C&amp;W attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>We have proposed a family of query-efficient algorithms based on a novel gradient-direction estimate, HopSkipJumpAttack, for decision-based generation of adversarial examples, which is capable of optimizing 2 and ∞ -distances for both targeted and untargeted attacks. Convergence analysis has been carried out given access to the gradient. We have also provided analysis for the error of our Monte Carlo estimate of gradient direction, which comes from three sources: bias at the boundary for a nonzero perturbation size, bias of deviation from the boundary, and variance. Theoretical analysis has provided insights for selecting the step size and the perturbation size, which leads to a hyperparameter-free algorithm. We have also carried out extensive experiments, showing HopSkipJumpAttack compares favorably to Boundary Attack in query efficiency, and achieves competitive performance on several defense mechanisms. 3 See https://github.com/MadryLab/mnist challenge.</p><p>Given the fact that HopSkipJumpAttack is able to craft a human-indistinguishable adversarial example within a realistic budget of queries, it becomes important for the community to consider the real-world impact of decision-based threat models. We have also demonstrated that HopSkipJumpAttack is able to achieve comparable or even superior performance to state-of-the-art white-box attacks on several defense mechanisms, under a much weaker threat model. In particular, masked gradients, stochastic gradients, and nondifferentiability are not barriers to our algorithm. Because of its effectiveness, efficiency, and applicability to nondifferentiable models, we suggest future research on adversarial defenses may evaluate the designed mechanism against HopSkipJumpAttack as a first step.</p><p>One limitation of all existing decision-based algorithms, including HopSkipJumpAttack, is that they require evaluation of the target model near the boundary. They may not work effectively by limiting the queries near the boundary, or by widening the decision boundary through insertion of an additional "unknown" class for inputs with low confidence. We have also observed that it still takes tens of thousands of model queries for HopSkipJumpAttack to craft imperceptible adversarial examples with a target class on ImageNet, which has a relatively large image size. Future work may seek the combination of HopSkipJumpAttack with transfer-based attack to resolve these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOFS</head><p>For notational simplicity, we use the shorthand S ≡ S x throughout the proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>We denote τ t := ξ t / ∇S(x t ) 2 , so that the update (3) at iterate t can be rewritten as</p><formula xml:id="formula_46">x t+1 = α t x + (1 − α t )(x t + τ t ∇S(x t )).<label>(18)</label></formula><p>Let the step size choice ξ t = η t x t − x with η t := t −q , we have</p><formula xml:id="formula_47">τ t = η t xt−x ∇S(xt) .</formula><p>The squared distance ratio is</p><formula xml:id="formula_48">x t+1 − x 2 2 x t − x 2 2 = (1 − α)(τ t ∇S(x t ) + x t − x ) 2 2 x t − x 2 2 . (<label>19</label></formula><formula xml:id="formula_49">)</formula><p>By a second-order Taylor series, we have</p><formula xml:id="formula_50">0 = ∇S(x t ), x t+1 − x t + 1 2 (x t+1 − x t ) T H t (x t+1 − x t ),<label>(20)</label></formula><p>where</p><formula xml:id="formula_51">H t = ∇ 2 S(βx t+1 + (1 − β)x t ) for some β ∈ [0, 1].</formula><p>Plugging equation <ref type="bibr" target="#b17">(18)</ref> into equation <ref type="bibr" target="#b19">(20)</ref> yields</p><formula xml:id="formula_52">∇S(x t ), −αv t + τ t ∇S(x t ) + 1 2 (−αv t + τ t ∇S(x t )) T H t (−αv t + τ t ∇S(x t )) = 0,<label>(21)</label></formula><p>where we define v t := x t − x + τ t ∇S(x t ). This can be rewritten as a quadratic equation with respect to α:</p><formula xml:id="formula_53">v T t H t v t α 2 − 2∇S(x t ) T (I + τ t H t )v t α + ∇S(x t ) T (τ 2 t H t + 2τ t I)∇S(x t ) = 0.<label>(22)</label></formula><p>Solving for α yields</p><formula xml:id="formula_54">α ≥ ∇S(x t ) T (τ 2 t H t + 2τ t I)∇S(x t ) 2∇S(x t ) T (I + τ t H t )v t . (<label>23</label></formula><formula xml:id="formula_55">)</formula><p>In order to simplify the notation, define ∇ t := ∇S(x t ) and d t := x t − x . Hence, we have</p><formula xml:id="formula_56">(1 − α) 2 ≤ r t + η t • 3 2 L dt 2 ∇t 2 r t + η t • (1 + 3 2 L dt 2 ∇t 2 ) 2 ,</formula><p>where</p><formula xml:id="formula_57">r t = x t − x , ∇S(x t ) x t − x 2 ∇S(x t ) 2 = d t , ∇ t d t 2 ∇ t 2 . (<label>24</label></formula><formula xml:id="formula_58">)</formula><p>Let κ t := 3 2 L dt 2 ∇t 2 . Then κ t is bounded when ∇ t 2 ≥ C and q &gt; 1 2 . Equation <ref type="bibr" target="#b18">(19)</ref> and the bound on</p><formula xml:id="formula_59">(1 − α) 2 yield x t+1 − x 2 2 x t − x 2 2 ≤ r t + η t κ t r t + η t (1 + κ t ) 2 • (η 2 t + 2η t r t + 1).<label>(25)</label></formula><p>Define</p><formula xml:id="formula_60">θ t := rt+ηtκt rt+ηt(1+κt) 2 • (η 2 t + 2η t r t + 1)</formula><p>. We analyze θ t in the following two different cases: r t &lt; η t and r t ≥ η t . In the first case, we have</p><formula xml:id="formula_61">θ t ≤ 1 + κ t 1 + (1 + κ t ) 2 • (η 2 t + 2η 2 t + 1).<label>(26)</label></formula><p>As long as η t → 0 as t → ∞, there exists a positive constant c 2 &gt; 0 such that θ t &lt; 1 − c 2 for t large enough.</p><p>In the second case, we have r t ≥ η t . Define λ t := ηt rt ≤ 1. We bound θ t by</p><formula xml:id="formula_62">θ t = (1 + 2λ t κ t + λ 2 t κ 2 t )(η 2 t + 2η t r t + 1) 1 + 2λ t (1 + κ t ) + λ 2 t (1 + κ t ) 2 ≤ 1 + 2λ t κ t + λ 2 t κ 2 t + 2λ t r 2 t 1 + 2λ t κ t + λ 2 t κ 2 t + 2λ t + η 2 t (4κ t + (1 + λ t κ t ) 2 + 2λ t κ 2 t ) ≤ 1 − 2λ t (1 − r 2 t ) 1 + 2λ t κ t + λ 2 t κ 2 t + 2λ t + cη 2 t ≤ 1 − c 1 λ t (1 − r 2 t ) + c 2 η 2 t ,</formula><p>where c 1 , c 2 are fixed constants. As the product of θ t over t is positive, we have</p><formula xml:id="formula_63">∞ t=1 log θ t = log Π ∞ t=1 θ t &gt; −∞. (<label>27</label></formula><formula xml:id="formula_64">)</formula><p>Then we have that there are at most a finite number of t that falls in the first case, r t &lt; η t . In the second case, Equation ( <ref type="formula" target="#formula_63">27</ref>) is equivalent to</p><formula xml:id="formula_65">∞ t=1 c 1 η t 1 − r 2 t r t − c 2 η 2 t &lt; ∞, which implies c 1 η t 1−r 2 t rt − c 2 η 2 t = o(t −1 ). When η t = t −q for some constant 1 2 &lt; q &lt; 1, we have 1 − r 2 t r t = o(t q−1 ).</formula><p>Hence we have 1 − r t = o(t q−1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 2</head><p>Let u be a random vector uniformly distributed on the sphere. By Taylor's theorem, for any δ ∈ (0, 1), we have</p><formula xml:id="formula_66">S(x t + δu) = δ∇S(x t ) T u + 1 2 δ 2 u T ∇ 2 S(x )u.<label>(28)</label></formula><p>for some x on the line between x t and x t + δu, where we have made use of the fact that S(x t ) = 0. As the function S has Lipschitz gradients, we can bound the second-order term as</p><formula xml:id="formula_67">| 1 2 δ 2 u T ∇ 2 S(x )u| ≤ 1 2 Lδ 2 . (<label>29</label></formula><formula xml:id="formula_68">)</formula><p>Let w := 1 2 Lδ. By the Taylor expansion and the bound on the second-order term by eigenvalues, when ∇S(x t ) T u &gt; w, we have</p><formula xml:id="formula_69">S(x t + δu) ≥ δ∇S(x t ) T u + 1 2 δ 2 u T ∇ 2 S(x )u ≥ δ(∇S(x t ) T u − 1<label>2</label></formula><p>Lδ) &gt; 0.</p><p>Similarly, we have S(x t + δu) &lt; 0 when ∇S(x t ) T u &lt; −w. Therefore, we have</p><formula xml:id="formula_70">φ x (x t + δu) = 1 if ∇S(x t ) T u &gt; w, −1 if ∇S(x t ) T u &lt; −w.</formula><p>We expand the vector ∇S(x t ) to an orthogonal bases in R d : v 1 = ∇S(x t )/ ∇S(x t ) 2 , v 2 , . . . , v d . The random vector u can be expressed as u = d i=1 β i v i , where β is uniformly distributed on the sphere. Denote the upper cap as E 1 := {∇S(x t ) T u &gt; w}, the annulus as E 2 := {|∇S(x t ) T u| &lt; w}, and the lower cap as E 3 := {∇S(x t ) T u &lt; −w}. Let p := P(E 2 ) be the probability of event E 2 . Thus we have P(E 1 ) = P(E 3 ) = (1 − p)/2. By symmetry, for any i = 1, we have</p><formula xml:id="formula_71">E[β i | E 1 ] = E[β i | E 3 ] = 0.</formula><p>Therefore, the expected value of the estimator is</p><formula xml:id="formula_72">E[φ x (x t + δu)u] = p • E[φ x (x t + δu)u | E 2 ] − 1 2 E[β 1 v 1 | E 1 ] − 1 2 E[−β 1 v 1 | E 3 ] + E[β 1 v 1 | E 1 ] + E[−β 1 v 1 | E 3 ]</formula><p>Exploiting the above derivation, we can bound the difference between</p><formula xml:id="formula_73">E[|β 1 |v 1 ] = E|β1| ∇S(xt) 2 ∇S(x t ) and E[φ x (x t + δu)u]: E[φ x (x t + δu)u] − E[|β 1 |v 1 ] 2 ≤ 2p + p = 3p, which yields cos ∠ (E[φ x (x t + δu)u], ∇S(x t )) ≥ 1 − 1 2 3p E|β 1 | 2 . (<label>30</label></formula><formula xml:id="formula_74">)</formula><p>We can bound p by observing that</p><formula xml:id="formula_75">∇S(xt) ∇S(xt) 2 , u 2 is a Beta distribution B( 1 2 , d−1 2 ): p = P ∇S(x t ) ∇S(x t ) 2 , u 2 ≤ w 2 ∇S(x t ) 2 2 ≤ 2w B( 1 2 , d−1 2 ) ∇S(x t ) 2 .</formula><p>Plugging into Equation (30), we get</p><formula xml:id="formula_76">cos ∠ (E[φ x (x t + δu)u], ∇S(x t )) ≥ 1 − 18w 2 (E|β 1 |) 2 B( 1 2 , d−1 2 ) 2 ∇S(x t ) 2 2 = 1 − 9L 2 δ 2 (d − 1) 2 8 ∇S(x t ) 2 2 .</formula><p>We also observe that</p><formula xml:id="formula_77">E ∇S(x t , δ) = E[φ x (x t + δu)u].</formula><p>As a consequence, we have established</p><formula xml:id="formula_78">cos ∠ E[ ∇S(x t , δ)], ∇S(x t ) ≥ 1 − 9L 2 δ 2 (d − 1) 2 8 ∇S(x t ) 2 2 .</formula><p>Taking δ → 0, we get </p><formula xml:id="formula_79">lim δ→0 cos∠ E[ ∇S(x t , δ)], ∇S(x t ) = 1.</formula><formula xml:id="formula_80">( ∇S(x t , δ)) = 1 (B − 1) 2 B a=1 E ξ a u a − E[ξu] 2 2 − 2E[ ξξ a ]+ E ξ2 + ( 2 B − 1 B 2 ) E[ξu] 2 + Eξu 2 2 B(B − 1) = B 2 Var( ∇S(x t , δ)) (B − 1) 2 − BE[ ξ2 ] (B − 1) 2 + (3B − 2) E[ξu] 2 2 B(B − 1) 2 ≤ B 2 Var( ∇S(x t , δ)) (B − 1) 2 − BE[ ξ2 ] (B − 1) 2 + 3B − 2 B(B − 1) 2 . (<label>31</label></formula><formula xml:id="formula_81">)</formula><p>The middle term can be expanded as</p><formula xml:id="formula_82">− B (B − 1) 2 E[ ξ2 ] = − 1 (B − 1) 2 − 4 B − 1 (Eξ − 1 2 ) 2 .</formula><p>Plugging into Equation ( <ref type="formula" target="#formula_80">31</ref>), we get</p><formula xml:id="formula_83">Var( ∇S(x t , δ)) = Var( ∇S(x t , δ)) 1 + 2B − 1 (B − 1) 2 − 2 σ 2 (B − 1) 2B(E[ξ] − 1 2 ) 2 − 1 . When E[ξ] satisfies (E[ξ] − 1 2 ) 2 &gt; 1 2B (1 + 2B−1 2B−2 σ 2 ), we have 2B − 1 (B − 1) 2 &lt; 2 σ 2 (B − 1) (2B(E[ξ] − 1 2 ) 2 − 1),</formula><p>which implies Var( ∇S(x t , δ)) &lt; Var( ∇S(x t , δ)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B SENSITIVITY ANALYSIS</head><p>In this section, we carry out experiments to evaluate the hyperparameters suggested by our theoretical analysis. We use a 20-layer ResNet <ref type="bibr" target="#b30">[31]</ref> trained over CIFAR-10 <ref type="bibr" target="#b26">[27]</ref>. We run the 2 -optimized HopSkipJumpAttack over a subset of randomly sampled images. a) Choice of step size: We compare several schemes of choosing step size at each step. The first scheme is suggested by Theorem 1: at the t-th step, we set ξ t = x t − x 2 / √ t, which we call "Scale with Distance (Sqrt. Decay)." We include the other two scales which scale with distance, "Scale with Distance (Linear Decay)" with ξ t = x t − x 2 /t and "Scale with Distance (No Decay)" with ξ t = x t − x 2 . We then include "Grid Search," which searchs step sizes over a logscale grid, and chooses the step size that best controls the distance with the original sample after projecting the updated sample back to the boundary via binary search. Finally, we include constant stepsizes at ξ t = 0.01, 0.1, 1.0. For all schemes, we always use geometric progression to decrease the step size by half until φ x (x t ) = 1 before the next binary search step.  Figure <ref type="figure" target="#fig_14">9</ref> plots the median distance against the number of queries for all schemes. We observe that the scheme suggested by Theorem 1 achieves the best performance in this experiment. Grid search costs extra query budget initially but eventually achieves a comparable convergence rate. When the step size scales with the distance but with inappropriately chosen decay, the algorithm converges slightly slower. The performance of the algorithm suffers from a constant step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Choice of perturbation size and introduction of baseline:</head><p>We now study the effectiveness of the proposed perturbation size and baseline for estimating gradient direction when the sample deviates from the boundary. In particular, we focus on the choice of δ t and the introduction of baseline analyzed in Section IV. Gradient direction estimation is carried out at perturbed images at the ith iteration, for i = 10, 20, 30, 40, 50, 60.</p><p>We use the cosine of the angle between the gradient-direction estimate and the truth gradient of the model as a metric.</p><p>Figure <ref type="figure" target="#fig_15">10</ref> shows the box plots of two gradient-direction estimates as δ t varies among 0.01δ √ dθ xt−1 − x 2 is our proposed choice. We observe that our proposed choice of δ t yields the highest cosine of the angle on average. Also, the baseline in ∇S further improves the performance, in particular when δ t is not chosen optimally so that there is severe unevenness in the distribution of perturbed images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C MODEL WITHOUT GRADIENTS</head><p>In this section, we evaluate HopSkipJumpAttack on a model without gradients. We aim to show HopSkipJumpAttack is able to craft adversarial examples under weaker conditions, such as non-differentiable models, or even discontinuous input transform.</p><p>Concretely, we implement input binarization followed by a random forest on MNIST. Binarization transforms an input image to an array of {0, 1}, but transforming all pixels larger than a given threshold to 1, and all pixels smaller than the threshold to 0. The algorithm for training random forests applies bootstrap aggregating to tree learners. We implement the random forest with default parameters in scikit-learn <ref type="bibr" target="#b41">[42]</ref>, using the Gini impurity as split criterion. For each split, √ d randomly selected features are used, where d = 28 × 28 is the number of pixels. We evaluate two random forests with different thresholds for binarization: 0.1 and 0.5. With the first threshold, the model achieves the highest accuracy, 96%, on natural test data. The second threshold yields the most robust performance under adversarial perturbation, with accuracy 94.5% on natural test data.</p><p>For both Boundary Attack and HopSkipJumpAttack, we adopt the same initialization and hyper-parameters as in Section V-A. The original image (with real values) is used as input to both attacks for model queries. When an image is fed into the model by the attacker, the model processes the image with binarization first, followed by the random forest. Such a design preserves the black-box assumption for decision-based attacks. We only focus on untargeted 2 attack here. Note that over 91% of the pixels on MNIST are either greater than 0.9 or less than 0.1, and thus require a perturbation of size at least 0.4 to change their outputs after being thresholded by 0.5. This fact makes ∞ perturbation inappropriate for crafting adversarial examples. Figure <ref type="figure" target="#fig_16">11</ref> shows the median distance (on a log scale) against the queries, with the first and third quartiles used as lower and upper error bars. Figure <ref type="figure" target="#fig_4">12</ref> shows the success rate against the distance threshold.</p><p>When the threshold is set to be 0. When the threshold is set to be 0.5, we have a more robust model. A median 2 distance of 3 is achieved by HopSkipJumpAttack through 3K model queries. It takes 25K queries to achieve 99% success rate at an 2 distance of 3 for HopSkipJumpAttack. On the other hand, we observe that Boundary Attack only achieves a median distance of 5 even with 25K model queries. This might result from the inefficiency in spending queries on random walk instead of "gradient direction" estimation step in HopSkipJumpAttack. We remark that the concept of "gradient direction" requires an alternative definition in the current setting, such as a formulation via subgradients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of accessible components of the target model for each of the three threat models. A white-box threat model assumes access to the whole model; a score-based threat model assumes access to the output layer; a decision-based threat model assumes access to the predicted label alone.</figDesc><graphic url="image-1.png" coords="1,311.71,197.22,104.79,64.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>finding adversarial instances for an m-ary classification model of the following type. The first component is a discriminant function F : R d → R m that accepts an input x ∈ [0, 1] d and produces an output y ∈ Δ m := {y ∈ [0, 1] m | m c=1 y c = 1}. The output vector y = (F 1 (x), . . . , F m (x)) can be viewed as a probability distribution over the label set [m] = {1, . . . , m}. Based on the function F , the classifier C : R d → [m] assigns input x to the class with maximum probability-that is, C(x) := arg max c∈[m]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Algorithm 1</head><label>21</label><figDesc>Figure 2: Intuitive explanation of HopSkipJumpAttack. (a) Perform a binary search to find the boundary, and then update xt → x t . (b) Estimate the gradient at the boundary point x t . (c) Geometric progression and then update x t → xt+1 . (d) Perform a binary search, and then update xt+1 → x t+1 .</figDesc><graphic url="image-5.png" coords="5,311.37,71.88,104.73,50.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Median distance versus number of model queries on MNIST with CNN, and CIFAR-10 with ResNet and DenseNet from top to bottom rows. 1st column: untargeted 2 . 2nd col.: targeted 2 . 3rd col.: untargeted ∞ . 4th col.: targeted ∞ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2</head><label>2</label><figDesc>distance can be interpreted in the following way: Given a byte image of size h×w×3, perturbation of size d in 2 distance on the rescaled input image amounts to perturbation on the original image of d/ √ h × w × 3 * 255 bits per pixel on average, in the range [0, 255]. The perturbation of size d in ∞ distance amounts to a maximum perturbation of 255 • d bits across all pixels on the raw image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Median distance versus number of model queries on CIFAR-100 with ResNet, DenseNet, and ImageNet with ResNet from top to bottom rows. 1st column: untargeted 2 . 2nd col.: targeted 2 . 3rd col.: untargeted ∞ . 4th col.: targeted ∞ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Success rate versus distance threshold for MNIST with CNN, and CIFAR-10 with ResNet, DenseNet from top to bottom rows. 1st column: untargeted 2 . 2nd column: targeted 2 . 3rd column: untargeted ∞ . 4th column: targeted ∞ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>budget is related to the input dimension, and the smoothness of decision boundary. We also observe the difference in model structure does not have a large influence on decision-based algorithms, if the training algorithm and the data set keep the same. For ResNet and DenseNet trained on a common data set, a decision-based algorithm achieves comparable performance in crafting adversarial examples, although DenseNet has a more complex structure than ResNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Success rate versus distance threshold for CIFAR-100 with ResNet, DenseNet, and ImageNet with ResNet from top to bottom rows. 1st column: untargeted 2 . 2nd column: targeted 2 . 3rd column: untargeted ∞ . 4th column: targeted ∞ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualized trajectories of HopSkipJumpAttack for optimizing 2 distance on randomly selected images in CIFAR-10 and ImageNet. 1st column: initialization (after blended with original images). 2nd-9th columns: images at 100, 200, 500, 1K, 2K, 5K, 10K, 25K model queries. 10th column: original images.</figDesc><graphic url="image-15.png" coords="12,120.82,201.20,180.66,83.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Success rate versus distance threshold for a distilled model, a region-based classifier and an adversarially trained model on MNIST. Blue, magenta, cyan and orange lines are used for HopSkipJumpAttack and Boundary Attack at the budget of 1K, 2K, 10K and 50K respectively. Different attacks are plotted with different line styles. An amplified figure is included near the critical ∞ -distance of 0.3 for adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>C. Proof of Theorem 3 Proof.</head><label>3</label><figDesc>For notational simplicity, we denote ξ b := φ x (x t + δu b ), and ξ = 1 B B b=1 ξ b = φ x . We use ξ, u to denote i.i.d. copies of ξ b and u b respectively. By exploiting independence of u a , u b and independence of ξ a u a , ξ b u b , the variance of the estimate with the baseline can be expressed as Var</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of various choices of step size.</figDesc><graphic url="image-17.png" coords="17,311.38,266.62,243.87,50.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Box plots of the cosine of the angle between the proposed estimates and the true gradient.</figDesc><graphic url="image-18.png" coords="18,56.43,71.96,243.76,146.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :Figure</head><label>11</label><figDesc>Figure 11: Median 2 distance versus number of model queries on MNIST with binarization + random forest. The threshold of binarization is set to be 0.1 and 0.5 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We demonstrate the superior efficiency of our algorithm over several state-of-the-art decision-based attacks through extensive experiments.• Through the evaluation of several defense mechanisms such as defensive distillation, region-based classification, adversarial training and input binarization, we suggest our attack can be used as a simple and efficient first step for researchers to evaluate new defense mechanisms.</figDesc><table><row><cell>• We propose a novel unbiased estimate of gradient direction</cell></row><row><cell>at the decision boundary based solely on access to model</cell></row><row><cell>decisions, and propose ways to control the error from</cell></row><row><cell>deviation from the boundary.</cell></row></table><note>• We design a family of algorithms, HopSkipJumpAttack, based on the proposed estimate and our analysis, which is hyperparameter-free, query-efficient and equipped with a convergence analysis. •</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Biased Boundary Attack is able to significantly reduce the number of model queries. However, it relies on the transferability between the substitute model and the target model, as with other transfer-based attacks.</figDesc><table><row><cell>Our algorithm does not rely on the additional assumption of</cell></row><row><cell>transferability. Instead, it achieves a significant improvement</cell></row><row><cell>over Boundary Attack through the exploitation of discarded</cell></row><row><cell>information into the gradient-direction estimation. Ilyas et al.</cell></row><row><cell>[9] proposed Limited attack in the label-only setting, which</cell></row><row><cell>directly performs projected gradient descent by estimating</cell></row><row><cell>gradients based on novel proxy scores. Cheng et al. [16]</cell></row><row><cell>introduced Opt attack, which transforms the original prob-</cell></row><row><cell>lem to a continuous version, and solves the new problem</cell></row><row><cell>via randomized zeroth-order gradient update. Our algorithm</cell></row><row><cell>approaches the original problem directly via a novel gradient-</cell></row><row><cell>direction estimate, leading to improved query efficiency over</cell></row><row><cell>both Limited Attack and Opt Attack. The majority of model</cell></row><row><cell>queries in HopSkipJumpAttack come in mini-batches, which</cell></row><row><cell>also leads to improved clock-time efficiency over Boundary</cell></row><row><cell>Attack.</cell></row></table><note>Brunner et al. [15]  introduced Biased Boundary Attack, which biases the sampling procedure by combining low-frequency random noise with the gradient from a substitute model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I :</head><label>I</label><figDesc>Median distance at various model queries. The smaller median distance at a given model query is bold-faced. BA and HSJA stand for Boundary Attack and HopSkipJumpAttack respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model Queries</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Distance</cell><cell>Data</cell><cell>Model</cell><cell>Objective</cell><cell></cell><cell>1K</cell><cell></cell><cell></cell><cell>5K</cell><cell></cell><cell></cell><cell>20K</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BA</cell><cell>Opt</cell><cell>HSJA</cell><cell>BA</cell><cell>Opt</cell><cell>HSJA</cell><cell>BA</cell><cell>Opt</cell><cell>HSJA</cell></row><row><cell></cell><cell>MNIST</cell><cell>CNN</cell><cell>Untargeted Targeted</cell><cell>6.14 5.41</cell><cell>6.79 4.84</cell><cell>2.46 3.26</cell><cell>5.45 5.38</cell><cell>3.76 3.90</cell><cell>1.67 2.24</cell><cell>1.50 1.98</cell><cell>2.07 2.49</cell><cell>1.48 1.96</cell></row><row><cell>2</cell><cell>CIFAR10 CIFAR100</cell><cell>ResNet DenseNet ResNet DenseNet</cell><cell>Untargeted Targeted Untargeted Targeted Untargeted Targeted Untargeted Targeted</cell><cell>2.78 7.83 2.57 7.70 1.34 9.30 1.47 8.83</cell><cell>2.07 8.21 1.78 7.65 1.20 12.43 1.22 11.72</cell><cell>0.56 2.53 0.48 1.75 0.20 6.12 0.25 5.10</cell><cell>2.34 5.91 2.12 5.33 1.12 7.40 1.23 6.76</cell><cell>0.77 4.76 0.67 3.47 0.41 8.34 0.34 8.22</cell><cell>0.21 0.41 0.18 0.34 0.08 0.92 0.11 0.75</cell><cell>0.27 0.59 0.21 0.35 0.10 1.61 0.12 0.91</cell><cell>0.29 1.06 0.28 0.78 0.14 4.06 0.13 2.89</cell><cell>0.13 0.21 0.12 0.19 0.06 0.29 0.08 0.26</cell></row><row><cell></cell><cell>ImageNet</cell><cell>ResNet</cell><cell>Untargeted Targeted</cell><cell>36.86 87.49</cell><cell>33.60 84.38</cell><cell>9.75 71.99</cell><cell>31.95 82.91</cell><cell>13.91 71.83</cell><cell>2.30 38.79</cell><cell>2.71 40.92</cell><cell>5.26 53.78</cell><cell>0.84 10.95</cell></row><row><cell></cell><cell>MNIST</cell><cell>CNN</cell><cell>Untargeted Targeted</cell><cell>0.788 0.567</cell><cell>0.641 0.630</cell><cell>0.235 0.298</cell><cell>0.700 0.564</cell><cell>0.587 0.514</cell><cell>0.167 0.211</cell><cell>0.243 0.347</cell><cell>0.545 0.325</cell><cell>0.136 0.175</cell></row><row><cell>∞</cell><cell>CIFAR10 CIFAR100</cell><cell>ResNet DenseNet ResNet DenseNet</cell><cell>Untargeted Targeted Untargeted Targeted Untargeted Targeted Untargeted Targeted</cell><cell>0.127 0.379 0.114 0.365 0.061 0.409 0.065 0.388</cell><cell>0.128 0.613 0.119 0.629 0.077 0.773 0.076 0.750</cell><cell>0.023 0.134 0.017 0.130 0.009 0.242 0.010 0.248</cell><cell>0.105 0.289 0.095 0.249 0.051 0.371 0.055 0.314</cell><cell>0.096 0.353 0.078 0.359 0.055 0.472 0.038 0.521</cell><cell>0.008 0.028 0.007 0.022 0.004 0.124 0.005 0.096</cell><cell>0.019 0.038 0.017 0.025 0.008 0.079 0.010 0.051</cell><cell>0.073 0.339 0.063 0.338 0.040 0.415 0.030 0.474</cell><cell>0.005 0.010 0.004 0.008 0.002 0.019 0.003 0.017</cell></row><row><cell></cell><cell>ImageNet</cell><cell>ResNet</cell><cell>Untargeted Targeted</cell><cell>0.262 0.615</cell><cell>0.287 0.872</cell><cell>0.057 0.329</cell><cell>0.234 0.596</cell><cell>0.271 0.615</cell><cell>0.017 0.219</cell><cell>0.030 0.326</cell><cell>0.248 0.486</cell><cell>0.007 0.091</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>3. As a comparison, BIM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">2 Attack against Defensive Distillation</cell><cell></cell><cell></cell><cell></cell><cell cols="6">2 Attack against Region-based Classification</cell><cell></cell><cell></cell><cell>2 Attack against Adversarial Training</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>10K</cell><cell>50K 50K</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>10K</cell><cell>50K 50K</cell><cell></cell><cell></cell><cell>1.0</cell><cell>50K 5 0 K</cell><cell>1 K</cell><cell>2K</cell><cell>10K</cell><cell>10 K</cell></row><row><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell>2K</cell><cell>1 0 K</cell><cell cols="2">HopSkipJump C&amp;W ( 2) Boundary</cell><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell>2 K</cell><cell>1 0 K 1 0 K</cell><cell cols="2">HopSkipJump Boundary BPDA DeepFool</cell><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell>2 K HopSkipJump Boundary C&amp;W ( 2) DeepFool</cell></row><row><cell></cell><cell>0.0</cell><cell>0</cell><cell>1K 1K</cell><cell>2</cell><cell>4 2 Distance 2K</cell><cell>DeepFool</cell><cell>6</cell><cell></cell><cell>0.0</cell><cell>0</cell><cell>1K 1K</cell><cell>2</cell><cell>4 2 Distance 2K</cell><cell></cell><cell>6</cell><cell></cell><cell>0.0</cell><cell>0</cell><cell>2</cell><cell>4 2 Distance 1K</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">∞ Attack against Defensive Distillation</cell><cell></cell><cell></cell><cell></cell><cell cols="6">∞ Attack against Region-based Classification</cell><cell></cell><cell></cell><cell>∞ Attack against Adversarial Training</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>10K</cell><cell>50K</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>10K</cell><cell>50K</cell><cell></cell><cell></cell><cell>1.0</cell><cell>HopSkipJump</cell></row><row><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell>2K</cell><cell></cell><cell cols="2">HopSkipJump Boundary C&amp;W ( ∞) FGSM</cell><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell></cell><cell></cell><cell>2K</cell><cell>10K 10K</cell><cell cols="2">50 K HopSkipJump BPDA Boundary</cell><cell>Success Rate</cell><cell>0.2 0.4 0.6 0.8</cell><cell>0.28 0.00 0.05 0.10 0.15</cell><cell>Boundary 0.29 0.30 BIM FGSM</cell><cell>0.31</cell><cell>0.32</cell></row><row><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>1K 1K</cell><cell cols="2">0.2 ∞ Distance 0.4 2K 10K</cell><cell>50K</cell><cell>0.6</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.1 1K 1K</cell><cell></cell><cell>0.2 ∞ Distance 0.3 2K</cell><cell>FGSM 0.4</cell><cell>0.5</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2 ∞ Distance 0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1, the random forest with binarization becomes extremely vulnerable to adversarial examples. Around 96% adversarial examples fall into the size-3 2 -neighborhood of the respective original examples with 1K model queries of HopSkipJumpAttack. The vulnerability is caused by the ease of activating pixels through increasing the strength by 0.1. It also indicates HopSkipJumpAttack and Boundary Attack are able to craft adversarial examples without smooth decision boundaries.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A hop, skip, and a jump originally referred to an exercise or game involving these movements dating from the early 1700s, but by the mid-1800s it was also being used figuratively for the short distance so covered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Authorized licensed use limited to: Macquarie University. Downloaded on September 26,2020 at 06:48:21 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">distances along varied queries on CIFAR10 and ImageNet can be found in Figure7. On CIFAR-10, we observe untargeted adversarial examples can be crafted within around 500</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENT</head><p>We would like to thank Nicolas Papernot and anonymous reviewers for providing their helpful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE European Symposium on Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2142" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prior convictions: Black-box adversarial attacks with bandits and priors</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical blackbox attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decisionbased adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Guessing smart: Biased sampling for efficient black-box adversarial attacks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Truong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09803</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Query-efficient hard-label black-box attack: An optimization-based approach</title>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online convex optimization in the bandit setting: gradient descent without a gradient</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Abraham D Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Tauman Kalai</surname></persName>
		</author>
		<author>
			<persName><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
				<meeting>the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic convex optimization with bandit feedback</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1035" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random gradient-free minimization of convex functions</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Spokoiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="527" to="566" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal rates for zero-order convex optimization: The power of two function evaluations</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><surname>Wibisono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2788" to="2806" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zeroth-order stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paishun</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3731" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Ronald</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><forename type="middle">Ward</forename><surname>Cheney</surname></persName>
		</author>
		<title level="m">Numerical Analysis: Mathematics of Scientific Computing</title>
				<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Concentration of Measure Phenomenon</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Behzadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhibhav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Gierke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foolbox: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chollet</forename><surname>Franc</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mitigating evasion attacks to deep neural networks via region-based classification</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Computer Security Applications Conference</title>
				<meeting>the 33rd Annual Computer Security Applications Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="369" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">D</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1310" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Safety and Security</title>
				<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
