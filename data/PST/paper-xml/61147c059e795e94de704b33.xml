<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-05">5 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zipeng</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-05">5 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.10137v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face generation</term>
					<term>Video synthesis</term>
					<term>Speech-driven animation</term>
					<term>Generative models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world talking faces often accompany with natural head movement. However, most existing talking face video generation methods only consider facial animation with fixed head pose. In this paper, we address this problem by proposing a deep neural network model that takes an audio signal A of a source person and a very short video V of a target person as input, and outputs a synthesized high-quality talking face video with personalized head pose (making use of the visual information in V), expression and lip synchronization (by considering both A and V). The most challenging issue in our work is that natural poses often cause in-plane and out-of-plane head rotations, which makes synthesized talking face video far from realistic. To address this challenge, we reconstruct 3D face animation and re-render it into synthesized frames. To fine tune these frames into realistic ones with smooth background transition, we propose a novel memory-augmented GAN module. By first training a general mapping based on a publicly available dataset and fine-tuning the mapping using the input short video of target person, we develop an effective strategy that only requires a small number of frames (about 300 frames) to learn personalized talking behavior including head pose. Extensive experiments and two user studies show that our method can generate high-quality (i.e., personalized head movements, expressions and good lip synchronization) talking face videos, which are naturally looking with more distinguishing head movement effects than the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual and auditory modalities are two important sensory channels in human-to-human or human-to-machine interaction. The information in these two modalities are strongly correlated <ref type="bibr" target="#b0">[1]</ref>. Recently, cross-modality learning and modeling have attracted more and more attention in interdisciplinary research, including computer vision, computer graphics and multimedia (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>).</p><p>In this paper, we focus on talking face video generation that transfers a segment of audio signal of a source person into the visual information of a target person. This kind of audio-driven vision models have a wide range of applications, such as bandwidth-limited video transformation, virtual anchors and role-playing game/move generation, etc. Recently, many works have been proposed for this purpose (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>). However, most of them only consider facial animation with fixed head pose.</p><p>In real-world scenarios, natural head movement plays an important role in high-quality communication <ref type="bibr" target="#b9">[10]</ref> and human perception is very sensitive to subtle head movement in real videos. In fact, human can easily feel uncomfortable in communication by talking with fixed head pose. In this paper, we propose a deep neural network model to gen- erate an audio-driven high-quality talking face video with personalized head pose.</p><p>Inferring head pose from speech (abbreviated as posefrom-speech) is not a new idea (e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>). Although some measurable correlations have been observed between speech and head pose <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, predicting head motion from speech is still a challenging problem. A practical method was suggested in <ref type="bibr" target="#b11">[12]</ref> that first infers facial activity from speech and then models head pose from facial features. In our work, by observing that simultaneously learning two related tasks in deep network may help improve the performance of both tasks, we simultaneously infers facial expressions and head pose from speech.</p><p>Since natural head poses often cause in-plane and outof-plane head rotations, it is very challenging to synthesize a realistic talking face video with high-quality facial animation and smooth background transition. To circumvent the difficult pose-from-speech problem and focus on addressing the realistic video synthesis challenge, we design the input of our system to include a segment of audio signal of a source person and a short (only a few seconds) talking face video of a target person. Note that with the popularization of smartphone, the cost of capturing a very short video (e.g., 10 seconds) is almost the same as taking a photo (e.g., selfie). Therefore we use both facial and audio information in the input short video to learn the personalized talking behavior of the target person (e.g., lip and head movements), which greatly simplifies our system.</p><p>To output a high-quality synthesized video of the target person with personalized head pose when speaking the input audio signal of source person, our system reconstructs 3D face animation and re-renders it into video frames. Given a light-weight rendering engine with limited information, these rendered frames are often far from realistic. We then propose a novel memory-augmented GAN module that can refine the rough rendered frames into realistic frames with smooth transition, according to the identity feature of the target person. To the best of our knowledge, our proposed method is the first system that can transfer the audio signal of an arbitrary source person into the face talking video of an arbitrary target person with personalized head pose. As a comparison, the previous work <ref type="bibr" target="#b6">[7]</ref> can only generate a high-quality talking face video with personalized head pose for a specified person (i.e., Obama) -since it requires a large number of training data related to this specified person -and thus, it cannot generalize to arbitrary subjects. Furthermore, when the input short talking face video is not available, our method can also use a face image as input and achieves comparable lip synchronization and video quality with previous methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Our code is publicly available 1 .</p><p>The contributions of this paper are mainly three-fold:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a novel deep neural network model that can transfer an audio signal of arbitrary source person into a high-quality talking face video of arbitrary target person, with personalized head pose and lip synchronization.</p><p>• Different from the network <ref type="bibr" target="#b13">[14]</ref> that fine tunes the rendering of a specified parametric face model into photo-realistic video frames, our memoryaugmented GAN module can generate photorealistic video frames for various face identities (i.e., corresponding to different target person).</p><p>• By first training a general mapping based on a publicly available dataset <ref type="bibr" target="#b2">[3]</ref> and fine-tuning the mapping using the input short video of the target person, we develop an effective strategy that only requires a small number of frames (about 300 frames) to learn personalized talking behavior including head pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Talking face generation</head><p>Existing talking face video generation methods can be broadly categorised into two classes according to the driven signal. One driven signal is video frames <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and the other is audio <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Video-driven talking face video generation (a.k.a face reenactment) transferred expression and sometimes head pose from a driving frame to a face image of target actor. Traditional optimization methods transferred expression using 3DMM parameters <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref> or image warping <ref type="bibr" target="#b15">[16]</ref>. Learning-based methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref> were trained by videos of target actor or general audio-visual data using GAN model conditioned on image or additional landmarks. Video-frame-driven methods only use one modality, i.e., visual information. Audio-driven methods make use of both visual and auditory modalities, which can be further classified into two sub-classes: talking face video generation for specific 1. https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose face <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref> and for arbitrary target face <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The latter methods usually take a clip of audio and one arbitrary face image as input. Chung et al. <ref type="bibr" target="#b3">[4]</ref> learned a joint embedding of the face and audio signal, and used an encoder-decoder CNN model to generate talking face video. Zhou et al. <ref type="bibr" target="#b8">[9]</ref> proposed a method in which both audio and video can serve as input by learning joint audio-visual representation. Chen et al. <ref type="bibr" target="#b1">[2]</ref> first transferred the audio to facial landmarks and then generated video frames conditioned on the landmarks. Song et al. <ref type="bibr" target="#b7">[8]</ref> proposed a conditional recurrent adversarial network that integrated audio and image features in recurrent units. However, in talking face videos generated by these 2D methods, the head pose is almost fixed during talking. This drawback is caused by the defect inherent in 2D-based methods, since it is difficult to only use 2D information alone for naturally modeling the change of pose. Although Song et al. <ref type="bibr" target="#b7">[8]</ref> mentioned that their method can be extended to personalized pose for a special case, full details on this extension were not yet presented. In comparison, we introduce 3D geometry information into the proposed system to simultaneously model personalized head pose, expression and lip synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D face reconstruction</head><p>3D face reconstruction aims to reconstruct 3D shape and appearance of human face from 2D images. A large number of methods have been proposed in this area and the reader is referred to the survey <ref type="bibr" target="#b24">[25]</ref> and reference therein. Most of these methods were based on 3D Morphable Model (3DMM) <ref type="bibr" target="#b25">[26]</ref>, which learned a PCA basis from scanned 3D face data set to represent general face shapes. Traditional methods fit 3DMM by an analysis-by-synthesis approach, which optimized 3DMM parameters by minimizing difference between rendered reconstruction and the given image <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>Learning-based methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> used CNN to learn a mapping from face images to 3DMM parameters. To deal with the lack of sufficient training data, some methods used synthetic data <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b37">[38]</ref> while others use unsupervised or weakly-supervised learning <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In this paper, we adopt the method <ref type="bibr" target="#b38">[39]</ref> for 3D face reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GANs and memory networks.</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b39">[40]</ref> have been successfully applied to many computer vision problems. The Pix2Pix proposed by Isola et al. <ref type="bibr" target="#b40">[41]</ref> has shown great power in image-to-image translation between two different domains. Later it was extended to video-to-video synthesis <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. It has also been applied to the field of facial animation and texture synthesis. Kim et al. <ref type="bibr" target="#b13">[14]</ref> use a GAN to transform rendered face image to realistic video frame. Although this method can achieve good results, it was only suitable for a specific target person, and it had to be trained by thousands of samples related to this specific person. Olszewski et al. <ref type="bibr" target="#b43">[44]</ref> proposed a network to generate realistic dynamic textures.</p><p>Memory network is a scheme to augment neural networks using external memory. It has been applied to question-answering systems <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, summarization <ref type="bibr" target="#b46">[47]</ref>, We render the 3D facial animation into video frames using the texture and lighting information obtained from input video. Then we fine tune these synthesized frames into realistic frames using a novel memory-augmented GAN module.</p><p>image captioning <ref type="bibr" target="#b47">[48]</ref>, and image colorization <ref type="bibr" target="#b48">[49]</ref>. Since this scheme can remember selected critical information, it is effective for one-shot or few-shot learning. In this paper, we use a GAN augmented with memory networks to fine tune rendered frames into realistic frames for arbitrary person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR METHOD</head><p>In this paper, we tackle the problem of generating highquality talking face video, when given an audio speech of a source person and a short video (about 10 seconds) of a target person. In addition to learn the transformation from the audio speech to lip motion and face expression, our talking face generation also considers the personalized talking behavior (i.e., head pose) of the target person.</p><p>To achieve this goal, our idea is to use 3D facial animation with personalized head pose as the kernel to bridge the gap between audio-visual-driven head pose learning and realistic talking face video generation. The flowchart of our method is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, which can be interpreted in the following two stages.</p><p>Stage 1: from audio-visual information to 3D facial animation. We use the LRW video dataset <ref type="bibr" target="#b2">[3]</ref> to train a general mapping from the audio speech to the facial expression and common head pose. Then, given an audio signal and a short video, we first reconstruct the 3D face (Section 3.1) and fine tune the general mapping to learn personalized talking behavior from the input video (Section 3.2). To this end, we obtain the 3D facial animation with personalized head pose.</p><p>Stage 2: from 3D facial animation to realistic talking face video generation. We render the 3D facial animation into video frames using the texture and lighting information obtained from input video. With these limited information, the graphic engine can only provide a rough rendering effect that is usually not realistic enough for a high-quality video. To refine these synthesized frames into realistic ones, we propose a novel memory-augmented GAN module (Section 3.4) that was also trained by the LRW video dataset. This GAN module can deal with various identities and generate high-quality frames containing realistic talking faces that matches the face identity extracted from input video.</p><p>Note that both mapping in the above two stages involves two steps: one step is the general mapping learned from the LRW video dataset and the second is a light-weight fine-tuning step that learns/retrieves personalized talking or rendering information from the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D face reconstruction</head><p>We adopt a state-of-the-art deep learning based method <ref type="bibr" target="#b38">[39]</ref> for 3D face reconstruction. It uses a CNN to fit a parametric model of 3D face geometry, texture and illumination to an input face photo I. This method reconstructs the 3DMM coefficients χ(I) = {α, β, δ, γ, p} ∈ R 257 , where α ∈ R 80 is the coefficient vector for face identity, β ∈ R 64 is for expression, δ ∈ R 80 is for texture, γ ∈ R 27 is the coefficient vector for illumination, and p ∈ R 6 is the pose vector including rotation and translation. Then the face shape S and face texture T can be represented as S = S + B id α + B exp β, T = T + B tex δ, where S and T are average shape and texture, B id , B exp and B tex are PCA basis for shape, expression and texture separately. Basel Face Model <ref type="bibr" target="#b49">[50]</ref> is used for B id and B tex , and FaceWareHouse <ref type="bibr" target="#b50">[51]</ref> is used for B exp .</p><p>The illumination is computed using the Lambertian surface assumption and approximated with spherical harmonics (SH) basis functions <ref type="bibr" target="#b51">[52]</ref>. The irradiance of vertex v i with normal vector n i and texture </p><formula xml:id="formula_0">t i is C(n i , t i , γ) = t i B 2 b=1 γ b Φ b (n i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mapping from audio to expression and pose</head><p>It is well recognized that the audio signal has strong correlation with lip and lower-half face movements. However, talking faces with only lower-half face movements are stiff and far from natural. In other words, upper-half face (including eyes and brows) movements and head pose are also essential for a natural talking face. We use both the audio information and the 3D face geometry information extracted from input video to establish a mapping from the input audio to the facial expression and head pose. Note that although a person may have different head poses when speaking the same word, the speaking style in a short period is often consistent and we provide a correlation analysis between audio and pose in Appendix A.</p><p>We extract the Mel-frequency cepstral coefficients (MFCC) feature of the input audio, and model the facial expression and head pose using 3DMM coefficients. To establish the mapping inbetween, we design a LSTM network as follows. Given the MFCC features of an audio sequence s = {s (1) , . . . , s (T ) }, a ground-truth expression coefficient sequence β = {β (1) , . . . , β (T ) }, and a ground-truth pose vector sequence p = {p (1) , . . . , p (T ) }, we generate predicted expression coefficient sequence β = { β (1) , . . . , β (T ) } and pose vector sequence p = { p (1) , . . . , p (T ) }. Denoting the LSTM network as R, our audio-to-expression-and-head pose mapping can be formulated as</p><formula xml:id="formula_1">[ β (t) , p (t) , h (t) , c (t) ] = R(E(s (t) ), h (t−1) , c (t−1) ),<label>(1)</label></formula><p>where E is an additional audio encoder that is applied to the MFCC feature of audio sequences s (t) , and h (t) , c (t)  are hidden state and cell state of LSTM unit at time t respectively. We use a loss function containing four loss terms to optimize the network: a mean squared error (MSE) loss for expression coefficients, a MSE loss for pose coefficients, an inter-frame continuity loss for pose, and an inter-frame continuity loss for expression. Denote the shorthand notation of Eq. ( <ref type="formula" target="#formula_1">1</ref>) as β = φ 1 (s), p = φ 2 (s), the loss function is formulated as:</p><formula xml:id="formula_2">L(R, E) = E s,β [(β − φ 1 (s)) 2 ] + λ 1 E s,p [(p − φ 2 (s)) 2 ] + λ 2 E s [ T −1 t=0 (φ 2 (s) (t+1) − φ 2 (s) (t) ) 2 ] + λ 3 E s [ T −1 t=0 (φ 1 (s) (t+1) − φ 1 (s) (t) ) 2 ],<label>(2)</label></formula><p>where inter-frame continuity loss is computed by the squared L 2 norm of the gradient of the pose / expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Rendering and background matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Rendering</head><p>By reconstructing the 3D face of the target person (Section 3.1) and generating the expression and pose sequences (Section 3.2), we collect a mixed sequence of 3DMM coefficients synchronized with audio speech, in which the identity, texture and illumination coefficients are from the target person, and expression and pose coefficients are from the audio. Given this mixed sequence of 3DMM coefficients, we can render a face image sequence using the rendering engine in <ref type="bibr" target="#b35">[36]</ref>.</p><p>If we compute the albedos from reconstructed 3DMM coefficients, these albedos are of low-frequency and too smooth, resulting in the rendered face images that do not appear visually similar to the input face images. An alternative is to compute a detailed albedo from input face images. I.e., we first project the reconstructed 3D shape (a face mesh) onto the image plane, and then we assign the pixel color to each mesh vertex. In this way, the albedo is computed by dividing illumination. Finally, the albedo from the frame with the most neutral expression and the smallest rotation angles is set as the albedo of the video.</p><p>We use the above mentioned both schemes in our method. In the general mapping, we use the detailed albedo for rendering, because videos in the LRW dataset are very short (about 1 second). In the personalized mapping (i.e., tuning by input short video), we use the low-frequency albedo to tolerate the change of head pose, and the input video (about 10 seconds) can provide more training data of the target person to fine tune the synthesized frames (rendered with a low-frequency albedo) into realistic ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Background matching</head><p>So far the rendered frames only have the facial part, without the hair and background regions that are also essential for a realistic talking face video. An intuitive solution is to match a background from the input video by matching the head pose. However, for a short video of about 10 seconds, we only have less than 300 frames to select a suitable background, which is very few and can be regarded as very sparse points in the possible high-dimensional pose space. Our experiment also shows that this intuitive solution cannot produce good video frames.</p><p>In our method, we propose to extract some keyframes from the synthesized pose sequence, where the keyframes correspond to critical head movements in the synthesized pose sequence. We choose the key frames to be the frames with largest head orientation in one axis in a short period of time, e.g., the frame with leftmost or rightmost head pose. Then we only match backgrounds for these keyframes. We call these matched backgrounds as key backgrounds. For those frames between two neighboring keyframes, we use linear interpolation to determine their backgrounds. The pose in each frame is also modified to fit the background. Finally the whole rendered frames are assembled by including the matched backgrounds.</p><p>If only a signal face image I is input instead of a short video, we obtain the matched background by rotating I to the predicted pose using the face profiling method in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Memory-augmented GAN for refining frames</head><p>The synthesized frames rendered by the light-weight graphic engine <ref type="bibr" target="#b35">[36]</ref> are usually far from realistic. To refine these frames into realistic ones, we propose a memoryaugmented GAN. The differences between our method and the previous GAN-based face reenactment (FR) methods <ref type="bibr" target="#b13">[14]</ref> are:</p><p>• FR only refines the frames for a single, specified face identity, while our method can deal with various face identities. I.e., given different identity features of target faces, our method can output different frame refinement effects with the same GAN model. • FR uses thousands of frames to train a network for a single, specified face identity, while we only use a few frames for each identity in the general mapping learning. Based on the general mapping, we fine tune the network using a small number of frames for the target face (from the input short video).</p><p>We model the frame refinement process as a function Φ that maps from the rendered frame (i.e., synthesized frame rendered by the graphic engine) domain R to the real frame domain T using paired training data {(r i , g i )}, r i ∈ R and g i ∈ T . To handle multiple-identity refinement, we build a GAN network that consists of a conditional generator G, a conditional discriminator D and an additional memory network M (Figure <ref type="figure" target="#fig_3">2</ref>). The memory network stores paired features, i.e., (spatial feature, identity feature), which are updated during the training process. Its role is to remember representative identities including rare instances in the training set, and retrieve the best-match identity feature during the test. The conditional generator takes a window of rendered frames (i.e., a subset of 3 adjacent frames r t−2 , r t−1 , r t ) and an identity feature as input, and synthesize a refined frame o t using the U-Net <ref type="bibr" target="#b52">[53]</ref> with AdaIN <ref type="bibr" target="#b53">[54]</ref>. The conditional discriminator takes a window of rendered frames and either a refined frame or a real frame as input, and decides whether the frame is real or synthesized.</p><p>Attention-based generator G. We use an attention-based generator to refine rendering frames. Given a window of rendered frames (r t−2 , r t−1 , r t ) and an identity feature f t (extracted from ArcFace <ref type="bibr" target="#b54">[55]</ref>), the generator synthesizes both a color mask C t and an attention mask A t , and outputs a refined frame o t that is the weighted average of the rendered frame and color mask:</p><formula xml:id="formula_3">o t = A t • r t + (1 − A t ) • C t<label>(3)</label></formula><p>The attention mask specifies how much each pixel in the generated color mask contributes to the final refinement. Our generator architecture is based on a U-Net structure 2  and has two modifications. (1) To generate two outputs (i.e., color and attention masks), we modify the last convolution block to two parallel convolution blocks, in which each one generates one mask. (2) To take both a window of rendered frames and an identity feature as input, we adopt AdaIN <ref type="bibr" target="#b53">[54]</ref> to incorporate identity features into our network, where AdaIN parameters are generated from input identity features. Experimental results show that our network can generate delicate target-person-dependent texture for various identities. Memory network M . We use a memory network to remember representative identities including rare instances in the training set, so that during the test we can retrieve similar identity feature from it. We adapt the network in <ref type="bibr" target="#b48">[49]</ref> in our system by modifying it to output continuous frames. In particular, our memory network stores paired spatial features and identity features. The spatial feature is extracted by (1) feeding the input rendered frame into ResNet18 <ref type="bibr" target="#b55">[56]</ref> pre-trained on ImageNet <ref type="bibr" target="#b56">[57]</ref>, (2) extracting the 'pool5' feature, and (3) passing the 'pool5' feature to a learn-able fully connected layer and normalization. The paired identity 2. The attention mechanism has also been used in the work <ref type="bibr" target="#b16">[17]</ref>. However, the difference in our network is that we also input an additional identity feature into the network, which enables generating different refining effects for different identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real video</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated</head><p>Real video Generated Fig. <ref type="figure">3</ref>. Comparison of real videos with natural head pose and our generated talking face videos with personalized behavior. Our method can achieve both good lip synchronization and personalized head pose.</p><p>feature is extracted by feeding the corresponding groundtruth frame into ArcFace <ref type="bibr" target="#b54">[55]</ref>.</p><p>During the training, we update the memory network using paired features extracted from the training set. This updating includes (1) a threshold triplet loss 3 <ref type="bibr" target="#b48">[49]</ref> to make spatial features of similar identities closer and spatial features of different identities farther, and (2) a memory item updating process, where either an existing feature pair is updated or an old pair is replaced 4 by a new pair. During the test, we retrieve the identity feature by using the spatial feature as query, finding its nearest spatial feature in memory and returning the corresponding identity feature. Noting that directly feeding this feature into the generator may lead to jittering effects, we smooth the retrieved features in multiple adjacent frames by interpolation and use the smoothed features as inputs for the generator.</p><p>Discriminator D. The conditional discriminator takes a window of rendered frames and a checking frame (either a refined frame or a real frame) as input, and discriminates whether the checking frame is real or not. We adopt Patch-GAN <ref type="bibr" target="#b40">[41]</ref> architecture as our discriminator.</p><p>Loss function. The loss function of our GAN model 5 has three terms: a GAN loss, an L 1 loss, and an attention loss <ref type="bibr" target="#b16">[17]</ref> to prevent the attention mask A from saturation, which also enforces the smoothness of the attention mask. Denoting the input rendered frames as r, the identity feature as f , and the ground truth real frames as g, the loss function is formulated as:</p><formula xml:id="formula_4">L(G, D) = (E r,g [log D(r, g)] + E r [log(1 − D(r, G(r, f )))]) + λ 1 E r,g [||g − G(r, f )|| 1 ] + λ 2 E r [||A|| 2 ] + λ 3 E r [ H,W i,j (A i+1,j − A i,j ) 2 + (A i,j+1 − A i,j ) 2 ]<label>(4)</label></formula><p>3. We use the cosine similarity for both spatial and identity features.</p><p>4. An old pair is replaced when the similarity between current identity feature and the closest identity feature is smaller than a threshold.</p><p>5. Note that during the training process, the memory network is updated separatedly and GAN is trained after each updating of the memory network.</p><p>We train the GAN model to optimize the loss function:</p><formula xml:id="formula_5">G * = argmin G max D L(G, D)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We implemented our method in PyTorch. All experiments are performed on a PC with a Titan Xp GPU. The code is publicly available 6 . The dynamic results in this section can be found in accompanying demo video 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup</head><p>In our model, the two components (audio to expression and pose by LSTM, and memory-augmented GAN) involves two training steps: (1) a general mapping trained by the LRW video dataset <ref type="bibr" target="#b2">[3]</ref> and (2) fine tuning the general mapping to learn the personalized talking behavior. At the fine tuning step, we collect 15 real-world talking face videos of single person from Youtube. In each video, we use its first 12 seconds (about 300 frames) as the training data. Given the well-trained general mapping, we observe that 300 frames 8 are sufficient for the fine tuning task. In Section 4.3, we evaluate our personalized fine tuning effect (see Figure <ref type="figure">3</ref> for two examples) on both the audio from the original Youtube video and the audio from the VoxCeleb and TCD dataset. Below we denote the general mapping and fine-tuned personalized mapping as Ours-G and Ours-P, respectively. The network is first trained in Ours-G (using general dataset) and then fine-tuned in Ours-P (for a specific person).</p><p>In our experiments, the parameters in Eq.( <ref type="formula" target="#formula_2">2</ref>) is λ 1 = 0.2, λ 2 = 0.01 and λ 3 = 0.0001. The parameters in Eq.( <ref type="formula" target="#formula_4">4</ref>) is λ 1 = 100, λ 2 = 2, λ 3 = 1e − 5. Our method works well for people of different races and ages. Some examples are illustrated in Figure <ref type="figure">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, our method involves two stages.</p><p>Here we evaluate the importance of these two stages.</p><p>In the first stage, we predict both the head pose and expression from the input audio. If we only predict the expression without the pose estimation, as shown in the second row of Figure <ref type="figure">5</ref>, the generated results are good in lip synchronization, but look rigid due to the fixed head position, which is far from natural.</p><p>There are two distinct characteristics in the second stage. First we include the identity feature in the input of GAN. Second, we add a memory network in the GAN model to store representative identities in the training set and retrieve the best-matched identity feature during the test. If we exclude the identity feature from input and the memory network from the GAN model, the personalized refining effect of different identities and expressions would be the same and then the network can not be well optimized. As shown in the third row of Figure <ref type="figure">5</ref>, without them, the generated results have bad mouth details (e.g., strange teeth), uneven cheek areas, and black spots on face. If we exclude the memory network from the GAN model but keep the identity feature input, and use the mean of identity features in fine tuning, the results (the middle row in Figure <ref type="figure" target="#fig_6">6</ref>) are not as good as our results (the last row in Figure <ref type="figure" target="#fig_6">6</ref>), which have much better fine details (e.g., wrinkles) and look more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state of the arts</head><p>As mentioned in Section 4.1, our model involves two important mappings: Ours-G and Ours-P. Note that (1) the inputs to the personalized mapping Ours-P are a short video of 300 frames (to fine-tune) and an audio, and (2) the inputs to the general mapping Ours-G are only one frame (since it does not need to fine-tune) and an audio.</p><p>In this section, we show that the Ours-P model can generate realistic talking face video with more distinguishing head movement effects than the state-of-the-art methods. Even for the degenerate case that uses a single face image as input, the Ours-G model can generate comparable lip synchronization with previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison with Ours-P</head><p>We first compare Ours-P with three state-of-the-art audiodriven talking face generation methods: YouSaidThat <ref type="bibr" target="#b3">[4]</ref>, DAVS <ref type="bibr" target="#b8">[9]</ref> and ATVG <ref type="bibr" target="#b1">[2]</ref>. These three methods are all 2Dbased and operate on the image directly, i.e., without using 3D face geometry and rendering. Thus their inputs include only one facial image and an audio. We emphasize that the head positions in the results output from these methods are DAVS ATVG You said that Ours-P Fig. <ref type="figure">7</ref>. Qualitative results of our method and state-of-the-art methods. The first row shows two ground-truth videos (segments from Youtube videos) and the other rows show the results of different methods (DAVS <ref type="bibr" target="#b8">[9]</ref>, You said that <ref type="bibr" target="#b3">[4]</ref>, ATVG <ref type="bibr" target="#b1">[2]</ref> and Ours-P). Our method achieves good image quality, lip synchronization (w.r.t. real video) and personalized head movements.</p><p>fixed. Although Ours-P takes more frames (for the purpose of fine-tuning) as input, we learn personalized head pose. Some qualitative results are shown in Figure <ref type="figure">7</ref>.</p><p>It is very challenging to evaluate the visual quality and naturalness of synthesized videos, in particular regarding the human face. We therefore design a user study to perform the assessment based on subjective score evaluation. To fine tune the network by considering personalized talking behavior, we collect 15 real-world talking face videos and use the portion of their first 12 seconds for training 15 personalized mappings. In our user study, for each of these 15 personalized mappings, we test two sets of audio: one is the audio from the remaining portions of the original real videos, and the other is an audio chosen from VoxCeleb <ref type="bibr" target="#b57">[58]</ref> or TCD <ref type="bibr" target="#b58">[59]</ref>. We choose these two datasets because they have a long audio segment to better visualize the change of head pose. Then we can construct 30 comparison groups. Each group have five videos: one original video and four generated videos by four methods. Each personalized video devotes to two groups, based on its two sets of audios. 20 participants attended the user study and each of them compared all 30 groups and answered 3 questions for each group. For a fair comparison, each group is presented by a randomly shuffled order of five videos. Participants are asked to select the best video according to three criteria: image quality, lip synchronization and the naturalness of talking. The results of subjective scores are summarized in Table <ref type="table">1</ref>, showing that our method achieves better performance in all three criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Subjective score evaluation of our method and state-of-the-art methods. Each row shows the percentages of a method chosen as the best for different criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Image quality Lip synchronization Natural DAVS <ref type="bibr" target="#b8">[9]</ref> 2.17% 2.33% 2.67% You said that <ref type="bibr" target="#b3">[4]</ref> 3.50% 20.50% 4.17% ATVG <ref type="bibr" target="#b1">[2]</ref> 5.67% 32.33% 9.50% Ours-P 88.67% 44.83% 83.67% TABLE 2 Quantitative results of our method and state-of-the-art methods (Chen <ref type="bibr" target="#b59">[60]</ref>, Wiles <ref type="bibr" target="#b17">[18]</ref>, You said that <ref type="bibr" target="#b3">[4]</ref>, DAVS <ref type="bibr" target="#b8">[9]</ref> and ATVG <ref type="bibr" target="#b1">[2]</ref>), evaluated on LRW dataset which contains 25,000 videos. All the methods are evaluated using the same evaluation criterion from ATVG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Chen</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with Ours-G</head><p>Since most previous talking face generation methods do not consider personalized head pose, we further compare our Ours-G (i.e., without fine tuning personalized talking behavior) with representative audio-driven methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b59">[60]</ref>. We directly compare the generated results by different methods with the ground-truth videos. We follow ATVG <ref type="bibr" target="#b1">[2]</ref> to apply three widely used metrics for audio-driven talking face generation evaluation, i.e., the classic PSNR and SSIM metrics for image quality evaluation,  <ref type="bibr" target="#b2">[3]</ref>, which says the word "absolutely" (the first four columns) and the word "abuse" (the last four columns), respectively. The second to the last rows are the generated results from four different methods. Our method has comparable results (well preserving facial texture and good lip synchronization) with ATVG <ref type="bibr" target="#b1">[2]</ref>. and the landmark distance (LMD) for accuracy evaluation of lip movement. The results are summarized in Table <ref type="table">2</ref>, showing that our method has the best PSNR values and has comparable SSIM and LMD metric values with ATVG <ref type="bibr" target="#b1">[2]</ref>. Some qualitative comparisons are shown in Figure <ref type="figure" target="#fig_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of head pose behavior</head><p>To objectively evaluate the quality of personalized head pose, we propose a new metric HS to measure the similarity of head poses between the generated video and real video. We use the three Euler angles to model head movements <ref type="bibr" target="#b60">[61]</ref>, i.e., pitch, yaw, and roll corresponding to the movement of head nod, head shake/turn, and head tilt, respectively. We compute a histogram P real of pose angles in real personalized video, and a histogram P gen of pose angles in the generated video. Then we compute the normalized Wasserstein distance W 1 <ref type="bibr" target="#b61">[62]</ref> between P real and P gen . The lower the distance, the more similar the two head pose distribution. Our new metric HS is formulated as</p><formula xml:id="formula_6">HS = 1 − W 1 (P real , P gen )<label>(6)</label></formula><p>where HS is in the range [0, 1] and larger HS indicates higher similarity of head pose. The average HS score of 15 pairs of personalized videos is 0.859, and the maximum and minimum score are 0.956 and 0.702 respectively, showing that our generated video has a high similarity to real video in term of head movement behavior.</p><p>To evaluate the validity of this new metric, we further perform another user study to examine the correlation be-tween the subjective evaluation and our metric values. 20 participants attended this user study. Each participant was asked to compare 15 pairs of generated videos and real videos. They ranked from 1 to 5 based on the head pose similarity of the two videos (1-not similar, 2-maybe not similar, 3-don't know, 4-maybe similar, 5-similar). The results of votes (in parentheses) are 1 (25), 2 (47), 3 (35), 4 (110) and 5 (83). The average score is 3.60, and the percentage of scores 4 and 5 ('maybe similar' and 'similar') is 64.3%. Only 24.0% ranks are 'not similar' or 'maybe not similar'. Furthermore, the correlation coefficient between subjective ranking and the HS metric is 0.65, demonstrating that our metric has strong positive correlation with human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a deep neural network model that generates a high-quality talking face video of a target person who speaks the audio of a source person. Since natural talking head poses often cause in-plane and out-of-plane head rotations, to overcome the difficulty of rendering a realistic frames directly from input video to output video, we reconstruct the 3D face and use the 3D facial animation to bridge the gap between audio-visual-driven head pose learning and realistic talking face video generation. The 3D facial animation incorporates personalized head pose and is rerendered into video frames using a graphic engine. Finally the rendered frames are fine-tuned into realistic ones using a memory-augmented GAN module. Experiments results and user studies show that our method can generate highquality talking head video with personalized head pose, and this distinct feature has not been considered in state-of-theart audio-driven talking face generation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A CORRELATION BETWEEN AUDIO AND HEAD POSE</head><p>Our proposed deep network model learns a mapping from audio features to facial expression and head pose. We infer the head pose from an audio, based on the observation that the speaking style of a person in a short period is often consistent. In our model, we have two training steps: (1) a general mapping trained by LRW, and (2) fine-tuning step using a short video of the target person as the training data. The head pose pose estimation of the target person is mainly learned during the fine-tuning step, because LRW includes different person's data and their head movements vary; so we design to learn the head movement behavior from the short video of the target person.</p><p>To verify the observation that we can infer the head pose from the audio we conduct a correlation analysis between the audio and pose in these short videos. We represent the audio using the MFCC feature and represent the pose using the three Eular angles (i.e., pitch, yaw, roll). For a MFCC feature s, we find all MFCC features in the same short video that are in its local neighborhood, and calculate the distance between the neighboring MFCC pair and the distance between the corresponding poses. We calculate the correlation between these two distances using a spherical neighborhood with radius= 0.5 * |s|. The average correlation coefficient of 15 short videos is 0.45, and the maximum and minimum correlation coefficients are 0.58 and 0.24 respectively. These results indicate there exists a positive correlation between the audio and pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B USER STUDY ON THE LENGTH OF INPUT SHORT VIDEO</head><p>In our method, we use a short video of a target person to fine tune the general mapping into a personalized mapping, which learns personalized talking behavior. Here, we study the relation between the length of the input short video and the quality of the output talking face video. We generate the results by inputting short videos of different lengths, i.e., 4s (100 frames), 8s (200 frames), 12s (300 frames), 20s (500 frames), and 32s (800 frames). Some qualitative results are shown in Figure <ref type="figure" target="#fig_8">9</ref>.</p><p>We first conducted an expert interview and asked an expert who is good at video quality assessment to choose the results that have the best quality and explain why. The expert chose the results trained by 300, 500 and 800 frames, and the reason was that the results trained with less frames have obviously lower image quality around the mouth and teeth areas, and somehow look strange.</p><p>Then we further conducted the following user study. We asked each user to (1) watch a real video, (2) watch the results generated by the model trained with N = 100, 200, 300, 500, 800 frames, and (3) select the best ones (in terms of visual quality) from generated results. Note that in our user study, more than one result can be selected as the best; i.e., the user can select multiple results that have the same best quality. 11 participants attended this user study.</p><p>For the results generated with N = 100, 200, 300, 500, 800 frames, 0%, 0%, 36.4%, 36.4%, 63.6% users selected them as the best one, respectively. These results validated that the models trained by less than 300 frames produce apparently worse results and the model trained by 300 frames achieves a good balance between visual quality and computational efficiency (using fewer frames for training).</p><p>Discussion on frame numbers in personalized and general mapping. N = 100 generates low video quality, possibly because in the personalized mapping (i.e. finetuning by the input short video), we use the low-frequency albedo (to tolerate the change of head pose), and it requires more frames to fine-tune the low-frequency albedo into realistic ones. While in the general mapping (i.e. trained by LRW, without fine-tuning by the short video), we use the detailed albedo. So using only one frame in the general mapping may generate a little bit more realistic results than using a few frames (e.g., 10-30 frames) to fine-tune the albedo in the personalized mapping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>R. Yi, Z. Ye, Y.-J. Liu are with MOE-Key Laboratory of Pervasive Computing, the Department of Computer Science and Technology, Tsinghua University, Beijing, China. Y.-J. Liu is the corresponding author. E-mail: liuyongjin@tsinghua.edu.cn. • J. Zhang is with the School of Mathematical Sciences, University of Science and Technology of China, Hefei, China. • H. Bao is with the college of Computer Science and Technology, Zhejiang University, Hangzhou, China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Flowchart of our method. (Stage 1) We train a general mapping from the input audio to the facial expression and common head pose. Then, we reconstruct the 3D face and fine tune the general mapping to learn personalized talking behavior from the input video. So we can obtain the 3D facial animation with personalized head pose. (Stage 2) We render the 3D facial animation into video frames using the texture and lighting information obtained from input video. Then we fine tune these synthesized frames into realistic frames using a novel memory-augmented GAN module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where Φ b : R 3 → R are SH basis functions, γ b are SH coefficients and B = 3 is the number of SH bands. The pose is represented by rotation angles and translation. A perspective camera model is used to project the 3D face model onto the image plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Our memory-augmented GAN for refining rendered frames into realistic frames. The generator takes a window of rendering frames and an identity feature as input, and generate a refined frame based on attention mechanism. Discriminator judges whether a frame is real or not. The memory network is introduced to remember representative identities during training and retrieve the best-match identity feature during test. During the training, the memory network is updated by paired spatial features and ground-truth identity features. During the test, the memory network retrieves the best-match identity feature using the spatial feature as query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>6. https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose 7. https://cg.cs.tsinghua.edu.cn/people/ ∼ Yongjin/Yongjin.htm 8. See Appendix B for details of this observation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Our method works well for people of different races and ages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Ablation study of replacing the memory network by using the mean of identity vectors in fine tuning. The first row shows the ground truth video (a segment from Youtube video). The middle row shows the results by replacing the memory network with the mean of identity vectors. The last low shows the results of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Qualitative results of our method (without fine tuning personalized talking behavior) and state-of-the-art methods. The first row is two examples of the ground truth, taken from LRW dataset<ref type="bibr" target="#b2">[3]</ref>, which says the word "absolutely" (the first four columns) and the word "abuse" (the last four columns), respectively. The second to the last rows are the generated results from four different methods. Our method has comparable results (well preserving facial texture and good lip synchronization) with ATVG<ref type="bibr" target="#b1">[2]</ref>.</figDesc><graphic url="image-84.png" coords="9,103.53,278.66,225.54,56.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Study on the relation between the length of the input short video and the quality of generated results. The first row shows the ground truth video (a segment from Youtube video). The remaining rows show results using the personalized mapping trained by a short video of 100, 200, 300, 500 and 800 frames respectively.</figDesc><graphic url="image-98.png" coords="10,352.56,326.74,215.23,53.80" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Auditory versus visual learning of temporal patterns</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Nazzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="478" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical crossmodal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Asian Conference on Computer Vision (ACCV 2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The visual microphone: passive recovery of sound from video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="79" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech2face: Learning the face behind a voice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7539" to="7548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI 2019)</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="919" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI 2019)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward a minimal representation of affective gestures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="118" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting head pose from speech with a conditional variational autoencoder</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Laycock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference of the International Speech Communication Association (INTERSPEECH</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3991" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint learning of facial expression and head pose from speech</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Laycock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2484" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rigid head motion in expressive speech animation: Analysis and synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1075" to="1086" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bringing portraits to life</title>
		<author>
			<persName><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANimation: Anatomically-aware facial animation from a single image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fewshot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.08233, 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One-shot face reenactment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">3251</biblScope>
			<date type="published" when="1908">1908. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photo-real talking head with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4884" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Realistic speech-driven facial animation with GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01251-8</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01251-8" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912">1912.05566, 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facevr: Real-time gaze-aware facial reenactment in virtual reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3D face reconstruction, tracking, and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="550" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 1999</title>
				<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH 1999</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3D face rigs from monocular video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D face reconstruction with geometry details from a single image</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4756" to="4770" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D face reconstruction by learning from synthetic data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision</title>
				<imprint>
			<date type="published" when="2016">3DV 2016. 2016</date>
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric CNN regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1585" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollh Öfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3735" to="3744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlinear 3D face morphable model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3D morphable model regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8377" to="8386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GANFIT: generative adversarial network fitting for high fidelity 3D face reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1155" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CNN-based real-time dense face reconstruction with inverse-rendered photorealistic face images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1294" to="1307" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate 3D face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yakovenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1152" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-shot video-to-video synthesis</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Realistic dynamic facial textures from a single image using GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5439" to="5448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
				<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Abstractive summarization of reddit posts with multi-level memory networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2519" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attend to you: Personalized image captioning with context sequence memory networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6432" to="6440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coloring with limited data: Few-shot colorization via memory augmented networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">292</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
				<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<publisher>SIG-GRAPH</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Voxceleb: A largescale speaker identification dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<publisher>INTER-SPEECH</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2616" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">TCD-TIMIT: an audio-visual corpus of continuous speech</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="538" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generalization of a vision-based computational model of mind-reading</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction, First International Conference (ACII 2005)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="582" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Topics in optimal transportation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
