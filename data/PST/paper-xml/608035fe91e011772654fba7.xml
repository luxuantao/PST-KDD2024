<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-21">April 21, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhuiyi Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhuiyi Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhuiyi Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Zhuiyi Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Zhuiyi Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-21">April 21, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.09864v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Position encoding in transformer architecture provides supervision for dependency modeling between elements at different positions in the sequence. We investigate various methods to encode positional information in transformer-based language models and propose a novel implementation named Rotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of being expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and capability of equipping the linear self-attention with relative position encoding. As a result, the enhanced transformer with rotary position embedding, or RoFormer, achieves superior performance in tasks with long texts. We release the theoretical analysis along with some preliminary experiment results on Chinese data. The undergoing experiment for English benchmark will soon be updated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The sequential order of words plays a vital role in natural language. Recurrent-based models (RNNs) encode tokens' order by recursively computing a hidden state along the time dimension. Convolution-based models (CNNs) <ref type="bibr" target="#b4">[5]</ref> were typically considered position-agnostic, but recent work <ref type="bibr" target="#b8">[9]</ref> has shown that the commonly used padding operation can implicitly learn position information. In recent years, the effectiveness of transformer-based models was shown on various natural language processing (NLP) tasks such as context representation learning <ref type="bibr" target="#b3">[4]</ref>, machine translation <ref type="bibr" target="#b20">[21]</ref>, and language modeling <ref type="bibr" target="#b15">[16]</ref>. Unlike recurrent-based and convolution-based models, transformer-based models utilize the self-attention architecture to capture the dependency among tokens in the context, which provides better parallelization than RNNs and can model longer intra-token relations than CNNs. <ref type="foot" target="#foot_0">1</ref>Since transformer-based models contain no recurrence and no convolution, and the self-attention architecture is shown to be position-agnostic <ref type="bibr" target="#b25">[26]</ref>, different approaches have been proposed to inject position information into the model. One line of works focuses on absolute position encoding, where absolute position encoding which are trainable <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref> or generated by pre-defined function <ref type="bibr" target="#b20">[21]</ref> were added to context representations. The other line of works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref> focuses on relative position encoding, which typically injects relative position information into the attention calculation. In addition to these approaches, <ref type="bibr" target="#b12">[13]</ref> has proposed to model the dependency of position encoding from the perspective with Neural ODE <ref type="bibr" target="#b0">[1]</ref>, and <ref type="bibr" target="#b21">[22]</ref> has proposed to model the position information in complex space.</p><p>In this work, we first establish a formal description of the position encoding problem in self-attention architecture and revisit previous works in Section 2. We then propose the rotary position encoding (RoPE) and study its properties in Section 3. We report preliminary experiments in section 4. Our contributions are as follows:</p><p>• We investigate previous works on relative position encoding and find most of them based on the decomposition of adding position encoding to the context representations. We propose to encode relative position by multiplying the context representations with a rotation matrix with a clear theoretical interpretation.</p><p>• We study the properties of RoPE and show that it decays with the relative distance increased, which is desired for natural language encoding. We argue that previous relative position encoding approaches are not compatible with linear self-attention and show that RoPE can be used in such mechanism.</p><p>• We demonstrate that RoFormer shows superior performance than peer models dealing with long texts. Preliminary experiments with pre-trained Chinese RoFormer<ref type="foot" target="#foot_1">2</ref> is carried out on downstream tasks. Benchmarks on English dataset is undergoing and will be released once finished.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary</head><p>Let S N = {w i } N i=1 be a sequence of N input tokens with w i being the i th element. The corresponding word embedding of S N can be denoted as</p><formula xml:id="formula_0">E N = {x i } N i=1</formula><p>, where x i ∈ R d is the d-dimensional word embedding vector of token w i without position information. The self-attention first incorporates position information to the word embeddings and transforms them into queries, keys, and value representations.</p><formula xml:id="formula_1">q m = f q (x m , m) k n = f k (x n , n) v n = f v (x n , n)<label>(1)</label></formula><p>Where q m , k n , v n incorporates the m th and n th position by functions f q , f k and f v respectively. The attention weights are then calculated using query and key representations, and the output can be computed as the weighted sum of value representations.</p><formula xml:id="formula_2">a m,n = exp( q m kn √ d ) N j=1 exp( q m kj √ d ) o m = N n=1 a m,n v n (2)</formula><p>The research on position encoding of transformer mainly focuses on choosing suitable function forms of eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Absolute position embedding</head><p>A typical choice of eq. ( <ref type="formula" target="#formula_1">1</ref>) is</p><formula xml:id="formula_3">f t:t∈{q,k,v} (x i , i) := W t:t∈{q,k,v} (x i + p i )<label>(3)</label></formula><p>Where p i ∈ R d is a d-dimensional vector depending of the position of token x i . <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref> used a set of trainable vectors p i ∈ {p t } L t=1 , where L is the maximum sequence length. On the other hand, <ref type="bibr" target="#b20">[21]</ref> has proposed to generate p i using the sinusoidal function.</p><formula xml:id="formula_4">p i,2t = sin(k/10000 2t/d ) p i,2t+1 = cos(k/10000 2t/d )<label>(4)</label></formula><p>Where p i,2t is the 2t th element of the d-dimensional vector p i . In Section x, we will show that our proposed RoPE is related to this approach from the perspective of using the sinusoidal function, but ours incorporates relative position information by multiplying sinusoidal function to the context representation instead of adding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relative position embedding</head><p>[18] used a different setting of eq. ( <ref type="formula" target="#formula_1">1</ref>) as following:</p><formula xml:id="formula_5">f q (x m ) := W q x m f k (x n , n) := W k (x n + pk r ) f v (x n , n) := W v (x n + pv r )<label>(5)</label></formula><p>Where pk r , pv r ∈ R d are trainable relative position embeddings. Note that r = clip(m − n, r min , r max ) represents the relative distance between position m and n. They clipped the relative distance with the hypothesis that precise relative position information is not useful beyond a certain distance.</p><p>Keeping the form of eq. ( <ref type="formula" target="#formula_3">3</ref>), <ref type="bibr" target="#b2">[3]</ref> has proposed to decompose the q m k n term in eq. ( <ref type="formula">2</ref>) as</p><formula xml:id="formula_6">q m k n = x m W q W k x n + x m W q W k p n + p m W q W k x n + p m W q W k p n<label>(6)</label></formula><p>They replaced absolute position embedding p n with its sinusoid-encoded relative counterpart pm−n and replaced absolute position p m in the third and fourth term with two trainable vectors u, v independent of the query positions. Further, W k is distinguished for the content-based and location-based key vectors x n and p n , denoted as W k and W k , resulting in:</p><formula xml:id="formula_7">q m k n = x m W q W k x n + x m W q W k pm−n + u W q W k x n + v W q W k pm−n<label>(7)</label></formula><p>It is worth mentioning that they remove the position information in the value term by setting f v (x j ) := W v x j . Later works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref> followed this step by only considering inject relative position information into the attention weights.</p><p>[17] revised eq. ( <ref type="formula" target="#formula_6">6</ref>) as</p><formula xml:id="formula_8">q m k n = x m W q W k x n + b i,j<label>(8)</label></formula><p>Where b i,j is a trainable bias. <ref type="bibr" target="#b10">[11]</ref> investigated the middle two terms of ?? and found little correlations between absolute positions and words. Follow <ref type="bibr" target="#b16">[17]</ref>, they have proposed to model a pair of words or positions by using different projection matrices.</p><formula xml:id="formula_9">q m k n = x m W q W k x n + p m U q U k p n + b i,j<label>(9)</label></formula><p>[6] argued that a relative positions of word pair can only be fully modeled by using both the middle two terms of ??, so they have proposed to replace the absolute position embeddings p m and p n in these two terms with relative position embeddings pm−n .</p><formula xml:id="formula_10">q m k n = x m W q W k x n + x m W q W k pm−n + p m−n W q W k x n<label>(10)</label></formula><p>[15] has compared four variants of relative position embeddings and shown that the variant similar to eq. ( <ref type="formula" target="#formula_10">10</ref>) is the most efficient among the other three.</p><p>All these works modified eq. ( <ref type="formula" target="#formula_6">6</ref>) based on the decomposition of eq. ( <ref type="formula" target="#formula_3">3</ref>) under the self-attention setting in eq. ( <ref type="formula">2</ref>), which is originally from <ref type="bibr" target="#b20">[21]</ref>. They share the same nature that the position information is injected by deliberately adding to the context representations. Different from these work, our approach aims to derive the relative position encoding directly from eq. ( <ref type="formula" target="#formula_1">1</ref>) under some constraints. In Section xxx, we show that the derived approach is more interpretable by incorporating relative position information with the rotation of context representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we discuss the proposed rotary position embedding (RoPE). We first formulate the relative position encoding problem in section (3.1), we then derive the RoPE in section (3.2) and investigate its properties in section (3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Language modeling in Transformer integrates position information of individual tokens through self-attention. We start from eq. ( <ref type="formula" target="#formula_1">1</ref>) and notice that the q m k n term in eq. ( <ref type="formula">2</ref>) actually facilitates information exchange between tokens at different positions. In order to incorporate relative position information, we require the inner product of query q m and key k n be formulated by a function g, which takes only the word embeddings x m , x n , and their relative position m − n as input variables. In other words, we hope the inner product encodes position information only in the relative form:</p><formula xml:id="formula_11">f q (x m , m), f k (x n , n) = g(x m , x n , m − n)<label>(11)</label></formula><p>Next, finding such a encoding mechanism is equivalent to solve the function f q (x m , m) and f k (x n , n) that conforms above relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rotary Position Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">A 2D case</head><p>We start from simple case with dimension d = 2. Under this setting, we make use of the geometric property of vectors on 2D plane and its complex form to prove (refer to Appendix A for more details) that a solution to our formulation eq. ( <ref type="formula" target="#formula_11">11</ref>) is:</p><formula xml:id="formula_12">f q (x m , m) = (W q x m )e imθ f k (x n , n) = (W k x n )e inθ g(x m , x n , m − n) = Re[(W q x m )(W k x n ) * e i(m−n)θ ]<label>(12)</label></formula><p>where Re[•] is the real part of a complex number and (W k x n ) * represents the conjugate complex number of (W k x n ). θ ∈ R is a preset non-zero constant. We can further write f {q,k} in matrix multiplication:</p><formula xml:id="formula_13">f {q,k} (x m , m) = cos mθ − sin mθ sin mθ cos mθ W<label>(11)</label></formula><p>{q,k} W</p><p>{q,k} W</p><p>m ) is x m expressed in the 2D coordinates. Similarly, function g can be turned into matrix form. Thus, we come up with a solution to formulation in section 3.1 under the 2D case. Specifically, incorporating relative position embedding is straightforward: simply rotate the affine-transformed word embedding vector by amount of angle in multiples of its position index. Due to this characteristic, we name it Rotary Position Embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">General form</head><p>In order to generalize our result in 2D to any x i ∈ R d where d is even, we divide the d-dimension space in to d/2 sub-spaces and combine them in merit of the linearity of inner product, turning f {q,k} into:</p><formula xml:id="formula_16">f {q,k} (x m , m) = R d Θ,m W {q,k} x m (14) where R d Θ,m =           cos mθ 1 − sin mθ 1 0 0 • • • 0 0 sin mθ 1 cos mθ 1 0 0 • • • 0 0 0 0 cos mθ 2 − sin mθ 2 • • • 0 0 0 0 sin mθ 2 cos mθ 2 • • • 0 0 . . . . . . . . . . . . . . . . . . . . . 0 0 0 0 • • • cos mθ d/2 − sin mθ d/2 0 0 0 0 • • • sin mθ d/2 cos mθ d/2          <label>(15)</label></formula><p>is the rotary matrix with pre-defined parameters Θ = {θ i = 10000 −2(i−1)/d , i ∈ [1, 2, ..., d/2]}. An graphic illustration of RoPE is shown in fig.</p><p>(1).</p><p>Applying our RoPE to self-attention in eq. ( <ref type="formula">2</ref>), we have:</p><formula xml:id="formula_17">q m k n = (R d Θ,m W q x m ) (R d Θ,n W k x n ) = x W q R d Θ,n−m W k x n<label>(16)</label></formula><p>where</p><formula xml:id="formula_18">R d Θ,n−m = (R d Θ,m ) R d Θ,n . Notice that R d</formula><p>Θ is an orthogonal matrix, which ensures the stability during the process of encoding position information. In addition, due to the sparsity of R d Θ , applying matrix multiplication directly as in eq. ( <ref type="formula" target="#formula_17">16</ref>) is not computational efficient, we provide another realization in Appendix B.</p><p>In contrast to the additive nature of position embedding method use by other works, i.e. eqs. ( <ref type="formula" target="#formula_3">3</ref>) to <ref type="bibr" target="#b9">(10)</ref>, our approach is multiplicative. Moreover, our RoPE naturally incorporates relative position through rotation matrix product instead of altering terms of additive position encoding in self-attention.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Properties of RoPE</head><p>Long-term decay: Following <ref type="bibr" target="#b20">[21]</ref>, we choose θ i = 10000 −2i/d . One can prove that this setting provides a long-term decay property (refer to Appendix C for more details), which means the inner-product will decay when the relative position increase. This property coincides with the intuition that a pair of tokens with long relative distance should have less connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoPE with linear attention:</head><p>The self-attention can be rewritten in a more general form.</p><formula xml:id="formula_19">Attention(Q, K, V) m = N n=1 sim(q m , k n )v n N n=1 sim(q m , k n )<label>(17)</label></formula><p>The original self-attention chooses sim(q m , k n ) = exp(q m k n / √ d). Notice that the original self-attention need to compute the inner product of query and key for every pair of tokens, which has quadratic complexity O(N 2 ). Follow <ref type="bibr" target="#b9">[10]</ref> , linear attentions reformulate equation 17 as</p><formula xml:id="formula_20">Attention(Q, K, V ) m = N n=1 φ(q m ) ϕ(k n )v n N n=1 φ(q m ) ϕ(k n ) (18)</formula><p>where φ(•), ϕ(•) are usually non-negative functions. <ref type="bibr" target="#b9">[10]</ref> has proposed φ(x) = ϕ(x) = elu(x) + 1 and first computed the multiplication between keys and values using the associative property of matrix multiplication. <ref type="bibr" target="#b18">[19]</ref> has proposed to use softmax function to normalize queries and keys separately before the inner product, which is equivalent to φ(q i ) = softmax(q i ) and φ(k j ) = exp(k j ). For more details about linear attentions, we encourage readers to refer to original papers. In this section, we focus on discussing incorporating RoPE with equation 18. Since RoPE injects information by rotation, which keeps the norm of hidden representations unchanged, we can combine RoPE with linear attentions by multiplying the rotation matrix with the outputs of the non-negative functions.</p><formula xml:id="formula_21">Attention(Q, K, V) m = N n=1 R d Θ,m φ(q m ) R d Θ,n ϕ(k n ) v n N n=1 φ(q m ) ϕ(k n )<label>(19)</label></formula><p>It is worth mentioning that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in the numerator could contain negative terms. Although the weights for each value v i in equation 19 are not strictly probabilistic normalized, we argue that such computation can still model the importance of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we show some preliminary experiment results of RoFormer in Chinese language modeling, complete benchmark on English tasks is in progress and will be released once done. We discuss the implementation of RoFormer in section (4.1) and show the pre-training results with Chinese language in section (4.2). To illustrate the performance of RoFormer on long texts, we choose a task with most documents exceeding 512 characters (section (4.3)) for downstream evaluation and discuss the results in section (4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Our RoFormer model is based on WoBERT <ref type="bibr" target="#b19">[20]</ref>, from which we replace the absolute position embedding with our proposed RoPE. For cross-comparison with other pre-trained Transformer-based models in Chinese, i.e. BERT <ref type="bibr" target="#b3">[4]</ref>, WoBERT <ref type="bibr" target="#b19">[20]</ref>, and NEZHA <ref type="bibr" target="#b22">[23]</ref>, we tabulate their tokenization level and position embedding in table <ref type="bibr" target="#b0">(1)</ref>.</p><p>BERT WoBERT NEZHA RoFormer tokenization level char word char word position embedding abs. abs. rel. RoPE Table <ref type="table">1</ref>: Cross-comparison between our RoFormer and other pre-trained models in Chinese. 'abs' and 'rel' annotates absolute position embedding and relative position embedding, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training</head><p>We pre-train RoFormer on approximately 34GB data which consists of contents from Chinese Wikipedia, news, forums, etc. The pre-training was done in multiple stages with changing batch size and maximum input sequence length in order to adapt the model with various scenarios, as shown in table <ref type="table">(</ref> According to table <ref type="bibr" target="#b1">(2)</ref>, the accuracy of RoFormer elevates with increasing upper bound of sequence length, which demonstrates the ability of RoFormer in dealing with long texts. We claim that this is attribute to the excellent generalizability of the proposed RoPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Downstream Tasks &amp; Dataset</head><p>We choose Chinese AI and Law 2019 Similar Case Matching dataset(CAIL2019-SCM) <ref type="bibr" target="#b23">[24]</ref> to illustrate the ability of RoFormer dealing with long texts in semantic text matching task. CAIL2019-SCM contains 8964 triplets of cases published by the Supreme People's Court of China. The input triplet, denoted as (A, B, C), are fact descriptions of three cases. The task is to predict whether the pair (A, B) is closer than (A, C) under a predefined similarity measure. Due to the background of CAIL2019-SCM dataset, most of its documents contain more than 512 characters, which is challenging for existing methods to capture document level information.</p><p>The amount of data we used in our experiment in shown in table <ref type="bibr" target="#b2">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split Count Train 5102</head><p>Valid 1500 Test 1536 Total 8138 Table <ref type="table">3</ref>: The number of triplets used in different splits of CAIL2019-SCM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We apply the pre-trained RoFormer model discussed in section (4.2) to downstream task CAIL2019-SCM with different input lengths. The model is compared with the pre-trained BERT and WoBERT model on the same pre-training data, as shown in table <ref type="bibr" target="#b3">(4)</ref>. With short text cut-offs, i.e. 512, RoFormer achieves comparable result to WoBERT and is slightly better than BERT implementation. However, when increase the maximum input text length to 1024 RoFormer outperforms WoBERT by an absolute improvement of 1.5%.  <ref type="table">4</ref>: Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off sequence length. The results are presented in terms of percent accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures. Our theoretical analysis showed that relative position can be naturally formulated using vector production in self-attention, after absolution position information being encoded through rotation matrix. In addition, we mathematically illustrated the advantageous properties of the proposed method when applied in transformer. Finally, our preliminary experiment on Chinese data demonstrated that the enhanced transformer performs superior in tasks with long text. Detailed experiments on English benchmarks are in progress and will be released soon.</p><p>Since the RHS of the equation is a constant irrelevant to m, function φ(m) with continuous integer inputs produce an arithmetic progression. Thus, it is straightforward to get:</p><formula xml:id="formula_22">φ(m) = mθ + γ<label>(30)</label></formula><p>Where θ, γ ∈ R are constants and θ is non-zero.</p><p>To summarize our solutions from Equations ( <ref type="formula">27</ref>) to (30):</p><formula xml:id="formula_23">f q (x q , m) = q e iθq+mθ+γ = qe i(mθ+γ) f k (x k , n) = k e iθ k +nθ+γ = ke i(nθ+γ)<label>(31)</label></formula><p>Finally, notice that we haven't set any constrains to functions in eq. ( <ref type="formula" target="#formula_15">22</ref>), thus f q (x m , 0) and f k (x n , 0) are left to choose freely. To make our result be comparable to 3, here we simply set:</p><formula xml:id="formula_24">q = f q (x m , 0) = W q x n k = f k (x n , 0) = W k x n<label>(32)</label></formula><p>With above and simply set γ = 0 in eq. ( <ref type="formula" target="#formula_23">31</ref>), the ultimate solution is:</p><formula xml:id="formula_25">f q (x m , m) = (W q x m )e imθ f k (x n , n) = (W k x n )e inθ<label>(33)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computational efficient realization of rotary matrix multiplication</head><p>Taking the advantage of the sparsity of R d Θ,m in eq. ( <ref type="formula" target="#formula_16">15</ref>), a more computational efficient realization of multiplication operation between matrix R d Θ and vector x ∈ R d is:</p><formula xml:id="formula_26">R d Θ,m x =          </formula><p>x 1 x 2 x 3 x 4 . . . </p><formula xml:id="formula_27">x d−1 x d           ⊗          </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Long-term decay of RoPE</head><p>We can group entries of vectors q = W q x m and k = W k x n in pairs, and the inner product of RoPE in 16 can be written as complex number multiplication.</p><formula xml:id="formula_28">(R d Θ,m W q x m ) (R d Θ,n W k x n ) = Re d/2−1 i=0 q [2i:2i+1] k * [2i:2i+1] e i(m−n)θi<label>(35)</label></formula><p>where q [2i:2i+1] represents the 2i th to (2i + 1) th entries of q. Denote h i = q </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Implementation of Rotary Position Embedding(RoPE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :i=1</head><label>2</label><figDesc>Figure 2: Long-term decay of RoPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>2). Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various stages. In each stage, we train the model with a specific combination of maximum sequence length and batch size.</figDesc><table><row><cell cols="6">stage max seq. length batch size training steps loss accuracy</cell></row><row><cell>1</cell><cell>512</cell><cell>256</cell><cell>200k</cell><cell>1.73</cell><cell>65.0%</cell></row><row><cell>2</cell><cell>1536</cell><cell>256</cell><cell>12.5k</cell><cell>1.61</cell><cell>66.8%</cell></row><row><cell>3</cell><cell>256</cell><cell>256</cell><cell>120k</cell><cell>1.75</cell><cell>64.6%</cell></row><row><cell>4</cell><cell>128</cell><cell>512</cell><cell>80k</cell><cell>1.83</cell><cell>63.4%</cell></row><row><cell>5</cell><cell>1536</cell><cell>256</cell><cell>10k</cell><cell>1.58</cell><cell>67.4%</cell></row><row><cell>6</cell><cell>512</cell><cell>512</cell><cell>30k</cell><cell>1.66</cell><cell>66.2%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">A stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The code and pre-trained Chinese model are available at https://github.com/ZhuiyiTechnology/roformer</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivation of RoPE under 2D</head><p>Under the case of d = 2, we consider two word embedding vectors x q , x k corresponds to query and key and their position m and n, respectively. According to eq. ( <ref type="formula">1</ref>), their position-encoded counterparts are:</p><p>Here the subscripts of q m , k n indicates the encoded position information. Assume there exists function g that defines inner product between vectors produced by f {q,k} :</p><p>We further ask below initial condition to be satisfied:</p><p>which denotes the vectors with empty position information encoded. With above settings, we manage to find a solution of f q , f k .</p><p>First, we take advantage of the geometric meaning of vector in 2D and its complex counter part, decompose functions in eqs. ( <ref type="formula">20</ref>) and ( <ref type="formula">21</ref>) into:</p><p>Where R f , R g and Θ f , Θ g are the radical and angular components for f {q,k} and g, respectively. Plug them into eq. ( <ref type="formula">21</ref>), we get relation:</p><p>with the corresponding initial condition as: q = q e iθq = R q (x q , 0)e iΘq(xq,0)</p><p>Where q , k and θ q , θ k are the radial and angular part of q and k on the 2D plane.</p><p>Next, we set m = n in eq. ( <ref type="formula">24</ref>) and take into account initial conditions in eq. ( <ref type="formula">25</ref>):</p><p>On one hand, from eq. (26a), we have a straightforward solution of R f :</p><p>Which means the radial functions R q , R k and R g are functions independent to position information.</p><p>On the other hand, according to eq. (26b), notice Θ q (x q , m)−θ q = Θ k (x k , m)−θ k indicates that the angular functions does not dependent on query and key, we set them to Θ f := Θ q = Θ k and term Θ f (x {q,k} , m) − θ {q,k} is a function of position m and is independent of word embedding x {q,k} , we denote it as φ(m), yielding:</p><p>Further, by plugging in n = m + 1 in eq. ( <ref type="formula">24</ref>) and consider above equation, we have: φ(m + 1) − φ(m) = Θ g (x q , x k , 1) + θ q − θ k (29)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Roman</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="6572" to="6583" />
		</imprint>
	</monogr>
	<note>Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.03654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Music transformer</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improve transformer models with better relative position embeddings</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="3327" to="3335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode?</title>
		<author>
			<persName><forename type="middle">Amirul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Bruce</surname></persName>
		</author>
		<idno>ArXiv, abs/2001.08248</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.15595</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to encode position for transformer with continuous dynamical model</title>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6327" to="6335" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
				<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3531" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wobert: Word-based chinese bert model -zhuiyiai</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding word order in complex embeddings</title>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nezha: Neural contextualized representation for chinese language understanding</title>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">08</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cail2019-scm: A dataset of similar case matching in legal domain</title>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions</title>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
