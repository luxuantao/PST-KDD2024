<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrosynthesis Prediction with Conditional Graph Logic Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
							<email>hadai@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengtao</forename><surname>†⇤</surname></persName>
							<email>chengtao.li@galixir.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Ant Financial</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Galixir Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
							<email>ccoley@mit.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<email>bodai@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>lsong@cc.gatech.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Ant Financial</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retrosynthesis Prediction with Conditional Graph Logic Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E213B4736D02A91C3832E4B5F5AD7E02</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.2% over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.</p><p>⇤ Work done while Hanjun was at Georgia Institute of Technology 1 We will focus on this "single step" version of retrosynthesis in our paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retrosynthesis planning is the procedure of identifying a series of reactions that lead to the synthesis of target product. It is first formalized by E. J. Corey <ref type="bibr" target="#b0">[1]</ref> and now becomes one of the fundamental problems in organic chemistry. Such problem of "working backwards from the target" is challenging, due to the size of the search space-the vast numbers of theoretically-possible transformations-and thus requires the skill and creativity from experienced chemists. Recently, various computer algorithms <ref type="bibr" target="#b1">[2]</ref> work in assistance to experienced chemists and save them tremendous time and effort.</p><p>The simplest formulation of retrosynthesis is to take the target product as input and predict possible reactants 1 . It is essentially the "reverse problem" of reaction prediction. In reaction prediction, the reactants (sometimes reagents as well) are given as the input and the desired outputs are possible products. In this case, atoms of desired products are the subset of reactants atoms, since the side products are often ignored (see <ref type="bibr">Fig 1)</ref>. Thus models are essentially designed to identify this subset in reactant atoms and reassemble them to be the product. This can be treated as a deductive reasoning process. In sharp contrast, retrosynthesis is to identify the superset of atoms in target products, and thus is an abductive reasoning process and requires "creativity" to be solved, making it a harder problem. Although recent advances in graph neural networks have led to superior performance in reaction prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, such advances do not transfer to retrosynthesis.</p><p>Computer-aided retrosynthesis designs have been deployed over the past years since <ref type="bibr" target="#b5">[6]</ref>. Some of them are completely rule-based systems <ref type="bibr" target="#b6">[7]</ref> and do not scale well due to high computation cost and incomplete coverage of the rules, especially when rules are expert-defined and not algorithmically extracted <ref type="bibr" target="#b1">[2]</ref>. Despite these limitations, they are very useful for encoding chemical transformations and easy to interpret. Based on this, the retrosim <ref type="bibr" target="#b7">[8]</ref> uses molecule and reaction fingerprint similarities to select the rules to apply for retrosynthesis. Other approaches have used neural classification models for this selection task <ref type="bibr" target="#b8">[9]</ref>. On the other hand, recently there have also been attempts to use the sequence-to-sequence model to directly predict SMILES<ref type="foot" target="#foot_0">2</ref> representation of reactants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> (and for the forward prediction problem, products <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>). Albeit simple and expressive, these approaches ignore the rich chemistry knowledge and thus require huge amount of training. Also such models lack interpretable reasoning behind their predictions.</p><p>The current landscape of computer-aided synthesis planning motivated us to pursue an algorithm that shares the interpretability of template-based methods while taking advantage of the scalability and expressiveness of neural networks to learn when such rules apply. In this paper, we propose Conditional Graph Logic Network towards this direction, where chemistry knowledge about reaction templates are treated as logic rules and a conditional graphical model is introduced to tolerate the noise in these rules. In this model, the variables are molecules while the synthetic relationships to be inferred are defined among groups of molecules. Furthermore, to handle the potentially infinite number of possible molecule entities, we exploit the neural graph embedding in this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contribution can be summarized as follows:</head><p>1) We propose a new graphical model for the challenging retrosynthesis task. Our model brings both the benefit of the capacity from neural embeddings, and the interpretability from tight integration of probabilistic models and chemical rules. 2) We propose an efficient hierarchical sampling method for approximate learning by exploiting the structure of rules. Such algorithm not only makes the training feasible, but also provides interpretations for predictions. 3) Experiments on the benchmark datasets show a significant 8.2% improvement over existing state-of-the-art methods in top-one accuracy.</p><p>Other related work: Recently there have been works using machine learning to enhance the rule systems. Most of them treat the rule selection as multi-class classification <ref type="bibr" target="#b8">[9]</ref> or hierarchical classification <ref type="bibr" target="#b13">[14]</ref> where similar rules are grouped into subcategories. One potential issue is that the model size grows with the number of rules. Our work directly models the conditional joint probability of both rules and the reactants using embeddings, where the model size is invariant to the rules.</p><p>On the other hand, researchers have tried to tackle the even harder problem of multi-step retrosynthesis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> using single-step retrosynthesis as a subroutine. So our improvement in single-step retrosynthesis could directly transfer into improvement of multi-step retrosynthesis <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>A chemical reaction can be seen as a transformation from set of N reactant molecules {R i } N i=1 to an outcome molecule O. Without loss of generality, we work with single-outcome reactions in this paper, as this is a standard formulation of the retrosynthetic problem and multi-outcome reactions can be split into multiple single-outcome ones. We refer to the set of atoms changed (e.g., bond being added or deleted) during the reaction as reaction centers. Given a reaction, the corresponding retrosynthesis template T is represented by a subgraph pattern rewriting rule<ref type="foot" target="#foot_1">3</ref> </p><formula xml:id="formula_0">T := o T ! r T 1 + r T 2 + . . . + r T N (T ) ,<label>(1)</label></formula><p>where N (•) represents the number of reactant subgraphs in the template, as illustrated in Figure . 1. Generally we can treat the subgraph pattern o T as the extracted reaction center from O, and r T i , i 2 1, 2, . . . , N(T ) as the corresponding pattern inside i-th reactant, though practically this will include neighboring structures of reaction centers as well.</p><p>We first introduce the notations to represent these chemical entities:</p><p>• Subgraph patterns: we use lower case letters to represent the subgraph patterns. </p><formula xml:id="formula_1">(T ) := I[o T ✓ O] • I[T 2 T ],<label>(2)</label></formula><p>where the subgraph pattern o T from the reaction template T is matched against the product O, i.e., o T is a subgraph of the product O. Second, II. Match reactants:</p><formula xml:id="formula_2">O,T (R) := O (T ) • I[|R| = N (T )] • Q N (T ) i=1 I[r T i ✓ R ⇡(i) ],<label>(3)</label></formula><p>where the set of subgraph patterns {r 1 , . . . , r N (T ) } from the reaction template are matched against the set of reactants R. The logic is that the size of the set of reactant R has to match the number of patterns in the reaction template T , and there exists a permutation ⇡(•) of the elements in the reactant set R such that each reactant matches a corresponding subgraph pattern in the template.</p><p>Since there will still be uncertainty in whether the reaction is possible from a chemical perspective even when the template matches, we want to capture such uncertainty by allowing each template/or logic reasoning rule to have a different confidence score. More specifically, we will use a template score function w 1 (T, O) given the product O, and the reactant score function w 2 (R, T, O) given the template T and the product O. Thus the overall probabilistic models for the reaction template T and the set of molecules R are designed as I. Match template:</p><formula xml:id="formula_3">p(T |O) / exp (w 1 (T, O)) • O (T ),<label>(4)</label></formula><formula xml:id="formula_4">II. Match reactants: p(R|T, O) / exp (w 2 (R, T, O)) • O,T (R).<label>(5)</label></formula><p>Given the above two step probabilistic reasoning models, the joint probability of a single-step retrosythetic proposal using reaction template T and reactant set R can be written as</p><formula xml:id="formula_5">p (R, T |O) / exp (w 1 (T, O) + w 2 (R, T, O)) • O (T ) O,T (R) ,<label>(6)</label></formula><p>In this energy-based model, whether the graphical model (GM) is directed or undirected is a design choice. We will present our directed GM design and the corresponding partition function in Sec 4 shortly. We name our model as Conditional Graph Logic Network (GLN) (Fig. <ref type="figure" target="#fig_1">2</ref>), as it is a conditional graphical model defined with logic rules, where the logic variables are graph structures (i.e., molecules, subgraph patterns, etc.). In this model, we assume that satisfying the templates is a necessary condition for the retrosynthesis, i.e., p (R, T |O) 6 = 0 only if O (T ) and O,T (R) are nonzero. Such restriction provides sparse structures into the model, and makes this abductive type of reasoning feasible. Reaction type conditional model: In some situations when performing the retrosynthetic analysis, the human expert may already have a certain type c of reaction in mind. In this case, our model can be easily adapted to incorporate this as well:</p><formula xml:id="formula_6">p(R, T |O, c) / exp (w 1 (T, O) + w 2 (R, T, O)) • O (T ) O,T (R) I[T 2 T c ]<label>(7</label></formula><p>) where T c is the set of retrosynthesis templates that belong to reaction type c.</p><p>GLN is related but significantly different from Markov Logic Network (MLN, which also uses graphical model to model uncertainty in logic rules). MLN treats the predicates of logic rules as latent variables, and the inference task is to get the posterior for them. While in GLN, the task is the structured prediction, and the predicates are implemented with subgraph matching. We show more details on this connection in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Design</head><p>Although the model we defined so far has some nice properties, the design of the components plays a critical role in capturing the uncertainty in the retrosynthesis. We first describe a decomposable design of p(T |O) in Sec. 4.1, for learning and sampling efficiency consideration; then in Sec. 4.2 we describe the parameterization of the scoring functions w 1 , w 2 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decomposable design of p(T |O)</head><p>Depending on how specific the reaction rules are, the template set T could be as large as the total number of reactions in extreme case. Thus directly model p(T |O) can lead to difficulties in learning and inference. By revisiting the logic rule defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>), we can see the subgraph pattern o T plays a critical role in choosing the template. Since we represent the templates as</p><formula xml:id="formula_7">T = (o T ! r T i N (T )</formula><p>i=1 ), it is natural to decompose the energy function w 1 (T, O) in Eq. ( <ref type="formula" target="#formula_3">4</ref>) as</p><formula xml:id="formula_8">w 1 (T, O) = v 1 o T , O + v 2 ⇣ r T i N (T ) i=1 , O ⌘ .</formula><p>Meanwhile, recall the template matching rule is also decomposable, so we obtain the resulting template probability model as:</p><formula xml:id="formula_9">p(T |O) = p(o T , r T i N (T ) i=1 |O) (8) = 1 Z(O) exp v 1 (o T , O) • I ⇥ o T 2 O ⇤ ⇣ exp ⇣ v 2 ⇣ r T i N (T ) i=1 , O ⌘ • I[(o T ! r T i N (T ) i=1 ) 2 T ]</formula><p>⌘⌘ , where the partition function Z (O) is defined as:</p><formula xml:id="formula_10">Z (O) = P o2F exp (v 1 (o, O)) • I [o 2 O] • ⇣ P {r}2P(F ) exp (v 2 ({r} , O) • I[(o ! {r}) 2 T ]) ⌘<label>(9)</label></formula><p>Here we abuse the notation a bit to denote the set of subgraph patterns as {r}.</p><p>With </p><formula xml:id="formula_11">• I[(o ! {r}) 2 T ]).</formula><p>In the end we obtain the templated represented as (o ! {r}).</p><p>In the literature there have been several attempts for modeling and learning p(T |O), e.g., multi-class classification <ref type="bibr" target="#b8">[9]</ref> or multiscale model with human defined template hierarchy <ref type="bibr" target="#b13">[14]</ref>. The proposed decomposable design follows the template specification naturally, and thus has nice graph structure parameterization and interpretation as will be covered in the next subsection.</p><p>Finally the directed graphical model design of Eq. ( <ref type="formula" target="#formula_5">6</ref>) is written as</p><formula xml:id="formula_12">p(R, T |O) = 1 Z(O)Z(T,O) exp ⇣⇣ v 1 o T , O + v 2 ⇣ r T i N (T ) i=1 ⌘ + w 2 (R, T, O) ⌘⌘ • O (T ) O,T (R) (10)</formula><p>where</p><formula xml:id="formula_13">Z(T, O) = P R2P(M) exp (w 2 (R, T, O)) • O,T (R)</formula><p>sums over all subsets of molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph</head><p>Neuralization for v 1 , v 2 and w 2</p><p>Since the arguments of the energy functions w 1 , w 2 are molecules, which can be represented by graphs, one natural choice is to design the parameterization based on the recent advances in graph neural networks (GNN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22</ref>]. Here we first present a brief review of the general form of GNNs, and then explain how we can utilize them to design the energy functions.</p><p>The graph embedding is a function g : M S F 7 ! R d that maps a graph into d-dimensional vector. We denote G = (V G , E G ) as the graph representation of some molecule or subgraph pattern, where</p><formula xml:id="formula_14">V G = {v i } |V G | i=1 is the set of atoms (nodes) and E G = e i = (e 1 i , e 2 i ) |E G |</formula><p>i=1 is the set of bonds (edges). We represent each undirected bond as two directional edges. Generally, the embedding of the graph is computed through the node embeddings h vi that are computed in an iterative fashion. Specifically, let h 0 vi = x vi initially, where x vi is a vector of node features, like the atomic number, aromaticity, etc. of the corresponding atom. Then the following update operator is applied recursively:</p><formula xml:id="formula_15">h l+1 v = F (x v , (h l u , x u!v u2N (v) ) where x u!v is the feature of edge u ! v.<label>(11)</label></formula><p>This procedure repeats for L steps. While there are many design choices for the so-called message passing operator F , we use the structure2vec <ref type="bibr" target="#b20">[21]</ref> due to its simplicity and efficient c++ binding with RDKit. Finally we have the parameterization</p><formula xml:id="formula_16">h l+1 v = (✓ 1 x v + ✓ 2 X u2N (v) h l u + ✓ 3 X u2N (v) (✓ 4 x u!v ))<label>(12)</label></formula><p>where (•) is some nonlinear activation function, e.g., relu or tanh, and ✓ = {✓ 1 , . . . , ✓ 4 } are the learnable parameters. Let the node embedding h v = h L v be the last output of F , then the final graph embedding is obtained via averaging over node embeddings:</p><formula xml:id="formula_17">g(G) = 1 |V G | P v2V G h v .</formula><p>Note that attention <ref type="bibr" target="#b22">[23]</ref> or other order invariant aggregation can also be used for such aggregation.</p><p>With the knowledge of GNN, we introduce the concrete parametrization for each component: </p><formula xml:id="formula_18">• Parameterizing v 1 : Given a molecule O, v 1 can</formula><formula xml:id="formula_19">v 2 ( r T i N (T ) i=1 , O) = g 3 (O) &gt; 0 @ 1 N (T ) N (T ) X i=1 g 4 (r T i )) 1 A<label>(13)</label></formula><p>• Parameterizing w 2 : This energy function also needs to take the set as input. Following the same design as v 2 , we have</p><formula xml:id="formula_20">w 2 (R, T, O) = g 5 (O) &gt; 1 |R| X R2R g 6 (R) ! . (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>Note that our GLN framework isn't limited to the specific parameterization above and is compatible with other parametrizations. For example, one can use condensed graph of reaction <ref type="bibr" target="#b24">[25]</ref> to represent R as a single graph. Other chemistry specialized GNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> can also be easily applied here. For the ablation study on these design choices, please refer to Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MLE with Efficient Inference</head><p>Given dataset </p><formula xml:id="formula_22">D = {(O i , T i , R i )}</formula><formula xml:id="formula_23">= b E D [w 1 (T, O) + w 2 (R, T, O) log Z (O) log Z (O, T )] ,<label>(15)</label></formula><p>The gradient of `(⇥) w.r.t. ⇥ can be derived <ref type="foot" target="#foot_2">4</ref> as</p><formula xml:id="formula_24">r ⇥ `(⇥) = b E D [r ⇥ w 1 (T, O)] b E O E T |O [r ⇥ w 1 (T, O)]<label>(16)</label></formula><formula xml:id="formula_25">+ b E D [r ⇥ w 2 (R, T, O)] b E O,T E R|T,O [r ⇥ w 2 (R, T, O)] ,</formula><p>where E T |O [•] and E R|O,T [•] stand for the expectation w.r.t. current model p (T |O) and p (R, T |O), respectively. With the gradient estimator ( <ref type="formula" target="#formula_24">16</ref>), we can apply the stochastic gradient descent (SGD) algorithm for optimizing <ref type="bibr" target="#b14">(15)</ref>.</p><p>Efficient inference for gradient approximation: Since R 2 P(M) is a combinatorial space, generally the expensive MCMC algorithm is required for sampling from p (R|T, O) to approximate <ref type="bibr" target="#b15">(16)</ref>. However, this can be largely accelerated by scrutinizing the logic property in the proposed model. Recall that the matching between template and reactants is the necessary condition for p (R, Then, the importance sampling leads to an unbiased gradient approximation b r ⇥ `(⇥) as illustrated in Algorithm 1. To make the algorithm more efficient in practice, we have adopted the following accelerations: In a dataset with 5 ⇥ 10 4 reactions, |T O | is about 80 and |R T,O | is roughly 10 on average. Therefore, we reduce the actual computational cost to a manageable constant. We further reduce the computation cost of sampling by generating the T and R uniformly from the support. Although these samples only cover the support of the model, we avoid the calculation of the forward pass of neural networks, achieving better computational complexity. In our experiment, such an approximation already achieves state-ofthe-art results. We would expect recent advances in energy based models would further boost the performance, which we leave as future work to investigate.</p><p>Remark on R T,O : Note that to get all possible sets of reactants that match the reaction template T and product O, we can efficiently use graph edit tools without limiting the reactants to be known in the dataset. This procedure works as follows: given a template T = o T ! r T 1 + . . . + r T N , 1) Enumerate all matches between subgraph pattern o T and target product O.</p><p>2) Instantiate a copy of the reactant atoms according to r T 1 , . . . , r T N for each match. 3) Copy over all of the connected atoms and atom properties from O.</p><p>This process is a routine in most Cheminformatics packages. In our paper we use runReactants from RDKit with the improvement of stereochemistry handling<ref type="foot" target="#foot_3">5</ref> to realize this.</p><p>Further acceleration via beam search: Given a product O, the prediction involves finding the pair (R, T ) that maximizes p(R, T |O). One possibility is to first enumerate T 2 T (O) and then R 2 R T,O . This is acceptable by exploiting the sparse support property induced by logic rules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>Dataset: We mainly evaluate our method on a benchmark dataset named USPTO-50k, which contains 50k reactions of 10 different types in the US patent literature. We use exactly the same training/validation/test splits as Coley et al. <ref type="bibr" target="#b7">[8]</ref>, which contain 80%/10%/10% of the total 50k reactions. Table <ref type="table" target="#tab_2">1</ref> contains the detailed information about the benchmark. Additionally, we also build a dataset from the entire USPTO 1976-2016 to verify the scalability of our method.</p><p>Baselines: Baseline algorithms consist of rule-based ones and neural network-based ones, or both. The expertSys is an expert system based on retrosynthetic reaction rules, where the rule is selected according to the popularity of the corresponding reaction type. The seq2seq <ref type="bibr" target="#b9">[10]</ref> and transformer <ref type="bibr" target="#b10">[11]</ref> are neural sequence-to-sequence-based learning model <ref type="bibr" target="#b27">[28]</ref> implemented with LSTM <ref type="bibr" target="#b28">[29]</ref> or Transformer <ref type="bibr" target="#b29">[30]</ref>. These models encode the canonicalized SMILES representation of the target compound as input, and directly output canonical SMILES of reactants. We also include some data-driven template-based models. The retrosim <ref type="bibr" target="#b7">[8]</ref> uses direct calculation of molecular similarities to rank the rules and resulting reactants. The neuralsym <ref type="bibr" target="#b8">[9]</ref> models p(T |O) as multi-class classification using MLP. All the results except neuralsym are obtained from their original reports, since we have the same experiment setting. Since neuralsym is not open-source, we reimplemented it using their best reported ELU512 model with the same method for parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metric:</head><p>The evaluation metric we used is the top-k exact match accuracy, which is commonly used in the literature. This metric compares whether the predicted set of reactants are exactly the same as ground truth reactants. The comparison is performed between canonical SMILES strings generated by RDKit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup of GLN:</head><p>We use rdchiral <ref type="bibr" target="#b30">[31]</ref> to extract the retrosynthesis templates from the training set. After removing duplicates, we obtained 11,647 unique template rules in total for USPTO-50k. These rules represent 93.3% coverage of the test set. That is to say, for each test instance we try to apply these rules and see if any of the rules gives exact match. Thus this is the theoretical upper bound of the rule-based approach using this particular degree of specificity, which is high enough for now. For more information about the statistics of these rules, please refer to Table <ref type="table" target="#tab_3">2</ref>.</p><p>We train our model for up to 150k updates with batch size of 64. It takes about 12 hours to train with a single GTX 1080Ti GPU. We tune embedding sizes in {128, 256}, GNN layers {3, 4, 5} and GNN aggregation in {max, mean, sum} using validation set. Our code is released at https://github.com/Hanjun-Dai/GLN. More details are included in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main results</head><p>We present the top-k exact match accuracy in Table <ref type="table" target="#tab_4">3</ref>, where k ranges from {1, 3, 5, 10, 20, 50}. We evaluate both the reaction class unknown and class conditional settings. Using the reaction class as prior knowledge represents some situations where the chemists already have an idea of how they would like to synthesize the product.</p><p>In all settings, our proposed GLN outperforms the baseline algorithms. And particularly for top-1 accuracy, our model performs significantly better than the second best method, with 8.2% higher accuracy with unknown reaction class, and 7.9% higher with reaction class given. This demonstrates the advantage of our method in this difficult setting and potential applicability in reality.     Moreover, our performance in the reaction class unknown setting even outperforms expertSys and seq2seq in the reaction conditional setting. Since the transformer paper didn't report top-k performance for k &gt; 10, we leave it as blank. Meanwhile, Karpov et al. <ref type="bibr" target="#b10">[11]</ref> also reports the result when training using training+validation set and tuning on the test set. With this extra priviledge, the top-1 accuracy of transformer is 42.7% which is still worse than our performance. This shows the benefit of our logic powered deep neural network model comparing to purely neural models, especially when the amount of data is limited.</p><p>Since the theoretical upper bound of this rule-based implementation is 93.3%, the top-50 accuracy for our method in each setting is quite close to this limit. This shows the probabilistic model we built matches the actual retrosynthesis target well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Interpret the predictions</head><p>Visualizing the predicted synthesis: In Fig <ref type="figure">3</ref> and<ref type="figure">4</ref>, we visualize the ground truth reaction and the top 3 predicted reactions (see Appendix C.6 for high resolution figures). For each reaction, we also highlight the corresponding reaction cores (i.e., the set of atoms get changed). This is done by matching the subgraphs from predicted retrosynthesis template with the target compound and generated reactants, respectively. Fig <ref type="figure">3</ref> shows that our correct prediction also gets almost the same reaction cores predicted as the ground truth. In this particular case, the explanation of our prediction aligns with the existing reaction knowledge.</p><p>Fig <ref type="figure">4</ref> shows a failure mode where none of the top-3 prediction matches. In this case we calculated the similarity between predicted reactants and ground truth ones using Dice similarity from RDKit. We find these are still similar in the molecule fingerprint level, which suggests that these predictions could be the potentially valid but unknown ones in the literature.  Visualizing the reaction center prediction: Here we visualize the prediction of probabilistic modeling of reaction center. This is done by calculating the inner product of each atom embedding in target molecule with the subgraph pattern embedding. Fig <ref type="figure" target="#fig_10">5</ref> shows the visualization of scores on the atoms that are part of the reaction center. The top-1 prediction assigns positive scores to these atoms (red ones), while the bottom-1 prediction (i.e., prediction with least probability) assigns large negative scores (blue ones). Note that although the reaction center in molecule and the corresponding subgraph pattern have the same structure, the matching scores differ a lot. This suggests that the model has learned to predict the activity of substructures inside molecule graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Study of the performance</head><p>In addition to the overall numbers in Table <ref type="table" target="#tab_4">3</ref>, we provide detailed study of the performances. This includes per-category performance, the accuracy of each module in hierarchical sampling and also the effect of the beam size. Due to the space limit, please refer to Appendix C. To see how this method scales up with the dataset size, we create a large dataset from the entire set of reactions from USPTO 1976-2016. There are 1,808,937 raw reactions in total. For the reactions with multiple products, we duplicate them into multiple ones with one product each. After removing the duplications and reactions with wrong atom mappings, we obtain roughly 1M unique reactions, which are further divided into train/valid/test sets with size 800k/100k/100k.</p><p>We train on single GPU for 3 days and report with the model having best validation accuracy. The results are presented in Table <ref type="table" target="#tab_5">4</ref>. We compare with the best two baselines from previous sections. Despite the noisiness of the full USPTO set relative to the clean USPTO-50k, our method still outperforms the two best baselines in top-k accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Evaluation: Retrosynthesis usually does not have a single right answer. Evaluation in this work is to reproduce what is reported for single-step retrosynthesis. This is a good, but imperfect benchmark, since there are potentially many reasonable ways to synthesize a single product.</p><p>Limitations: We share the limitations of all template-based methods. In our method, the template designs, more specifically, their specificities, remain as a design art and are hard to decide beforehand. Also, the scalability is still an issue since we rely on subgraph isomorphism during preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future work:</head><p>The subgraph isomorphism part can potentially be replaced with predictive model, while during inference the fast inner product search <ref type="bibr" target="#b31">[32]</ref> can be used to reduce computation cost. Also actively building templates or even inducing new ones could enhance the capacity and robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Chemical reactions and the retrosynthesis templates. The reaction centers are highlighted in each participant of the reaction. These centers are then extracted to form the corresponding template. Note that the atoms belong to the reaction side products (the dashed box in figure) are missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Retrosynthesis pipeline with GLN. The three dashed boxes from top to bottom represent set of templates T , subgraphs F and molecules M. Different colors represent retrosynthesis routes with different templates. The dashed lines represent potentially possible routes that are not observed. Reaction centers in products O are highlighted.</figDesc><graphic coords="4,185.79,164.85,73.08,61.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>be viewed as a scoring function of possible reaction centers inside O. Since the subgraph pattern o is also a graph, we parameterize it with inner product, i.e., v1 (o, O) = g 1 (o) &gt; g 2 (O). Such form can be treated as computing the compatibility between o and O. Note that due to our design choice, v 1 (o, O) can be written asv 1 (o, O) = P v2V O h &gt; v g 1 (o).Such form allows us to see the contribution of compatibility from each atom in O. • Parameterizing v 2 : The size of set of subgraph patterns r T i N (T ) i=1 varies for different template T . Inspired by the DeepSet [24], we use average pooling over the embeddings of each subgraph pattern to represent this set. Specifically,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>|D|</head><label></label><figDesc>i=1 with |D| reactions, we denote the parameters in w 1 (T, O) , w 2 (T, R, O) as ⇥ = (✓ 1 , ✓ 2 ), respectively. The maximum log-likelihood estimation (MLE) is a natural choice for parameter estimation. Since 8 (O, T, R) ⇠ D, O (T ) = 1 and O,T (R) = 1, we have the MLE optimization as max ⇥ `(⇥) := b E D [log p (R|T, O) p (T |O)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>17 ) 1</head><label>171</label><figDesc>T |O) 0 by design. On the other hand, given O, only a few templates T with reactants R have nonzero O (T ) and O,T (R). Then, we can sample T and R by importance sampling on restricted supported templates instead of MCMC over P (M). Rigorously, given O, we denote the matched templates as T O and the matched reactants based on T as R T,O , where T O = {T : O (T ) 6 = 0, 8T 2 T } and R T,O = {R : O,T (R) 6 = 0, 8R 2 P (M)} (Algorithm Importance Sampling for b r ⇥ `(⇥) 1: Input (R, T, O) ⇠ D, p (R|T, O) and p (T |O). 2: Construct T O according to O (T ). 3: Sample T / exp (w 1 (T, O)) , 8T 2 T O in hierarchical way, as in Sec. 4.1. 4: Construct R T,O according to O,T (R). 5: Sample R / exp (w 2 (R, T, O)). 6: Compute stochastic approximation b r ⇥ `(⇥) with sample ⇣ R, T, R, T , O ⌘ by (16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>• 1 )</head><label>1</label><figDesc>Decomposable modeling of p(T |O) as described in Sec. 4.1; • 2) Cache the computed T O and R (T, O) in advance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A</head><label></label><figDesc>more efficient way is to use beam search with size k. Firstly we find k reaction centers {o i } k i=1 with top v 1 (o, O). Next for each o 2 {o i } k i=1 we score the corresponding v 2 ({r} , O) • I [(o ! {r}) 2 T ]. In this stage the top k pairs {(o Tj , {r Tj i })} k j=1 (i.e., the templates) that maximize v 1 (o|O) + v 2 ({r} , O) are kept. Finally using these templates, we choose the best R 2 S k j=1 R Tj ,O that maximizes total score w 1 (T, O) + w 2 (R, T, O). Fig. 2 provides a visual explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Example successful predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reaction center prediction visualization. Red atoms indicate positive match scores, while blue ones having negative scores. The darkness of the color shows the magnitude of the score. Green parts highlight the substructure match between molecules and center structures.</figDesc><graphic coords="9,317.73,76.32,61.13,61.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Molecule: we use capital letters to represent the molecule graphs. By default, we use O for an outcome molecule, and R for a reactant molecule, or M for any molecule in general. • Set: sets are represented by calligraphic letters. We use M to denote the full set of possible molecules, T to denote all extracted retrosynthetic templates, and F to denote all the subgraph patterns that are involved in the known templates. We further use F o to denote the subgraphs appearing in reaction outcomes, and F r to denote those appearing in reactants, with F = F o S F r .</figDesc><table><row><cell cols="2">Task: Given a production or target molecule O, the goal of a one-step retrosynthetic analysis is to</cell></row><row><cell cols="2">identify a set of reactant molecules R 2 P(M) that can be used to synthesize the target O. Here P(M) is the power set of all molecules M.</cell></row><row><cell cols="2">3 Conditional Graph Logic Network</cell></row><row><cell cols="2">Let I[m ✓ M ] : F ⇥ M 7 ! {0, 1} be the predicate that indicates whether subgraph pattern m is a subgraph inside molecule M . This can be checked via subgraph matching. Then the use of a</cell></row><row><cell cols="2">1 + r T 2 + . . . + r T N (T ) for reasoning about a reaction can be retrosynthetic template T : o T ! r T decomposed into two-step logic. First,</cell></row><row><cell>I. Match template:</cell><cell>O</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset information.</figDesc><table><row><cell>USPTO 50k</cell><cell></cell></row><row><cell># train</cell><cell>40,008</cell></row><row><cell># val</cell><cell>5,001</cell></row><row><cell># test</cell><cell>5,007</cell></row><row><cell># rules</cell><cell>11,647</cell></row><row><cell># reaction types</cell><cell>10</cell></row><row><cell>Rule coverage</cell><cell>93.3%</cell></row><row><cell># unique centers</cell><cell>9,078</cell></row><row><cell cols="2">Avg. # centers per mol 29.31</cell></row><row><cell>Avg. # rules per mol</cell><cell>83.85</cell></row><row><cell>Avg. # reactants</cell><cell>1.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Reaction and template set information.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Top-k accuracy %</cell><cell></cell><cell></cell></row><row><cell>methods</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell></cell><cell cols="3">Reaction class unknown</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">transformer[11] 37.9 57.3 62.7</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>retrosim[8]</cell><cell cols="6">37.3 54.7 63.3 74.1 82.0 85.3</cell></row><row><cell>neuralsym[9]</cell><cell cols="6">44.4 65.3 72.4 78.9 82.2 83.1</cell></row><row><cell>GLN</cell><cell cols="6">52.6 68.0 75.1 83.1 88.5 92.1</cell></row><row><cell></cell><cell cols="4">Reaction class given as prior</cell><cell></cell><cell></cell></row><row><cell>expertSys[10]</cell><cell cols="6">35.4 52.3 59.1 65.1 68.6 69.5</cell></row><row><cell>seq2seq[10]</cell><cell cols="6">37.4 52.4 57.0 61.7 65.9 70.7</cell></row><row><cell>retrosim[8]</cell><cell cols="6">52.9 73.8 81.2 88.1 91.8 92.9</cell></row><row><cell>neuralsym[9]</cell><cell cols="6">55.3 76.0 81.4 85.1 86.5 86.9</cell></row><row><cell>GLN</cell><cell cols="6">63.2 77.5 83.4 89.1 92.1 93.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top-k exact match accuracy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Top-k accuracy on USPTO-full.</figDesc><table><row><cell>6.4 Large scale experiments on USPTO-full</cell><cell cols="3">retrosim neuralsym GLN</cell></row><row><cell>top-1</cell><cell>32.8</cell><cell>35.8</cell><cell>39.3</cell></row><row><cell>top-10</cell><cell>56.1</cell><cell>60.8</cell><cell>63.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.daylight.com/dayhtml/doc/theory/theory.smiles.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Commonly encoded using SMARTS/SMIRKS patterns</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We adopt the conventions 0 log 0 = 0<ref type="bibr" target="#b26">[27]</ref>, which is justified by continuity since x log x ! 0 as x ! 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/connorcoley/rdchiral.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank anonymous reviewers for providing constructive feedbacks. This project was supported in part by NSF grants CDS&amp;E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983 to L.S.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The logic of chemical synthesis: multistep synthesis of complex carbogenic molecules (nobel lecture)</title>
		<author>
			<persName><forename type="first">James</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><surname>Corey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Angewandte Chemie International Edition in English</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="455" to="465" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine learning in computer-aided synthesis planning</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klavs</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.accounts.8b00087</idno>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting organic reaction outcomes with weisfeiler-lehman network</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2607" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A graph-convolutional neural network model for the prediction of chemical reactivity</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Jamison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klavs</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1039/C8SC04228D</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A generative model for electron paths</title>
		<author>
			<persName><forename type="first">John</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwin</forename><forename type="middle">Hs</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computer-assisted design of complex organic syntheses</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Corey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Todd Wipke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">3902</biblScope>
			<biblScope unit="page" from="178" to="192" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Computer-assisted synthetic planning: The end of the beginning</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Szymkuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><forename type="middle">P</forename><surname>Gajewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Klucznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Molga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dittwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><surname>Startek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><surname>Bajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><forename type="middle">A</forename><surname>Grzybowski</surname></persName>
		</author>
		<idno type="DOI">10.1002/anie.201506101</idno>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="5904" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computer-assisted retrosynthesis based on molecular similarity</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Klavs</surname></persName>
		</author>
		<author>
			<persName><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1237" to="1245" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural-symbolic machine learning for retrosynthesis and reaction prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Marwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><surname>Waller</surname></persName>
		</author>
		<idno type="DOI">10.1002/chem.201605499</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="5966" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrosynthetic reaction prediction using neural sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Kawthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang</forename><surname>Luu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1103" to="1113" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A transformer model for retrosynthesis</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><surname>Tetko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">found in translation&quot;: predicting outcomes of complex organic chemistry reactions using neural sequence-tosequence models</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costas</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Laino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="6091" to="6098" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Molecular transformer for chemical reaction prediction and uncertainty estimation</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Laino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bolgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costas</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alpha</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.26434/chemrxiv.7297379</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Enhancing retrosynthetic reaction prediction with deep learning using multiscale reaction classification</title>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">L</forename><surname>Baylon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Cilfone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">R</forename><surname>Gulcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Chittenden</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="673" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Planning chemical syntheses with deep neural networks and symbolic ai</title>
		<author>
			<persName><forename type="first">Marwin Hs</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Preuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7698</biblScope>
			<biblScope unit="page">604</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>John S Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle Jm</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06569</idno>
		<title level="m">Learning retrosynthetic planning through self-play</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Condensed graph of reaction: considering a chemical reaction as one single pseudo molecule</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hoonakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Lachiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Varnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Wagner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rdchiral: An rdkit wrapper for handling stereochemistry in retrosynthetic template extraction and application</title>
		<author>
			<persName><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Klavs</surname></persName>
		</author>
		<author>
			<persName><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quantization based fast inner product search</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
