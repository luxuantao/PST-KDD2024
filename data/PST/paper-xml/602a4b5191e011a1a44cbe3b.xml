<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically-Conditioned Negative Samples for Efficient Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-12">12 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">James</forename><surname>O' Neill</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
						</author>
						<title level="a" type="main">Semantically-Conditioned Negative Samples for Efficient Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-12">12 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.06603v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Negative sampling is a limiting factor w.r.t. the generalization of metric-learned neural networks. We show that uniform negative sampling provides little information about the class boundaries and thus propose three novel techniques for efficient negative sampling: drawing negative samples from (1) the top-k most semantically similar classes, (2) the top-k most semantically similar samples and (3) interpolating between contrastive latent representations to create pseudo negatives. Our experiments on CIFAR-10, CIFAR-100 and Tiny-ImageNet-200 show that our proposed Semantically Conditioned Negative Sampling and Latent Mixup lead to consistent performance improvements. In the standard supervised learning setting, on average we increase test accuracy by 1.52% percentage points on CIFAR-10 across various network architectures. In the knowledge distillation setting, (1) the performance of student networks increase by 4.56% percentage points on Tiny-ImageNet-200 and 3.29% on CIFAR-100 over student networks trained with no teacher and (2) 1.23% and 1.72% respectively over a hard-tobeat baseline <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Training deep neural networks using contrastive learning has shown state of the art (SoTA) performance in domains such as computer vision <ref type="bibr" target="#b36">(Oord et al., 2018;</ref><ref type="bibr" target="#b20">He et al., 2020;</ref><ref type="bibr" target="#b14">Chen et al., 2020;</ref><ref type="bibr" target="#b21">Henaff, 2020)</ref>, speech recognition <ref type="bibr" target="#b36">(Oord et al., 2018)</ref> and natural language processing <ref type="bibr" target="#b34">(Mueller &amp; Thyagarajan, 2016;</ref><ref type="bibr" target="#b32">Logeswaran &amp; Lee, 2018;</ref><ref type="bibr" target="#b15">Fang &amp; Xie, 2020)</ref>. The generalization performance in contrastive learning heavily relies on the quality of negative samples used during training to define the classification boundary and requires a large number of negative samples. Hence, con-1 Department of Computer Science, University of Liverpool, Liverpool, England. Correspondence to: James O' Neill &lt;james.o-neill@liverpool.ac.uk&gt;, Danushka Bollegala &lt;danushka.bollegala@liverpool.ac.uk&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Under Review.</head><p>trastive learning relies on techniques that enable training of large batch sizes, such as learning a lookup table <ref type="bibr" target="#b52">(Xiao et al., 2017;</ref><ref type="bibr" target="#b51">Wu et al., 2018;</ref><ref type="bibr" target="#b20">He et al., 2020)</ref> to store lowerdimensional latent features of negative samples. However, training large models using contrastive learning can be inefficient when using uniformly sampled negatives (USNs) as the total number of potential negative sample pairs is O((N − N + )N ) where N is the number of training samples and N + is the number of positive class samples. Hence, even a lookup table may poorly estimate negative sample latent features, even for a large number of USNs and training epochs.</p><p>A complementary approach to improve the learning efficiency of a contrative learned neural network is to reduce the model size using model compression techniques such as knowledge distillation (KD; <ref type="bibr" target="#b13">Buciluǎ et al., 2006)</ref>. In neural networks, this is achieved by transferring the logits of a larger "teacher" network to learn a smaller "student" network <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>. There has been various KD methods proposed <ref type="bibr" target="#b41">(Romero et al., 2014;</ref><ref type="bibr" target="#b23">Hinton et al., 2015;</ref><ref type="bibr" target="#b0">Zagoruyko &amp; Komodakis, 2016a;</ref><ref type="bibr" target="#b54">Yim et al., 2017;</ref><ref type="bibr">Passalis &amp; Tefas, 2018;</ref><ref type="bibr">Tung &amp; Mori, 2019;</ref><ref type="bibr" target="#b39">Peng et al., 2019)</ref>. They involve minimizing KL divergence (KLD) between the student network and teacher network logits <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>, minimizing the squared error between the student and teacher network intermediate layers <ref type="bibr" target="#b41">(Romero et al., 2014)</ref>, metric learning approaches <ref type="bibr">(Tung &amp; Mori, 2019;</ref><ref type="bibr">Ahn et al., 2019;</ref><ref type="bibr" target="#b45">Tian et al., 2019;</ref><ref type="bibr" target="#b37">Park et al., 2019)</ref>, attention transfer of convolution maps <ref type="bibr" target="#b0">(Zagoruyko &amp; Komodakis, 2016a)</ref> and activation boundary transfer <ref type="bibr" target="#b22">(Heo et al., 2019)</ref>.</p><p>In this paper, we propose three efficient negative sampling (NS) alternatives to uniform NS and show their efficacy in standard supervised learning and the aforementioned KD setting. Our NS techniques are also complementary to the aforementioned lookup tables used for retrieving negative latent features. All three techniques have a common factor in that they produce negatives that are semantically similar to the positive targets on both instance-and class-levels. We collectively refer to these sampling methods as Semantically Conditioned Negative Sampling (SCNS) as we replace a uniform prior over negative samples with a conditional probability distribution defined by the embeddings produced by a pretrained network. SCNS provides more informative negatives in contrast to USNs where many samples are easy to classify and do not support the class boundaries. Additionally, it requires no additional parameters at training time and thus can be used with large training sets and models. The pretrained representations are used to estimate pairwise class-level and instance-level semantic similarities to define a top-k NS probability distribution. This reduces the number of negative pairs from O(N (N − N + )) to O <ref type="bibr">(N k)</ref> where k is the number of nearest neighbor negative samples for each sample. Below is a summary of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Class and Instance Conditioned Negative Sampling:</head><p>We define the NS distribution of each class by drawing negative samples proportional to the top-k cosine similarity between pretrained word embeddings of the class labels. We also propose top-k instance-level similarity for defining the NS distribution by performing a forward pass with a pretrained network prior to training.</p><p>2) Contrastive Representation Mixup: In subsection 3.2, we propose Latent Mixup (LM), a variant of <ref type="bibr">Mixup (Zhang et al., 2017)</ref> that operates on latent representations between teacher positive and negative representations to produce harder pseudo negative sample representations that lie closer to the class boundaries. This is also carried out for the student network representations and a distance (or divergence) is minimized between both mixed representations.</p><p>3) Theoretical Analysis of Conditioned Sampling: We reformulate the mutual information lower bound to account for semantic similarity of contrastive pairs and describe the sample efficiency of SCNS compared to uniform sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Research</head><p>Before describing our proposed methods, we review related work on the two most related aspects: efficient NS and KD.</p><p>Efficient Negative Sampling Efficient NS has been explored in the literature, predominantly for triplet learning. Semi-hard NS has been used to sample negative pairs yet are still further in Euclidean distance than the anchor-positive pair <ref type="bibr" target="#b43">(Schroff et al., 2015)</ref>. Oh <ref type="bibr" target="#b35">Song et al. (2016)</ref> combine contrastive and triplet losses to mine negative structured embedding samples, drawing negative samples proportional to a Gaussian distribution of negative sample distances to the anchor sample <ref type="bibr" target="#b18">Harwood et al. (2017)</ref>. <ref type="bibr" target="#b44">Suh et al. (2019)</ref> select hard negatives from the class-to-sample distances and then search on the instance-level within the selected class to retrieve negative samples. <ref type="bibr" target="#b49">Wu et al. (2017)</ref> proposed a distance weighted sampling that selects more stable and informative samples when compared to uniform sampling and show that data selection is at least as important as the choice of loss function. <ref type="bibr">Zhuang et al. (2019)</ref> define two neighborhoods using k-means clustering, close neighbors and dissimilar samples are background neighbors. <ref type="bibr" target="#b50">Wu et al. (2020)</ref> use ball discrimination to discriminate between hard and easy negative unsupervised representations where positive pairs are different views of the same image. <ref type="bibr" target="#b46">Tran et al. (2019)</ref> use the prior probability of observing a class to draw negative samples and then further sample instances within the chosen class based on the inner product with the anchor of the triplet, showing improvements over semi-hard NS without the use of informative priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation</head><p>The original KD objective minimizes the Kullbeck-Leibler divergence between student network and teacher network logits <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>. <ref type="bibr" target="#b41">Romero et al. (2014)</ref> instead restrict the student network hidden representation to behave similarly to the teacher network hidden representations by minimizing the squared error between corresponding layers of the two networks. The main restriction in this method is that both networks have to be the same depth and of similar architectures. Attention Transfer (AT) (Zagoruyko &amp; Komodakis, 2016a) performs KD by forcing the student network to mimic the attention maps over convolutional layers of the pretrained teacher network. <ref type="bibr">Passalis &amp; Tefas (2018)</ref> use Gaussian and Cosine-based kernel density estimators (KDEs) to maximize the similarity between the student and teacher probability distributions. They find consistent improvements over <ref type="bibr" target="#b23">Hinton et al. (2015)</ref> and Hint layers used in Fitnet <ref type="bibr" target="#b41">(Romero et al., 2014)</ref>. Similarity-Preserving (SP) <ref type="bibr">(Tung &amp; Mori, 2019)</ref> KD ensures that the activation patterns of the student network are similar to that in the teacher network for semantically similar input pairs. <ref type="bibr" target="#b39">Peng et al. (2019)</ref> propose Correlation Congruence (CC) to maximize multiple cross-correlations between samples of the same class. <ref type="bibr">Ahn et al. (2019)</ref> provide an information-theoretic view of KD by maximizing the mutual information between student and teacher networks through variational information maximization <ref type="bibr" target="#b10">(Barber &amp; Agakov, 2003)</ref>. Moreover, <ref type="bibr" target="#b37">Park et al. (2019)</ref> argue that the distance between relation structures created from multiple samples of the student and teacher networks should be minimized. They propose Relational KD (RKD), which involves the use of both distance-wise and angle-wise distillation losses that penalize structural discrepancies between multiple instance outputs from both networks. Contrastive Representation Distillation <ref type="bibr" target="#b45">(Tian et al., 2019)</ref> uses a CL objective to maximize a lower bound on the mutual to capture higher order dependencies between positive and negative samples, adapting their loss from <ref type="bibr" target="#b24">Hjelm et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We begin by defining a dataset as D := {(x i , y i )} N i=1 , which consists of N samples of an input vector x ∈ R n and a corresponding target y ∈ {0, 1} C where sample s i := (x i , y i ) and C is the number of classes. In the CL setting x = (x * , x + , x −,1 , .., x −,M ) where X + := (x * , x + ) and X − := (x * , x −,1 . . . x −,M ) for M negative pairs. We denote a neural network as f θ (x) which has parameters θ := (θ 1 , θ 2 , . . . θ . . . , θ L ) T where θ</p><formula xml:id="formula_0">l := {W l , b l }, W l ∈ R d l ×d l+1 , b ∈ R d l+1</formula><p>and d l denotes the dimensionality of the l-th layer. The input to each subsequent layer is denoted as h l ∈ R d l where x := h 0 and the corresponding output activation is denoted as z l = g(h l ). For brevity, we refer to z = g(h L ) as the unnormalized output where g : R d L → R p and z ∈ R p . However, when using a metric loss, g : R d L → R d L and therefore z ∈ R d L . In the former case, the cross-entropy loss is used for supervised learning and defined as</p><formula xml:id="formula_1">CE (D) := 1 N N i=1 p c=1 −y i,c , log ŷi,c where ŷi = σ(f θ (x i )/τ ), ŷi ∈ R p and τ ∈ (0, +∞) is the temperature of the softmax σ.</formula><p>We also consider the KD setting where a student network f S θ learns from a pretrained teacher network f T ω with pretrained and frozen parameters ω. The last hidden layer representation of f S θ is given as z S := f S θ (x) and similarly z T := f T ω (x). The Kullbeck-Leibler Divergence (KLD), D KLD , between z S and z T is defined in Equation <ref type="formula" target="#formula_2">1</ref>D KLD (y T ||y S ) = H(y T ) − y T log(y S )</p><formula xml:id="formula_2">y S = σ(z S /τ ), y T = σ(z T /τ )<label>(1)</label></formula><p>where H(y T ) is the entropy of the teacher distribution y T . Following <ref type="bibr" target="#b23">Hinton et al. (2015)</ref>, the weighted sum of crossentropy loss and KLD loss shown in Equation 2 is used as our main KD baseline, where α ∈ [0, 1].</p><formula xml:id="formula_3">KLD = (1 − α) CE (y S , y) + ατ 2 D KLD y S , y T (2)</formula><p>To carry out KD using the KLD loss, the outputs of the pretrained teacher f T ω are stored after performing a single forward pass over mini-batches B ⊂ D in our training set. These outputs are then retrieved for each mini-batch update of the smaller student network f S θ . Given this background, the next two subsections will describe our three main approaches to improving NS in contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantically Conditioned Negative Sampling</head><p>Here, we describe two of our three approaches for improving NS efficiency that both involve using f T ω to define a NS distribution. The first involves a cross-modal teacher network (i.e pretrained word embeddings for image classification) to define a class-level NS distribution and in the second f T ω is a pretrained image classifier that defines an instance-level NS distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">CLASS-LEVEL NEGATIVE SAMPLING</head><p>Our first method assumes that word embedding similarity between class labels highly correlates with image embedding similarity <ref type="bibr" target="#b30">(Leong &amp; Mihalcea, 2011;</ref><ref type="bibr" target="#b17">Frome et al., 2013)</ref>. Pretrained word embedding similarities are used to improve the sample efficiency of NS in contrastive learning and replace uniform NS that is typically used. The cosine similarity is measured between the pretrained word embeddings (z T wi , z T wj ) where (w i , w j ) are the class labels in the vocabulary V and |V| = C. This is carried out for all pairs to construct an all pair cosine similarity matrix Z V ∈ R |V|×|V| that is then row-normalized with the softmax function σ as P V := σ(Z V /τ ). Here, setting τ high leads to harder negative samples being chosen from the most similar classes. P represents the conditional probability matrix used to define X − by drawing samples as Equation 3 where D w</p><p>x − ∼ D w ∝ P w</p><p>(3) represents all samples (x 1 w , . . . x M w ) for a given class associated with w. This is repeated M times when using CL.</p><p>Hard k-Nearest Class-Level Negative Samples Instead of sampling over a possible M = |V|−1 number of negative samples, we can define the top-k most similar hard negative samples. The top-k cosine similarities from other labels in V are selected by applying Equation 4 where z T wi ∈ R dw and z wi := f T θ (w i ) of the class label w i .  Figure (1) shows a submatrix of P V as a heatmap corresponding to a subset of CIFAR-100 class labels on the x and y-axis. We see that "willow-tree" has a high similarity score with "maple-tree", "oak-tree", "palm-tree" and "pinetree". Therefore, samples from these class labels will be sampled more frequently as hard yet more informative negative samples. Similarly, ("man", "woman") and ("tractor", "pickup-truck") would be sampled at a higher rate than the remaining terms.</p><formula xml:id="formula_4">topk w (z T wi ) = arg max k =i cos(z T wi , z T w k )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">INSTANCE-LEVEL CONDITIONED SAMPLING</head><p>Class-level SCNS (or Class-SCNS) may not be granular enough as the conditional probability assigned to a class is the same for all samples within that class. In instancelevel SCNS we define the top-k nearest samples for each x ∈ D. A top-k instance similarity matrix produced by f T ω is iteratively constructed ∀x ∈ D and the outputs are stored in Z k</p><p>x ∈ R N ×k . As before, we define P k x and sample x − ∼ D k x ∝ P k x . Unlike Class-SCNS, f T ω is trained on the same modality (e.g images) as f S θ and P k x is now a SCNS matrix for each x * ∈ D and not per class label w ∈ V.</p><p>For image classification, we choose f T ω to be a pretrained CNN. We use the final hidden representation z T</p><p>x * ∈ R d which is a latent representation of an input image x ∈ R Ci×dw i ×d h i where C i is the number of input channels, d wi is the width of the input image and d hi is the input image height. However, if the l-th intermediate layer activation h ∈ R Co×d ho ×d ho is used to measure semantic similarity, we vectorize h to h ∈ R Cod ho d ho , where C o is the number of output channels, d ho is the height of the output feature maps and d wo is the output width. We note that Instance-SCNS may be favoured over Class-SCNS particularly when |V| is relatively low and k ≈ |V| (e.g CIFAR-10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">CONDITIONED SAMPLES WITH A LOOKUP TABLE</head><p>To further reduce training time we can combine SCNS with a lookup table <ref type="bibr" target="#b52">(Xiao et al., 2017)</ref> that stores negative sample features and updates during training. For ∀X + ∈ B, the dot product is computed between z S and the i-th column of the</p><formula xml:id="formula_5">L 2 row normalized lookup table V ∈ R d l ×Nv . If the i-th target y i is predicted then the update v i ← γv i + (1 − γ)z S is performed in the backward pass. A high γ ∈ [0, 1] results in a smaller update v i ∈ R d l in the lookup table. For X − , a circular queue Q ∈ R d l ×Nq is</formula><p>used where N q is the queue size and the dot product Q z S is computed. The current features are put into the queue and older features are removed from the queue during training. Equation 5 shows the conditional probability p i corresponding to the target y i where u n ∈ R d l is stored as the n-th row of Q. .</p><formula xml:id="formula_6">p i = e (v z S * /τ ) Nv m=1 e (v m z S * /τ ) + Nq n=1 e (u n z S * /τ )<label>(5)</label></formula><p>Algorithm 1 SCNS distillation algorithm.</p><p>1: input: mini-batch size M , number of batches N , student network f θ , teacher network gϕ, regularization terms γ+, γ−. 2: for k-NN sampled minibatch {Bi} N i=1 do 3:</p><p>= 0 4:</p><p>for all {x} M j=1 do 5:</p><p># Positive embedding features 6:</p><formula xml:id="formula_7">h S * , h S + , h S − = f S (x * ), f S (x+), f S (x−) 7: h T * , h T + , h T − = f S (x * ), f T (x+), f T (x−) 8: # contrastive features retrieved from lookup table 9: z S * , z S + , z S − = g S (h * ), g T (h+), g T (h−) 10: z T * , z T + , z T − = g T (h * ), g T (h+), g T (h−) 11: # Contrastive mixup representations 12: zS , zT = κ(z S − , z S * ), κ(z T − , z T * ) 13:</formula><p># student network prediction 14:</p><formula xml:id="formula_8">y S = σ(h S * W T ) 15:</formula><p># Cross-entropy loss 16:</p><formula xml:id="formula_9">:= + (1 − α) CE(y S , y) 17:</formula><p># Latent Mixup Loss 18:</p><formula xml:id="formula_10">:= + α KLD( zS , zT ) 19:</formula><p># KD loss of positive and negative samples 20:</p><formula xml:id="formula_11">:= − γ+ KD(z S + , z T + ) 21: := − γ− KD(z S − , z T − ) 22:</formula><p>end for 23:</p><p>perform gradient updates on f θ to minimize 24: end for 25: return encoder network f (•), and throw away g(•)</p><p>For the anchor sample x * , the conditional probability that the representation of a negative sample x − matches in the circular queue is given by Equation <ref type="formula" target="#formula_12">6</ref>.</p><formula xml:id="formula_12">q i = e (u i z S * /τ ) Nv m=1 e (v m z S * /τ ) + Nq n=1 e (u n z S * /τ )<label>(6)</label></formula><p>The gradient ∇ z S E z S [log p i ] is defined in the backward pass as Equation <ref type="formula">7</ref>,</p><formula xml:id="formula_13">∂ ∂z S = 1 τ (1 − p i )v − N V j=1, j =y p j v j − Nq k=1 q k u k (7)</formula><p>where y is the column of V corresponding to the y-th target y ∈ N + and y T in the KD setting. This lookup table can be used complementary to SCNS and we use it in our experiments. It is also easier to compare to prior CL approaches <ref type="bibr" target="#b45">(Tian et al., 2019)</ref> as they too use a lookup table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interpolating Contrastive Representations</head><p>Instead of using a pretrained network to define the negative samples that are close to the classification boundary, we can instead mix positive and negative representation to produce pseudo negative samples that are close to the positive sam-ple. <ref type="bibr">Mixup (Zhang et al., 2017)</ref> is a simple regularization technique that performs a linear interpolation of inputs.</p><p>Our proposed LM instead mixes the latent reprepsentations between positive and negative pairs given by the student and teacher networks as opposed to mixing the raw images. The motivations for this is that f S θ learns more about the geometry of the embedding space induced by f T ω and interpolating on a lower-dimensional manifold than the original input can lead to smoother interpolations. The interpolation function κ(z i , z j ) in Equation <ref type="formula">8</ref>outputs a contrastive mixture z from</p><formula xml:id="formula_14">z i ∈ R d , z j ∈ R d and the mixture coefficient ν ∈ [0, 1] is drawn from the beta distribution ν ∼ Beta(β, β) where β ∈ [0, ∞] and β → 0 approaches the empirical risk. κ(z i , z j ) = νz i + (1 − ν)z j (8)</formula><p>Both student and teacher LM representations and teacher targets are then computed as,</p><formula xml:id="formula_15">zS ij = κ(z S i , z S j ), zT ij = κ(z T i , z T j )<label>(9)</label></formula><formula xml:id="formula_16">ỹS ij = σ W T zS ij /τ , ỹT ij = σ κ(y T i , y T j )/τ<label>(10)</label></formula><p>where ỹT ij is a synthetic bimodal mixup target. Henceforth, we will denote mixup teacher targets as ỹT and LM representations as zS and zT . The objective can then be described by the KLD as Equation <ref type="formula" target="#formula_17">11</ref>where H is the entropy of the predicted teacher distribution over classes ỹT . When training from scratch with standard cross-entropy, the targets are mixed and renormalized with σ where τ performs label smoothing resulting in a peaked bimodal distribution.</p><formula xml:id="formula_17">D KLD ( ỹT || ỹS ) = H( ỹT ) − ỹT log( ỹS )<label>(11)</label></formula><p>Instead of using contrastive representation mixup with the KLD distillation objective, we also use it to mix between latent representations in the CL setting whereby representations of negative and positive samples are mixed to produce pseudo-hard negative sample representations. In this case zS := κ(z S i , z T j ) and similarly for the teacher network as shown in Line 12 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical Analysis of Conditioned Sampling</head><p>In this subsection we reformulate the MI lower bound to include the notion of semantic similarity between negative samples and their corresponding anchor. We then describe the difference in sample complexity between USNs and SCNS w.r.t. observing the top-k negative samples in the training data. We use the InfoNCE loss <ref type="bibr" target="#b36">(Oord et al., 2018)</ref> with SCNS for our experiments, as shown in Equation <ref type="formula">12</ref>,</p><formula xml:id="formula_18">= E (x * ,x+) ∼ D+ x−∼D−∝ Px − log exp(z * z + ) exp(z * z + ) + exp(z * z − ) (12)</formula><p>where x − is conditioned on the distribution P x as described in subsection 3.1. By minimizing the InfoNCE loss , we maximize the MI between the positive pair (x * , x + ). The optimal score for f (x * , x + ) is given by p(x * |x + )/p(x * ), substituting this into Equation <ref type="formula">12</ref>and splitting x into positive and negative samples X − gives:</p><formula xml:id="formula_19">= −E X log p(x * |x i )/p(x * ) p(x * |x+) p(x * ) + xi∈X− p(xi|x+) p(xi) = E X log 1 + p(x * ) p(x * |x + ) + xi∈X− p(x i |x + ) p(x i ) ≈ E X log 1 + p(x * ) p(x * |x + ) (M − 1)E xi p(x i |x + ) p(x i ) = E X log 1 + p(x * ) p(x * |x + ) (M − 1) ≥ E X log p(x * ) p(x * |x + ) M = −I(x * , x + ) + log(M )<label>(13)</label></formula><p>From Equation <ref type="formula" target="#formula_19">13</ref>, we see that I(x * , x + ) ≥ log(M ) − <ref type="bibr" target="#b36">(Oord et al., 2018)</ref> and the larger the number of negative samples, M , the tighter the MI bound. However, we argue that if a pretrained f T ω has training error close to 0, then log(M ) should be replaced with a term that accounts for the geometry of the embedding space as not all negative samples are equally important for reducing . Therefore, we express how top-k samples from SCNS tightens the lower bound estimate on MI when compared to using USNs. Given that we are not restricted to a distance, divergence or similarity between vectors, we refer to a general alignment function A that outputs an alignment score a ∈ [0, 1].</p><p>Given x * , the expected alignment score for the top-k negative samples is a k</p><formula xml:id="formula_20">x− := E x−∼D k x [A(z * , z − )] and for negative samples outside of the top-k samples, a r x− := E x−∼D r x [A(z * , z − )] where D r x * ⊆ D, D k x * ∈ D r x * , r = N − N y − k.</formula><p>and N y is the number of samples of class y. The alignment weight (AW) Ω x := 1−a k</p><p>x /(a k x +a r x ) is then used to represent the difference in 'closeness' between the top-k negative samples and the remaining negative samples. Lemma 1. Given Ω := M i=1 Ω x * , we can reformulate the MI lower bound when using SCNS as Equation <ref type="formula" target="#formula_21">14</ref>.</p><formula xml:id="formula_21">I(X, Y ) ≥ + log(2Ω)<label>(14)</label></formula><p>Proof. We substitute log(2Ω) for log(M ) in Equation <ref type="formula" target="#formula_19">13</ref>as a k x ≈ a r x in uniform sampling as M → ∞.</p><p>This lower bound favors top-k negative samples that have alignment with the positive class boundary and are relatively close compared to the negative samples outside of the top-k.</p><p>It is dependent on the loss of f * where ≈ 0 results in an accurate alignment estimation for all embedding pairs. Lemma 2. In the worst case, when all M negatives are equidistant to x * , forming a ring on the L 2 embedding hypersphere, SCNS is equivalent to uniform sampling.</p><p>Proof. This holds as log(2Ω) ≈ log(M ) when the centroids of both sets a k x = a r x and Ω = M/2. Therefore, in the worst case SCNS is equivalent to uniform sampling.</p><p>The above case can be due to degenerative representations used in the top-k SCNS similarity computation (i.e tr not close to 0) or a characteristic of the training data itself. Then, the relation between M USNs and top-k SCNSs can be formed as follows. Let Ω D U</p><p>x be the AW of USNs for x and Ω D k x be the AW for the top-k negative samples. When</p><formula xml:id="formula_22">|D U x | ≈ |D k x | we have, I(X, Y ) ≥ + log(2Ω D k x ) ≥ + log(2Ω U )<label>(15)</label></formula><p>given the non-uniform prior over negative samples as defined in SCNS. For some k</p><formula xml:id="formula_23">N , 2(Ω k − Ω U ) = 0 is met when a subset of D U</formula><p>x negative samples have zU x ≈ zk x where x denotes the aforementioned subset and zk</p><p>x is the centroid of the top-k negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Uniform Draws To Observe Top-k SCNS</head><p>We can now describe the expected number of USNs draws required to observe the top-k samples at least once for a given x * . Let C i denote the number of negative samples observed until the i-th new negative sample among the top-k samples is observed and N is the total number of samples until all top-k negative samples are observed. Since</p><formula xml:id="formula_24">C = k i=1 C i , E[C] = E k i=1 C i = k i=1 E C i where C i follows a geometric distribution with parameter (k + 1 − i)/M . Therefore, E[C i ] = M/(k + 1 − i)</formula><p>and thus the expected number of draws is given by Equation <ref type="formula">16</ref>.</p><formula xml:id="formula_25">E[C] = M k i=1 (k + 1 − i) −1 = M k i=1 i −1 (16)</formula><p>We reformulate this for mini-batch training where consecutive batches of size b for x * are drawn with replacement. This is a special case of the Coupon Collector's Problem ((CCP) Von <ref type="bibr" target="#b48">Schelling, 1954)</ref>. Theorem 3. The batch variant of the CCP formulates the probability of the expected number of batches of size b to observe top-k SCNS samples at least once as Equation 17</p><formula xml:id="formula_26">∞ j=0 P (K &gt; ib) = M −1 j=0 (-1) M −j+1 M j 1 1 − ( j M ) b<label>(17)</label></formula><p>Proof. See Appendix E for the proof.</p><p>As M grows more mini-batches are required to observe the top-k hard negative samples. Thus, b has to be larger for uniform sampling to cover the informative negative samples, coinciding with the MI formulation in Equation <ref type="formula" target="#formula_22">15</ref>. Further justifications are found in Appendix D and Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We Table <ref type="table" target="#tab_0">1</ref> shows this ablation, where bolded results represent the best performance within the horizontal lines of that section and † † corresponds to the best performance overall for the respective architecture. For 'InfoNCE-LM' and 'InfoNCE + LM', β = 0.01 corresponds to a slight mixing of negative pair latent representations and β = 0.5 leads to a U-shape probability density function in [0, 1], leading to increased LM.</p><p>We find that using the original negative samples and the latent mixture features (InfoNCE + LM) improves over only using the LM features (InfoNCE-LM). We also find  Knowledge Distillation We now discuss results of the KD experiments on CIFAR-100 <ref type="bibr" target="#b29">(Krizhevsky et al., 2009)</ref>. Figure <ref type="figure" target="#fig_2">2</ref> shows the test accuracies for each KD method where the student-teacher pair is different for each row and the colours correspond to [0-1] row normalized test accuracies to visualize the relative percentage increase or decrease between each KD method<ref type="foot" target="#foot_0">1</ref> We find that combining LM with InfoNCE + Instance-SCNS with a lookup table outperforms only using LM or instance-level SCNS with a lookup table. Moreover, 'InfoNCE-LM + Class-SCNS' outperforms all other KD methods for all but one studentteacher pairing (PKT found to have the highest test accuracy for 'resnet14-wrn-16-1' student-teacher pair). However, the original KLD distillation loss remains a very strong baseline that is competitive with CL and even outperforms our proposed non-contrastive baselines. We find a 0.24 point increase in the 0-1 normalized average score ('Kullbeck Leibler Distillation' = 0.75 and 'InfoNCE-LM + Class-SCNS'=0.99). We also find w.r.t. the student-teacher network capacity gap, increasing the capacity of the teacher network does not necessarily lead to improved student network performance if the gap is large. The performance difference between 'resnet14-wrn-16-1' and 'resnet14-wrn-40-1' is relatively small and 'resnet14-wrn-40-1' has higher accuracy than 'resnet14-wrn-16-1' in only 7/16 different loss functions. However, in 3/4 of the CL cases (4 rightmost columns of Figure <ref type="figure" target="#fig_2">2</ref>) the larger teacher network in 'resnet14-wrn-40-1' has significantly improved accuracy. Figure <ref type="figure" target="#fig_4">4</ref> shows the convergence time comparing InfoNCE when using USNs and SCNS for the resnet32-wrn-16-1 student-teacher pair   without any additional data augmentation. These models are used as the teacher networks that take in 256x256 images while the student network takes the original 64x64 input. This KD setup is slightly different as the teacher network is fine-tuned using transfer learning from the original ImageNet dataset, not from random initialization.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows each KD technique along with our proposed techniques from row 'Contrastive-PC' to the last row. In   almost all student-teacher network combinations, Instance-SCNS, Class-SCNS samples and both with Contrastive LM regularization have led to performance improvements over all previously proposed KD methods. However, the original KLD distillation loss <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref> remains a very strong baseline. We also find that increasing the capacity of the teacher network for the same sized student network can result in the same or poorer performance if the original student-teacher network capacity difference is large. For example, if we compare '-resnet14-wrn-16-1' to 'resnet14-wrn-40-1" we can see there is little difference in performance across the different KD methods. However, increasing the student network size closer to the teacher network leads to improved performance e.g 'resnet32-wrn-16-1' consistently improves over 'resnet14-wrn-16-1'.</p><p>Table <ref type="table" target="#tab_4">3</ref> shows the ablation of all three proposed methods on CIFAR-100. The most consistent gain in performance is found when using Instance-SCNS as it achieves the best performance for 4 out of 6 student networks. Class-SCNS performs the best for resnet20 student networks, which have relatively larger capacity compared to resnet8 and resnet14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed (1) semantically conditional negative sampling, a method that use pretrained networks to define a negative sampling distribution and (2) latent mixup, a simple strategy to form hard negative samples. We found that when used in a contrastive learning setting, both proposals consistently outperform previous knowledge distillation methods and improve contrastive learned models in the standard supervised learning setup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hardware Details</head><p>All experiments were run on a Titan RTX P8 24G memory graphics processing unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architectures</head><p>The below CNN architectures are used for standard supervised learning experiments on CIFAR10 and for KD experiments on CIFAR100 and Tiny-ImageNet-200.</p><p>• Wide Residual Network (WRN; Zagoruyko &amp; Komodakis, 2016b). WRN-d-w represents wide resnet with depth d and width factor w. • ResNet <ref type="bibr" target="#b19">(He et al., 2016)</ref>. We use resnet-d to represent cifar-style resnet with 3 groups of basic blocks, each with 16, 32, and 64 channels respectively. In our experiments, resnet8x4 and resnet32x4 indicate a 4 times wider network (namely, with 64, 128, and 256 channels for each of the block). • ResNet <ref type="bibr" target="#b19">(He et al., 2016)</ref>. ResNet-d represents ImageNet-style ResNet with Bottleneck blocks and more channels. • ResNeXt <ref type="bibr" target="#b53">(Xie et al., 2017)</ref>. ResNeXt ImageNet-style ResNet with Bottleneck blocks and more channels. • VGG <ref type="bibr">(Simonyan &amp; Zisserman, 2014)</ref>. The pretrained VGG network are adapted from its original ImageNet counterpart. • DenseNet <ref type="bibr">(Huang et al., 2017)</ref>. We use a pretrained Im-ageNet DenseNet-121 and fine-tune on Tiny-ImageNet-200 with upscale images (64x64 to 256x256).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Conditional Negative Sampling Details</head><p>Before running experiments for supervised learning and knowledge distillation, we must first define the negative sampling distribution on the instance-level or class-level.</p><p>For class-level SCNS we use cross-modal transfer by using word embeddings for the class labels. We use skipgram word vectors <ref type="bibr" target="#b33">(Mikolov et al., 2013)</ref> that are pretrained on GoogleNews and can be retrieved from https://code. google.com/archive/p/word2vec/. For class labels that are phrases, we average the pretrained word embeddings of each constituent embedding prior to computing cosine similarity. We find best results for our proposed method with the temperature τ = 5 when constructing P. This ensures that the distribution is not too flat and encourages tighter coupling of neighbours.</p><p>For instance-level SCNS, pair similarity is defined by a pretrained network of the same type that is used for training in the supervised learning setting. For knowledge distillation, the teacher network is used to define the pair similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Dataset and Model Details</head><p>For all models used in the standard supervised learning and KD settings, we use the cross-entropy loss optimized using Stochastic Gradient Descent (SGD) with a decay rate (different setting for each task). Additionally, hyperparameter tuning of β and k is tested on a randomly sampled 5% of the predefined training data of all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>For CIFAR-10, the learning rate is set to 0.01, momentum=0.9 and weight decay=0.0005. The images are randomly cropped and horizontally flipped and normalized along the input channels (as two tuple arguments (0.4914, 0.4822, 0.4465), <ref type="bibr">(0.2023, 0.1994, 0.2010)</ref> in the transforms.Normalize method in the torchvision library). The batch size is 128 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>For CIFAR-100 a dataset with 50k training images (500 samples per 100 classes) and 10k test images, the learning rate is set to 0.05 with a decay rate 0.1 at every 25 epochs after 100 epochs until the last 200 epoch. The batch size is set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tiny-ImageNet-200</head><p>For Tiny-ImageNet-200, we train for 100 epochs and decay the learning rate every 20 epochs. The batch size is also set to 64. The student is trained by a combination of cross-entropy classification objective and a KD objective as = CE + α KD .</p><p>Baseline KD Settings The abbreviations refer to correspond to the KD method names listed in Table <ref type="table" target="#tab_7">4</ref>.</p><p>The main KD influence factor α is set based on either the original paper settings or a set using a grid search over    <ref type="bibr" target="#b22">(Heo et al., 2019)</ref>: α = 0.2, distillation happens in a separate pre-training stage where only distillation objective applies. 9. FT <ref type="bibr" target="#b27">(Kim et al., 2018)</ref>: α = 500 10. FSP <ref type="bibr" target="#b54">(Yim et al., 2017)</ref>: α = 0, distillation happens in a separate pre-training stage where only distillation objective applies. 11. NST <ref type="bibr" target="#b26">(Huang &amp; Wang, 2017</ref>): α = 50 12. CRD Contrastive Representation Distillation <ref type="bibr" target="#b45">(Tian et al., 2019)</ref>: α = 0.8, in general α ∈ [0.5, 1.5] works well. 13. Kullbeck-Leibler Divergence Distillation (KLD) <ref type="bibr" target="#b23">(Hinton et al., 2015)</ref>: α = 0.9 and τ = 4.</p><p>Our Proposed KD Baseline Method Settings 1. Siamese-PC: This is the loss from Equation <ref type="formula">21</ref>. α = 0.8 2. Triplet CKA (Linear) or referred to as Linear-CKA: This is the loss from Equation 20 loss with a linear kernel -α = 0.8 and ζ = 0.2 3. Triplet CKA (RBF) or referred to as Linear-RBF: α = 0.8 and ζ = 0.2 4. Contrastive-CKA described in Equation <ref type="formula" target="#formula_31">20</ref>: α = 0.15 5. Contrastive-PC: This is the loss in Equation <ref type="formula">21</ref>applied to both the positive pair embeddings and negative sample pair embeddings.</p><p>Our Proposed SCNS-Based KD Settings 1. InfoNCE Loss with Instance-level SCNS: α = 0.9</p><p>2. InfoNCE-LM: This is the InfoNCE loss between latent mixup representations as in Equation <ref type="formula" target="#formula_15">9</ref>. Preprocessing Details For experiments with the CKA objective we group mini-batches by their targets as CKA operates on cross-correlations between samples of the same class. Therefore, random shuffling is carried out on the mini-batch level but not on the instance level. For all other objectives, standard random shuffling of the training data is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Metric Learning Distillation Objectives</head><p>In our work we also propose two correlation and kernelbased loss functions that can be used for both standard pointwise-based KD and metric-learned KD. These are used as alternatives from those described in the related research, which we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric-based Centered Kernel Alignment</head><p>The CKA function measures the closeness of two set of points that can be represented as matrices. Thus far it has only been used for analysing representation similarity in neural networks <ref type="bibr" target="#b28">(Kornblith et al., 2019)</ref> but not for optimizing a neural network. We propose to distil the knowledge of the teacher network by minimizing the alignment between student and teacher representations using CKA as a baseline.</p><p>For two arbitrary matrices Z i ∈ R M ×d L and Z j ∈ R M ×d L , each consisting of a set of neural network representations, the centered alignment (CA) can be expressed as,</p><formula xml:id="formula_27">CA(Z i , Z j ) = vec(Z i Z i ), vec(Z j Z j ) ||Z i Z i || F ||Z j Z j || F (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>where || • || F is the Frobenius norm. We can replace the the dot product in numerator with a kernel function K(•, •) to compute the CKA. The kernel function is smooth and differentiable, hence we use it as a loss function for KDbased metric learning to maximize the similarity between the positive class latent representations given by (z S + , z T + ) and negative class latent representations (z S − , z T − ). Equation 19 shows the formulation of CKA where a kernel is used instead of the dot product.</p><formula xml:id="formula_29">CKA(Z i , Z j ) =K(Z i , Z j ) − E Z [K(Z i , Z j )]− E Zj [K(Z i , Z j )] + E Zi,Zj [K(Z i , Z j )]<label>(19)</label></formula><p>In our experiments, a linear kernel and a radial basis function</p><formula xml:id="formula_30">(K(Z i , Z j ) = exp(−||vec(Z i ) − vec(Z j )|| 2 F /2S 2 ))</formula><p>were used where S 2 is the sample variance. To account for intravariations and inter-variations between between student and teacher representations the CKA loss is used as apart of a triplet loss that maximizes the kernel similarity between the positive pair of the student anchor and student positive sample and also the student anchor with the teacher anchor. This is shown as + CKA in Equation <ref type="formula" target="#formula_31">20</ref>, where z * represents the anchor sample. The same is computed for the negative pair, denoted by − CKA . Both losses are combined as one where ζ controls the tradeoff between positive pair losses and negative pair losses and m is the margin.</p><formula xml:id="formula_31">+ CKA =CKA(z S + , z S * ) + CKA(z S * , z T * ), − CKA =CKA(z S − , z S * ) + CKA(z S − , z T * ), CKA = max 0, ζ + CKA − (1 − ζ) − CKA + m<label>(20)</label></formula><p>Concretely, this will force all positive class representations to have high a CKA score within and across the samples for the student and teacher representations, and similarly for the negative pair of the triplet.</p><p>Pearson Correlation Representation Distillation An alternative to maximizing the mutual information between z S and z T <ref type="bibr" target="#b11">(Belghazi et al., 2018)</ref> is instead to maximize the linear interactions using a PC-based loss as a strong baseline. The objective to be maximized is expressed as Equation 21</p><formula xml:id="formula_32">+ PC =ρ PC (z S − , z T − ) + ρ PC (z S + , z T + ), − PC =ρ PC (z S − , z S + ) + ρ PC (z S − , z T + ), PC = max 0, ζ + PC − (1 − ζ) − PC + m (21)</formula><p>where ρ PC ∈ [−1, 1] computes the correlation coefficient. When using the PC loss with contrastive learning ('Contrastive-PC') we take the average loss as</p><formula xml:id="formula_33">1 N −1 N −1 i=1 ρ PC (z S −,i , z S + )</formula><p>and similarly for the remaining losses that use negative sample representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Connection Between Mutual Information &amp; Conditional Negative Sampling</head><p>In this section we describe contrastive learning with our proposed conditional negative sampling in terms of mutual information (MI). Let p(y) be the probability of observing the class label y and p(x, y) denote the probability density function of the corresponding joint distribution. Then, the MI is defined as Equation <ref type="formula" target="#formula_34">22</ref>I(X; Y ) = y x p(x, y) log p(x, y) p(x)p(y) dx</p><p>and can be further expressed in terms of the entropy H(X) and conditional entropy H(X|Y ) as shown in Equation <ref type="formula" target="#formula_35">23</ref>.</p><formula xml:id="formula_35">I(X; Y ) = y x p(x, y) log p(x|y) p(x) dx = − x p(x) log p(x) − (− x y log p(x, y)p(x|y)) = H(X) − H(X|Y )<label>(23)</label></formula><p>Then I(X; Y ) can be formulated as the KL divergence between p(x, y) and the product of marginals p(x) and p(y),</p><formula xml:id="formula_36">I(X; Y )=D KL p(x, y)||p(x)p(y) = E p(x,y) p(x, y) p(x)p(y)<label>(24)</label></formula><p>Hence, if the classifier can accurately distinguish between samples drawn from the joint p(x, y) and those drawn from the product of marginals p(x)p(y), then X and Y have a high MI. However, estimating MI between high-dimensional continuous variables is difficult and therefore easier to approximate by maximizing a lower bound on MI. This is known as the InfoMax principle <ref type="bibr" target="#b31">(Linsker, 1988)</ref>. In the below subsections, we describe how this MI lower bound is maximized using the InfoNCE loss <ref type="bibr" target="#b36">(Oord et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Estimating Mutual Information with InfoNCE</head><p>The InfoNCE loss maximizes the MI between z i and z j (which is bounded by the MI between z i and z j ). The optimal value for f (z j , z i ) is given by p(z j |z i )/p(z j ). Inserting this back into Equation <ref type="formula" target="#formula_4">4</ref>and splitting X into the positive sample and the negative examples X − results in:</p><formula xml:id="formula_37">= −E X log p(x * |xi) p(x * ) p(x * |x+) p(x * ) + xi∈X− p(xi|x+) p(xi) = E X log 1 + p(x * ) p(x * |x + ) + xi∈X− p(x i |x + ) p(x i ) ≈ E X log 1 + p(x * ) p(x * |x + ) (M − 1)E xi p(x i |x + ) p(x i ) = E X log 1 + p(x * ) p(x * |x + ) (M − 1) ≥ E X log p(x * ) p(x * |x + ) M = −I(x * , x + ) + log(M )<label>(25)</label></formula><p>Therefore, I(z j , z i ) ≥ log(N ) − which holds for any f , where higher leads to a looser MI bound. This MI bound becomes tighter as the number of negative sample pairs M increases and in turn is likely to reduce . In our work we argue that the log(M ) term be replaced with a term that accounts for the geometry of the embedding space as not all negative sample pairs are equally important for reducing . Therefore, the next subsection describes our formulation of the MI bound that incorporates the notion of semantic similarity between embeddings of the sample pairs in order to choose an informative M samples to tighten the bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Mutual Information Lower Bounds for Semantically Conditioned Negative Sampling</head><p>We formalize the connection between SCNS and maximizing MI between representations in the contrastive learning and formulate an expression that describes how SCNS tightens the lower bound estimate on MI. We begin by defining 'closeness' between representations {z * , z + , z − } ∈ Z of samples {x * , x + , x − } ∈ X . Given that we are not restricted to a distance, divergence or similarity between vectors, we refer to a general measure as an alignment function A that outputs an alignment score a ∈ [0, 1].</p><p>Given an anchor sample x * , the expected alignment score for the top-k negative samples is a k From the above, α ≈ 1 corresponds to negative samples  <ref type="bibr" target="#b52">(Xiao et al., 2017)</ref>. Blue circles are easy negative sample embeddings, green circles are the top-k most semantically similar embeddings for the target sample in green and black circles represent embedding centroids computed using a weighted average of embedding where the weights are the alignment score. lie very close to x * . We can then use the alignment weight (AW) Ω x * := 1 − a k</p><p>x * /(a k x * + a r x * ) to represent the difference in 'closeness' between the top-k negative samples and the remaining negative samples. This is visualized as the difference in alignment between the centroids in the embedding space shown in Figure <ref type="figure" target="#fig_9">5</ref>.</p><p>We can then replace M negative samples as Ω := M i=1 Ω xi and substitute log(M ) with log(2Ω) in Equation <ref type="formula" target="#formula_37">25</ref>. When the centroids of both negative samples sets are close (i.e a k</p><p>x * = a r x * ) then Ω = M/2. Hence log(2Ω) ≈ log(M ) when the negative samples are centered in the same region, in which case uniform sampling provides the same guarantees as SCNS. The new MI lower bound is then Equation <ref type="formula" target="#formula_38">26</ref>.</p><formula xml:id="formula_38">I(X, Y ) ≥ + log(2Ω)<label>(26)</label></formula><p>Intuitively, this bound favors top-k negative samples that are close to the positive class boundary but are also relatively close compared to the remaining negative samples. This is dependent on how the embedding space is constructed from the pretrained network and which A is chosen. Hence, this is clearly an estimation on the true MI lower bound.</p><p>We can now define the relation between M uniformly sampled negatives (USNs) and top-k SCNSs. Let Ω D U x be the AW of USNs for x * and Ω D k x * is the AW for the top-k negative samples. When |D U</p><p>x * | ≈ |D k x * | (i.e both negative sample sets lie on a ring on the hypersphere around x * ),</p><formula xml:id="formula_39">I(X, Y ) ≤ + log(2Ω D k x ) ≤ + log(2Ω U )<label>(27)</label></formula><p>given the non-uniform prior over negative samples as defined in SCNS. For some k N , 2(Ω k − Ω U ) = 0 is met when a subset of D U x negative samples have zU x ≈ zk x where the subscript x denotes the aforementioned subset and zk</p><p>x is the centroid of the top-k negative samples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Uniform vs SCNS Sample Complexity</head><p>In this section, we aim to identify the relationship between uniform sampling and SCNS by formulating how many draws are required to cover the the top-k samples. We begin by the simpler case of single draws with a uniform distribution and extend it to batches of negative samples of size b are drawn uniformly. We then repeat this with draws of unequal probability in subsection E.2, as is the case for SCNS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Number of Samples Until Observing Top-k SCNS Samples Under a Uniform Distribution</head><p>We now describe the expected number of i.i.d drawn negative samples from M to observe the top-k samples at least once for a given x. Let N i denote the number of negative samples observed until you see the i-th new negative sample among the top-k samples and N is the number of samples until all top-k negative samples are observed. Since</p><formula xml:id="formula_40">N = k i=1 N i , E[N ] = E k i=1 N i = k i=1 E N i<label>(28)</label></formula><p>where N i follows a geometric distribution with parameter In subsection E.1, we formulate the number of uniform negative samples required to cover the top-k negative samples at least once for a single consecutive draws. However, in practice mini-batch training is carried out and therefore it is necessary to reformulate this for consecutive mini-batch draws of size b for a given x with replacement. This is a special case of the Coupon Collector's Problem. Hence, we get a concise equivalent expression <ref type="bibr" target="#b16">(Flajolet et al., 1992)</ref>: From the above expression, we clearly see that when p i is defined by a non-uniform distribution, the number of draws is proportionally larger in N . However, our original goal is to only sample from the most probably top-k samples, in which case E[Z] is lower.</p><formula xml:id="formula_41">E[X − ] =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Results</head><p>Figure <ref type="figure" target="#fig_13">6</ref> is a boxplot of how the performance changes for different KD methods as the student-teacher capacity gap varies. The purpose of this is to identify how much the performance increases are due to larger capacity as opposed to the particular KD method used.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The k-nearest neighbor (k-NN) similarity scores are then stored in Z k V ∈ R |V|×k with a corresponding matrix I k V ∈ R |V|×k that stores the k-NN class indices Z k V retrieved by applying Equation 4. We focus on only sampling from the top-k classes and therefore define the normalized top-k class distribution matrix as P k V := σ(Z k V /τ ). A row vector of P k V is denoted as P k w consisting of the k truncated conditional probabilities corresponding to the k nearest class labels of w ∈ V. We then sample as in Equation 3 instead with top-k negative samples D k w ⊂ D w and |D k w | = k|D w |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. CIFAR-100 subset of word embedding class similarities</figDesc><graphic url="image-1.png" coords="3,313.29,555.16,222.30,137.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. CIFAR-100 Test Accuracy for Knowledge Distillation Approaches:The y-axis model naming convention is student-# convolutional layers-teacher-# convolutional layers-# fully-connected -layers and the x-axis denotes the KD method.</figDesc><graphic url="image-2.png" coords="7,71.82,67.06,453.24,178.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect of β in Latent Mixup and k in SCNS</figDesc><graphic url="image-3.png" coords="7,57.24,299.67,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. CIFAR-100 Convergence Time Comparison</figDesc><graphic url="image-4.png" coords="8,61.85,309.11,221.18,165.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1. Fitnets<ref type="bibr" target="#b41">(Romero et al., 2014)</ref>: α = 80 2. AT (Zagoruyko &amp; Komodakis, 2016a): α = 600 3. SP (Tung &amp; Mori, 2019): α = 2000 4. CC (Peng et al., 2019): α = 0.05 5. VID (Ahn et al., 2019): α = 0.8 6. RKD (Park et al., 2019): α 1 = 25 for distance and α 2 = 50 for angle. For this loss, we combine both term following the original paper. 7. PKT Probabilistic Knowledge Transfer (Passalis &amp; Tefas, 2018) (Passalis &amp; Tefas, 2018): α = 10000 8. AB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>3. InfoNCE+Class SCNS: This represents SCNS with an InfoNCE loss. α = 0.65 4. InfocNCE-LM + Class-SCNS: This represents SCNS with an InfoNCE loss with an second InfoNCE loss for latent mixup representations. α = 0.65</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>k x [A(z * , z − )] and for the negative samples outside of the top-k samples it is a r x− := E x−∼D r x [A(z * , z − )] where D r x * ⊆ D, D k x * ∈ D r x * , r = N − N y − k. and N y is the number of samples of class y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. k-NN Conditional Negative Sampling for embeddings on the L2 sphere retrieved from the lookup table<ref type="bibr" target="#b52">(Xiao et al., 2017)</ref>. Blue circles are easy negative sample embeddings, green circles are the top-k most semantically similar embeddings for the target sample in green and black circles represent embedding centroids computed using a weighted average of embedding where the weights are the alignment score.</figDesc><graphic url="image-5.png" coords="14,356.04,67.06,136.80,136.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(k + 1 − i)/M . Therefore E[N i ] = M k+1−i and E[N ] = M k i=1 (k + 1 − i) −1 = M k i=1 i −1 .E.1.1. NUMBER OF BATCHES TO COVER ALL SAMPLES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>pi+pj )x + . . . + (−1) N +1 e −(p1+...+p N )x (32)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>e −pix ) dx (33)The probability of sampling the i-th top-k negative sample is p i ≥ 0 such that p 1 + . . .+ p N = 1. To determine E[N ],we first assume that the number of negative samples to draw t as X − (t), follows a Poisson distribution with parameter λ = 1. Let I i be the inter-arrival time between the (i − 1)th and the i-th negative sample draw: I i has exponential distribution with parameter λ = 1. Let Z i be the time in which the i-negative sample arrives for the first time (hence Z i ∼ exp(p i )) and let Z = max{Z 1 , . . . , Z N } be the time in which we have observed all samples at least once. Note that Z = N i=0 I i and E[X] = E[Z], indeed: it suffices to calculate E[Z] to get E[N ]. Since Z = max{Z 1 , .., Z N }, we have F Z (t) = P(Z ≤ t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Tiny-ImageNet Boxplot Test Accuracy for Knowledge Distillation Approaches</figDesc><graphic url="image-6.png" coords="16,309.66,115.05,229.56,152.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. CIFAR-100 Test Accuracy for KD Approaches</figDesc><graphic url="image-8.png" coords="17,55.44,384.41,486.97,192.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Tiny-Imagenet 200 Test Accuracy for Knowledge Distillation Approaches</figDesc><graphic url="image-10.png" coords="18,56.99,402.97,482.90,167.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>now discuss the experimental results and note that additional details on hardware, datasets, model architectures and settings are found in Appendix A, B and C. Test accuracy (%) of student networks on CIFAR-10.</figDesc><table><row><cell cols="7">Standard Supervised Learning We first test our three</cell></row><row><cell cols="7">NS approaches in the standard supervised learning setting</cell></row><row><cell cols="7">on CIFAR-10. Several ResNet-based architectures (He et al.,</cell></row><row><cell cols="7">2016; Zagoruyko &amp; Komodakis, 2016b; Xie et al., 2017)</cell></row><row><cell cols="7">are trained with (1) cross entropy between LM representa-</cell></row><row><cell cols="7">tions and the cross-entropy between student predictions and</cell></row><row><cell cols="7">targets (Cross-Entropy + LM) and CL (InfoNCE + LM),</cell></row><row><cell cols="7">(2) only using CL with the LM representations of each con-</cell></row><row><cell cols="7">trastive pair (InfoNCE-LM) and (3) Class-SCNS (InfoNCE</cell></row><row><cell cols="7">+ Class-SCNS) and Instance-SCNS (InfoNCE + Instance-</cell></row><row><cell cols="3">SCNS) with the InfoNCE loss.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">resnet20 resnet32 resnet110 wrn-16-1 wrn-40-1 resnext32x4</cell></row><row><cell>Cross-Entropy</cell><cell>91.14</cell><cell>92.49</cell><cell>93.38</cell><cell>94.06</cell><cell>94.47</cell><cell>95.38</cell></row><row><cell>Cross-Entropy + LM (β=0.5)</cell><cell>92.74</cell><cell>92.73</cell><cell>93.53</cell><cell>94.26</cell><cell>94.54</cell><cell>95.47</cell></row><row><cell>InfoNCE</cell><cell>91.68</cell><cell>92.90</cell><cell>94.01</cell><cell>94.39</cell><cell>95.23</cell><cell>95.77</cell></row><row><cell>-LM (β=0.01)</cell><cell>91.72</cell><cell>92.93</cell><cell>94.09</cell><cell>94.48</cell><cell>95.55</cell><cell>95.87</cell></row><row><cell>-LM (β=0.05)</cell><cell>91.90</cell><cell>93.08</cell><cell>94.24</cell><cell>94.67</cell><cell>94.81</cell><cell>95.89</cell></row><row><cell>-LM (β=0.1)</cell><cell>91.97</cell><cell>93.17</cell><cell>94.42</cell><cell>94.81</cell><cell>94.95</cell><cell>96.01</cell></row><row><cell>-LM (β=0.2)</cell><cell>92.17</cell><cell>93.12</cell><cell>94.63</cell><cell>94.99</cell><cell>94.93</cell><cell>96.07</cell></row><row><cell>-LM (β= 0.5)</cell><cell>92.38</cell><cell>93.21</cell><cell>94.85</cell><cell>95.11</cell><cell>95.09</cell><cell>96.22</cell></row><row><cell>+ LM (β=0.01)</cell><cell>91.92</cell><cell>93.12</cell><cell>94.17</cell><cell>94.53</cell><cell>95.68</cell><cell>95.97</cell></row><row><cell>+ LM (β=0.05)</cell><cell>91.98</cell><cell>93.20</cell><cell>94.53</cell><cell>94.95</cell><cell>94.73</cell><cell>96.94</cell></row><row><cell>+ LM (β=0.1)</cell><cell>92.04</cell><cell>93.49</cell><cell>94.44</cell><cell>94.79</cell><cell>94.99</cell><cell>96.24</cell></row><row><cell>+ LM (β=0.2)</cell><cell>92.40</cell><cell>93.38</cell><cell>94.92</cell><cell>95.07</cell><cell>95.05</cell><cell>96.11</cell></row><row><cell>+ LM (β=0.5)</cell><cell>92.96  † †</cell><cell>93.56</cell><cell>95.02</cell><cell>95.29  † †</cell><cell>95.13</cell><cell>96.08</cell></row><row><cell>+ Class-SCNS (k = 1)</cell><cell>92.04</cell><cell>93.08</cell><cell>94.61</cell><cell>93.97</cell><cell>95.52</cell><cell>95.82</cell></row><row><cell>+ Class-SCNS (k = 2)</cell><cell>92.44</cell><cell>93.10</cell><cell>94.83</cell><cell>94.42</cell><cell>96.03  † †</cell><cell>96.09</cell></row><row><cell>+ Class-SCNS (k = 5)</cell><cell>91.98</cell><cell>93.02</cell><cell>94.43</cell><cell>93.88</cell><cell>95.23</cell><cell>95.77</cell></row><row><cell>+ Instance-SCNS (k=|D|/5)</cell><cell>92.01</cell><cell>93.27</cell><cell>94.34</cell><cell>93.49</cell><cell>95.28</cell><cell>95.19</cell></row><row><cell>+ Instance-SCNS (k=|D|/10)</cell><cell>92.12</cell><cell>93.29</cell><cell>94.09</cell><cell>93.58</cell><cell>95.32</cell><cell>95.41</cell></row><row><cell>+ Instance-SCNS (k=|D|/20)</cell><cell>92.20</cell><cell>93.25</cell><cell>94.83</cell><cell>94.14</cell><cell>95.66</cell><cell>95.99</cell></row><row><cell cols="2">+ Instance-SCNS (k=|D|/100) 92.42</cell><cell cols="2">94.39  † † 95.11  † †</cell><cell>94.56</cell><cell>95.91</cell><cell>96.09</cell></row><row><cell>-Instance SCNS (k=|D|/500)</cell><cell>92.38</cell><cell>93.57</cell><cell>95.02</cell><cell>93.36</cell><cell>95.72</cell><cell>96.27  † †</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test accuracy (%) of student networks on Tiny-ImageNet-200</figDesc><table /><note>on CIFAR-100. We see that Instance-SCNS converges after 106 training epochs, Class-level SCNS at 124 epochs while USNs converges at 181 epochs. Hence, both test accuracy and convergence time is improved by sampling hard negative samples via SCNS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>An ablation of efficient NS techniques on CIFAR-100.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Abbreviations for Knowledge Distillation Baselines few settings close to the original paper settings. For our proposed Pearson Correlation (PC) and Centered Kernel Alignment (CKA) KD objectives, we grid search over α ∈ [0.2, 0.5, 0.65, 0.8, 0.9] and ζ ∈ [0.1, 0.2, 0.5, 0.7, 0.9] for those objectives that use a margin-based triplet loss (e.g Triplet CKA). The parameter settings specified in the paper of previous KD methods is used and where not specified we manually grid search different settings of α.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The naming conventions of baselines, including our own KD baselines are described in Appendix B.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hence, as M grows more mini-batch updates are needed until the top-k hard negative samples are observed. Thus, b is required to be larger which is typically required in the MI formulation of NCE. To define the difference between USNs and SCNS we also need to define the CCP for unequal probabilities as defined by P x * for x * . We first make a distributional assumption. Here, we assume that the distances (or alignment) for x to its N negative samples follows a power law distribution. This is well-established for text <ref type="bibr" target="#b12">(Bollegala et al., 2010;</ref><ref type="bibr" target="#b40">Piantadosi, 2014)</ref> and we also observe a power law trend when computing the cosine similarities between all pairs with f T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Number of Samples Until Observing Top-k SCNS Samples Under an SCNS Distribution</head><p>In this subsection, we formulate the expected number of negative samples required for non-uniform sampling distributions, namely our proposed SCNS distribution provided by the teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum-Minimum Identity Approach</head><p>The number of draws required to observe all top-k NS is C = max{C 1 , . . . , C N } where N i has a conditional probability p i of being sampled as defined in SCNS. Since the minimum of N i and N j is the number of negative samples needed to obtain either the i-th top-k sample or the j-th top-k sample, it follows that for i = j, min(N i , N j ) has probability p i + p j and the same is true for the minimum of any finite number of these random variables. The Maximum-Minimums Identity <ref type="bibr" target="#b42">(Ross, 2014)</ref> is then used to compute the expected number of draws:</p><p>) We can then express the above in terms of the individual probabilities associated with drawing M negative samples conditioned on a given x * as, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>At (zagoruyko</surname></persName>
		</author>
		<author>
			<persName><surname>Komodakis</surname></persName>
		</author>
		<idno>43.81 (-8.41) 47.45 (-3.84) 45.48 (-2.32) 47.47 (-3.89) 47.01 (-2.22) 46.39 (-1.82) 39.45 (-2.60) 44.56 (-0.35) 45.20 (-0.43) 54.49 (-7.95</idno>
		<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sp (tung</surname></persName>
		</author>
		<author>
			<persName><surname>Mori</surname></persName>
		</author>
		<idno>48.12 (-4.10) 47.29 (-4.05) 44.30 (-3.50) 45.50 (-4.45) 46.45 (-2.26) 46.35 (-1.86) 38.47 (-3.57) 43.51 (-1.39) 44.08 (-1.55) 53.07 (-9.37</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pkt (passalis</surname></persName>
		</author>
		<author>
			<persName><surname>Tefas</surname></persName>
		</author>
		<idno>49.14 (-3.08) 48.82 (-2.47) 46.42 (-1.38) 48.03 (-3.21) 47.69 (-0.39) 48.22 (+0.01) 40.10 (-1.95) 45.18 (+0.27) 45.05 (-0.58) 55.00 (-7.43</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cc (peng</surname></persName>
		</author>
		<idno>42.35 (-9.87) 45.27 (-6.02) 46.59 (-1.21) 46.56 (-4.15) 46.75 (-1.41) 47.20 (-1.01) 40.08 (-1.97) 45.02 (+0.11) 44.10 (-1.53) 49.06 (-13.37</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Vid (ahn</surname></persName>
		</author>
		<idno>48.49 (-3.73) 48.12 (-3.17) 45.59 (-2.21) 47.63 (-2.81) 48.09 (-4.98) 43.63 (-4.57) 40.65 (-1.40) 44.40 (-0.51) 45.45 (-0.18) 54.75 (-7.69</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ft (kim</surname></persName>
		</author>
		<idno>44.19 (8.03) 47.59 (-3.70) 46.03 (-1.77) 45.28 (-3.72) 47.18 (-9.57) 39.04 (-9.17) 39.67 (-2.38) 38.48 (-6.43) 44.25 (-1.38) 52.31 (-10.12</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Crd (tian</surname></persName>
		</author>
		<idno>50.42 (-1.79) 47.98 (-3.31) 46.92 (-0.88) 49.15 (-1.54) 49.35 (-1.75) 49.21 (+0.60) 39.86 (-2.18) 44.34 (-0.57) 45.28 (-0.35) 56.91 (-5.53</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Contrastive-Cka</forename></persName>
		</author>
		<idno>RBF) 47.57 (-4.65) 48.71 (-2.58) 46.29 (-1.51) 46.87 (-3.59) 47.31 (-1.54) 47.07 (-1.14) 41.28 (-0.77) 47.05 (+2.14) 44.76 (-0.87) 52.50 (-9.93</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Contrastive-Cka</forename></persName>
		</author>
		<idno>Linear) 47.87 (-4.35) 47.90 (-3.39) 45.89 (-1.91) 47.42 (-3.27) 41.13 (-1.62) 46.99 (-1.21) 40.03 (-2.02) 44.63 (-0.28) 44.51 (-1.12) 54.16 (-8.28</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>References Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The im algorithm: a variational approach to information maximization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational duality: Unsupervised extraction of semantic relations between entities on the web</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Birthday paradox, coupon collectors, caching algorithms and self-organizing search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thimonier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="229" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3779" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00414</idno>
		<title level="m">Similarity of neural network representations revisited</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>cs.toronto.edu</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measuring the semantic relatedness between words and images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011)</title>
				<meeting>the Ninth International Conference on Computational Semantics (IWCS 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An application of the principle of maximum information preservation to linear systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="186" to="194" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">thirtieth AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zipf&apos;s word frequency law in natural language: A critical review and future directions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1112" to="1130" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Fitnets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Hints for thin deep nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A first course in probability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2015. 2014</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic classbased hard example mining for deep metric learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7251" to="7259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving collaborative metric learning with efficient negative sampling</title>
		<author>
			<persName><forename type="first">V.-A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Royo-Letelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1201" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Coupon collecting for unequal probabilities</title>
		<author>
			<persName><forename type="first">Von</forename><surname>Schelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="306" to="311" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13149</idno>
		<title level="m">On mutual information in contrastive learning for visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
