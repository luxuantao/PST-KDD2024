<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
							<email>jindi@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongxiao</forename><surname>He</surname></persName>
							<email>hedongxiao@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Huang</surname></persName>
							<email>yuxiaohuang@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Scienc</orgName>
								<orgName type="institution">George Washington University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Powerful Graph Convolutional Networks with Adaptive Propagation Mechanism for Homophily and Heterophily</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have been widely applied in various fields due to their significant power on processing graph-structured data. Typical GCN and its variants work under a homophily assumption (i.e., nodes with same class are prone to connect to each other), while ignoring the heterophily which exists in many real-world networks (i.e., nodes with different classes tend to form edges). Existing methods deal with heterophily by mainly aggregating higher-order neighborhoods or combing the immediate representations, which leads to noise and irrelevant information in the result. But these methods did not change the propagation mechanism which works under homophily assumption (that is a fundamental part of GCNs). This makes it difficult to distinguish the representation of nodes from different classes. To address this problem, in this paper we design a novel propagation mechanism, which can automatically change the propagation and aggregation process according to homophily or heterophily between node pairs. To adaptively learn the propagation process, we introduce two measurements of homophily degree between node pairs, which is learned based on topological and attribute information, respectively. Then we incorporate the learnable homophily degree into the graph convolution framework, which is trained in an end-to-end schema, enabling it to go beyond the assumption of homophily. More importantly, we theoretically prove that our model can constrain the similarity of representations between nodes according to their homophily degree. Experiments on seven real-world datasets demonstrate that this new approach outperforms the state-of-the-art methods under heterophily or low homophily, and gains competitive performance under homophily.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Networks (such as social networks, citation networks, biological networks and traffic networks) are ubiquitous structures that can model relational data. Network analysis <ref type="bibr" target="#b22">(Wang et al. 2016</ref><ref type="bibr" target="#b20">(Wang et al. , 2019;;</ref><ref type="bibr" target="#b1">Cao et al. 2021;</ref><ref type="bibr" target="#b9">Jin et al. 2021b;</ref><ref type="bibr" target="#b6">He et al. 2021</ref>) has been a hot research topic for decades and has been widely used in many scientific fields such as computer science, social science, biology and physics <ref type="bibr" target="#b12">(Li and Goldwasser 2019;</ref><ref type="bibr" target="#b24">Yan et al. 2019)</ref>. Recently, graph convolutional network (GCN) <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>, which exhibits significant power on processing graph-structured data, has gained great success and already been adapted in various network analysis tasks, including node classification, community detection, anomaly detection and recommender system <ref type="bibr" target="#b16">(Tang, Aggarwal, and Liu 2016;</ref><ref type="bibr" target="#b18">Tian et al. 2014;</ref><ref type="bibr" target="#b3">Gao, Denoyer, and Gallinari 2011;</ref><ref type="bibr" target="#b4">Gong et al. 2019;</ref><ref type="bibr" target="#b24">Yu and Han 2014;</ref><ref type="bibr" target="#b8">Jin et al. 2021a)</ref>.</p><p>Although many GCN-based methods have been proposed in recent years, such as GraphSage <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>, GAT <ref type="bibr" target="#b19">(Velickovic et al. 2018)</ref>, MixHop <ref type="bibr" target="#b0">(Abu-El-Haija et al. 2019)</ref>, and HIN <ref type="bibr" target="#b23">(Xu et al. 2019)</ref>, they implicitly assume that most connected nodes are from the same class or with similar attributes, which is typically called the homophily of network structure. GCN-based methods reflect the homophily assumption by feature propagation and aggregation within graph neighborhoods. And these methods have shown satisfactory performance in many downstream tasks on network with homophily. However, in real world, there are also many networks where most connected nodes are from different classes, which is called heterophily or low homophily. For example, in dating networks, most people tend to connect with people of the opposite gender. In protein networks, different types of amino are more likely to connect with each other. Under heterophily or low homophily, these methods suffer from poor performance, as the propagation mechanism within graph neighborhoods, which is the most fundamental part of GCN, is problematic and will mix irrelevant information from different classes. Therefore, existing GCN-based methods cannot adapt to the scenario of heterophily or low homophily.</p><p>Recently, some efforts have been dedicated to generalizing GCN to heterophilic networks. For example, Geom-GCN <ref type="bibr" target="#b14">(Pei et al. 2020)</ref> proposes a novel geometric aggregation scheme which aggregates immediate neighborhoods and distant nodes that have a certain similarity with the target node in a continuous space. H2GCN <ref type="bibr" target="#b26">(Zhu et al. 2020)</ref> applies some key designs, such as higher-order neighborhoods aggregation and combination of intermediate representations, to boost learning from graph with heterophily. GPR-GNN <ref type="bibr" target="#b2">(Chien et al. 2021</ref>) deals with heterophily and oversmoothing by combining each step of feature propagation with a learnable weight. GGCN <ref type="bibr" target="#b23">(Yan et al. 2021</ref>) al-lows negative message propagation between graph neighborhoods based on the similarity of representations in order to decouple the heterophily and oversmoothing problems. CPGNN <ref type="bibr" target="#b25">(Zhu et al. 2021</ref>) incorporates an interpretable compatibility matrix for modeling the heterophily or homophily level in graphs, enabling it to go beyond the assumption of strong homophily. However, these methods suffer from serious problems. They coarsely aggregate higher-order (distant neighbors), or combine the intermediate representations, to deal with heterophily. While doing so can fuse effective information to some extent, it will also result in introducing noise and irrelevant information that influence negatively the prediction of downstream tasks. Most importantly, these methods do not change the propagation mechanism which is the essential part of GCN and is problematic under heterophily. This also make the representation of nodes from different classes mixed and indistinguishable.</p><p>To solve this problem, we focus on designing an adaptive propagation mechanism for both heterophilic and homophilic networks, and giving a new HOmophily-Guided Graph Convolutional Network called HOG-GCN. In this new approach, we introduce a homophily degree matrix into the graph convolution framework, which is used to model the homophily and heterophily of networks and further conduct the propagation process. This homophily degree matrix can be learned from the attribute and topology information via extracting class-aware information during the propagation process. As a result, the new graph convolution framework can automatically change the feature propagation process via modeling the homophily degree between node pairs using homophily degree matrix. The learning process of homophily degree matrix and the feature propagation process are trained jointly in an end-to-end fashion. Finally, we theoretically prove that the adaptive propagation process can constrain the similarity of representations between nodes according to homophily degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We first give the notations and problem descriptions, then give the definition of homophily ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations and Problem Descriptions</head><p>Given an undirected, unweighted and attributed network</p><formula xml:id="formula_0">G = (V, E, X), where V = {v 1 , v 2 , ..., v n } is a set of n nodes, E = {e ij } ⊆ V ×V</formula><p>is a set of edges, and X ∈ R n×f is a set of node attributes, where m represents the number of attributes. The i-th row of X represents the attributes of node v i . The topological structure of G is represented by an adjacency matrix A = [a ij ] ∈ R n×n , where a ij = 1 if nodes v i and v j are connected, or a ij = 0 otherwise. We focus on the semi-supervised node classification task in this paper. This is, assume each node belongs to one out of C classes and we have known the labels of a set of nodes V L with |V L | n. Each node v i ∈ V L is assigned to a label y i ∈ L = {1, 2, ...C}. The objective of node classification task is to predict the labels of V \V L . Definition 1. Homophily Ratio. Given a network G = (V, E, X), the homophily ratio h = |{(u,v):(u,v)∈E∧yu=yv}| |E| is the fraction of edges which connect nodes that have the same class, i.e., intra-class edges.</p><p>The homophily ratio h measures the overall homophily level in the graph and thus we have h ∈ [0, 1]. To be specific, graphs with h closer to 1 tend to have more edges connecting nodes within the same class, or say stronger homophily; on the other hand, graphs with h closer to 0 tend to have more edges connecting nodes in different classes, or say a stronger heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Framework</head><p>We first give a brief overview of our approach, and then introduce the proposed new method in specific including detailed descriptions of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>To let the propagation mechanism of GCN essentially suitable for both homophily and heterophily, we propose a novel homophily-guided graph convolution framework that can automatically learn the propagation process according to the homophily degree between node pairs, which is called HOG-GCN. In specific, we incorporate a homophily degree matrix into the graph convolution framework for modeling the homophily and heterophily and further use it to adaptively change the propagation process between neighborhoods. The whole structure of the proposed approach is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of two components: homophily degree matrix estimation and homophily-guided propagation. The homophily degree matrix is learned from the attribute and topology information during the propagation process, which is further used to conduct the feature propagation between neighborhoods. In return, the propagation process can help learn better homophily degree matrix through downstream semi-supervised task. Therefore, these two components can be enhanced by each other and trained jointly. For the homophily degree matrix, we extract classaware information from node attributes and network topology, respectively, and then define the whole homophily degree based on these two types of information. From the perspective of attributes, we leverage multi-layer perceptron to extract class-aware information, since node attributes are not constrained by heterophily. Then, the homophily degree in attribute space can be further calculated. From the perspective of network topology, since it exhibits different degrees of heterophily, we consider making full use of the available label information to capture homophily degree in topology space. To this end, we propose a generalized label propagation technique with a learnable weight matrix, which can reflect homophily degree between node pairs in topology space. The intuition is that the influence between intra-class labels is greater than that between inter-class labels. For the homophily-guided propagation, we introduce the homophily degree matrix into the propagation process which can reveal the underlying distribution of homophily or heterophily in networks, enabling the framework to adaptively change the feature propagation weights according to homophily degree between neighborhoods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Homophily Degree Matrix Estimation</head><p>To adaptively change the propagation process for homophily and heterophily, we learn the homophily degree between node pairs during the propagation process. The homophily degree describes the extent to which two nodes belong to the same class. However, it is difficult to calculate the homophily degree directly from node labels, since only part of label information is available under the semi-supervised task. To this end, we consider estimating the homophily degree between node pairs from attribute space and topology space, respectively, and then combine them with adjustable parameters.</p><p>On one hand, from the perspective of attribute space, we apply graph-agnostic multi-layer perceptron (MLP) to extract class-aware information from original node attributes. The l-th layer of MLP is defined as:</p><formula xml:id="formula_1">Z (l) m = σ(Z (l−1) m W (l) m )<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">Z (0) m = X, W<label>(l)</label></formula><p>m is the learnable weight matrix for MLP, and σ the activation function. Denote the output of the final layer of MLP as Z m , then we can obtain the soft assignment matrix B ∈ R n×C as follows :</p><formula xml:id="formula_3">B = softmax(Z m )<label>(2)</label></formula><p>Each element B ic denotes the probability that node v i belongs to class c. Let all parameters of MLP be Θ m , then the optimal Θ * m is obtained by minimizing the following loss of predicted labels by MLP:</p><formula xml:id="formula_4">Θ * m = argmin Θm L mlp = argmin Θm 1 |V L | va∈V L J( bmlp a , y a )<label>(3)</label></formula><p>where bmlp a is the predicted labels of v a by MLP. Since soft assignment matrix B is learned under the guidance of semi-supervised classification, it can capture class-aware information in attribute space. Then based on the matrix B we can calculate the extent that two nodes belong to the same class, called homophily degree matrix, which is defined as:</p><formula xml:id="formula_5">S = BB T<label>(4)</label></formula><p>where S ij = b i b T j denotes the extent to which node v i and node v j belong to the same class.</p><p>It is worth noting that the homophily degree matrix S is estimated based on original node attributes, which are not constrained by the heterophily of networks. Therefore, it also holds in networks with heterophily or low homophily.</p><p>On the other hand, from the perspective of topology space, network topology contains many useful information even if it exhibits strong heterophily. However, it is also difficult to estimate the homophily degree directly based on the topology, as we may not know the distribution of heterophily of network in advance. To address this problem, we further apply label propagation technique to estimate the homophily degree matrix in topology space.</p><p>The classic label propagation typically assumes that two connected nodes are more likely to have the same class, and thus propagates labels iteratively between neighborhoods. Let Y</p><formula xml:id="formula_6">(l) = [y (l) 1 , y (l) 2 , ..., y (l)</formula><p>n ] T ∈ R n×C denotes the soft label matrix in iteration l &gt; 0, where y (l) i represents predicted label distribution for node v i in iteration l. When l = 0, the initial label matrix Y (0) is initialized by the labels of training data, i.e., Y (0) = [y</p><formula xml:id="formula_7">(0) 1 , y (0) 2 , ..., y<label>(0)</label></formula><p>n ] T , which consists of one-hot label indicator vectors y (0) i for v i ∈ V L or zero vectors otherwise (i.e., unlabeled nodes). Then the label propagation in iteration l is defined as follows:</p><formula xml:id="formula_8">Y (l) = D −1 AY (l−1) , y (l) i = y (0) i , ∀v i ∈ V L (5)</formula><p>where D is the degree matrix with entries D ii = j A ij . In this equation, all nodes propagate labels to their direct neighborhoods first, and then the labels of all labeled nodes are reset to their initial labels.</p><p>However, classic label propagation technique aims to capture the assumption of homophily, which can not adapt to networks with heterophily directly. To capture the homophily degree between node pairs, we generalize classic label propagation with a learnable weight matrix, which is trained under the guidance of labeled data. The key intuition is that the influence between intra-class label is greater than that between inter-class label. Thus the learned weight matrix can be used to represent the extent to which two nodes belong to same class. Since networks exhibit different degrees of heterophily, we perform label propagation over korder structure of network to capture more homophilic nodes (e.g., k = 2). The k-order structure is defined as:</p><formula xml:id="formula_9">A k = A + A 2 + ... + A k (6)</formula><p>Then the generalized label propagation in iteration l is defined as:</p><formula xml:id="formula_10">Y (l) = D −1 k (A k T )Y (l−1)<label>(7)</label></formula><p>where D −1 k is the diagonal degree matrix for matrix A k T . In the equation above, all nodes propagate labels to their korder neighborhoods according the learnable edge weights. Then we can learn the optimal edge weights T * by minimizing the loss of predicted labels by generalized label propagation:</p><formula xml:id="formula_11">T * = argmin T L lp = argmin T 1 |V L | va∈V L J(ŷ lp a , y a ) (8)</formula><p>where J is the cross-entropy loss, ŷlp a and y a are the predicted label distribution of v a by generalized label propagation and the true one-hot label of v a , respectively. Note that we do not add to or remove edges from graphs, but only learn the weights of existing k-order neighborhoods. The optimal T * maximizes the probability that each node is correctly classified by generalized label propagation, thus also increases the intra-class influence. This reflects the extent to which two nodes belongs to the same class. In order to form an end-to-end pattern, here we take the weight matrix T as the homophily degree matrix estimated from topology space.</p><p>At last, to make the model more effective and robust, we combine the hompily degree matrix estimated from attribute space and topology space with adjustable parameters as follows:</p><p>H = αS + βT (9) where α and β are hyper-parameters. Also of note, matrix S contains homophily degree between any node pairs while T learns homophily degree within k-order neighborhoods. It does not influence the performance, as we will filter the entries that do not involve in propagation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Homophily-guided Propagation</head><p>The core of our approach is adaptively learning the propagation process for homophily and heterophily. The homophily degree matrix which represents the extent to which two nodes belong to the same class can reflect the underlying distribution of homophily or heterophily of networks. Therefore, we incorporate the learnable homophily degree matrix into graph convolution framework to automatically change the propagation weights between neighborhoods according to homophily degree. To be specific, during the propagation process, we aim to increase the feature influence between underlying intra-class nodes and reducing feature influence between underlying inter-class nodes according to the homophily degree matrix. To achieve this, we assign different weights to different neighborhoods according to the homophily degree, which can distinguish the homophily or heterophily between neighborhoods. Similarly, as we may not know the underlying distribution of heterophily in advance, we perform feature propagation over the k-order neighborhoods, to capture more homophilic nodes. In addition, we deal with ego-representation and neighborhoodrepresentation separately to preserve more personalized information. Then, the feature propagation process of the proposed method HOG-GCN in iteration l is given by:</p><formula xml:id="formula_12">Z (l) = σ(µZ (l−1) W (l) e + ξ D−1 A k HZ (l−1) W (l)</formula><p>n ) (10) where µ and ξ denote the weights of ego-representation and neighborhood-representation respectively, D is the diagonal degree matrix, Z (0) = X is the original node attributes. σ is the activation function.</p><p>Also of note, our proposed graph convolution framework can be taken as learning proper attention weights. The most significant difference between our approach and those feature-based attention methods is that, the attention weights are learned based on feature similarity alone, while our proposed method measures the edge weights according to the underlying homophily degree, which is more task-oriented. In addition, it is often believed that the 2-hop neighborhoods of a node v is always homophily-dominant in expectation <ref type="bibr" target="#b26">(Zhu et al. 2020)</ref>. So, we also set k = 2 in this work since it yields best performance in experiments and has relatively lower complexity meanwhile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Objective</head><p>The whole framework consists of two components: homophily degree matrix estimation and homophily-guided propagation. The first component contains MLP and the generalized label propagation. The objective functions of MLP and generalized label propagation are give by Eq. (3) and Eq. ( <ref type="formula">8</ref>). In the second component, we incorporate the homophily degree matrix into the graph convolution framework. Denote all parameters of graph convolution operation as Θ g , then we can get the optimal Θ * g according the final output Z of HOG-GCN: R = softmax(Z)</p><formula xml:id="formula_13">Θ * g = argminL gcn = argmin Θg 1 |V L | va∈V L J(r gcn a , y a ) (11)</formula><p>In this model, the homophily degree matrix is learned from attribute and topology information during the propagation process and further used to conduct feature propagation. In return, the propagation process can help learn better homophily degree matrix. That is, these two components are enhanced by each other. Therefore, we combine these objectives to train the whole process in an end-to-end fashion as:</p><formula xml:id="formula_14">Θ * g , Θ * m , T * = argmin Θg,Θm,T L gcn + λL mlp + γL lp (12)</formula><p>where λ, γ are balance hyper-parameters. In this way, the feature propagation process is guided by current homophily degree matrix, and the result of propagation can conduct the learning of homophily degree matrix through semisupervised classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Analysis</head><p>In this section we prove that our new approach can constrain the similarity of representations between nodes within k-order neighborhoods according to homophily degree between them. In other words, nodes with higher homophily degree have more similar representation (Theorem 1). Theorem 1. Denote the output of HOG-GCN as Z, then its propagation process can be taken as the minimization of the objective:</p><formula xml:id="formula_15">O = 1 2 vi∈V vj ∈N k (vi) H ij Z i − Z j 2 (13)</formula><p>where N k (v i ) denotes the k-order neighborhoods of node v i , Z i and Z j are representations for node v i and v j respectively.</p><p>Proof: We can rewrite the above equation as:</p><formula xml:id="formula_16">O = 1 2 vi∈V vj ∈N k (vi) H ij Z i − Z j 2 = tr(Z T ( D − A k H)Z) (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>where D is the degree matrix for A k H. By setting derivative of Eq. ( <ref type="formula" target="#formula_16">14</ref>) with respect to Z to zero (assume H is a symmetric matrix), we have:</p><formula xml:id="formula_18">∂O ∂Z =2( D − A k H)Z = 0 ⇓ Z = D−1 (A k H)Z<label>(15)</label></formula><p>Then, it can be explained as a limit distribution where</p><formula xml:id="formula_19">Z limit = D−1 (A k H)Z limit (16)</formula><p>We use the following iterative form to approximate the limit with l → ∞:</p><formula xml:id="formula_20">Z (l) = D−1 (A k H)Z (l−1)</formula><p>(17) When ignoring the non-linear transformation and initialize Z (0) = XW , we have:</p><formula xml:id="formula_21">Z (l) = D−1 (A k H)Z (l−1) =[ D−1 (A k H)] 2 Z (l−2) =[ D−1 (A k H)] l Z (0) =[ D−1 (A k H)] l XW (18)</formula><p>which corresponds to the propagation process of HOG-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We first give the experimental setup, then compare our model HOG-GCN with the state-of-the-art methods on transductive node classification and visualization. Last, we give the parameter analysis and homophily degree matrix analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Datasets. We evaluate the performance of the proposed HOG-GCN and existing methods on seven real-world datasets. To demonstrate that HOG-GCN can adaptively learn propagation mechanism for both homophily and heterophily, we use four heterophilic networks and three homophilic networks for evaluation. The heterophilic networks include three web page datasets, Cornell, Texas and Wisconsin <ref type="bibr" target="#b14">(Pei et al. 2020)</ref>, and a film industry dataset Film <ref type="bibr" target="#b17">(Tang et al. 2009</ref>). The homophilic networks include Cora, Citeseer and Pubmed which are all citation networks <ref type="bibr" target="#b15">(Sen et al. 2008;</ref><ref type="bibr" target="#b13">Namata et al. 2012)</ref>. The detailed statistics of these datasets is summarized in Table <ref type="table" target="#tab_0">1</ref>, where H.R. represents the homophily ratio of networks.</p><p>Baselines. We compare our proposed approach HOG-GCN with the following baselines: (1) MLP, which only uses attribute information;</p><p>(2) DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), which uses network topology information alone through random walks;</p><p>(3) traditional GNN models : GCN (Kipf and Welling 2017) and GAT <ref type="bibr" target="#b19">(Velickovic et al. 2018)</ref>, which work under the assumption of homophily; and (4) GNN models tackling heterophily: Geom-GCN <ref type="bibr" target="#b14">(Pei et al. 2020</ref>), H2GCN <ref type="bibr" target="#b26">(Zhu et al. 2020)</ref>, CPGNN <ref type="bibr" target="#b25">(Zhu et al. 2021)</ref>, GPR-GNN <ref type="bibr" target="#b2">(Chien et al. 2021</ref>) and AM-GCN <ref type="bibr" target="#b21">(Wang et al. 2020)</ref>. Particularly, as CPGNN has four different variants, we choose the best two for comparison.</p><p>Parameter Setup. For all datasets, we generate 10 random splits for training, validation and test. For each split we select 48% of nodes in each class to form the training set, 32% of nodes for the validation set and the remaining as the test set. For a fair comparison, all methods share the same 10 random splits. All the parameters of the baseline methods were set as what were used by their authors. In our approach HOG-GCN, for the homophily degree matrix estimation, we use a two-layer MLP with 512 units in the hidden layer. For the homophily-guided propagation, we use two-layer graph convolution operation with 256 units in the hidden layer. We set α to 1 and β to 0.1, and set both γ and µ to 1. We adopt Adam optimizer <ref type="bibr" target="#b10">(Kingma and Ba 2015)</ref> and the default initialization in pytorch.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Classification</head><p>The node classification results are reported in Table <ref type="table" target="#tab_1">2</ref>. We use mean accuracy as the evaluation metric along with the standard deviation of 10 splits. We have the following observations.</p><p>• Our approach HOG-GCN outperforms all the other methods compared on all of the four heterophilic networks, i.e., Texas, Cornell, Wisconsin and Film. It demonstrates the importance of incorporating and learning the homophily degree matrix into graph convolution framework for automatically changing the propagation process. To be specific, HOG-GCN significantly outperforms the traditional GNN models, i.e., GCN and GAT, by 26.49% and 24.47% on average, since they cannot generalize to the scenario of heterophily (and are even defeated by MLP which only uses attributes). Compared with methods that focus on tackling heterophily, such as H2GCN, Geom-GCN, CPGNN and GPR-GNN, our HOG-GCN also achieves an improvement between 3.9% and 19.03% in terms of mean accuracy. These results demonstrate the effectiveness of HOG-GCN in the scenario of heterophily. • On homophilic networks (i.e., Cora, Citeseer, Pubmed), the proposed HOG-GCN performs better or comparable to the baselines. To be specific, HOG-GCN performs best on Citeseer, and achieves the second best on Pubmed. Notably, HOG-GCN outperforms GCN and GAT which have an implicit assumption of strong homophily on these three homophilic networks by 1.52% and 0.98% on average. These results demonstrate that our method has the best performance in the scenario of heterophily while maintaining comparable or better performance in the sce-nario of homophily, and further validate the effectiveness and robustness of the proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>For intuitively illustrating that our approach can gain better results, we use t-SNE <ref type="bibr" target="#b7">(Hinton 2008)</ref>, which can project the learned node representations onto a two-dimensional space, to visualize the derived representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Analysis</head><p>We also investigate the sensitivity of parameters in HOG-GCN. We take Texas, Cornell, Cora, Citeseer as examples, where Texas and Cornell exhibit heterophily while Cora and Citeseer exhibit homophily.</p><p>Analysis of order k. In our proposed method, we conduct generalized label propagation and homophily-guided feature propagation over the network topology within k-order neighborhoods, since we may not know the distribution of heterophily of network. Here, we vary the order k from 1 to 6 and report the classification results. The results are shown in Fig. <ref type="figure">3</ref>. As shown, results on Texas and Cornell have a significant improvement when k = 2 than k = 1. While on Cora and Citeseer, the method only has a slight improvement when k = 2 than k = 1, and the accuracy drops as increasing k from 2 to 6. Based on these results, we conclude that on heterophilic networks where most directly connected nodes are from different classes, we often need a proper receptive field to capture enough intra-class information; while on homophilic networks where most connected nodes are from the same class, direct neighborhoods may be enough to provide intra-class information. For both homophilic and heterophilic networks, aggregating much information from higher-order neighborhoods will mix much noise and irrelated information that will negatively influence the prediction of nodes. Analysis of weights α and β. In our model, α and β represent the weight of homophily degree matrix estimated from attribute space and topology, respectively. Here we investigate the influence of α and β on node classification by varying them from 0 to 1. The results are shown in Fig. <ref type="figure" target="#fig_4">4</ref>. On Texas dataset, the performance is relatively poor when ignoring attribute information or topology information (i.e., α is 0 or β is 0). The method performs best when α is 0.4 and β is 0.6, demonstrating that combining attribute information and topology information can learn better homophily degree matrix and further obtain better performance. On Cora dataset, the performance is relatively stable, demonstrating that homophilic networks may not need fine-grained guidance during the propagation process. The results demonstrate the importance of combining node attributes and network topology in the estimation of homophily degree matrix, and further validate the effectiveness and robustness of  Homophily Degree Matrix Analysis</p><p>In the proposed graph convolution framework, we incorporate a learnable homophily degree matrix, which can reflect the homophily or heterophily of networks, to adaptively change the propagation process. Therefore, in order to investigate whether the homophily degree matrix learned by our proposed model are meaningful, here we analyze the homophily degree distribution on homophilic edges (i.e., two nodes with same class) and heterophilic edges (i.e., two nodes with different classes) within 2-order neighborhoods, respectively. Fig. <ref type="figure" target="#fig_6">5</ref> shows the analysis results on a heterophilic network Texas and a homophilc network Cora.</p><p>As shown, for both homophilc and heterophilic networks, our approach can adaptively learn higher homophily degree between node with the same class, and lower homophily degree between nodes with different classes. During the feature propagation process, the homophilic edges are given greater weights while the heterophilic edges are given smaller weights according to homophily degree matrix. The results demonstrate that our approach can adaptively learn different propagation process for both homophily and heterophily, enabling it to go beyond the strong assumption of homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel homophily-guided graph convolution network which can be universally suitable for both homophilic and heterophilic networks. In specific, we incorporate a learnable homophily degree matrix into graph convolution framework for modeling the homophily and heterophily of networks and further adaptively changing the propagation process according to homophily degree between node pairs. The homophily degree matrix is learned from attribute and topology information via extracting classaware information, which can conduct the propagation process. In return, the result of propagation process can further help learn homophily degree matrix through downstream semi-supervised tasks. These two process can be enhanced by each other and trained jointly. Experiments on seven realworld datasets demonstrate that the proposed new approach outperforms existing methods under heterophily, while gains competitive performance under homophily.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of HOG-GCN, which consists of two components, including (a) homophily degree matrix estimation and (b) homophily-guided propagation.</figDesc><graphic url="image-1.png" coords="3,54.00,54.00,504.00,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization results on Citeseer dataset</figDesc><graphic url="image-5.png" coords="6,352.83,226.55,78.70,59.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Analysis results of order k</figDesc><graphic url="image-7.png" coords="6,337.23,379.62,97.78,73.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig.2shows the visualization results of GCN, H2GCN, CPGNN, GPR-GNN and our proposed HOG-GCN on Citeseer dataset as an illustrative example, where nodes with same color have same class label. As shown, the visualization results of GCN and CPGNN are less satisfactory in this case, since points with same color are dispersed and some points with different color are mixed with each other. The results of GPR-GNN and H2GCN are relatively better but the borders between different classes are still not so clear. Obviously, the visualization of our HOG-GCN performs much better, where the representations have the highest intra-class similarity and form more discernible clusters. The results of visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Analysis results of weight α and β</figDesc><graphic url="image-9.png" coords="7,62.19,56.03,107.32,82.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Analysis results of homophily degree distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets Texa. Wisc. Corn. Film Cora Cite. Pubm. Statistics of datasets 05±4.36 50.39±7.55 53.78±8.59 28.78±1.48 86.48±1.43 73.58±1.37 87.34±0.65 62.06 8.71 MLP 81.08±4.83 85.49±3.53 83.24±7.03 36.58±1.44 71.29±1.60 66.96±2.61 86.48±0.63 73.02 5.86 GAT 57.30±3.38 54.31±5.62 54.59±7.33 28.99±1.44 87.16±1.17 75.64±1.95 85.25±0.60 63.32 7.29 DeepWalk 49.19±3.38 53.51±5.10 44.12±9.12 23.74±0.56 80.08±1.84 53.59±2.63 81.14±0.54 55.05 10.71 H2GCN 79.73±7.27 82.55±4.33 78.38±4.35 36.71±1.41 86.48±1.63 75.56±2.18 88.77±0.65 75.45 4.14 CPGNN-MLP 79.73±6.54 84.53±6.48 73.51±6.02 36.73±1.03 84.57±1.59 72.10±2.70 87.67±0.72 74.12 5.43 CPGNN-Cheby 74.32±7.38 81.76±6.74 63.51±5.83 35.51±1.85 87.18±1.13 75.52±1.84 89.08±0.67 72.41 4.71 GPR-GNN 84.59±4.37 83.92±3.14 82.97±5.68 36.47±1.38 86.70±1.03 75.12±1.98 87.38±0.63 76.74 4.29 AM-GCN 78.38±7.25 81.76±4.96 78.38±4.98 33.60±1.17 86.66±1.36 76.01±1.90 86.78±0.60 74.51 5.43 Geom-GCN 66.22±6.65 62.55±5.22 55.68±8.04 32.39±1.49 84.91±1.12 73.16±1.92 88.41±0.63 66.19 7.43 HOG-GCN 85.17±4.40 86.67±3.36 84.32±4.32 36.82±0.84 87.04±1.10 76.15±1.88 88.79±0.40 77.85 1.43</figDesc><table><row><cell>Nodes</cell><cell>183</cell><cell>251</cell><cell>183</cell><cell>7600</cell><cell cols="3">2708 3327 19717</cell></row><row><cell>Edges</cell><cell>309</cell><cell>499</cell><cell>295</cell><cell cols="4">33544 5429 4732 44338</cell></row><row><cell>Features</cell><cell>1703</cell><cell>1703</cell><cell>1703</cell><cell>931</cell><cell cols="2">1433 3703</cell><cell>500</cell></row><row><cell>Classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>H.R.</cell><cell>0.09</cell><cell>0.19</cell><cell>0.30</cell><cell>0.22</cell><cell>0.81</cell><cell>0.74</cell><cell>0.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification results with mean value and standard deviation. The best result is bold and the second best is underlined.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">The Thirty-Sixth AAAI Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Natural Science Foundation of China (61876128, 61772361) and the George Washington University Facilitating Fund (UFF) FY21.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dual quaternion knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 9th International Conference on Learning Representations</title>
				<meeting>eeding of the 9th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal link prediction by integrating content and structure information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1169" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Computer Vision</title>
				<meeting>the 2019 International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial representation mechanism learning for network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3103193</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2021.3103193" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Vigiliae Christianae</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network via attribute completion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
				<meeting>the Web Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of community detection approaches: From statistical modeling to deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3104155</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2021.3104155" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A Method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 5th International Conference on Learning Representations</title>
				<meeting>eeding of the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks for political perspective detection in news media</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
				<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs (MLG)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Perozzi, B</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2020. 2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>Proceeding of the 8th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Node classification in signed social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
				<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 28th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 6th International Conference on Learning Representations</title>
				<meeting>eeding of the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ECOQUG: An effective ensemble community scoring function</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Data Engineering</title>
				<meeting>the 35th International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1702" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive multi-channel graph convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causality based propagation history ranking in social networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
				<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3917" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
	</analytic>
	<monogr>
		<title level="m">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
				<imprint>
			<date type="published" when="2019">2019. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 7th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GroupINN: Grouping-based interpretable neural network for classification of limited, noisy brain data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Solarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2019. 2014</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
	<note>Proceedings of the 7th ACM International Conference on Web Search and Data Mining</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
