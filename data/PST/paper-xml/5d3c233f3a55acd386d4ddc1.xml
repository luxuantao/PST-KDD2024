<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DYREP: LEARNING REPRESENTATIONS OVER DYNAMIC GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
							<email>rstrivedi@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
							<email>farajtabar@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prasenjeet</forename><surname>Biswal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
							<email>zha@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DYREP: LEARNING REPRESENTATIONS OVER DYNAMIC GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation Learning over graph structured data has received significant attention recently due to its ubiquitous applicability. However, most advancements have been made in static graph settings while efforts for jointly learning dynamic of the graph and dynamic on the graph are still in an infant stage. Two fundamental questions arise in learning over dynamic graphs: (i) How to elegantly model dynamical processes over graphs? (ii) How to leverage such a model to effectively encode evolving graph information into low-dimensional representations? We present DyRep -a novel modeling framework for dynamic graphs that posits representation learning as a latent mediation process bridging two observed processes namely -dynamics of the network (realized as topological evolution) and dynamics on the network (realized as activities between nodes). Concretely, we propose a two-time scale deep temporal point process model that captures the interleaved dynamics of the observed processes. This model is further parameterized by a temporal-attentive representation network that encodes temporally evolving structural information into node representations which in turn drives the nonlinear evolution of the observed graph dynamics. Our unified framework is trained using an efficient unsupervised procedure and has capability to generalize over unseen nodes. We demonstrate that DyRep outperforms state-of-the-art baselines for dynamic link prediction and time prediction tasks and present extensive qualitative insights into our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Representation learning over graph structured data has emerged as a keystone machine learning task due to its ubiquitous applicability in variety of domains such as social networks, bioinformatics, natural language processing, and relational knowledge bases. Learning node representations to effectively encode high-dimensional and non-Euclidean graph information is a challenging problem but recent advances in deep learning has helped important progress towards addressing it <ref type="bibr" target="#b4">(Cao et al., 2015;</ref><ref type="bibr" target="#b15">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b40">Tang et al., 2015;</ref><ref type="bibr" target="#b44">Wang et al., 2016a;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b48">Xu et al., 2017)</ref>, with majority of the approaches focusing on advancing the state-of-the-art in static graph setting. However, several domains now present highly dynamic data that exhibit complex temporal properties in addition to earlier cited challenges. For instance, social network communications, financial transaction graphs or longitudinal citation data contain fine-grained temporal information on nodes and edges that characterize the dynamic evolution of a graph and its properties over time.</p><p>These recent developments have created a conspicuous need for principled approaches to advance graph embedding techniques for dynamic graphs <ref type="bibr" target="#b17">(Hamilton et al., 2017b)</ref>. We focus on two pertinent questions fundamental to representation learning over dynamic graphs: (i) What can serve as an elegant model for dynamic processes over graphs? -A key modeling choice in existing representation learning techniques for dynamic graphs <ref type="bibr" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b54">Zhou et al., 2018;</ref><ref type="bibr" target="#b42">Trivedi et al., 2017;</ref><ref type="bibr" target="#b31">Ngyuyen et al., 2018;</ref><ref type="bibr" target="#b50">Yu et al., 2018)</ref> assume that graph dynamics evolve as a single time scale process. In contrast to these approaches, we observe that most real-world graphs exhibit at least two distinct dynamic processes that evolve at different time scales -Topological Evolution: where the number of nodes and edges are expected to grow (or shrink) over time leading to structural changes in the graph; and Node Interactions: which relates to activities between nodes that may or may not be structurally connected. Modeling interleaved dependencies between these non-linearly evolving dynamic processes is a crucial next step for advancing the formal models of dynamic graphs. (c) Communication Events (k=1) where nodes interact with each other. For both these processes, t p,k=0 &lt; (t 1 , t 2 , t 3 , t 4 , t 5 ) k=1 &lt; t q,k=0 &lt; (t 6 , t 7 ) k=1 &lt; t r,k=0 . (b) Evolving Representations.</p><p>(ii) How can one leverage such a model to learn dynamic node representations that are effectively able to capture evolving graph information over time? -Existing techniques in this direction can be divided into two approaches: a.) Discrete-Time Approach, where the evolution of a dynamic graph is observed as collection of static graph snapshots over time <ref type="bibr" target="#b55">(Zhu et al., 2016;</ref><ref type="bibr" target="#b14">Goyal et al., 2017;</ref><ref type="bibr" target="#b54">Zhou et al., 2018)</ref>. These approaches tend to preserve (encode) very limited structural information and capture temporal information at a very coarse level which leads to loss of information between snapshots and lack of ability to capture fine-grained temporal dynamics. Another challenge in such approaches is the selection of appropriate aggregation granularity which is often misspecified. b.) Continuous-Time Approach, where evolution is modeled at finer time granularity in order to address the above challenges. While existing approaches have demonstrated to be very effective in specific settings, they either model simple structural and complex temporal properties in a decoupled fashion <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> or use simple temporal models (exponential family in <ref type="bibr" target="#b31">(Ngyuyen et al., 2018)</ref>). But several domains exhibit highly nonlinear evolution of structural properties coupled with complex temporal dynamics and it remains an open problem to effectively model and learn informative representations capturing various dynamical properties of such complex systems.</p><p>As noted in <ref type="bibr" target="#b5">(Chazelle, 2012)</ref>, an important requirement to effectively learn over such dynamical systems is the ability to express the dynamical processes at different scales. We propose that any dynamic graph must be minimally expressed as a result of two fundamental processes evolving at different time scales: Association Process (dynamics of the network), that brings change in the graph structure and leads to long lasting information exchange between nodes; and Communication Process (dynamics on the network), that relates to activities between (not necessarily connected) nodes which leads to temporary information flow between them (Farine, 2017; <ref type="bibr" target="#b1">Artime et al., 2017)</ref>. We, then, posit our goal of learning node representations as modeling a latent mediation process that bridges the above two observed processes such that learned representations drive the complex temporal dynamics of both processes and these processes subsequently lead to the nonlinear evolution of node representations. Further, the information propagated across the graph is governed by the temporal dynamics of communication and association histories of nodes with its neighborhood. For instance, in a social network, when a node's neighborhood grows, it changes that node's representation which in turn affects her social interactions (association → embedding → communication). Similarly, when node's interaction behavior changes, it affects the representation of her neighbors and herself which in turn changes the structure and strength of her connections due to link addition or deletion (communication → embedding → association). We call this phenomenon -evolution through mediation and illustrate it graphically in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>In this work, we propose a novel representation learning framework for dynamic graphs, DyRep, to model interleaved evolution of two observed processes through latent mediation process expressed above and effectively learn richer node representations over time. Our framework ingests dynamic graph information in the form of association and communication events over time and updates the node representations as they appear in these events. We build a two-time scale deep temporal point process approach to capture the continuous-time fine-grained temporal dynamics of the two observed processes. We further parameterize the conditional intensity function of the temporal point process with a deep inductive representation network that learns functions to compute node representations. Finally, we couple the structural and temporal components of our framework by designing a novel Temporal Attention Mechanism, which induces temporal attentiveness over neighborhood nodes using the learned intensity function. This allows to capture highly interleaved and nonlinear dynamics governing node representations over time. We design an efficient unsupervised training procedure for end-to-end training of our framework. We demonstrate consistent and significant improvement over state-of-the-art representative baselines on two real-world dynamic graphs for the tasks of dynamic link prediction and time prediction. We further present an extensive qualitative analysis through embedding visualization and ablation studies to discern the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND PRELIMINARIES</head><p>2.1 RELATED WORK Representation Learning approaches for static graphs either perform node embedding <ref type="bibr" target="#b4">(Cao et al., 2015;</ref><ref type="bibr" target="#b15">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b40">Tang et al., 2015;</ref><ref type="bibr" target="#b44">Wang et al., 2016a;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b48">Xu et al., 2017)</ref> or sub-graph embedding <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016)</ref> which can also utilize convolutional neural networks <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b5">2016;</ref><ref type="bibr" target="#b3">Bruna et al., 2014)</ref>. Among them, GraphSage <ref type="bibr" target="#b16">(Hamilton et al., 2017a</ref>) is an inductive method for learning functions to compute node representations that can be generalized to unseen nodes. Most of these approaches only work with static graphs or can model evolving graphs without temporal information. Dynamic network embedding is pursued through various techniques such as matrix factorization <ref type="bibr" target="#b55">(Zhu et al., 2016)</ref>, structural properties <ref type="bibr" target="#b54">(Zhou et al., 2018)</ref>, CNN-based approaches <ref type="bibr" target="#b38">(Seo et al., 2016)</ref>, deep recurrent models <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>, and random walks <ref type="bibr" target="#b31">(Ngyuyen et al., 2018)</ref>. There exists a rich body of literature on temporal modeling of dynamic networks <ref type="bibr" target="#b22">(Kim et al., 2017)</ref>, that focus on link prediction tasks but their goal is orthogonal to our work as they build task specific methods and do not focus on representation learning. Authors in <ref type="bibr" target="#b49">(Yang et al., 2017;</ref><ref type="bibr" target="#b36">Sarkar et al., 2007)</ref> proposed models of learning dynamic embeddings but none of them consider time at finer level and do not capture both topological evolution and interactions simultaneously. In parallel, research on deep point process models include parametric approaches to learn intensity <ref type="bibr" target="#b8">(Du et al., 2016;</ref><ref type="bibr" target="#b29">Mei &amp; Eisner, 2017)</ref> using recurrent neural networks and GAN based approaches to learn intensity functions <ref type="bibr" target="#b45">(Xiao et al., 2017)</ref>. More detailed related works are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TEMPORAL POINT PROCESSES</head><p>Stochastic point processes <ref type="bibr" target="#b6">(Daley &amp; Vere-Jones, 2007)</ref> are random processes whose realization comprises of discrete events in time, t 1 , t 2 , . . .. A temporal point process is one such stochastic process that can be equivalently represented as a counting process, N (t), which contains the number of events up to time t. The common way to characterize temporal point processes is via the conditional intensity function λ(t), a stochastic model of rate of happening events given the previous events. Formally, λ(t)dt is the conditional probability of observing an event in the tiny window</p><formula xml:id="formula_0">[t, t + dt), λ(t)dt := P[event in [t, t + dt)|T (t)] = E[dN (t)|T (t)], where T (t) = t k |t k &lt; t is history until t.</formula><p>Similarly, for t &gt; t n and given history T = t 1 , . . . , t n , we characterize the conditional probability that no event happens during [t n , t) as S(t|T ) = exp − t tn λ(τ ) dτ , which is called survival function of the process <ref type="bibr" target="#b0">(Aalen et al., 2008)</ref>. Moreover, the conditional density that an event occurs at time t is defined as f (t) = λ(t) S(t). The intensity λ(t) is often designed to capture phenomena of interests -common forms include Poisson Process, Hawkes processes <ref type="bibr" target="#b10">(Farajtabar et al., 2014;</ref><ref type="bibr" target="#b18">Hawkes, 1971;</ref><ref type="bibr" target="#b46">Wang et al., 2016b;</ref><ref type="bibr" target="#b39">Tabibian et al., 2017)</ref>, Self-Correcting Process <ref type="bibr" target="#b19">(Isham &amp; Westcott, 1979)</ref>. Temporal Point Processes have previously been used to model both -dynamics on the network <ref type="bibr" target="#b12">(Farajtabar et al., 2016;</ref><ref type="bibr" target="#b52">Zarezade et al., 2017;</ref><ref type="bibr" target="#b13">Farajtabar et al., 2017)</ref> and dynamics of the network <ref type="bibr" target="#b41">(Tran et al., 2015;</ref><ref type="bibr" target="#b11">Farajtabar et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NOTATIONS AND DYNAMIC GRAPH SETTING</head><p>Notations. Let G t = (V t , E t ) denote graph G at time t, where V t is the set of nodes and E t is the set of edges in G t and the edges are undirected. Event Observation -Both communication and association processes are realized in the form of dyadic events observed between nodes on graph G over a temporal window [t 0 , T ] and ordered by time. We use the following canonical tuple representation for any type of event at time t of the form e = (u, v, t, k), where u, v are the two nodes involved in an event. t represents time of the event. k ∈ {0, 1} and we use k = 0 to signify events from the topological evolution process (association) and k = 1 to signify events from node interaction process (communication). Persistent edges in the graph only appear through topological events while interaction events do not contribute them. Hence, k represents an abstraction of scale (evolution rate) associated with processes that generate topological (dynamic of the network) and interaction events (dynamic on the network) respectively. We then represent complete set of P observed events ordered by time in window [0, T ] as O = {(u, v, t, k) p } P p=1 . Here, t p ∈ R + , 0 ≤ t p ≤ T . Appendix B discusses a marked point process view of such an event set. Node Representation-Let z v ∈ R d represent d-dimensional representation of node v. As the representation evolve over time, we qualify them as function of time: z v (t) -the representation of node v being updated after an event involving v at time t. We use z v ( t) for most recently updated embedding of node v just before t.</p><p>Dynamic Graph Setting. Let G t0 = (V t0 , E t0 ) be the initial snapshot of a graph at time t 0 . Please note that G t0 may be empty or it may contain an initial structure (association edges) but it will not have any communication history. Our framework observes evolution of graph as a stream of events O and hence any new node will always be observed as a part of such an event. This will induce a natural ordering over nodes as available from the data. As our method is inductive, we never learn node-specific representations and rather learn functions to compute node representations. In this work, we only support growth of network i.e. we only model addition of nodes and structural edges and leave deletion as future work. Further, for general description of the model, we will assume that an edge in the graph do not have types and nodes do not have attributes but we discuss the details on how to use our model to accommodate these features in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD: DYREP</head><p>The key idea of DyRep is to build a unified architecture that can ingest evolving information over graphs and effectively model the evolution through mediation phenomenon described in Section 1.</p><p>To achieve this, we design a two-time scale temporal point process model of observed processes and parameterize it with an inductive representation network which subsequently models the latent mediation process of learning node representations. The rationale behind our framework is that the observed set of events are the realizations of the nonlinear dynamic processes governing the changes in topological structure of graph and interactions between the nodes in the graph. Now, when an event is observed between two nodes, information flows from the neighborhood of one node to the other and affects the representations of the nodes accordingly. While a communication event (interaction) only propagates local information across two nodes, an association event changes the topology and thereby has more global effect. The goal is to learn node representations that encode information evolving due to such local and global effects and further drive the dynamics of the observed events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODELING TWO-TIME SCALE OBSERVED GRAPH DYNAMICS</head><p>The observations over dynamic graph contain temporal point patterns of two interleaved complex processes in the form of communication and association events respectively. At any time t, the occurrence of an event, from either of these processes, is dependent on the most recent state of the graph, i.e., two nodes will participate in any event based on their most current representations. Given an observed event p = (u, v, t, k), we define a continuous-time deep model of temporal point process using the conditional intensity function λ u,v k (t) that models the occurrence of event p between nodes u and v at time t:</p><formula xml:id="formula_1">λ u,v k (t) = f k (g u,v k ( t))</formula><p>(1) where t signifies the timepoint just before current event. The inner function g k ( t) computes the compatibility of the most recently updated representations of two nodes, z u ( t) and z v ( t) as follows:</p><formula xml:id="formula_2">g u,v k ( t) = ω T k • [z u ( t); z v ( t)]<label>(2)</label></formula><p>[;] signifies concatenation and ω k ∈ R 2d serves as the model parameter that learns time-scale specific compatibility. g k ( t) is a function of node representations learned through a representation network described in Section 3.2. This network parameterizes the intensity function of the point process model which serves as a unifying factor. Note that the dynamics are not two simple point processes dependent on each other, but, they are related through the mediation process and in the embedding space. Further, a well curated attention mechanism is employed to learn how the past drives future.</p><p>The choice of outer function f k needs to account for two critical criteria: 1) Intensity needs to be positive. 2) As mentioned before, the dynamics corresponding to communication and association processes evolve at different time scales. To account for this, we use a modified version of softplus function parameterized by a dynamics parameter ψ k to capture this timescale dependence:</p><formula xml:id="formula_3">f k (x) = ψ k log(1 + exp(x/ψ k ))<label>(3)</label></formula><p>where, x = g( t) in our case and ψ k (&gt; 0) is scalar time-scale parameter learned as part of training. ψ k corresponds to the rate of events arising from a corresponding process. In 1D event sequences, the formulation in (3) corresponds to the nonlinear transfer function in <ref type="bibr" target="#b29">(Mei &amp; Eisner, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LEARNING LATENT MEDIATION PROCESS VIA TEMPORALLY ATTENTIVE REPRESENTATION NETWORK</head><p>We build a deep recurrent architecture that parameterizes the intensity function in Eq. ( <ref type="formula">1</ref>) and learns functions to compute node representations. Specifically, after an event has occurred, the representation of both the participating nodes need to be updated to capture the effect of the observed event based on the principles of:</p><p>Self-Propagation. Self-propagation can be considered as a minimal component of the dynamics governing an individual node's evolution. A node evolves in the embedded space with respect to its previous position (e.g. set of features) and not in a random fashion.</p><p>Exogenous Drive. Some exogenous force may smoothly update the node's current features during the time interval (e.g. between two global events involving that node).</p><p>Localized Embedding Propagation. Two nodes involved in an event form a temporary (communication) or a permanent (association) pathway for the information to propagate from the neighborhood of one node to the other node. This corresponds to the influence of the nodes at second-order proximity passing through the other node participating in the event (See Appendix A for pictorial depiction).</p><p>To realize the above processes in our setting, we first describe an example setup: Consider nodes u and v participating in any type of event at time t. Let N u and N v denote the neighborhood of nodes u and v respectively. We discuss two key points here: 1) Node u serves as a bridge passing information from N u to node v and hence v receives the information in an aggregated form through u. 2) While each neighbor of u passes its information to v, the information that node u relays is governed by an aggregate function parametrized by u's communication and association history with its neighbors.</p><p>With this setup, for any event at time t, we update the embeddings for both nodes involved in the event using a recurrent architecture. Specifically, for p-th event of node v, we evolve z v as:</p><formula xml:id="formula_4">z v (t p ) = σ( W struct h u struct ( tp ) Localized Embedding Propagation + W rec z v ( tv p ) Self-Propagation + W t (t p − tv p ) Exogenous Drive ),<label>(4)</label></formula><p>where, h u struct ∈ R d is the output representation vectors obtained from aggregator function on node u's neighborhood and z v ( tv p ) is the recurrent state obtained from the previous representation of node v. t p is time point of current event, tp signifies the timepoint just before current event and tv p represent time point of previous event for node v. z v ( tv p = 0), the initial representation of a node v may be initialized either using input node features from dataset or random vector as per the setting. Eq. 4 is a neural network based functional form parameterized by W struct , W rec ∈ R d×d and W t ∈ R d that govern the aggregate effect of all the three inputs (graph structure, previous embedding and exogenous feature) respectively to compute representations. The above formulation is inductive (supports unseen nodes) and flexible (supports node and edge types) as discussed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">TEMPORALLY ATTENTIVE AGGREGATION</head><p>The Localized Embedding Propagation principle above captures rich structural properties based on neighborhood structure which is a key to any representation learning task over graphs. However, for a given node, not all of its neighbors are uniformly important and hence it becomes extremely important to capture information from each neighbor in some weighted fashion. Recently proposed attention mechanisms have shown great success in dealing with variable sized inputs, focusing on the most relevant parts of the input to make decisions. However, existing approaches consider attention as a static quantity. In dynamic graphs, changing neighborhood structure and interaction activities between nodes evolves importance of each neighbor to a node over time, thereby making attention itself a temporally evolving quantity. Further this quantity is dependent on the temporal history of association and communication of neighboring nodes through evolving representations. To this Algorithm 1 Update Algorithm for S and A <ref type="formula">1</ref>), most recently updated A( t) and S( t). Output: A(t) and S(t)</p><formula xml:id="formula_5">Input: Event record o = (u, v, t, k), Event Intensity λ u,v k (t) computed in (</formula><formula xml:id="formula_6">1. Update A : A(t) = A( t) if k = 0 then A uv (t) = A vu (t) = 1 ←{Association event} 2. Update S : S(t) = S( t) if k = 1 and A uv (t) = 0 return S(t), A(t) ←{Communication event, no Association exists} for j ∈ {u, v} do b = 1 |Nj (t)| where |N j (t)| is the size of N j (t) = {i : A ij (t) = 1} y ← S j (t) if k = 1 and A uv (t) = 1 then { ←{Communication event, Association exists}} y i = b + λ ji k (t)</formula><p>where i is the other node involved in the event.</p><p>←{λ computed in Eq. 2}</p><formula xml:id="formula_7">else if k = 0 and A uv (t) = 0 then { ←{Association event}} b = 1 |Nj ( t)| where |N j ( t)| is the size of N j ( t) = {i : A ij ( t) = 1} x = b − b y i = b + λ ji k (t)</formula><p>where i is the other node involved in the event ←{λ computed in Eq. 2} y w = y w − x; ∀w = i, y w = 0 end if Normalize y and set S j (t) ← y end for return S(t), A(t) end, we propose a novel Temporal Point Process based Attention Mechanism that uses temporal information to compute the attention coefficient for a structural edge between nodes. These coefficient are then used to compute the aggregate quantity (h struct ) required for embedding propagation.</p><p>Let A(t) ∈ R n×n be the adjacency matrix for graph G t at time t. Let S(t) ∈ R n×n be a stochastic matrix capturing the strength between pair of vertices at time t. One can consider S as a selection matrix that induces a natural selection process for a node -it would tend to communicate more with other nodes that it wants to associate with or has recently associated with. And it would want to attend less to non-interesting nodes. We start with following implication required for the construction of h u struct in (4): For any two nodes u and v at time t, S uv (t)</p><formula xml:id="formula_8">∈ [0, 1] if A uv (t) = 1 and S uv (t) = 0 if A uv (t) = 0. Denote N u (t) = {i : A iu (t) = 1} as the 1-hop neighborhood of node u at time t.</formula><p>To formally capture the difference in the influence of different neighbors, we propose a novel conditional intensity based attention layer that uses the matrix S to induce a shared attention mechanism to compute attention coefficients over neighborhood. Specifically, we perform localized attention for a given node u and compute the coefficients pertaining to the 1-hop neighbors i of node u as:</p><formula xml:id="formula_9">q ui (t) = exp(Sui( t)) i ∈Nu(t) exp(S ui ( t))</formula><p>, where q ui signifies the attention weight for the neighbor i at time t and hence it is a temporally evolving quantity. These attention coefficients are then used to compute the aggregate information h u struct ( t) for node u by employing an attended aggregation mechanism across neighbors as follows:</p><formula xml:id="formula_10">h u struct ( t) = max σ q ui (t) • h i ( t) , ∀i ∈ N u ( t) , where, h i ( t) = W h z i ( t) + b h and W h ∈ R d×d and b h ∈ R d are</formula><p>parameters governing the information propagated by each neighbor of u. z i ( t) ∈ R d is the most recent embedding for node i. The use of max operator is inspired from learning on general point sets <ref type="bibr" target="#b34">(Qi et al., 2017)</ref>. By applying max-pooling operator element-wise, the model effectively captures different aspects of the neighborhood. We found max to work slightly better as it considers temporal aspect of neighborhood which would be amortized if mean is used instead.</p><p>Connection to Neural Attention over Graphs. Our proposed temporal attention layer shares the motivation of recently proposed Graph Attention Networks (GAT) <ref type="bibr" target="#b43">(Veličković et al., 2018)</ref> and Gated Attention Networks (GaAN) <ref type="bibr" target="#b53">(Zhang et al., 2018)</ref> in the spirit of applying non-uniform attention over neighborhood. Both GAT and GaAN have demonstrated significant success in static graph setting. GAT advances GraphSage <ref type="bibr" target="#b16">(Hamilton et al., 2017a)</ref> by employing multi-head non-uniform attention over neighborhood and GaAN advances GAT by applying different weights to different heads in the multi-head attention formulation. The key innovation in our model is the parameterization of attention mechanism by a point process based temporal quantity S that is evolving and drives the impact that each neighbor has on the given node. Further, unlike static methods, we use these attention coefficients as input to the aggregator function for computing the temporal-structural effect of neighborhood. Finally, static methods use multi-head attention to stabilize learning by capturing multiple representation spaces but this is an inherent property in our layer as representations and event intensities update over time and hence new events help capture multiple representation spaces.</p><p>Construction and Update of S. We construct a single stochastic matrix S (used to parameterize attention in the earlier section) to capture complex temporal information. At the initial timepoint t = t 0 , we construct S(t 0 ) directly from A(t 0 ). Specifically, for a given node v, we initialize the elements of corresponding row vector S v (t 0 ) as:</p><formula xml:id="formula_11">S vu (t 0 ) = 0 if (v = u or A vu (t 0 ) = 0) and S vu (t 0 ) = 1 |Nv(t0)| if N v (t 0 ) = {u : A uv (t 0 ) = 1}.</formula><p>After observing an event o = (u, v, t, k) at time t &gt; t 0 , we make updates to A and S as per the observation of k. Specifically, A only gets updated for association events (k=0, change in structure). Note that S is parameter for a structural temporal attention which means temporal attention is only applied on structural neighborhood of a node. Hence, the values of S are only updated/active in two scenarios: a) the current event is an interaction between nodes which already has structural edge (A uv (t) = 1 and k = 1) and b) the current event is an association event (k = 0). Given a neighborhood of node u, b represents background (base) attention for each edge which is uniform attention based on neighborhood size. Whenever an event involving u occurs, this attention changes in following ways: For case (a), the attention value for corresponding S entries are updated using the intensity of the event. For case (b), repeat same as (a) but also adjust the background attention (by b − b , b and b being the new and old background attention respectively) for edge with other neighbors as the neighborhood size grows in this case. From mathematical viewpoint, this update resembles a standard temporal point process formulation where the term coming from b serves as background attention while λ can be viewed as endogenous intensity based attention. Algorithm 1 outlines complete update scenarios. In the directed graph case, updates to A will not be symmetric, which will subsequently affect the neighborhood structure and attention flow for a node. Appendix A provides a pictorial depiction of the complete DyRep framework discussed in this section. We provide an extensive ablation study in Appendix C that can help discern the contribution of all the above components in achieving our goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EFFICIENT LEARNING PROCEDURE</head><p>The complete parameter space for the current model is k (τ ) represent total survival probability for events that do not happen. While it is intractable (will require O(n 2 k) time) and unnecessary to compute the integral in the log-likelihood equation for all possible non-events in a stochastic setting, we can locally optimize L using mini-batch stochastic gradient descent where we estimate the integral using novel sampling technique. Algorithm 2 in Appendix H adopts a simple variant of Monte Carlo trick to compute the survival term of log-likelihood equation. Specifically, in each mini-batch, we sample non-events instead of considering all pairs of non-events (which can be millions). Let m be the mini-batch size and N be the number of samples. The complexity of Algorithm 2 will then be O(2mkN ) for the batch where the factor of 2 accounts for the update happening for two nodes per event which demonstrates linear scalability in number of events which is desired to tackle web-scale dynamic networks <ref type="bibr" target="#b32">(Paranjape et al., 2017)</ref>. The overall training procedure is adopted from <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> where the Backpropagation Through Time (BPTT) training is conducted over a global sequence, thereby maintaining the dependencies between events across sequences while avoiding gradient related issues. Implementation details are left to Appendix G. </p><formula xml:id="formula_12">Ω = {W struct , W rec , W t , W h , b h , {ω k } k=0,1 , {ψ k } k=0,1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TASKS AND METRICS</head><p>We study the effectiveness of DyRep by evaluating our model on tasks of dynamic link prediction and event time prediction tasks:</p><p>Dynamic Link Prediction. When any two nodes in a graph has increased rate of interaction events, they are more likely to get involved in further interactions and eventually these interactions may lead to the formation of structural link between them. Similarly, formation of the structural link may lead to increased likelihood of interactions between newly connected nodes. To understand, how well our model captures these phenomenon, we ask questions like: Which is the most likely node u that would undergo an event with a given node v governed by dynamics k at time t? The conditional density of such and event at time t can be computed:</p><formula xml:id="formula_13">f u,v k (t) = λ u,v k (t) • exp t t λ(s)</formula><p>ds , where t is the time of the most recent event on either dimension u or v. We use this conditional density to find most likely node.</p><p>For a given test record (u, v, t, k), we replace v with other entities in the graph and compute the density as above. We then rank all the entities in descending order of the density and report the rank of the ground truth entity. Please note that the latest embeddings of the nodes update even during the test while the parameters of the model remaining fixed. Hence, when ranking the entities, we remove any entities that creates a pair already seen in the test. We report Mean Average Rank (MAR) and HITS(@10) metric for dynamic link prediction.</p><p>Event Time Prediction. This is a relatively novel application where the aim is to compute the next time point when a particular type of event (structural or interaction) can occur. Given a pair of nodes (u, v) and event type k at time t, we use the above density formulation to compute conditional density at time t. The next time point t for the event can then be computed as: t = ∞ t tf u,v k (t)dt where the integral does not have an analytic form and hence we estimate it using Monte Carlo trick. For a given test record (u, v, t, k), we compute the next time this communication event may occur and report Mean Absolute Error (MAE) against the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">BASELINES</head><p>Dynamic Link Prediction. We compare the performance of our model against multiple representation learning baselines, four of which has capability to model evolving graphs. Specifically, we compare with Know-Evolve <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>-a state-of-the-art model for multi-relational dynamic graphs where each edge has time-stamp and type (communication events), DynGem <ref type="bibr" target="#b14">(Goyal et al., 2017</ref>)-divides timeline into discrete time points and learns embedding for the graph snapshots at these time points. DynTrd <ref type="bibr" target="#b54">(Zhou et al., 2018)</ref>    GAT is designed for supervised learning. In Appendix A (Ablation studies), we show results on one version where we only update attention based on Association events which is temporal analogous to GAT.</p><p>Event Time Prediction. We compare our model against (i) Know-Evolve which has the ability to predict time in a multi-relational dynamic graphs (II) Multi-dimensional Hawkes Process (MHP) <ref type="bibr" target="#b7">(Du et al., 2015)</ref> model where all events in graph are considered as dyadic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">EVALUATION SCHEME</head><p>We divide our test sets into n(= 6) slots based on time and report the performance for each time slot, thus providing comprehensive temporal evaluation of different methods. This method of reporting is expected to provide fine-grained insights on how various methods perform over time as they move farther from the learned training history. For dynamic baselines that do not explicitly model time (DynGem, DynTrd, GraphSage) and static baselines (Node2Vec), we adopt a sliding window training approach with warm-start method where we learn on initial train set and test for the first slot. Then we add the data from first slot in the train set and remove equal amount of data from start of train set and retrain the model using the embeddings from previous train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">EXPERIMENTAL RESULTS</head><p>Communication Event Prediction Performance. We first consider the task of predicting communication events between nodes which may or may not have a permanent edge (association) between them. Figure <ref type="figure" target="#fig_3">2</ref> (a-b) shows corresponding results.</p><p>Social Evolution. Our method significantly and consistently outperforms all the baselines on both metrics. While the performance of our method drops a little over time, it is expected due to the temporal recency affect on node's evolution. Know-Evolve can capture event dynamics well and shows consistently better rank than others but its performance deteriorates significantly in HITS@10 metric over time. We conjecture that features learned through edge-level modeling limits the predictive capacity of the method over time. The inability of DynGem (snapshot based dynamic), DynTrd and GraphSage (inductive) to significantly outperform Node2vec (transductive static baseline) demonstrate that discrete time snapshot based models fail to capture fine-grained dynamics of communication events.</p><p>Github dataset. We demonstrate comparable performance with both Know-Evolve and GraphSage on Rank metric. We would like to note that overall performance for all methods on rank metric is low. As we reported earlier, Github dataset is very sparse with very low clustering coefficient which makes it a challenging dataset to learn. It is expected that for a large number of nodes with no communication history, most of the methods will show comparable performance but our method outperforms all others when there is some history available. This is demonstrated by our significantly better performance for HITS@10 metric where we are able to do highly accurate prediction for nodes where we learn better history. This can also be attributed to our model's ability to capture the effect of evolving topology which is missed by Know-Evolve. Finally, we do not see significant decrease in performance of any method over time in this case which can again be attributed to roughly uniform distribution of nodes with no communication history across time slots.</p><p>Association Event Prediction Performance. Association events are not available for all time slots so Figure <ref type="figure" target="#fig_6">2 (c-d</ref>) report the aggregate number for this task. For both the datasets, our model significantly outperforms the baselines for this task. Specifically, our model's strong performance on HITS@10 metric across both datasets demonstrates its robustness in accurate learning from various properties of data. On Social evolution dataset, the number of association events are very small (only 485) and hence our strong performance shows that the model is able to capture the influence On the Github dataset, the network grows through new nodes and our model's strong performance across both metric demonstrates its inductive ability to generalize across new nodes across time. An interesting observation was poor performance of DynTrd which seems to be due to its objective to complete triangles. Github dataset is very sparse and has very few possibilities for triadic closure. Time Prediction Performance. Figure <ref type="figure">3</ref> demonstrates consistently better performance than stateof-the-art baseline for event time prediction on both datasets. While Know-Evolve models both processes as two different relations between entities, it does not explicitly capture the variance in the time scales of two processes. Further, Know-Evolve does not consider influence of neighborhood which may lead to capturing weaker temporal-structural dynamics across the graph. MHP uses specific parametric intensity function which fails to account for intricate dependencies across graph.</p><p>Qualitative Performance. We conducted a series of qualitative analysis to understand the discriminative power of evolving embeddings learned by DyRep. We compare our embeddings against GraphSage embeddings as it is state-of-the-art embedding method that is also inductive. Figure <ref type="figure">4</ref> (a-b) shows the tSNE embeddings learned by Dyrep (left) and GraphSage (right) respectively. The visualization demonstrates that DyRep embeddings have more discriminative power as it can effectively capture the distinctive and evolving structural features over time as aligned with empirical evidence. Figure <ref type="figure">4 (c-d</ref>) shows use case of two associated nodes (19 and 26) that has persistent edge but less communication for above two methods. DyRep keeps the embeddings nearby although not in same cluster (cos. dist. -0.649) which demonstrates its ability to learn the association and less communication dynamics between two nodes. For GraphSage the embeddings are on opposite ends of cluster with (cos. dist. -1.964). We provide more extensive analysis in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We introduced a novel modeling framework for dynamic graphs that effectively and efficiently learns node representations by posing representation learning as latent mediation process bridging dynamic processes of topological evolution and node interactions. We proposed a deep temporal point process model parameterized by temporally attentive representation network that models these complex and nonlinearly evolving dynamic processes and learns to encode structural-temporal information over graph into low dimensional representations. Our superior evaluation performance demonstrates the effectiveness of our approach compared to state-of-the-art methods. We present this work as the first generic and unified representation learning framework that adopts a novel modeling paradigm for dynamic graphs and support wide range of dynamic graph characteristics which can potentially have many exciting adaptations. As a part of our framework, we also propose a novel temporal point process based attention mechanism that can attend over neighborhood based on the history of communications and association events in the graph. Currently, DyRep does not support network shrinkage due to following reasons: (i) It is difficult to procure data with fine grained deletion time stamps and (ii) The temporal point process model requires more sophistication to support deletion. For example, one can augment the model with a survival process formulation to account for lack of node/edge at future time. Another interesting future direction could be to support encoding higher order dynamic structures. Figure <ref type="figure">5</ref>: Localized Embedding Propagation: An event is observed between nodes u and v and k can be 0 or 1 i.e. It can either be a topological event or interaction event. The first term in Eq 4. contains h struct which is computed for updating each node involved in the event. For node u, the update will come from h v struct (green flow) and for node v, the update will come from h u struct (red flow). Please note all embeddings are dynamically evolving hence the information flow after every event is different and evolves in a complex fashion. With this mechanism, the information is passed from neighbors of node u to node v and neighbors of node v to node u. (i) Interaction events lead to temporary pathway -such events can occur between nodes which are not connected. In that case, this flow will occur only once but it will not make u and v neighbors of each other (e.g. meeting at a conference). (ii) Topological events lead to permanent pathway -in this case u and v becomes neighbor of each other and hence will contribute to structural properties moving forward (e.g. being academic friends). The difference in number of blue arrows on each side signify different importance of each node to node u and node v respectively.</p><p>Overall Embedding Update Process. As a starting point, neighborhood only includes nodes connected by a structural edge. On observing an event, we update the embeddings of two nodes involved in the event using Eq 4. For a node u, the first term of Eq 4 (Localized Embedding Propagation) requires h struct which is the information that is passed from neighborhood (N v ) of node v to node u via node v (one can visualize v as being the message passer from its neighborhood to u). This information is used to update the embedding of node u. However, we posit that node v does not relay equal amount of information from its neighbors to node u. Rather, node v receives its information to be relayed based on its communication and association history with its neighbors (which relates to importance of each neighbor). This requires to compute the attention coefficients on the structural edges between node v and its neighbors. For any edge, we want this coefficient to be dependent on rate of events between the two nodes (thereby emulating real world phenomenon that one gains more information from people one interacts more with). Hence, we parameterize our attention module with the temporal point process parameter S uv . Algorithm 1 outlines the process of computing the value of this parameter. where 𝑖 ∈ 𝑁 * 𝑡 ̅ is the node in neighborhood of node u.</p><formula xml:id="formula_14">𝑞 *6 𝑡 ̅ = exp(𝑆 *6 (𝑡 ̅ )) ∑ exp(𝑆 *6 D (𝑡 ̅ )) 6 D ∈ E F -̅ 𝒛 G (𝑡 ̅ )</formula><p>Figure <ref type="figure">6</ref>: Temporal Point Process based Self-Attention: This figure illustrates the computation of h u struct for node u to pass to node v for the same event described before between nodes u and v at time t with any k. h u struct is computed by aggregating information from neighbors (1,2,3) of u. However, Nodes that are closely connected or has higher interactions tend to attend more to each other compared to nodes that are not connected or nodes between which interactions is less even in presence of connection. Further, every node has a specific attention span for other node and therefore attention itself is a temporally evolving quantity. DyRep computes the temporally evolving attention based on association and communication history between connected nodes. The attention coefficient function (q's) is parameterized by S which is computed using the intensity of events between connected nodes. Such attention mechanism allows the evolution of importance of neighbors to a particular node (u in this case) which aligns with real-world phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 COMPUTING S: ALGORITHM 1</head><p>Please check Figure <ref type="figure">7</ref> on next page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RATIONALE BEHIND DYREP FRAMEWORK</head><p>Connection to Marked Point Process. From a mathematical viewpoint, for any event e at time t, any information other than the time point can be considered a part of mark space describing the events. Hence, for DyRep, given a one-dimensional timeline, one can consider O = {(u, v, k) p , t p ) P p=1 as a marked process with the triple (u, v, k) representing the mark.</p><p>However, from machine learning perspective, using a single-dimensional process with such marks does not allow to efficiently and effectively discover or model the structure in the point process useful for learning intricate dependencies between events, participants of the events and dynamics governing those events. Hence, it is often important to extract the information out of the mark space and build an abstraction that helps to discover the structure in point process and make this learning parameter efficient. In our case, this translates to two components:</p><p>1. The nodes in the graph are considered as dimensions of the point process, thus making it a multi-dimensional point process where an event represents interaction/structure between the dimensions, thus allowing us to explicitly capture dependencies between nodes.</p><p>2. The topological evolution of networks happen at much different temporal scale than activities on a fixed topology network (e.g. rate of making friends vs liking a post on a social network). However both these processes affect each other's evolution in a complex and nonlinear Illustration of the update to S under two circumstances for events that involve node u: (i) Interaction events between neighbors (ii) Topological Event between non-neighbors. We only illustrate one node but update will happen for both nodes in the event (e.g. for (u, v), rows of both nodes will be updated asymmetrically due to different neighborhood size. Next attention for all other neighbors of both nodes (We only show for u here) are adjusted to reflect neighborhood size change. The matrix S is used for computing attention and hence does not get updated for interaction events between nodes which do not have an edge (for e.g. pair (1,2) may have an interaction event S 12 won't be updated as they are not neighbors.</p><p>fashion. Abstracting k to associate it with these different scales of evolution facilitates to model our purpose of expressing dynamic graphs at two time scales in a principled manner.</p><p>It also provides an ability to explicitly capture the influential dynamics <ref type="bibr" target="#b5">(Chazelle, 2012)</ref> of topological evolution on dynamics of network activities and vice versa (through the learned embedding -aka evolution through mediation.</p><p>Note that this distinction in use of mark information is also important as we learn representations for nodes (dimensions) but not for k. It is important to realize that k representing two different scales of event dynamics is not same as edge or interaction type. For instance, in case of typed persistent edge (e.g. wasbornIn, livesIn) or typed interaction (e.g. visit, fight), one would add type as another component in the mark space to represent an event while k still signifying different dynamic scales.</p><p>Comparison to <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref>. In the similar vein as above, the point process specification of <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> can also be considered as a marked process that models the typed interaction dynamics at a single time-scale and does not model topological evolution. In contrast to that, our method explicitly models dynamic graph process at two time scales. While both models use a point process based formulation for modeling temporal dynamics, there are several significant methodological differences between the two approaches: Deep Point Process Model -While one can augment the event specification in <ref type="bibr">(Trivedi et. al. 2017</ref>) with additional mark information, that itself is not adequate to achieve DyRep's modeling of dynamical process over graphs at multiple time scales. We employ a softplus function for f k which contains a dynamic specific scale parameter ψ k to achieve this while <ref type="bibr" target="#b42">(Trivedi et al. 2017</ref>) uses an exponential (exp) function for f with no scale parameter. Their intensity formulation attains a Rayleigh distribution which leads to a specific assumption about underlying dynamics which models fads where intensity of events drop rapidly between events after increasing. Our two-time scale model is more general and induces modularization, where each of two components allow complex, nonlinear and dependent dynamics towards a non-zero steady state intensity.</p><p>Graph Structure-As shown in <ref type="bibr" target="#b17">(Hamilton et al., 2017b)</ref>, the key idea behind representation learning over graphs is to capture both the global position and local neighborhood structural information of node into its representations. Hence, there has been significant research efforts invested in devising methods to incorporate graph structure into the computation of node representation. Aligned with these efforts, DyRep proposes a novel and sophisticated Localized Embedding Propagation principle that dynamically incorporates graph structure from both local neighborhood and faraway nodes (as interactions are allowed between nodes that do not have an edge). Contrary to that, <ref type="bibr" target="#b42">(Trivedi et al., 2017</ref>) uses single edge level information, specific to the relational setting, into their representations.</p><p>Deep Temporal Point Process Based Self-Attention-For learning over graphs, attention has been shown to be extremely valuable as importance of nodes differ significantly relative to each other. The state-of-the-art approaches have focused solely on static graphs with Graph Attention Networks <ref type="bibr" target="#b43">(Veličković et al., 2018)</ref> being the most recent one. Our attention mechanism for dynamic graphs present a significant and principled advancement over the existing state-of-the-art Graph based Neural Self-Attention techniques which only support static graphs. As <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> do not incorporate graph structure, they do not use any kind of attention mechanism.</p><p>Support for Node Attributes and Edge Types. Node types or attributes are supported in our work. In Eq. 4, z v ( tp v ) induces recurrence on node v's embedding, but when node v is observed for first time, z v ( tp v ) = x v where x v is randomly initialized or contains the raw node features available in data (which also includes type). One can also add an extra term in Eq. 4 to support high-dimension node attributes. Further, we also support different types of edges. If either the structural edge or an interaction has a type associated with it, our model can trivially support it in Eq. 3 and Eq. 4, first term h struct . Currently, for computing h struct , the formulation is shown to use aggregation over nodes. However, this aggregation can be augmented with edge type information as conventionally done in many representation learning frameworks <ref type="bibr" target="#b17">(Hamilton et al., 2017b)</ref>. Further, for more direct effect, Eq 3 can include edge type as third feature vector in the concatenation for computing g k .</p><p>Support for new nodes. As mentioned in Section 2.3 of the main paper, the data contains a set of dyadic events ordered in time. Hence, each event involves two nodes u and v. A new node will always appear as a part of such an event. Now, as mentioned above, the initial embedding of any new node u is given by z u ( tp u ) which can be randomly initialized or using the raw feature vector of the node u, x u . This allows the computation of intensity function for the event involving new node in Eq 1. Due to the inductive ability of our framework, we can then compute the embedding of the new node using Eq 4. There are two cases possible: Either one of the two nodes are new or both nodes are new. The mechanism for these two cases work as follows:</p><p>-Only one new node in observed event -To compute the embedding of new nodes, h struct is computed using neighborhood of the existing (other) node, z(t u 0 ) s the feature vector of the node or random and drift is 0. To compute the new embedding of existing node, h struct is the feature vector of the new node, self-propation uses the most recent embedding of the node and drift is based on previous time point. -Both nodes in the observed event are new -h struct is the feature vector of the feature vector of the other nodes, z(t u 0 ) s the feature vector of the node or random and drift is 0. Finally, Algorithm 1 does not require to handle new nodes any differently. As already available in the paper, both A and S are qualified by time and hence the matrices get updated every time. The starting dimension of the two matrices can be specified in two ways: (i) Construct both matrices of dimension = total possible no. of nodes in dataset and make the rows belonging to unseen nodes 0. (ii) Expand the dimensions of matrices as you start seeing new nodes. While we implement the first case, (ii) will be required in real-world streaming scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ABLATION STUDY</head><p>DyRep framework unifies several components that contribute to its effectiveness in learning rich node representation over complex and nonlinear processes in dynamic graphs. In this section, we provide insights on each component and how it is indispensable to the learning mechanism by performing an ablation study on various design choices of our model. Specifically, DyRep can be divided into three main parts: Multi-time scale point process model, Representation Update Formulation and Conditional Intensity Based Attention Mechanism. We focus on design choices available in each component and evaluate them on large github dataset. DyRep in the Figure <ref type="figure" target="#fig_9">8</ref> is the full model.</p><p>Multiple Time-Scale Processes. For this component, we perform two major tests:</p><p>• DyRep-Comm. In this variant, we make Eq 1., time-scale independent (i.e. remove k)</p><p>and we train on only Communication Events. But we evaluate on both communication and association events. Please note that this is possible as our framework can compute representations for unseen nodes. Hence during training they will only learn representation parameters based on communication events. It is observed that compared to the full model, the performance of model degrades in prediction for both types of events. But the decline is more prominent for the Association events compared to Communication Events. • DyRep-Assoc. In this variant, similar to above, we make Eq 1., time-scale independent and we train on only Association Events. But we evaluate on both communication and association events. It is observed that compared to the full model, the performance of model degrades in prediction for both types of events. But the decline is more prominent for the Communication events compared to Association Events.</p><p>The above two experiments show that considering events at a single time scale and not distinguishing between the processes hurt the performance. Although the performance is hurt more when communication events are not considered which may be due to the more availability of communication events due to its rapid frequency. We also performed a small test by training on all events but using a single scale parameter (ψ). The performance for both the dynamics degrades which demonstrates the effectiveness of ψ k .</p><p>Representation Update Formulation. For this component, we focus on Eq. 4 and switch off the components to observe its effect.</p><p>• DyRep-No-SP. In this variant, we switch off the self-propagation component and we observe that the overall performance is not hurt significantly by not using self-propagation. In general, this term provides a very weak feature and mainly captures the recurrent evolution of one's own latent features independent of others. It is observed that the deviation has increased for Association events which may point to the reason that there are few nodes who have links but highly varying frequency of communication and hence most of their features are either self-propagated or completely associated with others. • DyRep-No-Struct. In this variant, we remove the structural part of the model and as one would expect, the performance drops drastically in both the scenarios. This provides evidence to the necessity of building sophisticated structural encoders for dynamic graphs.</p><p>Intensity Attention Mechanism. For this component, we focus on Section 3.2 which builds the novel intensity based attention mechanism. Specifically, we carry following test:</p><p>• DyRep-No-Att. Here we completely remove the attention from the structural component and we see a significant drop in the performance. • DyRep-S-Comm. In this variant, we focus on Algorithm 1 and we only make update to the S matrix for Communication events but do not do it for Association events. This leads to slightly worse performance which helps to see how the S matrix is helping to mediate the two processes and not considering association events leads to loss of information. • DyRep-S-Assoc. In this variant, we focus on Algorithm 1 and we only make update to the S matrix for Association events but do not do it for Communication events. This leads to a significant drop in performance again validating the need for using both processes but its prominent effect also shows that communication events (dynamics on the network) is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPLORATORY ANALYSIS</head><p>We assess the quality of learned embeddings and the ability of model to capture both temporal and structural information. Let t 0 be the time point when train ended. Let t 1 be the timepoint when the first test slot ends.</p><p>Effect of Association and Communication on Embeddings. We conducted this experiment on Social dataset. We consider three use cases to demonstrate how the interactions and associations between the nodes changed their representations and visualize them to realize the effect.</p><p>• Nodes that did not have association before test but got linked during first test slot.</p><p>Nodes 46 and 76 got associated in test between test points 0 and 1. This reduced the cosine distance in both models but DyRep shows prominent effect of this association which should be the case. DyRep reduces the cosine distance from 1.231 to 0.005. Also, DyRep embeddings for these two points belong to different clusters initially but later converge to same cluster. In GraphSage, the cosine distance reduces from 1.011 to 0.199 and the embeddings still remain in original clusters. Figure <ref type="figure" target="#fig_11">9</ref> shows the visualization of embeddings at the two time points in both the methods. This demonstrates that our embeddings can capture association events effectively. • Nodes that did not have association but many communication events (114000). Nodes 27 and 70 is such a use case. DyRep embeddings consider the nodes to be in top 5 nearest neighbor of each other, in the same cluster and cosine distance of 0.005 which is aligned with the fact that nodes with large number of events tend to develop similar features over time.  Graphsage on the other hand considers them 32nd nearest neighbor, puts them in different clusters with cosine distance -0.792. Figure <ref type="figure" target="#fig_13">10</ref> shows the visualization of embeddings at the two time points in both the methods. This demonstrates the ability of DyRep's embedding to capture communication events and their temporal effect on embeddings effectively.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DETAILED RELATED WORK</head><p>Static Embedding Approaches. Representation Learning approaches for static graphs can be broadly classified into two categories -Node embedding approaches aim to encode structural information pertaining to a node to produce its low-dimensional representation <ref type="bibr" target="#b4">(Cao et al., 2015;</ref><ref type="bibr" target="#b15">Grover &amp; Leskovec, 2016;</ref><ref type="bibr" target="#b33">Perozzi et al., 2014;</ref><ref type="bibr" target="#b40">Tang et al., 2015;</ref><ref type="bibr" target="#b44">Wang et al., 2016a;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b48">Xu et al., 2017)</ref>. As they learn each individual node's representation, they are inherently transductive.</p><p>Recently, <ref type="bibr" target="#b16">(Hamilton et al., 2017a)</ref> proposed GraphSage, an inductive method for learning functions to compute node representations that can be generalized to unseen nodes. Sub-graph embedding techniques learn to encode higher order graph structures into low dimensional vector representations <ref type="bibr" target="#b37">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b26">Li et al., 2016;</ref><ref type="bibr" target="#b8">Dai et al., 2016)</ref>. Further, various approaches to use convolutional neural networks <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b5">2016;</ref><ref type="bibr" target="#b3">Bruna et al., 2014)</ref> over graphs have been proposed to capture sophisticated feature information but are generally less scalable. Most of these approaches only work with static graphs or can model evolving graphs without temporal information.</p><p>Dynamic Embedding Approaches. Preliminary approaches in dynamic representation learning have considered discrete time approach. <ref type="bibr" target="#b55">(Zhu et al., 2016)</ref> propose a temporal latent space model for link prediction using nonnegative matrix factorization. <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref> uses a warm start method to train across snapshots and employs a heuristic approach to learn stable embeddings over time but do not model time. <ref type="bibr" target="#b54">(Zhou et al., 2018)</ref> focuses on specific structure of triad to model how close triads are formed from open triads in dynamic networks. <ref type="bibr" target="#b38">(Seo et al., 2016)</ref> proposes a deep architecture based on combination of CNN to capture spatial characteristics and an RNN to capture temporal characteristics, to model structured sequences which in graph case will lead to discrete time model. <ref type="bibr" target="#b7">(Du et al., 2015)</ref> develops extends skip-gram based approaches for network embedding to dynamic setting where the graphs a re observed as discrete time snapshot and the goal is to learn embeddings that can preserve the optimality of skip-gram objective. NetWalk <ref type="bibr" target="#b50">(Yu et al., 2018)</ref> is a discrete-time dynamic embedding approach specifically designed for anomaly detection which uses clique based embedding techniques to learn vertex representations. Recently, <ref type="bibr" target="#b42">(Trivedi et al., 2017)</ref> proposed Know-Evolve, a deep recurrent architecture to model multi-relational timestamped edges that addresses the communication process. Unlike our approach, Know-Evolve models all edges at a single timescale, works for setting restricted to relational graphs and uses only edge-level structural information with no attention mechanism. DANE <ref type="bibr" target="#b25">(Li et al., 2017)</ref> proposes a network embedding method in dynamic environment but their dynamics consists of change in node's attributes over time and their current work can be considered orthogonal to our approach. <ref type="bibr" target="#b56">(Zuo et al., 2018)</ref> proposes a dynamic network formation model to learn node representations by employing a Hawkes process to model the temporal evolution of neighborhood for nodes. This work only considers association events. <ref type="bibr" target="#b31">(Ngyuyen et al., 2018)</ref> proposes a continuous time embedding framework that employs a temporal version of traditional random walks in a simple manner to capture temporally evolving neighborhood information.</p><p>Other models for dynamic networks. There exists a rich body of literature on temporal modeling of dynamic networks <ref type="bibr" target="#b22">(Kim et al., 2017</ref>) that focus on link prediction tasks but their goal is orthogonal to us as they build task specific methods and do not focus on representation learning. Further, there are several approaches in graph mining and temporal relational learning community <ref type="bibr" target="#b27">(Loglisci &amp; Malerba, 2017;</ref><ref type="bibr" target="#b28">Loglisci et al., 2015;</ref><ref type="bibr" target="#b9">Esteban et al., 2016;</ref><ref type="bibr" target="#b21">Jiang et al., 2016)</ref> that consider dynamic networks but are orthogonal to our current work. Research on learning dynamic embeddings has also progressed in linguistic community where the aim is to learn temporally evolving word embeddings <ref type="bibr" target="#b2">(Bamler &amp; Mandt, 2017;</ref><ref type="bibr" target="#b35">Rudolph &amp; Blei, 2018)</ref>. <ref type="bibr" target="#b49">(Yang et al., 2017;</ref><ref type="bibr" target="#b36">Sarkar et al., 2007)</ref> include some other approaches that propose model of learning dynamic embeddings in graph data but none of these models consider time at finer level and do not capture both topological evolution and interactions. <ref type="bibr" target="#b30">(Meng et al., 2018)</ref> proposes subgraph pattern neural networks that focuses on evolution of subgraphs instead of single nodes and links. They build a novel neural network architecture for supervised learning where the hidden layers represent the subgraph patterns observed in the data and output layer is used to perform prediction. <ref type="bibr" target="#b51">(Yuan et al., 2017)</ref> induces a dynamic graph from videos based on the visual correlation of object proposal that spans across the video. They further propose an LSTM based architecture to capture temporal dependencies over this induced graph and perform object detection. <ref type="bibr" target="#b20">(Jerfel et al., 2017)</ref> proposes a dynamic probabilistic model in bipartite case of user-item recommendation where the goal is to learn the evolution of user and item latent features</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Evolution Through Mediation. (a) Association events (k=0) where the node or edge grows.(c) Communication Events (k=1) where nodes interact with each other. For both these processes, t p,k=0 &lt; (t 1 , t 2 , t 3 , t 4 , t 5 ) k=1 &lt; t q,k=0 &lt; (t 6 , t 7 ) k=1 &lt; t r,k=0 . (b) Evolving Representations.</figDesc><graphic url="image-1.png" coords="2,108.00,81.86,396.00,167.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For a set O of P observed events, we learn these parameters by minimizing the negative log likelihood: L = − P p=1 log (λ p (t)) + T 0 Λ(τ )dτ , where λ p (t) = λ up,vp kp (t) represent the intensity of event at time t and Λ(τ ) = n u=1 n v=1 k∈{0,1} λ u,v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>focuses on specific structure of triad to model how close triads are formed from open triads in dynamic networks. GraphSage (Hamilton et al., 2017a)-an inductive representation learning method that learns sample and aggregation functions to learn representations instead of training for individual node. Node2Vec (Grover &amp; Leskovec, 2016)-simple transductive baseline to learn graph embeddings over static graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dynamic Link Prediction Performance for (a-b) Social Evolution Dataset (c-d) Github Dataset. We report HITS@10 results and zoomed versions in Appendix E. Best viewed in pdf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Time Prediction Performance (unit is hrs). Figure best viewed in pdf or colored print.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A. 2</head><label>2</label><figDesc>COMPUTING h struct : TEMPORAL POINT PROCESS BASED ATTENTION 𝜎(𝑞 *6 𝑡 ̅ * 𝒉 6 𝑡 ̅ ) ) 𝒉 6 𝑡 ̅ = 𝑾 9 𝒛 6 𝑡 ̅ + 𝑏 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure7: Computing S. Illustration of the update to S under two circumstances for events that involve node u: (i) Interaction events between neighbors (ii) Topological Event between non-neighbors. We only illustrate one node but update will happen for both nodes in the event (e.g. for (u, v), rows of both nodes will be updated asymmetrically due to different neighborhood size. (a) shows the initial state where u has 4 neighbors and hence background attention is uniform b = 0.25. (b) u has an interaction event with node 5. Update only happens to S u5 and S 5u based on intensity of the event. (c) u has a topological event with node 4. b changes to 0.2. b = 0.25 which is the previous b. Update happens to S u4 and S 4u based on intensity of event.Next attention for all other neighbors of both nodes (We only show for u here) are adjusted to reflect neighborhood size change. The matrix S is used for computing attention and hence does not get updated for interaction events between nodes which do not have an edge (for e.g. pair (1,2) may have an interaction event S 12 won't be updated as they are not neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure7: Computing S. Illustration of the update to S under two circumstances for events that involve node u: (i) Interaction events between neighbors (ii) Topological Event between non-neighbors. We only illustrate one node but update will happen for both nodes in the event (e.g. for (u, v), rows of both nodes will be updated asymmetrically due to different neighborhood size. (a) shows the initial state where u has 4 neighbors and hence background attention is uniform b = 0.25. (b) u has an interaction event with node 5. Update only happens to S u5 and S 5u based on intensity of the event. (c) u has a topological event with node 4. b changes to 0.2. b = 0.25 which is the previous b. Update happens to S u4 and S 4u based on intensity of event.Next attention for all other neighbors of both nodes (We only show for u here) are adjusted to reflect neighborhood size change. The matrix S is used for computing attention and hence does not get updated for interaction events between nodes which do not have an edge (for e.g. pair (1,2) may have an interaction event S 12 won't be updated as they are not neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Ablation Study on Github Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Use Case I. Top row: GraphSage Embeddings. Bottom Row: DyRep Embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Train End Time (b) Test Slot 1 End Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Use Case II. Top row: GraphSage Embeddings. Bottom Row: DyRep Embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Dynamic Link Prediction Performance: Top 2 rows show performance for Social Evolution Dataset. Bottom 2 rows show performance for Github Dataset. 1st and 3rd row show performance for Communication Events while 2nd and 4th row show performance for Association Events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>5.1 DATASETSWe evaluate DyRep and baselines on two real world datasets: Social Evolution Dataset released by MIT Human Dynamics Lab -#nodes: 83, #Initial Associations: 376, #Final Associations: 791, #Communications: 2016339 and Clustering Coefficient: 0.548. Github Dataset available at Github Archive -#nodes: 12328, #Initial Associations: 70640, #Final Associations: 166565, Comparison of DyRep with state-of-the-art approaches These datasets cover a range of configurations as Social Dataset is a small network with high clustering coefficient and over 2M events. In contrast, Github dataset forms a large network with low clustering coefficient and sparse events thus allowing us to test the robustness of our model. Further, Github dataset contains several unseen nodes which were never encountered during training.</figDesc><table><row><cell>Key</cell><cell>DyRep</cell><cell>Know-Evolve</cell><cell>DynGem</cell><cell>GraphSage</cell><cell>GAT</cell></row><row><cell>Properties</cell><cell>(Our Method)</cell><cell>(Dynamic)</cell><cell>(Dynamic)</cell><cell>(Static)</cell><cell>(Static)</cell></row><row><cell>Models Association</cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models Communication</cell><cell></cell><cell></cell><cell>X</cell><cell>X</cell><cell>X</cell></row><row><cell>Models Time</cell><cell></cell><cell></cell><cell>X</cell><cell>X</cell><cell>X</cell></row><row><cell>Learns Representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predicts Time</cell><cell></cell><cell></cell><cell>X</cell><cell>X</cell><cell>X</cell></row><row><cell>Graph Information</cell><cell>2nd-order</cell><cell>Single</cell><cell>1st and 2nd-order</cell><cell>2nd-order</cell><cell>1st-order</cell></row><row><cell></cell><cell>Neighborhood</cell><cell>Edge</cell><cell>Neighborhood</cell><cell>Neighborhood</cell><cell>Neighborhood</cell></row><row><cell>Attention Mechanism</cell><cell>Temporal Point Process</cell><cell>None</cell><cell>None</cell><cell>Sampling</cell><cell>Multi-head</cell></row><row><cell></cell><cell>(Non-Uniform)</cell><cell></cell><cell></cell><cell>(Uniform)</cell><cell>(Non-Uniform)</cell></row><row><cell>Learning</cell><cell>Unsupervised</cell><cell cols="2">Unsupervised Semi-Supervised</cell><cell>Unsupervised</cell><cell>Supervised</cell></row><row><cell cols="4">#Communications: 604649 and Clustering Coefficient: 0.087.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table1provides qualitative comparison between state-of-the-art methods and our framework. In our experiments, we compare with GraphSage instead of GAT as we share the unsupervised setting with GraphSage while</figDesc><table><row><cell>0 20 40 60 MAR</cell><cell>1</cell><cell>2 DynGem DynTrd</cell><cell>3 Time_Slot 4 DyRep GraphSage</cell><cell>5 Know-Evolve Node2Vec</cell><cell>6</cell><cell>0 10 20 30 MAR 40 50</cell><cell>11.0773 DyRep 13.4774 Know-Evolve Methods 42.5548 42.74 DynGem DynTrd</cell><cell>19.0348 GraphSage 40.5741 Node2Vec</cell><cell>3500 4000 4500 MAR</cell><cell>1</cell><cell>2 DynGem DynTrd</cell><cell>3 Time_Slot 4 DyRep GraphSage</cell><cell>5 Know-Evolve Node2Vec</cell><cell>6</cell><cell>0 1000 2000 MAR 3000 4000</cell><cell>2722.81</cell><cell>Methods 3762.024 4149.9546 Know-Evolve 3007.0233 DyRep DynGem DynTrd</cell><cell>3124.5371 GraphSage 4202.606 Node2Vec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 11: Use Case IV: DyRep Embeddings over time -From left to right and top to bottom. t are the timepoints when test with that id ended. Hence, t = 1 means the time when test slot 1 finished.</figDesc><table><row><cell>20 10 0 10</cell><cell>20 MAR 40 60</cell><cell></cell><cell cols="2">DynGem DynTrd</cell><cell>DyRep GraphSage</cell><cell cols="2">Know-Evolve Node2Vec</cell><cell>(46, 20) (18, 61) (0, 13) (72, 23) (19,26) (78, 69) (53, 3) (82, 48) others</cell><cell cols="4">20 15 10 5 0 5 10 15 HITS_10 0.25 0.50 0.75 1.00</cell><cell>DynGem DynTrd</cell><cell>DyRep GraphSage</cell><cell>Know-Evolve Node2Vec</cell><cell>(0, 13) (72, 23) (19,26) (78, 69) (53, 3) (82, 48) others (18, 61) (46, 20)</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell>10</cell><cell>0</cell><cell>10</cell><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell cols="2">20</cell><cell>10</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell></cell><cell>t = 1 3 Time_Slot 4</cell><cell></cell><cell>5</cell><cell>6</cell><cell>20</cell><cell cols="3">0.00</cell><cell>1</cell><cell>2</cell><cell>t = 2 3 Time_Slot 4</cell><cell>5</cell><cell>6</cell></row><row><cell>20 10 0 10 20 20 10 0 10 20</cell><cell cols="2">20 MAR 50 20 20 40 30 10 0 4000 15 MAR 4500</cell><cell cols="3">10 DyRep 5 Know-Evolve 0 t = 3 10 0 10 5 DynGem 10 DynTrd 13.4774 42.5548 42.74 11.0773 Methods t = 5 DynGem DynTrd DyRep GraphSage</cell><cell>20</cell><cell cols="2">15 GraphSage 20 Node2Vec 40.5741 (46, 20) (18, 61) (0, 13) (72, 23) (19,26) (78, 69) (53, 3) (82, 48) others (46, 20) (18, 61) (0, 13) (72, 23) (19,26) (78, 69) (53, 3) (82, 48) others 19.0348 Know-Evolve Node2Vec</cell><cell cols="3">20 15 10 5 0 5 10 15 30 20 10 0 10 20 HITS_10 0.1 0.8 0.6 0.4 0.2 0.0 0.2 HITS_10 0.3 0.4</cell><cell>20 20</cell><cell>0.787</cell><cell>10 DyRep Know-Evolve 0 t = 4 DynGem 10 DynTrd 10 0 10 0.6515 0.1032 0.1236 Methods t = 6 DynGem DynTrd DyRep GraphSage Know-Evolve 20 GraphSage Node2Vec 20 0.1949 0.1483 Node2Vec</cell><cell>(46, 20) (18, 61) (0, 13) (72, 23) (19,26) (78, 69) (53, 3) (82, 48) others (46, 20) (18, 61) (0, 13) (53, 3) (82, 48) others (78, 69) (19,26) (72, 23)</cell></row><row><cell cols="14">3500 E FULL EXPERIMENT RESULTS FOR BOTH DATASETS 1 2 3 4 5 6 Time_Slot 0.0 1 2 DyRep Know-Evolve DynGem DynTrd GraphSage Node2Vec DyRep Know-Evolve 3 Time_Slot 4 DynGem DynTrd</cell><cell>5 GraphSage Node2Vec</cell><cell>6</cell></row><row><cell cols="14">Figure 12 provides HITS@10 results in addition to the MAR results reported for Link Prediction in 4149.9546 4202.606 4000 0.323 0.3</cell></row><row><cell cols="8">3762.024 Section 5 (Experiments) of the main paper. 2722.81 3007.0233 3124.5371 1000 2000 3000 MAR</cell><cell></cell><cell cols="2">HITS_10</cell><cell>0.1 0.2</cell><cell></cell><cell>0.1173</cell></row><row><cell></cell><cell></cell><cell cols="2">0</cell><cell></cell><cell cols="2">Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.0246 Methods 0.0069</cell><cell>0.0232</cell><cell>0.0131</cell></row></table><note>• Temporal evolution of DyRep embeddings. In figure11we visualize the embedding positions of the nodes (tracked in red) as they evolve through time and forms and breaks from clusters.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We sincerely thank our anonymous ICLR reviewers for critical feedback that helped us to improve the clarity and precision of our presentation. We would also like to thank Jiachen Yang (Georgia Tech) for insightful comments on improving the presentation of the paper. This work was supported in part by NSF IIS-1717916, NSF CMMI-1745382, NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Information (U1609220) and National Science Foundation of China (61672231).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>under the context of Poisson factorization, thus considering the evolution processes of users' and items' latent features as independent of each other.</p><p>Deep Temporal Point Process Models. Recently, <ref type="bibr" target="#b8">(Du et al., 2016)</ref> has shown that fixed parametric form of point processes lead into the model misspecification issues ultimately affecting performance on real world datasets. <ref type="bibr" target="#b8">(Du et al., 2016)</ref> therefore propose a data driven alternative to instead learn the conditional intensity function from observed events and thereby increase its flexibility. Following that work, there have been increased attraction in topic of learning conditional intensity function using deep learning <ref type="bibr" target="#b29">(Mei &amp; Eisner, 2017)</ref> and also intensity free approach using GANS <ref type="bibr" target="#b45">(Xiao et al., 2017)</ref> for learning with deep generative temporal point process models. For the social evolution dataset, we consider Proximity, Calls and SMS records between users as communication events (k=1) and all Close Friendship records as association events (k=0). For Github dataset, we consider Star/Watch records as communication events (k=1) and Follow records as association events (k=0). The Social Evolution data is collected from Jan 2008 to to June, 30 2009. We consider the association events between user from Jan 2008-Sep 10, 2008 (survey date) to form the initial network and use the rest of data for our experiments. We collected Github data from Jan 2013 -Dec 2013. For the nodes in 2013, we consider Follow link that existed between them before 2013 to form the initial network. We pre-process both datasets to remove duplicate (not recurrent in time) records and self-loops. We also process Github dataset to only contain users (and not organizations) as nodes and we select nodes that have at least 40 communication (watch) events and 10 association (follow) events. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 TRAINING CONFIGURATIONS</head><p>We performed hyper parameter search for best performance for our method and all the baselines and used the following hyper-parameters to obtain the reported results:</p><p>-For social dataset: Num nodes = 100, Num Dynamics = 2, bptt (sequence length) = 200, embed_size = 32, hidden_unit_size = 32, nsamples (for survival) = 5, gradient_clip = 100 and no dropout.</p><p>-For github dataset: Num nodes = 12328, Num Dynamics = 2, bptt (sequence length) = 300, embed_size = 256, hidden_unit_size = 256, nsamples (for survival) = 5, gradient_clip = 100.</p><p>For baselines, we used the implementations provided by their authors and we report the range of configurations used for baseline here: max_iter = {1000, 5000, 10000}, bptt = {100, 200, 300}, lr = {0.0005, 0.0050.5, 0.1, 1}, embed_E = {32, 64, 128, 256}, embed_R = {32, 64, 128, 256}, hidden = {32, 64, 128, 256}, warm = 0, t_scale = 0.0001, w_scale = 0.1, num_epochs = {10, 50, 100, 500, 1000}. As mentioned in experiment section, we always train baselines with warmstart in a sliding window training fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Know-Evolve:</head><p>The code provided by the authors was implemented in C++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphSage:</head><p>The code was implemented in Tensorflow by the authors. We use only the unsupervised train module to generate embeddings. Node2Vec: We use the original python code with few changes in the hyper-parameters. We fix q in the node2vec as 0.8 for Social Dataset and 1 for Github dataset. DynGEM: We experiment on the original code implemented in Keras with Theano backend by the authors.</p><p>DynTrd: We use original code provided by the authors.</p><p>For tSNE embedding visualization in Figure <ref type="figure">4</ref>, we used sklearn.manif old.T SN E library to plot this figure with n components = 2, learning rate = 200, perplexity = 30, metric = "euclidean", min_grad_norm = 1e-9, early exaggeration = 4 and ran for 40,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H MONTE CARLO ESTIMATION FOR SURVIVAL TERM IN L FOR SECTION 4</head><p>Algorithm 2 Computation of integral term in L for a mini-batch</p><p>Algorithm 2 is a simple variant of Monte Carlo trick to compute the survival term of log-likelihood equation. Specifically, in each mini-batch, we sample non-events instead of considering all pairs of non-events (which can be millions). Let m be the mini-batch size and N be the number of samples. The complexity of Algorithm 2 will then be O(2mkN ) for the batch where the factor of 2 accounts for the update happening for two nodes per event which demonstrates linear scalability in number of events.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Survival and event history analysis: a process point of view</title>
		<author>
			<persName><forename type="first">Ornulf</forename><surname>Odd Aalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakon</forename><surname>Borgan</surname></persName>
		</author>
		<author>
			<persName><surname>Gjessing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Artime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">J</forename><surname>Ramasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxi</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04155</idno>
		<title level="m">Dynamics on networks: competition of temporal and topological correlations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic word embedding</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<editor>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012. 2016</date>
		</imprint>
	</monogr>
	<note>Natural algorithms and influence systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Introduction to the Theory of Point Processes: Volume I: Elementary Theory and Methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Jones</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Time sensitive recommendation from recurrent user activities</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent marked temporal point processes: Embedding event history to vector</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting the co-evolution of event and knowledge graphs</title>
		<author>
			<persName><forename type="first">Cristobal</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinchong</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><surname>Krompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FUSION</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shaping social activity by incentivizing users</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">Gomez</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coevolve: A joint point process model for information diffusion and network coevolution</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multistage campaigning in social networks</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Harati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Damien Farine. The dynamics of transmission and the dynamics of networks</title>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="415" to="418" />
		</imprint>
	</monogr>
	<note>Fake news mitigation via point process based intervention</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dyngem: Deep embedding method for dynamic graphs</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Workshop on Representation Learning for Graphs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectra of some self-exciting and mutually exciting point processes</title>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">G</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A self-correcting point process</title>
		<author>
			<persName><forename type="first">V</forename><surname>Isham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Westcott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="629" to="646" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic collaborative filtering with compund poisson factorization</title>
		<author>
			<persName><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">E</forename><surname>Basbug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">E</forename><surname>Engelhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Encoding temporal information for time-aware link prediction</title>
		<author>
			<persName><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A review of dynamic network models with latent variables</title>
		<author>
			<persName><forename type="first">Bomin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingzhou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyue</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10421</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attributed network embedding for learning in a dynamic environment</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilaing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Change</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging temporal autocorrelation of historical data for improving accuracy in network regression</title>
		<author>
			<persName><forename type="first">Corrado</forename><surname>Loglisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donato</forename><surname>Malerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Analysis and Data Mining: The ASA Data Science Journal</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relational mining for discovering changes in evolving networks</title>
		<author>
			<persName><forename type="first">Corrado</forename><surname>Loglisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelangelo</forename><surname>Ceci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donato</forename><surname>Malerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Subgraph pattern neural networks for high-order graph evolution prediction</title>
		<author>
			<persName><forename type="first">Changpin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><forename type="middle">S</forename><surname>Mouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Continuous-time dynamic network embeddings</title>
		<author>
			<persName><forename type="first">Giang</forename><surname>Hoang Ngyuyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Boaz</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyee</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic embeddings for language evolution</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A latent space approach to dynamic embedding of co-occurence data</title>
		<author>
			<persName><forename type="first">Purnamrita</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajid</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distilling information reliability and source trustworthiness from digital traces</title>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Tabibian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Netcodec: Community detection from individual activities</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiquiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Isotonic hawkes processes</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wasserstein learning of deep generative point process models</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Embedding identity and interest for social networks</title>
		<author>
			<persName><forename type="first">Linchuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08249</idno>
		<title level="m">Graph clustering with dynamic embedding</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Netwalk: A flexible deep embedding approach for anamoly detection in dynamic networks</title>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal dynamic graph lstm for action-driven video object detection</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Correlated cascades: Compete or cooperate</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Zarezade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Khodadadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Hamid R Rabiee</surname></persName>
		</author>
		<author>
			<persName><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic network embedding by modeling triadic closure process</title>
		<author>
			<persName><forename type="first">Lekui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Scalable temporal latent space inference for link prediction in dynamic social networks</title>
		<author>
			<persName><forename type="first">Linhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junming</forename><surname>Dong Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Embedding temporal network via neighborhood formation</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
