<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep relaxation: partial differential equations for optimizing deep neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Adam</forename><surname>Oberman</surname></persName>
							<email>adam.oberman@mcgill.ca</email>
							<idno type="ORCID">0000-0002-4214-7364</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
							<idno type="ORCID">0000-0002-4214-7364</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Carlier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep relaxation: partial differential equations for optimizing deep neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B64BD033C6323FCB03C2B00E96E283C5</idno>
					<idno type="DOI">10.1007/s40687-018-0148-y</idno>
					<note type="submission">Received: 10 February 2018 Accepted: 9 June 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Partial differential equations</term>
					<term>Stochastic gradient descent</term>
					<term>Neural networks</term>
					<term>Optimal control</term>
					<term>Proximal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entropy-SGD is a first-order optimization method which has been used successfully to train deep neural networks. This algorithm, which was motivated by statistical physics, is now interpreted as gradient descent on a modified loss function. The modified, or relaxed, loss function is the solution of a viscous Hamilton-Jacobi partial differential equation (PDE). Experimental results on modern, high-dimensional neural networks demonstrate that the algorithm converges faster than the benchmark stochastic gradient descent (SGD). Well-established PDE regularity results allow us to analyze the geometry of the relaxed energy landscape, confirming empirical evidence. Stochastic homogenization theory allows us to better understand the convergence of the algorithm. A stochastic control interpretation is used to prove that a modified algorithm converges faster than SGD in expectation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview of the main results</head><p>Deep neural networks are machine learning models that have achieved remarkable success in a number of domains from visual recognition and speech, to natural language processing and robotics <ref type="bibr" target="#b39">[40]</ref>. A rigorous understanding of the roots of this success, however, remains elusive. These models are a parametric class of functions whose parameters are found by minimizing a non-convex loss function, typically via first-order algorithms such as stochastic gradient descent (SGD) along with a variety of regularization techniques. In this paper, we study a modification of the SGD algorithm which leads to improvements in the training time and generalization error of deep neural networks.</p><p>Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref> and Baldassi et al. <ref type="bibr" target="#b3">[4]</ref> recently introduced a modification of the loss function of a neural network f (x) with x ∈ R n called local entropy; this function is denoted by f γ (x). The authors in Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref> showed that this modification is effective for training modern, high-dimensional deep neural networks. Our first result is to show that f γ (x) is the solution of a nonlinear partial differential equation (PDE); see Sect. We show that</p><formula xml:id="formula_0">f γ (x) = u(x, γ ),</formula><p>where u(x, t) is the solution of (viscous HJ). Using standard semi-concavity estimates in Sect. 6, the PDE interpretation immediately leads to regularity results for the solutions that are consistent with empirical observations made by Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref>. Algorithmically, our starting point is the continuous-time stochastic gradient descent equation, dx(t) = -∇f (x(t)) dt + β -1/2 dW (t) ( S G D )</p><p>for t ≥ 0 where W (t) ∈ R n is the n-dimensional Wiener process and the coefficient β -1 is the variance of a stochastic estimate of the gradient ∇f (x); see Sect. 2.2. Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref> compute the gradient of local entropy ∇f γ (x) by first computing the invariant measure, ρ(y), of the auxiliary stochastic differential equation (SDE) dy(s) = -γ -1 y(s) + ∇f (x(t)y(s)) ds + β -1/2 dW (s)</p><p>then by updating x(t) by averaging against this invariant measure dx(t) = -∇f (x(t)y) ρ(dy).</p><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The effectiveness of the algorithm may be partially explained by the fact that for small value of γ , that solutions of Fokker-Planck equation associated with the SDE (1) converge exponentially quickly to the invariant measure ρ. Tools from optimal transportation <ref type="bibr" target="#b56">[57]</ref> or functional inequalities <ref type="bibr" target="#b1">[2]</ref> imply that there is a threshold parameter related to the semiconvexity of f (x) for which the auxiliary SGD converges exponentially; we discuss this in Sect. 2.3 and Remark 6. Beyond this threshold, the auxiliary problem can be slow to converge.</p><p>In Sect. 4, we prove using homogenization of SDEs <ref type="bibr" target="#b50">[51]</ref> that (2) recovers the gradient of the local entropy function, u(x, γ ). In other words, in the homogenization limit, (2) is equivalent to dx(t) = -∇u(x(t), γ ) dt, <ref type="bibr" target="#b2">(3)</ref> where u(x, γ ) is the solution of (viscous HJ). Thus, smoothing the original loss function by computing the solution of the PDE directly is not practical due to the curse of dimensionality; the auxiliary SDE (1) accomplishes this naturally. The Elastic-SGD algorithm [66] is an influential algorithm which trains multiple deep networks in parallel with obtaining faster convergence in the distributed setting. We prove in Sect. 4.3 that the Elastic-SGD is equivalent to minimizing local entropy by the Entropy-SGD algorithm. The equivalence is a surprising result, since the former computes the average of a coupled realizations of SGD over multiple processors while the latter computes temporal averages to evaluate (2) over a single processor. This result follows naturally if the gradient dynamics is ergodic once the homogenization framework is established for Entropy-SGD.</p><p>The parameter γ can be modulated during the optimization; we call this technique scoping. If γ is large, the smoothing achieved by the PDE is larger while a small γ recovers the true gradients of the original loss function f (x) toward the end. Scoping is equivalent to replacing (3) by dx(t) = -∇u(x(t), Tt) dt, where T is a termination time for the algorithm. Scoping, which was introduced as effective heuristic in Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref>, is rigorously shown to be effective in Sect. <ref type="bibr" target="#b4">5</ref>. Additionally, we discuss its connections to nonlinear forward-backward equations, which appear in Mean Field Games; see <ref type="bibr">Remark 9.</ref> It is unusual in stochastic non-convex optimization to obtain a proof of the superiority of a particular algorithm compared to standard stochastic gradient descent. We obtain such a result in Theorem 12 of Sect. 5 where we prove an improvement (for slightly modified dynamics) in the expected value of the loss function using the local entropy dynamics as compared to that of SGD. This result is obtained using well-known techniques from stochastic optimal control theory [25] and the comparison principle for viscosity solutions. Again, while these techniques are well-established in nonlinear PDEs, we are not aware of their application in this context.</p><p>The effectiveness of the PDE regularization is illustrated in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep neural networks</head><p>A deep network is a nested composition of linear functions of an input datum ξ ∈ R d and a nonlinear function σ that is applied element-wise to its arguments. The output of a neural network, denoted by y(x; ξ ), can be written as</p><formula xml:id="formula_3">y(x; ξ ) = σ x p σ x p-2 . . . σ x 1 ξ . . . ;<label>( 4 )</label></formula><p>where x 1 , . . . , x p-1 ∈ R d×d and x p ∈ R d×K are the parameters or "weights" of the network. We will let x ∈ R n denote their concatenation after rearranging each one of them as a vector. The functions σ (•) are applied point-wise to each element of their argument. For instance, in a network with rectified linear units (ReLUs), we have σ (z) = max(0, z) for z ∈ R d . A neural network is thus a function with n parameters that produces an output y(x; ξ ) ∈ {1, . . . , K } for each input ξ .</p><p>In supervised learning or "training" of a deep network for the task of image classification, given a finite sample of inputs and outputs D = {(ξ i , y i )} N i=1 (dataset), one wishes to minimize the empirical loss 1 1 The empirical loss is a sample approximation of the expected loss, E x∼P f (x), which cannot be computed since the data distribution P is unknown. The extent to which the empirical loss (or a regularized version thereof) approximates the expected loss relates to generalization, i.e., the value of the loss function on ("test" or "validation") data drawn from P but not part of the training set D.</p><p>Fig. <ref type="figure">1</ref> One-dimensional loss function f (x) (black) and smoothing of f (x) by the viscous Hamilton-Jacobi equation (red) and the non-viscous Hamilton Jacobi equation (blue). The initial density (light gray) is evolved using the Fokker-Planck equation ( <ref type="formula" target="#formula_80">26</ref>) which uses solutions of the respective PDEs. The terminal density obtained using SGD (dark gray) remains concentrated in a local minimum. The function u viscous HJ (x, T ) (red) is convex, with the global minimum close to that of f (x); consequently, the terminal density (red) is concentrated near the minimum of f (x). The function u non-viscous HJ (x, T ) (blue) is non-convex, so the presence of gradients pointing away from the global minimum slows down the evolution. Nevertheless, the corresponding terminal density (blue) is concentrated in a nearly optimal local minimum. Note that u non-viscous HJ (x, T ) can be interpreted as a generalized convex envelope of f ; it is given by the Hopf-Lax formula (HL). Also note that both the value and location of some local extrema for non-viscous HJ are unchanged, while the regions of convexity widen. On the other hand, local maxima located in smooth concave regions see their concave regions shrink, value eventually decreases. This figure was produced by solving (26) using monotone finite differences <ref type="bibr" target="#b48">[49]</ref> f (x) := 1</p><formula xml:id="formula_4">N N i=1 f i (x);<label>(5)</label></formula><p>where f i (x) is a loss function that measures the discrepancy between the predictions y(x, ξ i ) for the sample ξ i and the true label y i . For instance, the zero-one loss is</p><formula xml:id="formula_5">f i (x) := 1 {y(x; ξ i ) = y i }</formula><p>whereas for regression, the loss could be a quadratic penalty</p><formula xml:id="formula_6">f i (x) := 1 2 y(x; ξ i ) -y i 2 2 .</formula><p>Regardless of the choice of the loss function, the overall objective f (x) in deep learning is a non-convex function of its argument x.</p><p>In the sequel, we will dispense with the specifics of the loss function and consider training a deep network as a generic non-convex optimization problem given by</p><formula xml:id="formula_7">x * = arg min x f (x).<label>(6)</label></formula><p>However, the particular relaxation and regularization schemes we describe are developed for, and tested on, classification tasks using deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stochastic gradient descent (SGD)</head><p>First-order methods are the default choice for optimizing <ref type="bibr" target="#b5">(6)</ref>, since the dimensionality of x is easily in the millions. Furthermore, typically, one cannot compute the entire gradient N -1 N i=1 ∇f i (x) at each iteration because N can also be in the millions for modern datasets. 2 Stochastic gradient descent <ref type="bibr" target="#b52">[53]</ref> consists of performing partial computations of the form</p><formula xml:id="formula_8">x k+1 = x k -η k ∇f i k (x k-1 ) where η k &gt; 0, k ∈ N,<label>(7)</label></formula><p>where i k is sampled uniformly at random from the set {1, . . . , N }, at each iteration, starting from a random initial condition x 0 . The stochastic nature of SGD arises from the approximation of the gradient using only a subset of data points. One can also average the gradient over a set of randomly chosen samples, called a "mini-batch." We denote the gradient on such a mini-batch by ∇f mb (x).</p><p>Variance reduction of the stochastic gradients of Sect. 2.2 leads to improved theoretical convergence rates for convex loss functions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b58">59]</ref> and is implemented in algorithms such as AdaGrad <ref type="bibr" target="#b20">[21]</ref>, SAG <ref type="bibr" target="#b58">[59]</ref>, SAGA <ref type="bibr" target="#b19">[20]</ref>, Adam <ref type="bibr" target="#b34">[35]</ref> and others, see the review by Bottou et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>State-of-the-art results in deep learning leverage upon a number of optimization heuristics. For instance, adapting the step size, i.e., changing η k in <ref type="bibr" target="#b6">(7)</ref> with time k, is common practice. But while classical convergence analysis suggests step size reduction of the form O(k -1 ) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b58">59]</ref>, piecewise constant values for the step size are common practice. Changing the mini-batch size i k in ( <ref type="formula" target="#formula_8">7</ref>) is equivalent to changing the coefficient amplitude of the noise in the gradient. As the mini-batch size goes to the size of the full data set, the coefficient of the stochastic term in <ref type="bibr" target="#b6">(7)</ref> vanishes <ref type="bibr" target="#b42">[43]</ref>. While stochastic gradient descent converges slower than gradient descent, i.e., without any stochasticity <ref type="bibr" target="#b58">[59]</ref>, in practice, deep networks are trained with SGD and obtain superior generalization performance only with small mini-batches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SGD in continuous time</head><p>Developments in this paper hinge upon interpreting SGD as a continuous-time stochastic differential equation. Along the lines of <ref type="bibr" target="#b26">[27]</ref>, we assume that the stochastic gradient ∇f mb (x k-1 ) in ( <ref type="formula" target="#formula_8">7</ref>) is unbiased with respect to the full gradient and has bounded variance, i.e., for all x ∈ R n ,</p><formula xml:id="formula_9">E ∇f mb (x) = ∇f (x), E ∇f mb (x) -∇f (x) 2 ≤ β -1 mb ;</formula><p>for some β -1 mb ≥ 0. We use the notation to emphasize that the noise is coming from the mini-batch gradients. The coefficient β -1 mb is zero if the mini-batch size is equal to the size of the dataset.</p><p>We omit the source of the noise and simply write β -1 while developing the theory; we will refer to the subscript mb again in Sect. 7 while providing algorithmic details. The discrete-time dynamics in <ref type="bibr" target="#b6">(7)</ref> can be modeled by the stochastic differential equation <ref type="bibr" target="#b42">[43]</ref> dx(t) = -∇f (x(t)) dt + β -1/2 dW (t).</p><p>2 For example, the ImageNet dataset <ref type="bibr" target="#b37">[38]</ref> has N = 1.25 million RGB images of size 224 × 224 (i.e., d ≈ 10 5 ) and K = 1000 distinct classes. A typical model, e.g., the Inception network [65] used for classification on this dataset has about N = 10 million parameters and is trained by running <ref type="bibr" target="#b6">(7)</ref> for k ≈ 10 5 updates; this takes roughly 100 hours with 8 graphics processing units (GPUs).</p><p>The generator, L , corresponding to (SGD) is defined for smooth functions φ as</p><formula xml:id="formula_10">L φ = -∇f • ∇ φ + β -1 2 φ. (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>The adjoint operator L * is given by</p><formula xml:id="formula_12">L * ρ = ∇ • (∇f ρ) + β -1 2 ρ. Given the function V (x) : X → R, define u(x, t) = E V (x(T )) x(t) = x (9)</formula><p>to be the expected value of V at the final time for the path (SGD) with initial data x(t) = x. By taking expectations in the Itô formula, it can be established that u satisfies the backward Kolmogorov equation</p><formula xml:id="formula_13">∂u ∂t = L u, for t &lt; s ≤ T,</formula><p>along with the terminal value u(x, T ) = V (x). Furthermore, ρ(x, t), the probability density of x(t) at time t, satisfies the Fokker-Planck equation <ref type="bibr" target="#b51">[52]</ref> </p><formula xml:id="formula_14">∂ ∂t ρ(x, t) = ∇ • ∇f (x) ρ(x, t) + β -1 2 ρ(x, t) ( F P )</formula><p>along with the initial condition ρ(x, 0) = ρ 0 (x), which represents the probability density of the initial distribution. With mild assumptions on f (x), in particular even if f is nonconvex, ρ(x, t) converges to the unique stationary solution of (FP) as t → ∞ [50, Section 4.5]. Elementary calculations show that the stationary solution is the Gibbs distribution</p><formula xml:id="formula_15">ρ ∞ (x; β) = Z(β) -1 e -βf (x) , (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>for a normalizing constant Z(β). The notation for the parameter β &gt; 0 comes from physics, where it corresponds to the inverse temperature. From <ref type="bibr" target="#b9">(10)</ref>, as β → ∞, the Gibbs distribution ρ ∞ concentrates on the global minimizers of f (x). Momentum is often used to accelerate convergence to this invariant distribution; this corresponds to running second-order Langevin dynamics <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">Chapter 6]</ref> given by</p><formula xml:id="formula_17">dx(t) = p(t) dt, c -1 dp(t) = -∇f (x(t)) + p(t) dt + β -1/2 dW (t),</formula><p>where c is the friction coefficient. In the over-damped limit c → ∞ we recover (SGD); see [64, Section 2.2.4]. The celebrated paper by Jordan et al. <ref type="bibr" target="#b33">[34]</ref> interpreted the Fokker-Planck equation as a gradient descent in the Wasserstein metric d W 2 <ref type="bibr" target="#b56">[57]</ref> of the energy functional</p><formula xml:id="formula_18">J (ρ) = V (x) ρ dx + β -1 2 ρ log ρ dx. If V (x) is λ-convex function f (x), i.e., ∇ 2 V (x)</formula><p>+ λI, is positive definite for all x, then the solution ρ(x, t) converges exponentially in the Wasserstein metric with rate λ to ρ ∞ <ref type="bibr" target="#b10">[11]</ref>,</p><formula xml:id="formula_19">d W 2 ρ(x, t), ρ ∞ ≤ d W 2 ρ(x, 0), ρ ∞ e -λt .</formula><p>Let us note that a less explicit, but more general, criterion for speed of convergence in the weighted L 2 norm is the Bakry-Emery criterion which uses PDE-based estimates of the spectral gap of the potential function <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Smoothing of the loss function</head><p>A number of models of deep neural networks have been used in the literature to analyze characteristics of the energy landscape. If one forgoes the nonlinearities, a deep network can be seen as a generalization of principal component analysis (PCA), i.e., as a matrix factorization problem; this is explored by Baldi and Hornik <ref type="bibr" target="#b5">[6]</ref>, Haeffele and Vidal <ref type="bibr" target="#b29">[30]</ref>, Soudry and Carmon [61], Saxe et al. <ref type="bibr" target="#b57">[58]</ref>, among others. Based on empirical results such as Dauphin et al. <ref type="bibr" target="#b18">[19]</ref>, authors in Bray and Dean <ref type="bibr" target="#b8">[9]</ref>, Fyodorov and Williams <ref type="bibr" target="#b25">[26]</ref>, Choromanska et al.</p><p>[17] and Chaudhari and Soatto <ref type="bibr" target="#b13">[14]</ref> have modeled the energy landscape of deep neural networks as a high-dimensional Gaussian random field. In spite of these analyses being drawn from vastly diverse areas of machine learning, they suggest that, in general, the energy landscape of deep networks is highly non-convex and rugged.</p><p>Smoothing is an effective way to improve the performance of optimization algorithms on such a landscape, and it is our primary focus in this paper. This can be done by convolution with a kernel <ref type="bibr" target="#b15">[16]</ref>; for specific architectures this can be done analytically <ref type="bibr" target="#b45">[46]</ref> or by averaging the gradient over random, local perturbations of the parameters <ref type="bibr" target="#b28">[29]</ref>. A recent paper by Li et al. <ref type="bibr" target="#b41">[42]</ref> suggests using extensive empirical analysis that the rugged energy landscape of deep architectures can be made smoother using residual architectures, e.g., by forcing the model to be an identity mapping <ref type="bibr" target="#b30">[31]</ref> if all the weights are zero. Our results provide a means to smoothen the energy landscape using new optimization algorithms for the same neural architecture. This paper compares and contrasts different optimization-based smoothing techniques in a unified mathematical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PDE interpretation of local entropy</head><p>Local entropy is a modified loss function first introduced as a means for studying the energy landscape of the discrete perceptron, i.e., a "shallow" neural network with one layer and discrete parameters, e.g., x ∈ {-1, 1} n <ref type="bibr" target="#b4">[5]</ref>. An analysis of local entropy using the replica method <ref type="bibr" target="#b44">[45]</ref> predicts dense clusters of solutions to the optimization problem <ref type="bibr" target="#b5">(6)</ref>. Parameters that lie within such clusters yield better error on samples outside the training set D, i.e., they have improved generalization error <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recent papers by Baldassi et al. <ref type="bibr" target="#b2">[3]</ref> and Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref> have suggested algorithmic methods to exploit the above observations. The later extended the notion of local entropy to continuous variables by replacing the loss function f (x) with</p><formula xml:id="formula_20">f γ (x) := u(x, γ ) = - 1 β log G β -1 γ * exp (-βf (x)) ;<label>(11)</label></formula><p>where</p><formula xml:id="formula_21">G γ (x) = (2πγ ) -d/2 exp -|x| 2 2γ</formula><p>is the heat kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Derivation of the viscous Hamilton-Jacobi PDE</head><p>The Cole-Hopf transformation [23, Section 4.4.1] is a classical tool used in PDEs. It relates solutions of the heat equation to those of the viscous Hamilton-Jacobi equation. For the convenience of the reader, we restate it here.</p><p>Lemma 1 (Cole-Hopf) The local entropy function f γ (x) = u(x, γ ) defined by <ref type="bibr" target="#b10">(11)</ref> is the solution of the initial value problem for the viscous Hamilton-Jacobi equation (viscous HJ) with initial values u(x, 0) = f (x). Moreover, the gradient is given by</p><formula xml:id="formula_22">∇u(x, t) = R n x -y t ρ ∞ 1 (dy; x) = R n ∇f (x -y) ρ ∞ 2 (dy; x), (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where ρ ∞ i (y; x) are probability distributions given by <ref type="formula" target="#formula_20">11</ref>), v = exp(-βu) solves the heat equation</p><formula xml:id="formula_24">ρ ∞ 1 (y; x) = Z -1 1 exp -βf (y) - β 2t x -y 2 , ρ ∞ 2 (y; x) = Z -1 2 exp -βf (x -y) - β 2t y 2 (<label>13</label></formula><formula xml:id="formula_25">)</formula><formula xml:id="formula_26">and Z i = Z i (x) are normalizing constants for i = 1, 2. Proof Define u(x, t) = -β -1 log v(x, t). From (</formula><formula xml:id="formula_27">v t = 1 2 β -1 v</formula><p>with initial data v(x, 0) = exp(-βf (x)). Taking partial derivatives gives</p><formula xml:id="formula_28">v t = -β v u t , ∇v = -β v ∇u, v = -β v u + β 2 v |∇u| 2 .</formula><p>Combining these expressions results in (viscous HJ). Differentiating v(x, t) = exp(-βu(x, t)) using the convolution in <ref type="bibr" target="#b10">(11)</ref>, gives up to positive constants which can be absorbed into the density,</p><formula xml:id="formula_29">∇u(x, t) = C ∇ x G t * e -βf (x) = C ∇ x G t (x -y) e -βf (y) dy = C ∇ x G t (y) e -βf (x-y) dy.</formula><p>Evaluating the last or second to last expression for the integral leads to the corresponding parts of ( <ref type="formula" target="#formula_22">12</ref>) and (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hopf-Lax formula for the Hamilton-Jacobi equation and dynamics for the gradient</head><p>In addition to the connection with (viscous HJ) provided by Lemma 1, we can also explore the non-viscous Hamilton-Jacobi equation. The Hamilton-Jacobi equation corresponds to the limiting case of (viscous HJ) as the viscosity term β -1 → 0. There are several reasons for studying this equation. It has a simple, explicit formula for the gradient. In addition, semi-concavity of the solution follows directly from the Hopf-Lax formula. Moreover, under certain convexity conditions, the gradient of the solution can be computed as an exponentially convergent gradient flow. The deterministic dynamics for the gradient are a special case of the stochastic dynamics for the viscous HJ equation which are discussed in another section. Let us therefore consider the Hamilton-Jacobi equation</p><formula xml:id="formula_30">u t = - 1 2 |∇u| 2 . (HJ)</formula><p>In the following lemma, we apply the well-known Hopf-Lax formula <ref type="bibr" target="#b22">[23]</ref> for the solution of the HJ equation. It is also called the inf -convolution of the functions f (x) and 1 2t |x| 2 <ref type="bibr" target="#b9">[10]</ref>. It is closely related to the proximal operator <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b53">54]</ref>.</p><formula xml:id="formula_31">0 1 8 ) 5 : 3 0 Lemma 2 Let u(x, t) be the viscosity solution of (HJ) with u(x, 0) = f (x). Then u(x, t) = inf y f (y) + 1 2t x -y 2 .</formula><p>(HL)</p><p>Define the proximal operator</p><formula xml:id="formula_32">prox tf (x) = arg min y f (y) + 1 2t x -y 2 . (<label>14</label></formula><formula xml:id="formula_33">)</formula><p>If y * = prox tf (x) is a singleton, then ∇ x u(x, t) exists, and</p><formula xml:id="formula_34">∇ x u(x, t) = x -y * t = ∇f (y * ), (<label>15</label></formula><formula xml:id="formula_35">)</formula><p>Proof The Hopf-Lax formula for the solution of the HJ equation can be found in <ref type="bibr" target="#b22">[23]</ref>. Danskin's theorem <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">Prop. 4.5.1]</ref> allows us to pass a gradient through an infimum, by applying it at the argminimum. The first equality is a direct application of Danskin's Theorem to (HL). For the second equality, rewrite (HL) with z = (xy)/t and the drop the prime, to obtain,</p><formula xml:id="formula_36">u(x, t) = inf z f (x -tz) + t 2 |z| 2 .</formula><p>Applying Danskin's theorem to the last expression gives</p><formula xml:id="formula_37">∇u(x, t) = ∇f (x -tz * ) = ∇f (y * ).</formula><p>Next we give a lemma which shows that under an auxiliary convexity condition, we can find ∇u(x, t) using exponentially convergent gradient dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 (Dynamics for HJ) For a fixed x, define h(x, y</head><formula xml:id="formula_38">; t) = f (x -ty) + t 2 y 2 .</formula><p>Suppose that t &gt; 0 is chosen so that h(x, y; t) is λ-convex as a function of y (meaning that D 2 y h(x, y, t) -λI is positive definite). The gradient p = ∇ x u(x, t) is then the unique steady solution of the dynamics</p><formula xml:id="formula_39">y (s) = -∇ y h(x, y(s); t) = -t y(s) -∇f (x -ty(s)) . (<label>16</label></formula><formula xml:id="formula_40">)</formula><p>Moreover, the convergence to p is exponential, i.e., y(s)p ≤ y(0)p e -λs .</p><p>Proof Note that ( <ref type="formula" target="#formula_39">16</ref>) is the gradient descent dynamics on h which is λ-convex by assumption. Thus, by standard techniques from ordinary differential equation (ODE) (see, for example <ref type="bibr" target="#b56">[57]</ref>) the dynamics are contraction, and the convergence is exponential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The invariant measure for a quadratic function</head><p>Local entropy computes a nonlinear average of the gradient in the neighborhood of x by weighing the gradients according to the steady-state measure ρ ∞ (y; x). It is useful to compute ∇f γ (x) when f (x) is quadratic, since this gives an estimate of the quasi-stable invariant measure near a local minimum of f . In this case, the invariant measure ρ ∞ (y; x) is a Gaussian distribution. <ref type="formula" target="#formula_24">13</ref>) is a normal distribution with mean μ and covariance given by</p><formula xml:id="formula_41">Lemma 4 Suppose f (x) is quadratic, with p = ∇ f (x) and Q = ∇ 2 f (x), then the invariant measure ρ ∞ 2 (y; x) of (</formula><formula xml:id="formula_42">μ = x -p and = (Q + γ I) -1 .</formula><p>In particular, for γ &gt; |Q| 2 , we have</p><formula xml:id="formula_43">μ ≈ x -γ -1 I -γ -2 Q p and ≈ γ -1 I -γ -2 Q, plus higher-order terms in γ .</formula><p>Proof Without loss of generality, a quadratic f (x) centered at x = 0 can be written as</p><formula xml:id="formula_44">f (x) = f (0) + p x + 1 2 x Qx, ⇒ f (x) + γ 2 x 2 = f (0) -μ p + 1 2 (x -μ) (Q + γ I) (x -μ)</formula><p>by completing the square. The distribution exp -f (y) - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Derivation of local entropy via homogenization of SDEs</head><p>The Hopf-Cole formula and the corresponding formula for the gradient in Lemma 1 involve integrals which, due to the curse of dimensionality, cannot be evaluated efficiently.</p><p>In this section, we obtain stochastic dynamics which through averaging (homogenization) give a computationally effective method to compute the gradient of the solution of the HJB PDE. The connection between the algorithm and the PDE was missing from Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition, the homogenization interpretation allows us to rigorously show that an algorithm called Elastic-SGD [66] that was heuristically connected to a distributed version of local entropy in Baldassi et al. <ref type="bibr" target="#b2">[3]</ref> is indeed equivalent to local entropy.</p><p>Homogenization is a technique used to analyze dynamical systems with multiple timescales that have a few fast variables that may be coupled with other variables which evolve slowly. Computing averages over the fast variables allows us to obtain averaged equations for the slow variables in the limit that the times scales separate. We refer the reader to Pavliotis and Stuart <ref type="bibr" target="#b50">[51,</ref><ref type="bibr">Chap. 10,</ref><ref type="bibr">17]</ref> or E <ref type="bibr" target="#b21">[22]</ref> for details. The convergence is strong: for rigorous convergence statements see Pavliotis and Stuart [51, Theorem 17.1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Background on homogenization</head><p>Consider the system of SDEs given by dx(s) = h(x, y) ds</p><formula xml:id="formula_45">dy(s) = 1 ε g(x, y) ds + 1 √ εβ dW (s);<label>(17)</label></formula><p>where h and g are sufficiently smooth functions, W (s) ∈ R n is the standard Wiener process, and ε &gt; 0 is the homogenization parameter which introduces a fast time-scale for the dynamics of y(s). Let us define the generator for the second equation to be</p><formula xml:id="formula_46">L 0 = g(x, y) • ∇ y + β -1 2 y .</formula><p>We can assume that, for fixed x, the fast-dynamics, y(s), has a unique invariant probability measure denoted by ρ ∞ (y; x) and the operator L 0 has a one-dimensional null-space characterized by</p><formula xml:id="formula_47">L 0 1(y) = 0, L * 0 ρ ∞ (y; x) = 0;</formula><p>here 1(y) are all constant functions in y and L * 0 is the adjoint of L 0 . In that case, it follows that in the limit ε → 0, the dynamics for x(s) in (17) converge to dX(s) = h(X) ds, where the homogenized vector field for X is defined as the average against the invariant measure. Moreover, by ergodicity, in the limit, the spatial average is equal to a long term average over the dynamics independent of the initial value y(0).</p><formula xml:id="formula_48">h(X) = h(X, y) ρ ∞ (dy; X) = lim T →∞ 1 T T 0 h(x, y(s)) ds.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Derivation of local entropy via homogenization of SDEs</head><p>Recall (12) of Lemma 1</p><formula xml:id="formula_49">∇f γ (x) = -γ -1 R n (x -y) ρ ∞ 1 (y; x) dy.</formula><p>Hence, let us consider the following system of SDEs</p><formula xml:id="formula_50">dx(s) = -γ -1 (x -y) ds, dy(s) = - 1 ε ∇f (y) + 1 γ (y -x) ds + β -1/2 √ ε dW (s). (Entropy-SGD) Write H(x, y; γ ) = f (y) + 1 2γ x -y 2 .</formula><p>The Fokker-Planck equation for the density of y(s) is given by</p><formula xml:id="formula_51">ρ t = L * 0 ρ = ∇ y • ∇ y Hρ + β -1 2 y ρ; (<label>18</label></formula><formula xml:id="formula_52">)</formula><p>The invariant measure for this Fokker-Planck equation is thus</p><formula xml:id="formula_53">ρ ∞ 1 (y; x) = Z -1 exp (-βH(x, y; γ )) ,</formula><p>which agrees with (13) in Lemma 1.</p><p>Theorem 5 As ε → 0, the system (Entropy-SGD) converges to the homogenized dynamics given by dX(s) = -∇f γ (X) ds.</p><formula xml:id="formula_54">Moreover, -∇f γ (x) = -γ -1 x -y where y = y ρ ∞ 1 (dy; X) = lim T →∞ 1 T T 0 y(s) ds, (<label>19</label></formula><formula xml:id="formula_55">)</formula><p>where y(s) is the solution of the second equation in (Entropy-SGD) for fixed x.</p><p>Proof The result follows immediately from the convergence result in Sect. 4.1 for the system (17). The homogenized vector field is</p><formula xml:id="formula_56">h(X) = -γ -1 (X -y) ρ ∞ 1 (y; X),</formula><p>which is equivalent to ∇f γ (X) from Lemma 1.</p><p>By Lemma 1, the following dynamics also converge to the gradient descent dynamics for</p><formula xml:id="formula_57">f γ . dx(s) = -∇f (x -y) ds dy(s) = - 1 ε y γ -∇f (x -y) ds + β -1/2 √ ε dW (s).</formula><p>(Entropy-SGD-2)</p><p>Remark 6 (Exponentially fast convergence) Theorem 5 relies upon the existence of an ergodic, invariant measure ρ ∞ 1 (y; x). Convergence to such a measure is exponentially fast if the underlying function, H(x, y; γ ), is convex in y; in our case, this happens if ∇ 2 f (x)+γ -1 I is positive definite for all x. f</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Elastic-SGD as local entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elastic</head><formula xml:id="formula_58">(x i ) + 1 2γ |x i -x| 2 . (<label>20</label></formula><formula xml:id="formula_59">)</formula><p>The formulation in <ref type="bibr" target="#b19">(20)</ref> lends itself easily to a distributed optimization where worker i performs the following update, which depends only on the average of the other workers</p><formula xml:id="formula_60">dy i (s) = -∇f (y i (s))ds - 1 γ (y i (s) -ȳ(s))ds + β -1/2 dW i (s). (<label>21</label></formula><formula xml:id="formula_61">)</formula><p>This algorithm was later shown to be connected to the local entropy loss f γ (x) by Baldassi et al. <ref type="bibr" target="#b2">[3]</ref> using arguments from replica theory. The results from the previous section can be modified to prove that the dynamics above converges to gradient descent of the local entropy. Consider the system dx(s) = 1 γ (xȳ) ds along with <ref type="bibr" target="#b20">(21)</ref>. As in Sect. 4.2, define H(x, y; γ ) =</p><formula xml:id="formula_62">n p i=1 f (y i ) + 1 2γ x -ȳ 2 .</formula><p>The invariant measure for the y(s) dynamics corresponding to the ε scaling of ( <ref type="formula" target="#formula_60">21</ref>) is given by</p><formula xml:id="formula_63">ρ ∞ (y; x) = Z -1 exp -β H(x, y; γ ) ,</formula><p>and the homogenized vector field is</p><formula xml:id="formula_64">h(X) = γ -1 X -ȳ ,</formula><p>where ȳ = ȳ ρ ∞ (dȳ; X). By ergodicity, we can replace the average over the invariant measure with a combination of the temporal average and the average over the workers</p><formula xml:id="formula_65">ȳ = lim T →∞ 1 T T 0 ȳ(s) ds. (<label>22</label></formula><formula xml:id="formula_66">)</formula><p>Thus, the same argument as in Theorem 5 applies, and we can conclude that the dynamics also converges to gradient descent for local entropy as n p → ∞.</p><p>Remark 7 (Variational Interpretation of the distributed algorithm) An alternate approach to understanding the distributed version of the algorithm, which we hope to study in the future, is modeling it as the gradient flow in the Wasserstein metric of a non-local functional. This functional is obtained by including an additional term in the Fokker-Planck functional,</p><formula xml:id="formula_67">J (ρ) = f γ (x)ρ dx + β -1 2 ρ log ρ dx + 1 2γ</formula><p>xy 2 ρ(x) ρ(y) dx dy; see <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Heat equation versus the viscous Hamilton-Jacobi equation</head><p>Lemma 1 showed that local entropy is the solution to the viscous Hamilton-Jacobi equation (viscous HJ). The homogenization dynamics in (Entropy-SGD) can thus be interpreted as a way of smoothing of the original loss f (x) to aid optimization. Other partial differential equations could also be used to achieve a similar effect. For instance, the following dynamics that corresponds to gradient descent on the solution of the the heat equation:</p><formula xml:id="formula_68">dx(s) = -∇f (x -y) ds dy(s) = - 1 εγ y ds + 1 √ εβ dW (s). (HEAT)</formula><p>Using a very similar analysis as above, the invariant distribution for the fast variables is ρ ∞ (dy; X) = G β -1 γ (y). In other words, the fast dynamics for y is decoupled from x and y(s) converges to a Gaussian distribution with mean zero and variance β -1 γ I. The homogenized dynamics is given by</p><formula xml:id="formula_69">dX(s) = -∇f (X) * G β -1 γ ds = -∇v(X, γ ),<label>(23)</label></formula><p>where v(x, γ ) is the solution of the heat equation v t = β -1 2 v with initial data v(x, 0) = f (x). The dynamics corresponds to a Gaussian averaging of the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 8 (Comparison between local entropy and heat equation dynamics)</head><p>The extra term in the y(s) dynamics for (Entropy-SGD-2) with respect to (HEAT) is exactly the distinction between the smoothing performed by local entropy and the smoothing performed by heat equation. The latter is common in the deep learning literature under different forms, e.g., Gulcehre et al. <ref type="bibr" target="#b28">[29]</ref> and Mobahi <ref type="bibr" target="#b45">[46]</ref>. The former version, however, has much better empirical performance (cf. experiments in Sect. 8 and Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref>) as well as improved convergence in theory (cf. <ref type="bibr">Theorem 12)</ref>. Moreover, at a critical point, the gradient term vanishes, and the operators (HEAT) and (Entropy-SGD-2) coincide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stochastic control interpretation</head><p>In this section, we consider the following controlled SDE dx(s) = -∇f (x(s)) ds -α(s) ds + β -1/2 dW (s), for t ≤ s ≤ T, (CSGD) along with x(t) = x. Controlled stochastic differential equations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">25]</ref> are generalizations of SDEs discussed earlier in Sect. 2.3. The use of stochastic control in this context is unusual; we are not aware of any works other than Li et al. <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Stochastic control for a variant of local entropy</head><p>For fixed controls α, the generator for (CSGD) is given by</p><formula xml:id="formula_70">L α := (-∇f -α) • ∇ + β -1 2 .</formula><p>Define the cost functional for a given solution x(•) of (CSGD) corresponding to the control α(•),</p><formula xml:id="formula_71">C (x(•), α(•)) = E V (x(T )) + 1 2 T t α(s) 2 ds . (<label>24</label></formula><formula xml:id="formula_72">)</formula><p>Here the terminal cost is a given function V : R n → R and we use the prototypical quadratic running cost. Define the value function</p><formula xml:id="formula_73">u(x, t) = min α(•) C (x(•), α(•)),</formula><p>to be the minimum expected cost, over admissible controls, and over paths which start at x(t) = x. From the definition, the value function satisfies</p><formula xml:id="formula_74">u(x, T ) = V (x).</formula><p>The dynamic programming principle expresses the value function as the unique viscosity solution of a Hamilton-Jacobi-Bellman (HJB) PDE <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">25]</ref>, u t = H(∇u, D 2 u) where the operator H is defined by</p><formula xml:id="formula_75">H(p, Q) = min α L α (p, Q) + 1 2 |α| 2 := min α (-∇f -α) • p + 1 2 |α| 2 + β -1 2 tr Q .</formula><p>The minimum is achieved at α = p, which results in</p><formula xml:id="formula_76">H(p, Q) = -∇f • p -1 2 |p| 2 + β -1</formula><p>2 tr Q. The resulting PDE for the value function is</p><formula xml:id="formula_77">-u t (x, t) = -∇f (x) • ∇u(x, t) - 1 2 ∇u(x, t) 2 + β -1 2 u(x, t), for t ≤ s ≤ T, (HJB)</formula><p>along with the terminal values u(x, T ) = V (x). We make note that in this case, the optimal control is equal to the gradient of the solution</p><formula xml:id="formula_78">α(x, t) = ∇u(x, t). (<label>25</label></formula><formula xml:id="formula_79">)</formula><p>Remark 9 (Forward-backward equations) Note that the PDE (HJB) is a terminal value problem in backwards time. This equation is well posed, and by relabeling time, we can obtain an initial value problem in forward time. The corresponding forward equation for the evolution is of a density under the dynamics (CSGD), involving the gradient of the value function. Together, these PDEs are the forward-backward system</p><formula xml:id="formula_80">-u t = -∇f • ∇u - 1 2 |∇u| 2 + β -1 2 u, ρ t = -∇ • ∇u ρ + ρ,<label>(26)</label></formula><p>for 0 ≤ s ≤ T along with the terminal and the initial data u(x, T ) = V (x), ρ(x, 0) = ρ 0 (x).</p><p>Remark 10 We could also obtain a stochastic control interpretation for (viscous HJ) by dropping the ∇f (x(s)) term in (CSGD). Then, by a similar argument, (HJB) results in (viscous HJ). Thus, we obtain the interpretation of solutions of (viscous HJ) as the value function of an optimal control problem, minimizing the cost function <ref type="bibr" target="#b23">(24)</ref> but with the modified dynamics. The reason for including the ∇f term in (CSGD) is that it allows us to prove the comparison principle in the next subsection.</p><p>Remark 11 (Mean field games) More generally, we can consider stochastic control problems where the control and the cost depend on the density of other players. In the case of "small" players, this can lead to mean field games <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>. In the distributed setting is it natural to consider couplings (control and costs) which depend on the density (or moments of the density). In this context, it may be possible to use mean field game theory to prove an analogue of Theorem 12 and obtain an estimate of the improvement in the terminal reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improvement in the value function</head><p>The optimal control interpretation above allows us to provide the following theorem for the improvement in the value function obtained by the dynamics (CSGD) using the optimal control (25), compared to stochastic gradient descent.</p><p>Theorem 12 Let x csgd (s) and x sgd (s) be solutions of (CSGD) and (SGD), respectively, with the same initial data x csgd (0) = x sgd (0) = x 0 . Fix a time t ≥ 0 and a terminal function, V (x). Then</p><formula xml:id="formula_81">E V (x csgd (t)) ≤ E V (x sgd (t)) - 1 2 E t 0 α(x csgd (s), s) 2 ds .</formula><p>The optimal control is given by α(x, t) = ∇u(x, t), where u(x, t) is the solution of (HJB) along with terminal data u(x, T ) = V (x).</p><p>Proof First consider (SGD) with generator L = -∇f (x) • ∇ + 1   2   given in <ref type="bibr" target="#b7">(8)</ref>. (Note the PDEs for stochastic are backward parabolic with terminal values. To be consistent with the rest of paper, we simply reverse time, which does not change the substance of the proof). The expected value function v(x, t) = E [V (x(t))] is the solution of the PDE v t = L v. Next, let u(x, t) be the solution of (HJB). The PDE (HJB) says</p><formula xml:id="formula_82">u t -L u - 1 2 |∇u| 2 = 0.</formula><p>so, in particular,</p><formula xml:id="formula_83">u t -L u ≥ 0.</formula><p>We also have u(x, T ) = v(x, T ) = V (x). By the maximum principle <ref type="bibr" target="#b22">[23]</ref> (possibly with additional technical assumptions as |x| → ∞), we can conclude</p><formula xml:id="formula_84">u(x, s) ≤ v(x, s), for all t ≤ s ≤ T ;<label>(27)</label></formula><p>Use the definition to get</p><formula xml:id="formula_85">v(x, t) = E [V (x(t)) | x(t) = x] ;</formula><p>the expectation is taken over the paths of (SGD). Similarly,</p><formula xml:id="formula_86">u(x, t) = E V (x(t)) + 1 2 t 0 |α| 2 ds</formula><p>over paths of (CSGD) with x(t) = x. Here α(•) is the optimal control. The interpretation along with <ref type="bibr" target="#b26">(27)</ref> gives the desired result. Note that the second term on the right hand side in Theorem 12 above can also be written as</p><formula xml:id="formula_87">1 2 E t 0 ∇u(x csgd (s), s) 2 ds .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Regularization, widening and semi-concavity</head><p>The PDE approaches discussed in Sect. 3 result in a smoother loss function. We exploit the fact that our regularized loss function is the solution of the viscous HJ equation. This PDE allows us to apply standard semi-concavity estimates from PDE theory <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> and quantify the amount of smoothing. Note that these estimates do not depend on the coefficient of viscosity, so they apply to the HJ equation as well. Indeed, semi-concavity estimates apply to solutions of PDEs in a more general setting which includes (viscous HJ) and (HJ). We begin with special examples which illustrate the widening of local minima. In particular, we study the limiting case of the HJ equation, which has the property that local minima are preserved for short times. We then prove semi-concavity and establish a more sensitive form of semi-concavity using the harmonic mean. Finally, we explore the relationship between wider robust minima and the local entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Widening for solutions of the Hamilton-Jacobi PDE</head><formula xml:id="formula_88">A function f (x) is semi-concave with a constant C if f (x) -C</formula><p>2 |x| 2 is concave, refer to Cannarsa and Sinestrari <ref type="bibr" target="#b9">[10]</ref>. Semi-concavity is a way of measuring the width of a local minimum: When a function is semi-concave with constant C, at a local minimum, no eigenvalue of the Hessian can be larger than C. The semi-concavity estimates below establish that near high curvature local minima widen faster than flat ones, when we evolve f by (viscous HJ).</p><p>It is illustrating to consider the following example.</p><p>Example 13 Let u(x, t) be the solution of (viscous HJ) with u(x, 0</p><formula xml:id="formula_89">) = c |x| 2 /2. Then u(x, t) = |x| 2 2(t + c -1 ) + β -1 n 2 log(t + c -1 ).</formula><p>In particular, in the non-viscous case, u(x, t) = |x| 2 2(t+c -1 ) .</p><p>Proof This follows from taking derivatives:</p><formula xml:id="formula_90">∇u(x, t) = x t + c -1 , |∇u| 2 2 = |x| 2 2(t + c -1 ) 2 , u(x, t) = n t + 1 , u t = - |x| 2 2(t + c -1 ) 2 + β -1 n 2 1 t + c -1 .</formula><p>In the previous example, the semi-concavity constant of u(x, t) is C(t) = 1/(c -1 + t) where c measures the curvature of the initial data. This shows that for very large c, the improvement is very fast, for example c = 10 8 leads to C(t = .01) ≈ 10, etc. In the sequel, we show how this result applies in general for both the viscous and the non-viscous cases. In this respect, for short times, solutions of (viscous HJ) and (HJ) widen faster than solutions of the heat equation.</p><p>Example 14 (Slower rate for the heat equation) In this example, we show that the heat equation can result in a very slow improvement of the semi-concavity. Let v 1 (x) be the first eigenfunction of the heat equation, with corresponding eigenvalue λ 1 , (for example v 1 (x) = sin(x)). In many cases λ 1 is of order 1. Then, since v(x, t) = exp(-λ 1 t) v 1 (x) is a solution of the heat equation, the seminconcavity constant for v(x, t) is C(t) = c 0 exp(-λ 1 t), where c 0 is the constant for v 1 (x). For short time, C(t) ≈ c 0 (1 -λ 1 t) which corresponds to a much slower decrease in C(t).</p><p>Another illustrative example is to study the widening of convex regions for the nonviscous Hamilton-Jacobi equation (HJ). As can be seen in Fig. <ref type="figure">1</ref>, solutions of the Hamilton-Jacobi have additional remarkable properties with respect to local minima. In particular, for small values of t (which depend on the derivatives of the initial function) local minima are preserved. To motivate these properties, consider the following examples. We begin with the Burgers equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 15 (Burgers equation in one dimension)</head><p>There is a well-known connection between Hamilton-Jacobi equations in one dimension and the Burgers equation <ref type="bibr" target="#b22">[23]</ref> which is the prototypical one-dimensional conservation law. Consider the solution u(x, t) of the non-viscous HJ equation (HJ) with initial value f (x) where x ∈ R. Let p(x, t) = u x (x, t). Then, p satisfies the Burgers equation</p><formula xml:id="formula_91">p t = -p p x .</formula><p>We can solve Burgers equation using the method of characteristics, for a short time, depending on the regularity of f (x). Set p(x, t) = u x (x, t). For short time, we can also use the fact that p(x, t) is the fixed point of ( <ref type="formula" target="#formula_39">16</ref>), which leads to p(x, t) = f (xtp(x, t)).</p><p>(</p><formula xml:id="formula_92">)<label>28</label></formula><p>In particular, the solutions of this equation remain classical until the characteristics intersect, and this time, t * , is determined by</p><formula xml:id="formula_93">t * f (x -pt * ) + 1 = 0, (<label>29</label></formula><formula xml:id="formula_94">)</formula><p>which recovers the convexity condition of Lemma 3.</p><p>Example 16 (Widening of convex regions in one dimension) In this example, we show that the widening of convex regions for one-dimensional displayed in Fig. <ref type="figure">1</ref> occurs in general. Let x be a local minimum of the smooth function f (x), and let 0 ≤ t &lt; t * , where the critical t * is defined by <ref type="bibr" target="#b28">(29)</ref>. Define the convexity interval as:</p><formula xml:id="formula_95">I(x, t) = the widest interval containing x where u(x, t) is convex Let I(x, 0) = [x 0 , x 1 ]</formula><p>, so that f (x) ≥ 0 on [x 0 , x 1 ] and f (x 0 ) = f (x 1 ) = 0. Then I(x, t) contains I(x, 0) for 0 ≤ t ≤ t * . Differentiating <ref type="bibr" target="#b27">(28)</ref>, and the solving for p x (x, t) leads to</p><formula xml:id="formula_96">p x = f (x -tp) (1 -tp x ) ⇒ p x = f (x -tp) 1 + t f (x -tp) .</formula><p>Since t ≤ t * , the last equation implies that p x has the same sign as f (xtp). Recall now that x 1 is an inflection point of f (x). Choose x such that x 1 = xtp(x, t), in other words, u xx (x, t) = 0. Note that x &gt; x 1 , since p(x, t) &gt; 0. Thus, the inflection point moves to the right of x 1 . Similarly, the inflection point x 0 moves to the left. So we have established that the interval is larger.</p><p>Remark 17 In the higher-dimensional case, well-known properties of the inf-convolution can be used to establish the following facts. If f (x) is convex, u(x, t) given by (HL) is also convex; thus, any minimizer of f also minimizes u(x, t). Moreover, for bounded f (x), using the fact that y * (x) -x = O( √ t), one can obtain a local version of the same result, viz., for short times t, a local minimum persists. Similar to the example in Fig. <ref type="figure">1</ref>, local minima in the solution vanish for longer times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Semi-concavity estimates</head><p>In this section, we establish semi-concavity estimates. These are well known and are included for the convenience of the reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 18 Suppose u(x, t) is the solution of (viscous HJ), and let</head><formula xml:id="formula_97">β -1 ≥ 0. If C k = sup x u x k x k (x, 0) and C Lap = sup x u(x, 0), 0 1 8 ) 5 : 3 0 Page 19 of 30 then sup x u x k x k (x, t) ≤ 1 C -1 k + t , and sup x u(x, t) ≤ 1 C -1 Lap + t/n . Proof 1.</formula><p>First consider the non-viscous case β -1 = 0. Define g(x, y) = f (y)</p><formula xml:id="formula_98">+ 1 2t</formula><p>xy 2 to be a function of x parametrized by y. Each g(x, y), considered as a function of x is semi-concave with constant 1/t. Then, u(x, t), the solution of (HJ), which is given by the Hopf-Lax formula (HL) is expressed as the infimum of such functions. Thus u(x, t) is also semi-concave with the same constant, 1/t. 2. Next, consider the viscous case β -1 &gt; 0. Let w = u x k x k , differentiating (viscous HJ) twice gives</p><formula xml:id="formula_99">w t + ∇u • ∇w + n i=1 u 2 x i x k = β -1 2 w. (<label>30</label></formula><formula xml:id="formula_100">)</formula><p>Using u 2 x k x k = w 2 , we have</p><formula xml:id="formula_101">w t + ∇u • ∇w - β -1 2 w ≤ -w 2 .</formula><p>Note that w(x, 0) ≤ C k and the function</p><formula xml:id="formula_102">φ(x, t) =<label>1</label></formula><p>C -1 k + t is a spatially independent super-solution of the preceding equation with w(x, 0) ≤ φ(x, 0). By the comparison principle applied to w and φ, we have</p><formula xml:id="formula_103">w(x, t) = u x k x k (x, t) ≤<label>1</label></formula><p>C -1 k + t for all x and for all t ≥ 0, which gives the first result. Now set v = u and sum <ref type="bibr" target="#b29">(30)</ref> over all k to obtain</p><formula xml:id="formula_104">v t + ∇u • ∇v + n i,j=1 u 2 x i x j = β -1 2 v.</formula><p>Thus,</p><formula xml:id="formula_105">v t + ∇u • ∇v - β -1 2 v ≤ - 1 n v 2</formula><p>by the Cauchy-Schwartz inequality. Now apply the comparison principle to v (x, t) =</p><formula xml:id="formula_106">C -1 Lap + t/n -1</formula><p>to obtain the second result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Estimates on the harmonic mean of the spectrum</head><p>Our semi-concavity estimates in Sect. 6.2 gave bounds on sup x u x k x k (x, t) and the Laplacian sup x u(x, t). In this section, we extend the above estimates to characterize the eigenvalue spectrum more precisely. Our approach is motivated by experimental evidence in Chaudhari et al. [13, Figure <ref type="figure">1</ref>] and Sagun et al. <ref type="bibr" target="#b55">[56]</ref>. The authors found that the Hessian of a typical deep network at a local minimum discovered by SGD has a very large proportion of its eigenvalues that are close to zero. This trend was observed for a variety of neural network architectures, datasets and dimensionality. For such a Hessian, instead of bounding the largest eigenvalue or the trace (which is also the Laplacian), we obtain estimates of the harmonic mean of the eigenvalues. This effectively discounts large eigenvalues and we get an improved estimate of the ones close to zero that dictate the width of a minimum. The harmonic mean of a vector x ∈ R n is</p><formula xml:id="formula_107">HM(x) = 1 n n i=1 1 x i -1</formula><p>.</p><p>The harmonic mean is more sensitive to small values as seen by</p><formula xml:id="formula_108">min i x i ≤ HM(x) ≤ n min i x i .</formula><p>which does not hold for the arithmetic mean. To give an example, the eigenspectrum of a typical network [13, Figure <ref type="figure">1</ref>] has an arithmetic mean of 0.0029, while its harmonic mean is ≈ 10 -10 which better reflects the large proportion of almost-zero eigenvalues.</p><p>Lemma 19 If is the vector of eigenvalues of the symmetric positive definite matrix A ∈ R n×n and D = (a 11 , . . . , a nn ) is its diagonal vector,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HM( ) ≤ HM(D).</head><p>Proof Majorization is a partial order on vectors with nonnegative components [44, Chap. 3] and for two vectors x, y ∈ R n , we say that x is majorized by y and write x y if x = Sy for some doubly stochastic matrix S. The diagonal of a symmetric, positive semi-definite matrix is majorized by the vector of its eigenvalues <ref type="bibr">[60]</ref>. For any Schur-concave function f (x), if x y we have</p><formula xml:id="formula_109">f (y) ≤ f (x)</formula><p>from Marshall et al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr">Chap. 9]</ref>. The harmonic mean is also a Schur-concave function which can be checked using the Schur-Ostrowski criterion</p><formula xml:id="formula_110">(x i -x j ) ∂ HM ∂x i - ∂ HM ∂x j ≤ 0 for all x ∈ R n +</formula><p>and we therefore have HM( ) ≤ HM(D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 20 If u(x, t) is a solution of (viscous HJ) and C</head><formula xml:id="formula_111">∈ R n is such that sup x u x k x k (x, t) ≤ C k for all k ≤ n, then at a local minimum x * of u(x, t), HM( ) ≤ 1 t + HM(C) -1</formula><p>where is the vector of eigenvalues of ∇ 2 u(x * , t).</p><p>Proof For a local minimum x * , we have HM(L) ≤ HM(D) from the previous lemma; here D is the diagonal of the Hessian ∇ 2 u(x min , t). From Lemma 18, we have</p><formula xml:id="formula_112">u x k x k (x * , t) ≤ C k (t) = 1 t + C -1 k .</formula><p>The result follows by observing that</p><formula xml:id="formula_113">HM(C(t)) = n n i=1 C k (t) -1 = 1 t + HM(C) -1 . 0 1 8 ) 5 : 3 0</formula><p>Page 21 of 30</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Algorithmic details</head><p>In this section, we compare and contrast the various algorithms in this paper and provide implementation details that are used for the empirical validation are provided in Sect. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Discretization of (Entropy-SGD)</head><p>The time derivative in the equation for y(s) in (Entropy-SGD) is discretized using the Euler-Maruyama method with time step (learning rate) η y as</p><formula xml:id="formula_114">y k+1 = y k -η y ∇f (y k ) + y k -x k γ + η y β -1 ε k , (<label>31</label></formula><formula xml:id="formula_115">)</formula><p>where ε k are zero mean, unit variance Gaussian random variables.</p><p>In practice, we do not have access to the full gradient ∇f (x) for a deep network and instead only have the gradient over a mini-batch ∇f mb (x) (cf. Sect. 2.2). In this case, there are two potential sources of noise: the coefficient β -1 mb arising from the mini-batch gradient, and any amount of extrinsic noise, with coefficient β -1 ex . Combining these two sources leads to the equation</p><formula xml:id="formula_116">y k+1 = y k -η y ∇f mb (y k ) + y k -x k γ + η y β -1 ex ε k ,<label>(32)</label></formula><p>which corresponds to <ref type="bibr" target="#b30">(31)</ref> with</p><formula xml:id="formula_117">β -1/2 = β -1/2 mb + β -1/2 ex .</formula><p>We initialize y k = x k if k/L is an integer. The number of time steps taken for the y variable is set to be L. This corresponds to ε = 1/L in (17) and to T = 1 in <ref type="bibr" target="#b18">(19)</ref>. This results in a discretization of the equation for x in (Entropy-SGD) given by</p><formula xml:id="formula_118">x k+1 = ⎧ ⎨ ⎩ x k -η γ -1 x k -y k if k/L is an integer, x k otherwise;<label>(33)</label></formula><p>Since we may be far from the limit ε → 0, T → ∞, instead of using the linear averaging for y in ( <ref type="formula" target="#formula_54">19</ref>), we will use a forward looking average. Such an averaging gives more weight to the later steps which is beneficial for large values of γ when the invariant measure ρ ∞ (y; x) in <ref type="bibr" target="#b18">(19)</ref> does not converge quickly. The two algorithms we describe will differ in the choices of L and β -1 ex and the definition of y . For Entropy-SGD, we set L = 20, β -1 ex = 10 8 , and set y to be</p><formula xml:id="formula_119">y k+1 = α y k + (1 -α) y k+1 ; y k is re-initialized to x k if k/L is an integer.</formula><p>The parameter α is used to perform an exponential averaging of the iterates y k . We set α = 0.75 for experiments in Sect. 8 using Entropy-SGD. This is equivalent to the original implementation of Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>The step size for the y k dynamics is fixed to η y = 0.1. It is a common practice to perform step size annealing while optimizing deep networks (cf. Sect. 2.2), i.e., we set η = 0.1 and reduce it by a factor of 5 after every 3 epochs (iterations on the entire dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Non-viscous Hamilton-Jacobi (HJ)</head><p>For the other algorithm which we denote as HJ, we set L = 5 and β -1 ex = 0, and set y k = y k in ( <ref type="formula" target="#formula_116">32</ref>) and <ref type="bibr" target="#b32">(33)</ref>, i.e., no averaging is performed and we simply take the last iterate of the y k updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 21</head><p>We can also construct an equivalent system of updates for HJ by discretizing (Entropy-SGD-2) and again setting β -1 ex = 0; this exploits the two different ways of computing the gradient in Lemma 1 and gives</p><formula xml:id="formula_120">y k+1 = 1 -γ -1 η y y k + η y ∇f mb (x k -y k ) x k+1 = ⎧ ⎨ ⎩ x k -η ∇f mb (x k -y k ) if k/L is an integer, x k else;<label>(34)</label></formula><p>and we initialize y k = 0 if k/L is an integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Heat equation</head><p>We perform the update corresponding to ( <ref type="formula" target="#formula_69">23</ref>)</p><formula xml:id="formula_121">x k+1 = x k - η L L i=1 ∇f mb (x k + ε i ) ,<label>(35)</label></formula><p>where ε i are Gaussian random variables with zero mean and variance γ I. Note that we have implemented the convolution in <ref type="bibr" target="#b22">(23)</ref> as an average over Gaussian perturbations of the parameters x. We again set L = 20 for our experiments in Sect. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Momentum</head><p>Momentum is a popular technique for accelerating the convergence of SGD <ref type="bibr" target="#b47">[48]</ref>. We employ this in our experiments by maintaining an auxiliary variable z k which intuitively corresponds to the velocity of x k . The x k update in ( <ref type="formula" target="#formula_118">33</ref>) is modified to be</p><formula xml:id="formula_122">x k+1 = z k -η ∇f mb ((z k -y k ), z k+1 = x k + δ x k -x k-L</formula><p>if k/L is an integer; x k and z k are left unchanged otherwise. The other updates for x k in ( <ref type="formula" target="#formula_120">34</ref>) and ( <ref type="formula" target="#formula_121">35</ref>) are modified similarly. The momentum parameter δ = 0.9 is fixed for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Scoping and tuning γ</head><p>Scoping involves setting γ to a large value and reducing it to zero as training progresses.</p><p>From the point of view of regularization, sending γ to zero at the end of the algorithm makes sense, since it allows us to recover the true loss function, indeed f γ (x) → f (x) as γ → 0. On the other hand, for large values of γ , in practice, the algorithm does not correctly compute the gradient of u. However, once γ is small enough to make the auxiliary function H strongly convex, then, as explained in Remark 6, the algorithm converges exponentially fast. For the updates in Sects we pick γ 0 ∈ [10 -4 , 10 -1 ] so as to obtain the best validation error while γ 1 is fixed to 10 -3 for all experiments. We have obtained similar results by normalizing γ 0 by the dimensionality of x, i.e., if we set γ 0 := γ 0 /n, this parameter becomes insensitive to different datasets and neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Empirical validation</head><p>We now discuss experimental results on deep neural networks that demonstrate that the PDE methods considered in this article achieve good regularization, aid optimization and lead to improved classification on modern datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Setup for deep networks</head><p>In machine learning, one typically splits the data into three sets: (i) the training set D, used to compute the optimization objective (empirical loss f ), (ii) the validation set, used to tune parameters of the optimization process which are not a part of the function f , e.g., mini-batch size, step size, number of iterations, and (iii) a test set, used to quantify generalization as measured by the empirical loss on previously unseen (sequestered) data. However, for the benchmark datasets considered here, it is customary in the literature to not use a separate test set. Instead, one typically reports test error on the validation set itself; for the sake of enabling direct comparison of numerical values, we follow the same practice here.</p><p>We use two standard computer vision datasets for the task of image classification in our experiments. The MNIST dataset <ref type="bibr" target="#b40">[41]</ref> contains 70,000 gray-scale images of size 28 × 28, each image portraying a hand-written digit between 0 to 9. This dataset is split into 60,000 training images and 10,000 images in the validation set. The CIFAR-10 dataset <ref type="bibr" target="#b36">[37]</ref> contains 60,000 RGB images of size 32 × 32 of 10 objects (aeroplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck). The images are split as 50,000 training images and 10,000 validation images. Figures <ref type="figure" target="#fig_5">2</ref> and<ref type="figure">3</ref> show a few example images from these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Pre-processing</head><p>We do not perform any pre-processing for the MNIST dataset. For CIFAR-10, we perform a global contrast normalization <ref type="bibr" target="#b17">[18]</ref> followed by a ZCA whitening transform (sometimes also called the "Mahalanobis transformation") <ref type="bibr" target="#b36">[37]</ref>. We use ZCA as opposed to principal component analysis (PCA) for data whitening because the former preserves visual characteristics of natural images unlike the latter; this is beneficial for convolutional neural networks. We do not perform any data augmentation, i.e., transformation of input images by mirror flips, translations, or other transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Loss function</head><p>We defined the zero-one loss f i (x) for a binary classification problem in Sect. 2.1. For classification tasks with K classes, it is common practice to construct a network with K distinct outputs. The output y(x; ξ i ) is thus a vector of length K that is further normalized to sum up to 1 using the softmax operation defined in Sect. 8.2. The cross-entropy loss typically used for optimizing deep networks is defined as:</p><formula xml:id="formula_123">f i (x) := - K k=1 1 {y i =k} log y(x; ξ i ) k .</formula><p>We use the cross-entropy loss for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Training procedure</head><p>We run the following algorithms for each neural network; their algorithmic details are given in Sect. 7. The ordering below corresponds to the ordering in the figures.</p><p>• Entropy-SGD: the algorithm of Chaudhari et al. <ref type="bibr" target="#b12">[13]</ref> described in Sect. 7.1, • HEAT: smoothing by the heat equation described in Sect. 7.3, • HJ: updates for the non-viscous Hamilton-Jacobi equation described in Sect. 7.2, • SGD: stochastic gradient descent given by ( <ref type="formula" target="#formula_8">7</ref>), All the above algorithms are stochastic in nature; in particular, they sample a mini-batch of images randomly from the training set at each iteration before computing the gradient. A significant difference between Entropy-SGD and HJ is that the former adds additional noise, while the latter only uses the intrinsic noise from the mini-batch gradients. We therefore report and plot the mean and standard deviation across 6 distinct random seeds for each experiment. The mini-batch size (|i t | in <ref type="bibr" target="#b6">(7)</ref>) is fixed at 128. An "epoch" is defined to be one pass over the entire dataset; for instance, it consists of 60,000/128 = 469 iterations of <ref type="bibr" target="#b6">(7)</ref> for MNIST with each mini-batch consisting of 128 different images.</p><p>We use SGD as a baseline for comparing the performance of the above algorithms in all our experiments. Remark 22 (Number of gradient evaluations) A deep network evaluates the average gradient over an entire mini-batch of 128 images in one single back-propagation operation <ref type="bibr" target="#b54">[55]</ref>. We define the number of back-props per weight update by L. SGD uses one back-prop per update (i.e., L = 1) while we use L = 20 back-props for Entropy-SGD (cf. Sect. 7.1), L = 5 back-props for the heat equation (cf. Sect. 7.3), and L = 5 back-props for the non-viscous HJ equation (cf. Sect. 7.2). For each of the algorithms, we plot the training loss or the validation error against the number of "effective epochs," i.e., the number of epochs is multiplied by L which gives us a uniform scale to measure the computational efficiency of various algorithms. This is a direct measure of the wall-clock time and is agnostic to the underlying hardware and software implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Fully connected network</head><p>Consider a "fully connected" network on MNIST mnistfc : input 784 → drop 0.2 → fc 1024 → drop 0.2 ×2 → fc 10 → softmax.</p><p>The input layer reshapes each MNIST image as a vector of size 784. The notation fc d denotes a "fully connected" dense matrix x k in (4) with d output dimensions, followed by the ReLU nonlinearity (cf. Sect. 2.1) and an operation known as batch normalization <ref type="bibr" target="#b32">[33]</ref> which whitens the output of each layer by subtracting the mean across a mini-batch and dividing by the standard deviation. The notation drop p denotes the dropout layer [63] which randomly sets a fraction p of the weights to zero at each iteration; this is a popular regularization technique in deep learning, see Kingma et al. <ref type="bibr" target="#b35">[36]</ref>, Achille and Soatto <ref type="bibr" target="#b0">[1]</ref> for a Bayesian perspective. For 10 classes in the classification problem, we create an output vector of length 10 in the last fc layer followed by softmax which picks the largest element of this vector, which is interpreted as the output (or "prediction") by this network. The mnistfc network has n = 1.86 million parameters.</p><p>As Fig. <ref type="figure" target="#fig_4">4a</ref> and Table <ref type="table" target="#tab_0">1</ref> show, the final validation error for all algorithms is quite similar. The convergence rate varies; in particular, Entropy-SGD converges fastest in this case. Note that mnistfc is a small network and the difference in the performance of the above algorithms, e.g., 1.08 % for Entropy-SGD versus 1.17 % for HJ, is minor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a b</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">LeNet</head><p>Our second network for MNIST is a convolutional neural network (CNN) denoted as follows:</p><p>LeNet The results for LeNet are described in Fig. <ref type="figure" target="#fig_4">4b</ref> and Table <ref type="table" target="#tab_0">1</ref>. This is a convolutional neural network and performs better than mnistfc on the same dataset. The final validation error is very similar for all algorithms at 0.50 % with the exception of the heat equation which only reaches 0.59 %. We can also see that the other algorithms converge quickly, in about half the number of effective epochs as that of mnistfc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">CIFAR</head><p>The CIFAR-10 dataset is more complex than MNIST and fully connected networks typically perform poorly. We will hence employ a convolutional network for this dataset. We use the All-CNN-C architecture introduced by Springenberg et al. The final convolutional layer in the above block denoted by conv * is different from others; while they perform convolution at every pixel otherwise known as a "stride" of 1 pixel, conv * , on the other hand, uses a stride of 2; this results in the image being down-sampled by a factor of two. Note that conv c,k,m with m = 1 does not result in any down-sampling. Max-pooling usually results in a drastic reduction of the image size and by replacing it with a strided convolution, All-CNN achieves improved performance with much fewer parameters than many other networks on CIFAR-10. The final layer denoted as mean-pool takes an input of size 10 × 8 × 8 and computes the spatial average to obtain an output vector of size 10. This network has n = 1.67 million parameters.  The first two rows correspond to low-dimensional problems. In these cases, all algorithms obtain comparable, state-of-the-art validation error; Entropy-SGD is significantly faster at training mnistfc while SGD is slightly faster for LeNet.</p><p>In the largest problem on CIFAR-10, HJ has the best validation error and is the fastest Best values are highlighted in bold has the lowest training cross-entropy loss of 0.046. Note that both HJ and Entropy-SGD converge faster than SGD. The heat equation performs again poorly on this dataset and has a much higher validation error than others (9.04%). This is consistent with our discussion in Sect. 4.4 which suggests that the viscous or non-viscous HJ equations result in better smoothing than the heat equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>Our results apply nonlinear PDEs, stochastic optimal control and stochastic homogenization to the analysis of two recent and effective algorithms for the optimization of neural networks. Our analysis also leads to new and improved algorithms for non-convex optimization.</p><p>We replaced the standard stochastic gradient descent (SGD) algorithm for the function f (x), with SGD on the two-variable function H(x, y; γ ) = f (y) + γ -1 |x -y| 2 /2, along with a small parameter ε (Entropy-SGD). Using the new system, in the limit ε → 0, we can provably and quantitatively improve the expected value of the original loss function. The effectiveness of our algorithm comes from the connection, via homogenization, of system of SDEs to the gradient of the function u(x, t) which is the solution of the (viscous HJ) PDE with initial data f (x). The function H is more convex in y that the original function f . The convexity of H in y is related to the exponentially fast convergence of the dynamics, a connection explained by the celebrated gradient flow interpretation of Fokker-Planck equation <ref type="bibr" target="#b33">[34]</ref>.</p><p>Section 4 shows that minimizing local entropy is equivalent to the influential distributed algorithm Elastic-SGD [66] if the underlying stochastic dynamics is ergodic. This insight may lead to new distributed algorithms which are presently of great interest <ref type="bibr" target="#b11">[12]</ref>.</p><p>On a practical level, training deep neural networks with a large number of hyperparameters is very costly, in terms of both human effort and computer time. Our analysis leads to better understanding of the parameters involved in the algorithms and provides an insight into the choice of hyper-parameters for these methods. In particular, (i) the parameter γ is now understood as the time t, in the PDE (viscous HJ), (ii) scoping of γ , which was seen as a heuristic, can now be interpreted as sending a regularization parameter to zero, (iii) we set the extrinsic noise parameter β -1 ex to zero, resulting in a simpler, more effective algorithm, and (iv) we now understand that below a critical value of γ (which is determined by the requirement that H be convex in y), the convergence of the dynamics in (Entropy-SGD) to the invariant measure is exponentially fast.</p><p>While simulated annealing and related methods work by modulating the level of noise in the dynamics, our algorithm works by modulating the smoothness of the underlying loss function. While most algorithms used in deep learning derive their motivation from the literature on convex optimization, the algorithms we have presented here are specialized to non-convex loss functions and have been shown to perform well on these problems both in theory and in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>∂ 2 ∂x 2 iu,</head><label>2</label><figDesc>t ≤ γ (viscous HJ) along with initial values u(x, 0) = f (x). Here we write u = n i=1 for the Laplacian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 2γ y 2</head><label>2</label><figDesc>is thus a normal distribution with mean μ = xp and variance = (Q + γ I) -1 and an appropriate normalization constant. The approximation = γ -1 I -γ -2 Q which avoids inverting the Hessian follows from the Neumann series for the matrix inverse; it converges for γ &gt; |Q| 2 and is accurate up to O γ -3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-SGD is an algorithm introduced by Zhang et al.[66]  for distributed training of deep neural networks and aims to minimize the communication overhead between a set of workers that together optimize replicated copies of the original function f (x). For n p &gt; 0 distinct workers, Let y = (y 1 , . . . , y n p )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 MNISTFig. 3</head><label>23</label><figDesc>Fig. 2 MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 mnistfc and LeNet on MNIST (best seen in color). a mnistfc: Validation error. b LeNet: Validation error</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 →</head><label>2</label><figDesc>: input 28×28 → conv 20,5,3 → drop 0.25 → conv 50,5,drop 0.25 → fc 500 → drop 0.25 → fc 10 → softmax.The notation conv c,k,m denotes a 2D convolutional layer with c output channels, each of which is the sum of a channel-wise convolution operation on the input using a learnable kernel of size k × k. It further adds ReLU nonlinearity, batch normalization and an operation known as max-pooling which down-samples the image by picking the maximum over a patch of size m × m. Convolutional networks typically perform much better than fully connected ones on image classification task despite having fewer parameters, LeNet has only n = 131, 220 parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 → conv 192, 3 ×2→</head><label>33</label><figDesc>[62] and add batch normalization: All-CNN : input 3×32×32 → drop 0.2 → block 96,3 → block 192,conv 10 → mean-pool 10 → softmax. where block d,3 : conv d,3,1 → conv d,3,1 → conv * d,3,1 → drop 0.5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Training loss and validation error on CIFAR-10 (best seen in color). a All-CNN: Training loss. b All-CNN: Validation error</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Summary of experimental results: Validation error (%) @ Effective epochs</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Entropy-SGD</cell><cell>HEAT</cell><cell>HJ</cell><cell>SGD</cell></row><row><cell>mnistfc</cell><cell>1.08 ± 0.02 @ 120</cell><cell>1.13 ± 0.02 @ 200</cell><cell>1.17 ± 0.04 @ 200</cell><cell>1.10 ± 0.01 @ 194</cell></row><row><cell>LeNet</cell><cell>0.5 ± 0.01 @ 80</cell><cell>0.59 ± 0.02 @ 75</cell><cell>0.5 ± 0.01 @ 70</cell><cell>0.5 ± 0.02 @ 67</cell></row><row><cell>All-CNN</cell><cell>7.96 ± 0.05 @ 160</cell><cell>9.04 ± 0.04 @ 150</cell><cell>7.89 ± 0.07 @ 145</cell><cell>7.94 ± 0.06 @ 195</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegements</head><p>AO is supported by a grant from the Simons Foundation (395980); PC and SS by ONR N000141712072, AFOSR FA95501510229 and ARO W911NF151056466731CS; SO by ONR N000141410683, N000141210838, N000141712162 and DOE DE-SC00183838. AO would like to thank the hospitality of the UCLA mathematics department where this work was completed.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information dropout: learning optimal representations through noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01353</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusions hypercontractives</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bakry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Émery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Séminaire de Probabilités XIX 1983/84</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="177" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unreasonable effectiveness of learning neural networks: from accessible states and robust ensembles to basic algorithmic schemes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingrosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucibello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="7655" to="E7662" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingrosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucibello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">128101</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local entropy as a measure for sampling solutions in constraint satisfaction problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingrosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucibello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech. Theory Exp</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23301</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: learning from examples without local minima</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ozdaglar</surname></persName>
		</author>
		<title level="m">Convex analysis and optimization</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04838</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistics of critical points of Gaussian fields on large-dimensional spaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">150201</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Cannarsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sinestrari</surname></persName>
		</author>
		<title level="m">Semiconcave Functions, Hamilton-Jacobi Equations, and Optimal Control</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contractions in the 2-wasserstein length space and thermalization of granular media</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Ration. Mech. Anal</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="263" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00424</idno>
		<title level="m">Parle: parallelizing stochastic gradient descent</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01838</idno>
		<title level="m">Entropy-SGD: biasing gradient descent into wide valleys</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the energy landscape of deep networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06485</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11029</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Smoothing methods for nonsmooth, nonconvex minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Arbor</title>
		<imprint>
			<biblScope unit="volume">1001</biblScope>
			<biblScope unit="issue">48109</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<title level="m">Principles of Multiscale Modeling</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graduate Studies in Mathematics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Partial Differential Equations</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Rishel</surname></persName>
		</author>
		<title level="m">Deterministic and sTochastic Optimal Control</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Soner</surname></persName>
		</author>
		<title level="m">Markov Processes and Viscosity Solutions</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fyodorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Phys</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1081" to="1116" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: training Imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Noisy activation functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Global optimality in tensor factorization, deep learning, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07540</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large population stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Malhamé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="252" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The variational formulation of the Fokker-Planck equation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Math. Anal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean field games</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lasry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jpn. J. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="260" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09913</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06251</idno>
		<title level="m">Stochastic modified equations and adaptive stochastic gradient algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Olkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Arnold</surname></persName>
		</author>
		<title level="m">Inequalities: Theory of Majorization and Its Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Virasoro</surname></persName>
		</author>
		<title level="m">Spin Glass Theory and Beyond: An Introduction to the Replica Method and Its Applications</title>
		<imprint>
			<publisher>World Scientific Publishing Company</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks by diffusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04114</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Proximité et dualité dans un espace hilbertien</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Soc. Math. Fr</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="273" to="299" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o (1/k2)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sov. Math. Dokl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convergent difference schemes for degenerate elliptic and parabolic equations: Hamilton-Jacobi equations and free boundary problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="879" to="895" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Pavliotis</surname></persName>
		</author>
		<title level="m">Stochastic Processes and Applications</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Pavliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuart</surname></persName>
		</author>
		<title level="m">Multiscale Methods: Averaging and Homogenization</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fokker-Planck equation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Risken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fokker-Planck Equation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="63" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monotone operators and the proximal point algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Control Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="898" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Model</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07476</idno>
		<title level="m">Singularity of the hessian in deep learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Santambrogio</surname></persName>
		</author>
		<title level="m">Optimal Transport for Applied Mathematicians</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Birkäuser</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="112" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
