<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Combining Inductive Logic Programming with Bayesian Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">Machine Learning Lab Albert-Ludwigs-University</orgName>
								<address>
									<addrLine>Georges-Köhler-Allee</addrLine>
									<postCode>079, D-79085</postCode>
									<settlement>Gebäude, Freiburg i. Brg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><surname>De Raedt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">Machine Learning Lab Albert-Ludwigs-University</orgName>
								<address>
									<addrLine>Georges-Köhler-Allee</addrLine>
									<postCode>079, D-79085</postCode>
									<settlement>Gebäude, Freiburg i. Brg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Combining Inductive Logic Programming with Bayesian Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD78C8E3FB3AEF01F22C3584F1984B62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, new representation languages that integrate first order logic with Bayesian networks have been developed. Bayesian logic programs are one of these languages. In this paper, we present results on combining Inductive Logic Programming (ILP) with Bayesian networks to learn both the qualitative and the quantitative components of Bayesian logic programs. More precisely, we show how to combine the ILP setting learning from interpretations with score-based techniques for learning Bayesian networks. Thus, the paper positively answers Koller and Pfeffer's question, whether techniques from ILP could help to learn the logical component of first order probabilistic models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an increasing interest in integrating probability theory with first order logic. One of the research streams <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref> aims at integrating two powerful and popular knowledge representation frameworks: Bayesian networks <ref type="bibr" target="#b22">[23]</ref> and first order logic. In 1997, Koller and Pfeffer <ref type="bibr" target="#b15">[16]</ref> address the question "where do the numbers come from?" for such frameworks. At the end of the same paper, they raise the question whether techniques from inductive logic programming (ILP) could help to learn the logical component of first order probabilistic models. In <ref type="bibr" target="#b14">[15]</ref> we suggested that the ILP setting learning from interpretations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref> is a good candidate for investigating this question. With this paper we would like to make our suggestions more concrete. We present a novel scheme to learn intensional clauses within Bayesian logic programs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. It combines techniques from ILP with techniques for learning Bayesian networks. More exactly, we will show that the learning from interpretations setting for ILP can be integrated with score-based Bayesian network learning techniques for learning Bayesian logic programs. Thus, we positively answer Koller and Pfeffer's question.</p><p>We proceed as follows. After briefly reviewing the framework of Bayesian logic programs in Section 2, we dicuss our learning approach in Section 3. We define the learning problem, introduce the scheme of the algorithm, and discuss it applied on a special class of propositional Bayesian logic programs, well-known under the name Bayesian networks, and applied on general Bayesian logic programs. Before concluding the paper, we relate our approach to other work in Section 5. We assume some familiarity with logic programming or Prolog (see e.g. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>) as well as with Bayesian networks (see e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bayesian Logic Programs</head><p>Throughout the paper we will use an example from genetics which is inspired by Friedman et al. <ref type="bibr" target="#b5">[6]</ref>: "it is a genetic model of the inheritance of a single gene that determines a person's X blood type bt(X). Each person X has two copies of the chromosome containing this gene, one, mc(Y), inherited from her mother m(Y,X), and one, pc(Z), inherited from her father f(Z,X)." We will use P to denote a probability distribution, e.g. P(x), and the normal letter P to denote a probability value, e.g. P (x = v), where v is a state of x.</p><p>The Bayesian logic program framework we will use in this paper is based on the Datalog subset of definite clausal logic, i.e. no functor symbols are allowed. The idea is that each Bayesian logic program specifies a Bayesian network, with one node for each (Bayesian) ground atom (see below). For a more expressive framework based on pure Prolog we refer to <ref type="bibr" target="#b13">[14]</ref>.</p><p>A Bayesian logic program B consist of two components, firstly a logical one, a set of Bayesian clauses (cf. below), and secondly a quantitative one, a set of conditional probability distributions and combining rules (cf. below) corresponding to that logical structure. A Bayesian (definite) clause c is an expression of the form</p><formula xml:id="formula_0">A | A 1 , . . . , A n</formula><p>where n ≥ 0, the A, A 1 , . . . , A n are Bayesian atoms and all Bayesian atoms are (implicitly) universally quantified. We define head(c) = A and body(c) = {A 1 , . . . , A n }. So, the differences between a Bayesian clause and a logical one are : (1) the atoms p(t 1 , ..., t n ) and predicates p arising are Bayesian, which means that they have an associated (finite) domain<ref type="foot" target="#foot_0">1</ref> dom(p), and (2) we use " | " instead of ":-". For instance, consider the Bayesian clause c bt(X) | mc(X), pc(X).</p><p>where dom(bt) = {a, b, ab, 0} and dom(mc) = dom(pc) = {a, b, 0}. It says that the blood type of a person X depends on the inherited genetical information of X. Note that the domain dom(p) has nothing to do with the notion of a domain in the logical sense. To summarize, a Bayesian logic program B consists of a (finite) set of Bayesian clauses. To each Bayesian clause c there is exactly one conditional probability distribution cpd(c) associated, and for each Bayesian predicate p there is exactly one associated combining rule cr(p).</p><p>The declarative semantics of Bayesian logic programs is given by the annotated dependency graph. The dependency graph DG(B) is that directed graph whose nodes correspond to the ground atoms in the least Herbrand model LH(B) (cf. below). It encodes the directly influenced by relation over the random variables in LH(B): there is an edge from a node x to a node y if and only if there exists a clause c ∈ B and a substitution θ, s.t. y = head(cθ), x ∈ body(cθ) and for all atoms z appearing in cθ : z ∈ LH(B). The direct predecessors of a graph node x are denoted as its parents, Pa(x). The Herbrand base HB(B) is the set of all random variables we could talk about. It is defined as if B were a logic program (cf. <ref type="bibr" target="#b17">[18]</ref>). The least Herbrand model LH(B) ⊆ HB(B) consists of all relevant random variables, the random variables over which a probability distribution is defined by B, as we will see. Again, LH(B) is defined as if B were be a logic program (cf. <ref type="bibr" target="#b17">[18]</ref>). It is the least fix point of the immediate consequence operator applied on the empty interpretation. Therefore, a ground atom which is true in the logical sense corresponds to a relevant random variables. Now, m(ann,dorothy). f(brian,dorothy). pc(ann). pc(brian). mc(ann). mc(brian). mc(X) | m(Y,X),mc(Y),pc(Y). pc(X) | f(Y,X),mc(Y),pc(Y). bt(X) | mc(X),pc(X).</p><p>(1) mc(X) pc(X) P(bt(X)) a a (0.97, 0.01, 0.01, 0.01) b a (0.01, 0.01, 0.97, 0.01) To see this, remember that if the conditions are fulfilled then DG(B) is a Bayesian network. Thus, given a total order x 1 . . . , x n of the nodes in DG(B) the distribution P B factorizes in the usual way:</p><formula xml:id="formula_1">• • • • • • • • • 0 0 (0.</formula><formula xml:id="formula_2">P B (x 1 . . . , x n ) = n i=1 P(x i | Pa x i ), where P(x i | Pa x i )</formula><p>is the combined conditional probability distribution associated to x i . A program B fulfilling the conditions is called well-defined, and we will consider such programs for the rest of the paper. The program bloodtype in Figure <ref type="figure" target="#fig_2">1</ref> encodes the regularities in our genetic example. Its grounded version, which is a Bayesian network, is given in Figure <ref type="figure" target="#fig_0">2</ref>. This illustrates that Bayesian networks <ref type="bibr" target="#b22">[23]</ref> are well-defined propositional Bayesian logic programs. Each nodeparents pair uniquely specifies a propositional Bayesian clause; we associate the identity as combining rule to each predicate; the conditional probability distributions are the ones of the Bayesian network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>m(ann,dorothy). f(brian,dorothy). pc(ann). pc(brian). mc(ann). mc(brian)</head><p>. mc(dorothy) | m(ann, dorothy),mc(ann),pc(ann). pc(dorothy) | f(brian, dorothy),mc(brian),pc(brian). bt(ann)</p><p>| mc(ann), pc(ann). bt(brian)</p><p>| mc(brian), pc(brian). bt(dorothy) | mc(dorothy),pc(dorothy). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structural Learning of Bayesian Logic Programs</head><p>Let us now focus on the logical structure of Bayesian logic programs. When designing Bayesian logic programs, the expert has to determine this (logical) structure of the Bayesian logic program by specifying the extensional and intensional predicates, and by providing definitions for each of the intensional predicates. Given this logical structure, the Bayesian logic program induces (the structure of) a Bayesian network whose nodes are the relevant<ref type="foot" target="#foot_1">2</ref> random variables. It is well-known that determining the structure of a Bayesian network, and therefore also of a Bayesian logic program, can be difficult and expensive. On the other hand, it is often easier to obtain a set D = {D 1 , . . . , D m } of data cases. A data case D i ∈ D has two parts, a logical and a probabilistic part.</p><p>The logical part of a data case D i ∈ D, denoted as Var(D i ), is a Herbrand interpretation. Consider e.g. the least Herbrand model LH(bloodtype) (cf. Figure <ref type="figure" target="#fig_0">2</ref>) and the logical atoms LH(bloodtype ) in the following case: {m(cecily, fred), f(henry, fred), pc(cecily), pc(henry), pc(fred), mc(cecily), mc(henry), mc(fred), bt(cecily), bt(henry), bt(fred)} These (logical) interpretations can be seen as the least Herbrand models of unknown Bayesian logic programs. They specify different sets of relevant random variables, depending on the given "extensional context". If we accept that the genetic laws are the same for both families then a learning algorithm should transform such extensionally defined predicates into intensionally defined ones, thus compressing the interpretations. This is precisely what ILP techniques are doing. The key assumption underlying any inductive technique is that the rules that are valid in one interpretation are likely to hold for any interpretation.</p><p>It thus seems clear that techniques for learning from interpretations can be adapted for learning the logical structure of Bayesian logic programs. Learning from interpretations is an instance of the non-monotonic learning setting of ILP (cf. <ref type="bibr" target="#b18">[19]</ref>), which uses only only positive examples (i.e. models).</p><p>So far, we have specified the logical part of the learning problem: we are looking for a set H of Bayesian clauses given a set D of data cases s.t. ∀D i ∈ D : LH(H ∪ Var(D i )) = Var(D i ), i.e. the Herbrand interpretation Var(D i ) is a model for H. The hypotheses H in the space H of hypotheses are sets of Bayesian clauses. However, we have to be more careful. A candidate set H ∈ H has to be acyclic on the data that means that for each D i ∈ D the induced Bayesian network over LH(H ∪ Var(D i )) has to be acyclic. Let us now focus on the quantitative components. The quantitative component of a Bayesian logic program is given by the associated conditional probability distributions and combining rules. We assume that the combining rules are fixed. Each data case D i ∈ D has a probabilistic part which is a partial assignment of states to the random variables in Var(D i ). We say that D i is a partially observed joint state of Var(D i ). As an example consider the following two data cases: where ? denotes an unknown state of a random variable. The partial assignments induce a joint distribution over the random variables of the logical parts. A candidate H ∈ H should reflect this distribution. In Bayesian networks the conditional probability distributions are typically learned using gradient descent or EM for a fixed structure of the Bayesian network. A scoring mechanism that evaluates how well a given structure H ∈ H matches the data is maximized. Therefore, we will assume a function score D : H → R.</p><formula xml:id="formula_3">{m(cecily, fred) = true, f(henry, fred) =?, pc(cecily) = a, pc(henry) = b, pc(fred) =?, mc(cecily) = b, mc(henry) = b, mc(fred) =?, bt(cecily) = ab, bt(henry) = b, bt(fred) =?} {m(ann, dorothy) = true, f(brian, dorothy) = true, pc(ann) = b, mc(ann) =?, mc(brian) = a, mc(dorothy) = a, pc(dorothy) = a,</formula><p>To summarize, the learning problem can be formulated as follows: The best match in this context refers to those parameters of the associated conditional probability distributions which maximize the scoring function. For a discussion on how the best match can be computed see <ref type="bibr" target="#b11">[12]</ref> or <ref type="bibr" target="#b15">[16]</ref>. The chosen scoring function is a crucial aspect of the algorithm. Normally, we can only hope to find a sub-optimal candidate. A heuristic learning algorithm solving this problem is given in Algorithm 1. Background knowledge can be incorporated in our approach in the following way. The background knowledge can be expressed as a fixed Bayesian logic program B. Then we search for a candidate H * which is together with B acyclic on the data such that for all D i ∈ D : LH(B ∪ H * ∪ Var(D i )) = Var(D i ), and B ∪ H * matches the data D best according to score D . In <ref type="bibr" target="#b13">[14]</ref>, we show how pure Prolog programs can be repesented as Bayesian logic prorgams w.r.t. the conditions 1 and 2 of Proposition 1. The basic idea is as follows. Assume that a logical clause A : -A 1 , . . . , A n is given. We encode the clause by the Bayesian clause A : -A 1 , . . . , A n where A, A 1 , . . . , A n are now Bayesian atoms over {true, false}. We associate to the Bayesian clause the conditional probability distribution of Figure <ref type="figure" target="#fig_2">1</ref>, and set the combining rule of A's predicate to max:</p><formula xml:id="formula_4">Given a set D = {D 1 , . . . , D m } of</formula><formula xml:id="formula_5">max{P(A | A i1 , . . . , A ini ) | i = 1, . . . , n} = P(A | ∪ n i=1 {A i1 , . . . , A ini }) := n max i=1 {P(A | A i1 , . . . , A ini )}.<label>(1)</label></formula><p>We will now explain Algorithm 1 and its underlying ideas in more details. The next section illustrates the algorithm for a special class of Bayesian logic programs: Bayesian networks. For Bayesian networks, the algorithm coincides with score-based methods for learning within Bayesian networks which are proven to be useful by the UAI community (see e.g. <ref type="bibr" target="#b8">[9]</ref>). Therefore, an extension to the first order case seems reasonable. It will turn out that the algorithm works for first order Bayesian logic programs, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Propositional Case: Bayesian Networks</head><p>Here we will show that Algorithm 1 is a generalization of score-based techniques for structural learning within Bayesian networks. To do so we briefly review these score-based techniques. Let x = {x 1 , . . . , x n } be a fixed set of random variables. The set x corresponds to a least Herbrand model of an unknown propositional Bayesian logic program representing a Bayesian network. The probabilistic dependencies among the relevant random variables are not known, i.e. the propositional Bayesian clauses are unknown. Therefore, we have to select such a propositional Bayesian logic program as a candidate and estimate its parameters. The data cases of the data D = {D 1 , . . . , D m } look like  which is a data case for the Bayesian network in Figure <ref type="figure" target="#fig_0">2</ref>. Note, that the atoms have to be interpreted as propositions. The set of candidate Bayesian logic programs spans the hypothesis space H. Each H ∈ H is a Bayesian logic program consisting of n propositional clauses: for each x i ∈ x a single clause c with head(c) = x i and body(c) ⊆ x \ {x i }. To traverse H we (1) specify two refinement operators ρ g : H → 2 H and ρ s : H → 2 H , that take a candidate and modify it to produce a set of candidates. The search algorithm performs informed search in H based on score D . In the case of Bayesian networks the operator ρ g (H) deletes a Bayesian proposition from the body of a Bayesian clause c i ∈ H, and the operator ρ s (H) adds a Bayesian proposition to the body of c i ∈ H . Usually, instances of scores are e.g. the minimum description length score <ref type="bibr" target="#b16">[17]</ref> or the Bayesian scores <ref type="bibr" target="#b9">[10]</ref>.</p><p>As a simple illustration we consider a greedy hill-climbing algorithm incorporating score D (H) := LL(D, H), the log-likelihood of the data D given a candidate structure H with the best parameters. We pick an initial candidate S ∈ H as starting point (e.g. the set of all propositions) and compute the likelihood LL(D, S) with the best parameters. Then, we use ρ(S) to compute the legal "neighbours" (candidates being acyclic) of S in H and score them. All neighbours (1)</p><formula xml:id="formula_6">a(X) | b(X) ,c(Y). b(X) | c(X). c(X) | d(X). a(X) | b(X). b(X) | c(X). c(X) | d(X). a(X) | b(X). b(X) | c(X). c(X) | d(X),a(X). a(X) | b(X), c(X). b(X) | c(X), d(X) c(X) | d(X). ρ g ρ s ρ s</formula><p>(2)</p><formula xml:id="formula_7">Fig. 3. (1)</formula><p>The use of refinement operators during structural search for Bayesian networks. We can add (ρs) a proposition to the body of a clause or delete (ρg) it from the body. <ref type="bibr" target="#b1">(2)</ref> The use of refinement operators during structural search within the framework of Bayesian logic programs. We can add (ρs) a constant-free atom to the body of a clause or delete (ρg) it from the body. Candidates crossed out in ( <ref type="formula" target="#formula_5">1</ref>) and ( <ref type="formula">2</ref>) are illegal because they are cyclic.</p><p>are valid (see below for a definition of validity). E.g. replacing pc(dorothy) with pc(dorothy) | pc(brian) gives such a "neighbour". We take that S ∈ ρ(S) with the best improvements in the score. The process is continued until no improvements in score are obtained. The use of the two refinement operators is illustrated in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The First Order Case</head><p>Here, we will explain the ideas underlying our algorithm in the first order case.</p><p>On the logical level it is similar to the ILP setting learning from interpretation which e.g. is used in the CLAUDIEN system ([4,5,1]): (1) all data cases are interpretations, and (2) a hypothesis should reflect what is in the data. The first point is carried over by enforcing each data case D i ∈ {D 1 , . . . , D m } to be a partially observed joint state of a Herbrand interpretation of an unknown Bayesian logic program. This also implies that all data cases are probabilistically independent <ref type="foot" target="#foot_2">3</ref> . The second point is enforced by requiring all hypotheses to be (logically) true in all data cases, i.e. the logical structure of the hypothesis is certain. Thus, the logical rules valid on the data cases are constraints on the space of hypotheses. The main difference to the pure logical setting is that we have to take the probabilistic parts of the data case into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Characteristic induction from interpretations). (adapted w.r.t. our purposes from [5]) Let D be a set of data cases and C the set of all clauses that can be part of a hypothesis. H ⊆ C is a logical solution iff H is a logically maximally general valid hypothesis. A hypothesis H ⊆ C is (logically) valid iff for all D i ∈ D: H is (logically) true in D i . A hypothesis H ⊆ C is a probabilistic solution iff H is a valid hypothesis and the Bayesian network induced by H on D is acyclic.</head><p>It is common to impose syntactic restrictions on the space H = 2 C of hypotheses through the language L, which determines the set C of clauses that can be part of a hypothesis. The language L is an important parameter of the induction task.</p><p>Language Assumption. In this paper, we assume that the alphabet of L only contains constant and predicate symbols that occur in one of the data cases, and we restrict C to range-restricted, constant-free clauses containing maximum k = 3 atoms in the body. Furthermore, we assume that the combining rules associated to the Bayesian predicates are given.</p><p>Let us discuss some properties of our setting. (1) Using partially observed joint states of interpretations as data cases is the first order equivalent of what is done in Bayesian network learning. There each data case is described by means of a partially observed joint state of a fixed, finite set of random variables. Furthermore, it implicitly corresponds to assuming that all relevant ground atoms of each data case are known: all random variables not stated in the data case are regarded to be not relevant (false in the logical sense). ( <ref type="formula">2</ref>) Hypotheses have to be valid. Intuitively, validity means that the hypothesis holds (logically) on the data, i.e. that the induced hypothesis postulates true regularities present in the data cases. Validity is a monotone property at the level of clauses, i.e. if H 1 and H 2 are valid with respect to a set of data cases D, then H 1 ∪ H 2 is valid. This means that all well-formed clauses in L can (logically) be considered completely independent of each other. Both arguments (1) and (2) together guarantee that no possible dependence among the random variables is lost. <ref type="bibr" target="#b2">(3)</ref> The condition of maximal generality appears in the definition because the most interesting hypotheses in the logical case are the most informative and hence the most general. Therefore, we will use a logical solution as initial hypotheses. But the best scored hypothesis has not to be maximally general, as the initial hypothesis in the next example shows. Here, our approach differs from the pure logical setting. We consider probabilistic solutions instead of logical solutions. The idea is to incorporate a scoring function known from learning of Bayesian networks to evaluate how well the given probabilistic solution matches the data.</p><p>The key to our proposed algorithm is the well-known definition of logical entailment (cf. <ref type="bibr" target="#b17">[18]</ref>). It induces a partial order on the set of hypotheses. To compute our initial (valid) hypotheses we use the CLAUDIEN algorithm. Roughly speaking, CLAUDIEN works as follows (for a detailed discussion we refer to <ref type="bibr" target="#b4">[5]</ref>). It keeps track of a list of candidate clauses Q, which is initialized to the maximally general clause (in L). It repeatedly deletes a clause c from Q, and tests whether c is valid on the data. If it is, c is added to the final hypothesis, otherwise, all maximally general specializations of c (in L) are computed (using a so-called refinement operator ρ, see below) and added back to Q. This process continues until Q is empty and all relevant parts of the search-space have been considered. We now have to define operators to traverse H. A logical specialization (or generalization) of a set H of Bayesian clauses could be achieved by specializing (or generalizing) single clauses c ∈ H. In our approach we use the two refinement operators ρ s : 2 H → H and ρ g : 2 H → H. The operator ρ s (H) adds constantfree atoms to the body of a single clause c ∈ H, and ρ g (H) deletes constant-free atoms from the body of a single clause c ∈ H. Figure <ref type="figure">3</ref> shows the different refinement operators for the general first order case and the propositional case for learning Bayesian networks. Instead of adding (deleting) propositions to (from) the body of a clause, they add (delete) according to our language assumption constant-free atoms. Furthermore, Figure <ref type="figure">3</ref> shows that using the refinement operators each probabilistic solution could be reached.</p><p>As a simple instantiation of Algorithm 1 we consider a greedy hill-climbing algorithm incorporating score D (H) := LL(D, H). It picks up a (logical) solution S ∈ H as starting point and computes LL(D, S) with the best parameters. For a discussion of how these parameters can be found we refer to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. E.g. having data cases over LH(bloodtype) and LH(bloodtype ), we choose as initial candidate</p><formula xml:id="formula_8">mc(X) | m(Y, X). pc(X) | f(Y, X). bt(X) | mc(X).</formula><p>It is likely that the initial candidate is not a probabilistic solution, although it is a logical solution. E.g. the blood type does not depend on the fatherly genetical information. Then, we use ρ s (S) and ρ g (S) to compute the legal "neighbours" of S in H and score them. E.g. one such a "neighbour" is given by replacing bt(X) | mc(X) with bt(X) | mc(X), pc(X). Let S be that valid and acyclic neighbour which is scored best. If LL(D, S) &lt; LL(D, S ), then we take S as new hypothesis. The process is continued until no improvements in score are obtained.</p><p>During the search we have to take care to prune away every hypothesis H which is invalid or leads to cyclic dependency graphs (on the data cases). This could be tested in time O(s • r 3 ) where r is the number of random variables of the largest data case in D and s is the number of clauses in H. To do so, we build the Bayesian networks induced by H over each Var(D i ) by computing the ground instances for each clause c ∈ H where the ground atoms are members of Var(D i ). This takes O(s • r 3 i ). Then, we test in O(r i ) for a topological order of the nodes in the induced Bayesian network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminary Experiments</head><p>We have implemented the algorithm in Sicstus Prolog 3.8.1. The implementation has an interface to Matlab to score hypotheses using the BNT toolbox <ref type="bibr" target="#b20">[21]</ref>. We considered two totally independent families using the predicates given by bloodtype having 12 respectively 15 family members. For each least Herbrand model 1000 samples from the induced Bayesian network were gathered.</p><p>The general question was whether we could learn the intensional rules of bloodtype. Therefore, we first had a look at the (logical) hypotheses space. The space could be seen as the first order equivalent of the space for learning the structure of Bayesian networks (see Figure <ref type="figure">3</ref>). In a further experiment the goal was to learn a definition for the predicate bt. We had fixed the definitions for the other predicates in two ways: (1) to the definitions the CLAUDIEN system had computed, and (2) to the definitions from the bloodtype Bayesian logic program. In both cases, the algorithm scored bt(X) | mc(X), pc(X) best, i.e. the algorithm has re-discovered the intensional definition which was originally used to build the data cases. Furthermore, the result shows that the best scored solution was independent of the fixed definitions. This could indicate that ideas about decomposable scoring functions can or should be lifted to the first order case. Although, these experiments are preliminary, they suggest that ILP techniques can be adapted for structural learning within first order probabilistic frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>To the best of our knowledge, there has not been much work on learning within first order extensions of Bayesian networks. Koller and Pfeffer <ref type="bibr" target="#b15">[16]</ref> show how to estimate the maximum likelihood parameters for Ngo and Haddawys's framework of probabilistic logic programs <ref type="bibr" target="#b21">[22]</ref> by adapting the EM algorithm. Kersting and De Raedt <ref type="bibr" target="#b11">[12]</ref> discuss a gradient-based method to solve the same problem for Bayesian logic programs. Friedman et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> tackle the problem of learning the logical structure of first order probabilistic models. They used Structural-EM for learning probabilistic relational models. This algorithm is similar to the standard EM method except that during iterations of this algorithm the structure is improved. As far as we know this approach, it does not consider logical constraints on the space of hypotheses in the way our approach does. Therefore, we suggest that both ideas can be combined. There exist also methods for learning within first order probabilistic frameworks which do not build on Bayesian networks. Sato et al. <ref type="bibr" target="#b24">[25]</ref> give a method for EM learning of PRISM programs. They do not incorporate ILP techniques. Cussens <ref type="bibr" target="#b2">[3]</ref> investigates EM like methods for estimating the parameters of stochastic logic programs. Within the same framework, Muggleton <ref type="bibr" target="#b19">[20]</ref> uses ILP techniques to learn the logical structure. The used ILP setting is different to learning from interpretations and seems not to be based on learning of Bayesian networks.</p><p>Finally, Bayesian logic programs are somewhat related to the BUGS language <ref type="bibr" target="#b7">[8]</ref>. The BUGS language is based on imperative programming. It uses concepts such as for-loops to model regularities in probabilistics models. So, the differences between Bayesian logic programs and BUGS are akin to the diferences between declarative programming languages (such as Prolog) and imperative ones. Therefore, adapting techniques from Inductive Logic Programming to learn the structure of BUGS programs seems not to be that easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>A new link between ILP and learning within Bayesian networks is presented. We have proposed a scheme for learning the structure of Bayesian logic programs.</p><p>It builds on the ILP setting learning from interpretations. We have argued that by adapting this setting score-based methods for structural learning of Bayesian networks could be updated to the first order case. The ILP setting is used to define and traverse the space of (logical) hypotheses. Instead of score-based greedy algorithm other UAI methods such as Structural-EM may be used. The experiments we have are promising. They show that our approach works. But the link established between ILP and Bayesian networks seems to be bi-directional. Can ideas developed in the UAI community be carried over to ILP?</p><p>The research within the UAI community has shown that score-based methods are useful. In order to see whether this still holds for the first-order case we will perform more detailed experiments. Experiments on real-world scale problems will be conducted. We will look for more elaborated scoring functions like e.g. scores based on the minimum description length principle. We will investigate more difficult tasks like learning multiple clauses definitions. The use of refinement operators adding or deleting non constant-free atoms should be explored. Furthermore, it would be interesting to weaken the assumption that a data case corresponds to a complete interpretation. Not assuming all relevant random variables are known would be interesting for learning intensional rules like nat(s(X)) | nat(X). Lifting the idea of decomposable scoring function to the first order case should result in a speeding up of the algorithm. In this sense, we believe that the proposed approach is a good point of departure for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The grounded version of the Bayesian logic program of Figure 1. It (directly) encodes a Bayesian network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>pc(brian) =?, bt(ann) = ab, bt(brian) =?, bt(dorothy) = a},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 .</head><label>1</label><figDesc>A greedy algorithm for searching the structure of Bayesian logic programs. {m(ann, dorothy) = true, f(brian, dorothy) = true, pc(ann) = a, mc(ann) =?, mc(brian) =?, mc(dorothy) = a, mc(dorothy) = a, pc(brian) = b, bt(ann) = a, bt(brian) =?, bt(dorothy) = a}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The domain dom(p) defines the states of random variables. Intuitively, a Bayesian predicate p generically represents a set of (finite) random variables. More precisely, each Bayesian ground atom g over p represents a (finite) random variable over the states dom(g) := dom(p). E.g. bt(ann) represents the blood type of a person named Ann as a random variable over the states {a, b, ab, 0}. Apart from that, most other logical notions carry over to Bayesian logic programs. So, we will speak of Bayesian predicates, terms, constants, substitutions, ground Bayesian clauses, Bayesian Herbrand interpretations etc. We will assume that all Bayesian clauses are range-restricted. A clause is rangerestricted iff all variables occurring in the head also occur in the body. Range restriction is often imposed in the database literature; it allows one to avoid derivation of non-ground true facts. To keep the expositions simple, we will assume that cpd(c) is represented as table, see Figure1. More elaborate representations like decision trees or rules are also possible. The distribution cpd(c) generically represents the conditional probability distributions of all ground instances cθ of the clause c. In general, one may have many clauses, e.g. clauses c 1 and the c 2</figDesc><table><row><cell>standard solu-</cell></row><row><cell>tion to obtain the distribution required are so called combining rules; func-</cell></row><row><cell>tions which map finite sets of conditional probability distributions {P(A |</cell></row><row><cell>A i1 , . . . , A ini ) | i = 1, . . . , m} onto one (combined) conditional probability distri-bution P(A | B 1 , . . . , B k ) with {B 1 , . . . , B k } ⊆ m i=1 {A i1 , . . . , A ini }. We assume</cell></row><row><cell>that for each Bayesian predicate p there is a corresponding combining rule cr,</cell></row><row><cell>such as noisy or.</cell></row></table><note><p><p><p>In order to represent a probabilistic model we associate with each Bayesian clause c a conditional probability distribution cpd(c) encoding P(head(c) | body(c)).</p>bt(X) | mc(X). bt(X) | pc(X).</p>and corresponding substitutions θ i that ground the clauses c i such that head(c 1 θ 1 ) = head(c 2 θ 2 ). They specify cpd(c 1 θ 1 ) and cpd(c 2 θ 2 ), but not the distribution required: P(head(c 1 θ 1 ) | body(c 1 ) ∪ body(c 2 )). The</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>data cases, a set H of Bayesian logic programs and a scoring function score D : H → R. Find a candidate H * ∈ H which is acyclic on the data such that for all D i ∈ D : LH(H * ∪ Var(D i )) = Var(D i ), and H * matches the data D best according to score D .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>The conditional probability distribution associated to a Bayesian clause A | A 1, . . . , An encoding a logical one.</figDesc><table><row><cell cols="2">P(A | A1, . . . , An)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>true</cell><cell>false</cell><cell cols="3">A1 A2 . . . An</cell></row><row><cell>1.0</cell><cell>0 .0</cell><cell cols="2">true true</cell><cell>true</cell></row><row><cell>0.0</cell><cell>1 .0</cell><cell cols="2">false true</cell><cell>true</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>0.0</cell><cell>1 .0</cell><cell cols="2">false false</cell><cell>false</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For the sake of simplicity we consider finite random variables, i.e. random variables having a finite set dom of states. However, the ideas generalize to discrete and continuous random variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In a sense, relevant random variables are those variables, which Cowell et al.[2,  p. 25]  mean when they say that the first phase in developing a Bayesian network involves to "specify the set of 'relevant' random variables".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>An assumption which one has to verify if using our method. In the case of families the assumption seems reasonable.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Stefan Kramer and Manfred Jaeger for helpful discussions on the proposed approach. Also, many thanks to the anonymous reviewers for their helpful comments on the initial draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ISIDD: An Interactive System for Inductive Databse Design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">421</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Probabilistic networks and expert systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parameter estimation in stochastic logic programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cussens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theory of clausal discovery</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruynooghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-1993)</title>
		<meeting>the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-1993)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1058" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dehaspe</surname></persName>
		</author>
		<title level="m">Clausal discovery. Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="99" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sixteenth International Joint Conference on Artificial Intelligence (IJCAI-1999)</title>
		<meeting>Sixteenth International Joint Conference on Artificial Intelligence (IJCAI-1999)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models with structural uncertainty</title>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data</title>
		<meeting>the AAAI-2000 Workshop on Learning Statistical Models from Relational Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A language and program for complex bayesian modelling</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Statistician</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A tutorial on learning with Bayesian networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<idno>MSR-TR-95-06</idno>
	</analytic>
	<monogr>
		<title level="j">One Microsoft Way</title>
		<imprint>
			<biblScope unit="volume">98052</biblScope>
			<date type="published" when="1995-03">March 1995</date>
			<publisher>Microsoft Corporation</publisher>
			<pubPlace>Redmond, WA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Advanced Technology Division</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<idno>MSR-TR-94-09</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relational Bayesian networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI-1997</title>
		<meeting>UAI-1997</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive Bayesian Logic Programs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian logic programs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
		<ptr target="http://SunSITE.Informatik.RWTH-Aachen.DE/Publications/CEUR-WS/" />
	</analytic>
	<monogr>
		<title level="m">Work-in-Progress Reports of the Tenth International Conference on Inductive Logic Programming (ILP -2000)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian logic programs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
		<idno>151</idno>
		<imprint>
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Freiburg, Institute for Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpreting Bayesian Logic Programs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning probabilities for noisy first-order rules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-1997)</title>
		<meeting>the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-1997)<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">August 23-29 1997</date>
			<biblScope unit="page" from="1316" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Bayesian belief networks: An approach based on the MDL principle</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Foundations of Logic Programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>2. edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive logic programming: Theory and methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="629" to="679" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning stochastic logic programs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</editor>
		<meeting>the AAAI-2000 Workshop on Learning Statistical Models from Relational Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.cs.berkeley.edu/˜murphyk/Bayes/bnt.html" />
		<title level="m">Bayes Net Toolbox for Matlab. U. C. Berkeley</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Answering queries form context-sensitive probabilistic knowledge bases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haddawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="147" to="177" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>2. edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probabilistic Horn abduction and Bayesian networks. Artificial Intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="81" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A viterbi-like algorithm and EM learning for statistical abduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kameya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI2000 Workshop on Fusion of Domain Knowledge with Data for Decision Support</title>
		<meeting>UAI2000 Workshop on Fusion of Domain Knowledge with Data for Decision Support</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Art of Prolog: Advanced Programming Techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
