<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GhostBERT: Generate More Features with Cheap Operations for BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
							<email>zhiqihuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
							<email>houlu3@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<email>shang.lifeng@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<email>jiang.xin@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
							<email>chen.xiao@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GhostBERT: Generate More Features with Cheap Operations for BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model's representation ability. In this paper, we propose GhostBERT, which generates more features with very cheap operations from the remaining features. In this way, GhostBERT has similar memory and computational cost as the pruned model, but enjoys much larger representation power. The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation. Empirical results on the GLUE benchmark on three backbone models (i.e., BERT, RoBERTa and ELECTRA) verify the efficacy of our proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there is a surge of research interests in compressing the transformer-based pre-trained language models like BERT into smaller ones using various compression methods, i.e., knowledge distillation <ref type="bibr" target="#b23">(Sanh et al., 2019;</ref><ref type="bibr" target="#b27">Sun et al., 2019;</ref><ref type="bibr" target="#b12">Jiao et al., 2020)</ref>, pruning <ref type="bibr" target="#b19">(Michel et al., 2019;</ref><ref type="bibr" target="#b4">Fan et al., 2019)</ref>, low-rank approximation <ref type="bibr" target="#b14">(Lan et al., 2020)</ref>, weight-sharing <ref type="bibr" target="#b14">(Lan et al., 2020)</ref>, dynamic networks with adaptive depth and/or width <ref type="bibr" target="#b15">(Liu et al., 2020;</ref><ref type="bibr" target="#b8">Hou et al., 2020;</ref><ref type="bibr" target="#b33">Xin et al., 2020;</ref><ref type="bibr" target="#b36">Zhou et al., 2020)</ref>, and quantization <ref type="bibr" target="#b25">(Shen et al., 2020;</ref><ref type="bibr" target="#b5">Fan et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2020;</ref><ref type="bibr" target="#b0">Bai et al., 2021)</ref>.</p><p>Previous works show that there are some redundant features in the BERT model, and unimportant attention heads or neurons can be pruned away without severe accuracy degradation <ref type="bibr" target="#b19">(Michel et al., 2019;</ref><ref type="bibr" target="#b8">Hou et al., 2020)</ref>. However, for computer vision (CV) tasks, it is shown in <ref type="bibr" target="#b7">(Han et al., 2020)</ref> that redundant features in convolutional neural networks also contribute positively to the performance, and using cheap linear operations to generate more ghost feature maps enhances the performance with few additional parameters. On the other hand, it is shown in <ref type="bibr" target="#b29">(Voita et al., 2019;</ref><ref type="bibr" target="#b13">Kovaleva et al., 2019;</ref><ref type="bibr" target="#b22">Rogers et al., 2020</ref>) that many attention maps in pre-trained language models exhibit typical positional patterns, e.g., diagonal or vertical, which can be easily generated from other similar ones using operations like convolution.</p><p>Based on the above two aspects, in this paper, we propose to use cheap ghost modules on top of the remaining important attention heads and neurons to generate more features, so as to compensate for the pruned ones. Considering that the convolution operation (1) encodes local context dependency, as a complement of the global self-attention in Transformer models <ref type="bibr" target="#b32">(Wu et al., 2020)</ref>; and (2) can generate some BERT features like positional attention maps from similar others, in this work, we propose to use the efficient 1-Dimensional Depthwise Separable Convolution <ref type="bibr" target="#b31">(Wu et al., 2019)</ref> as the basic operation in the ghost module. To ensure the generated ghost features have similar scales  as the original ones, we use a softmax function to normalize the convolution kernel.</p><p>Afterwards, we fine-tune the parameters in both the BERT backbone model and the added ghost modules. Note that the ghost modules are not necessarily applied to pruned models. They can also be directly applied to pre-trained language models for better performance while with negligible additional parameters and floating-point operations (FLOPs). Figure <ref type="figure" target="#fig_0">1</ref> summarizes the average accuracy versus parameter size and FLOPs on the GLUE benchmark, where adding ghost modules to both the unpruned (m = 12/12) and pruned (m &lt; 1) BERT models perform better than the counterparts without ghost modules. More experiments on the GLUE benchmark show that with only 0.4% more parameters and 0.9% more FLOPs, the proposed ghost modules improve the average accuracy of BERT-base, RoBERTa-base and ELECTRA-small by 0.9, 0.6, 2.4 points, respectively. When applying ghost modules to small or pruned models, the resultant models outperform other BERT compression methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we first introduce where to add ghost modules in a BERT model (Section 2.1), and then discuss the components and optimization details of the ghost module (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adding Ghost Modules to BERT</head><p>The BERT model is built with Transformer layers, each of which contains a Multi-Head Attention (MHA) layer and a Feed-Forward Network (FFN), as well as skip connections and layer normalizations. <ref type="bibr" target="#b8">Hou et al. (2020)</ref> show that the computations for attention heads of MHA and neurons in the intermediate layer of FFN can be performed in parallel. Thus the BERT model can be compressed in a structured manner by pruning parameters associated with these heads and neurons <ref type="bibr" target="#b8">(Hou et al., 2020)</ref>. In this paper, after pruning the unimportant heads and neurons, we employ cheap ghost modules upon the remaining ones to generate more ghost features to compensate for the pruned ones.</p><p>For simplicity of notation, we omit the bias terms in linear and convolution operations where applicable in the rest of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Ghost Module on MHA</head><p>Following <ref type="bibr" target="#b8">(Hou et al., 2020)</ref>, we divide the computation of MHA into the computation of each attention head. Specifically, suppose the sequence length and hidden state size are n and d, respectively. Each transformer layer consists of N H attention heads. For input matrix X ∈ R n×d , the h-th attention head computes its output as</p><formula xml:id="formula_0">H h (X) = Softmax(1/ √ d • XW Q h W K h X )XW V h W O h , where W Q h , W K h , W V h , W O h ∈ R d×d h with d h = d/N H</formula><p>are the projection matrices associated with it. In multi-head attention, N H heads are computed in parallel to get the final output:</p><formula xml:id="formula_1">MHA(X) = N H h=1 H h (X).</formula><p>(1)</p><p>Given a width multiplier m ≤ 1, we keep M = N h m heads and use them to generate F ghost features. The f th ghost feature is generated by</p><formula xml:id="formula_2">G f (X) = Nonlinear M h=1 G f,h (H h (X)) . (2)</formula><p>where G f,h is the proposed cheap ghost module which generates features from the h th attention head's representation to the f th ghost feature. ReLU is used as the nonlinearity function. Thus the computation of MHA in the GhostBERT is:</p><formula xml:id="formula_3">Ghost-MHA(X) = M h=1 H h (X)+ F f =1 G f (X). (3)</formula><p>Besides being added to the output of MHA, the ghost modules can also be added to other positions in MHA. Detailed discussions are in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Ghost Module on FFN</head><p>Similar to the attention heads in MHA, the computation of FFN can also be divided into computations for each neuron in the intermediate layer of FFN <ref type="bibr" target="#b8">(Hou et al., 2020)</ref>. With a slight abuse of notation, we still use X ∈ R n×d as the input to FFN. Denote the number of neurons in the intermediate layer as d f f , the computation of FFN can be written as: FFN(X) =</p><formula xml:id="formula_4">d f f i=1 GeLU XW 1 :,i W 2 i,:</formula><p>, where W 1 , W 2 are the weights in FFN.</p><p>For simplicity, we also use width multiplier m for FFN as MHA, and divide these neurons into N H folds, where each fold contains d f = d f f /N H neurons. For the h-th fold, its output can be computed as</p><formula xml:id="formula_5">H h (X) = GeLU XW 1 h W 2 h where W 1 h = W 1 :,(h−1)d f :hd f and W 2 h = W 2 (h−1)d f :hd f ,:</formula><p>are the parameters associated with it. In FFN, N H folds are computed in parallel to get the output:</p><formula xml:id="formula_6">FFN(X) = N H h=1 H h (X).<label>(4)</label></formula><p>For width multiplier m, we keep M folds of neurons and use ghost modules to generate F ghost features as in Equation (2). Thus the computation of FFN in the GhostBERT can be written as:</p><formula xml:id="formula_7">Ghost-FFN(X) = M h=1 H h (X)+ F f =1 G f (X). (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ghost Module</head><p>In the previous section, we discussed where we insert the ghost modules in the Transformer layer. In this section, we elaborate on the components and normalization of the ghost modules.</p><p>Generally speaking, any function can be used as the ghost module G in Equation (2). Considering that (i) convolution operation can encode local context dependency, as a compensation for the global self-attention <ref type="bibr" target="#b32">(Wu et al., 2020;</ref><ref type="bibr" target="#b11">Jiang et al., 2020)</ref>; and (ii) features like diagonal or vertical attention maps <ref type="bibr" target="#b13">(Kovaleva et al., 2019;</ref><ref type="bibr" target="#b22">Rogers et al., 2020)</ref> can be easily generated by convolving similar others, we consider using convolution as the basic operation in the ghost module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Convolution Type</head><p>With a slight abuse of notation, here we still use X ∈ R n×d as the input to the convolution, i.e., the output H h of h th head in MHA or h th fold of neurons in FFN. Denote O ∈ R n×d as the output of the convolution in the ghost module.</p><p>1-Dimensional convolution (Conv1D) over the sequence direction encodes local dependency over contexts, and has shown remarkable performance for NLP tasks <ref type="bibr" target="#b31">(Wu et al., 2019</ref><ref type="bibr" target="#b32">(Wu et al., , 2020))</ref>. To utilize the representation power of Conv1D without too much additional memory and computation, we choose 1-Dimensional Depthwise Separable Convolution (DWConv) <ref type="bibr" target="#b31">(Wu et al., 2019)</ref> for the ghost module. Compared with Conv1D, DWConv performs a convolution independently over every channel, and reduces the number of parameters from d 2 k to dk (where k is the convolution kernel size). Denote the weight of the DWConv operation as W ∈ R d×k . After applying DWConv, the output for the i th token and c th channel can be written as:</p><formula xml:id="formula_8">O i,c = DWConv(X :,c , W c,: , i, c) = k m=1 W c,m • X i− k+1 2 +m,c .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Normalization</head><p>Since the parameters of the BERT backbone model and the ghost modules can have quite different scales and optimization behaviors, we use a softmax function to normalize each convolution kernel W c,: across the sequence dimension as Softmax(W c,: ) before convolution as <ref type="bibr" target="#b31">Wu et al. (2019)</ref>. By softmax normalization, the weights in one kernel are summed up to 1, ensuring that the convolved output has a similar scale as the input. Thus after applying the ghost module, the output for the i th token and c th channel can be written as:</p><p>Ôi,c = DWConv(X :,c , Softmax(W c,: ), i, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Details</head><p>To turn a pre-trained BERT model into a smallersized GhostBERT, we do the following three steps:  <ref type="table">1</ref>: Development set results of the baseline pre-trained language models and our proposed method on the GLUE benchmark. Both pruned and unpruned BERT-base (resp. RoBERTa-base) are used as the backbone models for GhostBERT (resp. GhostRoBERTa). The unpruned ELECTRA-small is used as the backbone model for the GhostELECTRA-small. m is the width multiplier written in the form of proportion, whose numerator and denominator represent the remaining attention heads/folds of neurons and the total number of heads/folds, respectively.</p><p>Pruning. For a certain width multiplier m, we prune the attention heads in MHA and neurons in the intermediate layer of FFN from a pre-trained BERT-based model following <ref type="bibr" target="#b8">(Hou et al., 2020)</ref>.</p><p>Distillation. Then we add ghost modules to the pruned model as in Section 2.1. Suppose there are L Transformer layers. We distill the knowledge from the embedding (i.e., the output of the embedding layer) E, hidden states M l after MHA and F l after FFN (where l = 1, 2, • • • , L) from the full-sized teacher model to E m , M m l , F m l of the student GhostBERT. Following <ref type="bibr" target="#b12">(Jiao et al., 2020)</ref>, we use the augmented data for distillation. Denote MSE as the mean squared error, the three loss terms are emb = MSE(E m , E),</p><formula xml:id="formula_9">mha = L l=1 MSE(M m l , M l ), and f f n = L l=1 MSE(F m l , F l )</formula><p>, respectively. Thus, the distillation loss function is:</p><formula xml:id="formula_10">L distill = emb + mha + f f n .</formula><p>Fine-tuning. Denote y as the predicted logits, we finally fine-tune the GhostBERT with groundtruth labels ŷ as:</p><formula xml:id="formula_11">L f inetune = CrossEntropy(ŷ, y).</formula><p>Note that instead of being applied to pruned models, the cheap ghost modules can also be directly applied to a pre-trained model for better performance while with negligible additional parameters and FLOPs. In this case, the training procedure contains only the distillation and fine-tuning steps.</p><p>Empirically, to save memory and computation, we generate one ghost feature for each MHA or FFN (i.e., F = 1 in Equations ( <ref type="formula">3</ref>) and ( <ref type="formula">5</ref>)), and let all ghost modules G f,h share the same parameters with each other. As will be shown in Section 3, adding these simplified ghost modules already achieve clear performance gain empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>In this section, we show the efficacy of the proposed method with (pruned) BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> and ELEC-TRA <ref type="bibr" target="#b2">(Clark et al., 2020)</ref> as backbone models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Experiments are performed on the GLUE benchmark <ref type="bibr" target="#b30">(Wang et al., 2019)</ref>, which consists of various natural language understanding tasks. More statistics about the GLUE datasets are in Appendix A.1. Following <ref type="bibr" target="#b2">(Clark et al., 2020)</ref>, we report Spearman correlation for STS-B, Matthews correlation for CoLA and accuracy for the other tasks. For MNLI, we report the results on the matched section. The convolution kernel size in the ghost module is set as 3 unless otherwise stated. The detailed hyperparameters for training the GhostBERT are in Appendix A.2. The model with the best development set performance is used for testing. For each method, we also report the number of parameters and FLOPs at inference (Details can be found in Appendix A.3). We compare our proposed method against the following methods: (i) baseline pre-trained language models: BERT-base <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, RoBERTa-base <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> and ELECTRAsmall <ref type="bibr" target="#b2">(Clark et al., 2020)</ref>; (ii) BERT compression methods: TinyBERT <ref type="bibr" target="#b12">(Jiao et al., 2020)</ref>, Con-vBERT <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref>, and MobileBERT <ref type="bibr" target="#b28">(Sun et al., 2020)</ref>. The development set results of RoBERTa-base are from <ref type="bibr" target="#b8">Hou et al. (2020)</ref>. The test set results of ELECTRA, BERT-base and Con-vBERT are from <ref type="bibr" target="#b11">Jiang et al. (2020)</ref>. The others are from their original papers or repositories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Ghost Modules on Unpruned Models</head><p>Table <ref type="table">1</ref> shows the GLUE development set results of the baseline pre-trained language models and our proposed method. When the cheap ghost modules are directly applied to these unpruned pretrained models, better performances are achieved with only negligible additional parameters and FLOPs. Specifically, adding ghost modules to BERT-base, RoBERTa-base and ELECTRA-small increases the average development accuracy by 0.9, 0.6, 2.4 points with only 55.3K more parameters, and 14.2M more FLOPs. For the test set, the average performance gains are 0.8, 1.1, 2.4 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Ghost Modules on Pruned Models</head><p>Comparison with Baseline Models. From Table 1, when the ghost modules are applied to the pruned BERT (or RoBERTa) model with m &lt; 1, the proposed GhostBERT or GhostRoBERTa also achieves comparable performances as BERT-base or RoBERTa-base with fewer FLOPs. Specifically, GhostBERT (m = 6/12) and GhostRoBERTa (m = 9/12) perform similarly or even better than BERTbase and RoBERTa-base with only 50% and 75% FLOPs, respectively. In particular, when the compression ratio increases (i.e., m = 3/12, 1/12), we still achieve 99.6% performance (resp. 96.3%) with only 25% FLOPs (resp. 8%) of BERT-base model.</p><p>Comparison with Other Compression Methods. Table <ref type="table" target="#tab_1">2</ref> shows the comparison between the proposed method and other popular BERT compression methods. Under similar parameter sizes or FLOPs, the proposed GhostBERT performs comparably as the other BERT compression methods, while GhostRoBERTa often outperforms them. In particular, GhostELECTRA-small has over 1.5 points or higher accuracy gain than other similar-sized small models like ELECTRA-small, TinyBERT 4 and ConvBERT-small.</p><p>In Table <ref type="table" target="#tab_2">3</ref> and Figure <ref type="figure" target="#fig_0">1</ref>, we also compare the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>In this section, we perform ablation study in the (i) training procedure: including data augmentation (DA) and knowledge distillation (KD); (ii) ghost module: including convolution kernel size, softmax normalization over the convolution kernel and nonlinearity for each ghost feature in Equation ( <ref type="formula">2</ref>).</p><p>Training Procedure. Table <ref type="table" target="#tab_3">4</ref> verifies the effectiveness of the Data Augmentation (DA) and Knowledge Distillation (KD) upon the GhostBERT model with width multiplier m ∈ {3/12, 1/12}. The GhostBERT incurs severe accuracy drop without DA and KD. with a drop of 3.5 and 6.4 points on average, for m = 3/12 and 1/12, respectively.</p><p>Ghost Module. Table <ref type="table" target="#tab_3">4</ref> also shows the effectiveness of the softmax normalization over the convolution kernel and ReLU nonlinearity in Equation ( <ref type="formula">2</ref>).</p><p>As can be seen, dropping the softmax normalization or ReLU nonlinearity reduces the average accuracy by 0.8 and 1.6 points respectively for m = 3/12, and 0.9 and 2.2 points respectively for m = 1/12. Further, we explore whether the kernel size plays an important role in the DWConv in the ghost module. Figure <ref type="figure">3</ref> shows the results of GhostBERT with width multipliers m ∈ {3/12, 1/12}, with various convolution kernel sizes in DWConv. Average accuracy over five tasks is reported. Detailed results for each task can be found in Table <ref type="table" target="#tab_9">9</ref> in Appendix B.1. As can be seen, the performance of Ghost-BERT increases first and then decreases gradually as the kernel size increases. For both width multipliers, kernel size 3 performs best and is used as the default kernel size in other experiments unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this section, we discuss about different choices of which type of convolution to use in the ghost module (Section 4.1), and where to posit the ghost modules in a BERT model (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ghost Module Types</head><p>Besides the DWConv in Section 2.2, in this section, we discuss more options for the convolution in the ghost module. We follow the notation in Section 2.2 and denote the input, output, kernel size of the convolution as X, W and k, respectively.</p><p>1-Dimensional Convolution. If the kernel convolves input over the sequence direction (abbreviated as Conv1D S), the number of input and output channel is d, and the weight W has shape W ∈ R d×d×k . After applying Conv1D S, the output for the i th token and c th channel is:</p><formula xml:id="formula_12">O i,c = Conv1D S(X, W c,:,: , i, c) = d j=1 k m=1 W c,j,k • X i− k+1 2 +m,j .</formula><p>If the kernel convolves input over the feature direction (abbreviated as Conv1D F), the number of input and output channel is n, and the weight has shape W ∈ R n×n×k . After applying Conv1D F, the output for the i th token and c th channel is:</p><formula xml:id="formula_13">O i,c = Conv1D F(X, W i,:,: , i, c) = n j=1 k m=1 W i,j,m • X j,c− k+1 2 +m .</formula><p>2-Dimensional Convolution (Conv2D). For Conv2D, the number of input and output channels are both 1, and thus the weight W has shape W ∈ R 1×1×k×k . After applying Conv2D, the output for the i th token and c th channel is:</p><formula xml:id="formula_14">O i,c = Conv2D(X, W, i, c) = k w=1 k h=1 W :,:,h,w • X i− k+1 2 +h,c− k+1 2 +w .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison of Different Convolutions</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the comparison of using different convolutions for the ghost module. For 1-Dimensional convolution, Conv1D S performs better Conv1D F. This may because that convolving over the sequence urges the model to learn the dependencies among tokens. Though 2-Dimensional convolution (Conv2D) is quite successful in CV tasks, it performs much worse than Conv1D S here. This may because the two dimensions of feature maps in CV tasks encode similar information, while those of hidden states in Transformers encode quite different information (i.e., feature and sequence). Thus Conv2D results in worse performance than Conv1D S, though much fewer parameters and FLOPs are required.</p><p>On the other hand, DWConv achieves comparable performance as Conv1D S, while being much more efficient in terms of number of parameters and FLOPs, by performing the convolution independently over every feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ghost Module Positions</head><p>In this section, we explore more possible positions of adding the ghost module. For MHA, besides adding ghost module after the projection layer (After O in Figure <ref type="figure" target="#fig_3">4</ref>(c)) as in Section 2.1.1, we can also add it right after calculating the attention score (After QK in Figure <ref type="figure" target="#fig_3">4(a)</ref>), or after multiplying the attention score and the value layer (After V in Figure <ref type="figure" target="#fig_3">4(b)</ref>). For FFN, besides adding the ghost module after the second linear layer (After FFN2 in Figure <ref type="figure" target="#fig_3">4</ref>(e)) as in Section 2.1.1, we can also add it after the intermediate layer (After FFN1 in Figure <ref type="figure" target="#fig_3">4(d)</ref>). Note that we use Conv2D as the ghost module for After QK because the attention map encodes attention probabilities in both dimensions. For After QK and After V, to match the dimension of other parameters, the number of input and output channels are M and N H − M , respectively.</p><p>Table <ref type="table" target="#tab_6">6</ref> shows the results of adding one ghost module to the same position for each Transformer layer. As can be seen, adding ghost module upon the attention maps (After QK) performs best. However, since the parameters in the value and projection layer of MHA are left unpruned, After QK has much more parameters and FLOPs than the other positions. Adding ghost modules to the other four positions has similar average accuracy. Thus in this work, for MHA, we choose the most memory-and computation-efficient strategy After O. Similarly, for FFN, we also add ghost modules to the final output (After FFN2). From Table <ref type="table" target="#tab_6">6</ref>, our way of adding ghost modules has comparable performance as After QK, while being much more efficient in parameter size and FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Pruning in Transformer</head><p>Pruning removes unimportant connections or neurons in the network. Compared with pruning connections <ref type="bibr" target="#b34">(Yu et al., 2019;</ref><ref type="bibr" target="#b6">Gordon et al., 2020</ref>    <ref type="formula">2019</ref>) also prune the neurons and the embeddings. In the depth direction, pruning Transformer layers is proposed in LayerDrop <ref type="bibr" target="#b4">(Fan et al., 2019)</ref> via structured dropout. Efficient choice of Transformer layers at inference via early exit are also proposed in <ref type="bibr" target="#b15">(Liu et al., 2020;</ref><ref type="bibr" target="#b33">Xin et al., 2020;</ref><ref type="bibr" target="#b36">Zhou et al., 2020)</ref>. <ref type="bibr" target="#b8">Hou et al. (2020)</ref> perform structured pruning in both width and depth directions. The importance of attention heads and neurons in the intermediate layer of Feed-forward network is measured by their impact on the loss, and the least important heads and neurons are pruned away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Enhanced Representation in</head><p>Transformer-based Models The second group uses linear or convolutional module along with the self-attention mechanism for more powerful representation. The new module can be incorporated though serial connection to the original self-attention mechanism <ref type="bibr" target="#b18">(Mehta et al., 2020)</ref>, or be used in parallel with the original selfattention mechanism <ref type="bibr" target="#b32">(Wu et al., 2020;</ref><ref type="bibr" target="#b11">Jiang et al., 2020)</ref> to capture both local and global context dependency. Serial and parallel connections of these linear or convolution operations to Transformer layers are also extended to multi-task <ref type="bibr" target="#b9">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b26">Stickland and Murray, 2019)</ref> and multilingual tasks <ref type="bibr" target="#b21">(Pfeiffer et al., 2020)</ref>.</p><p>Note that the proposed ghost modules are orthogonal to the above methods in that these modules are used to generate more features for the Transformer models and can be easily integrated into existing methods to boost their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose GhostBERT to generate more features in pre-trained model with cheap operations. We use the softmax-normalized 1-Dimensional Convolutions as ghost modules and add them to the output of the MHA and FFN of each Transformer layer. Empirical results on BERT, RoBERTa and ELECTRA demonstrate that adding the proposed ghost modules enhances the representation power and boosts the performance of the original model by supplying more features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Statistics of GLUE datasets</head><p>The GLUE benchmark <ref type="bibr" target="#b30">(Wang et al., 2019)</ref> consists of various sentence understanding tasks, including two single-sentence classification tasks (CoLA and SST-2), three similarity and paraphrase tasks (MRPC, STS-B and QQP), and four inference tasks (MNLI, QNLI, RTE and WMLI). For MNLI task, we report the result on the matched section. For Winograd Schema (WNLI), it is a small natural inference dataset while even a majority baseline outperforms many methods on it. As is noted in the GLUE official website 1 , there are some issues with the construction of it. Like previous work <ref type="bibr" target="#b8">(Hou et al., 2020;</ref><ref type="bibr" target="#b11">Jiang et al., 2020)</ref>, we do not experiment on WNLI. We use the default train/development/test splits from the official website. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameters</head><p>We show the detailed hyperparameters for the distillation and fine-tuning stages in Section 2.3 of the proposed method on the GLUE benchmark in Table <ref type="table" target="#tab_8">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 FLOPs</head><p>Floating-point operations (FLOPs) measures the number of floating-point operations that the model performs for a single process and can be used as a measure of the computational complexity of deep neural network models. To count the FLOPs, we follow the setting in <ref type="bibr" target="#b8">(Hou et al., 2020)</ref> and infer FLOPs with batch size 1 and sequence length 128. Since the operations in the embedding lookup are relatively cheap compared to those in Transformer layers, following <ref type="bibr" target="#b8">(Hou et al., 2020;</ref><ref type="bibr" target="#b28">Sun et al., 2020)</ref>, we do not count them. Note that the reported FLOPs for ELECTRA <ref type="bibr" target="#b2">(Clark et al., 2020)</ref> and ConvBERT <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref> in their original papers include those for the embedding lookup, and are slightly different from the numbers in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Full Results of Different Convolution Kernel Sizes</head><p>In Table <ref type="table" target="#tab_9">9</ref>, we show the detailed results of different convolutions kernel sizes for each of the five tasks (SST-2, MRPC, CoLA, STS-B and RTE). As can be seen, for each task, DWConv with kernel size 3 has the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Full Results of Pruned BERT</head><p>In Table <ref type="table" target="#tab_10">10</ref>, we show the detailed results of the pruned BERT and the GhostBERT for each task. We can see that under the same training procedure, the GhostBERT outperforms the pruned BERT over all compared sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Full Results of Different Positions</head><p>Table <ref type="table" target="#tab_11">11</ref> shows the detailed results of adding ghost modules to different positions of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Generating More Features</head><p>As is mentioned at the end of Section 2.3, we generate only one ghost feature for each MHA and FFN, i.e., F = 1 to save computation and memory. Indeed, our framework has no limitation on F , and also allows the model to generate more features (i.e., F &gt; 1). In this section, we discuss the relationship between generating more ghost features and the computation/memory requirements. Following the notation in Section 2 and omitting the cheap computation of ReLU and softmax, generating F ghost features from M features for all L layers requires 2LM F dk additional parameters and 4LM F ndk additional FLOPs. Both of them scale linearly as F , and can be large when F is large. For instance, for BERT-base with d = 768, when n = 128, k = 3, M = 12 and F = 12, the additional #parameters and FLOPs are 8M and 2.0G respectively, accounting for 7.2% and 9.1% of the backbone model.</p><p>When F increases, the accuracy of GhostBERT first increases slowly and soon begins to saturate or decrease. E.g., for GhostBERT (m = 1/12), the average development accuracy on GLUE only increases from 80.3 to 80.6 when F increases from 1 to 4, and then saturates when F &gt; 4. For Ghost-BERT (m = 3/12), the highest accuracy 83.1 is achieved when F = 1 or 2, and then the accuracy begins to decrease.</p><p>Thus in the paper, we simply choose F=1 which is cheap, but already achieves good performance on most tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average GLUE development accuracy versus #params and FLOPs with the (pruned) BERT and our GhostBERT. m is the width multiplier of the model.</figDesc><graphic url="image-1.png" coords="1,318.26,238.66,196.46,92.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Adding ghost modules {G f,h } F,M f =1,h=1 to MHA and FFN. Ghost Module G f,h .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Using ghost modules to generate more features in BERT. G-MHA/FFN stands for Ghost-MHA/FFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Candidate positions to add the ghost module in the Transformer layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Various methods have been proposed to use linear or convolution operations to enhance the representation of the Transformer layers. The first group of research works replaces the self-attention mechanism or feed-forward networks with simpler and more efficient convolution operations, while maintaining comparable results. Wu et al. (2019) introduce the token-based dynamic depth-wise convolution to compute the importance of context elements, and achieve better results in various NLP tasks. Iandola et al. (2020) replace all the feed-forward networks with grouped convolution. AdaBERT (Chen et al., 2020) uses differentiable neural architecture to search for more efficient convolution-based NLP models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test set results of the baseline pre-trained language models, BERT compression methods and our proposed method on the GLUE benchmark.</figDesc><table><row><cell>Model</cell><cell cols="5">FLOPs(G) #params(M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.</cell></row><row><cell>BERT-base (Devlin et al., 2019)</cell><cell>22.5</cell><cell>110</cell><cell cols="3">84.6 90.5 89.2 66.4 93.5 84.8 52.1 85.8 80.9</cell></row><row><cell>RoBERTa-base (Liu et al., 2019)</cell><cell>22.5</cell><cell>125</cell><cell cols="3">86.0 92.5 88.7 73.0 94.6 86.5 50.5 88.1 82.5</cell></row><row><cell>ELECTRA-small (Clark et al., 2020)</cell><cell>1.7</cell><cell>14</cell><cell cols="3">79.7 87.7 88.0 60.8 89.1 83.7 54.6 80.3 78.0</cell></row><row><cell>TinyBERT 6 (Jiao et al., 2020)</cell><cell>11.3</cell><cell>67</cell><cell cols="3">84.6 90.4 89.1 70.0 93.1 87.3 51.1 83.7 81.2</cell></row><row><cell>TinyBERT 4 (Jiao et al., 2020)</cell><cell>1.2</cell><cell>15</cell><cell cols="3">82.5 87.7 89.2 66.6 92.6 86.4 44.1 80.4 78.7</cell></row><row><cell>ConvBERT-medium (Jiang et al., 2020)</cell><cell>4.7</cell><cell>17</cell><cell cols="3">82.1 88.7 88.4 65.3 89.2 84.6 56.4 82.9 79.7</cell></row><row><cell>ConvBERT-small (Jiang et al., 2020)</cell><cell>2.0</cell><cell>14</cell><cell cols="3">81.5 88.5 88.0 62.2 89.2 83.3 54.8 83.4 78.9</cell></row><row><cell>MobileBERT w/o OPT (Sun et al., 2020)</cell><cell>5.7</cell><cell>25</cell><cell cols="3">84.3 91.6 88.3 70.4 92.6 84.5 51.1 84.8 81.0</cell></row><row><cell>MobileBERT (Sun et al., 2020)</cell><cell>5.7</cell><cell>25</cell><cell>83.3 90.6 -66.2 92.8</cell><cell>-</cell><cell>50.5 84.4 -</cell></row><row><cell>MobileBERT-tiny (Sun et al., 2020)</cell><cell>3.1</cell><cell>15</cell><cell>81.5 89.5 -65.1 91.7</cell><cell>-</cell><cell>46.7 80.1 -</cell></row><row><cell>GhostBERT (m = 12/12)</cell><cell>22.5</cell><cell>110</cell><cell cols="3">84.6 91.1 89.3 70.2 93.1 86.9 54.6 83.8 81.7</cell></row><row><cell>GhostBERT (m = 9/12)</cell><cell>16.9</cell><cell>88</cell><cell cols="3">84.9 91.0 88.6 69.2 92.9 86.1 53.7 84.0 81.3</cell></row><row><cell>GhostBERT (m = 6/12)</cell><cell>11.3</cell><cell>67</cell><cell cols="3">84.2 90.8 89.1 69.6 93.1 84.0 53.4 83.1 80.9</cell></row><row><cell>GhostBERT (m = 3/12)</cell><cell>5.8</cell><cell>46</cell><cell cols="3">83.8 90.7 89 68.6 93.2 82.5 51.3 82.5 80.2</cell></row><row><cell>GhostBERT (m = 1/12)</cell><cell>2.0</cell><cell>32</cell><cell cols="3">82.5 89.3 88.7 65.0 92.9 81.0 41.3 80.0 77.6</cell></row><row><cell>GhostRoBERTa (m = 12/12)</cell><cell>22.5</cell><cell>125</cell><cell cols="3">87.9 93.0 89.6 74.6 95.1 88.0 52.4 88.3 83.6</cell></row><row><cell>GhostRoBERTa (m = 9/12)</cell><cell>16.9</cell><cell>103</cell><cell cols="3">87.7 92.6 89.5 73.0 94.5 85.7 51.9 87.1 82.8</cell></row><row><cell>GhostRoBERTa (m = 6/12)</cell><cell>11.3</cell><cell>82</cell><cell cols="3">86.3 92.1 89.5 71.5 94.5 86.8 51.2 87.0 82.4</cell></row><row><cell>GhostRoBERTa (m = 3/12)</cell><cell>5.8</cell><cell>61</cell><cell cols="3">85.5 91.2 89.1 68.5 93.4 85.3 48.9 84.7 80.8</cell></row><row><cell>GhostRoBERTa (m = 1/12)</cell><cell>2.0</cell><cell>47</cell><cell cols="3">81.3 88.6 88.5 62.8 92.1 82.8 39.7 81.8 77.2</cell></row><row><cell>GhostELECTRA-small (m = 4/4)</cell><cell>1.7</cell><cell>14</cell><cell cols="3">82.3 88.3 88.5 64.7 91.9 88.4 55.8 83.5 80.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>pruned BERT with and without ghost modules. For fair comparison, for the pruned model without ghost module, we use the same training procedure as Section 2.3. As can be seen, adding the ghost modules achieves considerable improvement with negligible additional memory and computation. Comparison of GhostBERT and pruned BERT. m stands for the width multiplier.</figDesc><table><row><cell></cell><cell cols="2">Pruned BERT</cell><cell></cell><cell cols="2">GhostBERT</cell><cell></cell></row><row><cell>m</cell><cell>FLOPs</cell><cell>#params</cell><cell>Avg.</cell><cell>FLOPs</cell><cell>#params</cell><cell>Avg.</cell></row><row><cell></cell><cell>(G)</cell><cell>(M)</cell><cell>acc</cell><cell>(G)</cell><cell>(M)</cell><cell>acc</cell></row><row><cell>1/12</cell><cell>2.0</cell><cell>32</cell><cell>78.2</cell><cell>2.0</cell><cell>32</cell><cell>80.3</cell></row><row><cell>3/12</cell><cell>5.8</cell><cell>46</cell><cell>81.8</cell><cell>5.8</cell><cell>46</cell><cell>83.1</cell></row><row><cell cols="2">6/12 11.3</cell><cell>67</cell><cell cols="2">82.2 11.3</cell><cell>67</cell><cell>83.5</cell></row><row><cell cols="2">9/12 16.9</cell><cell>88</cell><cell cols="2">83.2 16.9</cell><cell>88</cell><cell>84.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of data augmentation (DA), knowledge distillation (KD), softmax normalization over the convolution kernel, and nonlinearity. Results on the GLUE development set are reported.</figDesc><table><row><cell cols="2">GhostBERT</cell><cell>84.3</cell><cell>91.6</cell><cell cols="2">91.4 72.9 94.6</cell><cell>86.5</cell><cell>53.9</cell><cell>89.2</cell><cell>83.1</cell><cell></cell></row><row><cell>3/12</cell><cell cols="2">-DA &amp; KD 80.2 -Softmax 84.3</cell><cell>88.5 91.5</cell><cell cols="2">90.0 63.2 91.6 90.9 71.8 92.3</cell><cell>83.8 85.5</cell><cell>52.5 52.6</cell><cell>86.7 89.1</cell><cell>79.6 82.3</cell><cell></cell></row><row><cell></cell><cell>-ReLU</cell><cell>84.0</cell><cell>91.7</cell><cell cols="2">91.0 70.8 92.3</cell><cell>85.8</cell><cell>47.6</cell><cell>88.6</cell><cell>81.5</cell><cell></cell></row><row><cell cols="2">GhostBERT</cell><cell>82.8</cell><cell>90.0</cell><cell cols="2">90.5 66.1 92.8</cell><cell>86.0</cell><cell>46.1</cell><cell>87.8</cell><cell>80.3</cell><cell></cell></row><row><cell>1/12</cell><cell cols="2">-DA &amp; KD 76.0 -Softmax 82.6</cell><cell>83.4 90.0</cell><cell cols="2">86.6 58.1 86.6 90.4 65.3 92.1</cell><cell>80.6 85.5</cell><cell>35.8 40.8</cell><cell>84.4 88.1</cell><cell>73.9 79.4</cell><cell></cell></row><row><cell></cell><cell>-ReLU</cell><cell>82.7</cell><cell>89.8</cell><cell cols="2">90.6 60.3 92.0</cell><cell>84.8</cell><cell>37.8</cell><cell>87.1</cell><cell>78.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Figure 3: Average score over five tasks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">with various kernel sizes of DWConv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">in the ghost module.</cell></row><row><cell></cell><cell>Conv1D S</cell><cell></cell><cell cols="2">16.6</cell><cell>88</cell><cell>83.6</cell><cell>91.3</cell><cell cols="2">91.1 70.0 92.5</cell><cell>86.5</cell><cell>52.3</cell><cell>89.3</cell><cell>82.1</cell></row><row><cell>3/12</cell><cell>Conv1D F Conv2D</cell><cell></cell><cell></cell><cell>7.6 5.8</cell><cell>47 46</cell><cell>84.0 83.7</cell><cell>91.5 91.7</cell><cell cols="2">91.0 61.7 92.2 91.0 68.2 92.5</cell><cell>87.0 86.5</cell><cell>48.9 50.4</cell><cell>89.0 89.3</cell><cell>80.7 81.7</cell></row><row><cell></cell><cell cols="2">ours: DWConv</cell><cell></cell><cell>5.8</cell><cell>46</cell><cell>84.3</cell><cell>91.6</cell><cell cols="2">91.4 72.9 94.6</cell><cell>86.5</cell><cell>53.9</cell><cell>89.2</cell><cell>83.1</cell></row><row><cell></cell><cell>Conv1D S</cell><cell></cell><cell cols="2">12.9</cell><cell>74</cell><cell>82.6</cell><cell>90.1</cell><cell cols="2">90.4 66.1 92.0</cell><cell>85.5</cell><cell>47.5</cell><cell>88.2</cell><cell>80.3</cell></row><row><cell>1/12</cell><cell>Conv1D F Conv2D</cell><cell></cell><cell></cell><cell>3.9 2.1</cell><cell>33 32</cell><cell>82.2 81.7</cell><cell>86.6 89.2</cell><cell cols="2">90.0 54.9 92.3 90.1 63.2 91.7</cell><cell>72.8 83.8</cell><cell>31.2 37.9</cell><cell>87.3 88.0</cell><cell>74.7 78.2</cell></row><row><cell></cell><cell cols="2">ours: DWConv</cell><cell></cell><cell>2.0</cell><cell>32</cell><cell>82.8</cell><cell>90.0</cell><cell cols="2">90.5 66.1 92.8</cell><cell>86.0</cell><cell>46.1</cell><cell>87.8</cell><cell>80.3</cell></row></table><note>m Model MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg. m Convolution Type FLOPs(G) #params(M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different convolutions in the ghost module. The convolution kernel size is 3. The backbone model is BERT-base. Results on the GLUE development set are reported.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>; Sanh   </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ghost Ghost Ghost</cell><cell>Linear</cell><cell cols="2">Linear Linear Linear</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NonLinear NonLinear NonLinear</cell><cell cols="2">NonLinear NonLinear NonLinear</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input Input Input</cell><cell cols="2">Input Input Input</cell><cell></cell></row><row><cell></cell><cell>Mat Mul Mat Mul Mat Mul</cell><cell></cell><cell>Ghost Ghost Ghost</cell><cell></cell><cell></cell><cell>Ghost Ghost Ghost</cell><cell></cell><cell></cell><cell>Linear Linear</cell><cell>Ghost Ghost</cell></row><row><cell>Ghost Ghost Ghost</cell><cell></cell><cell></cell><cell></cell><cell>Mat Mul Mat Mul Mat Mul</cell><cell></cell><cell></cell><cell>Mat Mul Mat Mul Mat Mul</cell><cell></cell><cell>Ghost Ghost</cell><cell>Linear Linear</cell></row><row><cell cols="2">Scale &amp; Softmax Scale &amp; Softmax Scale &amp; Softmax</cell><cell></cell><cell cols="2">Scale &amp; Softmax Scale &amp; Softmax Scale &amp; Softmax</cell><cell></cell><cell cols="2">Scale &amp; Softmax Scale &amp; Softmax Scale &amp; Softmax</cell><cell></cell><cell>NonLinear NonLinear</cell><cell>NonLinear NonLinear</cell></row><row><cell cols="2">Mat Mul Mat Mul Mat Mul</cell><cell></cell><cell cols="2">Mat Mul Mat Mul Mat Mul</cell><cell></cell><cell cols="2">Mat Mul Mat Mul Mat Mul</cell><cell></cell></row><row><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell><cell>Linear Linear Linear</cell></row><row><cell></cell><cell>Input Input Input</cell><cell></cell><cell></cell><cell>Input Input Input</cell><cell></cell><cell></cell><cell>Input Input Input</cell><cell></cell><cell>Input Input</cell><cell>Input Input</cell></row><row><cell cols="2">(a) After QK.</cell><cell></cell><cell cols="3">(b) After V.</cell><cell cols="2">(c) After O.</cell><cell cols="2">(d) After FFN1.</cell><cell>Ghost Ghost (e) After FFN2.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mat Mul Mat Mul</cell><cell>Ghost Ghost</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ghost Ghost</cell><cell></cell><cell></cell><cell></cell><cell>Mat Mul Mat Mul</cell><cell>Mat Mul Mat Mul</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Scale &amp; Scale &amp;</cell><cell></cell><cell cols="2">Scale &amp; Scale &amp;</cell><cell>Scale &amp; Scale &amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Softmax Softmax</cell><cell></cell><cell cols="2">Softmax Softmax</cell><cell>Softmax Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mat Mul Mat Mul</cell><cell></cell><cell cols="2">Mat Mul Mat Mul</cell><cell>Mat Mul Mat Mul</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell><cell>Linear Linear</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input Input</cell><cell></cell><cell></cell><cell>Input Input</cell><cell>Input Input</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different positions to add the ghost module. Average development set accuracy on five GLUE tasks (RTE, SST-2, MRPC, CoLA and STS-B) are reported. The pruned BERT with width multiplier 1/12 is used as backbone model .</figDesc><table><row><cell>Position</cell><cell>Convolution Type</cell><cell>FLOPs (G)</cell><cell>#params (M)</cell><cell>Avg. acc</cell></row><row><cell>After QK</cell><cell>Conv2D</cell><cell>5.4</cell><cell>45</cell><cell>75.9</cell></row><row><cell>After V</cell><cell>DWConv</cell><cell>3.6</cell><cell>39</cell><cell>74.3</cell></row><row><cell>After O</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>74.4</cell></row><row><cell>After FFN1</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>74.3</cell></row><row><cell>After FFN2</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>74.3</cell></row><row><cell cols="2">Ours: After O&amp;FFN2 DWConv</cell><cell>2.0</cell><cell>32</cell><cell>75.8</cell></row><row><cell cols="5">et al., 2020), structured pruning prunes away a</cell></row><row><cell cols="5">group of parameters without changing the model</cell></row><row><cell cols="5">topology and is more favored for hardware and real</cell></row><row><cell>inference speedup.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">In the width direction, Michel et al. (2019); Voita</cell></row><row><cell cols="5">et al. (2019) retain the performance after pruning a</cell></row><row><cell cols="5">large percentage of attention heads in a structured</cell></row><row><cell cols="5">manner. Besides attention heads, McCarley et al.</cell></row><row><cell>(</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Statistics of the GLUE datasets. All tasks are single-sentence or sentence-pair classification tasks, except STS-B, which is a regression task. MNLI has three classes while all other classification tasks have two.</figDesc><table><row><cell cols="3">Corpus Train Test Task</cell><cell>Metrics</cell></row><row><cell></cell><cell></cell><cell>Single-Sentence Tasks</cell><cell></cell></row><row><cell>CoLA</cell><cell>8.5k</cell><cell>1k acceptability</cell><cell>Matthews corr.</cell></row><row><cell>SST-2</cell><cell cols="2">67k 1.8k sentiment</cell><cell>acc.</cell></row><row><cell></cell><cell cols="2">Similarity and Paraphrase Tasks</cell><cell></cell></row><row><cell>MRPC</cell><cell cols="2">3.7k 1.7k paraphrase</cell><cell>acc.</cell></row><row><cell>STS-B</cell><cell cols="3">7k 1.4k sentence similarity Spearman corr.</cell></row><row><cell>QQP</cell><cell cols="2">364k 391k paraphrase</cell><cell>acc.</cell></row><row><cell></cell><cell></cell><cell>Inference Tasks</cell><cell></cell></row><row><cell>MNLI</cell><cell>393k</cell><cell>20k NLI</cell><cell>matched acc.</cell></row><row><cell>QNLI</cell><cell cols="2">105k 5.4k QA/NLI</cell><cell>acc.</cell></row><row><cell>RTE</cell><cell>2.5k</cell><cell>3k NLI</cell><cell>acc.</cell></row><row><cell>WNLI</cell><cell>634</cell><cell>146 coreference/NLI</cell><cell>acc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters for the distillation and finetuning stages in training GhostBERT on the GLUE benchmark.</figDesc><table><row><cell>1 https://gluebenchmark.com/faq</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of different kernel sizes on the development set on five tasks of GLUE. m stands for the width multiplier. m Model FLOPs(G) #params(M) MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.</figDesc><table><row><cell cols="6">m Kernel Size SST-2 MRPC CoLA STS-B RTE Avg.</cell></row><row><cell></cell><cell>1</cell><cell>92.5</cell><cell>85.3</cell><cell>54.8</cell><cell>89.1 70.8 78.5</cell></row><row><cell></cell><cell>3</cell><cell>94.6</cell><cell>86.5</cell><cell>53.9</cell><cell>89.2 72.9 79.4</cell></row><row><cell>3/12</cell><cell>5</cell><cell>92.7</cell><cell>86.5</cell><cell>53.5</cell><cell>89.0 70.4 78.4</cell></row><row><cell></cell><cell>9</cell><cell>92.6</cell><cell>85.5</cell><cell>53.4</cell><cell>89.0 69.3 78.0</cell></row><row><cell></cell><cell>17</cell><cell>92.4</cell><cell>84.8</cell><cell>53.3</cell><cell>88.9 68.2 77.5</cell></row><row><cell></cell><cell>1</cell><cell>92.1</cell><cell>85.3</cell><cell>41.7</cell><cell>87.6 64.3 74.2</cell></row><row><cell></cell><cell>3</cell><cell>92.8</cell><cell>86.0</cell><cell>46.1</cell><cell>87.8 66.1 75.8</cell></row><row><cell>1/12</cell><cell>5</cell><cell>92.2</cell><cell>85.3</cell><cell>41.1</cell><cell>87.5 64.6 74.2</cell></row><row><cell></cell><cell>9</cell><cell>92.1</cell><cell>84.8</cell><cell>41.4</cell><cell>87.5 63.9 73.9</cell></row><row><cell></cell><cell>17</cell><cell>92.0</cell><cell>84.3</cell><cell>40.9</cell><cell>87.5 63.5 73.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Development set results of the pruned BERT and GhostBERT. m is the width multiplier of the model. Position Convolution Type FLOPs(G) #params(M) RTE SST-2 MRPC CoLA STS-B Avg.</figDesc><table><row><cell>After QK</cell><cell>Conv2D</cell><cell>5.4</cell><cell>45</cell><cell>65.3 91.9</cell><cell>85.8</cell><cell>48.5</cell><cell>87.9 75.9</cell></row><row><cell>After V</cell><cell>DWConv</cell><cell>3.6</cell><cell>39</cell><cell>62.1 91.6</cell><cell>86 7</cell><cell>44.3</cell><cell>87.7 74.3</cell></row><row><cell>After O</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>63.2 91.7</cell><cell>85.5</cell><cell>43.8</cell><cell>87.8 74.4</cell></row><row><cell>After FFN1</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>64.6 91.2</cell><cell>84.6</cell><cell>43.3</cell><cell>87.7 74.3</cell></row><row><cell>After FFN2</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>62.5 91.5</cell><cell>84.8</cell><cell>44.9</cell><cell>87.7 74.3</cell></row><row><cell>Ours: After O&amp;FFN2</cell><cell>DWConv</cell><cell>2.0</cell><cell>32</cell><cell>66.1 92.8</cell><cell>86.0</cell><cell>46.1</cell><cell>87.8 75.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison of different ghost positions on the development set on five tasks of GLUE. BERT-base is set as backbone model with the width multiplier 1/12.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank MindSpore for the partial support of this work, which is a new deep learning computing framework.</p><p>Given the superior performance of Huawei Ascend AI Processor and MindSpore computing framework, our code will be released based on MindSpore at (https: //gitee.com/mindspore/mindspore/tree/ master/model_zoo/research/nlp/ghostbert).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Binarybert: Pushing the limit of bert quantization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adabert: Task-adaptive BERT compression with differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ELECTRA: pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In North American Chapter of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07320</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compressing bert: Studying the effects of weight pruning on transfer learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrews</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08307</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynabert: Dynamic bert with adaptive width and depth</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeezebert: What can computer vision teach nlp about efficient neural networks?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</title>
				<meeting>SustaiNLP: Workshop on Simple and Efficient Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convbert: Improving BERT with span-based dynamic convolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4356" to="4365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fastbert: a self-distilling bert with adaptive inference time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structured pruning of a bert-based question answering model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06360</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delight: Very deep and light-weight transformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR, abs/2008.00623</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://www.mindspore.cn.2021" />
		<title level="m">MindSpore</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mad-x: An adapter-based framework for multi-task cross-lingual transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12327</idno>
		<title level="m">A primer in bertology: What we know about how bert works</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bert and pals: Projected attention layers for efficient adaptation in multi-task learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5986" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deebert: Dynamic early exiting for accelerating bert inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ternarybert: Distillation-aware ultra-low bit bert</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bert loses patience: Fast and robust inference with early exit</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
