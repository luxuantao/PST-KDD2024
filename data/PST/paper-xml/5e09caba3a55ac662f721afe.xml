<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Adversarial Training with Transferable Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-27">27 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haizhong</forename><surname>Zheng</surname></persName>
							<email>hzzheng@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
							<email>ziqizh@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
							<email>jcgu@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
							<email>aprakash@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Adversarial Training with Transferable Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-27">27 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.11969v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial training is an effective defense method to protect classification models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we first show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efficiency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and requires 12 ∼ 14× less training time on MNIST and CIFAR10 datasets with comparable model robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art deep learning models for computer vision tasks have been found to be vulnerable to adversarial examples <ref type="bibr" target="#b30">[31]</ref>. Even a small perturbation on the image can fool a well-trained model. Recent works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> also show that adversarial examples can be physically realized, which can lead to serious safety issues. Design of robust models, which correctly classifies adversarial examples, is an active research area <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>, with adversarial training <ref type="bibr" target="#b20">[21]</ref> being one of the most effective methods. It formulates training as a game between adversarial attacks and the model: the stronger the adversarial examples generated to train the model are, the more robust the model is.</p><p>To generate strong adversarial examples, iterative attacks <ref type="bibr" target="#b15">[16]</ref>, which use multiple attack iterations to generate adversarial examples, are widely adopted in various adversarial training methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>. Since adversarial perturbations are usually bounded by a constrained space S and attack perturbations outside S need to be projected back to S, k-step projected gradient descent method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> (PGDk) has been widely adopted to generate adversarial examples. Typically, using more attack iterations (higher value of k) produces stronger adversarial examples <ref type="bibr" target="#b20">[21]</ref>. However, each attack iteration needs to compute the gradient on the input, which causes a large computational overhead. As shown in Table <ref type="table">1</ref>, the training time of adversarial training can be close to 100 times larger than natural training.</p><p>Recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> show that adversarial examples can be transferred between models: adversarial examples generated for one model can still stay adversarial to another model. The key insight in our work, which we experimentally verified, is that, because of high transferability between models (i.e., checkpoints) from neighboring training epochs, attack strength can be accumulated across epochs by repeatedly reusing the adversarial perturbation from the previous epoch.</p><p>We take advantage of this insight in coming up with a novel adversarial training method called ATTA (Adversarial Training with Transferable Adversarial examples) that can be significantly faster than state-of-the-art methods while achieving similar model robustness. In traditional adversarial training, when a new epoch begins, the attack algorithm generates adversarial examples from natural images, which ignores the fact that these perturbations can be reused effectively. In contrast, we show that it can be advantageous to reuse these adversarial perturbations through epochs. Even using only one attack iteration to generate adversarial examples, ATTA-1 can still achieve comparable robustness with respect to traditional PGD-40.</p><p>We apply our technique on Madry's Adversarial Training method (MAT) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b38">[39]</ref> and evaluate the performance on both MNIST and CIFAR10 dataset. Compared with traditional PGD attack, our method improves training efficiency by up to 12.2× (14.1×) on MNIST (CI-FAR10) with comparable model robustness. Trained with ATTA, the adversarial accuracy of MAT can be improved up to 7.2%. Noticeably, for MNIST, with one attack iteration, our method can achieve 96.31% adversarial accuracy within 5 minutes. For CIFAR10, compared to MAT whose training time is more than one day, our method achieves comparable adversarial accuracy in about 2 hours.</p><p>We also compare our method with two fast adversarial training methods, YOPO <ref type="bibr" target="#b36">[37]</ref> and Free <ref type="bibr" target="#b26">[27]</ref>. Evaluation results show that our method is both more effective and efficient than these two methods.</p><p>Contribution. To the best of our knowledge, our work is the first to enhance the efficiency and effectiveness of adversarial training by taking advantage of high transferability between models from different epochs. In summary, we make the following contributions:</p><p>• We are the first to reveal the high transferability between models of neighboring epochs in adversarial training. With this property, we verify that the attack strength can be accumulated across epochs by reusing adversarial perturbations from the previous epoch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adversarial training and transferability</head><p>In this section, we introduce relevant background on adversarial training and transferability of adversarial examples. We also discuss the trade-off between training time and model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adversarial Training</head><p>Adversarial training is an effective defense method to train robust models against adversarial attacks. By using adversarial attacks as a data augmentation method, a model trained with adversarial examples achieves considerable robustness. Recently, lots of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> focuse on analyzing and improving adversarial machine learning. Madry et al. <ref type="bibr" target="#b20">[21]</ref> first formulate adversarial training as a min-max optimization problem:</p><formula xml:id="formula_0">min f ∈H E (x,y)∼D [max δ∈S L(f (x + δ), y)] (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where H is the hypothesis space, D is the distribution of the training dataset, L is a loss function, and S is the allowed perturbation space that is usually selected as an L-p norm ball around x. The basic strategy in adversarial training is, given a natural image x, to find a perturbed image x + δ that maximizes the loss with respect to correct classification. The model is then trained on generated adversarial examples. In this work, We consider adversarial examples with a high loss to have high attack strength. PGD-k attack based adversarial training: Unfortunately, solving the inner maximization problem is hard. Iterative attack <ref type="bibr" target="#b15">[16]</ref> is commonly used to generate strong adversarial examples as an approximate solution for the inner maximization problem of Equation <ref type="formula" target="#formula_0">1</ref>. Since adversarial perturbations are usually bounded by the allowed perturbation space S, PGD-k (k-step projected gradient descent <ref type="bibr" target="#b15">[16]</ref>) is adopted to conduct iterative attack <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. PGD-k adversarial attack is the multi-step projected gradient descent on a negative loss function:</p><formula xml:id="formula_2">x t+1 = Π x+S (x t + α sgn(∆ xt L(θ, x t , y)))</formula><p>In the above, x t is the adversarial example in the t-th attack iteration, α is the attack step size, and Π is the projection function to project adversarial examples back to the allowed perturbation space S.</p><p>With a higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss <ref type="bibr" target="#b20">[21]</ref> els. This property is named as transferability. This property is usually leveraged to perform a black-box attack <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. To attack a targeted model f t , the attacker generates transferable adversarial examples from the source model f s . The higher the transferability between f s and f t is, the higher the success rate the attack has. Substitute model training is a commonly used method to train a source model f s . Rather than the benchmark label y, f s is trained with f t (x) which is the prediction result of the targeted model <ref type="bibr" target="#b23">[24]</ref> to achieve a higher black-box attack success rate. While our work does not use black-box attacks, we do rely on a similar intuition as behind substitute model training, namely, two models with similar behavior and decision boundaries are likely to have higher transferability between each other. We use this intuition to show high transferability between models from neighboring training epochs, as discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attack strength accumulation</head><p>In this section, we first conduct a study and find that models from neighboring epochs show very high transferability and are naturally good substitute models to each other. Based on this observation, we design an accumulative PGD-k attack that accumulates attack strength by reusing adversarial perturbations from one epoch to the next. Compared to traditional PGD-k attack, accumulative PGD-k attack achieves much higher attack strength with fewer number of attack iterations in each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High transferability between epochs</head><p>Transferability between models in different training epochs of the same training program has not been studied. Because fluctuations of model parameters between different epochs are very small, we think that they are likely to have similar behavior and similar decision boundaries, which should lead to high transferability between these models.</p><p>To evaluate the transferability between models from training epochs, we adversarially train a model T as the targeted model, while saving intermediate models at the end of three immediately prior epochs as T 1 , T 2 , and T 3 , with T 3 being the model from the epoch immediately prior to T . We measure the transferability of adversarial examples of each of T i with T . For comparison, we also train three additional models S 1 , S 2 , and S 3 that are trained by exactly the same training method as T but with different random seeds. And we measure the transferability between each of S i and T .</p><p>To measure transferability from the source model to the targeted model, we use two metrics. The first metric is error rate transferability used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, which is the ratio of the number of adversarial examples misclassified by source model to that of the targeted model. The other metric is loss transferability, which is the ratio of the loss value caused by adversarial examples on the source model to the loss value caused by the same examples on the targeted model.</p><p>We conduct experiments on both MNIST and CIFAR10 dataset, and the results are shown on Figure <ref type="figure" target="#fig_0">1</ref>. We find that, compared to the baseline models S i , the models T i from neighboring epochs of T have higher transferability for both transferability metrics (the transferability metrics for all models T i are larger than 0.93). This provides strong empirical evidence that adversarial examples generated in one epoch still retain some strength in subsequent epochs.</p><p>Inspired by the above result, we state the following hypothesis. Hypothesis: </p><formula xml:id="formula_3">Repeatedly</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Accumulative PGD-k attack</head><p>To validate the aforementioned hypothesis, we design an accumulative PGD-k attack. As shown in Figure <ref type="figure" target="#fig_1">2b</ref>, we longitudinally connect models in each epoch by directly reusing the attack perturbation of the previous epoch. Accumulative PGD-k attack Figure <ref type="figure" target="#fig_1">2b</ref> generates adversarial examples for f n−m+1 first. Then, for the following epochs, the attack is performed based on the accumulated perturbations from previous epochs. To compare the attack strength of two attacks, we use Madry's method <ref type="bibr" target="#b20">[21]</ref> to adversarially train two models on MNIST and CIFAR10 and evaluate the loss value L(f n (x * n ), y) of adversarial examples generated by two attacks. Figure <ref type="figure" target="#fig_4">3</ref> summarises the evaluation result. We can find that, with more epochs m involved in the attack, accumulative PGD-k attack can achieve a higher loss value with the same number of attack iterations k.</p><p>Especially, when adversarial examples are transferred through a large number of epochs, even accumulative PGD-1 attack can cause high attack loss. For MNIST, accumulative PGD-1 attack can achieve the same attack loss as traditional PGD-35 attack when m = 40. For CIFAR10, accumulative PGD-1 attack can achieve the same attack loss as traditional PGD-12 attack when m = 10. This result indicates that, with high transferability between epochs, adversarial perturbations can be reused ef-   </p><formula xml:id="formula_4">x * i+1 = A(f i+1 , x, y, C(x * i ))</formula><p>where A is the attack algorithm, C is a connection function (described in the next section). As shown in the previous section, adversarial examples can achieve high attack strength as they are transferred across epochs via the above linking process, rather than starting from natural images. This, in turn, should allow us to train a more robust model with fewer attack iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Connection function design</head><p>Designing connection function can help us overcome two challenges that we encountered in achieving high transferability of adversarial examples from one epoch to the next:</p><p>1. Data augmentation problem: Data augmentation is a commonly used technique to improve the performance of deep learning. It applies randomly transformation on original images so that models can be trained with various images in different epochs. This difference can cause a mismatch of images between epochs. Since the perturbation is calculated with the gradient of image, if we directly reuse the perturbation, the mismatch between reused perturbation and new augmented image can cause a decrease in attack strength. Simply removing data augmentation also hurts the robustness. We experimentally show this negative influence of these two alternatives in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Drastic model parameter change problem:</head><p>As discussed in Section 3, similar parameters between models tends to cause a similar decision boundary and thus high transferability. Unfortunately, model parameters tend to change drastically at the early stages of training. Thus, adversarial perturbations in early epochs tend to be useless for subsequent epochs. Overcoming challenges: Inverse data augmentation. To address the first issue, we propose a technique called inverse data augmentation so that adversarial examples retain a high transferability between training epochs despite data augmentation. Figure <ref type="figure" target="#fig_6">5</ref> shows the workflow with inverse data augmentation. Some transformations (like cropping and rotation) pad augmented images with background pixels. To transfer a perturbation on background pixels, our method transfers the padded image x i rather than standard images so that the background pixels used by data augmentation can be covered by these paddings.</p><p>After the adversarial perturbation of augmented image is generated, we can perform the inverse transformation<ref type="foot" target="#foot_1">2</ref> T −<ref type="foot" target="#foot_0">1</ref> (•) to calculate the inversed perturbation T −1 (∆ i ) on the padded image x i . By adding T −1 (∆ i ) to x i , we can store and transfer all perturbation information in x i+1 to next epoch. (Note that, when we perform the adversarial attack on the augmented image x i,aug , the perturbation is still bounded by the natural image rather than x i,aug .)</p><p>Periodically reset perturbation. To solve the second issue, we propose a straightforward but effective solution: our method resets the perturbation and lets adversarial perturbations be accumulated from the beginning periodically, which mitigates the impact caused by early perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attack Loss</head><p>Various attack loss functions are used in different adversarial training algorithms. For example, TRADES <ref type="bibr" target="#b38">[39]</ref> uses robustness loss (Equation <ref type="formula" target="#formula_5">2</ref>) as the loss to generate the adversarial examples.</p><formula xml:id="formula_5">L(f (x * ), f (x))<label>(2)</label></formula><p>where L is the loss function, f is the model and x, x * are natural and adversarial example respectively. This loss represents how much the adversarial example diverges from the natural image. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> shows that this loss has a better performance in the TRADES algorithm.</p><p>In our method, we use the following loss function:</p><formula xml:id="formula_6">L(f (x * ), y)<label>(3)</label></formula><p>It represents how much the adversarial examples diverges to the benchmark label.</p><p>The objection (f (x)) of Equation <ref type="formula" target="#formula_5">2</ref>for adversarial attack varies across epochs, which may weaken the transferability between epochs. Equation 3 applies a fixed objection (y), which doesn't have this concern. In addition, Equation 3 has a smaller computational graph, which can reduce computational overhead during training.</p><p>The overall training method is described in Algorithm 1. β ← a small random perturbation 8:</p><p>x ← x nat + β Store the transformation T aug for the inverse augmentation: 11:</p><p>x aug , x nat,aug , T aug ← data aug(x, x adv ) 12:</p><p>x * ← A(f θ , x nat,aug , y, x aug , ) 13:</p><formula xml:id="formula_7">θ ← θ − ∇ f θ ∂L(f θ ,x * ,y) ∂θ 14:</formula><p>x ← inverse aug(x, x * , T aug )</p><p>15:</p><p>end for 16: end for <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In this section, we integrate ATTA with two popular adversarial training methods: Madry's Adversarial Training (MAT) <ref type="bibr" target="#b20">[21]</ref> and TRADES <ref type="bibr" target="#b38">[39]</ref>. By evaluating the training time and robustness, we show that ATTA can provide a better trade-off than other adversarial training methods. To understand the contribution of each component to robustness, we conduct an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Following the literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, we use both MNIST <ref type="bibr" target="#b16">[17]</ref> and CIFAR10 dataset <ref type="bibr" target="#b14">[15]</ref> to evaluate ATTA.</p><p>For the MNIST dataset, the model has four convolutional layers followed by three full-connected layers which is same architecture as used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. The adversarial perturbation is bounded by l ∞ ball with size = 0.3.</p><p>For the CIFAR10 dataset, we use the wide residual network  which is same as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. The perturbation is bounded by l ∞ ball with size = 0.031.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficiency and effectiveness of ATTA</head><p>In this part, we evaluate the training efficiency and the robustness of ATTA, comparing it to state-of-the-art adversarial training algorithms. To better verify effectiveness of ATTA, we also evaluate ATTA under various attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Training efficiency</head><p>We select four state-of-the-art adversarial training methods as baselines: MAT <ref type="bibr" target="#b20">[21]</ref>, TRADES <ref type="bibr" target="#b38">[39]</ref>, YOPO <ref type="bibr" target="#b36">[37]</ref> and Free <ref type="bibr" target="#b26">[27]</ref>. For MNIST, the model trained with ATTA can achieve a comparable robustness with up to 12.2 times training efficiency and, for CIFAR10, our method can achieve a comparable robustness with up to 14. CIFAR10. We summarise the experiment results of CI-FAR10 in Table <ref type="table" target="#tab_5">4</ref>. For MAT, compared to PGD-10, ATTA-1 achieves 3.89% higher adversarial accuracy with about 15.1 times training efficiency, and ATTA-10 improves adversarial accuracy by 7.26% with 2.9 times training efficiency when the model is trained with 10 attack iterations. For TRADES, ATTA-3 achieves comparable adversarial accuracy to PGD-10 with 5.3 times faster training efficiency. By comparing the experiment results with YOPO and Free, for MAT, our method is 3.78 (1.5) times faster than Free (YOPO) with 3.28% (2.72%) better adversarial accuracy.</p><p>To better understand the performance of ATTA, we present the trade-off between training time and robustness of different methods in Figure <ref type="figure" target="#fig_9">6</ref>. We find that our method (indicated by the solid markers in the left-top corner) gives </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Defense under other attacks</head><p>To verify the robustness of our method, we evaluate models in the previous section with other attacks: PGD-100 <ref type="bibr" target="#b15">[16]</ref>, FGSM <ref type="bibr" target="#b9">[10]</ref>, CW-20 <ref type="bibr" target="#b2">[3]</ref>.  As shown in Table <ref type="table" target="#tab_7">5</ref>, models trained with ATTA are still robust to other attacks. Compared to baselines, our method still achieves a comparable or better robustness under other attacks. We find that, although ATTA-40 has a similar robustness to PGD-40 under PGD-20 attack, with a stronger attack (e.g. PGD-100), ATTA-40 shows a better robustness (2% higher adversarial accuracy.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study</head><p>To study the contribution of each component to robustness, we do an ablation study on inverse data augmentation and different attack loss functions.</p><p>Inverse data augmentation. To study the robustness gain of inverse data augmentation(i.d.a.), we use ATTA-1 to adversarially train models by reusing the adversarial perturbation directly. As shown in Table <ref type="table" target="#tab_8">6</ref>, for both MAT (ATTA-1) and TRADES (ATTA-1), models trained with inverse data augmentation achieve about 6% higher accuracy, which means that inverse data augmentation does help improve the transferability between training epochs. As discussed in 4.1, another alternative is to remove data augmentation. However, Table <ref type="table" target="#tab_8">6</ref> shows that removal of data augmentation hurts both natural accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack</head><p>Natural PGD-20 Attack loss. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> show that, for TRADES, using Equation 2 leads to a better robustness. However, in this attack loss, we noticed that both inputs to the loss function L are related to the model f . Since the model f is updated every epoch, compared to Equation 3 whose y is fixed, the instability of f (x * ) and f (x) may have a larger influence on transferability. To analyze the performance difference between these two attack losses in ATTA, we train two TRADES(ATTA-1) models with different attack losses. In Table <ref type="table" target="#tab_9">7</ref>, we find that Equation 3 leads to 2.48% higher accuracy against PGD-20 attack. This result suggests that the higher stability of Equation 3 helps ATTA increase transferability between training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack</head><p>Natural PGD-20 TRADES loss 85.95% 52.08%</p><p>MAT loss 85.04% 54.50% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Adversarial training is first proposed in <ref type="bibr" target="#b15">[16]</ref> and is formulated as a min-max optimization problem <ref type="bibr" target="#b20">[21]</ref>. As one of the most effective defense methods, lots of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> focus on enhancing either the efficiency or effectiveness of adversarial training. YOPO <ref type="bibr" target="#b36">[37]</ref> finds that an adversary update is majorly coupled with the first layer. It can speed up the training progress by just updating the first layer. Shafahi et al. <ref type="bibr" target="#b26">[27]</ref> improve the training efficiency by recycling the gradient information computed when updating model parameters to generate adversarial examples. In <ref type="bibr" target="#b38">[39]</ref>, TRADES improves the robustness of an adversarially trained model by adding a robustness regularizer in the loss function.</p><p>Transferability of adversarial examples. Szegedy et al. <ref type="bibr" target="#b30">[31]</ref> first describes the transferability of adversarial examples. This property is usually used to perform black-box attack between models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. Lie et al. <ref type="bibr" target="#b19">[20]</ref> show that adversarial examples generated by an ensemble of multiple models are more transferable to a targeted model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Scalability to large dataset. One downside to our method is the extra space needed to store the adversarial perturbation for each image, which may limit the scalability of ATTA when the model is trained on larger datasets (e.g., ImageNet). However, we believe this is not going to be a serious issue, since asynchronous IO pipelines are widely implemented in existing DL frameworks <ref type="bibr" target="#b5">[6]</ref>, which should allow ATTA to store perturbations data on hard disks without significantly impacting efficiency.</p><p>Transferability between training epochs. Adversarial attacks augment the training data to improve the model robustness. Our work points out that, unlike images augmented by traditional data augmentation methods that are independent between epochs, adversarial examples generated by adversarial attacks show high relevance transferability between epochs. We hope this finding can inspire other researchers to enhance adversarial training from a new perspective (e.g., improving transferability between epochs). which is same with other works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. We set the epoch period to reset perturbation as 10 epochs. Following YOPO <ref type="bibr" target="#b36">[37]</ref>, the model is trained for 40 epochs with an initial 0.1 learning rate, a 0.01 learning rate after 30 epochs, and a 0.001 learning rate after 35 epochs. To evaluate the model robustness, we perform the PGD, M-PGD and CW attack with a 0.003 step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>For the baseline, we use the author implementation of MAT <ref type="foot" target="#foot_3">4</ref> [21], TRADES <ref type="foot" target="#foot_4">5</ref> [39], YOPO<ref type="foot" target="#foot_5">6</ref>  <ref type="bibr" target="#b38">[39]</ref>, and Free<ref type="foot" target="#foot_6">7</ref>  <ref type="bibr" target="#b26">[27]</ref> with the hyper-parameters recommended in their works, and we select 1/λ as 6 for TRADES (both ATTA and PGD).</p><p>In Section 3, which analyzes the transferability between training epochs, we use MAT with PGD-10 to train models and PGD-20 to calculate loss value and error rate.</p><p>Each experiment is taken on one idle NVIDIA GeForce RTX 2080 Ti GPU. Except PGD attack, we implement other attacks with Adversarial Robustness Toolbox <ref type="bibr" target="#b21">[22]</ref>.  We find that, although ATTA-1 just performs one attack iteration in each epoch, it generates similar perturbations to PGD-40 (PGD-10) in MNIST (CIFAR10). The effect of inverse data augmentation is shown in Figure <ref type="figure" target="#fig_13">7b</ref>. There are some perturbations on the padded pixels in the third row (ATTA-1), but perturbations just generated by PGD-10 (shown in fifth row) just appear on cropped pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Complete evaluation results</head><p>We put the complete evaluation result in this section as a supplement to Section 5.2.  We evaluate defense methods under additional attacks and the evaluation results are shown in Table <ref type="table" target="#tab_10">8</ref> and Table <ref type="table">9</ref>. Similar to the conclusion stated in Section 5.2, compared to other methods, ATTA achieves comparable robustness with much less training time, which shows a better trade-off on training efficiency and robustness. With the same number of attack iterations, ATTA needs less time to train the model. As mentioned in Section 3.2, with the accumulation of adversarial perturbations, ATTA can use the same number of attack iterations to achieve a higher attack strength, which helps the model converge faster.</p><p>Natural accuracy v.s. Adversarial accuracy. In this paper, we find that higher adversarial accuracy can lower natural accuracy. This trade-off has been observed and explained in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. A recent work <ref type="bibr" target="#b12">[13]</ref> points out that features used by naturally trained models are highly-predictive but not robust and adversarially trained models tend to use robust features rather than highly-predictive features, which may cause this trade-off. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Error rate transferability and loss transferability with the different source models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The traditional PGD-k attack (a) and the accumulative PGD-k attack through m epochs (b). The PGD-k attack is performed on the model in the red rectangle to get adversarial examples.</figDesc><graphic url="image-2.png" coords="4,338.12,231.66,188.98,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>fectively, which allows us to use fewer attack iterations to generate the same or stronger adversarial examples. Reuse of perturbations across epochs can help us reduce the number of attack iterations k in PGD-k, leading to more efficient adversarial training. Next section describes our proposed algorithm, ATTA (Adversarial Training with Transferable Adversarial examples), based on this property.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Given the number of epochs m, the relationship between the number of attack iterations k and the attack loss. m = 1 stands for the traditional PGD-k attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Traditional adversarial training (PGD-k) (a) and ATTA-k (b). C is the connection function which improves the transferability between epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The workflow of inversed data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 1 :</head><label>11</label><figDesc>Adversarial Training with Transferable Adversarial Examples (ATTA) Input: Padded training dataset D nat , model f θ , attack algorithm A, perturbation bound , the number of epochs to reset perturbation reset 2: Initialize θ 3: Initialize D by cloning D nat 4: for epoch = 1 • • • N do 5: for x nat , y in D nat and corresponding x ∈ D do 6: if epoch % reset = 0 then 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The scatter plot presents adversarial accuracy against PGD-20 attack and training time of different adversarial training methods on CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>C. 1</head><label>1</label><figDesc>Qualitative study on training imagesTo compare the quality of adversarial examples generated by PGD and ATTA, we visualize some adversarial examples generated by both methods. For MNIST, we choose the model checkpoint trained by MAT-ATTA-1 at epoch 40.For CIFAR10, we choose the model checkpoint trained by MAT-ATTA-1 at epoch 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7</head><label>7</label><figDesc>shows the adversarial examples and perturbations used to train the model (ATTA-1) and genereated by PGD-40 (PGD-10) attack on MNIST (CIFAR10) model in each class. To better visualize the perturbation, we re-scale the perturbation by calculating 2xp (where x p is the perturbation and is the L ∞ bound of adversarial attack). This shifts the L ∞ ball to the scale of [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of natural images, adversarial examples and corresponding perturbations in each class for MNIST and CIFAR10. The first row in (a) and (b) shows the natural images. The second and third rows show the adversarial examples and perturbations generated by ATTA-1. The fourth and fifth rows show the adversarial examples and perturbations generated by PGD-40 in (a) and PGD-10 in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>reusing perturbations from the previous epoch can accumulate attack strength epoch by epoch. Compared to current methods that iterate from natural examples in each epoch, this can allow us to use few attack iterations to generate the same strong adversarial examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>1 times training efficiency. Compared to MAT trained with PGD, our method improve the accuracy up to 7.2% with 3.7 times training efficiency. The result of different attacks on MNIST dataset.</figDesc><table><row><cell>Defense</cell><cell>Attack</cell><cell>Natural PGD-40</cell><cell>Time (sec)</cell></row><row><cell></cell><cell>PGD-1</cell><cell>99.52% 15.82%</cell><cell>226</cell></row><row><cell></cell><cell>PGD-40</cell><cell cols="2">99.37% 96.21% 3933</cell></row><row><cell>MAT</cell><cell cols="2">YOPO-5-10 99.15% 93.69%</cell><cell>789</cell></row><row><cell></cell><cell>ATTA-1</cell><cell>99.45% 96.31%</cell><cell>297</cell></row><row><cell></cell><cell>ATTA-40</cell><cell cols="2">99.23% 97.28% 4650</cell></row><row><cell></cell><cell>PGD-1</cell><cell>99.41% 39.53%</cell><cell>583</cell></row><row><cell>TRADES</cell><cell>PGD-40</cell><cell cols="2">98.89% 96.54% 6544</cell></row><row><cell></cell><cell>ATTA-1</cell><cell>99.03% 96.10%</cell><cell>460</cell></row><row><cell></cell><cell>ATTA-40</cell><cell cols="2">98.21% 96.03% 4660</cell></row></table><note>MNIST. The experiment results of MNIST are summarised in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>For MAT, to achieve comparable robustness, ATTA is about 12.2 times faster than the tradi-tional PGD training method. Even with one attack iteration, model trained with ATTA-1 achieves 96.31% adversarial accuracy within 297 seconds. For TRADES, we get a similar result. With one attack iteration, ATTA-1 achieves 96.1% adversarial accuracy within 460 seconds, which is about 13 times faster than TRADES(PGD-40). Compared to another fast adversarial training method YOPO, ATTA-1 is about 2.7 times faster and achieves higher robustness (96.1% versus YOPO's 93.69%).</figDesc><table><row><cell>Defense</cell><cell>Attack</cell><cell cols="2">Natural PGD-20</cell><cell>Time (min)</cell></row><row><cell></cell><cell>PGD-1</cell><cell>93.18%</cell><cell>22.3%</cell><cell>435</cell></row><row><cell></cell><cell>PGD-3</cell><cell cols="2">89.95% 41.38%</cell><cell>785</cell></row><row><cell></cell><cell>PGD-10</cell><cell cols="3">87.49% 47.07% 2027</cell></row><row><cell>MAT</cell><cell cols="3">Free(m = 8) 85.54% 47.68%</cell><cell>640</cell></row><row><cell></cell><cell>YOPO-5-3</cell><cell cols="2">86.43% 48.24%</cell><cell>335</cell></row><row><cell></cell><cell>ATTA-1</cell><cell cols="2">85.71% 50.96%</cell><cell>134</cell></row><row><cell></cell><cell>ATTA-3</cell><cell cols="2">85.44% 52.56%</cell><cell>267</cell></row><row><cell></cell><cell>ATTA-10</cell><cell cols="2">83.80% 54.33%</cell><cell>690</cell></row><row><cell></cell><cell>PGD-1</cell><cell cols="2">93.58% 35.52%</cell><cell>592</cell></row><row><cell></cell><cell>PGD-3</cell><cell cols="2">88.52% 54.20%</cell><cell>861</cell></row><row><cell></cell><cell>PGD-10</cell><cell cols="2">84.13% 56.6%</cell><cell>2028</cell></row><row><cell>TRADES</cell><cell cols="3">YOPO-3-4 3 87.82% 46.13%</cell><cell>-</cell></row><row><cell></cell><cell>ATTA-1</cell><cell cols="2">85.04% 54.50%</cell><cell>199</cell></row><row><cell></cell><cell>ATTA-3</cell><cell cols="2">84.23% 56.36%</cell><cell>320</cell></row><row><cell></cell><cell>ATTA-10</cell><cell cols="2">83.67% 57.34%</cell><cell>752</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The result of different attacks on CIFAR10 dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The robustness comparison between ATTA and PGD under other attacks. The first 'M' and 'T' stand for MAT and TRADES, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The accuracy under PGD attack of models trained by ATTA-1 with or without d.a.(data augmentation) and i.d.a. (inversed data augmentation).</figDesc><table><row><cell>MAT(w/o d.a., w/o i.d.a.)</cell><cell>82.53% 41.64%</cell></row><row><cell>MAT(w/ d.a., w/o i.d.a.)</cell><cell>91.65% 42.55%</cell></row><row><cell>MAT(w/ d.a., w/ i.d.a.)</cell><cell>85.71% 50.96%</cell></row><row><cell cols="2">TRADES(w/o d.a., w/o i.d.a.) 81.87% 41.65%</cell></row><row><cell cols="2">TRADES(w/ d.a., w/o i.d.a.) 90.46% 48.38%</cell></row><row><cell>TRADES(w/ d.a., w/ i.d.a.)</cell><cell>85.04% 54.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The accuracy under PGD attack of models trained by TRADES(ATTA-1) with MAT attack loss and TRADES attack loss.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>8 Acknowledgement</head><label>8</label><figDesc>This work is supported by NSF Grant No.1646392. ATTA is a new method for iterative attack based adversarial training that significantly speeds up training time and improves model robustness. The key insight behind ATTA is high transferability between models from neighboring epochs, which is firstly revealed in this paper. Based on this property, the attack strength in ATTA can be accumulated across epochs by repeatedly reusing adversarial perturbations from the previous epoch. This allows ATTA to generate the same (or even stronger) adversarial examples with fewer attack iterations. We evaluate ATTA and compare it with state-of-the-art adversarial training methods. It greatly shortens the training time with comparable or even better model robustness. More importantly, ATTA is a generic method and can be applied to enhance the performance of other iterative attack based adversarial training methods.</figDesc><table><row><cell>9 Conclusion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Table 9  also shows that, when models are trained with stronger attacks (more attack iterations), the models tend to have higher adversarial accuracy but lower natural accuracy. The result of various attacks on MNIST dataset.</figDesc><table><row><cell>Defense</cell><cell>Attack</cell><cell cols="4">Natural PGD-40 PGD-100 M-PGD-40 FGSM</cell><cell>CW-20</cell><cell>Time (sec)</cell></row><row><cell></cell><cell>PGD-1</cell><cell>99.52% 15.82%</cell><cell>4.17%</cell><cell>13.99%</cell><cell cols="2">81.84% 98.99% 226</cell></row><row><cell></cell><cell>PGD-10</cell><cell>99.52% 34.92%</cell><cell>16.90%</cell><cell>36.88%</cell><cell cols="2">94.32% 99.38% 1649</cell></row><row><cell></cell><cell>PGD-40</cell><cell>99.37% 96.21%</cell><cell>94.69%</cell><cell>96.79%</cell><cell cols="2">97.37% 99.06% 3933</cell></row><row><cell>MAT</cell><cell cols="2">YOPO-5-10 99.15% 93.69%</cell><cell>87.93%</cell><cell>99.04%</cell><cell cols="2">94.51% 99.03% 789</cell></row><row><cell></cell><cell>ATTA-1</cell><cell>99.45% 96.31%</cell><cell>95.11%</cell><cell>96.15%</cell><cell cols="2">98.31% 99.14% 297</cell></row><row><cell></cell><cell>ATTA-10</cell><cell>99.41% 97.36%</cell><cell>96.75%</cell><cell>97.45%</cell><cell cols="2">98.37% 99.3% 1687</cell></row><row><cell></cell><cell>ATTA-40</cell><cell>99.23% 97.28%</cell><cell>96.85%</cell><cell>97.51%</cell><cell cols="2">98.55% 98.02% 4650</cell></row><row><cell></cell><cell>PGD-1</cell><cell>99.41% 39.53%</cell><cell>31.28%</cell><cell>36.42%</cell><cell cols="2">71.01% 99.17% 583</cell></row><row><cell></cell><cell>PGD-10</cell><cell>99.37% 50.06%</cell><cell>25.71%</cell><cell>50.59%</cell><cell cols="2">95.12% 99.24% 1823</cell></row><row><cell>TRADES</cell><cell>PGD-40</cell><cell>98.89% 96.54%</cell><cell>95.54%</cell><cell>96.79%</cell><cell cols="2">97.83% 98.87% 6544</cell></row><row><cell></cell><cell>ATTA-1</cell><cell>99.03% 96.10%</cell><cell>94.24%</cell><cell>95.86%</cell><cell cols="2">98.38% 98.93% 460</cell></row><row><cell></cell><cell>ATTA-10</cell><cell>98.83% 96.86%</cell><cell>96.34%</cell><cell>96.90%</cell><cell cols="2">98.11% 98.68% 1686</cell></row><row><cell></cell><cell>ATTA-40</cell><cell>98.21% 96.03%</cell><cell>96.33%</cell><cell>96.80%</cell><cell cols="2">97.85% 98.32% 4660</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that the x * i+1 is still bounded by the natural image x, rather than C(x * i ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In adversarial training, most data augmentation methods used are linear transformations which are easy to be inversed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The author-implemented YOPO-3-4 can't converge in our experiment. We pick the accuracy data from YOPO paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/MadryLab/mnist_challenge https://github.com/MadryLab/cifar10_challenge</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/yaodongyu/TRADES</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/a1600012888/ YOPO-You-Only-Propagate-Once</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/ashafahi/free_adv_train</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview</head><p>This supplementary material provides details on our experiment and additional evaluation results. In Section B, we introduce the detailed setup of our experiment. In Section C, we compare adversarial examples generated by ATTA and PGD and show that, even with one attack iteration, ATTA-1 can generate similar perturbations to PGD-40 (PGD-10) on MNIST (CIFAR10). We also provide the complete evaluation results in Section C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment setup</head><p>We provide additional details on the implementation, model architecture, and hyper-parameters used in this work.</p><p>MNIST. We use the same model architecture used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, which has four convolutional layers followed by three fully-connected layers. The adversarial examples used to train the model are bounded by l ∞ ball with size = 0.3 and the step size for each attack iteration is 0.01. We do not apply any data augmentation (and inverse data augmentation) on MNIST and set the epoch period to reset perturbation as infinity which means that perturbations are not reset during the training. The model is trained for 60 epochs with an initial 0.1 learning rate and a 0.01 learning rate after 55 epochs, which is the same as <ref type="bibr" target="#b36">[37]</ref>. To evaluate the model robustness, we perform the PGD <ref type="bibr" target="#b15">[16]</ref>, M-PGD <ref type="bibr" target="#b7">[8]</ref> and CW <ref type="bibr" target="#b2">[3]</ref> attack with a 0.01 step size and set decay factor as 1 for M-PGD (momentum PGD).</p><p>CIFAR10. Following other works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>, we use Wide-Resnet- <ref type="bibr">34-10 [36]</ref> as the model architecture. The adversarial examples used to train the model are bounded by l ∞ ball with size = 0.031. For ATTA-1, 2, 3, we use 0.015, 0.01, 0.01 as the step size, respectively. For ATTAk (k &gt; 3), we use 0.007 as the step size. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to attack: Adversarial transformation networks</title>
		<author>
			<persName><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2687" to="2695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum adversarial training</title>
		<author>
			<persName><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yair</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13736</idno>
		<title level="m">Unlabeled data improves adversarial robustness</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based blackbox attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Characterizing deep-learning i/o workloads in tensorflow</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Steven Wd Chien</surname></persName>
		</author>
		<author>
			<persName><surname>Markidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Sishtla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Narasimhamurthy</surname></persName>
		</author>
		<author>
			<persName><surname>Laure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 3rd International Workshop on Parallel Data Storage &amp; Data Intensive Scalable Computing Systems (PDSW-DISCS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Jeremy M Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning models</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2266" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02175</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial defense via learning to generate diverse attacks</title>
		<author>
			<persName><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2740" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2008">2016. 1, 2, 7, 8</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Hyeungill</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyeob</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungwoo</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03387</idno>
		<title level="m">Generative adversarial trainer: Defense to adversarial perturbations with gan</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning transferable adversarial examples via ghost networks</title>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03413</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2008">2018. 1, 2, 4, 6, 8</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adversarial robustness toolbox v1</title>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Sinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Ngoc Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beat</forename><surname>Buesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Edwards</surname></persName>
		</author>
		<idno>1807.01069</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
				<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2017. 1, 3, 8</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5014" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12843</idno>
		<title level="m">Adversarial training for free!</title>
				<imprint>
			<date type="published" when="2008">2019. 2, 6, 8</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11304</idno>
		<title level="m">Universal adversarial training</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName><forename type="first">Mahmood</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sruti</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujo</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Physical adversarial examples for object detectors</title>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2008">2014. 1, 2, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename><surname>Zico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2730" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The limitations of adversarial training and the blind-spot attack</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2019. 1, 2, 5, 6, 8</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
