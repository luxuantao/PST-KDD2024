<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NextGen-Malloc: Giving Memory Allocator Its Own Room in the House</title>
				<funder ref="#_gmpBQjz #_6QJEFZN">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Cisco and Amazon Web Services</orgName>
					<orgName type="abbreviated">AWS</orgName>
				</funder>
				<funder ref="#_EZJpDfj">
					<orgName type="full">Providence, RI, USA</orgName>
				</funder>
				<funder ref="#_rKXMzEM #_VtPSEgR">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruihao</forename><surname>Li</surname></persName>
							<email>liruihao@utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Kavi</surname></persName>
							<email>krishna.kavi@unt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
							<email>ljohn@ece.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qinzhe</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gayatri</forename><surname>Mehta</surname></persName>
							<email>gayatri.mehta@unt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Neeraja</forename><forename type="middle">J</forename><surname>Yad- Wadkar</surname></persName>
							<email>neeraja@austin.utexas.edu</email>
						</author>
						<author>
							<persName><surname>Nextgen-Malloc</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Univerisy of North Texas Gayatri Mehta</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Univerisy of North Texas Neeraja J. Yadwadkar</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NextGen-Malloc: Giving Memory Allocator Its Own Room in the House</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3593856.3595911</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Memory Management</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memory allocation and management have a significant impact on performance and energy of modern applications. We observe that performance can vary by as much as 72% in some applications based on which memory allocator is used. Many current allocators are multi-threaded to support concurrent allocation requests from different threads. However, such multi-threading comes at the cost of maintaining complex metadata that is tightly coupled and intertwined with user data. When memory management functions and other user programs run on the same core, the metadata used by management functions may pollute the processor caches and other resources.</p><p>In this paper, we make a case for offloading memory allocation (and other similar management functions) from main processing cores to other processing units to boost performance, reduce energy consumption, and customize services to specific applications or application domains. To offload these multi-threaded fine-granularity functions, we propose to decouple the metadata of these functions from the rest of application data to reduce the overhead of inter-thread metadata synchronization. We draw attention to the following key questions to realize this opportunity: (a) What are the tradeoffs and challenges in offloading memory allocation to a dedicated core? (b) Should we use general-purpose cores or special-purpose cores for executing critical system management functions? (c) Can this methodology apply to heterogeneous systems (e.g., with GPUs, accelerators) and other service functions as well?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data intensive applications, including machine learning (ML) training and inference serving rely on efficient management of large amounts of data. With these increasing capacities and deep memory hierarchies, the memory wall <ref type="bibr" target="#b33">[35]</ref> (viz., the gap between processor speeds and memory access latencies) is becoming an ever increasing impediment. ML accelerators spend only 23% of the overall cycles on computations and more than 50% on data preparation <ref type="bibr" target="#b14">[15]</ref>. Even in general purpose warehouse-scale datacenters, 50% of compute cycles are idle waiting for memory accesses <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, causing performance degradation for the applications. To alleviate such performance bottlenecks, efficient memory management mechanisms <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> that reduce memory needs (by reducing fragmentation) and cache misses (by reducing conflicts between metadata and application data) are crucial.</p><p>To address the need for efficient memory allocation and management, multiple software solutions, in the form of optimized Malloc libraries, have been created (e.g., TCMalloc <ref type="bibr" target="#b13">[14]</ref> from Google and Mimalloc <ref type="bibr" target="#b17">[18]</ref> from Microsoft). Such libraries are typically multi-threaded to support concurrent requests for memory allocation and management, and rely on complex metadata to accomplish the allocation and management. However, if the same core executes memory management and application code, maintaining such metadata can pollute processor caches, cause conflicts on other resources and can affect overall performance.</p><p>To understand the effectiveness of existing software solutions for memory management, we compare the performance of four different memory allocators (PTMalloc2 <ref type="bibr" target="#b11">[12]</ref>, Jemalloc <ref type="bibr" target="#b8">[9]</ref>, TCMalloc <ref type="bibr" target="#b13">[14]</ref>, and Mimalloc <ref type="bibr" target="#b17">[18]</ref>) for xalancbmk (a representative workload from SPEC cpu2017 <ref type="bibr">[23,</ref><ref type="bibr" target="#b23">25]</ref>, which performs transformations on XML data). We observe that with an enhanced memory allocator, the overall system performance can be improved by as much as 1.72? (see Figure <ref type="figure">1</ref>). On other allocation intensive workloads, e.g., xmalloc and cache-scratch in mimalloc-bench <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>, performance variance can be more than 10? depending on the allocator used. We notice that the choice of memory allocator not only impacts the performance of the allocator code itself but also other parts of the program. For example, although only 2% of the execution time is spent on malloc and free by xalancbmk, we can see as much as 72% difference in overall execution time when Mimalloc is used instead of PTMalloc2. This makes a compelling case for further investigating and optimizing these memory management functions.</p><p>Existing memory allocation mechanisms that are implemented in software achieve higher performance compared with the default Glibc (PTMalloc2) memory allocators. However, they still fall short in leveraging system-level optimizations, like reducing cache pollution and TLB misses, which cannot be implemented without understanding underlying hardware. In addition, software-only solutions always suffer from balancing allocation speed and memory consumption (partly due to fragmentation), making it difficult to develop a one-size-fits-all allocation solution.</p><p>Alternatively, hardware accelerators are considered for implementing memory allocation mechanisms. For instance, Kanev et al. <ref type="bibr" target="#b16">[17]</ref>, proposed a separate cache inside main CPUs for memory management algorithms, thus eliminating cache conflicts between memory management metadata and application data, while still utilizing powerful main CPUs for memory management. Mass et al. <ref type="bibr" target="#b18">[19]</ref>, used a near-memory accelerator to offload garbage collection functions. However, most hardware solutions rely on customized hardware units, that (a) make it infeasible to use them as general purpose memory management solution, or (b) necessitate frequent changes to the hardware as algorithms evolve.</p><p>Next generation memory allocators will likely continue to be complex. One way to handle their complexity without impacting application performance is to isolate the allocation functions from rest of the code and provide separate resources to it. This will prevent allocators from polluting the cache and interfering with other metadata of applications. However, current allocators cannot easily be "plucked" out of the code and offloaded to dedicated cores, because the metadata is usually tightly coupled to the user data.</p><p>In this paper, we introduce NextGen-Malloc, a novel memory allocator that restructures the memory management metadata and makes it possible to offload the allocation to a separate dedicated core. The design of NextGen-Malloc necessitates innovations in software, hardware and their codesign. Just like a baby in a big family, the memory allocator is growing up. Now it is time to give it a new room (core) in our house (CPU). New research questions will arise with the growing child NextGen-Malloc. We will explore these questions in this paper. How to tradeoff the overhead (additional inter-core communication) and the benefit (a reduction of cache pollution and asynchronous execution) of offloading memory allocators? The choice of the room type (viz., type of processing core) is another research question. Should the room be the same as other rooms (i.e., other CPU cores), or a small room is enough for memory allocation? Can the room be used for other functions instead of exclusively for memory allocation?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>This section identifies the primary reasons and inefficiencies contributing to the performance differences among different memory allocators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Memory Allocators</head><p>Memory management operations exist at two levels: user level and kernel level. User-level memory management operations are implemented by a UMA (user-level memory allocator), such as PTmalloc2 <ref type="bibr" target="#b11">[12]</ref> (the default Glibc implementation), TCMalloc <ref type="bibr" target="#b13">[14]</ref> (by Google), and Mimalloc <ref type="bibr" target="#b17">[18]</ref> (by Microsoft). There are other domain-specific UMAs as well, such as NVAlloc <ref type="bibr" target="#b6">[7]</ref> for non-volatile memory. For the kernellevel operations, ???? () system calls are used to manage private anonymous mapping segments for each process. But relying on ???? () system call for each ?????? () can result in significant performance penalties caused by switching between user and kernel modes. To minimize these overheads, most UMAs request an entire page 1 , which is usually larger than the size requested by ?????? () calls. For subsequent ?????? () calls, the UMA can satisfy the requests out of the page and make additional ???? () calls only when space in the allocated page is exhausted.</p><p>Observation: Managing the memory space within preallocated pages in a way that strikes a balance between allocation speed and memory fragmentation is an open research problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Malloc impacts more than you think!</head><p>The implementation of memory management functions can potentially have significant impact on the overall execution time of applications. In this section, we present evidence to support the claim we made earlier: "Though only 2% of time is spent on memory allocation, more than 70% performance difference can exist with different implementations".</p><p>The above claim may appear counter-intuitive. Table <ref type="table" target="#tab_0">1</ref> lists the hardware PMU (Performance Monitor Unit) event counts for xalancbmk with the four memory allocators. Based on the profiling results, the number of LLC (Last Level Cache) load and store misses, and the number of dTLB (data Translation Lookaside Buffer) load misses are reduced dramatically (besides absolute miss numbers, the miss rate reduced as well) on the three state-of-the-art industry-level allocators (Jemalloc, Mimalloc, and TCMalloc) compared with the default Glibc allocator (PTMalloc2). The impact of TLB misses is non-negligible, which can incur 100s of cycles in modern processors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">27]</ref>. In addition, in warehouse-scale computers, half of the CPU back-end cycles can be spent on serving data cache requests <ref type="bibr" target="#b15">[16]</ref>.</p><p>Observation: TLB and LLC misses are key factors affecting the overall performance and sensitive to different memory allocators. Alleviating the cache pollution is a potential direction for the next-generation memory allocators to explore, to achieve a lower number of TLB and LLC misses. 1 The page size for UMA may be different from OS page size <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Existing Allocators are Limiting</head><p>Modern UMAs support concurrent memory ?????? () and ? ??? () operations for multi-threaded programs, which requires processing ?????? () and ? ??? () requests from different physical cores and different address spaces simultaneously. As the number of cores within a single socket keeps increasing, it will compound the challenges for software UMAs in handling thread contentions: the inter-core metadata synchronization requires atomic operations (or locks) to maintain the upper-level global free lists. Software mutex locks are used to control access to metadata to process requests from different cores. The cost of using such software locks is high since cross-core communication is involved, causing a critical performance bottleneck as the communication overheads increase with the number of cores <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b32">34]</ref>.</p><p>Multiple software solutions have been used to address the multi-thread contention issue. TCMalloc <ref type="bibr" target="#b12">[13]</ref> uses per-CPU/thread cache to maintain metadata associated with each logical core, avoiding locks for most memory allocations and deallocations. Mimalloc <ref type="bibr" target="#b17">[18]</ref> uses three page-local shared free lists to increase locality, avoid contention, and support a highly-tuned allocation and free on fast path. However, maintaining thread-local caches will increase metadata size, resulting in more heap memory consumption and more cache pollution for the user program. Also, the number of logical threads may exceed the number of available physical cores, resulting in more overhead in switching between threadlocal metadata.</p><p>To demonstrate the potential cache pollution in multithreaded UMAs, we collect the PMU event counts for xmalloc [4] workload<ref type="foot" target="#foot_0">2</ref> on TCMalloc (listed in Table <ref type="table" target="#tab_2">2</ref>). As the number of threads increases, they contend with each other for accessing the thread-local cache. This results in a significant LLC miss increment (more than 10x when the number of threads increases from 1 to 8).</p><p>Observation: It is challenging to address thread contention well without understanding the underlying hardware mechanisms for synchronization and communication.</p><p>Expectation: If the execution of the memory allocator and user program are separated, all thread-local metadata can reside within one core, alleviating cache pollution. This can be achieved by making ?????? () run on a dedicated core separating from other user programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPPORTUNITIES</head><p>Merely offloading memory allocators to a dedicated core is not what NextGen-Malloc entails. There are several challenges to be solved to make it a reality and to achieve its full potential. We present some impactful avenues here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Offload Malloc to its Own Core</head><p>By offloading memory management functions to a dedicated core, the overall performance can potentially benefit from parallel/concurrent execution and a reduction of cache pollution. However, offloading these fine granularity functions is still a challenge for current computer systems, considering the overhead of inter-core communications and the tight coupling between data and metadata. We detail the challenges and propose remedies that can make NextGen-Malloc a beneficial reality.</p><p>3.1.1 Challenges. While it is not uncommon to see CPU resource underutilization in datacenters (for example, 60% of the Virtual Machines on Microsoft Azure have an average CPU utilization lower than 20% <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">33]</ref>), and while offloading system functions to free cores to boost overall performance has been tried for other problems ( e.g., Shenango <ref type="bibr" target="#b22">[24]</ref> dedicates a single busy-spinning core per machine to a centralized software entity), there are unique challenges to offloading memory allocators. The majority of memory allocation function calls are fine-grained and can be finished within 100 cycles <ref type="bibr" target="#b16">[17]</ref>. Thus memory allocation amounts to 0.05 ?? on a 2GHz machine, while in Shenango <ref type="bibr" target="#b22">[24]</ref> the granularity is 5 ?? (the busy-spinning core reschedules jobs among cores every 5 ??, and the duration of each job is usually longer than 5 ??). In comparison to allocation time-scales, the overhead of inter-core communication is non-negligible. A single Atomic Read-Modify-Write (RMW) instruction needed for inter-core synchronization can take 67 cycles on average on a Sandy Bridge machine <ref type="bibr" target="#b24">[26]</ref>, and almost 700 cycles in the worst case <ref type="bibr" target="#b2">[3]</ref>. MMT <ref type="bibr" target="#b29">[31]</ref> explored offloading memory allocation tasks to a memory management thread in a 2-core system. The performance did not improve without aggressive preallocations, which makes it only work for workloads with known allocation patterns.</p><p>Expectation: Whether the overall performance can benefit from offloading memory allocators to a dedicated core is still an open question. It depends on whether the inter-core synchronization overhead can be reduced to acceptable levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Strategy 1 -Decoupling.</head><p>Decoupling the metadata memory space from user data is one of the solutions that may lead to the overall performance gains from offloading the memory allocator. Although MMT <ref type="bibr" target="#b29">[31]</ref> offloads the memory allocator to a separate thread, the cache pollution issue cannot be alleviated unless all metadata is accessed exclusively by the dedicated core for memory management. Before discussing allocator decoupling, we will discuss Metadata Layout first. Figure <ref type="figure">2</ref> shows two representative metadata layouts, Aggregated Layout (used by Mimalloc) and Segregated Layout (used by TCMalloc).</p><p>In Aggregated Layout, the first 8 bytes (assuming 64-bit word size) of each free block are used as the pointer to the next free block. Global Head and Tail pointers are used for each page (each page has a different number of blocks, depending on the block size (the block size is not necessarily a power of 2 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>). If a block is accessed directly after the ?????? () call, since 56 bytes (assuming 64-byte cache line size) of the allocated data will already be cached by the ?????? () function, better spatial localities will result for the user program. In addition, the memory block can easily be located in the ? ??? () phase, since each block can be indexed by the address directly.</p><p>However, optimizing locality for aggregated layout calls for (1) the allocator and user program must be executed on the same physical core, (2) the allocated memory must be accessed immediately after it is allocated, (3) the cached memory block must not be evicted before it is deallocated.</p><p>In Segregated Layout, metadata is decoupled from the allocated storage, which may address the cache pollution issue. Instead of an 8-byte pointer, a smaller index (16-bit for example) can be used to indicate the location of the free page, reducing memory fragmentation as well. Segregated layout requires more complicated control logic for locating the pointer to the freed data during the ? ??? () phase. Cache pollution can still exist since CPUs have to access data needed for malloc functions and other computational functions as well.</p><p>Expectation: Trade-offs always exist between the aggregated and segregated layouts and there is no global optimal layout. However, segregated layout is more suitable for offloading memory allocators in NextGen-Malloc. With segregated layout, the address space of metadata and user data can be separated. The additional cost in ? ??? () would not be an issue, since the entire ? ??? () phase is not on the critical path and can be executed asynchronously in the dedicated core. made to critical resources to avoid race conditions. When offloading the memory allocator to a dedicated core, all memory management operations will be handled by one specific core, metadata contention should be eliminated theoretically (from a hardware perspective). Therefore, we can remove redundant atomic operations in UMAs, since sequential execution can be guaranteed if all allocation codes are running in one specific core. Questions: Whether the contention is reduced or not is still an open question, depending on the trade-off between the introduced synchronization overhead in NextGen-Malloc and removed atomic operations in current UMAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type of Core to Offload to</head><p>One possibility is to offload allocation to an idle core in the system. An alternative solution is to use a near-memory core. Harvesting an idle core may be more cost-effective, while a near-memory core could be energy-efficient and/or offer opportunities for customization. A general-purpose core makes it compatible with other system instruction sets and ease of compiling UMAs. A single-threaded in-order integer CPU may be adequate for a near-memory design, since memory address computations do not need floating point operations or complex control logic. The near-memory core will likely have lower memory access latencies; thus requiring only a small (micro) cache for buffering metadata. It is crucial to explore these design choices as they have the potential to improve performance while reducing energy/power consumption of memory management functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">What else can NextGen-Malloc offer?</head><p>In addition to UMAs, NextGen-Malloc can also be applied to other memory management scenarios. There are several opportunities that can be explored extending NextGen-Malloc.</p><p>3.3.1 Optimizing GPU Malloc. In addition to CPU-only memory allocation systems, memory management functions on CPU-GPU heterogeneous systems can be included in NextGen-Malloc as well. When GPU memory spaces are included, more trade-offs will be there. For example, in Nvidia GPUs, UVM (Unified Virtual Memory) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">36]</ref> is used to make the address space shared across the CPU hosts and GPU devices. Redundant memory transmission, data allocation granularity, and address space mapping are open questions. Asynchronous allocation can be used, which can also be part of the asynchronous CUDA memory copy. Both CPU and GPU memory allocators can be decoupled from user programs for faster address translation. All these features can be explored in NextGen-Malloc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Other Functions to Offload. In addition to user-level memory allocations, NextGen-Malloc can be used for other memory management functions. This flexibility comes from the programmable feature of NextGen-Malloc. In the kernel space, NextGen-Malloc can be combined with other resource management mechanisms, e.g., Caladan <ref type="bibr" target="#b9">[10]</ref>, to balance CPU and memory usage of different tasks. More intelligence can be programmed to observe allocation requests and utilize such information to predictively preallocate memory to reduce allocation latencies.</p><p>Also, there are opportunities in memory management for other scenarios, including managed languages (e.g., Java and .NET) and serverless functions (e.g., AWS Lambda <ref type="bibr" target="#b1">[2]</ref> and Azure Functions <ref type="bibr" target="#b20">[21]</ref>). With GC (Garbage Collector) invoked, LLC miss rate is reduced in .NET <ref type="bibr" target="#b7">[8]</ref>. With different GC settings, the performance of the program can be affected a lot. Research opportunities for using NextGen-Malloc to process garbage collection will be worth exploring. Booting a function in FaaS (Functions-as-a-Service) systems through cold start can introduce extensive overhead, including additional memory consumption and allocation time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b30">32]</ref>. To avoid duplicate runtime library initialization in different containers, NextGen-Malloc can be extended to monitor inter-process memory heap similarities in FaaS systems as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEXTGEN-MALLOC IS FEASIBLE</head><p>As outlined in previous sections, NextGen-Malloc can support asynchronous execution and reduce LLC and TLB misses by separating the memory space of memory management</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>HOTOS ' 23 ,Figure 1 :</head><label>231</label><figDesc>Figure 1: Execution time sensitivity to memory allocation: variations up to 72% in xalancbmk, though only 2% time spent on malloc and free.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Processor performance monitor data for xalancbmk.TLB misses vary more than 10x and LLC load misses vary 4x between TCMalloc and PTMalloc2.</figDesc><table><row><cell>Allocator</cell><cell cols="2">PTMalloc2 JeMalloc</cell><cell cols="2">TCMalloc Mimalloc</cell></row><row><cell>cycles</cell><cell cols="4">1.177E+12 7.115E+11 7.091E+11 6.959E+11</cell></row><row><cell>instructions</cell><cell cols="4">1.282E+12 1.320E+12 1.264E+12 1.262E+12</cell></row><row><cell>LLC-load-MPKI</cell><cell>0.317</cell><cell>0.072</cell><cell>0.080</cell><cell>0.117</cell></row><row><cell>LLC-store-MPKI</cell><cell>0.277</cell><cell>0.123</cell><cell>0.099</cell><cell>0.105</cell></row><row><cell>dTLB-load-MPKI</cell><cell>1.407</cell><cell>0.112</cell><cell>0.130</cell><cell>0.129</cell></row><row><cell cols="2">dTLB-store-MPKI 0.029</cell><cell>0.022</cell><cell>0.020</cell><cell>0.022</cell></row><row><cell>Allocator</cell><cell cols="2">PTMalloc2 JeMalloc</cell><cell cols="2">TCMalloc Mimalloc</cell></row><row><cell>cycles</cell><cell cols="4">1.177E+12 7.115E+11 7.091E+11 6.959E+11</cell></row><row><cell>instructions</cell><cell cols="4">1.282E+12 1.320E+12 1.264E+12 1.262E+12</cell></row><row><cell>LLC-load-misses</cell><cell cols="4">4.059E+08 9.445E+07 1.016E+08 1.477E+08</cell></row><row><cell cols="5">LLC-store-misses 3.554E+08 1.630E+08 1.254E+08 1.321E+08</cell></row><row><cell cols="5">dTLB-load-misses 1.804E+09 1.482E+08 1.641E+08 1.628E+08</cell></row><row><cell cols="5">dTLB-store-misses 3.669E+07 2.937E+07 2.591E+07 2.787E+07</cell></row><row><cell># of threads</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>cycles</cell><cell cols="4">4.333E+10 6.708E+10 1.148E+11 1.963E+11</cell></row><row><cell>instructions</cell><cell cols="4">2.387E+10 2.979E+10 3.893E+10 4.874E+10</cell></row><row><cell>LLC-load-misses</cell><cell cols="4">1.223E+05 2.196E+05 2.477E+06 1.175E+07</cell></row><row><cell>LLC-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>store-misses 3.676E+06 4.231E+06 1.650E+07 5.402E+07</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PMU data for xmalloc on TCMalloc using different number of threads. LLC misses increase more than 10x when the number of threads increases from 1 to 8.</figDesc><table><row><cell>Allocator</cell><cell>PTMalloc2 JeMalloc</cell><cell>TCMalloc Mimalloc</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>store-misses 3.554E+08 1.630E+08 1.254E+08 1.321E+08 dTLB-load-misses 1.804E+09 1.482E+08 1.641E+08 1.628E+08</head><label></label><figDesc></figDesc><table><row><cell cols="5">dTLB-store-misses 3.669E+07 2.937E+07 2.591E+07 2.787E+07</cell></row><row><cell># of threads</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>cycles</cell><cell cols="4">4.333E+10 6.708E+10 1.148E+11 1.963E+11</cell></row><row><cell>instructions</cell><cell cols="4">2.387E+10 2.979E+10 3.893E+10 4.874E+10</cell></row><row><cell>LLC-load-misses LLC-</cell><cell cols="4">1.223E+05 2.196E+05 2.477E+06 1.175E+07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>store-misses 3.676E+06 4.231E+06 1.650E+07 5.402E+07</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>3.1.3 Strategy 2:Removing unnecessary atomic operations in UMAs. In current UMA designs, if one thread tries to free a memory block that was allocated by another running thread, contention will result (as described in Section 2.3) and atomic operations are required to sequentialize changes</figDesc><table><row><cell>Aggreged Layout</cell><cell>8 8 8</cell><cell>8</cell><cell>8 8</cell><cell>8</cell><cell>8 8 8 head</cell><cell>8</cell><cell>8 8</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tail</cell><cell></cell></row><row><cell>Segregated Layout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Figure 2: Metadata (pointers to blocks) shown in blue is de-</cell></row><row><cell cols="9">coupled from the allocated data in segregated layout whereas</cell></row><row><cell cols="9">interspersed with data in aggregated layout. Data blocks are</cell></row><row><cell cols="4">64, 128, or 256 bytes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>xmalloc<ref type="bibr" target="#b3">[4]</ref> is a multi-threaded benchmark by Lever and Boreham and used to exercise cases where a thread allocates data but a different thread deallocates the allocated blocks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement: We thank our shepherd, <rs type="person">Aurojit Panda</rs>, and the anonymous reviewers for their constructive feedback and insightful comments. This research was supported in part by <rs type="funder">NSF</rs> grant number <rs type="grantNumber">1763848</rs> and <rs type="grantNumber">1828105</rs>, and gifts from <rs type="funder">Cisco and Amazon Web Services (AWS)</rs>. The authors would also like to acknowledge the computing servers donated by <rs type="person">Ampere Computing</rs> and TACC. Any opinions, findings, conclusions, or recommendations are those of the authors and not of the National Science Foundation or other sponsors.</p></div>
			</div>
			<div type="funding">
<div><p>: <rs type="projectName">Giving Memory Allocator Its Own Room in the House. In Workshop on Hot Topics in Operating Systems</rs> (<rs type="grantNumber">HOTOS '23</rs>), June <rs type="grantNumber">22-24</rs>, <rs type="grantNumber">2023</rs>, <rs type="funder">Providence, RI, USA</rs>. <rs type="institution">ACM, New York, NY, USA</rs>, 8 pages.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gmpBQjz">
					<idno type="grant-number">1763848</idno>
				</org>
				<org type="funding" xml:id="_6QJEFZN">
					<idno type="grant-number">1828105</idno>
				</org>
				<org type="funded-project" xml:id="_rKXMzEM">
					<idno type="grant-number">HOTOS &apos;23</idno>
					<orgName type="project" subtype="full">Giving Memory Allocator Its Own Room in the House. In Workshop on Hot Topics in Operating Systems</orgName>
				</org>
				<org type="funding" xml:id="_VtPSEgR">
					<idno type="grant-number">22-24</idno>
				</org>
				<org type="funding" xml:id="_EZJpDfj">
					<idno type="grant-number">2023</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>function metadata between user data, but at the cost of additional inter-core communication. In this section, we model this trade-off analytically and validate it by prototyping NextGen-Malloc for execution on real machines and comparing it with state-of-the-art memory allocators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NextGen-Malloc Performance Estimates</head><p>We use xalancbmk as an example in an analytical model to demonstrate the potential performance improvement if NextGen-Malloc is used. In xalancbmk, ?????? () and ? ??? () functions are called 279, 759, 405 times in total (138, 401, 260 ?????? () and 141, 394, 145 ? ??? ()). The performance overhead for these functions comes from atomic operations needed at the beginning and end of each malloc and free function calls. Assuming a 67-cycle latency for one atomic operation <ref type="bibr" target="#b2">[3]</ref>, there will be around 75 billion additional cycles introduced by NextGen-Malloc. We assume that any performance benefit of NextGen-Malloc is from reduced LLC and TLB misses. Comparing Mimalloc to Glibc, we can calculate that the average LLC and TLB miss penalty is 214 cycles. To amortize the overhead, NextGen-Malloc has to achieve a reduction of at least 1.25 Cache/TLB misses in each ?????? ()/? ??? () and the subsequent user program before the next ?????? ()/? ??? () comes. This is possible to achieve in NextGen-Malloc, since there are 7 load and store instructions in each ?????? () and 10 in each ? ??? () (for Mimalloc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Validating NextGen-Malloc</head><p>We prototype NextGen-Malloc on an AWS-A1 bare metal machine, with 16 Armv8-A Cortex-A72 (ARM uses a weaker memory model, which reduces the inter-core synchronization overhead.) cores and 32 GiB memory, by overwriting functions defined in standard C libraries, e.g., ?????? () and ? ??? (), and compile it as a shared library. In NextGen-Malloc, we spawn a child thread from the main thread when the process is forked, pin it to a specific core, and let the spawnedthread check signals from the main thread for incoming ?????? () and ? ??? () requests. We use the ?????? () function as an example (shown in Code 1) to demonstrate the synchronization system in NextGen-Malloc, which is developed for the communication between the spawned child thread and main threads.</p><p>Two atomic variables ??????_????? and ??????_???? are used at the beginning and end of ???????_?????? () and ?????? (), which are executed by the child thread and main thread, respectively. The ?????????_???? and ?????????_????? are the input and output of ?????? () functions, and this information is transferred between two threads. In this way, all other ?????? () related metadata will be stored exclusively on the dedicated core, alleviating cache pollution that exists in state-of-the-art memory allocators. We still use xalancbmk as the workload. We make a sideby-side comparison between Mimalloc and NextGen-Malloc in Table <ref type="table">3</ref>. NextGen-Malloc achieves a 4.51% performance improvement, which is also coming from a reduction of dTLB load, LLC load, and LLC store misses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we draw attention to the opportunity to achieve significant performance gains by offloading finegranularity service functions like memory allocation to a separate core/device. These memory allocation functions cause cache pollution when they are run on the same core as the application code due to the tight coupling of their metadata with user data. We discuss challenges in offloading such management functions to a separate core and propose avenues for future research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In-depth analyses of unified virtual memory system for GPU accelerated computing</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://aws.amazon.com/lambda/" />
		<title level="m">Amazon. 2023. AWS Lambda</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Free atomics: hardware atomic operations without fences</title>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Asgharzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">M</forename><surname>Cebrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Malloc () performance in a multithreaded Linux environment</title>
		<author>
			<persName><forename type="first">David</forename><surname>Boreham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 USENIX Annual Technical Conference (USENIX ATC 00)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From warm to hot starts: Leveraging runtimes for the serverless era</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumer</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems</title>
		<meeting>the Workshop on Hot Topics in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="153" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">NVAlloc: rethinking heap metadata management in persistent memory allocators</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuibing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="115" to="127" />
		</imprint>
	</monogr>
	<note>Xian-He Sun, and Gang Chen</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Performance characterization of. net benchmarks</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable concurrent malloc (3) implementation for FreeBSD</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the bsdcan conference</title>
		<meeting>of the bsdcan conference<address><addrLine>ottawa, canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Caladan: Mitigating interference at microsecond timescales</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Agile paging: Exceeding the best of nested and shadow paging</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="707" to="718" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Wolfram Gloger&apos;s malloc homepage</title>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Gloger</surname></persName>
		</author>
		<ptr target="http://www.malloc.de/en/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/google/tcmalloc/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond malloc efficiency to fleet efficiency: a hugepage-aware memory allocator</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darryl</forename><surname>Gove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi21/presentation/hunter" />
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21). USENIX Association</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="257" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tipp</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750392</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750392" />
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="158" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mallacc: Accelerating Memory Allocation</title>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><surname>Sam Likun Xi</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
		<idno type="DOI">10.1145/3037697.3037736</idno>
		<ptr target="https://doi.org/10.1145/3037697.3037736" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Xi&apos;an, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="33" to="45" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mimalloc: Free List Sharding in Action</title>
		<author>
			<persName><forename type="first">Daan</forename><surname>Leijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<idno>MSR-TR-2019-18</idno>
		<ptr target="https://www.microsoft.com/en-us/research/publication/mimalloc-free-list-sharding-in-action/" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hardware accelerator for tracing garbage collection</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="138" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive Huge-Page Subrelease for Non-Moving Memory Allocators in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darryl</forename><surname>Gove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Turner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459898.3463905</idno>
		<ptr target="https://doi.org/10.1145/3459898.3463905" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM SIGPLAN International Symposium on Memory Management (Virtual, Canada) (ISMM 2021)</title>
		<meeting>the 2021 ACM SIGPLAN International Symposium on Memory Management (Virtual, Canada) (ISMM 2021)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="28" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://azure.microsoft.com/en-us/products/functions/" />
		<title level="m">Azure Functions</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://github.com/daanx/mimalloc-bench/" />
		<title level="m">Mimalloc-bench</title>
		<imprint>
			<publisher>Microsoft</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shenango: Achieving high {CPU} efficiency for latency-sensitive datacenter workloads</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wait of a decade: Did SPEC CPU 2017 broaden the performance horizon?</title>
		<author>
			<persName><forename type="first">Reena</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="271" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast RMWs for TSO: Semantics and implementation</title>
		<author>
			<persName><forename type="first">Bharghava</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susmit</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Elver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking TLB designs in virtualized environments: A very large part-of-memory TLB</title>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="469" to="480" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memory deduplication for serverless computing with medes</title>
		<author>
			<persName><forename type="first">Divyanshu</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Singhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Computer Systems</title>
		<meeting>the Seventeenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="714" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating the cost of atomic operations on modern architectures</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Schweizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Parallel Architecture and Compilation (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="445" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shahrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gohar</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Batum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Laureano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colby</forename><surname>Tresness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc20/presentation/shahrad" />
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20). USENIX Association</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mmt: Exploiting fine-grained parallelism in dynamic memory management</title>
		<author>
			<persName><forename type="first">Devesh</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Symposium on Parallel &amp; Distributed Processing (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Edouard Bugnion, and Boris Grot. 2021. Benchmarking, analysis, and optimization of serverless function snapshots</title>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plamen</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<biblScope unit="page" from="559" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SmartHarvest: harvesting idle CPUs safely and efficiently in the cloud</title>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Vanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neeraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Computer Systems</title>
		<meeting>the Sixteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual-Link: A Scalable Multi-Producer Multi-Consumer Message Queue Architecture for Cross-Core Communication</title>
		<author>
			<persName><forename type="first">Qinzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashen</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerstlauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">Wm</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UVM Discard: Eliminating Redundant Memory Transfers for Accelerators</title>
		<author>
			<persName><forename type="first">Weixi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hairgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
