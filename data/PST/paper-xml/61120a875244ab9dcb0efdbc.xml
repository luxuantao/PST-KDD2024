<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clio: A Hardware-Software Co-Designed Disaggregated Memory System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-07">7 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuhao</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clio: A Hardware-Software Co-Designed Disaggregated Memory System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-07">7 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.03492v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memory disaggregation has attracted great attention recently because of its benefits in efficient memory utilization and ease of management. Research on memory disaggregation so far has taken a software approach, running disaggregated memory management software either at servers that act as disaggregated memory nodes or at servers on the client side. This paper proposes a hardware-based disaggregated memory device, Clio, that manages disaggregated memory at the device side with novel software-hardware co-designs. Clio includes a hardware-based virtual memory system, a customized network stack, and a framework for computation offloading. Clio achieves low median and tail latency, high throughput, excellent scalability, and low energy cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern data-center applications like graph computing, data analytics, and deep learning have increasing demand for access to large amounts of memory <ref type="bibr" target="#b4">[5]</ref>. Unfortunately, servers are facing memory capacity walls because of pin, space, and power limitations <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b69">71]</ref>. Going forward, it is imperative for datacenters to seek solutions that can go beyond what a (local) machine can offer, i.e., using remote memory. At the same time, data centers are seeing the needs from management and resource utilization perspectives to disaggregate resources <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b66">68]</ref>-separating hardware resources into different network-attached pools that can be scaled and managed independently. These real needs have pushed the idea of memory disaggregation (MemDisagg for short): organizing computation and memory resources as two separate network-attached pools, one with compute nodes (CNs) and one with memory nodes (MNs).</p><p>So far, MemDisagg research has all taken one of two approaches, building/emulating MNs with either regular servers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56]</ref> or raw memory devices with no processing power <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b63">65]</ref>. The fundamental issues of server-based approaches such as RDMA-based systems are the monetary cost of a host server and the inherent performance and scalability limitations caused by the way NICs interact with the host server's virtual memory system. Raw-* Zhiyuan Guo and Yizhou Shan are co-first authors.</p><p>device-based solutions have low costs. However, they introduce performance, security, and management problems because when MNs have no processing power, all the data and control planes have to be handled at CNs <ref type="bibr" target="#b63">[65]</ref>.</p><p>Server-based MNs and MNs with no processing power are two extreme approaches of building MNs. This work seeks a sweet spot in the middle by proposing a hardware-based MemDisagg solution that has the right amount of processing power at MNs. Furthermore, we take a clean-slate approach by starting from the requirements of MemDisagg and designing a MemDisagg-native system.</p><p>We built an open-source, distributed programmable hardware-based disaggregated memory framework. With this platform, applications running at different CNs can allocate and access memory from multiple MNs using a unified virtual memory interface (e.g., allocate a remote virtual memory region and read/write to it at byte granularity), and they can offload certain computation to MNs. This paper presents how we built a basic, single-MN hardware platform (called Clio) in this framework, leaving questions like how to distribute/migrate memory across MNs and how to handle MN failures to a future paper.</p><p>Clio includes a CN-side user-space library called CLib and a new hardware-based MN device called CBoard. Multiple application processes running on different CNs could allocate memory from the same CBoard, with each process having its own remote virtual memory address space. A CBoard consists of two main components: 1) a hardware chip that integrates a thin network stack and a virtual memory system to handle data requests (the fast path), and 2) an ARM processor that runs software to handle metadata requests and assists the fast path with background tasks (the slow path).</p><p>In building Clio, we explore new requirements, challenges, and benefits of MemDisagg. Specifically, we answer three important research questions.</p><p>First, how does the design and implementation of a dedicated hardware MN differ from server and programmable NIC designs? Current MemDisagg solutions rely on a host server (its OS and MMU) to provide a virtual memory system so that accesses to the memory are protected and flexible. Using a whole server just for the virtual memory system is overkill and unnecessarily adds monetary and energy costs to MemDisagg. Another possibility is to use a low-power processor (e.g., ARM) in a SmartNIC to run the virtual memory system <ref type="bibr" target="#b39">[41]</ref>. However, doing so has high performance impact mainly because the virtual memory system is on a separate chip from the NIC. Overall, serverbased approaches have cost overheads while SmartNIC solutions have performance overheads. We took a clean-slate approach by building a hardware-based virtual memory system that is integrated with a customized hardware network stack, both of which are designed specifically for handling virtual memory requests sent over the network.</p><p>Second, how can a low-cost MN host TBs of memory and support thousands of concurrent application processes? Different from traditional (local) memory, an MN is intended to be shared by many applications running at different CNs, and the more applications it can support, the more efficiently its memory can be utilized. Thus, we aim to have each MN host TBs of memory for thousands of concurrent applications processes. However, a hardware design is constrained by the limited resources in a hardware chip such as on-chip memory. Compared to traditional software-based virtual memory systems, how can an MN use orders of magnitude less resources while achieving orders of magnitude higher scalability? Current solutions like RDMA NIC resort to caching, by swapping metadata between on-chip memory and host server memory, which inevitably comes with performance overhead (as high as 4× for a cache miss <ref type="bibr" target="#b62">[64]</ref>).</p><p>Our clean-slate approach is to carefully examine each virtual-memory and networking task and to redesign them to 1) eliminate states and metadata whenever possible (e.g., by minimizing indirection), 2) move complex but nonperformance-critical states, metadata, and tasks to the software slow path, 3) shift functionalities to the CN (CLib) to reduce MN's complexity (e.g., our network transport runs at CLib, and MN is "transport-less"), and 4) design boundedsize, inherently scalable data structures. As a result, each MN (CBoard) could support TBs of memory and thousands of application processes with only 1.5 MB on-chip memory.</p><p>Third, how to minimize tail latency in a MemDisagg system? Tail latency is important in data centers especially for workloads that have large fanouts (e.g., Spark jobs). Although much effort has focused on improving the network and core scheduling for low tail latency <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b52">54]</ref>, the memory system has largely been overlooked. However, the (remote) memory system is what contributes to extreme long tails in a MemDisagg system. For example, RDMA's round-trip latency is around 1-2 µs in the common case, but its tail could be as long as 16.8 ms (Figure <ref type="figure" target="#fig_4">6</ref> and §2.2).</p><p>We reexamine traditional memory system and propose a set of novel mechanisms to bound Clio's tail latency. Our core idea is to include all the functionalities that are needed to fulfill all types of data requests in one hardware pipeline and to make this hardware pipeline performance deterministic. This pipeline takes one incoming data unit every cycle (i.e., no pipeline stalls) and completes every request in a fixed number of cycles, which yields 100 Gbps throughput, 2.5 µs at median and 3.2 µs at 99-percentile end-to-end latency (Figure <ref type="figure" target="#fig_6">7</ref>). Two major technical hurdles in achieving this performance are to perform page table lookups and to handle page faults in a bounded, short time period. For the former, we propose a new overflow-free hash-based page table that bounds all page table lookups to at most one DRAM access (instead of the long page table walk in a traditional CPU architecture). For the latter, we propose a new mechanism to handle page faults in hardware with bounded cycles (instead of the costly process of interrupt and handling page faults in the OS).</p><p>We prototyped CBoard with an FPGA and built three applications using Clio: a FaaS-style image compression utility, a radix-tree index, and a key-value store. We compared Clio with native RDMA, two RDMA-based disaggregated/remote memory systems <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b63">65]</ref>, a software emulation of hardware-based disaggregated memory <ref type="bibr" target="#b54">[56]</ref>, and a softwarebased SmartNIC <ref type="bibr" target="#b41">[43]</ref>. Clio scales much better and has orders of magnitude lower tail latency than RDMA, while achieving similar throughput and median latency as RDMA (even with the slower FPGA frequency in our prototype). Clio has 1.1× to 3.4× energy saving compared to CPU-based and SmartNIC-based disaggregated memory systems and is 2.7× faster than SmartNIC solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Memory Disaggregation</head><p>Resource disaggregation separates different types of resources into different pools, each of which can be independently managed and scaled. Applications can allocate resources from any node in a resource pool, resulting in tight resource packing. Because of these benefits, many datacenters have adopted the idea of disaggregation, often at the storage layer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b66">68]</ref>. With the success of disaggregated storage, researchers in academia and industry have also sought ways to disaggregate memory (and persistent memory) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b67">69]</ref>. Different from storage disaggregation, MemDisagg needs to achieve at least an order of magnitude higher performance and it should offer a byte-addressable interface. Thus, MemDisagg poses new challenges and requires new designs. This section discusses the requirements of MemDisagg and why existing solutions cannot fully meet them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MemDisagg Design Goals</head><p>In general, MemDisagg has the following features, some of which are hard requirements while others are desired goals. R1: Hosting large amounts of memory. To keep the number of memory devices and total cost of a cluster low, each MN should host hundreds GBs to few TBs of memory that is expected to be close to fully utilized. Furthermore, we should allow applications to create and use many disjoint memory regions to make user programming flexible. R2: Supporting huge number of concurrent clients. To ensure tight and efficient resource packing, we should allow many (e.g., thousands of) client processes running on tens of CNs to access and share an MN. This scenario is especially important for new data-center trends like serverless computing and microservices. R3: Low-latency and high-throughput. We envision future systems to have a new memory hierarchy, where disaggregated memory is larger and slower than local memory but still faster than storage. We believe a good performance target of MemDisagg is to match the state-of-the-art network speed, i.e., 100 Gbps throughput (for bigger requests) and sub-2 µs median end-to-end latency (for smaller requests). R4: Low tail latency. Maintaining a low tail latency is important in meeting SLOs in data centers. Long tails like RDMA's 16.8 ms remote memory access can be detrimental to applications that are short running (e.g., serverless computing workloads) or have large fan-outs or big DAGs (because they need to wait for the slowest step to finish) <ref type="bibr" target="#b15">[17]</ref>. R5: Protected memory accesses. As an MN can be shared by multi-tenant applications running at CNs, we should properly isolate memory spaces used by them. Moreover, to prevent compromised or malicious system software running at CNs from reading/writing arbitrary memory at MNs, CNs should not directly access MNs' physical memory and MNs should check the permission of memory accesses. R6: Low cost. A major goal and benefit of resource disaggregation is cost reduction. Previous work <ref type="bibr" target="#b54">[56]</ref> has shown that MemDisagg could improve memory resource utilization by around 50% (i.e., a MemDisagg cluster only needs to host half of the memory compared to a non-disaggregated cluster). This means that 1) a MemDisagg system should aim to have close-to-full utilization of its memory and have minimal memory waste, and 2) building and running an MN should not double the cost of hosting memory, as such a MemDisagg system would cost even more than no disaggregation. Using a server to build an MN is thus not a good option, since a server box costs more than the DRAM it hosts. R7: Flexible. With the fast development of datacenter applications, hardware, and network, a sustainable MemDisagg solution should be flexible and extendable, for example, to support high-level APIs like pointer chasing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b53">55]</ref>, to offload some application logic to memory devices <ref type="bibr" target="#b53">[55,</ref><ref type="bibr" target="#b56">58]</ref>, or to incorporate different network transports <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b44">46]</ref> and congestion control algorithms <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b57">59</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Server-Based Disaggregated Memory</head><p>MemDisagg research so far has mainly taken a server-based approach by using regular servers as MNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b67">69]</ref>, usually on top of RDMA. The common limitation of these systems is their reliance on a host server, violating R6. RDMA has a set of scalability and tail-latency problems. A process (P M ) running at an MN needs to allocate memory in its virtual memory address space and register the allocated memory (called a memory region, or MR) with the RDMA NIC (RNIC). The host OS and MMU set up and manage the page table that maps P M 's virtual addresses (VAs) to physical memory addresses (PAs). To avoid always accessing host memory for address mapping, RNICs cache page table entries (PTEs), but when more PTEs are accessed than what this cache can hold, RDMA performance degrades significantly (Figure <ref type="figure">5</ref> and <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b64">66]</ref>). Similarly, RNICs cache MR metadata and incur degraded performance when the cache fills. Thus, RDMA has serious performance issues with either large memory (PTEs) or many disjoint memory regions (MRs), violating R1. Moreover, RDMA uses a slow way to support on-demand allocation: the RNIC interrupts the host OS for handling page faults. From our experiments, a faulting RDMA access is 14100× slower than a no-fault access. Page faults happen at the initial accesses to allocated virtual memory addresses, causing long tails (violating R4).</p><p>To mitigate the above performance and scalability issues, most RDMA-based systems today <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b64">66]</ref> preallocate a big MR with huge pages and pin it in physical memory, which results in inefficient memory space utilization and violates R6. Even with this approach, there can still be a scalability issue (R2), as RDMA uses MR as the protection domain and needs to create one MR for each client that needs isolation.</p><p>In addition to problems caused by RDMA's memory system design, reliable RDMA, the mode used by most MemDisagg solutions, suffers from a connection (QP) scalability issue, also violating R2. Finally, today's RNICs violate R7 because of their rigid one-sided RDMA interface and the close-sourced, hardware-based transport implementation. Solutions like 1RMA <ref type="bibr" target="#b57">[59]</ref>, Swift <ref type="bibr" target="#b34">[36]</ref>, HPCC <ref type="bibr" target="#b36">[38]</ref>, IRN <ref type="bibr" target="#b43">[45]</ref>, and Tonic <ref type="bibr" target="#b8">[9]</ref> mitigate the above issues by either onloading part of the transport back to software or proposing new hardware design. LegoOS <ref type="bibr" target="#b54">[56]</ref> is a distributed operating system designed for resource disaggregation. Its MN includes a virtual memory system that maps VAs of application processes running at CNs directly to MN PAs. Clio's MN performs the same type of address translation. However, LegoOS emulates MN devices using regular servers and builds its virtual memory system in software, which has a stark difference from a hardware-based virtual memory system. For example, Le-goOS uses a thread pool that handles incoming memory requests by looking up a hash table for address translation and permission checking. This software approach is the major performance bottleneck in LegoOS ( §7), violating R3. Moreover, LegoOS uses RDMA for its network communication hence inherits its limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Physical Disaggregated Memory</head><p>One way to build MemDisagg without a host server is to treat it as raw, physical memory, a model we call PDM. The PDM model has been adopted by a set of coherent interconnect proposals [15, 24], HPE's Memory-Driven Computing project <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b65">67]</ref>, and a recent research project that emulates PDM with regular servers <ref type="bibr" target="#b63">[65]</ref>. To prevent applications from accessing raw physical memory, these solutions add an indirection layer at CNs in hardware <ref type="bibr">[15,</ref><ref type="bibr" target="#b22">24]</ref> or software <ref type="bibr" target="#b63">[65]</ref> to map client process VAs or keys to MN PAs.</p><p>There are several common problems with all the PDM solutions. First, because MNs in PDM are raw memory, CNs need multiple network round trips to access an MN for complex operations like pointer chasing and concurrent operations that need synchronization <ref type="bibr" target="#b63">[65]</ref>, violating R3 and R7. Second, PDM requires the client side to manage disaggregated memory, which is much harder and performs worse compared to memory-side management (violating R3). For example, CNs need to coordinate with each other or use a global server <ref type="bibr" target="#b63">[65]</ref> to perform tasks like memory allocation. Third, exposing physical memory creates potential security issues (R5). MNs have to trust that CNs will never access beyond their allocated physical memory regions. Finally, all existing PDM solutions require physical memory pinning at MNs, causing memory wastes and violating R6.</p><p>In addition to the above problems, none of the coherent interconnects or HPE's Memory-Driven Computing have been fully built. When they do, they will require new hardware at all endpoints and new switches. Moreover, the interconnects automatically make caches at different endpoints coherent, which could affect performance R3 (because of network communication) and not always necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Clio Overview</head><p>Clio co-designs software with hardware, CNs with MNs, and network stack with virtual memory system, so that at the MN, the entire data path is handled in hardware with high throughput, low (tail) latency, and minimal hardware resources. This section gives an overview of Clio's interface and architecture (Figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clio Interface</head><p>Similar to recent MemDisagg proposals <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">55]</ref>, our current implementation adopts a non-transparent interface where applications (running at CNs) allocate and access disaggregated memory via explicit API calls, since doing so gives users opportunities to perform application-specific performance optimizations. <ref type="foot" target="#foot_0">1</ref> Apart from the regular (local) virtual memory address space, each process has a separate Remote virtual memory Address Space (RAS for short). Each application process has a unique global PID across all CNs which is assigned by Clio when the application starts. Overall, programming in RAS is similar to traditional multi-threaded programming except that memory read and write are explicit and that processes running on different CNs can share memory in the same RAS. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the usage of Clio with a simple code example.   An application process can perform a set of virtual memory operations in its RAS, including ralloc, rfree, rread, rwrite, and a set of atomic and synchronization primitives (e.g., rlock, runlock, rfence). ralloc works like malloc and returns a VA in RAS, and rread and rwrite can then be issued to any allocated VAs. As with the traditional virtual memory interface, an RAS has a byte granularity for allocation and access. For ralloc, rfree, rread, and rwrite, we offer two versions, synchronous and asynchronous, for users to choose between performance and consistency levels. A synchronous API blocks until the result is ready. An asynchronous API is nonblocking, and the application calls rpoll to get its result. Intra-thread request ordering. Within a thread, synchronous APIs follow strict ordering, and asynchronous APIs follow a release order. Specifically, asynchronous operations may be executed out of order as long as 1) all asynchronous operations before an rrelease complete before the rrelease returns, and 2) rrelease operations are strictly ordered. On top of this release order, we check read/write dependencies (WAR, RAW, WAW) for asynchronous operations targeting the same page. The resulting memory consistency level is the same as architectures like ARMv8 <ref type="bibr" target="#b9">[10]</ref>. In addition, we also ensure consistency between metadata and data operations, by ensuring that potentially conflicting operations execute synchronously in the program order. For example, if there is an ongoing rfree request to a VA range, no read or write to that range can start until the rfree finishes. Finally, failed or unresponsive requests are transparently retried, and they follow the same ordering guarantees. Thread synchronization and data coherence. Threads and processes can share data (even when they are not on the same CN). Similar to traditional concurrent programming, Clio threads can use synchronization primitives to build critical sections (e.g., with rlock) and other synchronization mechanisms (e.g., flushing all requests with rfence). An application can choose to cache data read from rread at the CN (e.g., by maintaining local rbuf in the code example). Different processes sharing data in an RAS could have their own cached copies at different CNs. Clio does not make these cached copies coherent automatically and lets applications choose their own coherence mechanisms and policies. We made this deliberate decision because automatic cache coherence on every read/write would incur high performance overhead with commodity Ethernet infrastructure and application semantics could reduce this overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clio Architecture</head><p>In Clio, CNs are regular servers, each equipped with a regular Ethernet NIC and connected to a ToR switch. MNs are our customized devices directly connected to a ToR switch.</p><p>Applications run at CNs on top of our library called CLib. CLib handles application requests in the user space. It is in charge of request ordering, request retry, and congestion control. Similar to DPDK <ref type="bibr" target="#b16">[18]</ref>, CLib bypasses kernel and has zero memory copy capability.</p><p>By design, 2 an MN in Clio is a CBoard that consists of an ASIC that runs the hardware logic for all data accesses (we call it the fast path), an ARM processor which runs software for handling metadata and control operations (i.e., the slow path), and an FPGA that hosts application computation offloading (i.e., the extend path). An incoming request arrives at the ASIC and travels through standard Ethernet physical and MAC layers and a Match-and-Action-Table (MAT) that decides which of the three paths the request should go to based on the request type. If the request is a data access (fast path), it stays in the ASIC and goes through a hardwarebased virtual memory system that performs three tasks in the same pipeline: address translation, permission checking, and page fault handling (if any). Afterwards, the actual memory access is performed through the memory controller, and the response is formed and sent out through the network stack. Metadata operations such as memory allocation are sent to the slow path. Finally, requests with customized, high-level operations such as pointer chasing and offloaded computation are handled in the FPGA extend path. In the rest of the paper, we focus on the design of the fast and the slow paths and how they interact with each other. 2 We prototyped CBoard's ASIC part with an FPGA ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Clio Design</head><p>This section presents the design challenges of building a hardware-based MemDisagg system and our solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Challenges and Principles</head><p>Building a hardware-based MemDisagg platform is a previously unexplored area and introduces new challenges mainly because of hardware's restrictions and the unique requirements of MemDisagg. Challenge 1: The hardware should avoid maintaining or processing complex data structures, because unlike software, hardware has limited resources such as on-chip memory and logic cells. For example, Linux and many other software systems use trees (e.g., the vma tree) for allocation. Maintaining and searching a big tree data structure in hardware, however, would require huge on-chip memory and many logic cells to perform the look up operation (or alternatively use less resources but suffer from performance loss). Challenge 2: States and buffers that the hardware uses should have bounded sizes, so that they can be statically planned and fit into the on-chip memory. Although swapping between on-chip and off-chip memory is possible, doing so would increase both tail latency and hardware logic complexity. Thus, it is desirable to resort as little as possible to swapping. Achieving the bounded buffer/state goal is even harder when we simultaneously need to meet our scalability goals. Unfortunately, traditional software approaches involve many states and buffers that are large and unscalable. For example, today's reliable network transport layer maintains per-connection sequence numbers and buffer unacknowledged packets for packet ordering and retransmission, and they grow with the number of connections. Challenge 3: The hardware pipeline should be deterministic and smooth, i.e., it uses a bounded, known number of cycles to process a data unit, and for each cycle, the pipeline can take in one new data unit (from the network). The former would ensure low tail latency, while the latter would guarantee a throughput that could match network line rate. Another subtle benefit of a deterministic pipeline is that we can know the maximum time a data unit stays at MN, which could help bound the size of certain buffers (e.g., §4.5). However, many traditional hardware solutions are not designed to be deterministic and smooth, and we cannot directly adopt their approaches. For example, traditional CPU pipelines could have stalls because of data hazards and have non-deterministic latency to handle memory instructions.</p><p>To confront these challenges, we took a clean-slate approach by designing Clio's virtual memory system and network system to follow the following principles. Principle 1: Eliminate states whenever possible. Not all states in server-based solutions are necessary if we could redesign the hardware. For example, we get rid of RDMA's MR indirection and its metadata altogether by directly map-ping from application process RAS VAs to PAs (instead of to MRs then to PAs). Principle 2: Moving non-critical operations and states to software and making the hardware fast path deterministic. If an operation is non-critical and it involves complex processing logic and/or metadata, our idea is to move it to the software slow path running in an ARM processor. For example, VA allocation (ralloc) is expected to be a rare operation because applications know the disaggregated nature and would typically have only a few large allocations during the execution. Handling ralloc, however, would involve dealing with complex allocation trees. We thus handle ralloc and rfree in the software slow path. Furthermore, in order to make the fast path performance deterministic, we decouple all slow-path tasks from the performance critical path by asynchronously performing them in the background. Note that page fault is a relatively critical operation, as all first accesses to allocated virtual pages will cause a fault, and applications like serverless computing could access large amounts of (new) memory in a short period of time. Principle 3: Shifting functionalities, states, and buffers to CNs. While hardware resources are scarce at MNs, CNs have sufficient memory and processing power, and it is faster to develop functionalities in CN software. A viable solution is to shift states and functionalities from MNs to CNs. The key question here is how much and what to shift; shifting too much would make Clio similar to PDM and suffer from various performance and security issues of PDM. Our strategy is to shift functionalities to CNs only if doing so 1) could largely reduce hardware resource consumption at MNs, 2) does not slow down common-case foreground data operations, 3) does not sacrifice security guarantees, and 4) adds bounded memory space and CPU cycle overheads to CNs. As a tradeoff, the shift may result in certain uncommon operations (e.g., handling a failed request) being slower. Principle 4: Making off-chip data structures efficient and scalable. Principles 1 to 3 allow us to reduce MN hardware to only the most essential functionalities and states. For these states, we store them in off-chip memory and cache a fixed amount of them in on-chip memory. Different from most caching solutions, our focus is to make the access to off-chip data structure fast and scalable, i.e., all cache misses have bounded latency regardless of the number of client processes accessing an MN or the size of memory the MN hosts. Principle 5: Making the hardware fast path smooth by treating each data unit independently at MN. If data units have dependencies (e.g., must be executed in a certain order), then the fast path cannot always execute a data unit when receiving it. To handle one data unit per cycle and reach network line rate, we make each data unit independent by including all the information needed to process a unit in it and by allowing MNs to execute data units in any order that they arrive. To deliver our consistency guarantees, we opt for enforcing request ordering at CNs before sending them out. The rest of this section presents how we follow these principles to design Clio's three main functionalities: memory address translation and protection, page fault handling, and networking. We also briefly discuss our offloading support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalable, Fast Address Translation</head><p>Similar to traditional virtual memory system, we use fix-size pages as address allocation and translation unit, while data is byte-addressable. Despite the similarity in the goal of address translation, the radix-tree-style, per-address space page table design used by all current architectures <ref type="bibr" target="#b58">[60]</ref> do not fit MemDisagg for two reasons. First, each request from the network could be from a different client process. If each process has its own page table, MN would need to cache and look up the page table root, causing additional overhead. Second, a multi-level page table design may require multiple DRAM accesses when there is a TLB miss <ref type="bibr" target="#b71">[73]</ref>. TLB misses will be much more common in a MemDisagg environment, since with more applications sharing an MN, the total working set size is much bigger than that in a single-server setting, while the TLB size in an MN will be similar or even smaller than a single server's TLB (for cost concerns). To make matters worse, each DRAM access is more costly for systems like RDMA NIC which has to cross the PCIe bus to access the page table in main memory <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b62">64]</ref>. Flat, single page table design (Principle 4). We propose a new overflow-free hash-based page table design that sets the total page table size according to the physical memory size and bounds address translation to at most one DRAM access. Specifically, we store all page table entries (PTEs) from all processes in a single hash table whose size is proportional to the physical memory size of an MN. The location of this page table is fixed in the off-chip DRAM and is known by the fast path address translation unit, thus avoiding a lookup. As we anticipate applications to allocate big chunks of VAs in their RAS, we use huge pages and support a configurable set of page sizes. With 4 MB page size, the hash table consumes only 0.4% of the physical memory.</p><p>The hash value of a VA and its PID are used as the index to determine which hash bucket the corresponding PTE goes to.</p><p>Each hash bucket has a fixed number of (K) slots. To access the page table, we always fetch the entire bucket including all K slots using a single DRAM access.</p><p>A well-known problem with hash-based page table design is hash collisions that could overflowing a bucket. Existing hash-based page table designs rely on collision chaining <ref type="bibr" target="#b11">[12]</ref> or open addressing <ref type="bibr" target="#b71">[73]</ref> to handle overflows, both require multiple DRAM accesses or even costly software intervention. In order to bound address translation to at most one DRAM access, we use a novel technique to avoid hash overflows at VA allocation time. VA allocation (Principle 2). The slow path software handles ralloc requests and allocate VA. The software allocator maintains a per-process VA allocation tree that records allocated VA ranges and permissions, similar to the Linux vma tree <ref type="bibr" target="#b33">[35]</ref>. To allocate size k of VAs, it first finds an available address range of size k in the tree. It then calculates the hash values of the virtual pages in this address range and checks if inserting them to the page table would cause any hash overflow. If so, it does another search for available VAs. These steps repeat until it finds a valid VA range.</p><p>Our design trades potential retry overhead at allocation time (at slow path) for better run-time performance and simpler hardware design (at fast path). This overhead is manageable because 1) each retry takes only a few microseconds with our implementation, 2) we employ huge pages, which means fewer pages need to be allocated, 3) we choose a hash function that has very low collision rate <ref type="bibr" target="#b68">[70]</ref>, and 4) we set the page table to have extra slots (2× by default). We find no conflicts when memory is below half utilized and has only up to 60 retries when memory is close to full (Figure <ref type="figure" target="#fig_0">13</ref>). TLB. Clio implements a TLB in a fix-sized on-chip memory area and looks it up using content-addressable-memory in the fast path. On a TLB miss, the fast path fetches the PTE from off-chip memory and inserts it to the TLB by replacing an existing TLB entry with the LRU policy. When updating a PTE, the fast path also updates the TLB, in a way that ensures the consistency of inflight operations. Limitation. A downside of our overflow-free VA allocation design is that it cannot guarantee that a specific VA can be inserted to the page table. This is not a problem for regular VA allocation but could be problematic for allocations that require a fixed VA (e.g., mmap(MAP FIXED)). Currently, Clio finds a new VA range if the user-specified range cannot be inserted to the page table. Applications that must map at fixed VAs (e.g., libraries) will need to use local memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Low-Tail-Latency Page Fault Handling</head><p>Page faults are traditionally signaled by the hardware and handled by the OS, and they can happen when a PTE is invalid (VA created, PA not allocated) or when there is a permission violation. While the latter is uncommon, the former happens at every initial access to a VA and could be common (e.g., serverless computing and microservices both frequently start many short running processes, incurring many initial-access page faults). Unfortunately, today's page fault handling mechanism is slow because of the costly interrupt and trap-to-kernel process. For example, a remote page fault via RDMA costs 16.8 ms. To avoid page faults, most RDMA-based system pre-allocate big chunks of physical memory and pin them physically. However, doing so results in memory wastes and makes it hard for an MN to pack more applications.</p><p>We propose to handle page faults in hardware and with bounded latency-a constant three cycles to be more specific with our implementation of CBoard. Achieving this performance is not easy. While handling permission-violation faults in hardware is easy (just by sending an error message as the request response), handling initial-access faults in hardware is challenging, as initial accesses require PA allocation, which is a complex operation that involves manipulating complex data structures. Thus, PA allocation should be performed by the slow path (Challenge 1). However, if the fast-path page fault handler has to wait for the slow path to generate a PA for each page fault, it will be slow.</p><p>To solve this problem, we propose an asynchronous design to shift PA allocation off the performance critical path (Principle 2). Specifically, we maintain a set of free physical page numbers in an async buffer, which the ARM continuously fulfills by finding free physical page addresses and reserving them without actually using the pages. During a page fault, the page fault handler simply fetches a pre-allocated physical page address. Note that even though a single PA allocation operation has a non-trivial delay, the throughput of generating PAs and filling the async buffer is higher than network line rate. Thus, the fast path can always find free PAs in the async buffer in time. After getting a PA from the async buffer and establishing a valid PTE, the page fault handler performs three tasks in parallel: writing the PTE to the off-chip page table, inserting the PTE to the TLB, and continuing the original faulting request. This parallel design hides the performance overhead of the first two tasks, allowing foreground request to proceed immediately.</p><p>A recent work <ref type="bibr" target="#b35">[37]</ref> also handles page fault in hardware. Its focus is on the complex interaction with kernel and storage devices, and it is a simulation-only work. Clio uses a different design for handling page fault in hardware with the goal of low tail latency, and we built it in real hardware. Putting virtual memory system together. We illustrate how CBoard's virtual memory system works using a simple example of allocating some memory and writing to it. The first step (ralloc) is handled by the slow path, which allocates a VA range by finding an available set of slots in the hash page table. The slow path forwards the new PTEs to the fast path, which inserts them to the page table. At this point, the PTEs are invalid. This VA range is returned to the client. When the client performs the first write, the request goes to the fast path. There will be a TLB miss, followed by a fetch of the PTE. Since the PTE is invalid, the page fault handler will be triggered, which fetches a free PA from the async buffer and establishes the valid PTE. It will then execute the write, update the page table, and insert the PTE to TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Stateless and Bufferless MN Network</head><p>With huge amounts of research and development efforts, today's data-center network systems are highly optimized in their performance. Our goal of Clio's network system is unique and fit MemDisagg's requirements-minimizing the network stack's hardware resource consumption at MNs and achieving great scalability, while maintaining similar performance as today's fast network. Traditional software-based reliable transports like Linux TCP incurs high performance overhead. Today's hardware-based reliable transports like RDMA are fast, but they require a fair amount of (on-chip) memory to maintain states, e.g., per-connection sequence numbers, congestion states <ref type="bibr" target="#b8">[9]</ref>, or bitmaps <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b43">45]</ref>, not meeting our low-cost goal.</p><p>Our insight is that different from general-purpose network communication where each endpoint can be both the sender (requester) and the receiver (responder) that exchange general-purpose messages, MNs only respond to requests sent by CNs, and these requests are all memory-related operations that have their specific properties. Based this insight, we design a new network system with two main ideas. Our first idea is to maintain transport logic, states, and buffers only at CNs, essentially making MNs "transportless" and thus stateless and bufferless (Principle 3)<ref type="foot" target="#foot_1">3</ref> . Our second idea is to relax the reliability of the transport and instead enforce ordering and loss recovery at the memory request level, so that MNs' hardware pipeline can process data units as soon as they arrive (Principle 5).</p><p>With these ideas, we implemented a transport in CLib at CNs. CLib bypasses the kernel to directly issue raw Ethernet requests to a regular Ethernet NIC. The MN includes only standard Ethernet physical and link layers and a slim ackgeneration layer ( §5). We now describe our detail design. Removing connections with request-response semantics. Connections (i.e., QPs) are a major scalability issue with RDMA. Similar to recent works <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b57">59]</ref>, we make our network system connection-less with memory request-response pairs. Applications running at CNs directly initiate Clio APIs to an MN without any connections. CLib uses the response of each Clio request as the ACK and matches it to the request using a request ID. Lifting reliability to the memory request level. Instead of triggering a retransmission protocol for every lost/corrupted packet at the transport layer, CLib retries the entire memory request if any packet is lost or corrupted (either in the send-ing or the receiving direction). On the receiving path, MN's network stack only checks packet's integrity. If a packet is corrupted, the MN immediately sends a NACK to the sender CN. CLib retries a memory request if one of three situations happen: a NACK is received, the response from MN is corrupted, or no response is received within a dynamic retransmission timeout (RTO) period. The RTO is computed using the moving average of prior end-to-end RTTs. In addition to lifting retransmission from transport to the request level, we also lift ordering to the memory request level and allow out-of-order packet delivery (see details in §4.5). CN-managed congestion and incast control. Our goal of controlling congestion in the network and handling incast that can happen both at a CN and an MN is to eliminate states at MN. To this end, we build the entire congestion and incast control at CN in the CLib. To control congestion, CLib uses a simple delay-based, reactive policy that uses end-to-end RTT delay as the congestion signal, similar to recent sendermanaged, delay-based mechanisms <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b57">59]</ref>. Each CN maintains one congestion window, cwnd, per MN that controls the maximum number of outstanding requests that can be made to the MN from this CN. We adjust cwnd based on measured delay using a standard Additive Increase Multiplicative Decrease (AIMD) manner.</p><p>To handle incast to a CN, we exploit the fact that the CN knows the sizes of expected responses for the requests that it sends out and that responses are the major incoming traffic to it. Each CLib maintains one incast window, iwnd, and sends a request only when both cwnd and iwnd have room.</p><p>Handling incast to an MN at CNs is more challenging, as we cannot throttle incoming traffic at the MN side or would otherwise maintain states at MNs. To handle MN incast at CNs, we draw inspiration from Swift <ref type="bibr" target="#b34">[36]</ref> by allowing cwnd to fall below one packet when long delay is observed at a CN. For example, a cwnd of 0.1 means that the CN can only send a packet within 10 RTTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Request Ordering and Data Consistency</head><p>Allowing intra-request packet re-ordering. Enforcing packet ordering above the link layer normally requires maintaining states (e.g., packet sequence ID) at both the sender and the receiver. To avoid maintaining such states at MNs, our approach is to deal with packet reordering only at CNs in CLib (Principle 3). Specifically, CLib splits a request that is bigger than MTU into several link-layer packets and attaches a Clio header to each packet, which includes sender-receiver addresses, a request ID, and request type. This enables the MN to treat each packet independently (Principle 5). It executes packets as soon as they arrive, even if they are not in the sending order. This out-of-order data placement semantic is in line with RDMA specification <ref type="bibr" target="#b43">[45]</ref>. Note that only write requests will be bigger than MTU, and the order of data writing within a write request does not affect correctness as long as proper inter-request ordering is followed.</p><p>When a CN receives multiple link-layer packets belonging to the same request response, CLib reassembles them before delivering to the application. Enforcing intra-thread inter-request ordering at CN based on memory semantics. As explained in §3.1, Clio supports both synchronous and asynchronous APIs. Since only one synchronous request can be outstanding in a thread, there cannot be any inter-request reordering problem. On the other hand, asynchronous requests require proper ordering since there can be multiple outstanding asynchronous requests and we need to enforce proper dependency (WAW, RAW, and WAR) and flush requests before rrelease. We enforce these ordering requirements at CNs in CLib instead of at MNs (Principle 3) for two reasons. First, enforcing ordering at MNs requires more on-chip memory and complex logic in hardware. Second, even if we enforce ordering at MNs, network reordering would still break end-to-end ordering guarantees, and we relax network ordering guarantees to minimize the hardware resource consumption at MNs ( §4.4).</p><p>Specifically, CLib keeps track of all inflight requests and matches every new request's virtual page number (VPN) to the inflight ones'. If a WAR, RAW, or WAW dependency is detected, CLib blocks the new request until the conflicting request finishes. When CLib sees a rrelease operation, it waits until all inflight requests return or time out. We currently track dependencies at the page granularity mainly to reduce tracking complexity and metadata overhead. The down side is that false dependencies could happen (e.g., two accesses to the same page but different addresses). False dependencies could be reduced by dynamically adapting the tracking granularity if application access patterns are tracked-we leave this improvement for future work. Inter-thread/process consistency. Multi-threaded or multiprocess concurrent programming on Clio could use the synchronization primitives Clio provides to ensure data consistency ( §3.1). We implemented all synchronization primitives like rlock and rfence at MN, because they need to work across threads and processes that possibly reside on different CNs. Before a request enters either the fast or the slow paths, MN checks if it is a synchronization primitive. For primitives like rlock that internally is implemented using atomic operations like TAS, MN blocks future atomic operations until the current one completes. For rfence, MN blocks all future requests until all inflight ones complete. Inflight synchronization primitives are one of the only two cases where MN needs to maintain states. As these operations are infrequent and each operation executes in bounded time, the hardware resources for maintaining these states are minimal and bounded. Idempotence. In Clio, most types of requests like rread and rwrite are idempotent and can be retried multiple times with the same result. Clio also includes a few types of non-idempotent requests such as atomic increment. To ensure that retrying non-idempotent requests will not gen-erate wrong results, we maintain a small ring buffer at MN to record the request IDs and results of K recently executed non-idempotent requests. If MN receives a request with the same ID in the buffer, it will not execute it and directly send the cached result as the response. An MN will not run any new non-idempotent requests when the ring buffer is full. To properly maintain the ring buffer, CNs send an ACK back to MN after receiving the response from it. This ACK will free a slot in the ring buffer. As non-idempotent requests are rare, MN only needs to maintain a small K-sized buffer, and this is the one of the only two cases where MN maintains states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Extension and Offloading Support</head><p>To avoid network round trips when working with complex data structures and/or performing data-intensive operations, we extend the core MN to support high-level APIs and application computation offloading in the extend path, which includes an FPGA chip and the ARM processor. We only have space to give a high-level overview of the extend path, leaving details to a follow-on paper. Users can write and deploy application offloads both in FPGA and in software. An offload can either be the handler of a high-level API (e.g., pointer chasing) or an entire function (e.g., data filtering). To ease the development of offloads, Clio offers the same virtual memory interface as the one to applications running at CNs. Each offload has its own address space, and it could also share data with processes running at CNs. Developing offloads is thus closer to traditional multi-threaded programming (in terms of memory accesses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CBoard Prototyping</head><p>We prototyped CBoard with a low-cost Xilinx MPSoC board <ref type="bibr" target="#b70">[72]</ref> and build the hardware fast path (which is anticipated to be built in ASIC) with FPGA. This board consists of a small FPGA with 504K logic cells (LUTs) and 4.75 MB FPGA memory (BRAM), a quad-core ARM Cortex-A53 processor, two 10 Gbps SFP+ ports connected to the FPGA, and 2 GB of off-chip on-board memory. This board has several differences from our anticipated real CBoard: its network port bandwidth and on-board memory size are both much lower than our projection, and like all FPGA prototypes, its clock frequency is much lower than real ASIC. Unfortunately, no board on the market offers the combination of small FPGA/ARM (required for low cost) with large memory and high-speed network ports.</p><p>Nonetheless, certain features of this board are likely to exist in a real CBoard, and these features guide our implementation. Its ARM processor and the FPGA connect through an interconnect that has high bandwidth (90 GB/s) but high delay (40 µs). Although better interconnects could be built, crossing ARM and FPGA would inevitably incur non-trivial performance overhead. With this board, both the ARM and the FPGA can access an on-board DRAM through a DDR interface, but the ARM's access is much slower because it has to first physically cross the FPGA then to the DRAM. A better design would connect the ARM directly to the DRAM, but it will still be slower for the ARM to access on-board DRAM than its local on-chip memory.</p><p>Currently, the FPGA prototype of Clio (excluding computation offloads and third-party IPs<ref type="foot" target="#foot_2">4</ref> ) includes a total of 5.6K SLOC written in SpinalHDL <ref type="bibr" target="#b59">[61]</ref>, 2K in C HLS, and 17K in C for host and ARM. All Clio's FPGA modules run at 250 MHz clock frequency and 512-bit data width. They all achieve an Initiation Interval (II) of one (II is the number of clock cycles between the start time of consecutive loop iterations, and it decides the maximum achievable throughput). Achieving an II of one is not easy and requires careful pipeline design in all the modules.</p><p>Below, we pick some techniques used in our prototyping implementation that will still be applicable in a real CBoard.</p><p>To mitigate the problem of slow accesses to on-board DRAM from ARM, we maintain shadow copies of metadata at ARM's local DRAM. For example, we store a shadow version of the page table in ARM's local memory, and keep it in sync with the real page table in the on-board DRAM. We employ an efficient polling mechanism for ARM/FPGA communication. We dedicate one ARM core to busy poll an RX ring buffer between ARM and FPGA, where the FPGA posts tasks for ARM. This polling thread hands over tasks to other worker threads for task handling and post responses to a TX ring buffer.</p><p>CBoard's network stack builds on top of standard, vendorsupplied Ethernet physical and link layer IPs, with just an additional thin checksum-verify and ack-generation layer on top. This layer uses much less resources compared to a normal RDMA-like stack ( §7.3). We use lossless Ethernet with Priority Flow Control (PFC) for less packet loss and retransmission. Since PFC has issues like head-of-line blocking <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b72">74]</ref>, we rely on our congestion and incast control to avoid triggering PFC as much as possible.</p><p>To assist Clio users in building their applications, we implemented a simple software simulator of CBoard which works with CLib for developers to test their code without the need to run an actual CBoard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Building Applications on Clio</head><p>We built three applications on top of Clio, one that uses the basic Clio APIs, one that uses a high-level, extended API, and one that offloads computation to MNs. Image compression. We build a simple image compression/decompression utility that runs purely at CN. Each client of the utility (e.g., a Facebook user) has its own collection of photos, stored in two arrays at MN, one for compressed and one for original. Because clients' photos need to be protected from each other, we use one process per client to run the utility. The utility simply reads a photo from MN using rread, compresses/decompresses it, and writes it back to the other array using rwrite. We implemented this utility with 1K C code in 3 developer days. Radix tree. To demonstrate how to build a data structure on Clio using Clio's extended API, we built a radix tree with linked lists and pointers. Data-structure-level systems like AIFM <ref type="bibr" target="#b53">[55]</ref> could follow this example to make simple changes to run on Clio. At MN, we built an extended pointerchasing API in hardware to perform a value comparison at each chased node and returns either when there is a match or the next pointer becomes null. Searching the radix tree mainly involves going through layers of nodes and performing this modified pointer chasing API. We implemented the radix tree with 300 C code and 150 SpinalHDL code at MN in less than one developer day. Key-value store. We built Clio-KV, a key-value store that supports concurrent create/update/read/delete key-value entries with atomic write and read committed consistency guarantees. Clio-KV runs at MN as computation offloads. Users can access it through a key-value interface from CNs. The Clio-KV module has its own address space. It stores keyvalue pair data and a chained hash table for metadata in this virtual memory address space and accesses them with Clio virtual memory APIs. We implemented Clio-KV with 772 SpinalHDL code in 6 developer days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Our evaluation reveals the scalability, throughput, median and tail latency, energy and resource consumption of Clio. We compare Clio's end-to-end performance with industrygrade NICs (ASIC) and well-tuned RDMA-based software systems. All Clio's results are FPGA-based, which would be improved with ASIC implementation. Nonetheless, Clio significantly outperforms RDMA on scalability and tail latency, while being similar on other measurements. Environment. We evaluated Clio on our local cluster of four CNs and one Xilinx ZCU106 board, all connected to a Nvidia 40 Gbps VPI switch. Each CN is a Dell PowerEdge R740 server equipped with a Xeon Gold 5128 CPU and a 40 Gbps Nvidia Connect-X3 NIC, with two of them also have a Nvidia BlueField SmartNIC <ref type="bibr" target="#b41">[43]</ref>. We also include results from CloudLab <ref type="bibr" target="#b13">[14]</ref> with the Nvidia Connect-X5 NIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Basic Microbenchmark Performance</head><p>Scalability. We first compare the scalability of Clio and RDMA. Figure <ref type="figure">4</ref> measures the latency of Clio and RDMA as the number of client processes increases. For RDMA, each process uses its own QP. Since Clio is connectionless, it scales perfectly with the number of processes. RDMA scales poorly with its QP, and the problem persist with newer generations of RNIC, which is also confirmed by others <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b62">64]</ref>.</p><p>Figure <ref type="figure">5</ref> evaluates the scalability with respect to PTEs and memory regions. For the memory region test, we register        multiple MRs using the same physical memory for RDMA. For Clio (which gets rid of the MR concept), we use multiple processes to share the same memory, resulting in one PTE per process. RDMA's performance starts to degrade when there are more than 2 8 (local cluster) or 2 12 (CloudLab), and the scalability wrt MR is worse than wrt PTE. In fact, RDMA fails to run beyond 2 18 MRs. In contrast, Clio scales well and never fails (at least up to 4 TB memory <ref type="foot" target="#foot_3">5</ref> ). It has two levels of latency: lower latency below 2 4 for TLB hit and higher above 2 4 for TLB miss (which results in a constant of one DRAM access). A real CBoard could use a larger TLB if optimal performance is desired.</p><p>These experiments confirm that Clio can handle thousands of concurrent clients and TBs of memory. Latency variation. Figure <ref type="figure" target="#fig_4">6</ref> plots the latency of reading/writing 16 B data when the operation results in a TLB hit, a TLB miss, a first-access page fault, and MR miss (for RDMA only). RDMA's performance degrades significantly with misses. Its page fault handling cost is startling high -16.8 ms. We confirm the same effect on CloudLab. Clio only incurs a small TLB miss cost and no additional cost of page fault handling.</p><p>We also include a projection of Clio's latency if it was to be implemented using a real ASIC-based CBoard. Specifically, we collect the latency breakdown of time spent on the network wire and at CN, number of cycles on FPGA, and time on accessing on-board DRAM. We maintain the first part, scale the FPGA part to ASIC's frequency (2 GHz), use DDR access time collected on our server to replace the access time to on-board DRAM (which goes through a slow board memory controller). Our projected read latency is bet-ter than RDMA, while write is worse. We suspect that it is due to Nvidia RNIC's optimization of replying to a write before it is written to DRAM, which Clio could also adopt.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> plots the request latency CDF of continuously running read/write 16 B data. Clearly, Clio has much less latency variation and a much shorter tail than RDMA. Read/write throughput. We measure Clio's throughput by varying the number of concurrent client threads (Figure <ref type="figure" target="#fig_7">8</ref>). Clio's default asynchronous APIs quickly reach the line rate of our testbed (9.4 Gbps maximum throughput). Its synchronous APIs could also reach line rate fairly quickly. Figure <ref type="figure">9</ref> measures the maximum throughput Clio's FPGA implementation could reach without the bottleneck of the board's 10 Gbps port. Both read and write can reach more than 110 Gbps when request size is large. Read throughput is lower than write when request size is smaller. We discovered that the throughput bottleneck is at a third-party nonpipelined DMA IP (which could potentially be improved).</p><p>Comparison with other systems. We compare Clio with native one-sided RDMA, Clover <ref type="bibr" target="#b63">[65]</ref>, HERD <ref type="bibr" target="#b32">[34]</ref>, and Le-goOS <ref type="bibr" target="#b54">[56]</ref>. We ran HERD on both CPU and BlueField (HERD-BF). Clover is a passive disaggregated persistent memory system that we adapted as a passive disaggregated memory (PDM) system. HERD is an RDMA-based system that supports a key-value interface with an RPC-like architecture. LegoOS emulates MNs using regular servers and builds its virtual memory system in software.</p><p>Clio's performance is similar to HERD and close to native RDMA. Clover's write is the worst because of it uses at least 2 RTTs for writes to deliver its consistency guarantees without any processing power at MNs. HERD-BF's latency is much higher than when HERD runs on CPU due to the slow communication between BlueField's Connect-X5 chip and ARM processor chip. LegoOS's latency is almost two      times higher than Clio's when request size is small. In addition, from our experiment, LegoOS can only reach a peak throughput of 77 Gbps, while Clio can reach 110 Gbps. Le-goOS' performance overhead comes from LegoOS' software approach, demonstrating the necessity of a hardware-based solution like Clio. Allocation performance. Figure <ref type="figure" target="#fig_1">12</ref> shows Clio's VA and PA allocation comparing to RDMA's MR registration performance (ODP means RDMA is in On-Demand-Paging mode). Clio's PA allocation takes less than 20 µs, and the VA allocation is much faster than RDMA MR registration, although both get slower with larger allocation/registration size. Figure <ref type="figure" target="#fig_0">13</ref> shows the number of retries at allocation time with three alloc sizes (1, 10, and 100 pages) as the physical memory gets filled up. There is no retry when memory is below half utilized. Even when memory is close to full, there are at most 60 retries per allocation request, with roughly 0.5 ms per retry. This confirms that our design of avoiding hash overflows at allocation time is practical. Close look at CBoard components. To further understand Clio's performance, we profile different parts of Clio's processing for read and write of 4 B to 1 KB. CLib adds a very small overhead (250 ns in total), thanks to our efficient threading model and network stack implementation. Figure <ref type="figure" target="#fig_0">14</ref> shows the latency breakdown at CBoard. Time to fetch data from DRAM (DDRAccess) and to transfer it over the wire (WireDelay) are the main contributor to read latency, especially with large read size. Both could be largely improved in a real CBoard with better memory controller and higher frequency. TLB miss (which takes one DRAM read) is the other main part of all the latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Application Performance</head><p>Image Compression. We run a workload where each client compresses and decompresses 1000 256*256-pixel images with increasing number of concurrently running clients. Figure <ref type="figure" target="#fig_0">15</ref> shows the total runtime per client. Clio's performance stays the same as the number of clients increase. RDMA's performance does not scale because it requires each client to register a different MR. Radix Tree. Figure <ref type="figure" target="#fig_4">16</ref> shows the latency of searching a key in pre-populated radix trees when varying the tree size. RDMA's performance is worse than Clio, because it requires multiple RTTs to traverse the tree, while Clio only needs one RTT for each pointer chasing. Unlike Clio, RDMA's performance also scales poorly. Key-value store. In Figure <ref type="figure" target="#fig_10">17</ref>, we evaluate Clio-KV using the YCSB benchmark <ref type="bibr" target="#b0">[1]</ref> and compare it to Clover, HERD, and HERD-BF. We run two CNs and 8 threads per CN. We use 100K key-value entries and run 100K operations per test, with YCSB's default key-value size of 1 KB. The accesses to keys follow the Zipf distribution (θ = 0.99). We use three YCSB workloads with different get-set ratios: 100% get (workload C), 5% set (B), and 50% set (A). Clio-KV outperforms all the other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Energy Cost and FPGA Utilization</head><p>We measure the total energy used for running YCSB workloads by collecting the total CPU (or FPGA) cycles and the Watt of a CPU core <ref type="bibr" target="#b1">[2]</ref>, ARM processor <ref type="bibr" target="#b51">[53]</ref>, and FPGA (measured). We omit the energy used by DRAM and NICs in all the calculation. Clover, a system that centers its design around low cost, has slightly higher energy than Clio. Even though there is no processing at MNs for Clover, its CNs use more cycles to process and manage memory. HERD consumes 1.6× to 3× more energy than Clio, mainly because its CPU overhead at MNs. Surprisingly, HERD-BF consumes the most energy, even though it is a low-power ARM-based SmartNIC. This is because of its worse performance and longer total runtime. Figure <ref type="figure" target="#fig_13">19</ref> compares the FPGA utilization among Clio, StRoM's RoCEv2 <ref type="bibr" target="#b56">[58]</ref>, and Tonic's selective ack stack <ref type="bibr" target="#b8">[9]</ref>. With our design that is tailored to save resources, Clio consumes roughly one third of the total resources. Both StRoM and Tonic include just a network stack yet they consume more resources than Clio. Within Clio, the virtual memory (VirtMem) and the network stack (NetStack) consume a small fraction of the total resources, with the rest being vendor IPs (PHY, MAC, DDR4, and interconnect). To put things in perspective, we implement a RDMA-like Go-back-N network stack which supports 1K connections. It uses 2.5× more logic than what our current network stack consumes. In all, our efficient hardware implementation leaves most FPGA resources available for application offloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented Clio, a new hardware-based disaggregated memory system. Our FPGA prototype demonstrates that Clio achieves great performance, scalability, and cost saving. This work not only helps the future development of MemDisagg solutions but also demonstrate how to implement a core OS subsystem in the hardware.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Clio Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of Using Clio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Process (Connection) Scalability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of TLB Miss and page fault. Clio-ASIC are projected values of TLB hit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Latency CDF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: End-to-End Goodput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :Figure 13 :Figure 14 :Figure 16 :</head><label>12131416</label><figDesc>Figure 12: Alloc/Free Latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Key-Value Store YCSB Latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Energy Comparison. Darker/lighter shades represent energy spent at MNs and CNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: FPGA Utilization.</figDesc><graphic url="image-3.png" coords="12,432.99,192.72,123.82,62.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>CBoard Design. Green, yellow, and red areas are anticipated to be built with ASIC, FPGA, and low-power cores.</figDesc><table><row><cell></cell><cell></cell><cell>VA Alloc/Update/Free</cell><cell cols="2">Offline PA</cell><cell></cell><cell></cell></row><row><cell>Offload1</cell><cell></cell><cell>Addr Space Creation</cell><cell cols="2">Allocation</cell><cell cols="2">Slow Path</cell></row><row><cell>Offload1</cell><cell>PTE update</cell><cell>Address Translation TLB Manager Async Buffers</cell><cell cols="2">insert PTE to TLB</cell><cell cols="2">Fast Path</cell></row><row><cell>MAT Extend Offload2 synchronize primitives</cell><cell>header data</cell><cell cols="2">Data Access TLB miss hit Request Data Buffer Hash &amp; get PTE invalid valid</cell><cell>Page Fault Handler</cell><cell>Write PTE</cell><cell>Memory Access</cell><cell>Hash DRAM</cell></row><row><cell>MAC+PHY Network</cell><cell></cell><cell cols="2">Generate Response</cell><cell></cell><cell></cell><cell></cell><cell>Page Table</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Write Latency.</figDesc><table><row><cell></cell><cell>0 20 40 60 80 100 Goodput (Gbps) 120</cell><cell>Read Write</cell><cell>Latency (us)</cell><cell>0 2 4 6 8 10</cell><cell>Clio Clover RDMA</cell><cell>HERD−BF HERD LegoOS</cell><cell></cell><cell>Latency (us)</cell><cell>0 2 4 6 10 8</cell><cell>Clio Clover RDMA</cell><cell>HERD−BF HERD LegoOS</cell></row><row><cell></cell><cell cols="2">Request Size (B) 64 128 256 512 1K 2K 4K 8K</cell><cell></cell><cell>4</cell><cell cols="2">Request Size (B) 16 64 256 1K</cell><cell>4K</cell><cell></cell><cell>4</cell><cell>Request Size (B) 16 64 256 1K</cell><cell>4K</cell></row><row><cell>1 KB requests.</cell><cell cols="2">Figure 9: On-board Goodput. FPGA test module generates requests at maximum speed.</cell><cell cols="5">Figure 10: HERD-BF: HERD running on Blue-Read Latency. Field SmartNIC.</cell><cell cols="3">Figure 11: Clover requires at least 2 RTTs for write.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">CBoard could potentially work with a transparent MemDisagg solution if CNs can directly intercept memory instructions and issue remote requests using application VAs (e.g., with LegoOS's processor architecture).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">1RMA<ref type="bibr" target="#b57">[59]</ref>, a recent server-based remote memory system, onloads most of its retransmission and congestion logic from the NIC to the host CPU. As a result, 1RMA's NIC is simple. However, 1RMA relies on a companion host to function, violating the "server-less" goal of MNs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We use several third-party IPs: 1) vendor-supplied interconnect and DDR IPs, 2) an open-source MAC and PHY network stack<ref type="bibr" target="#b20">[22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">This calculation is based on the number of PTEs, while our experiments map all of them to a small range of memory as our testbed only has 2 GB physical memory.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Repository</forename><surname>Ycsb Github</surname></persName>
		</author>
		<ptr target="https://github.com/brianfrankcooper/YCSB" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://ark.intel.com/content/www/us/en/ark/products/192444/intel-xeon-gold-5218-processor-22m-cache-2-30-ghz.html" />
	</analytic>
	<monogr>
		<title level="j">Intel Xeon Gold</title>
		<imprint>
			<biblScope unit="volume">5128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing Far Memory Data Structures: Think Outside the Box</title>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS &apos;19)</title>
				<meeting>the Workshop on Hot Topics in Operating Systems (HotOS &apos;19)<address><addrLine>Bertinoro, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
	<note>Stanko Novakovic, and Sharad Singhal</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">pangu -the high performance distributed file system by alibaba cloud</title>
		<author>
			<persName><surname>Alibaba</surname></persName>
		</author>
		<ptr target="https://www.alibabacloud.com/blog/pangu-the-high-performance-distributed-file-system-by-alibaba-cloud594059" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can far memory improve job throughput?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Branner-Augmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Computer Systems (EuroSys &apos;20)</title>
				<meeting>the Fifteenth European Conference on Computer Systems (EuroSys &apos;20)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Amazon elastic block store</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ebs/?nc1=hls" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Amazon s3</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/s3/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disaggregation and the Application</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Nanavati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Workshop on Hot Topics in Cloud Computing</title>
				<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enabling programmable transport protocols in high-speed nics</title>
		<author>
			<persName><forename type="first">Mina</forename><surname>Tahmasbi Arashloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lavrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manya</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Symposium on Networked Systems Design and Implementation</title>
				<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/armv8-a-architecture-2016-additions" />
		<title level="m">ARMv8</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FireBox: A Hardware Building Block for 2020 Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote talk at the 12th USENIX Conference on File and Storage Technologies</title>
				<imprint>
			<date type="published" when="2014-02">February 2014</date>
		</imprint>
	</monogr>
	<note>FAST &apos;14</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Translation caching: Skip, don&apos;t walk (the page table)</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10</title>
				<meeting>the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taking advantage of a disaggregated storage and compute architecture</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ergin</forename><surname>Seyfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spark+AI Summit 2019 (SAIS &apos;19)</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://www.cloudlab.us/" />
		<title level="m">CloudLab</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rpcvalet: Ni-driven tail-aware balancing of µsscale rpcs</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The tail at scale</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barroso</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://www.dpdk.org/" />
		<title level="m">DPDK</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FaRM: Fast Remote Memory</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dragojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orion</forename><surname>Hodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation (NSDI &apos;14)</title>
				<meeting>the 11th USENIX Conference on Networked Systems Design and Implementation (NSDI &apos;14)<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introducing bryce canyon: Our next-generation storage platform</title>
		<author>
			<persName><surname>Facebook</surname></persName>
		</author>
		<ptr target="https://code.fb.com/data-center-engineering/introducing-bryce-canyon-our-next-generation-storage-platform" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond Processor-centric Operating Systems</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Faraboschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Marsland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Milojicic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Workshop on Hot Topics in Operating Systems (HotOS &apos;15)</title>
				<imprint>
			<publisher>Kartause Ittingen, Switzerland</publisher>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Corundum: An Open-Source 100-Gbps NIC</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Forencich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IEEE International Symposium on Field-Programmable Custom Computing Machines (FCCM &apos;20)</title>
				<meeting><address><addrLine>Fayetteville,AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">When cloud storage meets RDMA</title>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqing</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingkui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiesheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th USENIX Symposium on Networked Systems Design and Implementation</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>NSDI 21</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gen-Z</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="https://genzconsortium.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient Memory Disaggregation with Infiniswap</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;17)</title>
				<meeting>the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;17)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Re-architecting datacenter networks and stacks for low latency and high performance</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costin</forename><surname>Raiciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Agache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Voinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Wójcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;17)</title>
				<meeting>the Conference of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;17)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Hewlett</forename><surname>Packard</surname></persName>
		</author>
		<ptr target="http://www.hpl.hp.com/research/systems-research/themachine/" />
		<title level="m">The Machine: A New Kind of Computer</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Memory Technology Evolution: An Overview of System Memory Technologies the 9th edition</title>
		<author>
			<persName><surname>Hewlett-Packard</surname></persName>
		</author>
		<ptr target="https://support.hpe.com/hpesc/public/docDisplay?docId=emrna-c00256987" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="https://www.hpe.com/us/en/newsroom/blog-post/2017/05/memory-driven-computing-explained.html" />
		<title level="m">Memory-Driven Computing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Hewlett Packard Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mallery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serhat</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Jepsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12114</idno>
		<title level="m">The nanopu: Redesigning the cpunetwork interface to minimize rpc tail latency</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://www.intel.com/content/www/us/en/architecture-and-technology/intel-rack-scale-architecture.html" />
		<title level="m">Intel Rack Scale Architecture: Faster Service Delivery and Lower TCO</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ITRS. International Technology Roadmap for Semiconductors</title>
		<imprint/>
	</monogr>
	<note>SIA) 2014 Edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shinjuku: Preemptive scheduling for usecond-scale tail latency</title>
		<author>
			<persName><forename type="first">Kostis</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">Tigar</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mazières</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Andersen. Using RDMA Efficiently for Key-value Services</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM Conference on Special Interest Group on Data Communication (SIGCOMM &apos;14)</title>
				<meeting>the 2014 ACM Conference on Special Interest Group on Data Communication (SIGCOMM &apos;14)<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Red-black trees (rbtree) in linux</title>
		<ptr target="https://www.kernel.org/doc/Documentation/rbtree.txt" />
	</analytic>
	<monogr>
		<title level="m">Linux Kernel</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Swift: Delay is simple and effective for congestion control in the datacenter</title>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaogong</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Springborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Alfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication, SIGCOMM &apos;20</title>
				<meeting>the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication, SIGCOMM &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A case for hardware-based demand paging</title>
		<author>
			<persName><forename type="first">Gyusun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonsuk</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeonghun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae Jun</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</title>
				<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture, ISCA &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HPCC: High Precision Congestion Control</title>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongqiang</forename><surname>Harry Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;19)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM &apos;19)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disaggregated memory for expansion and sharing in blade servers</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA &apos;09)</title>
				<meeting>the 36th Annual International Symposium on Computer Architecture (ISCA &apos;09)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">System-level implications of disaggregated memory</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshio</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">Renato</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Auyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE 18th International Symposium on High-Performance Computer Architecture (HPCA &apos;12)</title>
				<meeting>the 2012 IEEE 18th International Symposium on High-Performance Computer Architecture (HPCA &apos;12)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02">February 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Offloading Distributed Applications onto SmartNICs Using IPipe</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM &apos;19)</title>
				<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM &apos;19)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Memory efficient loss recovery for hardware-based transport in datacenter</title>
		<author>
			<persName><forename type="first">Yuanwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Asia-Pacific Workshop on Networking, APNet&apos;17</title>
				<meeting>the First Asia-Pacific Workshop on Networking, APNet&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<author>
			<persName><surname>Bluefield Smartnic</surname></persName>
		</author>
		<ptr target="http://www.mellanox.com/related-docs/prodadaptercards/PBBlueFieldSmartNIC.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">TIMELY: RTT-based Congestion Control for the Datacenter</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">The</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monia</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaogong</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><surname>Zats</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communica</title>
		<imprint/>
	</monogr>
	<note>tion Review (SIGCOMM &apos;15</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Revisiting network support for rdma</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shpiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Zahavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM &apos;18</title>
				<meeting>the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Homa: A receiver-driven lowlatency transport protocol using network priorities</title>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Behnam Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication</title>
				<meeting>the 2018 Conference of the ACM Special Interest Group on Data Communication</meeting>
		<imprint>
			<publisher>SIG-COMM</publisher>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding pcie performance for end host networking</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Fernando Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>López-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM &apos;18</title>
				<meeting>the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Welcome to zombieland: Practical and energy-efficient memory disaggregation in a datacenter</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Nitu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Teabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Tchana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canturk</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hagimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys &apos;18</title>
				<meeting>the Thirteenth EuroSys Conference, EuroSys &apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Welcome to zombieland: Practical and energy-efficient memory disaggregation in a datacenter</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Nitu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Teabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Tchana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canturk</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hagimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference (EuroSys &apos;18)</title>
				<meeting>the Thirteenth EuroSys Conference (EuroSys &apos;18)<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dan Tsafrir, and Marcos Aguilera. Storm: A fast transactional dataplane for remote data structures</title>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Pismenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on Systems and Storage (SYSTOR &apos;19)</title>
				<meeting>the 12th ACM International Conference on Systems and Storage (SYSTOR &apos;19)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The ramcloud storage system</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions Computer System</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Running 8-bit dynamic fixed-point convolutional neural network on low-cost arm platforms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mingyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weisheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Chinese Automation Congress (CAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Zygos: Achieving low tail latency for microsecondscale networked tasks</title>
		<author>
			<persName><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles, SOSP &apos;17</title>
				<meeting>the 26th Symposium on Operating Systems Principles, SOSP &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">AIFM: High-performance, application-integrated far memory</title>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;20)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Legoos: A disseminated, distributed OS for hardware resource disaggregation</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;18)</title>
				<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distributed shared persistent memory</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Annual Symposium on Cloud Computing (SOCC &apos;17)</title>
				<meeting>the 8th Annual Symposium on Cloud Computing (SOCC &apos;17)<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Smart Remote Memory</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Chiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Computer Systems (EuroSys &apos;20)</title>
				<meeting>the Fifteenth European Conference on Computer Systems (EuroSys &apos;20)<address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Re-envisioning remote memory access for multitenant datacenters</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Singhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Wong-Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milo</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moray</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Cauble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">L</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Scherpelz</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures</title>
				<meeting>the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>and Protocols for Computer Communication (SIGCOMM &apos;20</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Elastic cuckoo page tables: Rethinking virtual memory translation for parallelism</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spinalhdl</surname></persName>
		</author>
		<author>
			<persName><surname>Spinalhdl</surname></persName>
		</author>
		<ptr target="https://github.com/SpinalHDL/SpinalHDL" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Introduction to storage area networks</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pall</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">Hugo</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanmuganathan</forename><surname>Kumaravel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libor</forename><surname>Miklas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Redbooks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Alibaba singles&apos; day 2019 had a record peak order rate of 544,000 per second</title>
		<ptr target="https://techpp.com/2019/11/19/alibaba-singles-day-2019-record/" />
	</analytic>
	<monogr>
		<title level="j">TECHPP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pythia: Remote oracles for the masses</title>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium (USENIX Security 19</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Disaggregating Persistent Memory and Controlling Them from Remote: An Exploration of Passive Disaggregated Key-Value Stores</title>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC &apos;20)</title>
				<meeting>the 2020 USENIX Annual Technical Conference (ATC &apos;20)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">LITE Kernel RDMA Support for Datacenter Applications</title>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</title>
				<meeting>the 26th Symposium on Operating Systems Principles (SOSP &apos;17)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Memory-Oriented Distributed Computing at Rack Scale</title>
		<author>
			<persName><forename type="first">Haris</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milind</forename><surname>Chabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwon</forename><surname>Se</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuvraj</forename><surname>Lillibridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing, (SoCC &apos;18)</title>
				<meeting>the ACM Symposium on Cloud Computing, (SoCC &apos;18)<address><addrLine>Carlsbad, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Building An Elastic Query Engine on Disaggregated Storage</title>
		<author>
			<persName><forename type="first">Midhul</forename><surname>Vuppalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Motivala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Cruanes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;20)</title>
				<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Semeru: A memory-disaggregated managed runtime</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><forename type="middle">Harry</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;20)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">jenkins hash function</title>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Jenkinshashfunction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">Wm</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995-03">March 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<author>
			<persName><surname>Zynq</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/boards-and-kits/zcu106.html" />
		<title level="m">UltraScale+ MPSoC ZCU106 Evaluation Kit</title>
				<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<title level="m">Proceedings of the 2016 ACM SIGMET-RICS International Conference on Measurement and Modeling of Computer Science, SIGMETRICS &apos;16</title>
				<meeting>the 2016 ACM SIGMET-RICS International Conference on Measurement and Modeling of Computer Science, SIGMETRICS &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Hash, don&apos;t cache (the page table</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Congestion Control for Large-Scale RDMA Deployments</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Lipshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehonatan</forename><surname>Liron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Haj Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication (SIGCOMM &apos;15)</title>
				<meeting>the 2015 ACM Conference on Special Interest Group on Data Communication (SIGCOMM &apos;15)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
