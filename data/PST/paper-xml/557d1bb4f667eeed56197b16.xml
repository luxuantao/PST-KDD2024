<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundary Detection by Constrained Optimization &apos;</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>MEMBER, IEEE</roleName><forename type="first">Donald</forename><surname>Geman</surname></persName>
						</author>
						<author>
							<persName><roleName>MEMBER, IEEE</roleName><forename type="first">Stuart</forename><surname>Geman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Graffigne</surname></persName>
						</author>
						<author>
							<persName><roleName>MEMBER, IEEE</roleName><forename type="first">Ping</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Scientific Research (CNRS)</orgName>
								<orgName type="department" key="dep2">Equipe de Statistique Appliquke</orgName>
								<orgName type="institution">UniversitC de Paris-Sud</orgName>
								<address>
									<addrLine>Bitiment 425</addrLine>
									<postCode>91405</postCode>
									<settlement>Orsay Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">Uni-versity of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<postCode>02048</postCode>
									<settlement>Codex, Mansfield</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boundary Detection by Constrained Optimization &apos;</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">03AD2485F72E00681B83D0D8DFA8180A</idno>
					<note type="submission">received April 18, 1988; revised September 14, 1989. Rec-</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zndex Terms-Annealing</term>
					<term>Bayesian inference</term>
					<term>boundary finding</term>
					<term>constrained optimization</term>
					<term>Gibbs distribution</term>
					<term>MAP estimate</term>
					<term>Markov random field</term>
					<term>segmentation</term>
					<term>stochastic relaxation</term>
					<term>texture discrimination</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use a statistical framework for finding boundaries and for partitioning scenes into homogeneous regions. The model is a joint probability distribution for the array of pixel gray levels and an array of "labels." In boundary finding, the labels are binary, zero, or one, representing the absence o r presence of boundary elements. In partitioning, the label values a r e generic: two labels are the same when the corresponding scene locations a r e considered to belong to the same region. The distribution incorporates a measure of disparity between certain spatial features of pairs of blocks of pixel gray levels, using the Kolmogorov-Smirnov nonparametric measure of difference between the distributions of these features. Large disparities encourage intervening boundaries and distinct partition labels. The number of model parameters is minimized by forbidding label configurations that a r e inconsistent with prior beliefs, such as those defining very small regions, or redundant or blindly ending boundary placements. Forbidden configurations are assigned probability zero. We examine the MAP (marimum a posterion') estimator of boundary placements and partitionings. The forbidden states introduce constraints into the calculation of these configurations. Stochastic relaxation methods are extended to accommodate constrained optimization, and experiments are performed on some texture collages and some natural scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tations related to image segmentation: partition and boundary labels. These, in tum, are special cases of a general "label model" in the form of an "energy functional'' involving two components, one of which expresses the interactions between the labels and the (intensity) data, and the other encodes constraints derived from general information or expectations about label patterns. The labeling we seek is, by definition, the minimum of this energy.</p><p>The partition labels do not classify. Instead they are generic and are assigned to blocks of pixels; the size of the blocks (or label resolution) is variable, and depends on the resolution of the data and the intended interpretations. The boundary labels are just "on"/"off ," and are also of variable resolution, associated with an interpixel sublattice. In both cases, the interaction term incorporates a measure of disparity between certain spatial features of pairs of blocks of pixel gray levels, using the Kolmogorov-Smirnov nonparametric measure of difference between the distributions of these features. Large disparities encourage intervening boundaries and distinct partition labels. The number of model parameters is reduced by forbidding label configurations that are inconsistent with prior expectations, such as those defining very small regions, or redundant or blindly ending boundary placements. These forbidden states introduce constraints into the calculation of the optimal label configuration.</p><p>Both models are applied mainly to the problem of texture segmentation. The data is a gray-level image consisting of textured regions, such as a mosaic of microtextures from the Brodatz album [6], a patch of rug inside plastic, or radar-imaged ice floes in water. It is wellknown that humans perceive "textural" boundaries between regions of approximately the same average brightness because the regions themselves, although containing sharp intensity changes, are perceived as "homogeneous" based on other properties, namely those involving the spatial distribution of intensity values. The goal, then, is to find the (visually) distinct texture regions, either by assigning categorical labels to the pixels, or by constructing a boundary map. Obviously, the problem is more difficult than texture classification, in which we are presented with only one texture from a given list; segmentation may be complicated by an absence of information about the number of textures, or about the size, shape, or number of regions.</p><p>There is no "model" for the individual textures, and hence no capacity for texture synthesis. Our approach is 0162-8828/90/0700-0609$01 .OO 0 1990 IEEE therefore different from those for classification and segmentation in which textures are discriminated based on model parameters which are estimated from specific texture samples. Partitionings and boundary placements are driven by the observed spatial statistics as summarized by selected features. Still, the labeling is not unsupervised because in some cases we use "training samples" to determine feature thresholds for the disparity measures; see Sections I1 and 111.</p><p>Since many visually distinct textures have nearly identical histograms, segmentation must rely on features, or transformations, which go beyond the raw gray levels, involving various spatial statistics. We experimented with several conventional classes of features, in particular the well-known ones based on cooccurrence matrices [ 161, [3 13, but finally adopted a new class based mainly on "directional residuals" and involving third and higher order distributions. However, our viewpoint is exactly that expressed by <ref type="bibr">Zobrist and Thompson [62]</ref>, Triendl and Hendersen <ref type="bibr" target="#b58">[59]</ref>, and others: instead of trying to find exactly the "right" features to convert texture to tone differences, one should find a mechanism for integrating multiple, even redundant, ''cues.'' The label model provides a coherent method for integrating such information and, simultaneously, organizing the label patterns.</p><p>Whereas texture discrimination may be regarded as the detection of discontinuities in surface composition, we also apply the boundary model to the problem of locating sudden changes in depth (occluding boundaries) or shape (surface creases, etc.). The idea is to define contours which are faithful to the 3-D scene but avoid the "nonphysical" edges due to noise, digitization, texture, lighting, etc. In particular, the boundaries should be connected, unique, and reasonably smooth, unlike the typical output of a pure edge detector. Indeed, we formulate boundary detection as a single optimization problem, fusing the detection of edges with their pruning, linking, smoothing, and so on. Obviously, there are discontinuities, such as shadows, which are essentially impossible to distinguish from the occluding and shape boundaries, at least without information from multiple sensors or a rich knowledge base, in which case boundary classification becomes possible.</p><p>Our model enjoys some invariance properties. In particular, due to the use of the Kolmogorov-Smirnov distance, the detected labels are unaffected by linear changes in illumination, and in some cases by all monotone data transformations. Such distortions often result from sensor nonlinearities, digitizers, and film properties. In regard to rotation invariance, the estimated labeling will be independent of the relative orientation of the training samples with the image data to the extent that the features are rotation invariant. The pixel lattice precludes true invariance. However, since our collection of features is basically isotropic, our results are approximately invariant. See <ref type="bibr" target="#b36">[38]</ref> for an approach to rotation-invariant classijication.</p><p>Apparently, some of these ideas have been around for a while. For example, the Kolmogorov-Smirnov statistic is recommended in [51], and reference is made to still earlier papers; more recently, see <ref type="bibr" target="#b59">[60]</ref>. Moreover, the distributional properties of residuals (from surface-fitting) are advocated in <ref type="bibr">[29]</ref>, <ref type="bibr" target="#b46">[48]</ref> for detecting discontinuities. It is certainly our contention that the statistical warehouse is full of useful tools for computer vision.</p><p>Finally, our model may be interpreted in a Bayesian framework as a "prior" joint probability distribution for the array of pixel gray levels and the array of labels. Forbidden configurations are assigned probability zero, and the label estimate is then associated with the MAP (maximum a posteriori) estimate. We shall discuss this interpretation in more detail later on. Suffice to say that, whereas our formulation of the problem and definition of the "best" labeling are formally independent of the stochastic outlook, our optimization procedures are in fact strongly motivated by this viewpoint. In fact, in order to deal with the more difficult textures, the deterministic (' 'zero-temperature") relaxation algorithm we use for most of our experiments must be replaced by a version of stochastic relaxation extended to accommodate constrained sampling and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applications</head><p>Texture is a dominant feature in remotely sensed images and regions cannot be distinguished by methods based solely on shading, such as edge detectors or clustering algorithms. Specifically, for example, one might wish to determine the concentration of ice in synthetic aperture radar images of the ocean, or analyze multispectral satellite data for land use classification. Another application is to wafer inspection: low magnification views of memory arrays appear as highly structured textures, and other geometries have a characteristic, but random, graining. In addition, the use of boundary maps as the input to further processing is ubiquitous in computer vision; for example, algorithms for stereopsis, optical flow, and simple object recognition are often based on matching boundary segments. Other applications include the analysis of medical images, automated navigation, and the detection of roads, faults, field boundaries, etc. in remotely sensed images. (Obviously no generic algorithm provides "off-the-shelf" solutions to real problems; genuine applications require substantial adaptations.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label Model</head><p>Let x = { xs, s E S } and y = { yij, 1 5 i, j I N ] denote, respectively, the labels and the data; thus x, is the label at "site" s E S and yQ is the gray-level at pixel ( i , j ) . The set S of label sites is a regular lattice, distinct from that of the pixels, and typically more sparse; the coarseness depends on the label resolution o. For partitioning, we associate each site s E S with a block of pixels, "sitting below it," if we were to stack the label lattice on top of the pixel lattice. In the boundary model, Y; = .</p><p>pairs of nearby sites in S define boundary segments, and these are associated with pairs of pixel blocks, sitting "across from each other," with respect to the segments (see Fig. <ref type="figure" target="#fig_7">6</ref>). Later, we will define a neighborhood system for S such that the bonding is nearest-neighbor (relative to a ) in the boundary model, whereas in the region model there are interactions at all scales. This has important consequences for the distribution of local minima in the "energy landscape"; see Section 11. Other energy functionals with global interactions can be found in [22], <ref type="bibr" target="#b23">[26]</ref>, and [471.</p><p>The "interaction" between x and y is defined in terms of an enerw function Y, -" 4</p><p>The summation extends over all "neighboring pairs" (or "bonds") ( s, t ), s, t E S ; 9,,t( y) is a measure of the disparity between the two blocks of pixel data associated with the label sites s, t E S . *,,,(x) depends only on the labels x, and x,. In fact, we simply take *,,,(x) = 1x,x, in the boundary model and *,,,(x) = SXs=,, in the partition model. In this way, in the "low energy states," large disparities (a &gt; 0 ) will typically be coupled with an active boundary (x, = x, = 1 ) or with dissimilar region labels (x, # x,) and small disparities (9 &lt; 0) will be coupled with an inactive boundary (x, = 0 or x, = 0) or with equal region labels (x, = x,).</p><p>The interaction between the labels and the data is based on various disparity measures for comparing two (possibly distant) blocks of image data. These measures derive from the raw data as well as from various transformations. We experiment with several transformations y -+ y ' for texture analysis, for example transformations of the form where Eai = 1 and { tj } are pixels nearby to pixel t in the same row, column, or diagonal. We call these "directional residuals," regarding Eaj y,, as a "predictor" of y,.</p><p>We have also investigated raw gray levels ( y ' = y), other features based on local measures of gray-level range, and isotropic versions of ( 1 . 1 ) . In any case, disparity is measured by the Kolmogorov-Smirnov statistic (or distance), a common tool in nonparametric statistics which has desirable invariance properties. (In particular, using the directional residuals, the disparity measure is invariant to linear distortions ( y o -, ayi/ + b ) of the raw data, and using the raw data itself for comparisons, the disparity measure is invariant to all monotone (data) transformations.) The general form of the 9 term is then where 4 is monotone increasing, p denotes a distance based on the Kolmogorov-Smirnov statistic (see Section 11), and yif;, yiij) are the data in the two blocks associated with ( s, t ) for the ith transform. Often, we simply take m = 1 a n d y ( " = y.</p><p>The other component in the model is a penalty function V(x) which counts the number of "taboo patterns" in x; states x for which V ( x ) &gt; 0 are "forbidden." For example, boundary maps are penalized for dead-ends, "clutter," density, etc. whereas partitions are penalized for too many transitions or regions which are "too small. "</p><p>Given the observed image y, our estimate i? = a( y ) is then any solution to the constrained optimization problem minimize,: v ( x ) = U ( x , y ) .</p><p>(1.2)</p><p>We seek to minimize the energy of the data-label interaction over all possible nonforbidden label states x.</p><p>The rationale for constrained optimization is that our expectations about certain types of labels are quite precise and rigid. For example, most "physical boundaries" are smooth, persistent, and well-localized; consequently it is reasonable to impose these assumptions on image boundaries, and corresponding restrictions on partition geometries. Contrast this with other inference problems, for example restoring an image degraded by blur and noise. Aside from contraints derived from scene-specific knowledge, the only reasonable generic constraints might be ''piecewise continuity, " and generally the degree of ambiguity favors more flexible constraints, such as those in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U , or in the energy functions used in [20] and [22].</head><p>There are no multiplicative parameters in the model, such as the "smoothing" or "weighting" parameters in <ref type="bibr" target="#b18">[20]</ref>, and [22]; in effect, the energy is U + X V with X = 00. Thresholds must be selected for the disparity measures; this may be done "in the blind," but the performance of the algorithm is certainly increased if these choices are data-driven, either from prior experience with the types of textures, or by extracting training samples, as we have done here (see Section V). Fortunately, the algorithm is not unduly sensitive to these choices within a certain range of values. Other inputs include the label resolution, block sizes, and penalty patterns. The algorithm is robust against these choices provided that modest information is available about the pixel resolution. The selection of penalty patterns is nearly selfevident: as already noted, our expectations about the geometry of regions and boundaries are very concrete and easily encoded in local constraints.</p><formula xml:id="formula_0">[3], [17], [19],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Constrained Stochastic Relaxation</head><p>The search for i , the optimal labeling, is based on a version of stochastic relaxation which incorporates hard constraints. The theoretical foundations are laid out in [ 181, although there is enough information provided here (see Section IV) to keep this paper self-contained. (See also Grenander [28] and Linsker <ref type="bibr" target="#b39">[41]</ref> for similar modifications of stochastic relaxation.) We employ two algorithms, both of which are approximations to precise computational procedures, in which convergence is at least theoretically guaranteed, if not always realized in practice.</p><p>Introduce a control parameter t corresponding to "temperature" and another control parameter X, corresponding to a Lagrange multiplier for the constraint V = 0. Let where y (the data) is fixed, t k L 0, and X k 7 03. One Monte Carlo sampling from the Gibbs measures with energy functions U,. If tk is fixed, then under suitable conditions on the growth of X k , the sequence xk converges to a sample from the Gibbs distribution with energy U , but restricted to V = 0, i.e., constrained to assign probability zero to the forbidden configurations. We refer to this as "constrained stochastic relaxation. " On the other hand, if Xk 7 03 and tk L 0, both at suitably (coupled) rates (see Section IV), then the sequence converges to a solution of (1.2); this is "constrained simulated annealing. "</p><p>The first algorithm we use is deterministic, with the temperature fixed at zero, and is essentially the ICM ("iterated conditional mode") algorithm of Besag [2], although X = hk 7 a. The convergence is rapid and the performance is sufficient for all but the most difficult images. The second algorithm is constrained stochastic relaxation with the temperature fixed at some "small" value (but, again, X = hk 7 03); the idea is simply that the very likely states of the constrained Gibbs distribution should be "close" to the mode, i.e., f . This "low temperature sampling' ' is more computationally demanding than ICM, but invariably arrives at a better labeling; see Section V on experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Related Work</head><p>The subject of edge detection is very active; however, as we have already mentioned, the raw output of standard edge detectors, for example those based on differential operators, would generally require considerable "processing" to produce unique, smooth, persistent boundaries. The work in "regularization theory" [44], [49], as well as our earlier paper [20], is different in that "edge" or "line" process is employed there as a device for suspending continuity constraints in the context of reconstructing an image from sparse data, restoring an image corrupted by blur and noise, and similar tasks not aimed at boundary detection per se.</p><p>In contrast, there is much related work in the area of texture segmentation. In Triendl <ref type="bibr" target="#b56">[58]</ref> and Triendl and  Thompson [59], local property values referred to as "texture parameters" are computed in a neighborhood of each pixel and edge maps are computed for several principal components of these feature images. These maps are integrated across components, and then spatially, resulting in a final boundary map. Similarly, in Thompson [57] (see also <ref type="bibr">Zobrist and Thompson [62]</ref>), the idea is to integrate multiple cues into a single "distance function" which is a linear combination of elementary measures, and which generates a sequence of states f k , k = 1 , 2, * --3 by is intended to measure the overall similarity between two regions. Edges are then detected using the Robert's cross gradient applied to this distance; there is no effort to organize the responses. Our experiments compare favorably to those in [57] and [59], both of which utilize collages of natural textures similar to ours. Local filtering is also the focus of Laws' approach [40], although the mechanism for combining cues is entirely different from ours.</p><p>Among the statistical approaches to texture segmentation are those in which the image data is regarded as a realization of a two-dimensional stochastic process, or random field. In Simchony and Chellappa <ref type="bibr" target="#b51">[53]</ref> and in Derin and Cole [12], the image is modeled as a two-layered Markov random field: the "upper level" is the region process, a simple Ising-type process, and the "bottom level" is the observed intensity process, with some distribution (e.g., Gaussian) conditional on the regions. Both papers employ stochastic relaxation to search for the MAP and related estimators. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Stochastic Formulation</head><p>This work is an extension, incorporating hard constraints, of a Bayesian paradigm in which prior probability models are constructed for both the observed and unobserved scene attributes. The degradation process is modeled as a conditional probability distribution, and the estimates are obtained by a form of Monte Carlo sampling. This framework has been applied by numerous researchers, including applications to image restoration To facilitate placing our model in the general framework, let us write x = ( x L , x p ) , where x L is the vector of boundary or region labels, and x p is the vector of pixel gray levels. These are the relevant image attributes for the problem at hand. The prior distribution ll is a probability for x: 0 I n(x) I 1 , C , I I ( x ) = 1 , where C, is the summation over all configurations of x (all assignments of gray levels and boundary placements, for example). Adopting the Gibbs representation in the unconstrained case we would represent II as 1</p><formula xml:id="formula_1">I n ( x ) = -exp { - i = C e x p { - X</formula><p>The real-valued function U is called the energy, and evidently determines II. In our case we separate the prior energy into a pixel-label interaction term and a pure label contribution. The former, U ( x L , x p ), promotes placements of boundaries, or assignments of distinct labels, between regions in the image that demonstrate distinct spatial patterns. The pure label contribution is to inhibit "blind" endings of boundaries, redundant boundary representations, excessively small or thin regions, and other unexpected label configurations. Rather than inhibiting these configurations with high energy via large parameters, we have found it convenient, especially when working with boundaries and partitionings, to extend this framework, by allowing ''infinite energies" (zero probabilities) in the prior distribution. (See Moussouris <ref type="bibr" target="#b44">[46]</ref> for an analysis of Gibbs measures with "forbidden states. ") We forbid these configurations by introducing V = V ( x L ) 2 0 and concentrating on { x : V ( x L ) = O}. The prior, then, is of the form The constraint, V ( x L ) = 0, amounts to a placement of infinite energy barriers in the "energy landscape. " These inhibit the free flow that is essential to the good performance of stochastic relaxation; indeed the theory will1 in general break down, and convergence is no longer guaranteed. As indicated above, a simple and effective solu- tion is to introduce the barriers gradually during the relaxation process. Again, the supporting convergence theory is spelled out in <ref type="bibr">[ 181.</ref> There is usually a problem-specific degradation that precludes directly observing x. It may be the blur and noise introduced by the camera and recording devices, the attenuated Radon transform that figures into emission tomography, or, as in this case, an "occlusion": we assume the gray levels x p are observed (uncorrupted) whereas the labels x L are of course unobserved. Thus the data is y = x p and our only interest is in estimating the unobserved label process xL. II ( y I x ) is singular, and the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>posterior distribution for x L given the data is</head><p>There is skepticism about the MAP estimator: see, e.g., <ref type="bibr" target="#b19">[2]</ref>, [15], <ref type="bibr" target="#b42">[44]</ref>. It has sometimes been found to be too "global," leading to gross mislabeling in certain classification problems and "over-smoothing" in surface reconstruction and image restoration. (See [7] for a different view.) The discussion paper of Besag <ref type="bibr" target="#b19">[2]</ref> has shed much light on the subject; see especially the remarks of Silverman [52] on MAP versus simulations from the posterior n(x I y), and the remarkable comparisons between the exact MAP estimate and approximations derived from simulated annealing in the commentary of Greig, Porteous, and Seheult <ref type="bibr" target="#b24">[27]</ref>. However, pixel-based error measures are too local for boundary analysis. In particular, the Bayes rule based on misclassification error rate, namely the marginal (individual component) modes of n(x I y ) , is unsuitable because this estimator lacks the fine structure we expect of boundary maps; placement decisions cannot be based on the data alone-pending labels (i.e., context) must be considered. See <ref type="bibr" target="#b42">[44]</ref>, [50], and <ref type="bibr" target="#b60">[61]</ref> for discussions of alternative loss functions and performance criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">PARTITION MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Partitionings</head><p>Denote the pixel (image) lattice { ( i , j ): 1 I i, j I N } by SI and let S, (formally S in Section I) be the label lattice, just a copy of SI in the case of the partition model. For each experiment, a resolution o is chosen, which determines a sublattice Sfp' C s, , and the coarseness of the partitioning. Larger 0's will correspond to coarser partitionings and give more reliable results (see Section V), but they lose boundary detail. Specifically, let Recall that the observation process, or data, consists of gray levels y,, s E SI. With the usual gray-level discretization, the state, or configuration, space for the data is a,= { { y s } : s ~S r , 0 _ ( y , ~2 <ref type="bibr">5 5 )</ref> The configuration space for the partitioning x is determined by U and by a maximum number of allowed regions P : a y ' = { { x , ] : s E sp, 0 5 x, I P -1 ) .</p><p>Recall that the labels are generic: x defines a partitioning by identifying sites with a given label ( 0 , 1, * . , p -1 ) as belonging to the same region. Only the sublattice Sfp) is labeled, and a maximum number of labels (regions) is fixed a priori. A prior estimate of the number of distinct (but not necessarily connected) regions must be available, since the model often subdivides homogeneous regions when P is too large (see Section V). The boundary model (Section 111) is more robust in this regard. Each label site s E S, is associated with a square block D, E SI of pixel sites centered at s. (Recall that S, is just a copy of S,; we sometimes use "s" ambiguously to reference a site in S, und the corresponding site in s/.) x, labels the pixels in 0,: { { yr }: r E D, } . As we will see shortly, the partitioning is based on the spatial statistics of these (overlapping) subimages. The size of D, is therefore important. We have experimented only with textures (the boundary model has been applied more generally), and is obvious that for these the pixel blocks { D, )seSy must be large enough to capture the characteristic pattern of the texture, at least in comparison to the other textures present. Of course, "large enough" is with respect to the features used, but in the absence of a multiscale analysis, an a priori choice of scale is unavoidable. In all of our experiments, 1 D, 1 = 441, a 21 x 21 square block of pixels. There is again a resolution issue: larger blocks characterize the textures more reliably, having less within-region variation, but boundary detail is sacrificed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label-Data Interaction</head><p>We establish a neighborhood system on the label lattice Sp': each s E Sp' is associated with a set of neighbors N, C Sp'. The system is symmetric, meaning that s E N , e r E N , . As we shall see, the neighborhood system largely determines the computational burden. For now we will proceed as though the neighborhood system is given, but we will have much more to say about it shortly. Let ( s , t ) , denote a neighbor pair, meaning s, t E Sfp', s E N,. We will introduce a disparity measure a,,, = a,,,( y ) for each neighbor pair ( s, t ),. Roughly speaking, +,,, measures the similarity between the pixel gray levels in the two pixel blocks associated with s, t E Sp'.</p><p>For the partition model, + , , t is simply -1 ("similar") or + 1 ("dissimilar"); it is more complicated for the boundary model. The interaction energy is then where 6{ , ,</p><p>= 1 if x, = x,, and 0 otherwise. In the low energy states, similar (resp. dissimilar) pairs, a,,, = -1 (resp. a,,, = + l ) , are associated with identical (resp. distinct) labels: x, = x, (resp. x, # x,). Although U ( x , y ) is conceived of as the interaction term in a prior distribution that is jointly on x and y, only the posterior distribution is actually used, and y is fixed by observation.</p><p>It would be interesting, and perhaps instructive (see <ref type="bibr" target="#b33">[35]</ref>), to sample from the joint distribution, but computationally very expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Neighborhood System</head><p>A simple example will serve to highlight the issues.</p><p>Suppose y has R constant gray-level (untextured) regions ( y , E ( 0 , 1 , . . * R -l}, s E S I ) , and u = 1 (full resolution). Of course, in this case y is a labeling, so there is no point in bringing in the partition process x; but this is just an illustrative example. The obvious disparity measure is simply @, , , = -l if y, = y,, and + l otherwise:</p><p>Entertain, for the time being, a nearest neighbor system on S,, which is the natural choice. To be concrete, take N, to be the four (two horizontal and two vertical) nearest neighbors of s. There are three essential difficulties with this choice of neighborhood system. Two can be readily appreciated:</p><p>(See Fig. <ref type="figure">1</ref> . ) If R = 2 and P = 3 , and if region "0" (i.e., { s E SI: y, = 0 } ) is split into two disjoint pieces by region "1" (i.e., {s E SI: y, = l } ) , then (2.1) has 0 0 0 1 1 0 0 1 1 1 2 2 1 1 2 2 2 1 1 0 0 0 0 0 1 1 0 0 1 1 1 2 2 1 1 2 2 2 1 1 0 0 0 0 1 1 1 0 0 1 1 2 2 2 1 1 2 2 1 1 1 0 0 0 0 1 1 0 0 0 1 1 2 2 1 1 1 2 2 1 1 0 0 0 0 1 1 0 0 0 0 1 2 2 1 1 1 1 2 1 1 0 0 0 0 0 1 1 0 0 0 0 1 2 2 1 1 1 1 2 1 1 0 0 0 0 1 1 1 0 0 0 0 2 2 2 1 1 1 1 1 1 1 0 0 0 0 Fig. <ref type="figure">1</ref> . Left: original "image" and a correct labeling. Middle: a correct labeling. Right: spurious labeling. 0 0 0 1 1 2 2 1 1 1 2 2 0 0 0 0 0 1 1 0 0 0 0 0 1 1 2 2 1 1 1 2 2 0 0 0 0 0 1 1 0 0 0 0 1 1 1 2 2 1 1 2 2 2 0 0 0 0 1 1 1 0 0 0 0 1 1 2 2 2 1 1 2 2 0 0 0 0 0 1 1 0 0 0 0 1 1 2 2 2 2 1 2 2 0 0 0 0 0 1 1 0 0 0 0 0 1 1 2 2 2 2 1 2 2 0 0 0 0 0 1 1 0 0 0 0 1 1 1 2 2 2 2 2 2 2 0 0 0 0 1 1 1 0 0 0 0 Fig. <ref type="figure">2</ref>. Left: original "image" and a correct labeling. Middle: a correct labeling. Right: spurious labeling.</p><p>two kinds of global minima: correct labelings, in which there are two populations of labels corresponding to the two gray-level regions; and spurious labelings, in which the three regions (two of type "0" and one of type "1") are given three distinct labels.</p><p>(See Fig. <ref type="figure">2</ref>.) If R = 3, and region "0" does not neighbor region "2", then there are again two kinds of global minima: correct labelings have three labels; spurious labelings have only two, incorrectly identifying regions "0" and "2".</p><p>Quite obviously, the model requires more global interactions. In particular, just a few long range interactions would distinguish the correct from the spurious labelings. Only a correct labeling would achieve the global minimum of U in these two examples.</p><p>The third difficulty with local neighborhoods is computational, and is already apparent when R = 1 and P = 2. This time there are only two global minima, and each is a desirable labeling ( { x, = 0 V, E S, } or { x, = 1 Vs E S, } ) . But, with N = 5 12, for example, consider the label configuration in which xu = 0 whenever 1 I i I 256 and xu = 1 whenever 257 I i I 512, a half "black" and half "white" picture. This is a local minimum, and rather severe in that it would take very many "uphill" or "flat" moves (single site changes) to arrive at either of the global minima. SR is a local relaxation algorithm, and despite the various convergence theorems, the practical fact of the matter is that "wide" local minima such as these are impossible to cope with. (But, there are some encouraging results in the direction of multiscale relaxation, see [24], <ref type="bibr" target="#b4">[ 5 ]</ref> , [54].) Many readers will be reminded of the Ising model, in the absence of an external field, and the notorious difficulty of finding its (two) global minima by Monte Carlo relaxation. In fact, the R = 1 , P = 2 energy landscape is identical to that of the Ising model, as is readily demonstrated by a suitable transformation of the label variables. (Indeed, the same goes for the R = 2, P = 2 case, although this is less obvious. A suitable transformation identifies the two Ising minima with the two acceptable labelings: x, = 0 e y, = 0 and x, = 1 e y, = 0 . ) These local minima can be mostly eliminated by introducing long range interactions in the label lattice Sp'; the same remedy as for the label ambiguities. We will provide a heuristic argument for the important role of long range interactions in creating a favorable energy landscape. In any case, simulations firmly establish their utility. First recall that the distance between two sites in a graph is the smallest number of edges that must be crossed in traveling from one site to the other. Notice that in the four nearest neighbor graph (two-dimensional lattice) the average distance between sites is large. Correct partitioning requires all pairs of label sites to resolve their relationships ("same" or "different"), as dictated by the statistics of their associated pixel blocks. Of course most pairs are not neighbors. With a local relaxation, such as SR, the resolution is achieved by propagating relationships through intervening sites. Thus the task is facilitated by minimizing the number of intervening sites, and a relatively small number of long range connections can drastically reduce the typical number of these.</p><p>The largest distance over all pairs of sites is the diameter of a graph. In an appropriate limiting (large graph) sense, random graphs have minimum diameter among all graphs of fixed degree.' In light of our heuristics, this suggests a random neighborhood system for Sp'. Indeed, random neighborhoods have a remarkable effect on the structure of local minima for these systems. In a series of experiments, with "perfect" disparity data (such as the (T = 1 gray-level problems discussed above) we could always achieve the global minimum by single-site iterative improvement when adopting a random graph neighborhood configuration, using rather modest degrees for large graphs. We conjecture, but have been unable to prove, that even with the degree a vanishingly small fraction of the graph size, random graphs (in the "large graph limit") have no local minima, under the Ising potential or the potential U ( x , y ) with perfect disparity data <ref type="bibr">(2. I )</ref> .</p><p>Of course the disparity data is not usually perfect. In challenging texture discrimination tasks there will be pixel blocks from the same texture that are measured as dissimilar (a,, = 1 ) and others from distinct textures that are measured as similar ( = -1 ). Under these circumstances it helps to also have near neighbor interactions, since these tend to bond neighboring label sites and thereby increase the effective number of long range interactions per site. Although near neighbors were not always needed to get the best results, we settled on using four near neighbors, and sixteen random neighbors per site, in each of our experiments (see Section V). By "near neighbors" we mean the closest two horizontal and two vertical neighbors whose associated pixel blocks do not overlap. For example, with 0 = 7, and using 21 x 21 blocks, the near neighbors have two intervening sites in Sp). Details on the generation of the (pseudo) random neighbors can be found in <ref type="bibr">[25]</ref>. Overall, perhaps the most effective ' A graph hasfired degree if each site has the same number of neighbors. The degree is then the number of neighbors per site.</p><p>neighborhood system would have a gradual fall-off of interaction densities with distance, a system with an equal number of neighbors at each Manhattan distance, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Kolmogorov-Smirnov Statistic</head><p>At the heart of the partitioning and boundary algorithms is a disparity measure ay,,. Recall that i f s , tare neighbors in Sp'( (s, t ) , ) then a,,/ is a measure of disparity between two corresponding blocks of pixel data, { { y , } : r E D,} and { { y r } : r E D , } in the case of the partition model. We base as,, on the Kolmogorov-Smirnov distance, a measure of separation between two probability distributions, well-known in statistics. When applied to the sample distributions (i.e., histograms) for two sets of data, say For our purposes, the data U ( ' ) and d 2 ) consist of either the (raw) gray levels, or (in most cases of texture discrimination) transformations of these, restricted to blocks of pixels; these blocks are adjacent in the boundary model, but may be well separated in the partition model (recall that we employ a largely random topology). In either case, the assumptions made in statistical testing are generally violated: it may be unreasonable to assume that the gray levels in a block of pixels represent independent and identically distributed observations from some underlying probability distribution (although this is occasionally done). Of course, the size of the blocks relative to the image structures is very important. The blocks may contain hundreds of pixels, but if they are still small relative to the image structures, then the formal assumption will be more nearly satisfied. At any rate, the formal theory is primarily motivational. The distance (2.2) is an effective "measure of homogeneity" which is invariant to pointwise (monotone) data transformations induced by lighting and other factors.</p><formula xml:id="formula_3">U ( ' ) = { U \ ' ) , v i ' ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Disparity Measures</head><p>Sometimes, just gray level histograms are enough for good partitionings, as with the SAR image of water and ice (see Section V). In these cases, disparity is measured as follows. Recall that D , , s E Sp), is a square block of pixel sites (always 21 x 21 in the partitioning experiments) centered at s. Let y ( D , ) = { yr: r E D , } . Given (possibly distant) neighbors s, t ESP), we define @ 5 , r using the Kolmogorov-Smimov statistic and a threshold c: In other words, @,,r is 1 or -1 depending on whether the Kolmogorov-Smimov statistic is above threshold or not.</p><p>Of course, many distinct textures have nearly identical histograms (see [21] for some experiments with partitioning and classification of such textures, also in the Bayesian framework). In these cases, discrimination will rely on features, or transformations, that go beyond raw gray levels, involving various spatial statistics. We use several of these at once, defining @, , , to be 1 if the Kolmogorov-Smimov statistic associated with any of these transformations exceeds a transformation-specific threshold, and -1 otherwise. The philosophy is simple: if enough transformations are employed, then two distinct textures will differ significantly in at least one of the aspects represented by the transformations. Unfortunately, the implementation of this idea is complicated; more transformations mean more thresholds to adjust, and more possibilities for "false alarms" based on "normal" variations within homogeneous regions.</p><p>Proceeding more formally, let A denote one such data transformation and put y' = A ( y ) , the transformed image. In general, y: is a function of both y , and the graylevels in a window centered at t E s,. For example, y: might be the mean, range, or variance of y in a neighborhood of t , or a measure of the local "energy" or "entropy." Or, y,! might be a directional residual defined in (1.1); isotropic residuals, in which the pixels { t , } surround t , are also effective. Notice that any A given by (1.1) is linear in the sense that if y , + ay, + b, Vs E SI then y: --* 1 a I y: , Vs E SI, and recall that the Kolmogorov-Smimov statistic is invariant with respect to such changes. This invariance is shared by other features, such as the mean, variance, and range. It should also be noted that these transforms are decidedly multivariate, depending (statistically) on the marginal distributions of the data of at least dimension three. Many approaches to texture analysis are based solely on the one-or two-dimensional marginals, i.e., the gray-level histogram and cooccurrence matrices. We were not able to reliably detect some of the boundaries between the Brodatz microtextures with these standard features. Perhaps the jury is still out.</p><p>Given a family of transformations, A , , A*, * --, A,, we define @s,r = max [2~(d(yI"(D,),y")(Dt)) &gt; c , ) ( y ( ' ) )l ]</p><formula xml:id="formula_4">1 &lt; i s m (2.3)</formula><p>where y'" = A, ( y ) , 1 I i 5 m, and y ( ' ) ( D , ) = {y!'), s E D , } , r = s, t . The thresholds c1, * * , c, are chosen to limit the percentage of "false alarms" (cases of exceeding threshold for pairs of blocks within the same texture); see Section V.</p><p>The disparity measure (2.3) inherits the aforementioned invariance to linear shifts for many transforms, including all "differences of averages. " More importantly, perhaps, imagine we are comparing two pairs of image blocks, each pair in a different region of the image. Then, roughly speaking, the two distances are automatically calibrated, regardless of the differing statistical properties of the two regions; i.e., the disparity measure has the same interpretation anywhere in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F, Penalties</head><p>Recall that V(x) counts the total number of "penalties'' associated with x E np9'). There are two kinds of "forbidden" configurations that give rise to penalties: roughly, these correspond to very small regions and very narrow regions. Fix r~ and s E Sp). Let E, be the 5 x 5 block of sites in Sp) centered at s. A configuration x is "small at s E SF)" if fewer than nine labels in { x,: t E E,} agree with x,. Notice that a right comer at s is allowed; there are exactly nine agreements. The total number of penalties for "small regions" is</p><formula xml:id="formula_5">(2.4)</formula><p>Obviously, the numbers "5" and "9", as well as other penalty parameters below, are quite arbitrary, and could reasonably be scale-dependent.</p><p>As for "thin regions," these are regions that have a horizontal or vertical "neck" that is only one label-site wide (at resolution a). Let 7 h be a one-site horizontal translation within Sp), and let 7, be the analogous vertical translation. Penalties arise when either { xSpr,, # x, and x, # x, + T,, } or { x,r l , # x, and x, # x, + r l , } . The number of ' 'thin-region'' penalties is therefore where V ( x ) is the number of penalties in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">BOUNDARY MODEL A . Boundary Maps</head><p>The pixel lattice is again SI. Let S, denote another regular lattice interspersed among the pixels (see Fig. <ref type="figure">3</ref>) and of dimension ( N -1 ) X ( N -1 ); these are the "boundary sites." We will associate s = ( i , j ) E S, with the pixel ( i , j ) E SI to the upper left of s.</p><p>Given y , a gray-level image, we wish to assign values to the boundary variables x = { x,, s E S , } , where x, = 1 (resp. 0) indicates the presence (resp. absence) of a boundary at site s E S,. We have already discussed the corresponding interpretation of the boundary map x = x ( y ) in terms of physical discontinuities in the underlying three-dimensional scene. We establish a boundary resolution or grid size a 2 1, analogous to the resolution used earlier for the partition model. Let S g ) C SB denote the sublattice { ( i u + 1 , j u + 1 ) : 1 5 i , j 5 ( N -2 ) / u } .</p><p>Only the variables x,, s E Sfp', interact directly with the data; the remaining variables x,, s E S, \ Sg), are determined by those on the "grid" Sfp'. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boundary-Data Interaction</head><p>Let ( s, t ),, s, t E Sfp' denote a nearest-neighbor pair relativetothegrid. Thus,s = ( i u + l , j a + 1 ) , t = ( k a + 1, la + 1 ) is such a (horizontal or vertical) pair if either i = k a n d j = I ? 1, o r j = I and i = k +_ 1 . We identify ( s, t ), with the elementary boundary segment consisting of the horizontal or vertical string of u + 1 sites (in S,) including s, t and the u -1 sites "in between."</p><p>The energy function U ( x , y ) should promote boundary maps x which are faithful to the data y in the sense that "large" values of As,,( y ) are associated with "on" segments (x,x, = I ) and "small" values with "off" segments (x,x, = 0). There are no a priori constraints on x at this point; in fact, because of digitization effects, textures, and so-on, the energy U will typically favor maps x with undesirable deadends, multiple representations, high curvature, etc. These will be penalized later on. A simple choice for the x / y interaction is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>u ( x , Y &gt; = ( 1 -xsxr)$(A,.,(Y)&gt; (3.1) (S,f)O</head><p>where the summation extends over all nearest-neighbor pairs (s, t ) , ; the "weighting function" +(x), x 1 0, will be described presently. The energy in (3. l ) , which is similar to a "spin-glass" in statistical mechanics, is a variation of the ones we used in our previous work [19], <ref type="bibr" target="#b18">[20]</ref>; when a = 1 , the variable xsxr corresponds directly to the "edge" or "line" variables in <ref type="bibr" target="#b18">[20]</ref> and <ref type="bibr" target="#b45">[47]</ref>. Since y is given, the term 1 -x , ~, can be replaced by -x,xr with no change in the resulting boundary interpretation. By contrast, in [20] we were concerned with image restoration and regarded both x and y as unobservable; the data then consists of some transformation of y , involving for example blur and noise. In that case, or in conceiving U as defining a prior distribution over both y and x, the bond between the associated pixels should be broken when the edge is active, i.e., 1 x,x, = 0. The term 1x,xt is exactly analogous to the "controlled-continuity functions" in [56]. See also <ref type="bibr" target="#b33">[35]</ref> for experiments involving simulations from a related Markov random field model; the resulting "fantasies" ( y , x ) are generated by stochastic relaxation and yield insight into the nature of these layered Markov models.</p><p>Returning to (3. I), a little reflection shows that 4 should be increasing, with 4 ( 0 ) &lt; 0 &lt; $ ( + m ) ; otherwise, if $ were never negative, the energy would always be minimized with x, 3 1. The intercept d* = $-'(O) is critical; values of A above (resp. below) d* will promote (resp. inhibit) boundary formation. The influence of this threshold is reduced by choosing 4' ( d " ) = 0. We employ the simple quadratic Notice that if we select (Y = max A,y,, then the maximum "penalty" (+ ( 0 ) = -1 ) and "reward" ( + ( a ) = 1 ) are balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Disparity Measures</head><p>We employ one type of measure for depth and shape boundaries and another for the texture experiments. In the former case, the disparity measure involves the (raw) gray-levels only, whereas for texture discrimination we also consider data transforms based on the directional residuals (1.1). Except when U = 1, the data sets are compared by the Kolmogorov-Smimov distance.</p><p>The disparity measure should gauge the intensity "flux," As,, = A$P,'(y) 2 0 , across ( s , t ) , , i.e., orthogonal to the associated segment. At the highest resolution ( U = l ) , the measure 1 ys, -y,, 1 (where s*, t* are the two pixels associated with the boundary sites s, t-see Fig. <ref type="figure" target="#fig_9">5</ref>) can be effective for simple scenes but necessitates a single differential threshold: differences above d* (resp. below d* ) promote (resp. inhibit) boundary formation. Typically, however, this measure will fluctuate considerably over the image, complicating the selection of d*.</p><p>(Such is the case, e.g., for the "cart" scene, see Section V .) Moreover, this measure lacks any invariance properties, as will be explained below. A more effective measure is one of the form where the sum extends over parallel edges ( si, ti ) in the immediate vicinity of ( s, t ). Thus the difference I ys.y,, 1 is "modulated" by adjacent, competing differences.</p><p>The result is a spatially varying threshold and the distribution of As,t( y ) across the image is less v@able than that of 1 ys, -y,* I. Choosing y = const. x A , where A is the mean (raw) absolute intensity difference over all (vertical and horizontal) bonds, renders As,, ( y ) invariant to linear transformations of the data; that is, <ref type="figure" target="#fig_22">A s ,</ref><ref type="figure">,</ref><ref type="figure" target="#fig_22">( y ) =   A s ,</ref><ref type="figure">,</ref><ref type="figure" target="#fig_22">( a</ref>  At lower resolution, let D,, and D,, denote two adjacent blocks of pixels, of equal size and shape. An example is illustrated in Fig. <ref type="figure" target="#fig_7">6</ref> for the case of two square blocks of size 52 = 25 pixels which straddle a vertical boundary segment with U = 3. Let y ( D , ) = { y s , s E D , } , r = s*, t*, be the corresponding gray-levels and set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As.,(Y&gt; = d ( Y ( D S * ) &gt; Y(D,*))3 ( 3 . 4 )</head><p>where d is the Kolmogorov-Smirnov distance discussed in Section 11. This is the disparity measure used for the House and Ice Floe scenes (see Section V). One difficulty with (3.4) is that the distance between two nonoverlapping histograms is the maximum value, namely 1, regardless of the amount of separation. Thus, two constant regions differing by a single gray-level are as "far apart" as two differing by 255 levels. Thus, it is occasionally necessary to "desensitize" (3.4) for example by "smearing" the data or perhaps adding some noise to it; see Section V .</p><p>Raw gray-level data is generally not satisfactory for discriminating textures. Instead, as discussed in Section 11, we base the disparity measure on several data transformations, involving higher order spatial statistics, such as the directional residuals defined in (1.1). Given a family A,, A2, * * . , A, of these transforms (see Section II), a resolution U, and blocks D,,, D,* as above, define <ref type="figure" target="#fig_22">A ,</ref><ref type="figure">,</ref><ref type="figure">,</ref><ref type="figure" target="#fig_22">( y ) = max [ c ;</ref><ref type="figure">' d ( y ' " ( D ,</ref><ref type="figure">. ) ,</ref><ref type="figure">Y ( ~) ( D ~</ref> Finally, we note that (3.5) has the same desirable invariance properties as the measure constructed for partitioning (Section 11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I s i s m</head><p>D . Penalties V ( x ) again denotes the total number of "penalties" associated with x E ap). These penalties are simply local binary pattems over subsets of S g ) . Fig. <ref type="figure">7</ref> illustrates a family of four such pattems; they can be associated with any resolution U by the obvious scaling. These correspond, respectively, to an isolated or abandoned segment, sharp turn, quadruple junction, and "small" structure. Depending on U, the pixel resolution, and scene information, we may or may not wish to include the latter three. For example, including the last one with U = 6 would prohibit detection of a square structure of pixel size 6 x 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Summary</head><p>We are given 1) a gray-level image y = { y u } ;</p><p>2) a resolution level U = 1, 2, * * . ;</p><p>3) a disparity measure @,,t( y ) = + ( As,t( y ) ) for each neighbor pair ( s, t &gt;, in the sublattice S g ) ; 4) a collection of penalty patterns. The (MAP) boundary estimate i = i ( y ) is any solution</p><formula xml:id="formula_6">x E Qg) of the constrained optimization minimizex:v(x)=o (1 -xsxt&gt;+(As,r&lt;Y&gt;) ( s , r ) ,</formula><p>where V(x) is the number of penalties in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ALGORITHMS</head><p>We begin with an abstract formulation of the optimization and sampling problems outlined in Sections 1-111. Recall that our search algorithms are motivated by the stochastic formulation of the problem, in particular the role of the posterior distribution n (x I y ). However, y is fixed by observation, and can be ignored in this discussion of computational issues. Thus, we are given two functions U and V on a space of configurations 52 = { (xs,, xs2, . . . <ref type="figure">,</ref><ref type="figure">x ,</ref><ref type="figure">,</ref><ref type="figure">) : x,</ref><ref type="figure">,</ref><ref type="figure" target="#fig_22">E A</ref> </p><formula xml:id="formula_7">Q* = {x: ~( x ) = U } v = min ~( x ) X exp { -w } I I * ( x ) = &amp;*(x) C exp { -~(xj)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X ' € i 2 *</head><p>We wish to solve the constrained optimization problem minimize { U ( x ) : V ( x ) = U } or to sample from the Gibbs distribution IT*. (Recall that sampling at low temperature allows us to approximate the global minimum or to entertain other estimates, for instance the posterior mean.)</p><p>We have studied [20] Monte Carlo site-replacement algorithms for the unconstrained versions of these problems: stochastic relaxation (SR) for sampling, and stochastic relaxation with simulated annealing (SA) for optimization. SA was devised in [8] and [39] for minimizing a "cost functional" U (e.g., the tour length for the traveling salesman problem) by regarding U as the energy of a physical system and simulating the dynamics of chemical annealing. The effect is to drive the system towards the "ground states," i.e., the minimizers of U . This is accomplished by applying the Metropolis (relaxation) algorithm to the Boltzmann distribution</p><formula xml:id="formula_8">(4.1 )</formula><p>at successively lower values of the "temperature" t.</p><p>We presented two theorems in 1201: one for generating a sequence { X ( k ) } which converges in distribution to (4.1) for t = 1 (SR), and one for generating a sequence { X ( k ) } having asymptotic distribution the uniform measure over Q, = (x E Q: U ( x ) = U } , U = minx U ( x ) , (SR with SA). The essence of the latter algorithm is a "cooling schedule" t = t,, t2, --* for establishing conver-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>gence. SA has been extensively studied recently [4], [lo],</head><p>[23], 1241, <ref type="bibr" target="#b28">[30]</ref>, 1321, 1341, [ 5 5 ] ; see also the comprehen- sive review [l] and the references therein.</p><p>Results concerning constrained SR and SA are reported in [18], which was motivated by a desire to find a theoretical foundation for the algorithms used here. We have deviated from the instructions in 1181, with regard to the cooling schedule, but at least we know that the algorithms represent approximations to rigorous results.</p><p>Both algorithms produce a Markov chain on Q by sampling from the low-order, marginal conditional distributions of the free Gibbs measures exp </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{ -t -' ( U ( x )</head><formula xml:id="formula_9">+ A V ( ~) ) } C X ' exp { -t -' ( U ( x ' ) + A V ( ~' ) ) ) ' n(x; t , A) = It is easy to check that lim n(x; 1, A) = IT*(x) X + W (4.2)</formula><p>and that lim n(x; t , A) = (4.3) However, we can evaluate ratios n ( x ; t , A ) / I I ( z ; t , A),</p><formula xml:id="formula_10">A -0 ,</formula><p>x, z E 52, and hence conditional probabilities. The price for indirect sampling is that we must restrict the rate of growth of X and the rate of decrease o f t .</p><p>Fix two sequences { t k } , { A,}, a "site visitation" schedule { A k } , Ak C S, and let &amp;(x) = n ( x ; tk, X k ) .</p><p>The set Ak is the cluster of sites to be updated at "time" k; the "centers" of the clusters are addressed in a raster scan. In our experiments we take either I Ak 1 = 1 or 1 Ak 1 = 5,inwhichcasetheAk'sareoftheform { ( i , j ) , ( i + 1 , j ) , ( il , j ) , ( i , j + 11, ( i , j -I ) } .</p><p>Define a nonhomogeneous Markov chain { X( k), k = 0, 1, 2, -} on Q as follows. Put X(0) = q arbitrarily. Given X(k) = (X,,(k), Then, under suitable conditions on { t k } and { hk} , either lim P(X(k) = x ( X ( 0 ) = q ) = II*(x) or the limit is IIo(x). The condition in the former case (constrained SR) is that tk = 1, Xk 7 03, and X k 5 const. log k. The condition for convergence to no (constrained SA) is that tk \1 0, X k 7 03 and tklXk I const. * log k.</p><formula xml:id="formula_11">, XsM(k)), define X,(k + 1 ) = X , ( k ) f o ~-s $ A ~+</formula><p>The algorithm yields a solution to the constrained optimization problem (1.2) in the sense that the asymptotic distribution of X ( k) is uniform over the solution set: if the solution is unique, i.e., Cl,* = {xo}, then X(k) -+ xo in probability. See [ 181 for proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k + m</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approximations</head><p>The logarithmic rate is certainly slow. Still, we often adhere to it for ordinary annealing; others [36], <ref type="bibr" target="#b45">[47]</ref> have as well. We refer the reader to <ref type="bibr" target="#b33">[35]</ref> for some interesting comparisons between schedules. It is commonplace to find linear ( t k = toa k ) and exponential ( tk = ( 1 -~) ~t ~, y small) schedules; here k refers to the number of sweeps or iterations of S; in our experiments s = sP' or sP'.</p><p>We now describe several protocols used in our experiments. One variant we do not use is to fix X k = X very large and do ordinary annealing, which might appear sensible since the solutions to min { U ( x ) : V ( x ) = 0 } coincide with those of min { U ( x ) + h V ( x ) } for all X sufficiently large (due to the fact that Q is finite). However this is not practical: unless to is very large and tk is reduced very slowly, the system immediately gets stuck in local energy minima of U + AV which are basically independent of the data, although faithful to the constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It is better to begin with states faithjid to the data and slowly impose the constraints, a standard technique in conventional optimization.</head><p>One variation of constrained SR that has been effective</p><p>is "low-temperature sampling": fix tk = E (small) and let X k 7 03. The idea is to reach a likely state of the posterior distribution II (x I y ) . In practice, we allow X k to grow linearly; the details are in Section V.</p><p>Another variation is the analog for constrained relaxation of "zero-temperature'' sampling, which has been extensively studied by Besag <ref type="bibr" target="#b19">[2]</ref> under the name ICM (for "iterated conditional modes"); see also [ 1 11, [ 141, and  [22]. Without constraints, this algorithm, which is deterministic, results in a sequence of states X(k) which monotonically decrease the global energy, i.e. , increase the posterior likelihood. The constrained version operates as follows. Recall that when the set of sites Ak + is visited for updating, we defined X( k + 1 ) by replacing the coordinates of X(k) in Ak+ l by a sample drawn from the conditional distribution of I l k + on { x,, s E Ak + } given the values { x, = X, ( k ) , s 6 Ak + } . Suppose we replace the sample with the mode, i.e., the most likely vector { x,, s E Ak + } conditional upon { x, = X, ( k ) , s g! Ak + I }. In essence, we fix tk = 0 . This generates a deterministic se-quenceX(k), k = 0, 1, 2, * -depending only onX(O), nk, and { A k } . (Notice that the mode is unaffected by tk since it corresponds to the minimum of U ( x) + X k V ( x). ) Then, during the kth sweep, with X = X k , the energy U + X k I/ is successively reduced, just as in ICM where hk = 0. Of course since there is nojixed (reference) energy, the algorithm cannot be conceived as one of iterative improvement. Several experiments were run with both the stochastic and deterministic algorithms; see Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Summary</head><p>Each experiment used one of the following two variations on constrained relaxation (see Section V for details):</p><p>Choose a label resolution U and an associated pixel block size.</p><p>Select features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choose a site visitation schedule { A k } , Ak C S k =</head><p>Fix a Zinear growth schedule for { Xk } .</p><p>Choose X(O), arbitrary. Set k = 0. Deterministic Algorithm:</p><formula xml:id="formula_12">0 ) Set tk = 1, k = 1, 2, - 1) Define X,(k + 1 ) = X,(k), s $ A k + l and define { X, ( k + 1 ): s E Ak + } to be the (multivariate) mode of 1 , 2 , * . -. * . n k + l ( X s , S E A k + l ( x s = X,(k), S $ A k + l ) . 2) k = k + 1. 3) Go to 1. Stochastic Algorithm: 0) Set tk = E , k = 1, 2, * ( E "small").</formula><p>1) Define X,y ( k + 1 ) = X,y ( k ) , s 6 Ak + and define { X, ( k + 1): s E Ak + } to be a (multivariate) sample from</p><formula xml:id="formula_13">I I k + I ( x , J E A k + l I ~, = X s ( k ) , s g ! A k + I ) .</formula><p>2) k = k + 1.</p><p>3) Go to 1. Usually, but not always, the deterministic algorithm was sufficient (again, see Section V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. EXPERIMENTS'</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Partition Model</head><p>There are three experiments: an L-band synthetic aperture radar (SAR) image3 of ice floes in the ocean (Fig. <ref type="figure">9</ref>), a texture mosaic constructed from the Brodatz album <ref type="bibr" target="#b5">[6]</ref> (Fig. <ref type="figure">lo</ref>), and another mosaic from pieces of rug, plastic, and cloth (Fig. <ref type="figure" target="#fig_17">11</ref>).</p><p>Processing: In each experiment the partitioning was randomly initiated; the labels, x,s E Sp), were chosen in-Thereafter, label sites were visited and updated one at a time, by a "raster scan" sweep through the label array. MAP partitionings were approximated by "zero-temperature" sampling (see Section IV), with X = Xk increasing with the number of sweeps. Specifically, X was held at 0 through the first 10 sweeps, and thereafter was raised by 1 every 5 sweeps: hk = 0, k = 1, lo; Xk = 1, k = 11, . 15; X k = 2, k = 16, -20; etc. Most probably, A could have been increased more rapidly, perhaps with every sweep, without substantially changing the results, but this was not systematically investigated. For the three experiments shown in Figs. 9, 10, and 11, between 15 and 50 sweeps sufficed to bring the changes in labels to a halt; see below for more details. Recall that zero-temperature sampling corresponds to choosing the conditional mode. Occasionally there are ties, and these were resolved by choosing randomly, and uniformly, from the collection of modes.</p><p>As a general rule, results were less reliable at higher resolutions (lower a's) and when more labels were allowed (higher values of P ). In these cases, repeated experiments, with different initializations, often produced different results. With P too large, homogeneous regions were frequently subdivided, being assigned two or three labels. With U too small, the tendency was to mislabel small patches within a given texture. It is likely that many of these mistakes correspond to local minima; perhaps some could be corrected by following a proper annealing schedule (see Section IV), and by more careful choices of thresholds (see below). Here again, definitive experiments have not been done.</p><p>Measures of Disparity: Recall that the disparity measure is derived from the Kolmogorov-Smirnov distance between blocks of pixel data under various transformations, as defined in Section 11, equation (2.3). For the SAR image, good partitionings were obtained using only the raw data: m = 1 and y ( l ) is just y in (2.3). Evidently, gray-level distributions are enough to segment the water and ice "textures," at least when supplemented by the "prior constraints" embodied in the penalty term V ( x ) .</p><p>dependently and uniformly from 0, 1, --, P -1.</p><p>'Fortran code and terminal sessions are available. 'We are grateful to the Radar Division at ERIM for providing us with the SAR image (collected for the U.S. Geological Survey under Contract 14-08-0001-21748 and the Office of Naval Research under Contract N-00014-81-C-0692 and N-00014-81-C-0295).</p><p>The texture collages in Figs. 10 and 11 are harder. We used four data transformations in addition to the raw pixel data. Hence, for these experiments m = 5, ycl) = y, and y ( * ) , *y(5) are based on various transforms. In particular, y;" measures the intensity range in the 7 x 7 pixel block V, centered at s: maxtevSYr -minteV,Yr; y1*' = yf" is the "residual" (1.1) obtained by comparing y, to the 24 "boundary pixels" (dV,) of V, (i.e., all pixels on the perimeter of the 7 x 7 block): yl" = and y'4' and y(5) are horizontal and vertical "directional residuals" :</p><p>Parameter Selection: The resolution ( a ) was 7 for the SAR picture (Fig. <ref type="figure">9</ref>); 15 for the Brodatz collage; and 13 for the pieces of rug, plastic, and cloth. These numbers were chosen more or less ad hoc, but are small enough to capture the important detail of the respective pictures while not so small as to incur the degraded performance, seen at higher resolutions and mentioned earlier.</p><p>The number of allowed labels is also important; recall that too many usually results in over-segmentation. This was actually used to advantage in the SAR experiment (Fig. <ref type="figure">9</ref>), where there are evidently two varieties of ice. The best segmentations were obtained by allowing three labels. Invariably, two would be assigned to the ice, and one to the water. Using just two labels led to mistakes within the ice regions, although there was little experimentation with the Kolmogorov-Smirnov threshold, and no attempt was made with the data transforms ( m &gt; 1 ) used for the collages. In the other experiments, the number of labels was set to the number of texture species in the scene.</p><p>The most important parameters were the thresholds,</p><p>{ ci} 1 I i I m, associated with the Kolmogorov-Smirnov statistics [see (2.3)]. For the SAR experiment, m = 1, and the threshold was guessed, a priori; it was found that small changes are reflected only in the lesser details of the segmentation. For the collages ( m = 5 ), the thresholds were chosen by examining histograms of Kolmogorov-Smirnov distances for block pairs within homogeneous samples of the textures. Thresholds were set so that no more than 3 or 4 % of these intraregion distances would be above threshold (a "false alarm"). Of course, we would have preferred to find more or less universal thresholds, one for each data transform, but this may not be possible. Conceivably, with enough of the "right" transforms, one could set conservative (high) and nearly universal thresholds, and be assured that visibly distinct textures would be segmented with respect to at least one of the transforms. Recall that the disparity measure (2.3) is constructed to signal "different" when the distance between blocks, with respect to any of the transforms, exceeds threshold. Fig. <ref type="figure">9</ref> (SAR): As mentioned earlier, three labels were used, with the expectation that the ice would segment into two regions (basically, dark and light). The resolution was U = 7, and the Kolmogorov-Smimov statistic was computed only on the raw data, so m = 1. The threshold was cI = 0.15. The original image is 512 X 512 (the pixel resolution is about 4 by 4 m ) , but to avoid special treatment of the boundary, only the 462 X 462 piece shown in Fig. <ref type="figure">9</ref>(a) was processed. The label lattice Sp) is 64 x 64. Fig. <ref type="figure">9(b)</ref> shows the evolution of the partitioning during the relaxation. For display. gray levels were arbitrarily assigned to the labels. The upper left panel is the random starting configuration. In successive panels are the states of the labels after each five iterations (full sweeps). In the bottom right panel, the two labels associated with ice are combined, "by hand." x 16 label lattice. The Kolmogorov-Smirnov thresholds were c , = 0.90, c2 = 0.49, ci = 0.20, c4 = 0.11, and c5 = 0.12, corresponding to the same data transforms used for the Brodatz textures (Fig. <ref type="figure" target="#fig_15">10</ref>). The experiment makes apparent a huzurd of long range bonds: the gradual but marked lighting variation across the top of the image produces a large Kolmogorov-Smimov distance when raw pixel blocks from the left and right sides are compared. This makes it necessary to essentially ignore the raw data Kolmogorov-Smirnov statistic, and base the partitioning on the four data transformations; hence the threshold cI = 0.9. The transformed data are far less sensitive to lighting gradients.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bounday Model</head><p>There are five test images: one made indoors from tinker toys ("cart"), an outdoor scene of a house, another of ice floes in the ocean (the same SAR image used above), and two texture mosaics constructed from the Brodatz album.</p><p>Processing: All the experiments were performed with the same site-visitation schedule. Given the resolution U. which varies among experiments, the sites of the sublattice S f ' were addressed in a raster-scan andjive sites were simultaneously updated. Specifically, at each visit to the site (io + 1 , j o + 1 ), the values of the boundary process at this site and its four nearest neighbors, { ( ( i 1 ) U + 1, ( j k 1 ) U + 1 ) }, were replaced based on the conditional distribution of these five boundary variables given the variables at the other sites and the data J. Of course this distribution is concentrated on the 2s = 32 possible configurations for these five variables.</p><p>Two update mechanisms were employed: stochastic relaxation and the "zero-temperature," deterministic variation discussed earlier. In the former case, the updated binary quintuple is a sample from the aforementioned conditional distribution, which varies depending on the penalty weight AL for the kth sweep of the lattice S F ' . Of course "O-temperature" refers to replacing the sample by the conditional model. In both cases we let Ak grow linearly.</p><p>Stochastic relaxation at low temperature is more effective than at zero temperature (essentially iterative improvement). However, deterministic relaxation sufficed for all but two scenes, the ice floes and the four texture collage; these results could not be duplicated with deterministic relaxation. In one case, we present both results for comparison.</p><p>Generally, deterministic relaxation stabilizes in 5-10 sweeps whereas stochastic relaxation requires more sweeps, perhaps 20-60. We provide several pictures showing the evolution of the algorithm.</p><p>Penalties: All the experiments were conducted with the same forbidden patterns, namely those in Fig. <ref type="figure" target="#fig_18">12</ref>, with the exception of the house scene, for which the last pattern was omitted. (At the resolution used for the house, namely U = 3, the inclusion of that pattern would inhibit the formation of structures at the scale of six pixels; many such nontrivial structures appear in that scene.) Thus, the penalty function V ( x ) records a unit penalty for each occurrence in the boundary map x = { x I , s E S g ) } of any of the five patterns depicted in Fig. <ref type="figure" target="#fig_18">12</ref>. It is interesting to note that in no case was the final labeling completely free of penalties, i.e., V ( a ) = 0. Perhaps this could be achieved with a proper annealing schedule, or with updates of more than five sites.</p><p>Measures of Disparity: All the experiments are based on instances of the measures (3.3)-(3.5) described in Section 111.</p><p>1) For the first experiment, the cart scene, the boundary resolution is U = 1 and we employed the measure given in (3.3) with y = l O A and the raw difference 1 y,,.</p><p>y,*l modulated by the four nearest differences of the same orientation as ( s*, t* ) . Thus, for the horizontal pair ( s, t ) of adjacent boundary sites, where s* = (i, j ) , t* = ( i + 1 , j ) , and is the mean absolute intensity difference over the image. The utility seems largely impervious to the choice of the scaling constant (here = 10) for the mean as well as to the range of the modulation.</p><p>2) We used the Kolmogorov-Smirnov measure (3.4) for both the house and ice floes scenes. For the house, we chose U = 3 and blocks of size 25: the setup is depicted in Fig. <ref type="figure" target="#fig_7">6</ref>. Due to the uniform character of the background (e.g., the sky) the distance (3.4) was computed based on the transformed data yIi = j I , + qr,, where { q r J } are independent variables, and distributed with a triangular density, specifically that of 1O(u, + U ? ) , u l , U ? uniform (and independent) on [ 0, 1 1.</p><p>The boundary resolution for the radar experiment is U = 8. reflecting the larger important structures there; the image is 512 x 512. The dynamic range is very narrow and the difference between the dark water and somewhat less dark ice is essentially one of texture, due in part to the customary speckle noise. In particular, the ice cannot be well-differentiated from the water based on shading alone. The disparity measure is (3.4), applied to the raw image data over 24 x 24 blocks. The problem encountered in the house scene is actually alleviated by the speckle.</p><p>3) The texture mosaic experiments are based on the measure (3.5) for a particular family A I , * , A5 of five data transformations or "features." In each case, the resolution is U = 5 and block size is 21 X 21. Recall that these five features are combined into a single measure of change according to the formula in (3.5). The transformations used are the range maxtc V, Yt -mints V, Yt Zjl' = over a 7 x 7 window Vs centered at pixel s, and the four directional residuals These residuals were then uniformly averaged over V,, yielding the final features y ( ' ) ,</p><p>It is instructive to compare the Kolmogorov-Smimov differences for the raw and transformed data over these texture mosaics. Typically, if one looks at the resulting two histograms of differences for a give transform, on finds that, whereas the raw (Kolmogorov-Smimov) differences are actually larger at the texture borders, the transitions between the borders and interiors are sharper for the transformed data. Detecting the boundaries with the raw data necessitates an unacceptable number of "false alarms" in the sense of interior "microedges." house, and ice floes, the parameters a and d* were chosen as follows. Find the mean disparity over all (vertical and horizontal) values of As,, for relevant bonds ( s, t ); take a equal to the 99th percentile of those above the mean and d* equal to the 70th percentile of those above the mean. This yields the values cy = 150, d* = 42 for the cart scene; recall that for this experiment, both the grid and block sizes are unity. For the house scene ( (T = 3 ) the Kolmogorov-Smimov statistics were computed over    [Fig. <ref type="figure">17</ref> VI. GENERALIZATIONS These models may be extended in many directions, or combined into a single model. What follows is a brief description of three such generalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Other Boundary Primitives</head><p>The elementary boundary unit or primitive is a horizontal or vertical segment whose length is resolution-dependent, namely 0 + 1 in pixel units. As a result, a discontinuity running at 45" is detected and localized with less reliability than one at 0" or 90", where the disparity measure has maximum sensitivity. An obvious remedy is to replace the pairs (s, r ) , by other "primitives," e.g., a distinguished family of triples, such as the six represented (up to translates) in Fig. <ref type="figure" target="#fig_25">18</ref>.</p><p>More generally, for any such family a consider the interaction term where A,( y ) is a measure of the disparity in the "A-direction." In Fig. <ref type="figure" target="#fig_25">18</ref>, these directions are, respectively, w / 4 , 7r/4, -w/4, --w/4, w / 2 , and 0. One can imagine a variety of ways to situate two appropriately shaped sets of pixels to straddle (and abut) segments such as these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multivariate Datu</head><p>Data may be available from several sensors, e.g., multispectral satellite data, or an optical camera and laser ra- Another possibility is to attempt to classify boundaries, especially if range data is also available. Let y'" and y'" be range and brightness values, and let x, assume three values, say 0, 1, 2, corresponding to "off ," "occluding ( = depth) boundary" and "other boundary" (for instance a crease or shadow). Now rig the energy function to couple A ( y ( ' ) ) with 4 = { 4, }, 5, = 6, } ( x , ) and A ( $')) with q = ( q s } , qs = 6il,2}(xs); for example, just add the two corresponding terms to form U . The penalty patterns are the usual ones regarding dead-ends etc. (with 0's and 2's as well as 0's and l's), and mixtures of 0, 1, 2 corresponding to physically implausible (or impossible) transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Region-Boundary Model</head><p>Put the boundary and region labels into a single model, for example of the form U , ( X ' , y ) + U 2 ( x ' , y ) . Now penalize improper local configurations in the pair (x', x'), for example "type 1" errors (a boundary "between" like region labels) and "type 2" errors (no boundary "between" unlike labels). The problem may be that there are deep local minima which are unfaithful to the data but difficult to escape from, at least without updating many sites.</p><p>VII. SUMMARY We have developed algorithms for partitioning an image, possibly textured, into homogeneous regions and for locating boundaries at significant transitions. Both are based on a scale-dependent notion of disparity, or gradient. and both incorporate prior expectations about regular boundary or region configurations.</p><p>The disparity measure scores the difference between the statistical structures of two scale-dependent blocks of pixels. We have experimented with several measures. Ideally, the disparity will be large when there is an apparent difference, either in gray-level or in texture, between the blocks. Usually, it was necessary to tune the measure to the particular textures or structures involved; a more universal measure may require both better preprocessing (e.g., first extracting reflectance from intensity [33]) and better use of "high-level" information about expected macrostructures and shapes. For texture discrimination, by either partitions or boundary placement, we introduce a class of features, or transformations, that are decidedly multivariate, depending on the spatial distribution of large numbers of pixel gray levels. Our disparity measure is then a composite of measures of differences in the histograms of the block data, under the various transformations. Low-order features, such as those derived solely from raw gray-level histograms and cooccurrence matrices, were not as effective in our framework.</p><p>Disparity measures between pairs of pixel blocks drive the segmentations or boundary placements through a "label model, " that specifies likely label configurations conditional on disparity data. For partitioning, labels are generic and associated with local blocks of the image. Two labels are the same if their respective regions are judged to be instances of the same texture. For boundary placement, the labels are zero or one, and interpreted as indicating, respectively, the absence or presence of boundary elements. A priori knowledge about acceptable label configurations, which, for example, may preclude very small or thin regions, or cluttered boundary elements, is applied by restricting labels to an appropriate subset of all possible configurations. The result of modeling disparity-label interactions and of defining restricted configurations can be regarded as a Gibbs distribution jointly on pixel gray levels and label configurations, with the marginal label distribution supported on a subset of the configuration space.</p><p>Partitioning and boundary finding is accomplished by approximating the maximum a posreriori (MAP) label configuration, conditioned on observed pixel data. Because certain configurations are forbidden, MAP estimation amounts to constrained optimization. Stochastic relaxation and simulated annealing are extended to accommodate constraints by introducing a nonnegative constraint function that is zero only for allowed label configurations. The constraint function, with a multiplicative constant, is added to the posterior energy, and the constant is slowly increased during relaxation. Straightforward calculations establish an upper bound on the rate of increase of this multiplicative constant that ensures convergence of the relaxation and annealing algorithms to the desired limits. In a series of partitioning and boundaryfinding experiments, deterministic and other fast variations of the constrained relaxation algorithm are found to be effective.</p><p>The partitioning model is appropriate when a small number of homogeneous regions are present. Disjoint instances of a common texture are automatically identified. The boundary model can be effective in complex, multitextured, scenes. Both models sometimes require prior training to adjust parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>IEEE</head><label></label><figDesc>Log Number 90361 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[ 20 ]</head><label>20</label><figDesc>, [2], [9], [24], [28], image synthesis [28], computed tomography [22], texture and boundary analysis [ 171, [53], [ 191 , [2 11 , [25], scene segmentation based on optical flow [47] or shading and texture [ 111, [ 131, frame-toframe matching for computing optical flow and stereo disparity [36], and surface reconstruction [43], [44].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>,</head><label></label><figDesc>v i : ) } and d 2 ) = { vi2', vi2', -* , U ( * ) } it provides a test statistics for the hypothesis that v(Irand d 2 ) are samples from the same underlying probability distribution, meaning that Fl = F2 where, for i = 1, 2, U ( ' ) = { v y ) , dj'), * * , U : ' } are independent and identically distributed with F, ( t ) = P(25('' 5 t ) . The test is designed for continuous distributions and has a powerful invariance property which will be discussed below. The sample distribution function of a data set { vl, v2, -9 U , 1 is . . . Thus, P is a step function, with jumps occurring at the points { vk } . It characterizes the histogram. Now consider two se;s of data U ( ' ) , d 2 ) with sample distribution functions Fl, F2. The Kolmogorov-Smimov distance (or statistic) is tke maximum (vertical) distance between the graphs of F , , F2, i.e., d(v"', d 2 ' ) = max IP,(t) -P 2 ( t ) l . (2.2) We write d(v"', d 2 ) ) to emphasize the data (which in our case consists of blocks of possibly transforme$ pixel intensity values); the conventional notation is d ( FI, F 2 ) . The invariance property is the following. Suppose U ( ' ) , d 2 ' are samples from continuous distributions F , , F2. The under the ("homogeneity") hypothesis FI = F2, theprobability distribution of d (as a random variable) is independent of the (common) underlying distribution. Basically, this stems from the fact that d is invariant to strictly monotone transformations of the data, i.e., -a &lt; / &lt; + a where qji' = T( U:") and T is strictly increasing or decreasing. Thus, in two sample tests for homogeneity, one rejects the null hypothesis that F , = F2 if d(v"', d 2 ' ) I d*, where d* depends only on n , and n2, and on the signi$cance level of the test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>@s,r = 2 ~{ ~~~( D ~~, ~( ~r ) ) &gt; ~} ( y )-1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 . 5 )</head><label>25</label><figDesc>and V ( x ) is just the sum of (2.4) and (2.5). gray-level image y = { yij &gt;;2) a resolution u = 1 , 2 , * * -, and a maximum number 3) a disparity measure CP,,,( y ) for each pair ( s, t ), in 4) a collection of penalty patterns.The (MAP) partitioning . 2 = a( y ) is then any solution x E QfP, ') of the constrained optimization of labels P ; the sublattice Sfp';</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4</head><label>4</label><figDesc>shows the grids Sh2) and Sb3' for N = 8; the sites off the grid are denoted by dots. The selection of a influences the interpretation of x, the computational load, the interaction range at the pixel level, and is related to the role played by the size of the spatial filter in edge detection methods based on differential operators. Finally, let QI and Qg) denote the state spaces of intensity arrays and boundary maps respectively; that is, QI = { { y s } : s € S I , 0 5 ys 5 2551, n p = { (x,}: s E sp, x, E (0, I } } . Sometimes, we simply write Q, for fig'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Pixel sites ( O ) and boundary sites ( + ) for a 3 x 3 lattice</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>y + 6 )</head><label>6</label><figDesc>for any a , b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5 . P i x i pairs (:&gt;'s) associated with horizontal and vertical boundarysegments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>5 )</head><label>5</label><figDesc>where y ( ; ) = Ai ( y ) , 1 5 i I m , and y ( ' ) ( D , ) = { y:'), s E D , } , exactly as in Section 11. Then A.v,,( y ) &gt; d* (and, hence, +(As,,( y ) ) &gt; 0) if and only if d ( y ( i ) ( D . y * ) , y ( ' ) ( D , * ) )2 d*cj for some transform i . The thresholds cl, * -, c, are again chosen to limit "false alarms."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7 . Forbidden boundary patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>, 1 s i I M } where A is finite, M is very large, and S = { sl, s2, * , s M } is a collection of "sites," typically a 2-D lattice. Write x = (x,,, . . . , xsM) for an element of Q, and let</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>= { a E Q*: U(o) = E } , , $ = minx,,* U ( x ) . Let IIo denote the uniform measure in (4.3). Sampling directly from I I ( x ; t , A ) is impossible due to the size of Q; otherwise just use (4.2) and (4.3) to generate a sequence of random variables X ( k ) , k = 1 , 2, * , with values in Q, and limiting distribution either II* or no.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>~a n d l e t {X,(k + l ) : s ~A ~+ ~} b e a (multivariate) sample from the conditional probability distribution I&amp;+l(x,, s E A k f l Ix, = X,(k), s 6 &amp; + I ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>. (a) Synthetic aperture radar image of oceanic ice floes. (b) Evo- lution of the label states from random initialization (upper left) to final partition (lower right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 (</head><label>10</label><figDesc>Fig. 10 (Brodurz Textures): The Kolmogorov-Smirnov thresholds were c I = 0.40, c2 = 0.53, c3 = 0.26. c4 = 0.28, and cs = 0.19, corresponding to the transforms 4.' I , *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Fig. 1 l(b) displays the evolution of the partitioning during relaxation. The layout is the same one used in the previous figures, showing every 5 iterations. except that there are 10 iterations between the final two (b) bels. upper left to lower right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (a) Texture collage: rug, plastic. cloth. (h) Evolution of the la-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Forbidden patterns for the boundary experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Finally, the values</head><label></label><figDesc>of the constants c I , . . * , c5 used in the construction of As,, [see (3.5)] are selected by restricting the percentage of false alarms. The details are given in the following section. Parameter Selection: Recall that the total change across the boundary segment ( s , t ) is measured by 4 ( As,r( y ) ) , where 4 is given in (3.2). Given A, there are two parameters to choose: a normalizing constant a and the intercept d* = 4-'(0). For the object boundary experiments, namely the cart, * * Y ' ~) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>5 X 5 Fig. 13 .Fig. 15 .</head><label>51315</label><figDesc>Fig. 13. (a) Irnage a i il tinker toy c a r t (b) Evolution or the boundaries (upper lcfl to lowcr right) with moltiplc-sitc stochastic relaxation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 14 (</head><label>14</label><figDesc>Fig. 14   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>( a )</head><label>a</label><figDesc>House scenc. (h) Final boundary placcmenls at resolution c (b) Pig. 16. (a) Collage of nine Brodetz texturces. (b) Final boundary placemcnts with deterministic relaxalion (Mi) and stochastic relaxatiun (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>I E t P</head><label>t</label><figDesc>TRANSACTIOhS ON PATTERN ANALYSIS AND MACHINF IhTELLIGENCE. VOL 12. NO 7. JULY li)i)o (b) Fig. 17. (a) Collage of four Brodatz textures. (b) T w o runs of stochastic relaxation showing every third s w e e p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>(a)]: raffia (upper left), sand (upper right), wool (bottom), and pigskin (center). Two runs (diRerent seeds) are shown [Fig. 17(b)] individual frames representing every third sweep.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Family of possible boundary primitives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Modestino et al. [45] use</head><label></label><figDesc></figDesc><table><row><cell>random tessellations for the upper level and maximum</cell></row><row><cell>likelihood estimation based on cooccurrence data for the</cell></row><row><cell>labeling. In each case, the random field parameters are</cell></row><row><cell>assumed known in advance or are estimated from training</cell></row><row><cell>samples. The image data in [12] is generated from the</cell></row><row><cell>model, and simpler than the actual texture collages in [53]</cell></row><row><cell>and [45]. Again, our results compare favorably.</cell></row><row><cell>Finally, Kashyap and Eom [37] employ a "long cor-</cell></row><row><cell>relation" random field model and statistical hypothesis</cell></row><row><cell>testing to obtain a boundary map. Boundary placements</cell></row><row><cell>are estimated in each small strip of the image based on</cell></row><row><cell>least-squares estimates of the model parameters. There is</cell></row><row><cell>no training data and the results are good, although, nat-</cell></row><row><cell>urally, the boundaries are somewhat disorganized.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We are indebted to E. Bienenstock for recommending the random topology employed in the partition model, and for pointing out a fascinating connection to some work in neural modeling <ref type="bibr" target="#b40">[42]</ref>.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ommended for acceptance by A. K. Jain. The work of D. Geman and P. Dong was supported in part by the Office of Naval Research under Contract "14-86-K-0027. The work of S . Geman and C . Graffigne was supported in part by the Army Research Office under Contract DAAL03-86-K-0171 to the Center for Intelligent Control Systems, by the National Science Foundation under Grant DMS-8352087, and by the General Motors Research Laboratories. D. Geman is with the Department of Mathematics and Statistics, University of Massachusetts, Amherst, MA 01003. S . Geman is with the Division of Applied Mathematics, Brown University, Providence, RI 02912. C . Graffigne was with the Division of Applied Mathematics, Brown University, Providence, RI 02912. She is now with the French National</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simulated annealing: A pedestrian review of the theory and some applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Laarhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">N A 7 0 Adwrirecl Stud! Institlrte on Pattern Recognition: Theor! arid Applicatiorr\. Spa. Belgium</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the statistical analysis of ditty pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Roy. Statist. Soc., series B</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>with discussion</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The least disturbance principle and weak constidints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Partern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="393" to="399" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The n-city travelling salesman problem: Statistical mechanics and Metropolis algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bonomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Lutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Re\%</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="551" to="568" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-level approaches to large scale problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. lnt. ConRr. Mathemariciuns</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Gleason</surname></persName>
		</editor>
		<meeting>lnt. ConRr. Mathemariciuns</meeting>
		<imprint>
			<publisher>Amer. Math. Soc.. Providence. RI</publisher>
			<date type="published" when="1986">1986. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Texture: A Phorographir Album f o r Artists and Desigr7-ers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brodatl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Dover</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of image restoration methods</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Trussell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Hunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3385" to="3390" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A thermodynamical approach to the travelling salesman problem: An efficient simulation algorithm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inst. Phys. Biophys., Comenius Univ</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<pubPlace>Bratislava, Czechoslovakia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image restoration using an estimated Markov model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chalmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dep. Math., Univ</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Paris; Orsay, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple parallel hierarchical and relaxation algorithms for segmenting noncausal Markovian random fields</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Inrell</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1987">1987</date>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
	</monogr>
	<note>Inst. Math., Academia Sinica</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmentation of textured images using Gibbs random fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Derin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compur. Vision. Graphics, Image Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="72" to="98" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling and segmentation of noisy and textured images using Gibbs random fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Derin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Partern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="39" to="55" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A parallel image segmentation algorithm using relaxation with varying neighborhoods and its mapping to array processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Derin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dep. Elec. Comput. Eng., Univ. Massachusetts, Tech. Rep</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning the parameters of a hidden Markov random field image model: A simple example</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dekesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition Theory and Applications</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequential synthesis of natural textures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gagalowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cornput. Vision, Gruphics, Image Processing</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="289" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stochastic model for boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Cofnput</title>
		<imprint>
			<date type="published" when="1987-05">May 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relaxation and annealing with constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Division Appl. Math., Brown Univ., Complex Systems Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locating texture and object boundaries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graffigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition Theor?. and Applications</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kittler</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions. and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Inre</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Markov random field image models and their applications to computer vision</title>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graffigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Congr. Marhemaricians</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Gleason</surname></persName>
		</editor>
		<meeting>Int. Congr. Marhemaricians</meeting>
		<imprint>
			<publisher>Amer. Math. Soc</publisher>
			<date type="published" when="1986">1986. 1987</date>
		</imprint>
	</monogr>
	<note>Providence. RI.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical methods for tomographic image reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Mcclure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. IS1 (Proc. 46th Session Int. Srarisriral Instirure)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonstationary Markov chains and convergence of the annealing algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Truns. Puttern Anal. Machine Inrell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="164" to="180" />
			<date type="published" when="1241-05">198.5. 1241. Feb. 1989</date>
		</imprint>
	</monogr>
	<note>J . Statist. Phys.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Experiments in texture analysis and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Graffigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Division Appli. Math., Brown Univ</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Besag. J . Roy. Starisr. Soc</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>Discussion on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Seheult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Besag. J . R o y Srutist</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tutorial in pattern theory</title>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Division Appl. Math.. Brown Univ.. Lecture Notes Volume</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discontinuity detection for visual surface reconstruction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compur. Vision. Graphics. Iinccjie Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="141" to="163" />
			<date type="published" when="1985">1985. 1987. 1987</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Soc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="259" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial survey of theory and applications of simulated annealing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24rh /€€E Con$ Decisiori arid Control</title>
		<meeting>24rh /€€E Con$ Decisiori arid Control</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M K</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><surname>Denstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sysr.. Man. Cyhrrn</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Simulated annealing via Sobolev inequalities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Holley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stroock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The variational approach to shape from shading</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compur. Visioti. Graphics, Image ProcessinLy</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="174" to="208" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large time behaviors of perturbed diffusion Markov processes with applications</title>
		<author>
			<persName><forename type="first">C.-R</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Sheu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inst. Math., Academic Sinica</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Taipei. Taiwan</pubPlace>
		</imprint>
	</monogr>
	<note>I, 11, and Ill</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Markov random fantasies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kashko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Buxton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Parallel stochastic optimization in computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kashko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Buxton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Texture boundary detection based on the long correlation model</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">/E€€ Trans. Pattern Anal. Machine In</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A model-based method for rotation invariant texture classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khotanzad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">l€EE Trans. Parrern Anal. Machine ltirell.</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="472" to="481" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gellatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Textured image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Laws</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">940</biblScope>
		</imprint>
		<respStmt>
			<orgName>Univ. Southern California, Los Angeles, USCIPl Rep</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An iterative-improvement penalty-function-driven wire routing system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I E M J . Res. Dewlop</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="613" to="624" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Statistical coding and short-term synaptic plasticity: A scheme for knowledge representation in the brain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Der Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<editor>E. Bienenstock. F. Fajelman SouliC, and G. Weisbuch</editor>
		<imprint/>
	</monogr>
	<note>in Disordered System and Biological 0rgani:arion (NATO AS1 Series</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Surface reconstruction preserving discontinuities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Marroquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell. Lab., Massachusetts Inst. Technol., Memo</title>
		<imprint>
			<biblScope unit="volume">792</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic solution of ill-posed problems in computational vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Marroquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Amer. Starisr. As-SOC</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="76" to="89" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Texture discrimination based upon an assumed stochastic texture model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W R W</forename><surname>Modestino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><surname>Vickers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Muchine Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="557" to="580" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gibbs and Markov random systems with constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moussouris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Srarisr. Phys</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scene segmentation froin visual motion using global optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parrern Anal. M achine Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="220" to="228" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A critical survey of image analysis methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expanded version of paper delivered to 8th Int. Conf. Pattern Recognition</title>
		<meeting><address><addrLine>Paris. France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986-10">Oct. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Computational vision and regularization theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Narure</title>
		<imprint>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="page" from="314" to="319" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Statistics. images. and pattern recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canudian J . Stati.sr</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="83" to="84" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image segmentation and image models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. I€€€</title>
		<meeting>I€€€</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="764" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Besag. J . Roy. Srutisr. Soc</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic and deterministic algorithms for texture segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Simchony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dep. EE-Syst.. Univ. Southern California</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonuniversal critical dynamics in Monte Carlo Fimulations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Swendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-convex optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Szu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Con$ Real Time Signal Processing f X</title>
		<meeting>SPIE Con$ Real Time Signal essing f X</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">698</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Regularization of inverse visual problems involving discontinuities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note type="report_type">fEEE Trans. Parrerri Anal. Machine /rite</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Textural boundary analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">f € E € Trans. Coni</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Texturerkennung and texturreproduktion</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Triendl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kyherfell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="1986">1989. 1986</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">/</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1977. 1973</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A model for texture edges</title>
		<author>
			<persName><forename type="first">E</forename><surname>Triendl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Con</title>
		<meeting>5th Int. Con<address><addrLine>Miami Beach. FL</addrLine></address></meeting>
		<imprint>
			<date>Dec. 1-4</date>
		</imprint>
	</monogr>
	<note>Parrerri Recognition</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Finding texture boundaries in images</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Voorhees</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Error measures for scene segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Yasnoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Mui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bacus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parrern Recog</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Building a distance function for Gestalt grouping</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Zobrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I€€€ T r a m . Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="1975">1975. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">His research interests are in stochastic processes. image processing, and computer vision</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1965 and the Ph.D. degree in mathematics from Northwestern University, Evanston. IL. in 1970. Since 1970 he has been a member of the Department of Mathematics and Statistics, University of Massachusetts, Amherst. where he is currently Professor of Mathematics and Statistics</title>
		<meeting><address><addrLine>Urbana; Ann</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1976">1976-1977. 1983. 1986</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois ; UniversitC de Paris-Sud</orgName>
		</respStmt>
	</monogr>
	<note>Brown University. Stuart Geman (M&apos;84) received the B.A. degree in physics from the University of Michigan</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
