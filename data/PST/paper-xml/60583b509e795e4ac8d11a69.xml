<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training Graph Transformer with Multimodal Side Information for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Alibaba-NTU Singapore Joint Research Institute &amp; LILY Research Centre</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Alibaba-NTU Singapore Joint Research Institute &amp; LILY Research Centre</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susen</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyi</forename><surname>Lei</surname></persName>
							<email>chenyi.lcy@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haihong</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
							<email>juyong@ustc.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
							<email>ascymiao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Alibaba-NTU Singapore Joint Research Institute &amp; LILY Research Centre</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Alibaba-NTU Singapore Joint Research Institute &amp; LILY Research Centre</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Pre</surname></persName>
						</author>
						<title level="a" type="main">Pre-training Graph Transformer with Multimodal Side Information for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475709</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation Systems</term>
					<term>Pre-training Model</term>
					<term>Graph Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Side information of items, e.g., images and text description, has shown to be effective in contributing to accurate recommendations. Inspired by the recent success of pre-training models on natural language and images, we propose a pre-training strategy to learn item representations by considering both item side information and their relationships. We relate items by common user activities, e.g., co-purchase, and construct a homogeneous item graph. This graph provides a unified view of item relations and their associated side information in multimodality. We develop a novel sampling algorithm named MCNSampling to select contextual neighbors for each item. The proposed Pre-trained Multimodal Graph Transformer (PMGT) learns item representations with two objectives: 1) graph structure reconstruction, and 2) masked node feature reconstruction. Experimental results on real datasets demonstrate that the proposed PMGT model effectively exploits the multimodality side information to achieve better accuracies in downstream tasks including item recommendation and click-through ratio prediction. In addition, we also report a case study of testing PMGT in an online setting with 600 thousand users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, a good range of recommendation techniques have been proposed, from classic collaborative filtering techniques <ref type="bibr" target="#b29">[30]</ref> to the recent deep learning models <ref type="bibr" target="#b39">[40]</ref>. Besides the interactions between users and items, the multimodality side information of items has also been exploited and showed effectiveness in further improving recommendation accuracy <ref type="bibr" target="#b30">[31]</ref>. Example side information of items include textual descriptions, images, and videos, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). Traditional methods exploit item side information by manual feature engineering <ref type="bibr" target="#b18">[19]</ref>, and then employ factorization machine <ref type="bibr" target="#b28">[29]</ref> or gradient boosting machine <ref type="bibr" target="#b3">[4]</ref> to predict users' preferences on items. These methods often require domain-specific knowledge, and are time-consuming. Deep learning-based methods leverage the strong representation learning ability of neural networks to exploit item side information, for learning the user and/or item representations. However, existing solutions only consider a specific type of side information of items for the dedicated recommendation applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. The full multimodality side information of items are not fully exploited.</p><p>Inspired by the successes of unsupervised pre-training strategies designed for natural language processing <ref type="bibr" target="#b6">[7]</ref> and graph data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>, we propose to develop an unsupervised pre-training framework to fully exploit the multimodality side information for item representations learning. Illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, we construct an item multimodal graph to provide a unified view of items with their associated multimodality side information. In this item multimodal graph, each node is an item and the edges model their relationships (e.g., co-purchase or co-viewership, depending on the application domain). We then pre-train graph neural network (GNN) on this item multimodal graph to enable the GNN model to capture both item relationships and their multimodality side information.</p><p>The unified item multimodal graph well distinguishes our work from previous studies where side information of items and their relationships are studied separately. We argue that these two types of information complement each other in solving recommendation problems. We hence focus on effective pre-training on top of this item multimodal graph to benefit item recommendation. Moreover, we also show that our pre-training benefit other E-commerce applications like click-through ratio (CTR) prediction.  The contributions made in this paper are as follows. First, we propose a novel pre-training framework, namely Pre-trained Multimodal Graph Transformer (PMGT), to exploit items' multimodality information through unsupervised learning. To the best of our knowledge, this is the first deep pre-training method developed to exploit the multimodality side information of items for recommender systems. Second, we decompose the learning objective of PMGT into two sub-objectives: (i) graph structure reconstruction, and (ii) masked node feature reconstruction. To handle large-scale graph data, we develop an algorithm, named Mini-batch Contextual Neighbors Sampling (i.e., MCNSampling), for effective and scalable training. Moreover, we employ the attention mechanism to aggregate the item's multimodality information, and a diversitypromoting Transformer framework to model the influences between an item and its contextual neighbors in the graph. Lastly, to demonstrate the effectiveness of the proposed PMGT model, we conduct extensive experiments on real datasets for different applications. The results on two downstream tasks, i.e., item recommendation and CTR prediction, demonstrate that PMGT is more effective than existing graph-based pre-training methods in fully exploiting items' multimodality information. To further show the effectiveness of PMGT, we report a case study of applying this model in an online E-commerce platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Pre-training methods have been widely applied in computer vision (CV) and natural language processing (NLP) tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. It has been shown that pre-training is effective in boosting performances of various downstream applications. For example, a general pretraining paradigm for CV tasks is firstly training a model on the ImageNet dataset <ref type="bibr" target="#b5">[6]</ref>, and then fine-tuning the pre-trained model for a specific task. NLP is another domain where pre-training is usually adopted. The shallow pre-training methods, e.g., word2vec <ref type="bibr" target="#b22">[23]</ref> and GloVe <ref type="bibr" target="#b24">[25]</ref>, learn word representations based on the word cooccurrence patterns in a corpus of documents. Recently, significant progress has been made in developing deep pre-training models for contextual word representations. For example, ELMo <ref type="bibr" target="#b26">[27]</ref> employs a bidirectional language model to learn high-quality deep contextdependent word representations. The BERT <ref type="bibr" target="#b6">[7]</ref> and XLNET <ref type="bibr" target="#b37">[38]</ref> models use attention mechanisms to learn the word representations.</p><p>Significant improvements are achieved when applying these pretrained models on various NLP tasks.</p><p>On graph data, many embedding techniques have been developed in recent years <ref type="bibr" target="#b2">[3]</ref>. Representative shallow graph embedding methods include TransE <ref type="bibr" target="#b1">[2]</ref>, DeepWalk <ref type="bibr" target="#b25">[26]</ref>, LINE <ref type="bibr" target="#b32">[33]</ref>, and Node2vec <ref type="bibr" target="#b7">[8]</ref>. The recent popularity of GNNs motivates the development of pretraining strategies for GNN models. In general, these methods pretrain GNNs by solving the graph reconstruction problem. For example, Kipf et al. introduce the variational graph autoencoder (VGAE) framework for graph reconstruction <ref type="bibr" target="#b17">[18]</ref>. Hamilton et al. propose a general inductive framework called GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, which exploits node features to generate node embeddings by sampling and aggregating features from a node's local neighborhood. Velickovic et al.</p><p>propose the Deep Graph Infomax model <ref type="bibr" target="#b34">[35]</ref> that aims to maximize the mutual information between the node representations and the representation of the graph. Recently, self-supervised learning <ref type="bibr" target="#b15">[16]</ref> is employed to simultaneously pre-train GNNs at both node and graph levels, and an example is the self-supervised learning method for graph neural networks <ref type="bibr" target="#b13">[14]</ref>. Similarly, the generative framework GPT-GNN <ref type="bibr" target="#b14">[15]</ref> employs a self-supervised attributed graph generation task to pre-train GNNs, by effectively capturing both semantic and structural properties of the graph. In <ref type="bibr" target="#b27">[28]</ref>, a self-supervised graph neural network pre-training model is proposed to capture the universal network topological properties across multiple networks. In <ref type="bibr" target="#b38">[39]</ref>, the proposed Graph-BERT model employs attention mechanism to aggregate the neighborhood information of a target node in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED PRE-TRAINING MODEL</head><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the proposed PMGT framework. Observe that PMGT contains four main components: 1) contextual neighbors sampling, 2) node embedding initialization, 3) transformer-based encoder, and 4) graph reconstruction. Before we detail each component in this section, we provide the preliminary background.</p><p>In this work, we construct a homogeneous graph G = (V, E) to provide a uniform view of items' multimodality side information and their relationships. Here, V denotes the set of nodes (i.e., items), and E denotes the set of edges between them. <ref type="foot" target="#foot_0">1</ref> Each node â„ has multiple types of side information. We denote the ğ‘–-th modality feature of the node â„ by x ğ‘– â„ , and the number of modality by ğ‘š. In G, we denote the one-hop neighbors of â„ by N â„ , and use ğœ” â„ğ‘¡ to denote the weight of the edge between two nodes â„ and ğ‘¡, where ğœ” â„ğ‘¡ &gt; 0. For a node â„, we use C â„ to denote its contextual neighbors selected by a sampling algorithm, e.g., the MCNSampling algorithm. Given the item graph G and the contextual neighbors of each node, PMGT aims to obtain the node representations that can capture the multimodality information of nodes and the graph structure. Then, the learned node representations can be applied in downstream tasks directly or with adjustments such as fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contextual Neighbors Sampling</head><p>For each node â„, there exist some relevant nodes in the graph that may help enrich its representation. These relevant nodes are referred as the contextual neighbors of â„. To efficiently select contextual neighbors for a batch of nodes during the training of PMGT, we develop a sampling algorithm named MCNSampling. MCNSampling iteratively samples a list of nodes for a target node â„ with a predefined sampling depth ğ¾. Let S ğ‘˜âˆ’1 â„ denote the bag of nodes sampled at the (ğ‘˜ âˆ’1)-th step. For each node ğ‘¡ in S ğ‘˜âˆ’1 â„ , we randomly sample ğ‘› ğ‘˜ nodes with replacement from ğ‘¡'s one-hop neighbors N ğ‘¡ at the ğ‘˜-th step. The probability that a node ğ‘¡ â€² âˆˆ N ğ‘¡ being sampled is proportional to the weight ğœ” ğ‘¡ğ‘¡ â€² of the edge between nodes ğ‘¡ and ğ‘¡ â€² . Note that a node may appear multiple times in S ğ‘˜âˆ’1 â„ . In the MCNSampling algorithm, we treat all node instances in S ğ‘˜âˆ’1 â„ as "different nodes" and perform the sampling procedure.</p><p>In our sampling algorithm, we select contextual neighbors by considering 1) the sampled frequency of a node, and 2) the number of sampling steps between the target node â„ and a sampled node in the sampling process. For every node ğ‘¡ âˆˆ V \ â„, we empirically define its importance to the target node â„ at the ğ‘˜-th sampling step (ğ‘˜ â‰¤ ğ¾) as follows,</p><formula xml:id="formula_0">ğ‘  ğ‘˜ ğ‘¡ = ğ‘“ ğ‘˜ ğ‘¡ Ã— (ğ¾ âˆ’ ğ‘˜ + 1),<label>(1)</label></formula><p>where ğ‘“ ğ‘˜ ğ‘¡ denotes the number of times ğ‘¡ appearing in S ğ‘˜ â„ . That is, a node ğ‘¡ is considered more relevant to the target node â„, if ğ‘¡ is sampled more frequently and it has a smaller sampling steps to â„. The final importance score of a node ğ‘¡ to â„ is defined as follows,</p><formula xml:id="formula_1">ğ‘  ğ‘¡ = ğ¾ ğ‘˜=1 ğ‘  ğ‘˜ ğ‘¡ .<label>(2)</label></formula><p>Then, we sort all nodes in V \â„ according to their importance scores in descending order, and choose the ğ‘† top-ranked nodes as the sampled contextual neighbors of â„. The details of the MCNSamping algorithm are summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Node Embedding Initialization</head><p>After the neighborhood sampling, we concatenate the target node â„ and its ordered contextual neighbors C â„ , denoted by</p><formula xml:id="formula_2">I â„ = [â„, â„ 1 , â„ 2 , â€¢ â€¢ â€¢ , â„ ğ‘† ].</formula><p>â„ ğ‘— is the ğ‘—-th node in C â„ , and 1 â‰¤ ğ‘— â‰¤ ğ‘†. For each node ğ‘¡ âˆˆ I â„ , we apply the attention mechanism to obtain its multimodal representation M ğ‘¡ as follows,</p><formula xml:id="formula_3">X ğ‘– ğ‘¡ = x ğ‘– ğ‘¡ W ğ‘– ğ‘€ + b ğ‘– ğ‘€ , 1 â‰¤ ğ‘– â‰¤ ğ‘š X ğ‘¡ = X 1 ğ‘¡ âŠ• X 2 ğ‘¡ âŠ• â€¢ â€¢ â€¢ âŠ• X ğ‘š ğ‘¡ , ğ›¼ ğ‘¡ = softmax tanh(X ğ‘¡ )W ğ‘  + b ğ‘  , M ğ‘¡ = ğ‘š ğ‘– ğ›¼ ğ‘– ğ‘¡ X ğ‘– ğ‘¡ ,<label>(3)</label></formula><p>where W ğ‘– ğ‘€ âˆˆ R ğ‘‘ ğ‘– Ã—ğ‘‘ 0 and b ğ‘– ğ‘€ âˆˆ R 1Ã—ğ‘‘ 0 denote weight matrix and bias term for the ğ‘–-th modality, W ğ‘  âˆˆ R (ğ‘šğ‘‘ 0 )Ã—ğ‘š and b ğ‘  âˆˆ R 1Ã—ğ‘š denote weight matrix and bias term for attention mechanism. âŠ• is the concatenation operation. That is, the multimodality side information of each item is concatenated to contribute to the comprehensive representation learning. ğ›¼ ğ‘– ğ‘¡ denotes the ğ‘–-th element of ğ›¼ ğ‘¡ . The position of a node in the input list I â„ reflects its importance to the target node â„. Thus, we argue that the order of nodes in I â„ is important in learning node representations. The position-id embedding is used to identify the node order information of an input list,</p><formula xml:id="formula_4">P ğ‘¡ = P-Embedding ğ‘ (ğ‘¡) ,<label>(4)</label></formula><p>where ğ‘ (ğ‘¡) denotes the position id of node ğ‘¡ in I â„ , P ğ‘¡ âˆˆ R 1Ã—ğ‘‘ 0 denotes the position-based embedding for ğ‘¡. Our main objective is to obtain the representation of the target node â„. Intuitively, the target node and its contextual neighbors should play different roles in the pre-training. To identify the role differences, we add the following role-based embedding to each node ğ‘¡ âˆˆ I â„ ,</p><formula xml:id="formula_5">R ğ‘¡ = R-Embedding ğ‘Ÿ (ğ‘¡) ,<label>(5)</label></formula><p>where ğ‘Ÿ (ğ‘¡) and R ğ‘¡ âˆˆ R 1Ã—ğ‘‘ 0 denote the role label and role-based embedding of the node ğ‘¡, respectively. In practice, we set the role label of the target node as "Target" and the role labels of the contextual neighbors as "Context". Based on the embeddings stated above, we aggregate them together to define the initial input embedding for a node ğ‘¡ âˆˆ I â„ as,</p><formula xml:id="formula_6">H 0 ğ‘¡ = Aggregate M ğ‘¡ , P ğ‘¡ , R ğ‘¡ .<label>(6)</label></formula><p>In this work, we simply define the Aggregate(â€¢) function as vector summation. The initial input embeddings for the nodes in the input list I â„ can be stacked into a matrix </p><formula xml:id="formula_7">H 0 = [H 0 0 ; H 0 1 ; â€¢ â€¢ â€¢ ; H 0 ğ‘† ] âˆˆ R (ğ‘†+1)Ã—ğ‘‘ 0 ,</formula><formula xml:id="formula_8">S 0 â† [â„], S 1 , S 2 , â€¢ â€¢ â€¢ S ğ¾ â† [ ], ğ‘  ğ‘¡ â† 0 âˆ€ğ‘¡ âˆˆ V \ â„; 4: for ğ‘˜ = 1, 2, â€¢ â€¢ â€¢ , ğ¾ do 5:</formula><p>for ğ‘¡ âˆˆ S ğ‘˜âˆ’1 do Sort the nodes in V \ â„ according to the importance scores in descending order;</p><p>11:</p><p>Choose ğ‘† top-ranked nodes as the contextual neighbors C â„ of â„ and append it to C ğµ ; 12: end for 13: return C ğµ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transformer-based Graph Encoder</head><p>We use the Transformer framework <ref type="bibr" target="#b33">[34]</ref> to model the mutual influences between a node and its contextual neighbors. Given the node representations H â„“âˆ’1 at the (â„“ âˆ’ 1)-th layer, the output at the â„“-th layer of original Transformer model is defined as follows,</p><formula xml:id="formula_9">H â„“ = FFN softmax( QK âŠ¤ ğ‘‘ â„ )V , Q = H â„“âˆ’1 W â„“ ğ‘„ , K = H â„“âˆ’1 W â„“ ğ¾ , V = H â„“âˆ’1 W â„“ ğ‘‰ ,<label>(7)</label></formula><p>where W â„“ ğ‘„ , W â„“ ğ¾ , W â„“ ğ‘‰ âˆˆ R ğ‘‘ 0 Ã—ğ‘‘ 0 denote the weight matrices, FFN(â€¢) is the feed forward network. Here, we omit the residual network in the formula for convenience.</p><p>For the target node â„, there may exist some sampled nodes in C â„ , whose representations are similar to the representation of â„. Assume that all the sampled contextual neighbors are relevant to â„. We hope the proposed model can capture the diversity of the sampled contextual neighbors, by concentrating on the nodes that are relevant but not very similar to the target node. To achieve this objective, we design a diversity-promoting attention mechanism and include it into the attention network of Transformer,</p><formula xml:id="formula_10">S = H â„“âˆ’1 W â„“ ğ‘† , U 1 = softmax E âˆ’ SS âŠ¤ ||S|| 2 ||S|| âŠ¤ 2 + I ,<label>(8)</label></formula><formula xml:id="formula_11">U 2 = softmax( QK âŠ¤ ğ‘‘ â„</formula><p>),</p><formula xml:id="formula_12">H â„“ = FFN (ğ›½U 1 + (1 âˆ’ ğ›½)U 2 )V ,<label>(9)</label></formula><p>where W ğ‘™ ğ‘† âˆˆ R ğ‘‘ 0 Ã—ğ‘‘ 0 is the weight matrix, E âˆˆ R (ğ‘†+1)Ã—(ğ‘†+1) is a matrix where all its elements are 1, ||S|| 2 âˆˆ R (ğ‘†+1)Ã—1 denotes the â„“ 2 row norm of S, and I âˆˆ R (ğ‘†+1)Ã—(ğ‘†+1) denotes the identity matrix.</p><p>In Eq. ( <ref type="formula" target="#formula_10">8</ref>), "-" denotes the element-wise division of two matrices. Note that the larger the similarity between two different nodes, the smaller the attention weight between them in U 1 . The objective of adding I in the definition of U 1 is to include the node's self information. ğ›½ is a constant (0 â‰¤ ğ›½ â‰¤ 1) balancing the contributions of the two attention weights. After obtaining the output H ğ¿ at the last layer of the encoder, we obtain H ğ¿ 0 as the representation of target node â„, denoted as h for simplicity. Then, H ğ¿ will be used in the following pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Optimization</head><p>PMGT is pre-trained with the following two objectives: 1) graph structure reconstruction, and 2) masked node feature reconstruction. To ensure the learned node representations can capture the graph structure, we define the following loss function <ref type="bibr" target="#b8">[9]</ref>,</p><formula xml:id="formula_13">L ğ‘’ğ‘‘ğ‘”ğ‘’ = 1 |V | â„ âˆˆV 1 |N â„ | ğ‘¡ âˆˆN â„ âˆ’ log ğœ ( h âŠ¤ t ||h|| 2 ||t|| 2 ) âˆ’ ğ‘„ â€¢ E ğ‘¡ ğ‘› âˆ¼ğ‘ƒ ğ‘› (ğ‘¡ ) log ğœ (âˆ’ h âŠ¤ t ğ‘› ||h|| 2 ||t ğ‘› || 2 ) ,<label>(10)</label></formula><p>where ğœ (â€¢) is the sigmoid function, ğ‘ƒ ğ‘› and ğ‘„ denote the negative sampling distribution and the number of negative samples. The node feature reconstruction task focuses on capturing the multimodal features in the learned node representations. Previous methods, e.g., GRAPH-BERT <ref type="bibr" target="#b38">[39]</ref>, design an attribute reconstruction task without masking operations. Thus, the models' abilities in aggregating the features of different nodes may be limited. In this work, we design a masked node feature reconstruction task, which aims to reconstruct the features of masked nodes by other non-masked nodes in I â„ . As the representation of the target node â„ is needed to reconstruct the graph structure in Eq. ( <ref type="formula" target="#formula_13">10</ref>), we do not apply the masking operation to the target node â„. Following <ref type="bibr" target="#b6">[7]</ref>, we randomly choose 20% of nodes in the list I â„ \â„ for masking. If the node ğ‘¡ is chosen, we replace ğ‘¡ with: 1) the [Mask] node 80% of the time, (2) a random node 10% of the time, and (3) the unchanged node ğ‘¡ 10% of the time. Then, the masked item list will be input to the model, and the output H ğ¿ will be used to reconstruct the multimodal features of the masked nodes. We set the input features of the [Mask] node to 0, and define the feature reconstruction loss as follows,</p><formula xml:id="formula_14">L ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = 1 |V | â„ âˆˆV 1 |M â„ | ğ‘¡ âˆˆM â„ ğ‘š ğ‘– H ğ¿ ğ‘¡ W ğ‘– ğ‘Ÿ âˆ’ x ğ‘– ğ‘¡ 2 2 ,<label>(11)</label></formula><p>where M â„ denotes the set of masked nodes in I â„ , H ğ¿ ğ‘¡ denotes the representation of ğ‘¡ in H ğ¿ , and W ğ‘– ğ‘Ÿ is the weight matrix for the ğ‘–-th modality information reconstruction.</p><p>The model parameters of PMGT can be learned by minimizing the combined objective function,</p><formula xml:id="formula_15">L ğ‘’ğ‘‘ğ‘”ğ‘’ + ğœ†L ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>The entire framework can be effectively trained by the end-to-end backpropagation algorithm. To make the training of the model more stable, a mini-batch of nodes are randomly sampled to update the model. When applying the pre-trained PMGT model in downstream tasks, the learned node representations can be either fed into the new tasks directly or with necessary adjustment (e.g., fine-tuning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Settings</head><p>4.1.1 Experimental Datasets. The experiments are performed on the Amazon review dataset <ref type="bibr" target="#b23">[24]</ref> and Movielens-20M dataset<ref type="foot" target="#foot_1">2</ref> . Amazon Datasets. We choose the following 5-score review subsets of the Amazon review dataset for experiments, i.e., "Video Games", "Toys and Games", and "Tools and Home Improvement" (respectively denoted by VG, TG, and THI). The metadata of a product includes its text description and the URL of its image <ref type="foot" target="#foot_2">3</ref> , which are used to extract the textual and visual features of the product respectively. In the experiments, we use the rating data generated before 2015-01-01 for building the product graph, and the rating data generated since 2015-01-01 for studying the performance of the item recommendation and CTR prediction tasks. In these two downstream tasks, we convert all the observed review ratings to be positive interactions and filter out the products that are not included in the product graph. Moreover, we build the product graph based on users' review behaviors. Let ğ‘Ÿ â„ğ‘¡ denotes the number of users who have commonly reviewed the two products â„ and ğ‘¡. If ğ‘Ÿ â„ğ‘¡ â‰¥ 3, we connect the two products â„ and ğ‘¡ in the graph. In the graph, there inevitably exist some popular nodes with large degrees. This usually causes that the popular nodes with large degrees are more likely to be sampled in the contextual neighbors sampling procedure. To alleviate this problem, we define the edge weight based on the vertex degrees of an edge. Empirically, we define the weight ğœ” â„ğ‘¡ of the edge ğ‘’ â„ğ‘¡ between the nodes â„ and ğ‘¡ as follows,</p><formula xml:id="formula_17">ğœ” â„ğ‘¡ = log(ğ‘Ÿ â„ğ‘¡ ) + 1 log( ğ‘‘ â„ * ğ‘‘ ğ‘¡ ) + 1 ,<label>(13)</label></formula><p>where ğ‘‘ â„ and ğ‘‘ ğ‘¡ denote the degrees of the nodes (i.e., products) â„ and ğ‘¡ in the graph, respectively. The operation of log(â€¢) alleviates the problem of large variance of edge weights in the product graph. By inversely proportional to the degrees of nodes â„ ad ğ‘¡, we reduce the weights of popular nodes. Movielens-20M Dataset. For the Movielens-20M dataset (denoted by ML), we construct the movie graph based on the tags of movies. The tags of a movie are obtained from the MovieLens Tag Genome Dataset <ref type="foot" target="#foot_3">4</ref> , which includes 11 million computed tag-movie relevance scores from a pool of 1,100 tags applied to 10,000 movies. For each movie, we only keep the tags with relevance scores larger than 0.9. Then, we use ğ‘Ÿ â„ğ‘¡ to denote the number of common tags two movies â„ and ğ‘¡ have. If ğ‘Ÿ â„ğ‘¡ â‰¥ 3, we construct an edge between the two movies â„ and ğ‘¡, and the weight ğœ” â„ğ‘¡ of the edge ğ‘’ â„ğ‘¡ between two nodes â„ and ğ‘¡ is defined following Eq. ( <ref type="formula" target="#formula_17">13</ref>). We collect the movie trailers from Youtube and extract keyframes for each movie trailer. These keyframes are used as the movie images for extracting the visual modality of a movie. The movie descriptions are collected from TMDB <ref type="foot" target="#foot_4">5</ref> . In addition, the rating data generated since 2008-01-01 are used to evaluate the performance of item recommendation and CTR prediction tasks, where we keep ratings larger than 3 as positive interactions. Multimodal Feature Extraction. In the experiments, we use the pre-trained Inception-v4 network <ref type="bibr" target="#b31">[32]</ref> to extract the visual features of each image. Then, we average the visual features of all the images of an item (i.e., product or movie) to obtain its visual modality. For the text description of an item, we utilize the pre-trained BERT model <ref type="bibr" target="#b6">[7]</ref> to extract the features of each sentence. Then, we average the features of all the sentences in the description of an item to obtain its textual modality. Empirically, we set the length of the sentence to 128. For the ML dataset, we separate the audio track from the movie trailer with FFmpeg<ref type="foot" target="#foot_5">6</ref> and adopt VGGish <ref type="bibr" target="#b12">[13]</ref> to obtain the acoustic modality of the movie. The dimensionality of the visual, textual, and acoustic modalities are 1,536, 768, and 128, respectively. Table <ref type="table" target="#tab_2">1</ref> summarizes the statistics of these experimental datasets used for item recommendation and CTR prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Setup and Metrics.</head><p>After the data pre-processing in Section 4.1.1, an item multimodal graph G is built, and a set of user-item interactions D ğ‘‘ğ‘œğ‘¤ğ‘› is prepared for studying downstream tasks.</p><p>In the pre-training task, we randomly keep 90% of nodes and their relationships in G to pre-train the item representations. The remaining 10% of nodes (denoted by V ğ‘£ğ‘ğ‘™ ) are used to construct the validation data for choosing the hyper-parameters of different pre-training models. For each node ğ‘¡ in V ğ‘£ğ‘ğ‘™ , we randomly sample a node ğ‘¡ + from its one-hop neighbors in G, and use the pair (ğ‘¡, ğ‘¡ + ) as a positive example in validation data. And we also randomly sample a node ğ‘¡ âˆ’ that is not connected with ğ‘¡ in G, and use the pair (ğ‘¡, ğ‘¡ âˆ’ ) as a negative example in validation data. The model parameters are chosen based on the AUC (i.e., Area under the ROC Curve) computed on all the validation data.</p><p>For downstream tasks, we choose Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b11">[12]</ref> as the base model for item recommendation task, and Deep &amp; Cross Network (DCN) <ref type="bibr" target="#b35">[36]</ref> as the base model for CTR prediction task. For item recommendation task, we randomly select 80% of the interactions in D ğ‘‘ğ‘œğ‘¤ğ‘› as training data to update the NCF model, and the remaining 20% of interactions in D ğ‘‘ğ‘œğ‘¤ğ‘› are used for testing. Moreover, we also randomly hold out 10% of the training data for tuning the hyper-parameters of NCF. In the training of NCF, for each positive interaction pair, we randomly sample one item that has no interactions with the user as negative feedback to update the model. The item recommendation performances achieved by NCF are evaluated by Recall@10, Recall@20, NDCG@10, and NDCG@20 (respectively denote by REC-R@10, REC-R@20, REC-N@10, and REC-N@20). To improve the evaluation efficiency, we randomly sample 1000 items that the testing user has not interacted with to compute the recall and NDCG. For CTR prediction task, we use D ğ‘‘ğ‘œğ‘¤ğ‘› to simulate the CTR data. For each possible user-item pair in D ğ‘‘ğ‘œğ‘¤ğ‘› , we randomly sample 5 items that have no interactions  <ref type="bibr" target="#b0">[1]</ref> and AliGraph <ref type="bibr" target="#b40">[41]</ref> frameworks. Adam <ref type="bibr" target="#b16">[17]</ref> is used as the optimizer for learning model parameters, and the learning rate is chosen from {10 âˆ’4 , 10 âˆ’3 , 10 âˆ’2 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>After pre-training on the item graph, we use the pre-trained item representations to initialize the item embeddings in the downstream tasks. Then, we train the NCF and DCN models and fine-tune the item embeddings based on the user-item interaction data. Table <ref type="table" target="#tab_3">2</ref> summarizes the performances of NCF and DCN initialized with item representations pre-trained by different methods. We make the following observations. Compared with the random initialization, initializing the base models with pre-trained item representations usually achieves better item recommendation and CTR prediction performances. This demonstrates the pre-training strategies can benefit downstream tasks in recommendation scenarios. The deep pre-training methods GRAPH-BERT, GPT-GNN, and PMGT usually outperform other shallow pre-training methods, by employing GNN to aggregate the neighbor information, and using node feature reconstruction and graph structure reconstruction tasks to pretrain the model. Moreover, PMGT usually achieves the best item recommendation and CTR prediction performances on all datasets. This demonstrates the effectiveness of PMGT in exploiting the item graph structure and item features. In addition, we can also note that PMGT achieves smaller improvements on the ML dataset. One potential reason is that the interaction data of the ML dataset are denser. Thus, with the random initialization, the base models can learn sufficiently good item representations for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We also study the effectiveness of PMGT in exploiting different modality information. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, we can note that the original methods considering multimodality information usually   outperform the variants that only consider single modality information. This observation is as expected. It indicates that representing items with multimodality information can achieve better performance. PMGT is superior to GRAPH-BERT and GPT-GNN with considering single modality information in most scenarios. This observation again demonstrates that PMGT is more effective in capturing different types of modality information than baseline methods. Moreover, we also study the effectiveness of the two graph reconstruction tasks in learning node representations. Figure <ref type="figure" target="#fig_4">4</ref> summarizes the performances of PMGT variants on the TG and ML datasets. We can note that the original PMGT model with two tasks consistently outperforms the variants using a single task as the pre-training objective. This indicates that both the graph structure reconstruction task and the masked node feature reconstruction task are essential for learning useful node representations to benefit downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Sensitivity Study</head><p>In this section, we study the performances of PMGT with respect to (w.r.t.) different settings of three important hyper-parameters. Firstly, we vary the number of Transformer layers ğ¿ from 1 to 5. As shown in Figure <ref type="figure" target="#fig_5">5a</ref>, the best item recommendation and CTR prediction performances are achieved by setting ğ¿ to 3 and 2, respectively. Further stacking more layers does not help improve the performances of downstream tasks. Moreover, we vary the weight of the diversity-promoting attention score ğ›½ in {0, 0.2, 0.5, 0.8, 1.0}. As shown in Figure <ref type="figure" target="#fig_5">5b</ref>, the recommendation accuracy can be improved by considering diversity-promoting attention in the Transformer-based encoder, when ğ›½ is set to 0.5, 0.8, and 1.0. This indicates it is important to consider the diversity of contextual neighbors when learning the node representations. Moreover, we also note that the best performances of both downstream tasks are achieved by setting ğ›½ to 1.0. However, more experiments are needed to study whether this is a general observation on different datasets.</p><p>In the current implementation, we keep ğ›½ to enable the flexibility of PMGT. Figure <ref type="figure" target="#fig_5">5c</ref> summarizes the performances of PMGT w.r.t. different settings of the number of contextual neighbors ğ‘†. We observe that PMGT usually achieves good performances by setting ğ‘† to a small value (e.g., 5 and 10). This indicates that a small number of contextual neighbors can capture the important neighborhood information of a node. Further increase of ğ‘† tends to include noise information, thus may not help improve the model performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Convergence Speed Study</head><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the convergence speed of the training losses of both NCF and DCN models on the TG dataset. We initialize the item embeddings in the NCF and DCN models by the following strategies: 1) random initialization, 2) initializing using the representations pre-trained by GRAPH-BERT, 3) initializing using the representations pre-trained by GPT-GNN, and 4) initializing using the representations pre-trained by PMGT. We can make the following observations. Compared with the random initialization, initializing item embeddings with the pre-trained representations achieves faster convergence speed. This once again demonstrates the effectiveness of the pre-training strategies. Moreover, both NCF and DCN models achieve the fastest convergence speed by using pre-trained representations by PMGT. The pre-trained PMGT model gives the downstream model a good initial state, thus can help the downstream model achieves faster and earlier convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY IN ONLINE PLATFORM</head><p>A case study is conducted in the video recommendation scenario of one of the world's largest E-commerce platforms. The video graph is built based on users' watching behaviors. Let ğ‘Ÿ â„ğ‘¡ denote the number of users who have watched the videos â„ and ğ‘¡ within one hour. If ğ‘Ÿ â„ğ‘¡ â‰¥ 10, we build an edge between the nodes â„ and ğ‘¡ in the video graph. The weight of the edge ğ‘’ â„ğ‘¡ between â„ and ğ‘¡ is defined following Eq. ( <ref type="formula" target="#formula_17">13</ref>). Finally, there are about 4 million nodes and 500 millions of edges in the video graph used for this case study. Given the pre-trained video representations by PMGT, for a user, we retrieve 50 most similar videos for each video she has watched, based on the Cosine similarity between the video representations. Then, ItemKNN is used to rank and recommend the retrieved videos to the user. After three days of online testing, for 600 thousand users, the number of new video plays increases by 6.80%, compared with the online baseline method using the video representation learned by UNITER <ref type="bibr" target="#b4">[5]</ref>. Figure <ref type="figure" target="#fig_8">7</ref> shows two video retrieval examples. We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>This paper proposes a novel pre-training GNN framework, named PMGT (i.e., Pre-trained Multimodal Graph Transformer), which exploits items' multimodal information guided by the unsupervised learning tasks on graph. Two graph reconstruction tasks, i.e., graph structure reconstruction and masked node feature reconstruction, are used as learning objectives to pre-train the model. The learned representation of an item not only integrates the multimodal information of the item itself but also aggregates the information of its contextual neighbors in the graph. The superiority of PMGT has been validated by two downstream tasks (i.e., item recommendation and CTR prediction) on real datasets. In this work, we focus on the homogeneous graph of items. For future work, we would like to investigate how to extend the proposed model to process the heterogeneous item graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example about item multimodal graph. Here, (a) shows the uses' purchasing history, and (b) shows the item multimodal graph built based on the co-purchase pattern between items. The weight of an edge in the item multimodal graph includes the co-purchase times between two items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Item side information and users' purchasing history, and (b) Item multimodal graph built on co-purchase relationship. In this graph, each node denotes an item with its visual and textual features extracted from the image and text description respectively. An edge between two items is weighed by the number of co-purchases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) An overview of the proposed PMGT framework. PMGT contains four components (illustrated from the left to the right): contextual neighbors sampling, node embedding initialization, transformer-based graph encoder, and graph reconstruction. GSR and NFR, in the last step, denote the graph structure reconstruction task and the masked node feature reconstruction task, respectively. (b) The node embedding is initialized by considering the node's multimodal features, position-id embedding, and role-label embedding.</figDesc><graphic url="image-4.png" coords="3,135.12,95.60,68.69,83.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performances of PMGT, GRAPH-BERT and GPT-GNN considering different modality information on TG and ML datasets. V, T, A denotes the visual, textual, and acoustic modality information respectively. O denotes original models considering all the modality information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performances of PMGT considering different graph reconstruction tasks on TG and ML datasets. NFR and GSR denote PMGT only considering the node feature reconstruction task and graph structure reconstruction task, respectively. O denotes the original PMGT considering both tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance trend of PMGT with respect to different settings of ğ¿, ğ›½, and ğ‘† on TG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The convergence speed of the training loss of (a) NCF model and (b) DCN model on the TG dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of video retrieval based on the video representations pre-trained by PMGT and UNITER. can note that the videos retrieved based on the representations pre-trained by PMGT are more diverse than those retrieved based on the representations pre-trained by UNITER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where H 0 0 corresponds to the target node â„. Graph G, batch of nodes B, sampling depth ğ¾, sampling size {ğ‘› ğ‘˜ } ğ¾ ğ‘˜=1 , number of contextual neighbors ğ‘†; Output: Sampled contextual neighbors C ğµ ;</figDesc><table><row><cell>Algorithm 1 MCNSampling Algorithm</cell></row><row><cell>Input:</cell></row></table><note>1: C ğµ â† [ ]; 2: for â„ âˆˆ B do 3:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Count the frequency ğ‘“ ğ‘˜ ğ‘¡ of each distinct node ğ‘¡ in S ğ‘˜ and update its score ğ‘  ğ‘¡ â† ğ‘  ğ‘¡ + ğ‘“ ğ‘˜ ğ‘¡ * (ğ¾ âˆ’ ğ‘˜ + 1);</figDesc><table><row><cell>6:</cell><cell>Sample ğ‘› ğ‘˜ nodes with replacement from N ğ‘¡ and append</cell></row><row><cell></cell><cell>them to S ğ‘˜ ;</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell>8:</cell><cell></cell></row><row><cell>9:</cell><cell>end for</cell></row><row><cell>10:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the experimental datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="5">Data for Downstream Tasks # Users # Items # Interact. # Nodes # Edges Item Graph</cell></row><row><cell>VG</cell><cell>4,525</cell><cell>3,921</cell><cell>27,780</cell><cell>5,032</cell><cell>83,981</cell></row><row><cell>TG</cell><cell>31,109</cell><cell>13,870</cell><cell>182,744</cell><cell cols="2">17,388 232,720</cell></row><row><cell>THI</cell><cell>20,082</cell><cell>11,170</cell><cell>109,717</cell><cell cols="2">15,619 178,834</cell></row><row><cell>ML</cell><cell>27,715</cell><cell>4,253</cell><cell>2,179,386</cell><cell cols="2">4,271 249,498</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performances of item recommendation (REC) and CTR prediction by using different pre-training methods. Best results are in boldface and second best underlined. to construct the negative samples of the CTR data. All the interaction pairs in D ğ‘‘ğ‘œğ‘¤ğ‘› are used as the positive examples of the CTR data. Then, we randomly sample 80% of the CTR data to train the DCN model, and use the remaining 20% of CTR data to testing the CTR prediction performances. Moreover, 10% of the training data are also held out for tuning the hyper-parameters of DCN. The CTR prediction performances are evaluated by AUC (denoted by CTR-AUC). For the pre-training and downstream tasks, we set the dimensionality of latent space ğ‘‘ 0 to 128. In the experiments, we empirically set the sampling depth ğ¾ to 3, and the sampling sizes ğ‘› 1 , ğ‘› 2 , ğ‘› 3 to 16, 8, 4 respectively. The number of contextual neighbors ğ‘† is selected from {5, 10, 20, 30, 40}. The number of transformer layers ğ¿ is chosen from {1, 2, 3, 4, 5}. The weight of diversity-promoting attention ğ›½ is selected from {0, 0.2, 0.5, 0.8, 1.0}. ğœ† is empirically set to 1. We implement PMGT based on TensorFlow</figDesc><table><row><cell cols="2">Datasets Metrics</cell><cell cols="8">Random DeepWalk LINE TransAE GraphSAGE GRAPH-BERT GPT-GNN PMGT</cell></row><row><cell></cell><cell>REC-R@10</cell><cell>0.1994</cell><cell>0.2236</cell><cell>0.2234</cell><cell>0.1989</cell><cell>0.2052</cell><cell>0.2380</cell><cell>0.2251</cell><cell>0.2480</cell></row><row><cell></cell><cell>REC-N@10</cell><cell>0.1278</cell><cell>0.1441</cell><cell>0.1420</cell><cell>0.1217</cell><cell>0.1301</cell><cell>0.1491</cell><cell>0.1343</cell><cell>0.1625</cell></row><row><cell>VG</cell><cell>REC-R@20</cell><cell>0.2742</cell><cell>0.3179</cell><cell>0.3169</cell><cell>0.2903</cell><cell>0.2821</cell><cell>0.3330</cell><cell>0.3269</cell><cell>0.3405</cell></row><row><cell></cell><cell>REC-N@20</cell><cell>0.1494</cell><cell>0.1711</cell><cell>0.1690</cell><cell>0.1480</cell><cell>0.1522</cell><cell>0.1767</cell><cell>0.1636</cell><cell>0.1890</cell></row><row><cell></cell><cell>CTR-AUC</cell><cell>0.7311</cell><cell>0.768</cell><cell>0.7762</cell><cell>0.7675</cell><cell>0.7674</cell><cell>0.7746</cell><cell>0.7839</cell><cell>0.7990</cell></row><row><cell></cell><cell>REC-R@10</cell><cell>0.2147</cell><cell>0.2787</cell><cell>0.2805</cell><cell>0.2137</cell><cell>0.2391</cell><cell>0.2858</cell><cell>0.2598</cell><cell>0.3032</cell></row><row><cell></cell><cell>REC-N@10</cell><cell>0.1380</cell><cell>0.1847</cell><cell>0.1857</cell><cell>0.1358</cell><cell>0.1514</cell><cell>0.1942</cell><cell>0.1671</cell><cell>0.2056</cell></row><row><cell>TG</cell><cell>REC-R@20</cell><cell>0.3068</cell><cell>0.3807</cell><cell>0.3873</cell><cell>0.3051</cell><cell>0.3352</cell><cell>0.3808</cell><cell>0.3608</cell><cell>0.4030</cell></row><row><cell></cell><cell>REC-N@20</cell><cell>0.1644</cell><cell>0.2141</cell><cell>0.2162</cell><cell>0.1620</cell><cell>0.1790</cell><cell>0.2215</cell><cell>0.1962</cell><cell>0.2342</cell></row><row><cell></cell><cell>CTR-AUC</cell><cell>0.8047</cell><cell>0.8289</cell><cell>0.8326</cell><cell>0.8214</cell><cell>0.8266</cell><cell>0.8322</cell><cell>0.8328</cell><cell>0.8370</cell></row><row><cell></cell><cell>REC-R@10</cell><cell>0.1957</cell><cell>0.2043</cell><cell>0.1626</cell><cell>0.1425</cell><cell>0.1921</cell><cell>0.2023</cell><cell>0.1680</cell><cell>0.2358</cell></row><row><cell></cell><cell>REC-N@10</cell><cell>0.1360</cell><cell>0.1399</cell><cell>0.0996</cell><cell>0.0861</cell><cell>0.1320</cell><cell>0.1462</cell><cell>0.1047</cell><cell>0.1707</cell></row><row><cell>THI</cell><cell>REC-R@20</cell><cell>0.2555</cell><cell>0.2756</cell><cell>0.2403</cell><cell>0.2191</cell><cell>0.2577</cell><cell>0.2627</cell><cell>0.2451</cell><cell>0.3025</cell></row><row><cell></cell><cell>REC-N@20</cell><cell>0.1529</cell><cell>0.1598</cell><cell>0.1214</cell><cell>0.1077</cell><cell>0.1505</cell><cell>0.1632</cell><cell>0.1264</cell><cell>0.1895</cell></row><row><cell></cell><cell>CTR-AUC</cell><cell>0.7652</cell><cell>0.7850</cell><cell>0.7896</cell><cell>0.7815</cell><cell>0.7643</cell><cell>0.7878</cell><cell>0.7817</cell><cell>0.7933</cell></row><row><cell></cell><cell>REC-R@10</cell><cell>0.3104</cell><cell>0.3144</cell><cell>0.3112</cell><cell>0.3096</cell><cell>0.3139</cell><cell>0.3136</cell><cell>0.3162</cell><cell>0.3161</cell></row><row><cell></cell><cell>REC-N@10</cell><cell>0.4714</cell><cell>0.4793</cell><cell>0.4763</cell><cell>0.4703</cell><cell>0.4769</cell><cell>0.4781</cell><cell>0.4823</cell><cell>0.4814</cell></row><row><cell>ML</cell><cell>REC-R@20</cell><cell>0.4556</cell><cell>0.4618</cell><cell>0.4589</cell><cell>0.4549</cell><cell>0.4606</cell><cell>0.4581</cell><cell>0.4628</cell><cell>0.4640</cell></row><row><cell></cell><cell>REC-N@20</cell><cell>0.4829</cell><cell>0.4911</cell><cell>0.4881</cell><cell>0.4822</cell><cell>0.4888</cell><cell>0.4883</cell><cell>0.4928</cell><cell>0.4928</cell></row><row><cell></cell><cell>CTR-AUC</cell><cell>0.9109</cell><cell>0.9218</cell><cell>0.9200</cell><cell>0.9194</cell><cell>0.9145</cell><cell>0.9184</cell><cell>0.9191</cell><cell>0.9205</cell></row><row><cell cols="5">with the user 4.1.3 Baseline Methods. We compare the proposed PMGT model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">with the following pre-training methods: 1) Random: The item em-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">beddings in the downstream tasks are randomly initialized; 2) Deep-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Walk [26]: This method learns node representations by sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">a large number of paths in the graph and maximizing the average</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">logarithmic probability of all vertex context pairs in sampled paths;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">3) LINE [33]: This graph embedding method is trained to preserve</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">the first-and second-order proximities of nodes in the graph; 4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">GraphSAGE [9]: This GNN model forces connected nodes to have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">similar embeddings by aggregating the information of neighbor-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ing nodes; 5) TransAE [37]: This method combines a multimodal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">encoder and the TransE [2] model to learn the node representa-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tions; 6) GRAPH-BERT [39]: This method applies Transformer to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">aggregate neighbors' information without masking operations on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">the nodes; 7) GPT-GNN [15]: This method employs the attribute</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">generation and edge generation tasks to pre-train the GNN model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">For a fair comparison, we use the same multimodal representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">in Eq. (3) as the inputs for all pre-training methods, except Random.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1.4 Implementation Details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In the context of recommendation, the relationships between items can be defined by their interactions with users, e.g., co-purchase or co-click. More details about the construction of item graph are presented in Section 4.1.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://grouplens.org/datasets/movielens/20m/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://nijianmo.github.io/amazon/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://grouplens.org/datasets/movielens/tag-genome/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://www.themoviedb.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">http://ffmpeg.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported, in part, by Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI), Nanyang Technological University, Singapore. This research is also supported, in part, by the National Research Foundation, Prime Minister's Office, Singapore under its AI Singapore Programme (AISG Award No: AISG-GC-2019-003) and under its NRF Investigatorship Programme (NRFI Award No. NRF-NRFI05-2019-0002). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">MartÃ­n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pretraining</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
				<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Repeat buyer prediction for e-commerce</title>
		<author>
			<persName><forename type="first">Guimei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianneng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextualized Graph Attention Network for Recommendation with Item Knowledge Graph</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Review Graph Representations for Recommendation</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to pretrain graph neural networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4276" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Research commentary on recommendations with side information: A survey and research directions</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">100879</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
				<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal data enhanced representation learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Zikang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiudan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks. IEEE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">AliGraph: a comprehensive graph neural network platform</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2094" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
