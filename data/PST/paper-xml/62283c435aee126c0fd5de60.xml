<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Learning for Temporally Coherent Talking Face Generation With Articulator Synergy</title>
				<funder ref="#_bzRHgbz">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_rfeHBtC">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_mgjHGbc #_42h7dHw #_agWQF4D #_UMh4cgn">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Ur8PjnQ">
					<orgName type="full">Anhui Postdoctoral Research Activities Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
							<idno type="ORCID">0000-0001-6403-761X</idno>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
							<email>htxie@ustc.edu.cn</email>
							<idno type="ORCID">0000-0002-6249-5315</idno>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0002-1151-1792</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Hongtao Xie, Yongdong Zhang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Learning for Temporally Coherent Talking Face Generation With Articulator Synergy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2021.3091863</idno>
					<note type="submission">received February 7, 2021; revised May 27, 2021; accepted June 14, 2021. Date of publication June 25, 2021; date of current version June 9, 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Articulator synergy</term>
					<term>multimodal learning</term>
					<term>talking face generation</term>
					<term>adversarial training</term>
					<term>video synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Talking face generation is a demanding task to synthesize a high quality video with accurate lip synchronization and rhythmic head motion. However, existing methods always suffer from unrealistic facial animations, because 1) they only take single-mode input, but ignore the complementarity of multimodal inputs for lip-sync improvement; 2) they only explore lip movements, but ignore the articulator synergy between lips and jaw; 3) they generate each video frame in a temporalindependent way, but ignore the temporal continuity among the entire video. To address these limitations, in this paper, we present a novel method to generate realistic and temporally coherent talking heads by considering multimodal inputs, articulator synergy, inter-frame consistency and intra-frame consistency. Firstly, for landmark prediction, a novel Multiple Synergy Network (MSN) is proposed to improve the accuracy of landmark prediction by incorporating multimodal inputs (i.e., audio and text inputs). Besides, instead of merely considering lip landmarks, we also explore the jaw movements to ensure articulator synergy among lips and jaw. Secondly, for realistic video generation, a Video Consistency Network (VCN) is proposed conditioned on the predicted landmarks. In VCN, the optical flow is adopted to model the temporal continuity between frames to ensure inter-frame consistency. Meanwhile, a mouth generation branch is proposed to enhance mouth texture and the corresponding mouth mask is employed to ensure intra-frame consistency between the mouth area and the others. Extensive experiments demonstrate that our approach exhibits excellent superiority on lip-sync and can generate photo-realistic facial animations. Project is available at http://imcc.ustc.edu.cn/project/tfgen/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a wide range of applications in multimedia, e.g., video editing <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b7">[8]</ref>, visual dubbing <ref type="bibr" target="#b8">[9]</ref>, bandwidth reduction in video coding/transmission <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref> and human-computer interaction <ref type="bibr" target="#b13">[14]</ref>. However, generating realistic talking heads is multifaceted and especially challenging, requiring accurate mouth movements synced with audio/text input, plausible articulator synergy and temporal continuity between frames.</p><p>The majority of this research involves single-modal analysis of facial animation, which focuses on mapping audio to realistic facial movements <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b21">[22]</ref>, as shown in Fig. <ref type="figure" target="#fig_9">1 (a)</ref>. Besides, a few works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> also utilize text input to generate realistic facial videos via introducing a text-to-speech synthesizer or building the visemes. However, in talking face generation, the accuracy and synchronization of mouth movements are especially essential for the lip-sync. Theoretically, in human speech production, it is the movements of articulators, e.g., the tongue, jaw and lips, that generate the acoustic signal. Meanwhile, audio and text have significant information correspondence, and the acoustic signal is time alignment with the text information. Thus, mouth movements are temporal synchrony between audio and text inputs, and both inputs have a close relationship with mouth movements <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Lacking the complementarity of multimodal inputs, the aforementioned methods suffer from low accuracy in landmark prediction and unsatisfactory visual quality in facial animations.</p><p>Moreover, in speech production, the human body system organizes articulators, such as the jaw and lips, into synergies to produce speech sounds by articulatory movements <ref type="bibr" target="#b24">[25]</ref>. This mechanism is called articulator synergy <ref type="bibr" target="#b25">[26]</ref>. However, existing methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref> tend to merely study the lip movements (shown in Fig. <ref type="figure" target="#fig_9">1 (a)</ref>) and ignore other regions (e.g., the jaw), which are also closely relevant to the speech content and move consistently with the lip movements. As shown in <ref type="bibr">Fig 2,</ref><ref type="bibr"></ref> when only the lip movements are considered, the generated frames suffer from unrealistic philtrum and movement inconsistency between jaw and lips. This phenomenon is contrary to the laws of physiological movements of human body and results in unrealistic facial animation generation. Essentially, facial animation synthesis is a temporaldependent problem, whereas existing works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> tend to formulate it as a temporal-independent image generation problem (shown in Fig. <ref type="figure" target="#fig_9">1 (a)</ref>). Without considering the temporal dependency between adjacent frames, these methods often suffer from temporally incoherent videos with pixel jitter and unsatisfactory visual quality. Thus, to tackle these problems, more attempts employ a temporal generative adversarial network (GAN) to model the temporal dynamics <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, or propose a dynamically adjustable pixel-wise loss with an attention mechanism to solve the pixel jittering problem <ref type="bibr" target="#b20">[21]</ref>. These methods can generate lip-synced, temporally coherent talking face videos from an audio signal and a single still image. However, they pay more attention to the synchronization and bypass the unconscious head movements/expressions, resulting in generating unreal facial animations with rigid lip motion and unchanged facial expression/pose. Besides, to generate complex head motions and poses, related methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> decouple appearance and motion information, and use a dense motion field to generate temporally coherent videos via image warping. Nevertheless, animating a still image by a driving video, the generated videos tend to contain noticeable artifacts and pixel jitter when the subjects have a large motion.</p><p>As discussed above, 1) existing methods merely focus on single-modal learning, while ignoring the complementarity of multimodal inputs to improve the lip-sync. 2) These methods focus on exploring the correlation between the lip movements and audio/text input, but ignore the jaw movements, which violates the mechanism of articulator synergy. 3) They tend to regard this task as a temporal-independent image generation problem, without considering the temporal dependency between frames. Consequently, these methods suffer from unrealistic talking face generation with obvious pixel jitter and inconsistency between jaw and lips.</p><p>In this paper, we propose a novel method including two steps, (i) landmark prediction of lips and jaw given multimodal inputs, and (ii) temporally coherent facial animation generation conditioned on the predicted landmarks. Fig. <ref type="figure">3</ref> shows the overall framework of the proposed method. For the first step, a novel network Multiple Synergy Network (MSN) is proposed to accurately predict landmarks given multimodal context of text and audio. The proposed MSN, adopting an encoder-decoder architecture based on recurrent neural network (RNN), can effectively process sequential data and integrate various input modalities to enhance the prediction precision. Besides, according to the mechanism of articulator synergy, the jaw and lips move synchronously during speaking. Thus, in this step, not only lips but also jaw is studied to explore the synchronization between audio/text input. For clarity, the lip landmarks and jaw landmarks are collectively referred to as mouth landmarks in the following. For the second step, conditioned on the predicted mouth landmarks, a novel network Video Consistency Network (VCN) is proposed to generate lip-synced, temporal continuity facial animations. In VCN, the optical flow <ref type="bibr" target="#b28">[29]</ref> is introduced to model the temporal dependency between frames for inter-frame consistency. This flow can avoid the pixel jittering problem and maintain the smooth transition of facial movements over the entire animations. In general, the optical flow can be estimated accurately on the background regions with global transformation, but it is difficult to be estimated accurately on the regions (e.g., the mouth area), which often have a large motion and only occupy a small portion of a facial image <ref type="bibr" target="#b29">[30]</ref>. Consequently, the generated mouth area tends to be blurry, leading to the intra-frame inconsistency with obvious texture differences between mouth area and the other areas. Therefore, to enhance the mouth texture, a mouth generation branch, with the corresponding mouth mask, is proposed to generate realistic mouth regions. The mouth mask is employed to blend the mouth area, generated from the mouth generation branch, with the other facial areas warped by the optical flow. In this way, the mouth generation branch with the corresponding mouth mask, can not only generate realistic mouth regions, but also ensure the facial texture consistency and movement synergy between the mouth area and the other areas for the intra-frame consistency.</p><p>Overall, with the multimodal inputs, articulator synergy, and inter-frame/intra-frame consistencies, our method can generate plausible video frames with accurate lip movements. Our contributions are summarized as follows.</p><p>1) We study the complementarity of multimodal inputs in talking face generation, and a novel MSN is proposed to enhance the accuracy of landmark prediction. 2) We explore the articulator synergy mechanism in this task, and not only lip movements but also jaw movements are studied to explore the synchronization. 3) We focus on studying the temporal coherence in talking face generation, and a novel network VCN is proposed to generate photo-realistic, lip-synced and smooth transition videos by considering the inter-frame consistency and intra-frame consistency.</p><p>Fig. <ref type="figure">3</ref>. Our approach includes two steps: mouth shape prediction and video synthesis. For mouth shape prediction, the MSN is proposed to predict lip and jaw landmarks with both text and audio inputs. For video synthesis, conditioned on the predicted landmarks and target videos, we can obtain a sequence of manipulated face sketches that have natural head poses and background derived from target videos and accurate mouth movements synced with audio/text input. Then the VCN is proposed to synthesize realistic facial animations considering the intra-frame and inter-frame consistencies.</p><p>The remainder of this paper is organized as follows. Section II reviews the related work. Section III introduces the details of our method, including mouth landmark prediction and video synthesis. Section IV presents the experimental setting and results, followed by the conclusion in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, three related works, including audio-visual learning, speech-driven facial animation and GAN-based video synthesis, are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Audio-Visual Learning</head><p>With the emergence of deep neural networks, bridging the correspondences between auditory and visual signals has attracted increasing interest in recent years. The synchronization between audio and visual information has been leveraged for diverse tasks <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b38">[39]</ref>. Given large quantities of unlabeled videos, related works <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref> present a joint audio-visual model to isolate each single speech signal from a mixture of sounds for sound separation and explicitly discover objects that produce sounds for sound localization <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Instead of relying on image semantics, Zhao et al. <ref type="bibr" target="#b36">[37]</ref> introduce the dense optical flow to model the motion information with dense trajectories for sound separation. This method is able to separate sounds from duets of the same instruments through curriculum learning, which is impossible for the purely appearance-based approaches. Different from <ref type="bibr" target="#b36">[37]</ref>, Gan et al. <ref type="bibr" target="#b33">[34]</ref> propose a keypoint-based structured representation to explicitly model the body dynamics. Compared to previous appearance and low-level motion-based models, this method contributes to improving the performance on audio-visual source separation.</p><p>In addition to sound localization and sound separation, some other interesting audio-visual researches include plausible music or sound synthesis for silent videos <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref>, binaural audio synthesis by leveraging video <ref type="bibr" target="#b34">[35]</ref>, multi-modal action recognition <ref type="bibr" target="#b31">[32]</ref> and talking face generation <ref type="bibr" target="#b27">[28]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speech-Driven Facial Animation Generation</head><p>Speech-driven facial animation generation aims to transfer an audio clip into the visual information of a target character with accurate lip synchronization <ref type="bibr" target="#b39">[40]</ref>. Given an audio clip, Suwajanakorn et al. <ref type="bibr" target="#b14">[15]</ref> first introduce a time-delayed long short-term memory (LSTM) to convert audio input into sparse mouth shapes, and generate photo-realistic mouth texture. Then they compose the generated lip regions with retrieved frames from a large video corpus of the target person to produce photorealistic videos. Even though this method can synthesize fairly high-quality, lip-synced videos of Obama, it is difficult to adapt to other persons due to the rigid matching scheme, which restricts the generalization capability. Similarly, Kumar et al. <ref type="bibr" target="#b17">[18]</ref> propose a ObamaNet, that first predicts mouth landmarks by LSTM, then employs a GAN-based network <ref type="bibr" target="#b40">[41]</ref> to generate each frame independently. However, without considering the temporal coherence, this work often results in temporally incoherent videos with obvious pixel jitter. To model the temporal dynamics, some works <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> use a RNN-based generator and a sequence discriminator to smooth the facial movement transition. Besides, Chen et al. <ref type="bibr" target="#b0">[1]</ref> adopt a cascade GAN approach to generate realistic facial videos, and propose a dynamically adjustable pixel-wise loss with an attention mechanism to avoid pixel jitter. However, given limited facial samples, these methods suffer from unchanged facial expression/pose and unsatisfactory visual quality.</p><p>Inspired by the methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we also utilize landmarks as the intermediate representations to bridge the gap between audio/text input and video frames. However, different from these methods, not only lips but also jaw is studied to explore the correlation between inputs for articulator synergy. Besides, to improve the prediction accuracy, a novel MSN is proposed for landmark prediction with multimodal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GAN-Based Video Synthesis</head><p>Recent advances in deep generative models, especially GAN <ref type="bibr" target="#b41">[42]</ref>, have become popular tools for manipulating or generating images/videos with a high level of realism. For instance, Wang et al. <ref type="bibr" target="#b29">[30]</ref> propose a novel coarse-to-fine network to learn a mapping from a source video to a photo-realistic video. To model the temporal dependency, they introduce optical flow to warp the current frame to generate an estimation of the next frame, and propose a conditional video discriminator to ensure the smooth transition of consecutive output frames. Additionally, Wiles et al. <ref type="bibr" target="#b3">[4]</ref> propose a network X2Face that uses a dense motion field to generate the output video via image warping, and show the generation process driven by different modalities, such as driving frames, audio or pose codes. However, even though this method allows for the generation to be conditioned on different modalities, the generation quality is still unsatisfactory, such as noticeable pixel jitter, blurry mouth area and distorted face when the subjects have a large motion.</p><p>In this paper, a network called VCN is proposed to generate photo-realistic video frames conditioned on the predicted mouth landmarks. In VCN, the optical flow <ref type="bibr" target="#b29">[30]</ref> is employed to model the temporal dependency between video frames for the inter-frame consistency. Besides, to enhance local mouth texture, a novel mouth generation branch is proposed to generate the mouth region and ensure the intra-frame consistency between the mouth region and the others by adopting the corresponding mouth mask. In this way, our proposed method is able to generate high-resolution, lip-sync and temporally coherent facial animations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>To generate high-quality facial animations, our proposed method is mainly split into two steps, mouth landmark prediction and video synthesis, as shown in Fig. <ref type="figure">3</ref>. Each step is described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mouth Landmark Prediction</head><p>For mouth landmark prediction, a network MSN is proposed to predict the mouth landmarks given multimodal inputs. In this section, we first describe the data processing by representing the audio using acoustic features, the text using linguistic features and the mouth shape by 33 landmarks. And then we describe the proposed MSN for landmark prediction in detail.</p><p>1) Data Processing: For acoustic features, 40-dimensional Mel-Cepstral Coefficients (MCEP) <ref type="bibr" target="#b42">[43]</ref> and 1-dimensional logenergy, with delta and delta-delta features, are extracted by the STRAIGHT vocoder <ref type="bibr" target="#b43">[44]</ref>. The acoustic features are normalized to zero mean and unit variance <ref type="bibr" target="#b23">[24]</ref>.</p><p>For linguistic features, 416-dimension binary features and 9-dimension numerical features are extracted from text by Merlin <ref type="bibr" target="#b44">[45]</ref>, an open-source text-to-speech (TTS) toolkit. More specifically, the fully-context labels, formatted as Hidden Markov Model (HMM)-Based Speech Synthesis Toolkit (HTS)-style labels,<ref type="foot" target="#foot_1">1</ref> are firstly generated by force-alignment with Hidden Markov Model Toolkit <ref type="bibr" target="#b45">[46]</ref>. In such labels, we can obtain the fully-context phoneme identities, including the short pause information, and abundant linguistic features. Besides, the phone-level time boundaries, containing the start time and end time, are also indicated, which is crucial to realize the time alignment between phonemes and mouth landmarks. Secondly, the Merlin converts the fully-context labels into vectors of binary and continuous linguistic features by using the HTS-style questions. Among the linguistic features, the 416-dimension binary features include abundant linguistic contexts, like the phoneme identities, the position information of phoneme, syllable and word, etc. And the 9-dimensional numerical features include the position information, such as the state position within phoneme, the frame position within state and phoneme, and the duration of state and phoneme. The overall linguistic features are normalized to the range of [0.01, 0.99] <ref type="bibr" target="#b44">[45]</ref>. For mouth landmarks, to make the face invariant to image size, location and rotation, the face in each video frame is firstly detected and frontalized using a single, unmodified 3D surface <ref type="bibr" target="#b46">[47]</ref>. And then, for each frontalized face, the mouth landmarks are detected by dlib detector <ref type="bibr" target="#b47">[48]</ref> which gives 33 keypoints along the outer and inner contours of lips and jaw (the red points in Fig. <ref type="figure" target="#fig_2">4 (b)</ref>). Nevertheless, during face frontalization, it is difficult to model identical 3D facial shape for unconstrained photos with diverse facial poses <ref type="bibr" target="#b48">[49]</ref>. Thus, to further align the mouth landmarks, all keypoints are transformed and aligned based on the center point of lips (as shown the green point in Fig. <ref type="figure" target="#fig_2">4 (b)</ref>). More specifically, we first select a frontalized facial frame with closed mouth as the reference sample. And set the middle value of the minimum and maximum points along the vertical axis of lips as the center point c(x,y) = p(x,y) min +p (x,y) max 2 (as shown the green point in Fig. <ref type="figure">(b)</ref>), where c(x,y) is the center point. p(x,y) min and p(x,y) max represent the minimum and maximum points along the vertical axis of lips in the reference sample, respectively. Then all keypoints are transformed and aligned based on the center point as:  2) MSN for Landmark Prediction: Given the multimodal inputs, a network called MSN is proposed for mouth landmark prediction. The MSN consists of two encoders for input modalities and a decoder for mouth landmark prediction, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. For each encoder, a temporal convolutional network (TCN) <ref type="bibr" target="#b49">[50]</ref> is employed to process these input sequential data. Compared to the canonical recurrent architectures (e.g., LSTM <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>), TCN does not use gating mechanisms and exhibits substantially longer memory during processing sequential data. Besides, in TCN, adopting the dilated convolutions, with large filter sizes and dilation factor, contributes to increasing the receptive field for an extremely large effective history. Consequently, TCN shows competitive results over RNN across a broad range of sequence modeling tasks <ref type="bibr" target="#b49">[50]</ref>. For mouth landmark prediction, a multilayered bidirectional gated recurrent unit (GRU) network <ref type="bibr" target="#b50">[51]</ref> is used as the decoder in MSN. In our method, the number of layers is set to 2. During decoding, the encoded features of text and audio inputs are concatenated to form concatenated feature vectors f = (f text , f audio ). And then, the decoder takes the feature vectors f as input and generates a sequence of mouth landmarks p = {p 1 , p2 , . . ., pn }.</p><formula xml:id="formula_0">p = p + (c (x,y) -c (x,y) ) = p + p(x,y) min + p(x,y) max 2 - p (x,y) min + p (x,y) max 2<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Synthesis</head><p>Conditioned on the predicted mouth landmarks, a network VCN is proposed to synthesize photo-realistic and lip-synced facial videos, which consists of a generator and two conditional discriminators. Instead of directly using the predicted landmarks, the inputs of VCN are the mouth masks and manipulable face sketches, where the mouth landmarks of face sketches are replaced with the predicted mouth landmarks, as shown in Fig. <ref type="figure">3</ref>. In this way, the manipulable face sketches have natural head poses and background information extracted from target videos and accurate mouth movements synced to the audio/text input. In this section, the face sketch generation and the proposed VCN are described as follows.</p><p>1) Face Sketch Generation.: For face sketch generation, the dlib face detector <ref type="bibr" target="#b47">[48]</ref> is firstly used to detect 68 facial keypoints from each image of target videos, and the mouth landmarks are modified with the predicted mouth landmarks. Secondly, Canny edge detector <ref type="bibr" target="#b51">[52]</ref> is employed to extract the edges outsides the face regions of target videos as the background. Finally, combining facial keypoints with predicted mouth landmarks and Canny edges, we can generate a sequence of face sketches synced to the audio/text input.</p><p>However, given different target videos, the generated face sketches may have different head poses or stand closer or farther to the camera than one another. If we directly modify the mouth landmarks with the predicted landmarks, the mouth shape may be inconsistent with the head pose, scale and position. Consequently, the generated video frames suffer from overlarge, inconsistent and asymmetric mouth regions, as shown in Fig. <ref type="figure" target="#fig_6">6</ref>. Thus, we design a mouth normalization method to reasonably match the mouth region by finding a suitable transformation between the predicted landmarks and target images in terms of rotation, scale and translation factors.</p><p>To reasonably rotate the predicted mouth landmarks, we need to determine the head pose ? of target images. We first find the center of each eye (e x 1 , e y 1 ) and (e x 2 , e y 2 ) respectively, and denote the connection of the two points as l 1 . Then the head pose ? can be obtained by calculating the angle between l 1 and horizontal line, and the rotation factor can be denoted as cos ? -sin ? sin ? cos ? . Besides, to find a suitable scale factor, we determine the width of jaw based on the predicted landmarks and the landmarks detected from target images. Namely, measure the horizontal distances between the 1st and 13th points (shown in Fig. <ref type="figure" target="#fig_2">4 (c</ref>)) of the predicted landmarks and the landmarks detected from the target images as d 2 and d 1 , and the scale factor can be denoted as</p><formula xml:id="formula_1">d 1 d 2 .</formula><p>Additionally, similar to the equation (1), for the translation factor, we determine the center point of lips on the predicted landmarks and the landmarks detected from target images as c (x,y) and c (x,y) . And the translation factor can be represented as (c (x,y)c (x,y) ). Thus, the predicted mouth landmarks p can be normalized as:</p><formula xml:id="formula_2">p = d 1 d 2 cos ? -sin ? sin ? cos ? p + (c (x,y) -c (x,y) )<label>(2)</label></formula><p>Fig. <ref type="figure">7</ref>. The network architecture of the generator in VCN. In VCN, the optical flow is adopted to model the temporal dependency between consecutive frames for inter-frame consistency. Besides, the mouth generation branch is proposed to enhance mouth texture, and the mouth mask is employed to ensure the intra-frame consistency between the mouth region and the other regions.</p><p>where p represents the normalized landmarks. In this way, the normalized landmarks p have the same pose as that of target images and can reasonably match face sketches.</p><p>2) VCN for Video Generation.: Given the mouth masks and manipulable face sketches, the VCN network, consisting of a generator and two discriminators, is proposed to generate highquality and lip-synced talking heads.</p><p>Generator. Talking face generation is a temporal-dependent task. Thus, to model the inter-frame consistency, the optical flow <ref type="bibr" target="#b29">[30]</ref> is adopted in VCN to generate temporally coherent videos and realize the smooth transition of facial movements. However, the optical flow is generally difficult to be estimated on large-motion areas, like the mouth region, which have a large motion and only occupy a small portion of an image. Consequently, the mouth area tends to be blurry. Therefore, to improve the mouth texture, a mouth generation branch is proposed in VCN to generate the mouth texture from scratch. And in this branch, the corresponding mouth mask is adopted to ensure the intra-frame consistency between the mouth area and the others.</p><p>The overall architecture of the generator is shown in Fig. <ref type="figure">7</ref>. To model the inter-frame consistency, an encoder-decoder network is proposed to generate the hallucinated image ht , optical flow wt-1 and mask mt . More specifically, given manipulable face sketches s t t-L , mouth masks q t t-L and past images xt-1 t-L as inputs, two branches of encoders are firstly adopted to exploit the facial structure information and facial texture features, respectively. Additionally, for facial animation generation, each region of face is closely related, and it is essential to capture the global dependency and highly detailed features within internal representations of face images. Hence, a self-attention module <ref type="bibr" target="#b52">[53]</ref> is employed to learn highly detailed features across face images in each branch. Secondly, concatenated the features extracted from each branch of the encoder as inputs, two branches of decoders, containing a series of residual blocks and sub-pixel convolution layers, are employed to generate the hallucinated image ht , optical flow wt-1 and mask mt . By the estimated optical flow wt-1 , the next frame xt can be generated warped from the past image xt-1 , which can model the temporal dynamics between adjacent frames. To model the intra-frame consistency and improve the mouth texture, a mouth generation branch is proposed to generate realistic mouth regions with the corresponding mouth mask. The mouth generation branch also adopts an encoder-decoder network architecture, including a series of convolution layers, residual blocks and sub-pixel convolution layers.</p><p>Formally, the estimated frames can be obtained by a function F in a recursive manner, and the function F can be represented as:</p><formula xml:id="formula_3">F (x t-1 t-L , sq t t-L ) = q t hm,t + (1 -q t ) {(1 -mt ) wt-1 (x t-1 ) + mt ht } (3)</formula><p>where sq T 1 = {s 1 , q 1 , . . ., s t , q t , . . ., s T , q T } is the sequence of facial sketches and mouth masks, t = 1, . . ., T . s t represents the facial sketch at time t. q t is the mouth mask at time t. The mouth mask q t is generated from the outer contours of lips, the points from 14th to 25th in Fig. <ref type="figure" target="#fig_2">4 (c</ref>). Using the lip landmarks, we can generate the binary mask q t for the corresponding mouth part. Note that, during training, these lip landmarks are detected by the dlib face detector <ref type="bibr" target="#b47">[48]</ref> (ground truth), while during test, these lip landmarks are predicted from the MSN. x T 1 = {x 1 , x 2 , . . ., x T } and xT 1 = {x 1 , x2 , . . ., xT } denote the sequence of real images and generated images. wt-1 is the estimated optical flow from xt-1 to xt . By wt-1 (x t-1 ), xt can be obtained warped from xt-1 . mt is the occlusion mask with continuous values between 0 and 1. ht denotes the hallucinated image, synthesizing background content in occluded areas from scratch. hm,t denotes the synthesized mouth area. L denotes the past time parameter and is set to L = 2 in our method. Here, it is worth mentioning that, during training, the generator is trained with 30-frame video clips (as described in section IV. A)), whereas we can generate convincing videos with longer lengths during test via recursively applying F .</p><p>For this generator, it is noted that the initial frame should be first generated by transforming from the corresponding face sketch, which falls into the category of image-to-image translation. Thus, we introduce the pix2pixHD <ref type="bibr" target="#b53">[54]</ref>, the off-the-shelf image translation model, to generate the initial frame.</p><p>Discriminator. In VCN, two types of discriminators are employed as follows.</p><p>Conditional image discriminator D I . In D I , we adopted the multi-scale PatchGAN architecture <ref type="bibr" target="#b40">[41]</ref>, only penalizing structure at the scale of image patches, to differentiate between real and synthesized images. The D I should output 0 for a fake pair (x t , s t ) and 1 for a true pair (x t , s t ).</p><p>Conditional video discriminator D V . In D V , the multi-scale PatchGAN architecture is also adopted to ensure the temporal dynamics between consecutive synthesized video frames <ref type="bibr" target="#b29">[30]</ref>. The D V should output 0 for a fake pair (x t-1 t-K , w t-2 t-K ) and 1 for a true pair (x t-1 t-K , w t-2 t-K ), where w t-2 t-K represents K -1 optical flow for K adjacent real images x t-1 t-K , and the optical flow w is extracted by FlowNet2 <ref type="bibr" target="#b28">[29]</ref> as the ground truth. Moreover, to further ensure the long-term temporal consistency, the real/generated videos are subsampled by a factor of K, and the discriminator D V can take K adjacent frames in the new sequence as input to discriminate whether it is a true pair or not. In our method, K is set as 3.</p><p>3) Learning Objective Function.: The sequential video synthesis function F can be solved: min F (max</p><formula xml:id="formula_4">D I I (F, D I ) + max D V V (F, D V )) + ? w w (F ) + ? m mI (F, D I ) + ? m mV (F, D V ) + ? vgg vgg (F ) + ? mouth mouth (F )<label>(4)</label></formula><p>where I represents the image GAN loss defined by the conditional image discriminator D I . V represents the video GAN loss defined by conditional video discriminator D V . ? w , ? m , ? vgg and ? mouth are the weight and set to 10. The image GAN loss I is represented as:</p><formula xml:id="formula_5">E ? I (x T 1 ,s T 1 ) [logD I (x k , s k )] + E ? I (x T 1 ,s T 1 ) [log(1 -D I (x k , s k )]<label>(5</label></formula><p>) where ? I represents a random image sampling operator, and k is an integer randomly sampled from 1 to T .</p><p>Similarly, the video GAN Loss V is represented as:</p><formula xml:id="formula_6">E ? V (w T -1 1 ,x T 1 ,s T 1 ) [logD V (x k -1 k -K , w k -2 k -K )]+ E ? V (w T -1 1 ,x T 1 ,s T 1 ) [log(1 -D V (x k -1 k -K , w k -2 k -K ))]<label>(6)</label></formula><p>where ? V denotes a sampling operator that randomly retrieves K consecutive frames, k is an integer uniformly sampled from</p><formula xml:id="formula_7">K + 1 to T + 1.</formula><p>The flow loss w is represented as:</p><formula xml:id="formula_8">w = 1 T -1 T -1 t=1 ( wt -w t 1 + wt (x t ) -x t+1 1 ) (7)</formula><p>The flow loss w consists of two parts, (i) the endpoint error between the flow w t (ground truth) and the estimated flow wt and (ii) the warping loss between the real image x t+1 and the estimated image wt (x t ). Besides, the discriminator feature matching losses mI and mV (collectively referred to as m ) and perceptual loss vgg <ref type="bibr" target="#b53">[54]</ref> are also introduced to improve the training stability and generated image quality.</p><p>Moreover, synthesizing realistic teeth is surprisingly challenging because the teeth must appear rigid, sharp, pose aligned, lit correctly, and properly occluded by the lips. Besides, humans are sensitive to pixel jitter, and any subtle artifacts may influence the lip-sync. Thus, to enhance teeth texture, the mouth-mask loss is proposed and defined as:</p><formula xml:id="formula_9">mouth = 1 T -1 T -1 t=1 ( xt q t -x t q t 1 )<label>(8)</label></formula><p>IV. EXPERIMENT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Dataset. In this paper, we do a case study on Donald John Trump for talking face generation. Trump is ideally suited as an initial research subject, because there are an abundance of public videos with the face region controllable and facing the camera. We downloaded 20 weekly presidential addresses of Trump available online. Each address lasts about 3 minutes, resulting in a total of 1 h of videos. The audio and landmarks are extracted from these addresses, and the text is obtained from video caption. In this experiment, each video is split into fixed-length (30 consecutive frames) clips, and 3300 clips are obtained. We randomly split out 5 percent of the video clips for validation and test respectively, and the rest are used for training.</p><p>Besides, for a fair comparison with other methods and verifying the generalization capacity, Obama is also used to benchmark lip-sync methods <ref type="bibr" target="#b17">[18]</ref>. Similar to <ref type="bibr" target="#b17">[18]</ref>, 300 weekly presidential addresses of Obama are downloaded. Each video is also split into video clips with a fixed length of 30 frames. 20 percent of videos are used for validation and 80 percent are used for training.</p><p>Moreover, FaceForensics dataset <ref type="bibr" target="#b54">[55]</ref> is also used to verify the generalization capacity. The dataset contains 854 videos of news briefing from different reporters, divided into 704 videos for training and 150 videos for validation.</p><p>Evaluation metrics. To evaluate the accuracy of predicted mouth landmarks, Root Mean Square Error (RMSE) <ref type="bibr" target="#b23">[24]</ref> is adopted to measure the difference between the predicted mouth landmarks p and the real ones p integrally as: where T denotes the temporal length of videos and n denotes the total number of landmark points on each image (33 points).</p><formula xml:id="formula_10">RM SE = 1 T ? n T t=1 n n =1 (p t,n -p t,n ) 2<label>(9)</label></formula><p>To evaluate the visual quality of the video frames, common reconstruction metrics, such as the peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) indexes <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b55">[56]</ref>, are employed to reflect the image reconstruction quality and the structural similarity. Besides, the Landmark Distance (LMD) <ref type="bibr" target="#b56">[57]</ref> is also introduced to evaluate whether the synthesized video contains accurate lip movements that correspond to ground truth.</p><p>Implementation details. In our method, the whole network is trained step by step. Concretely, the MSN is first trained for mouth landmark prediction with multimodal inputs, and then based on the predicted mouth landmarks, the VCN is trained for realistic talking face generation. In MSN, the encoder is a four-layered TCN, with dilation factors d = 1, 2, 4, 8 and filter size k = 2, and the decoder is composed of a two-layer GRU. The network is trained using the adaptive gradient (Adagrad) algorithm with learning rate (lr) = 0.001. We gradually update the lr until the network converges.</p><p>For the VCN, each input image is cropped at the center of face and resized to 256 ? 256. The network is trained using the adaptive moment estimation (ADAM) optimizer with lr = 0.0002 and betas = (0.5, 0.999), and is evaluated by the LSGAN loss <ref type="bibr" target="#b57">[58]</ref>. During training, when the generator is fixed, the discriminators, D I and D V , are optimized simultaneously. And when the discriminators are fixed, the generator is optimized. The detailed experimental process is shown as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mouth Shape Prediction</head><p>For talking face generation, it is non-trivial to predict accurate landmarks to ensure audio-visual synchronization. For this task, the MSN is proposed to improve the prediction precision with multimodal inputs. Moreover, we also investigate the prediction accuracy, with only audio or text available, to verify the effectiveness of multimodal inputs. Fig. <ref type="figure" target="#fig_7">8</ref> and Table <ref type="table" target="#tab_0">I</ref> show the comparison of predicted landmarks and RMSEs on the test dataset with different inputs. From Fig. <ref type="figure" target="#fig_7">8</ref>, it is observed that the predicted landmarks are the most consistent with ground truth when the inputs are both text and audio. Meanwhile, the predicted lip and jaw movements meet articulator synergy. Concretely, when the mouth opens, the jaw moves down, and when the mouth closes, the jaw moves up. Besides, in Table <ref type="table" target="#tab_0">I</ref>, the RMSE is the lowest with both text and audio inputs. These results demonstrate that both lip movements and jaw movements have a close relationship with text and audio inputs, and considering the complementarity of different inputs, our MSN achieves the best performance with multimodal inputs. In addition, we find that the predicted results with only text input are better than  those with only audio input, e.g., the RMSE is lower and the predicted landmarks are closer to the ground truth, given only text as input. These results indicate that, compared to the acoustic features, the linguistic features include abundant fully-context information, phoneme-level position information and the temporal information of state and phoneme, which contributes to improving the predicted precision of mouth landmarks. Besides, we compare the performance of our MSN with that of a single-layer, unidirectional LSTM, which is often adopted to keypoint prediction tasks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In Table <ref type="table" target="#tab_0">I</ref>, the RMSEs predicted from LSTM are obviously higher than those predicted from MSN. The reason is that, compared to the single-layer LSTM network, the MSN can significantly extract the features from audio/text by encoders, and employs bidirectional GRU to make use of the long-range context in both forward and backward directions. To this end, our MSN exhibits compelling superiority on mouth landmark prediction.</p><p>Furthermore, we also conduct experiments to study the robustness of the proposed MSN, where Gaussian noises with different standard deviations are added to the audio signal during test. Table II shows the RMSEs with different noises. From Table <ref type="table" target="#tab_1">II</ref>, it is observed that the RMSE becomes higher with increasing of the standard deviation. However, compared to the results with only audio and noise as inputs, adding text input can substantially slow down the growth of RMSE. For example, compared to the results without noise, the RMSE is merely increased by 0.038 given text, audio and noise (std = 0.08) as inputs, which is significantly lower than the result, increased by 3.318, with only audio and noise (std = 0.08) as inputs. These results indicate that text input can help to improve the robustness of our method, and our MSN shows good robustness on landmark prediction with both text and audio inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Synthesis and Evaluation</head><p>As discussed above, the MSN achieves the best performance on mouth landmark prediction with multimodal inputs. Therefore, to ensure lip-sync, the landmarks predicted from both text and audio are adopted for video synthesis in our method.</p><p>After the mouth landmarks predicted, we can generate a sequence of face sketches synced to the input, as described in section III.B 1). Then given the manipulable face sketches and corresponding mouth masks, the VCN is adopted to generate realistic talking head videos. Fig. <ref type="figure" target="#fig_8">9</ref> shows the synthesized frames with different scenes (e.g., backgrounds, head poses and so on), given the same audio clip and text as inputs. As shown in Fig. <ref type="figure" target="#fig_8">9</ref> (a)(b), our VCN is capable of generating photo-realistic, lip-synced and temporally coherent videos. The reason is that the VCN can effectively capture the dynamics of the entire face by optical flow and improve the mouth texture generation by the mouth generation branch. Besides, in Fig. <ref type="figure" target="#fig_8">9 (b)-(e)</ref>, the VCN can generate lip-synced animations with different backgrounds, expressions (e.g., blinks and frowns) and head poses. This result demonstrates that, using the landmarks as intermediate high-level representations, our method can synthesize video Fig. <ref type="figure" target="#fig_9">10</ref>. The synthesized video frames with/without the predicted jaw landmarks. (a) When the mouth is opening, (1) given both jaw and lip landmarks, the jaw moves down meeting the articulator synergy, (2) but given only lip landmarks, the jaw moves up causing the movement inconsistency between jaw and lips. (b) When the mouth is closing, (1) given both jaw and lip landmarks, the jaw moves up meeting the articulator synergy, (2) but given only lip landmarks, the jaw moves down causing the movement inconsistency between jaw and lips.</p><p>frames with different natural head poses and realistic backgrounds, which helps to improve the sense of reality. Effect of the jaw landmarks. Theoretically, speech is produced through the synergy of the human body's articulators, such as the tongue, jaw and lips. To guarantee the lip-sync of facial animations, it is essential to ensure the articulator synergy between the lips and jaw. Fig. <ref type="figure" target="#fig_9">10</ref> shows the comparison of synthesized images with or without the jaw landmarks. Explicitly, Fig. <ref type="figure" target="#fig_9">10  (a</ref>  <ref type="formula" target="#formula_2">2</ref>) represent the synthesized video frames from the manipulated face sketches, where only the lip landmarks, predicted from the MSN, are employed, and the others, including the jaw landmarks, are extracted from the target videos. From Fig. <ref type="figure" target="#fig_9">10</ref>, it is observed that the synthesized video frames meet articulator synergy with both jaw landmarks and lip landmarks. For example, when the mouth is opening, the jaw moves down, and when the mouth is closing, the jaw moves up. However, compared to the results with both jaw landmarks and lip landmarks, the generated images, with only lip landmarks, suffer from disproportional facial structure and movement inconsistency between jaw and lips. This result demonstrates that considering the jaw landmarks can ensure the movement consistency between jaw and lips, which helps to improve the lip-sync and realism of face animations.</p><p>Effect of the optical flow. The optical flow can represent the apparent motion of image objects between two consecutive frames. Given the predicted optical flow, our VCN can recursively generate the next frame warped from the current frame and synthesize smooth transition facial animations. Table <ref type="table" target="#tab_2">III</ref>    shows the comparison scores with or without the optical flow. From Table <ref type="table" target="#tab_2">III</ref>, it is observed that the scores with optical flow are higher than those without optical flow. This result demonstrates that the optical flow plays an important role in modeling the temporal dynamics among the entire videos and ensuring the inter-frame consistency.</p><p>Effect of the mouth generation branch. For talking face generation, even though the optical flow can model the temporal dependency between frames, it is difficult to be estimated accurately when the region, especially the mouth region, has a large motion and occupies a small portion. This phenomenon results in the generated mouth area with poor visual quality. As shown in Table <ref type="table" target="#tab_0">IV</ref>, compared to the results without the mouth generation branch, our method can achieve higher scores with this branch. This result demonstrates that the mouth generation branch can effectively generate the mouth region and improve the fidelity of mouth area. Besides, using the mouth mask, our network can integrate different regions and ensure the intra-frame consistency.</p><p>Effect of the weight ? mouth . Generating realistic teeth is especially challenging, requiring pose aligned, lit correctly, properly occluded by the lips and fine detail in the teeth. To enhance the teeth texture, the mouth loss is proposed, and Table <ref type="table">V</ref> shows the scores with different weights of ? mouth when the inputs are both text and audio. From Table V, it is observed that the PSNR and SSIM scores are the highest when the ? mouth is set to 10. This result demonstrates that, given a suitable weight ? mouth ,  the proposed mouth loss contributes to generating vivid facial images, especially the fine detail generation of teeth area.</p><p>Effects of the GAN loss functions. To verify the effectiveness of our GAN loss functions, we compare the scores with different losses. From Fig. <ref type="figure" target="#fig_11">11</ref> and Table VI, it is observed that applying these losses helps to improve the visual quality and our VCN obtains the highest scores. Specifically, compared to the results without w , the scores and image quality are substantially improved with w . This result verifies that the quality of the generated image greatly depends on the accuracy of estimated optical flow, and w plays a great role in video generation. Similarly, compared to the results without V , the scores are significantly increased with V . This result illuminates that the conditional video discriminator can integrate multi-frame information, and contributes to the temporal continuity between frames. In addition, m and vgg can help to mitigate image blur and improve the quality of generated images. Moreover, generating realistic teeth is especially challenging, requiring pose aligned, lit correctly, properly occluded by the lips. Compared to the results without mouth , the results with mouth have clearer teeth texture and higher scores. This result demonstrates that the mouth contributes to generating fine details in the teeth and producing photo-realistic facial images.</p><p>2) Comparison With Baselines.: Fig. <ref type="figure" target="#fig_12">12</ref> and Table <ref type="table" target="#tab_4">VII</ref> show the comparison of our synthesis results with recently reported state-of-the-art baselines <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>. For a fair comparison, all videos are generated based on the open source codes 2,3,4,5 given the same utterance and target video as those in our method. Besides, note that the method <ref type="bibr" target="#b17">[18]</ref> merely restores the mouth region, while the other baselines and our method can generate the entire face. To fairly evaluate the generation methods and results, the PSNR and SSIM are calculated based on the mouth region for the method <ref type="bibr" target="#b17">[18]</ref>, and the other scores (PSNR and SSIM) are obtained based on the entire face. From Fig. <ref type="figure" target="#fig_12">12</ref> and Table VII, we can find that our method achieves the best performance and scores (i.e., the highest PSNR and SSIM scores and the lowest LMD score). More specifically, compared to the results obtained from Chen et al. <ref type="bibr" target="#b0">[1]</ref> that just focuses on the facial area, our method can produce higher resolution facial images with natural head movements, facial expressions and backgrounds. The reason is that, in our method, using the face sketches as the intermediary can bridge the audio/text input to the video, and provide the head pose and background information. To this end, our method achieves better performance. In addition, compared to the results obtained from Kumar et al. <ref type="bibr" target="#b17">[18]</ref>, our method can produce higher resolution mouth regions (the red regions) and generate smooth entire facial texture (the blue regions), whereas the images generated from Kumar et al. <ref type="bibr" target="#b17">[18]</ref> have obvious boundaries and mismatch texture. This is because that Kumar et al. <ref type="bibr" target="#b17">[18]</ref> just restore the pixels around the mouth, causing the inconsistency between the restored mouth area and the others. On the contrary, our method can generate the entire face to ensure texture consistency. Besides, the LMD obtained from our method is lower than that obtained from Kumar et al. <ref type="bibr" target="#b17">[18]</ref> by a large margin. This result verifies that utilizing the complementarity of multimedia inputs, our method can predict accurate mouth landmarks. Then conditioned on the predicted landmarks, our VCN can generate lip-sync facial animations. Furthermore, as shown in Fig. <ref type="figure" target="#fig_12">12 (c</ref>)(e), the results generated from Wiles et al. <ref type="bibr" target="#b3">[4]</ref> and Siarohin et al. <ref type="bibr" target="#b1">[2]</ref> have blurry mouth area (the red regions) and low resolution facial images. Compared to these methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, our method can generate more precise mouth movements and clearer mouth area, especially the teeth. These results demonstrate that the proposed mouth generation branch and mouth-mask loss contribute to focusing on the mouth area to enhance mouth texture, and ensuring the intra-frame consistency between the mouth region and the others for photo-realistic talking face generation. More importantly, our method can generate temporally coherent videos by modeling the temporal dynamics, while the baselines exist obvious pixel jitter. This result demonstrates that the optical flow in our method plays an important role in generating temporal continuity videos. The specific comparison and more results can be found in the supplementary video.</p><p>3) The Generalization Experiment.: To prove the generalization capability of our method, we also train our network on the presidential addresses of Obama and FaceForensics dataset with different subjects. As shown in Fig. <ref type="figure" target="#fig_13">13</ref>, we can find that, given the same audio and text as inputs, our method can generate photo-realistic and temporally coherent facial animations with natural head movements. Besides, employing the mouth landmarks as the bridge between multimodal inputs and videos, all the synthesized frames have the same mouth shapes as those of the ground truth and meet articulator synergy. This result indicates that our approach can generate realistic and lip-synced facial animations, and exhibits good superiority on generalization capability to achieve arbitrary-subject talking face generation if its dataset is available.</p><p>4) User Study.: A user study is conducted to investigate the visual quality of our generated results with Kumar et al. <ref type="bibr" target="#b17">[18]</ref>, Wiles et al. <ref type="bibr" target="#b3">[4]</ref>, Chen et al. <ref type="bibr" target="#b0">[1]</ref>, Siarohin et al. <ref type="bibr" target="#b1">[2]</ref> in a video level. Similar to <ref type="bibr" target="#b27">[28]</ref>, the generated videos are evaluated in terms of three criteria: whether the generated talking faces could be regarded as realistic (realistic rate), whether the generated talking faces keep temporal coherence between adjacent frames (temporally coherent rate) and whether the generated talking faces are lip-sync with the corresponding audio/text input (lip-sync rate). In this study, 20 participants are involved in this study, and they are proficient in English to complete this evaluation. For the videos, we randomly selected 5 audio samples and corresponding text from the testing set and generated the facial animations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE VIII THE EVALUATION OF OUR GENERATION RESULTS AND THE REPRODUCED BASELINES</head><p>from different methods. Meanwhile, the real videos were also used in this study. During the evaluation, we shuffle all the sample videos and the participants are not aware of the mapping between videos to methods. The range of score is from 0 (the worst result) to 1 (the best result), and the results are averaged over persons and videos. From Table <ref type="table" target="#tab_4">VIII</ref>, it is observed that our generation videos greatly outperform others in terms of the realistic rate, temporally coherent rate and lip-sync rate. This result demonstrates that the optical flow plays an important role in modeling the temporal dependency between adjacent frames to avoid pixel jitter, and the proposed mouth generation branch and multimodal inputs help to enhance mouth texture and improve the lip-sync. Besides, compared to the results obtained from Chen et al. <ref type="bibr" target="#b0">[1]</ref>, our method achieves much higher scores in terms of the realistic rate. This result demonstrates that natural head movements and realistic background also contribute to improving the visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we present a systematic research on photo-realistic talking face generation with multimodal inputs, articulator synergy, inter-frame consistency and intra-frame consistency. In our method, we explore the complementarity of multimodal inputs and verify that integrating different input information contributions to improving the prediction precision of mouth landmarks. Moreover, we also study the synchronization among lips and jaw to ensure the articulator synergy. During video synthesis, the optical flow is adopted to model the temporal dependency between adjacent frames for inter-frame consistency. Besides, a mouth generation branch is proposed to improve the fidelity of mouth area, then utilize the corresponding mouth mask to ensure the intra-consistency between mouth area and the others. Overall, the well-designed approach exhibits impressive performance on generating realistic, lip-synced and temporally coherent videos. In the future, it is necessary to generate natural human bodies to further improve the sense of reality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Compared to existing methods (a) our proposed method, (b) can accurately predict the lip and jaw landmarks with multimodal inputs, and generate highly realistic video frames considering the intra-frame consistency and inter-frame consistency.</figDesc><graphic url="image-1.png" coords="1,308.51,166.73,244.10,159.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The real face image. (b) The generated image considering both lip and jaw shapes. In this case, the movements of jaw and lips meet articulator synergy. (c) The generated image just considering lip shape. In this case, the generated images suffer from unrealistic philtrum and movement inconsistency between jaw and lips.</figDesc><graphic url="image-2.png" coords="2,110.39,67.01,62.42,62.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The real image. (b) The frontalized image. The green point denotes the center point of lips. (c) The mouth landmarks with sample annotations. (d)-(e) The distributions of frontalized landmarks and further aligned landmarks of one sample.</figDesc><graphic url="image-8.png" coords="4,320.51,67.25,211.82,154.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1) where p = {p 1 , p 2 , . . ., p n } and p = {p 1 , p 2 , . . ., p n } respectively represent the frontalized keypoints (as shown in Fig. (d)) and further aligned keypoinyts on facial images (as shown in Fig. 4 (e)). And n = 33 is the number of keypoints. c (x,y) = p (x,y) min +p (x,y) max 2denotes the center point of each facial frame. From Fig.4(d)(e), it is obvious that the mouth landmarks can be further aligned based on the center point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The MSN for mouth landmark prediction.</figDesc><graphic url="image-9.png" coords="5,45.59,67.37,244.70,74.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>In MSN, we use a four-layered TCN, with dilation factors d = 1, 2, 4, 8 and filter size k = 2, where each input feature has a receptive field of 16. The two input modalities are encoded with different encoder networks. After encoding, we can obtain 64-D feature vectors for audio modality f audio and 128-D feature vectors for text modality f text .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The generated images without considering the mouth normalization method. (a) Overlarge mouth region without considering the scale factor. (b) Inconsistent mouth region without considering the rotation factor. (c) Asymmetric mouth region without considering the translation factor.</figDesc><graphic url="image-10.png" coords="5,308.15,66.77,244.82,172.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The comparison of predicted landmarks on the test dataset with different inputs. (a) Ground truth. (b)-(d) The predicted mouth landmarks with different inputs.</figDesc><graphic url="image-12.png" coords="8,358.55,67.25,190.46,193.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Given an audio clip and corresponding text, (a) ground truth, (b) the synthesized results on the same scene of Trump, and (c)-(e) the synthesized results on different scenes of Trump.</figDesc><graphic url="image-13.png" coords="9,56.15,66.77,233.06,290.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 )</head><label>1</label><figDesc>Ablation Study.: Comprehensive ablation experiments are conducted to study the effectiveness of the core modules and different GAN loss functions. For a fair comparison, all the following experiments are conducted on the test dataset of Trump. The detailed implementations are discussed as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>)(1) and Fig. 10 (b)(1) represent the synthesized video frames from the manipulated face sketches, where the jaw landmarks and lip landmarks are predicted from the MSN, and the others are extracted from the target videos. Fig. 10 (a)(2) and Fig. 10 (b)(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The generated frames with different GAN loss functions. (a) Ground truth. (b) The generated frame with I loss. (c) The generated frame with I + V losses. (d) The generated frame with I + V + w losses. (e) The generated frame with I + V + w + vgg losses. (f) The generated frame with I + V + w + vgg + m losses. (g) The generated frame with I + V + w + vgg + m + mouth losses (VCN).</figDesc><graphic url="image-15.png" coords="10,51.83,66.65,486.14,71.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (a) Ground truth. (b)-(f) Comparison of different methods given the same utterance and the same target video of Trump.</figDesc><graphic url="image-16.png" coords="11,80.15,67.37,461.54,172.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Results on different subjects given the same audio and text inputs. (a) The ground truth. (b)-(c) The generated frames on the datasets of Trump and Obama. (d) The generated frames on the FaceForensics dataset.</figDesc><graphic url="image-17.png" coords="12,65.15,67.25,209.66,216.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-11.png" coords="6,51.95,66.88,486.00,221.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>COMPARISON OF RMSES WITH DIFFERENT INPUTS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>COMPARISON OF RMSES WITH DIFFERENT NOISES. STD DENOTES THE STANDARD DEVIATION OF NOISE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III SCORES</head><label>III</label><figDesc>WITH/WITHOUT THE OPTICAL FLOW</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI SCORES</head><label>VI</label><figDesc>WITH DIFFERENT GAN LOSSES WHEN THE INPUTS ARE BOTH TEXT AND AUDIO</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 09:15:08 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>[Online]. Available: http://www.cs.columbia.edu/ ? ecooper/tts/lab_format. pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>[Online]. Available: https://github.com/oawiles/X2Face.git</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/lelechen63/ATVGnet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>[Online]. Available: https://github.com/karanvivekbhargava/obamanet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>[Online]. Available: https://github.com/AliaksandrSiarohin/first-ordermodel.git</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">National Nature Science Foundation of China</rs> under Grants <rs type="grantNumber">62022076</rs>, <rs type="grantNumber">U1936210</rs>, <rs type="grantNumber">62032006</rs>, and <rs type="grantNumber">62102127</rs>, in part by the <rs type="funder">China Postdoctoral Science Foundation</rs> under Grant <rs type="grantNumber">2020M682035</rs>, in part by <rs type="funder">Anhui Postdoctoral Research Activities Foundation</rs> under Grant <rs type="grantNumber">2020B436</rs>, and in part by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> under Grant <rs type="grantNumber">WK3480000011</rs>. The associate editor coordinating the review of this manuscript and approving it for publication was <rs type="person">Dr. Xavier Alameda-Pineda</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mgjHGbc">
					<idno type="grant-number">62022076</idno>
				</org>
				<org type="funding" xml:id="_42h7dHw">
					<idno type="grant-number">U1936210</idno>
				</org>
				<org type="funding" xml:id="_agWQF4D">
					<idno type="grant-number">62032006</idno>
				</org>
				<org type="funding" xml:id="_UMh4cgn">
					<idno type="grant-number">62102127</idno>
				</org>
				<org type="funding" xml:id="_bzRHgbz">
					<idno type="grant-number">2020M682035</idno>
				</org>
				<org type="funding" xml:id="_Ur8PjnQ">
					<idno type="grant-number">2020B436</idno>
				</org>
				<org type="funding" xml:id="_rfeHBtC">
					<idno type="grant-number">WK3480000011</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Inf</title>
		<meeting>Conf. Neural Inf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7137" to="7147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Story-driven video editing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.3037461</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Common and innovative visuals: A sparsity modeling framework for video</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Moghadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Radha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4055" to="4069" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video inpainting with shortterm windows: Application to object removal and error concealment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ebdelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3034" to="3047" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PRRNet: Pixel-region relation network for face forgery detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">107950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frequency-aware discriminative feature learning supervised by single-center loss for face forgery detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6458" to="6467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph.)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">163</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talkinghead synthesis for video conferencing</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit. (CVPR)</title>
		<meeting>IEEE/CVF Conf. Comput. Vision Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06">Jun. 2021</date>
			<biblScope unit="page" from="10039" to="10049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tiling in interactive panoramic video: Approaches and evaluation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Gaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1819" to="1831" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A signal adaptive prediction filter for video coding using directional total variation: Mathematical framework and parameter selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rasch</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2020.3030590</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9678" to="9688" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-aware convolutional neural network for in-loop filtering in high efficiency video coding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3343" to="3356" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep head pose: Gaze-direction estimation in multimodal video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthesizing obama: Learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oba-maNet: Photo-realistic lip-sync from text</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01442</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop Machine Learn. Creativity Design</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Twenty-Eighth Inter. Joint Conf. Artif. Intell. (IJCAI)</title>
		<meeting>Twenty-Eighth Inter. Joint Conf. Artif. Intell. (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="919" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven facial animation with temporal GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09313</idno>
	</analytic>
	<monogr>
		<title level="m">British Machine Vis. Conf. (BMVC)</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Talking-head generation with rhythmic head motion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05201</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An analysis of HMM-based prediction of articulatory movements</title>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLTRCNN based 3rd articulatory movement prediction: Learning articulatory synchronicity from both text and audio inputs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1621" to="1632" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taskdependence of articulator synergies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toutios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustical Soc. America</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1504" to="1520" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Preliminaries to a theory of action with reference to vision</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Turvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceiving, Acting Knowing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal inputs driven talking face generation with spatial-temporal dependency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="216" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conf</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Neural Info. Proc. Syst. (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1152" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating visually aligned sound from videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2020.3009820</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8292" to="8302" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with selfsupervised multisensory features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Music gesture for visual sound separation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">487</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2.5D visual sound</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised learning of audio-visual objects from video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Sound of Motions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1904.05979</idno>
		<ptr target="http://arxiv.org/abs/1904.05979" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1744" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Foley music: Learning to generate music from videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="758" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: a speakerindependent audio-visual model for speech separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Audio-driven talking face video generation with natural head pose</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10137</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An adaptive algorithm for mel-cepstral analysis of speech</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masuda-Katsuse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Merlin: An open source neural network speech synthesis system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSW</title>
		<meeting>SSW<address><addrLine>Sunnyvale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The HTK book</title>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cambridge Univ. Eng. Dept</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">175</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DLIB-ML: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">To frontalize or not to frontalize: A study of face pre-processing techniques and their impact on recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. Appl. Comput. Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Methods in Natural Lang. Process</title>
		<imprint>
			<biblScope unit="page" from="1724" to="1734" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Faceforensics: A large-scale video dataset for forgery detection in human faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>R?ssler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09179E</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image quality metrics: PSNR vs. SSIM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Pattern Recognit</title>
		<meeting>20th Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="538" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
