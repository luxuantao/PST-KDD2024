<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Virtualized Memory Translation with Nested Elastic Cuckoo Page Tables</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jovan</forename><surname>Stojkovic</surname></persName>
							<email>jovans2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
							<email>dskarlat@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Apostolos</forename><surname>Kokolis</surname></persName>
							<email>kokolis2@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
							<email>tyxu@illinois.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
							<email>torrella@illinois.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Virtualized Memory Translation with Nested Elastic Cuckoo Page Tables</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503222.3507720</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Virtual Memory</term>
					<term>Page Tables</term>
					<term>Virtualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major reason why nested or virtualized address translations are slow is because current systems organize page tables in a multi-level tree that is accessed in a sequential manner. A nested translation may potentially require up to twenty-four sequential memory accesses. To address this problem, this paper presents the first page table design that supports parallel nested address translation. The design is based on using hashed page tables (HPTs) for both guest and host. However, directly extending a native HPT design to a nested environment leads to minor gains. Instead, our design solves a new set of challenges that appear in nested environments. Our scheme eliminates all but three of the potentially twenty-four sequential steps of a nested translation-while judiciously limiting the number of parallel memory accesses issued to avoid over-consuming cache bandwidth. As a result, compared to conventional nested radix tables, our design speeds-up the execution of a set of applications by an average of 1.19x (for 4KB pages) and 1.24x (when huge pages are used). In addition, we also show a migration path from current nested radix page tables to our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Software and its engineering → Operating systems; Virtual memory; • Computer systems organization → Architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cloud computing relies on virtualization hardware to provide strong isolation and enable server consolidation. Virtual Machines (VMs) are multiplexed over hardware resources and offer a safe sandbox for user services. Today, all major cloud providers use VMs, e.g., Amazon's EC2 <ref type="bibr" target="#b6">[7]</ref>, Microsoft's Azure <ref type="bibr">[62]</ref>, and Google's Compute Engine <ref type="bibr" target="#b31">[32]</ref>. Moreover, to reduce the overheads of VMs, lightweight virtualization frameworks have emerged, such as AWS's Firecracker <ref type="bibr" target="#b1">[2]</ref> and Google's gVisor <ref type="bibr" target="#b32">[33]</ref>. With these frameworks, some of the main performance overheads of traditional VMs, such as long boot-up times, have been successfully curbed.</p><p>However, in spite of nearly twenty years since the inception of virtualization hardware <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref> and extensive research <ref type="bibr">[3, 6, 13, 15, 16, 25, 29, 30, 38, 39, 52-54, 66, 72, 76, 80, 89]</ref>, address translation still introduces substantial performance overhead in virtualized systems. A major reason why address translation has high overhead is because page tables are currently organized in a multi-level tree that is accessed in a sequential manner. This organization is called radix page tables.</p><p>In a native (i.e., not virtualized) environment, a virtual to physical address translation requires traversing a tree with four levels of page tables. Such traversal potentially requires issuing up to four sequential memory accesses. In a nested (i.e., virtualized) environment, accessing the table at each level of this tree (now called guest page table level) itself requires performing a translation that traverses four levels of pages tables (now called host page table levels). Traversing these four levels of host page tables again potentially requires issuing up to four sequential memory accesses. When every step is counted, a nested address translation can require up to twenty-four sequential memory accesses. If this is not bad enough, this problem is likely to get worse soon: new processors such as Intel's Sunny Cove <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref> add a fifth level to the tree. As a result, a nested translation may require up to thirty-five sequential memory accesses.</p><p>To reduce the overhead of both native and nested address translation, current systems rely on supporting huge pages <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b82">84]</ref> and hardware caching of address translations. Huge pages reduce the metadata required to support translations. Hardware caching reduces the need to perform expensive memory accesses during translation. Adopted solutions for hardware caching include larger multi-level TLBs and, to reduce the cost of TLB misses, Page Walk caches in the Memory Management Unit (MMU) of the processor.</p><p>For the specific case of nested translation, the MMU of current systems further includes Nested TLBs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. They cache the traversal of the four host pages table levels needed to access each guest page table level.</p><p>Unfortunately, these techniques are now becoming insufficient. Even with them, nested address translation can account for more than 50% of the execution time of applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52]</ref>. With TLB access times already overtaking those of the L2 cache <ref type="bibr" target="#b85">[87]</ref>, and the upcoming commoditization of terabytes of main memory capacity <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b84">86]</ref>, a redesign of the address translation mechanisms seems inevitable.</p><p>To address this challenge, we propose to speed-up the process of nested address translation by exploiting parallelism. For that, we rely on hashed page tables (HPTs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b81">83,</ref><ref type="bibr" target="#b87">89]</ref>. HPTs fundamentally eliminate the sequential steps of radix page tables. In theory, they perform address translation in one step, by hashing a virtual page number into a hash key, accessing the corresponding table entry, and reading the physical page number from there. HPTs have been implemented in the past <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref> and recent work <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b87">89]</ref> has solved some of their traditional shortcomings <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b87">89]</ref>.</p><p>In particular, in this paper, we consider our previously-proposed native HPT design called Elastic Cuckoo Page Tables (ECPTs) <ref type="bibr" target="#b77">[79]</ref>. If we directly extend the ECPT design in a nested manner for both guest and host, we find that the resulting system delivers minor performance gains over nested radix page tables. The main reason is that a nested ECPT translation sometimes results in many parallel memory accesses, which consume too much bandwidth. Consequently, we analyze the translation mechanisms and redesign them so that they issue fewer parallel memory accesses.</p><p>Specifically, we focus on three aspects of a nested ECPT design. The first one concerns misses on a key hardware cache that ECPTs use to cache guest metadata. On a miss, to find the correct location requested by the access, the hardware needs to first translate the missing guest address, causing additional memory accesses. To minimize this problem, we introduce a new hardware cache in the MMU called the Shortcut Translation Cache (STC). The STC keeps the mapping between missing guest addresses and their translations. Intuitively, the STC caches translations of ECPT metadata in a manner logically similar to how the Nested TLB <ref type="bibr" target="#b15">[16]</ref> caches translations of radix page tables. The impact of the STC is a reduction in the number of memory accesses caused by address translation.</p><p>The second aspect of our design has to do with some metadata that the native ECPT design kept uncached. In a nested ECPT design, such metadata does benefit from hardware caching. Hence, our design caches the metadata in the MMU-some of it adaptively, depending on the locality of the application. The result is a further reduction in the number of memory accesses caused by address translation.</p><p>The final aspect of our design leverages the fact that the system may know the page size used by the page tables. If it does, we can further reduce the number of memory accesses involved in address translation.</p><p>We call the resulting design Nested ECPTs. To our knowledge, Nested ECPTs is the first practical design for parallel nested address translation. It eliminates all but three of the potentially twenty-four sequential steps of a nested radix translation. Moreover, we also show how industry can migrate from the current nested radix page tables to Nested ECPTs.</p><p>We evaluate Nested ECPTs with full-system simulations. We show that they successfully exploit parallelism during nested address translation and deliver substantially higher performance than nested radix page tables. Compared to nested radix tables, Nested ECPTs speed-up the execution of a set of applications by an average of 1.19x (for 4KB pages) and 1.24x (when huge pages are used).</p><p>Overall, our contributions are: • Nested ECPTs, the first page table design for parallel nested address translation.</p><p>• A migration path from current systems to Nested ECPTs.</p><p>• An evaluation of Nested ECPTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Radix Page Tables</head><p>Radix page tables, which are the design used by most current architectures, organize the translation in a multi-level tree. To translate a memory address, the hardware walks over each level of the tree sequentially. Currently, the x86-64 architecture implements a 4-level tree. In addition, x86-64 supports large pages of 2MB and 1GB. For such pages, the translation is shortened by one or two levels.</p><p>To reduce translation overhead, MMUs have small per-core caches called Page Walk Caches (PWCs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. PWCs store intermediate page table entries. On a TLB miss, the hardware performs a page walk. The walk involves checking the PWC to obtain the desired translation and, on a PWC miss, accessing the memory hierarchy to get the translation. Native Address Translation. Figure <ref type="figure" target="#fig_0">1</ref> shows a native x86-64 page walk to translate a virtual address (VA) to its physical address (PA). The hardware walks through four levels of tables (L 4 to L 1 ) called PGD, PUD, PMD, and PTE. The hardware first reads the CR3 register, which stores the base of the L 4 table. By adding CR3 and bits 47-39, the hardware obtains the L 4 entry whose content is the base of the correct L 3 table. Then, by adding such content and bits 38-30, one obtains the L 3 entry whose content is the base of the correct L 2 table. The process continues until the correct L 1 entry is read. The L 1 entry stores the physical page number, which together with the page offset (bits 11-0) is the PA. Thus, in the worst case, a page walk requires 4 sequential memory access.  For 2MB or 1GB page sizes, the page walk ends at L 2 or at L 3 , respectively, resulting in up to three or two memory references. The recently-accessed L 4 , L 3 , and L 2 table entries are cached in the PWC for future access, but L 1 entries are not <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46]</ref>. Virtualized Address Translation. Address translation in a virtualized environment is more complex because the physical memory is managed by the hypervisor and is not exposed to guest OSes. A PA viewed by a guest OS is in fact a guest physical address (gPA), which needs to be further translated into a host physical address (hPA). Hence, the nested address translation of a guest virtual address (gVA) involves two phases: from gVA to gPA, and from gPA to hPA.</p><formula xml:id="formula_0">L 4 L 3 L 2 L 1 + CR3</formula><p>Modern hardware-assisted virtualization implements nested paging (e.g., Intel's EPT <ref type="bibr" target="#b45">[46]</ref> and AMD's nested paging <ref type="bibr" target="#b8">[9]</ref>). Nested paging uses two layers of page tables: each guest OS maintains a guest page table that maps gVAs to gPAs, and the hypervisor manages a host page table per guest that maps gPAs to hPAs.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a nested page walk in x86-  In order to access each level of the guest page table (gL 𝑖 ), where 𝑖 = {4,3,2,1} in Steps 5, 10, 15, and 20, the hardware first needs to translate the gPA of the gL 𝑖 table to an hPA. The translation of such gPA requires a page walk that iterates over the hL 4 to hL 1 levels of the host page table (Steps 1-4, 6-9, 11-14, and <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. At the end, the gL 1 entry at Step 20 produces the gPA of the target page. Then, a final walk is needed to translate this gPA to the hPA of the page (Steps 21-24). Finally, the hardware adds the address obtained from Step 24 to the page offset to obtain the final hPA. The process potentially requires up to 24 sequential memory references (Steps 1-24).</p><p>Support for huge pages changes the page walk as follows. If the host supports 2MB or 1GB pages, the hL 1 levels of the translation or both the hL 1 and hL 2 levels, respectively, are eliminated. If, in addition, the guest supports 2MB or 1GB pages, the gL 1 level or both the gL 1 and gL 2 levels, respectively, are eliminated.</p><p>Recently-accessed table entries of gL 𝑖 for 𝑖 = {4,3,2}, and of hL 𝑖 for 𝑖 = {4,3,2,1} are cached in PWCs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. Further, there are Nested TLBs (NTLBs) that cache the translation of the gPA of Level 𝑖 guest page table (gL 𝑖 ) to its hPA, as shown with dotted lines in Figure <ref type="figure" target="#fig_1">2</ref>. These cached translations can skip four host page table accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hashed Page Tables</head><p>Hashed page tables (HPTs) are an alternative design to radix page tables. They have been implemented in the IBM PowerPC, HP PA-RISC, and Intel Itanium (IA-64) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. In HPTs, address translation is performed by hashing the virtual page number, indexing a hash table, and reading the physical page number from the entry. Unrealistically assuming no hash collisions, no multiple page sizes, and no page sharing across processes, only one memory reference is needed for address translation in the native setup.</p><p>In a virtualized environment that uses HPTs, only three memory references are needed for a nested address translation <ref type="bibr" target="#b87">[89]</ref>-again unrealistically assuming no hash collisions, no multiple page sizes, and no page sharing.</p><p>The idea is as follows. We use the gVA to access the guest HPT and read the gPA of the target data page; this is the outcome of Step 20 in Figure <ref type="figure" target="#fig_1">2</ref>. However, to find the guest HPT entry in host memory, we first need to access the host HPT. This is the equivalent of Steps 1-19 in Figure <ref type="figure" target="#fig_1">2</ref>. Finally, once we get the gPA of the target data page, we need to access again the host HPT to obtain the hPA of the target page. This is the equivalent of Steps 21-24 in Figure <ref type="figure" target="#fig_1">2</ref>. Overall, at most three memory accesses are needed: host HPT, guest HPT, and host HPT.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows such a nested page walk. On the right, we show the host memory, which is real. On the left, we show in dashes the memory as seen by the guest, which is virtualized.  In Step 1 , the hardware attempts to use the gVA to access the guest HPT (gHPT) entry (shown in the figure as gPTE). It does so by hashing the gVA using the guest hash function (gH) and adding the resulting hash key to the base of the gHPT (stored in the gCR3 register). However, the resulting gPA is virtualized and cannot be directly accessed (hence the question mark in the figure). It needs to be translated to an hPA by accessing the host HPT (hHPT). Therefore, it is hashed using the host hash function (hH) and the resulting hash key is added to the base address of the hHPT (stored in the hCR3 register). The resulting entry in the hHPT (an hPTE) tells where the gPTE is.</p><p>In Step 2 , the hardware uses the contents of this hPTE as a pointer to access the desired gPTE. This gPTE contains the gPA of the target data page.</p><p>In Step 3 , the hardware translates this gPA to the hPA of the data page. This is again done by hashing gPA using hH, adding the resulting hash key to hCR3, and accessing the resulting entry in the hHPT. The contents of this hPTE is a pointer to the target data page in host memory.</p><p>HPTs have shortcomings. The first one is that hash collisions are expensive: resolving them requires either memory accesses to walk a collision chain <ref type="bibr" target="#b13">[14]</ref>, open-addressed hash table slots <ref type="bibr" target="#b87">[89]</ref>, or invoking the OS <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>. The second shortcoming is that, to avoid dynamic resizing of hash tables, the traditional design uses a single HPT shared by all the processes. Such a design cannot support multiple page sizes or page sharing between processes without introducing additional levels of translation. For example, the PowerPC architecture implements a two-level translation procedure for each memory reference to support huge pages and page sharing <ref type="bibr" target="#b40">[41]</ref>. The result is additional memory references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Elastic Cuckoo Page Tables (ECPTs)</head><p>A design that addresses the shortcomings of HPTs is Elastic Cuckoo Page Tables (ECPTs) <ref type="bibr" target="#b77">[79]</ref>. ECPTs resolve hash collisions by using cuckoo hashing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b63">65]</ref>. A target entry can be in one of 𝑑 locations in a 𝑑-way (or 𝑑-ary) cuckoo hash table. Insertions always find space by evicting existing entries and rehashing them in the other ways until, in practically all cases, all entries find a slot <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b63">65]</ref>.</p><p>ECPTs use process-private HPTs and, hence, support both multiple page sizes and page sharing without introducing any additional level of translation. ECPTs scale the hash tables on demand according to the memory requirements of the process. Hash table resizing is performed while the process is running.</p><p>With this design, a translation is found by looking-up all 𝑑 ways of an ECPT in parallel. If the system has multiple page sizes, each size has an ECPT. Accesses to the different ECPTs and to their different ways are in parallel. An ECPT entry contains a virtual page number (VPN) tag and multiple consecutive page translation entries packed together to exploit spatial locality. For example, in systems with 64B cache lines, eight consecutive translation entries are stored in a cache line and utilize a single tag.</p><p>Issuing many ECPT accesses in parallel can consume substantial bandwidth. To minimize this problem, ECPTs are augmented with Cuckoo Walk Tables (CWTs). CWTs record which ECPT and which way in that ECPT store a given translation. CWTs are cached in a Cuckoo Walk Cache (CWC) in the MMU. Before the hardware attempts to access the ECPTs, it first checks the CWC and, on a hit, prunes the number of parallel accesses issued to the ECPTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARALLEL NESTED TRANSLATION: PLAIN DESIGN</head><p>Our goal is to support high-performance parallel nested address translation. For that, we explore using HPTs for both guest and host, which fundamentally eliminate the sequential steps of radix page tables. We base our design on ECPTs, since past work has shown that they are a competitive design in a native environment.</p><p>In this section, we build a design that directly incorporates the ECPT structures from <ref type="bibr" target="#b77">[79]</ref> into host and guest HPTs. We call the design Plain Nested ECPTs. As the evaluation will show, this design leads to minor performance gains over nested radix page tables. In Section 4, we discuss the reasons for the small gains and modify the design to address the challenges of a nested environment. We call the modified design Advanced Nested ECPTs. Our evaluation section shows the performance of both designs.</p><p>Our Plain Nested ECPT design can be understood by examining Figure <ref type="figure" target="#fig_3">3</ref>. The hHPT and gHPT are implemented as host ECPTs (hECPTs) and guest ECPTs (gECPTs). There are as many hECPTs (and gECPTs) as page sizes are supported by the host (and guest). To be compatible with the x86 architecture, we assume three page sizes: 1GB, 2MB, and 4KB. Correspondingly, we name the three hECPTs as PUD-, PMD-, and PTE-hECPT, respectively, and the three gECPTs as PUD-, PMD-, and PTE-gECPT, respectively. Note that we are not limited to these specific page sizes, as ECPTs can support any page sizes. Each hECPT and gECPT is organized in 𝑑 ways.</p><p>We describe the Plain Nested ECPT design in two steps. First, we do not limit the number of parallel memory accesses issued. Then, we augment the design with special caches <ref type="bibr" target="#b77">[79]</ref> to minimize the number of parallel memory accesses. We use the term Nested ECPT Walk to refer to a nested page walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design without Limiting Memory Accesses</head><p>Figure <ref type="figure" target="#fig_4">4</ref> shows how the design supports the three steps of a nested translation from Figure <ref type="figure" target="#fig_3">3</ref>. We assume that each ECPT has 3 ways. We now discuss each step.</p><p>Step 1 : From gVA to hPTE. This step takes the Virtual Page Number (VPN) of the gVA and obtains potentially multiple hECPT entries that may contain a pointer to the gECPT entry with the gPA. The entries in hECPTs are referred to as hPTEs.</p><p>The process is shown in Figure <ref type="figure" target="#fig_4">4</ref>. Given that the gVA can reside in guest pages of 𝑛 = 3 different sizes, potentially all the three gECPTs are looked-up (PUD-, PMD-, and PTE-gECPT). Each of these gECPTs has 𝑑 = 3 ways which, potentially, also need to be looked-up. Each way of each gECPT is pointed to by a gCR3 𝑖,𝑗 register, where 𝑖 is the page size and 𝑗 is the way ID. Note that each gECPT and way can potentially use a different guest hash function (gH) but, for simplicity, Figure <ref type="figure" target="#fig_4">4</ref> shows a single gH for all the ways and all the gECPTs. Overall, in summary, the VPN of the gVA is first hashed with the corresponding gH for each gECPT and way, and then added to the corresponding gCR3 𝑖,𝑗 . The result is 𝑛 × 𝑑 addresses which are gPAs of gECPT entries.</p><p>Each of these addresses needs to be translated to host physical. Since the desired translation can reside in host pages of different sizes, the hardware needs to check potentially all 𝑛 hECPTs (i.e., (bits 47-15) (bits 47-33)</p><formula xml:id="formula_1">+ + + + + + hCR3i,j + + + + + + + + + gPA1 … gPAn×d hPA1 … hPA𝑛 ! ×𝑑 !</formula><p>gHi,j gHi,j gHi,j gHi,j gHi,j gHi,j gHi,j gHi,j gHi,j hHi,j hHi,j hHi,j hHi,j hHi,j hHi,j hHi,j hHi,j hHi,j PUD-, PMD-, and PTE-hECPT) and, in each one, potentially all 𝑑 ways. Hence, for each hECPT and way, the address is first hashed with the corresponding ℎ𝐻 , and then added to the corresponding hCR3 𝑖,𝑗 . Overall, the total number of parallel accesses to hECPTs in Step 1 is potentially</p><formula xml:id="formula_2">𝑛 2 × 𝑑 2 .</formula><p>In practice, the number of accesses is much smaller thanks to the caching structures that will be described later. Section 9 evaluates several applications with 𝑛 = 3 and 𝑑 = 3 and shows that the average number of parallel hECPT accesses in Step 1 is 2.8.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows again the worst-case Step 1 with 𝑛 = 3 and 𝑑 = 3. The three ways of the three gECPTs are looked up using the correct gVA bits, assuming that each table entry contains eight consecutive translation entries with a single tag. In each case, the correct gH 𝑖,𝑗 and gCR3 𝑖,𝑗 are used. For each of the resulting 𝑛 × 𝑑 gPAs, the process is repeated in the hECPTs, creating a worst-case 𝑛 2 × 𝑑 2 hPAs.</p><p>At this point, each of these hECPT accesses will read an hPTE and check the tag for a match. Note that the hPTEs are tagged with host VPN, not with guest VPN. For each gPA 𝑖 obtained in the first step of Figure <ref type="figure" target="#fig_5">5</ref>, at most only one of the 𝑛 × 𝑑 hECPT accesses will declare a tag match. But, since the hardware is looking for a guest VPN, the hardware will not know at the end of Step 1 which of these potential 𝑛 × 𝑑 tag-hitting hPTEs is the one that contains a pointer to the desired gECPT entry. For that, it will need to execute Step 2 .</p><p>Step 2 : From hPTE to gPA of the data page. This step uses the pointers in the (at most) 𝑛 × 𝑑 matching hPTEs to access gECPT entries. Then, the hardware checks the tags of these gECPT entries for a match with the original gVA VPN. At most one gECPT entry succeeds. The contents of this gPTE has the gPA of the data page.</p><p>While, in the worst case, 𝑛 × 𝑑 parallel accesses are needed in Step 2 , in practice, our experiments with 𝑛 = 3 and 𝑑 = 3 show an average of 2.8 parallel accesses in Step 2 .</p><p>Step 3 : From gPA to hPA of the data page. This final step finds the hPA as shown in Figure <ref type="figure" target="#fig_4">4</ref>. In the worst case, it has to access each of the 𝑛 hECPTs and, in an hECPT, each of its 𝑑 ways. For each hECPT and way, the gPA is hashed with the corresponding hH, and then added to the corresponding hCR3 𝑖,𝑗 . Of the resulting accesses to the hECPTs, at most one finds an hPTE with matching gPA VPN. That hPTE contains the hPA of the data page (Figure <ref type="figure" target="#fig_3">3</ref>).</p><p>In the worst case, this step involves 𝑛 × 𝑑 parallel accesses to the hECPTs. In practice, our experiments with 𝑛 = 3 and 𝑑 = 3 show an average of 1.6 parallel accesses in Step 3 .</p><p>Overall, a Nested ECPT walk requires, in the very worst case, 𝑛 2 × 𝑑 2 parallel memory accesses, then 𝑛 × 𝑑 parallel memory accesses, and then 𝑛 × 𝑑 parallel accesses. In practice, using the caching structures described next, the average number of accesses observed are 2.8, then 2.8, and then 1.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Augmenting the Design with Caches</head><p>The ECPT design for native translations <ref type="bibr" target="#b77">[79]</ref> includes Cuckoo Walk Tables (CWTs), which are software structures that help reduce the number of parallel look-ups in a page table walk. There is one CWT for each page size (i.e., PUD-CWT, PMD-CWT, and PTE-CWT), which contains information about which way of the ECPT (if any) has a given translation. Further, entries from these CWTs are cached in a special hardware Cuckoo Walk Cache (CWC) in the MMU. After a TLB miss, the hardware first accesses the CWC and, on a hit, learns the subset of ECPTs (i.e., PUD, PMD, or PTE) and of ways in these ECPTs that it needs to access. In the best case, the hardware only needs to access one way of one ECPT.</p><p>In this paper, we propose to have both guest and host Cuckoo Walk Tables (gCWTs and hCWTs), and one guest and one host Cuckoo Walk Cache (gCWC and hCWC). The guest OS manages gCWTs, while the hypervisor manages hCWTs. The hardware automatically accesses the gCWC and hCWC during translation.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the Nested ECPT walk from Figure <ref type="figure" target="#fig_4">4</ref> optimized with CWCs. It shows the best case, where each look-up of gCWC and hCWC determines that a single memory access is needed. For this reason, the figure shows a single arrow coming out of every CWC, and the use of only one CR3 (generically represented by CR3 𝑖0,𝑗0 ). In this case, the walk requires only three memory accesses, which are shown with an asterisk where they occur. After the walk completes, the hPA is loaded into the TLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PARALLEL NESTED TRANSLATION: ADVANCED DESIGN</head><p>Unfortunately, as we will see in the evaluation, the Plain Nested ECPT design delivers only about 5% average performance gains over nested radix page tables. The reason is that an ECPT design that worked well in a native environment sometimes results in many parallel memory accesses in a nested ECPT walk, and consumes substantial bandwidth. Consequently, to improve the nested ECPT design, we analyze the translation mechanisms and redesign some of them so that fewer parallel memory accesses are issued. Specifically, we focus on three aspects of the Nested ECPT design. The first one concerns misses in the gCWC. After a miss, as the hardware attempts to load the corresponding entry into the gCWC in the background, the hardware needs to find the correct memory location to load from. For that, it needs to first translate the missing guest address into a host address, causing additional memory accesses.</p><p>The second issue has to do with PTE hCWT data. The native ECPT design did not cache PTE CWT data in the CWC due to insufficient locality. In a nested ECPT design, the PTE hCWT data does benefit from being cached-in some cases adaptively.</p><p>The final issue is that a nested environment can leverage system knowledge of the page size used by the page tables.</p><p>In this section, we modify the Plain Nested ECPT design to address these issues. We call the new design Advanced Nested ECPTs. We consider each of the three issues in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Translating Guest Cuckoo Walk Tables</head><p>During a walk, when a CWC misses, the hardware has no choice but to continue the translation by issuing the 𝑛 ×𝑑 accesses in that step. Then, in the background, the hardware accesses the corresponding CWT and loads the missing entry into the CWC.</p><p>If the miss occurred in the hCWC and, therefore, an hCWT entry needs to be accessed, the hardware directly accesses the hCWT. However, if the miss occurred in the gCWC, as the hardware tries to access the corresponding gCWT entry, it finds that it only knows the guest PA of it. Therefore, it first needs to go through a translation step to locate the host PA of the gCWT entry. After that, the hardware can proceed with the access to the gCWT as in the case of the hCWT.</p><p>Locating the host PA of a gCWT entry involves a process similar to the translation of a gPA to an hPA shown in Step 3 of Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>Unfortunately, this translation process introduces operations and memory traffic in the background that hurt system performance and can also potentially slow down subsequent accesses to the gCWC.</p><p>To eliminate these problems, we consider two possible approaches. One is for the hypervisor to map the gCWTs to a known, contiguous region in the host physical memory. In this way, the hardware can directly access the gCWTs without the need for any additional translation. This design is simple and feasible because gCWTs are very small in size. However, it would require that the guest OS communicate the allocation of the gCWTs to the hypervisor, and thus makes the virtualization process less transparent.</p><p>The second approach is to cache the gPA to hPA translations of gCWT entries in a very small MMU cache. We call this new cache the Shortcut Translation Cache (STC). This is a more virtualizationfriendly approach. Our Advanced design uses this approach. With the STC, we remove the operations and memory traffic involved in translating gCWT accesses, improving system performance. We will see in Section 9.4 that a 10-entry STC achieves a hit rate close to 100%.</p><p>Intuitively, the STC caches the translations of gCWT data in a manner logically similar to how the Nested TLB <ref type="bibr" target="#b15">[16]</ref> caches the translations of radix page tables. Both structures cache translations of guest PAs to host PAs, although the addresses correspond to completely different data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Caching PTE hCWT Entries</head><p>In a native ECPT design, when accessing PTE-ECPTs, caching PTE CWT entries in the CWC could reduce the number of memory accesses. Unfortunately, since the CWC is small, applications with highly-random access patterns can lead to CWC thrashing. The result is increased memory accesses due to subsequent fetches of the needed PTE CWT entries. As a result, the native ECPT design <ref type="bibr" target="#b77">[79]</ref> opted not to use a PTE CWT. Similarly, the Plain Nested ECPT design uses no PTE gCWT or PTE hCWT.</p><p>In the Advanced design, we again do not use a PTE gCWT for the guest due to poor locality, and also due to a new optimization that we introduce in Section 4.3. However, we find that using this structure for the host (PTE hCWT) and caching it in the hCWC can be beneficial. There are two opportunities to use the hCWC, shown in Steps 1 and 3 in Figure <ref type="figure" target="#fig_6">6</ref>. We consider each case in turn. Caching PTE hCWT Entries in Step 1 . This step probes the hECPTs to identify the location of gECPT entries. Due to the small size of the gECPTs (especially compared to the address space of the application and its data), and due to the large coverage per entry, this step enjoys very high locality. As a result, in our Advanced design, in Step 1 , we use a PTE hCWT and cache it in the hCWC.</p><p>Adaptive Caching of PTE hCWT Entries in Step 3 . This step probes the hECPTs to identify the location of the requested data pages. The locality of such accesses is very application dependent. Some applications exhibit page locality, while others do not. Therefore, in our Advanced design, we use Adaptive Caching of PTE hCWT entries to a different hCWC in Step 3 . Specifically, we start by enabling the caching of PTE hCWT entries and, as the application runs, monitor the hit rate of such entries in the hCWC. If their hit rate is low, then PTE hCWT caching is disabled. Then, the hardware monitors the hit rate of the PMD hCWT entries (the next level of cuckoo walk tables) in the hCWC. If the hit rate of PMD hCWT entries in the hCWC is very high, the caching of PTE hCWT entries is re-enabled. By monitoring both hit rates, individual applications typically converge soon to one of the two states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Leveraging Page Sizes Used by Page Tables</head><p>In a nested environment, if we know that page tables use pages of only a single size, we can optimize page walks. For example, assume that we know that page tables use only 4KB pages. We can then trim the number of parallel memory accesses performed during the second part of Step 1 in Figure <ref type="figure" target="#fig_4">4</ref>. At that time, the hardware looks-up the hECPTs, looking for an hPTE entry that points to the gECPTs (Figure <ref type="figure" target="#fig_3">3</ref>). We know that the gECPTs are allocated in 4KB pages. Hence, only the PTE-hECPT needs to be looked-up, and the PUD-hECPT and PMD-hECPT can be skipped.</p><p>While this optimization may not apply in the future, it is applicable to today's state-of-the-art systems. Specifically, in hypervisors such as KVM, host page tables are limited to using only 4KB page allocations <ref type="bibr" target="#b54">[55]</ref>. Similarly, OS kernels only use 4KB pages for native page tables and, in virtualized environments, for guest page tables. One reason for this choice is the fact that the page tables for most processes are relatively small and, hence, using large pages would lead to significant memory waste. In addition, 4KB pages provide flexibility when main memory is fragmented, and can be allocated and initialized quickly. Finally and perhaps as importantly, legacy reasons have resulted in popular processors using only 4KB pages for page tables.</p><p>In our Advanced design, we assume that page tables use only 4KB pages and apply this optimization. As we will see in Section 9.1, this improvement has a minor impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Avoiding Stale hECPT Entries in the Plain and Advanced Designs</head><p>We point out a design decision that, because of its basic importance, we apply to both the Plain and the Advanced Nested ECPT designs.</p><p>To understand it, consider nested radix page tables (Figure <ref type="figure" target="#fig_1">2</ref>). There, caching the address translation of a level of the guest page table (gL 𝑖 ) in the NTLB is beneficial. The closest parallel to NTLBs in Nested ECPTs would be to cache in the MMU the translation of hPTEs-to-gPTEs in Step 2 of Figure <ref type="figure" target="#fig_6">6</ref>. If this worked, one would eliminate one of the three sequential memory access steps of Nested ECPTs. However, we find that this approach is not desirable. The reason is that, in Nested ECPT systems, the hPA of a gPTE changes often, for two reasons. First, due to cuckoo rehashing, inserting an entry in a gECPT may shuffle other gPTEs between the 𝑑 ways of the gECPT.</p><p>Second, due to the dynamic resizing of a gECPT, entries from the old gECPT are migrated to the new one. In either case, when the hPA of a gPTE changes, the hPTE that maintained the original pointer to the gPTE becomes stale. To avoid flushing such translations, neither the Plain nor the Advanced Nested ECPT design caches the mapping of hPTEs-to-gPTEs in Step 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OVERALL DESIGN</head><p>From now on, we refer to the Advanced design as simply the Nested ECPT design. Figure <ref type="figure" target="#fig_7">7</ref> shows the complete layout of the guest and host memories, and the modules in the MMU in Nested ECPTs. For simplicity, the three gECPTs, three hECPTs, three gCWTs, and three hCWTs are each combined into a single box. Further, in host memory, we only show the single relevant entry of the gECPTs (called gPTE in the figure) and of the gCWTs (called gCWT entry in the figure). This is because such tables may not be contiguous in host memory. The solid arrows show the steps of a nested translation after a miss. The steps are numbered as in Figure <ref type="figure" target="#fig_6">6</ref>. In Step 1 , the hardware takes the gVA, looks-up the gCWC and hCWC and, in the best case, issues a single memory access that reads an hPTE. In Step 2 , the hardware uses the contents of the hPTE to issue a memory access that reads the gPTE with the target gPA. Finally, in Step 3 , the hardware takes the gPA, looks-up the hCWC and, in the best case, issues a single memory access to read the hPTE that contains the target hPA. The pair {gVA, hPA} is loaded into the TLB.</p><p>The dashed arrows show the operations when CWCs miss. On an hCWC miss, an entry from the corresponding hCWT is loaded into the hCWC. On a gCWC miss, as described in Section 4.1, the hardware first finds the hPTE that contains the host physical address of the corresponding gCWT entry. Assume that this hPTE is the bottom-most hPTE in the figure. Then, the hardware loads this hPTE into the STC for fast translation in the future. Finally, the hardware uses this hPTE to access the target gCWT entry, which it loads into the gCWC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MIGRATION PATH TO NESTED ECPTS</head><p>Nested ECPTs are a radical redesign of the page tables. To move current nested radix page table systems to Nested ECPTs, we propose two possible migration paths. One is for the machine to fully support both nested radix page tables and Nested ECPTs. At machine boot-up, a control register selects one of the two page table designs. This approach provides flexibility. However, it is clumsy and requires the hardware logic and MMU caching structures of the two approaches.</p><p>A more reasonable migration path is to use radix page tables in the guest OS and ECPTs in the host. This hybrid design supports legacy OS kernels. Thanks to the VM abstraction, the guest OS does not need changes, while the hypervisor is modified to support ECPTs for high performance.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows a nested page walk in this Hybrid Design. We start from the design in Figure <ref type="figure" target="#fig_1">2</ref> and replace the four ℎ𝐿 𝑖 steps in each level of guest radix page table with Step 3 in Figure <ref type="figure" target="#fig_4">4</ref>. This step translates the gPA of an entry in Level i of the guest radix page table to its hPA. As discussed in Section 3.2, this step tries to use the hCWC to obtain the target hPTE in a single memory access, as shown in Step 3 of Figure <ref type="figure" target="#fig_6">6</ref>. Finally, once the gPA of the target data page is found in Step 8, a final Step 3 from Figure <ref type="figure" target="#fig_4">4</ref> is used to locate its hPA. A nested page walk now involves 9 sequential steps. As in the nested radix design, NTLBs can be used to eliminate these look-ups. NTLBs' operation is shown with dashed lines. + gL 4 gPA gCR3 gVA gVA <ref type="bibr">[47:39]</ref> + gVA <ref type="bibr">[38:30]</ref> gVA <ref type="bibr">[29:21]</ref> gVA <ref type="bibr">[20:12]</ref> gVA <ref type="bibr">[11:0]</ref>  The hCWC used in each row of the walk is slightly different because the data locality of PTE-CWTs decreases as we move from top to bottom. Specifically, based on our experiments, the hCWC in the first and second rows from the top cache PUD-, PMD-, and PTE-CWTs. The hCWC in the third row caches PUD-and PMD-CWTs and, adaptively, PTE-CWTs. The hCWC in the fourth and fifth row only cache PUD-and PMD-CWTs. This hybrid design replaces some of the sequential steps of radix page tables with the parallel steps of ECPTs. It is a design point that sits in between the two approaches. While this design is easy to use, it still largely requires the hardware resources of the two prior approaches: hCWCs, radix page walk caches, and NTLB caches. They all co-exist in the MMU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OS/HYPERVISOR SUPPORT</head><p>This paper focuses on the architecture design of Nested ECPTs. Nested translations spend practically all of their time in hardwareassisted translation in user space, not in the OS or hypervisor software. The OS is invoked only in page faults, which are rare in the steady state of our applications.</p><p>For this reason, we can model and evaluate Nested ECPTs with a full-system simulator running KVM-based full-featured VMs on simulated hardware-without implementing ECPTs in Linux or KVM. Specifically, we use the Simics <ref type="bibr" target="#b58">[59]</ref> full-system simulator and instrument the OS with the Intel SAE <ref type="bibr" target="#b21">[22]</ref>. The simulator intercepts on-the-fly all the virtual memory operations of the KVM and guest OS (plus all the instructions executed). These operations are then processed by our back-end cycle-level processor/memory simulator. We leverage Simics to provide the actual memory and page table contents to the timing backend for each memory address of both the host and the guest. With modest modifications to Linux and KVM, this methodology allows us to implement and evaluate any page table organization in the simulator, while using the actual page table entries, support THP, and reflect any updates by guest and host. The high-level OS-and hypervisor-level memory management operations remain the same, as they are unaware of the underlying page table structures.</p><p>Our future work is to implement ECPTs in Linux and KVM. In this case, the ECPT designs, including both native and Nested ECPTs, will maintain the same interfaces as the radix page tables in the virtual memory system. Their support in the OS and hypervisor is relatively easy. Conceptually, only the page table implementations need to be replaced to support ECPTs.</p><p>In the Linux kernel, the majority of the changes required to support ECPTs are in the memory management code and under the page table handling <ref type="bibr" target="#b56">[57]</ref>. Most of the page table functionality is handled in the kernel through a set of macros and functions that assist in allocating page tables and locating page table entries. Supporting ECPTs requires modifications to these functions. However, page table usage (e.g., checking the dirty bit) and modification operations (e.g., setting the present bit) remain the same. This is because these functions operate on a per page-table entry basis, which remains practically the same with ECPTs. Furthermore, since this functionality is reused by KVM and the guest OS, the support of ECPTs is naturally reused in Nested ECPTs. Overall, the changes added for ECPTs and Nested ECPTs are likely to be hidden behind a relatively small interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION METHODOLOGY</head><p>Modeled Architectures. We use the Simics <ref type="bibr" target="#b58">[59]</ref> full-system simulator integrated with the SST framework <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b72">74]</ref> and the DRAM-Sim2 <ref type="bibr" target="#b73">[75]</ref> memory simulator to model a server architecture with 8 cores and 80GB of main memory. We model the ten page table architecture configurations shown in Table <ref type="table" target="#tab_2">1</ref>. For both native and nested page tables, we model a system with radix page tables and a system with our Advanced ECPT design. We also model the Nested Hybrid Design of Section 6. In all cases, we model environments that only use 4KB pages and environments that also use 2MB pages, by enabling Transparent Huge Pages (THP) in Linux <ref type="bibr" target="#b82">[84]</ref>. For the nested environments, we deploy a KVM VM that runs on the 8 cores of the host and can utilize 80GB of memory. The nested THP enables THP for both the host and the guest. The architecture parameters are shown in Table <ref type="table" target="#tab_3">2</ref>. Each core has private L1 and L2 caches. The L3 cache is shared and physically distributed. Cache misses are handled through Miss Status Handling Registers (MSHRs). Each core has private L1 and L2 TLBs, and 4 page table walkers. The radix page tables have a per-core page walk cache (PWC) and, when nested, they additionally have a per-core nested PWC (NPWC) and a per-core Nested TLB (NTLB). Table <ref type="table" target="#tab_3">2</ref> shows the sizes of the guest and host structures in all the nested designs; the sizes in the native designs are the same as the guest ones in the nested designs. There is no PTE-gCWT in the nested design and no PTE-CWT in the native one because of the low locality of the data. Note that, in Nested ECPTs, we use separate hCWCs for Step 1 and Step 3 .</p><p>To be conservative, we sized the structures in the ECPT designs to make their total size strictly smaller than those in the radix designs. The MMU caches in Radix, ECPTs, Nested Radix, Nested ECPTs, and Nested Hybrid use 768, 672, 1680, 1488, and 1408 bytes, respectively. Table <ref type="table" target="#tab_4">3</ref> reports the estimated area and power of these structures. For the measurements, we use Cacti <ref type="bibr" target="#b11">[12]</ref> with 22nm technology. From the table, we see that these structures consume little area and power in all the designs. Applications. We evaluate a variety of applications with different levels of TLB pressure. Table <ref type="table" target="#tab_5">4</ref> shows the domain, the suite, the name, and the memory footprint for each application. For each application, we perform full-system simulations of all the different configurations evaluated. We instrument the applications to identify the region of interest. In that region, we warm-up the architectural state for 50M instructions, and then measure 500M instructions. Our simulation methodology is deterministic, producing the same result for every run that we start from a given checkpoint. For this reason, our plots in the next section do not show any error bars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EVALUATION 9.1 Performance of Nested ECPTs</head><p>Figure <ref type="figure">9</ref> shows the speedup of the different architecture configurations of Table <ref type="table" target="#tab_2">1</ref> over the Nested Radix configuration. The figure shows the results for each application and the geometric mean of all the applications. The native configurations are only shown in  where Nested Radix services relatively more page walk accesses from main memory compared to Nested ECPTs. Consider now the two nested configurations with huge pages: Nested ECPTs THP (second bars) and Nested Radix THP (sixth bars). Huge pages improve performance significantly. This is especially the case for GUPS, which can exploit huge pages for the whole dataset, and SysBench. With huge pages, Nested ECPTs THP speedsup the applications over Nested Radix THP even more: 1.05x-1.59x, with an average of 1.24x.</p><p>Overall, replacing radix page tables with ECPTs in a nested environment improves the performance across the board for all programs, often substantially.</p><p>Consider now the new techniques. Without any new technique, the average speedup of Nested ECPTs over Nested Radix is only 3% and 5% without and with THP, respectively. Hence, the new techniques are responsible for most of the speedups of Nested ECPTs. From Figure <ref type="figure">9</ref>, without THP, the average speedup contributions of STC, Step-1 Caching, Step-3 Adaptive Caching, and 4KB Page Allocation are 6.8%, 4.6%, 4.2%, and 0.4%, respectively. With THP, the speedup contributions are 7.9%, 6.5%, 4.1%, and 0.5%. Clearly, the first three techniques have a substantial impact. The high effectiveness of STC is due, in part, to the fact that it reduces the number of MMU-initiated L2 misses by 17%.</p><p>The fourth technique does not help the steady-state of the applications much. However, it speeds-up page walks during the warmup, when walks are more expensive: the lack of cached information causes a walk to access ECPTs for all page sizes. Our technique minimizes this effect. While not shown in the figure, if we include the warm-up period, 4KB Page Table Allocation speeds-up the 95th percentile tail latency of the page walks for the whole application by an average of 9.4% (no THP) and 8.9% (with THP). So, it is also important.</p><p>Consider now the Nested Hybrid design. For all the applications, Nested Hybrid performs better than Nested Radix but worse than Nested ECPTs. On average, Nested ECPTs is 7% and 11% faster than Nested Hybrid for 4KB pages and THP, respectively. Still, on average, Nested Hybrid outperforms Nested Radix by 12% and 13% for 4KB pages and THP, respectively. The results highlight the significant performance improvement attained by only migrating the host to ECPTs.</p><p>For reference, the figure also shows the speedups of the native configurations, relative to Nested Radix. As expected, the native configurations are generally faster than the nested ones, since they do less work. However, there are a few exceptions where the nested designs with huge pages deliver higher speedups than the native ones without huge pages. This effect occurs in applications where huge pages are highly useful, such as in GUPS, SysBench, and DC. We have verified some of this behavior with real-systems measurements. Due to these applications, we see that the average speedup of Nested ECPTs THP over Radix is 1.11x.</p><p>To gain further insight, Figure <ref type="figure" target="#fig_0">10</ref> shows the number of MMU busy cycles in Nested Radix and Nested ECPTs, normalized to Nested Radix. These are cycles when the MMU is busy servicing L2 TLB misses, including when the MMU is waiting for its outstanding memory requests. The figure shows that Nested ECPTs designs uniformly spend substantially fewer cycles in translation than Nested Radix designs. On average, Nested ECPTs use 25% and 31% fewer MMU busy cycles than Nested Radix for 4KB-only and THP. The per-application results in Figure <ref type="figure" target="#fig_0">10</ref> are only weakly correlated with Figure <ref type="figure">9</ref>, since Figure <ref type="figure" target="#fig_0">10</ref> shows normalized cycle counts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">MMU and Cache Characterization</head><p>The performance gains provided by Nested ECPTs are due to two effects. The first one is its ability to issue memory accesses in parallel when performing the nested page translation-even if it issues more accesses than Nested Radix. The second effect is that Nested ECPTs only fetch into the caches actual translations, building-up useful state in the memory hierarchy. This is in contrast to Nested Radix, which fetches many intermediate translation entries during a walk. These entries cause pollution in the memory hierarchy.</p><p>To understand these effects, Fig. <ref type="figure" target="#fig_10">13</ref> characterizes the behavior of the MMU, L2 cache, and L3 cache for the nested environments. Starting from the top, Figure <ref type="figure" target="#fig_10">13</ref>(a), shows the number of requests that the MMU issues to the cache hierarchy per Kilo instruction (RPKI). In Nested Radix, these requests are those issued to obtain a translation, while in Nested ECPTs, they are those that request translations and those that request hCWT/gCWT entries. We can see that the ECPT configurations issue more requests-on average 13% and 15% more for 4KB pages and THP, respectively. However, many of these accesses are issued in parallel.  These misses are initiated by both the MMU and the processor. Across configurations, the differences are mostly caused by the requests issued by the MMU. We see that the requests issued by the MMU in Nested ECPT designs enjoy higher cache locality than those in Nested Radix: in the L2, both designs have similar average MPKI; in the L3, Nested ECPTs has a 10% and 11% lower MPKI than Nested Radix for 4KB-only pages and THP, respectively. Therefore, while Nested ECPTs issue more MMU requests, they end up issuing fewer main memory accesses. As indicated above, a major reason is that they only fetch actual translations, while Nested Radix fetch intermediate translations, polluting caches.</p><p>The L2 and L3 miss patterns in Nested ECPTs are not any more bursty or demanding than in Nested Radix. On average, with Nested ECPTs, the L2 and L3 use 4.4 and 3.8 MSHRs, respectively, at a time. The same numbers for Nested ECPTs THP are 4.2 and 3.6. The maximum number of MSHRs in use in L2 or L3 is 12.</p><p>To shed additional light, Figure <ref type="figure" target="#fig_0">11</ref> shows a histogram of the latency of the nested page walks in the MUMmer application for Nested Radix THP and Nested ECPTs THP. For each page walk, we measure the latency in cycles from when the L2 TLB miss occurs until the page walk completes. Then, we group the page walks in bins according to their latency. The figure is annotated with the ranges of latencies for caches, 1st DRAM access, 2nd, 3rd, and so on. We see from the figure that Nested Radix THP exhibits a long tail of page walks of several hundreds of cycles. This is because of the sequential pointer chasing process that Nested Radix imposes.</p><p>In contrast, Nested ECPTs THP page walks are typically over with a latency equivalent to about four DRAM accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Characterizing Nested ECPT Walks</head><p>When the hardware accesses the hCWC or gCWC in a nested ECPT walk (Figure <ref type="figure" target="#fig_6">6</ref>), it may issue from 1 to 𝑛 × 𝑑 accesses. The paper that introduced ECPTs for native environments <ref type="bibr" target="#b77">[79]</ref> used a naming convention to refer to the different possible outcomes: Direct Walk if it issues 1 memory access, Size Walk if it accesses all the 𝑑 ways of one ECPT, Partial Walk if it accesses at worst all the ways of two ECPTs, and Complete Walk if it accesses all the 𝑑 ways of all the 𝑛 ECPTs. Outcomes with few accesses are preferred.</p><p>Figure <ref type="figure" target="#fig_12">14</ref> shows the relative frequency of these types of outcomes for Nested ECPTs THP using the complete mappings of the applications. For each application, the left bar is the information provided by the hCWT, while the right bar is the information provided by the gCWT. The gCWT aims to trim the first part of Step 1 of the walk as shown in Figure <ref type="figure" target="#fig_4">4</ref>. The hCWT prunes the second part of Step 1 as well as Step 3 . Figure <ref type="figure" target="#fig_12">14</ref> shows that, in the host, the majority of the walks are the very cheap direct walks (90% on average). Direct walks are common because the hypervisor frequently uses huge pages. In the guest, the majority of the walks are size walks (82% on average). The exceptions are GUPS, SysBench and MUMmer, where huge pages are very effective and, therefore, direct walks dominate. Complete walks are negligible for both guest and host.</p><p>Overall, most of the information obtained from CWTs results in outcomes with few accesses. It can be shown that, on average for our applications, a Nested ECPT walk (Figure <ref type="figure" target="#fig_4">4</ref>) with THP issues 2.8 parallel memory accesses in Step 1 , 2.8 in Step 2 , and 1.6 in Step 3 . Without THP, only Step 3 changes: it has slightly more accesses on average, namely 1.7.</p><p>Finally, we measure the average hit rates of MMU caches for Nested ECPT THP. The hit rate of our 10-entry STC is 99%. If we reduced its size to 8 and 4 entries, its hit rate would be ≈90% and ≈50%, respectively, which are too low. In the gCWC, the hit rates are 99% for its PUD entries and 86% for its PMD entries. In the hCWC, the hit rates are 99% for its PUD entries, 80% for its PMD entries and, for its PTE entries, 99% in Step 1 and 67% in Step 3 . Overall, gCWC and hCWC effectively reduce the number of accesses issued by nested ECPTs walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Memory Consumption</head><p>On average across all the applications, the memory required to hold all the page table entries is 60MB. This number is computed by multiplying the number of page table entries by 8 bytes, and therefore is independent of the page table organization chosen.</p><p>However, when we measure the memory used by all the virtual memory structures, the number is higher, due to various structure overheads: 84MB for Nested Radix (of which 56MB are for host and 28MB for guest structures) and 97MB for Nested ECPTs (of which 61MB are for host and 36MB for guest structures). Overall, Nested ECPTs only use slightly more memory than Nested Radix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">Comparison to Other Advanced Designs</head><p>Agile Paging <ref type="bibr" target="#b29">[30]</ref> combines nested and shadow paging <ref type="bibr" target="#b83">[85]</ref> by leveraging the idea that the page table entries of upper levels in the radix tree are unlikely to be changed. However, Agile Paging still requires 4 sequential memory accesses in the best case scenario, as well as some hypervisor intervention cost. We simulate an ideal Agile implementation with at most 4 sequential memory requests, all the caching structures of radix, and no hypervisor costs. Nested ECPTs outperform this ideal Agile Paging design by 16% on average. POM-TLB <ref type="bibr" target="#b74">[76]</ref> is a large TLB that is part of memory. Although the design eliminates a significant portion of page table walks, an L2 TLB miss may be propagated to the POM-TLB in DRAM and, on a miss, still require a page walk. simulate the POM-TLB design with a perfect page size predictor. On average, nested ECPTs outperform POM-TLB by 14%.</p><p>Flat nested page tables <ref type="bibr" target="#b2">[3]</ref> combine a guest radix page table with a host flat page table. The design reduces the maximum number of sequential memory references from 24 to 9. We simulate this design and find that Nested ECPTs outperforms flat nested page tables by 12% (no THP) and 15% (THP) on average. The limitation of flat nested page tables is the potentially up to 9 sequential memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">OTHER RELATED WORK</head><p>A number of studies have measured the overhead of nested page table walks for virtualized memory translation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b74">76]</ref>. To reduce TLB misses, prior work has proposed new TLB designs with support for clustering, coalescing, entry-sharing, speculation, multiple page sizes, and prefetching <ref type="bibr">[11, 14, 19, 20, 24, 34, 37, 51, 61, 63, 68-71, 77, 78, 81, 88]</ref>. Furthermore, a few virtualization-specific TLB designs have been proposed, including large part-of-memory TLBs <ref type="bibr" target="#b74">[76]</ref>, context-aware TLBs <ref type="bibr" target="#b59">[60]</ref>, and TLB designs with virtualization support <ref type="bibr" target="#b22">[23]</ref>.</p><p>To reduce nested page walk overhead, MMU caches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, devirtualized memory <ref type="bibr" target="#b38">[39]</ref>, application-managed memory translation <ref type="bibr" target="#b3">[4]</ref>, and optimized huge page support <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b65">67]</ref> have been proposed.</p><p>Moreover, other designs have been proposed to reduce the memory references required for nested page walks by exploiting virtual and physical address space contiguity <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. These approaches create translations that map very large contiguous regions of virtual memory to contiguous physical memory. As a result, the number of required translation entries reduces, potentially lowering overhead. While promising, these approaches face the challenge that creating very large contiguous physical address spaces in actively-used cloud platforms is hard. Indeed, even finding the more modest 2MB-sized pages supported by Linux Transparent Huge Pages (THP) is often hard <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56]</ref>. Going beyond them is harder. One important characteristic of Nested ECPTs is that it does not rely on the need for physical memory contiguity. This paper presented the first page table design for parallel nested address translation. The design, called Nested Elastic Cuckoo Page Tables (Nested ECPTs), eliminates all but three of the potentially twenty-four sequential steps of a nested radix page table translationwhile judiciously limiting the number of parallel memory accesses issued to avoid over-consuming cache hierarchy bandwidth. As a result, compared to conventional nested radix tables, Nested ECPTs speed-up the average execution time of a set of applications by 1.19x (for 4KB pages) and by 1.24x (when huge pages are used). In addition, we described a possible migration path from current systems to Nested ECPTs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Native page walk in the x86-64 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Nested page walk in x86-64. The numbers in the squares or circles are the steps in the translation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nested page walk with hashed page tables (HPTs) that unrealistically assumes no hash collisions, no multiple page sizes, and no page sharing. For simplicity, this figure shows a contiguous gHPT in host memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Nested ECPT walk with worst-case number of memory accesses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Worst-case Step 1 in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Nested ECPT walk with Cuckoo Walk Caches (CWCs). The figure shows the best-case memory accesses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Layout of the guest and host memory, and the modules in the MMU with Nested ECPTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Nested Hybrid page walk: the guest OS uses radix page tables and the host uses ECPTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Hit rate of the PTE hCWT entries (left) and PMD hCWT entries (right) in the hCWC. An interval is 5M cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Characterizing the MMU and cache subsystem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figures 13 (</head><label>13</label><figDesc>Figures13(b)  and13(c)  show the normalized misses PKI (MPKI) in the L2 and L3 caches, respectively, for the same configurations. These misses are initiated by both the MMU and the processor. Across configurations, the differences are mostly caused by the requests issued by the MMU. We see that the requests issued by the MMU in Nested ECPT designs enjoy higher cache locality than those in Nested Radix: in the L2, both designs have similar average MPKI; in the L3, Nested ECPTs has a 10% and 11% lower MPKI than Nested Radix for 4KB-only pages and THP, respectively. Therefore, while Nested ECPTs issue more MMU requests, they end up issuing fewer main memory accesses. As indicated above, a major reason is that they only fetch actual translations, while Nested Radix fetch intermediate translations, polluting caches.The L2 and L3 miss patterns in Nested ECPTs are not any more bursty or demanding than in Nested Radix. On average, with Nested ECPTs, the L2 and L3 use 4.4 and 3.8 MSHRs, respectively, at a time. The same numbers for Nested ECPTs THP are 4.2 and 3.6. The maximum number of MSHRs in use in L2 or L3 is 12.To shed additional light, Figure11shows a histogram of the latency of the nested page walks in the MUMmer application for Nested Radix THP and Nested ECPTs THP. For each page walk, we measure the latency in cycles from when the L2 TLB miss occurs until the page walk completes. Then, we group the page walks in bins according to their latency. The figure is annotated with the ranges of latencies for caches, 1st DRAM access, 2nd, 3rd, and so on. We see from the figure that Nested Radix THP exhibits a long tail of page walks of several hundreds of cycles. This is because of the sequential pointer chasing process that Nested Radix imposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Breakdown of the types of host (left bar) and guest walks (right bar) for each application in Nested ECPTs THP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>64. Square boxes are levels of the host page table (hL 𝑖 ) and circular boxes are levels of the guest page table (gL 𝑖 ). The translation stars with a gVA and produces an hPA.</figDesc><table><row><cell>gVA</cell><cell cols="2">gCR3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gVA[47:39]</cell><cell>+</cell><cell>gL 4 gPA</cell><cell>hL 4 1</cell><cell>hL 3 2</cell><cell>hL 2 3</cell><cell>hL 1 4</cell><cell cols="2">gL 4 5</cell></row><row><cell>gVA[38:30]</cell><cell>+</cell><cell>gL 3 gPA</cell><cell>hL 4 6</cell><cell>hL 3 7</cell><cell>hL 2 8</cell><cell>hL 1 9</cell><cell cols="2">gL 3 10</cell></row><row><cell>gVA[29:21]</cell><cell>+</cell><cell>gL 2 gPA</cell><cell>hL 4 11</cell><cell>hL 3 12</cell><cell>hL 2 13</cell><cell>hL 1 14</cell><cell cols="2">gL 2 15</cell></row><row><cell>gVA[20:12]</cell><cell>+</cell><cell>gL 1 gPA</cell><cell>hL 4 16</cell><cell>hL 3 17</cell><cell>hL 2 18</cell><cell>hL 1 19</cell><cell cols="2">gL 1 20</cell></row><row><cell>gVA[11:0]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Data Page gPA</cell><cell>hL 4 21</cell><cell>hL 3 22</cell><cell>hL 2 23</cell><cell>hL 1 24</cell><cell>+</cell><cell>hPA To TLB</cell></row><row><cell cols="2">EPTP</cell><cell></cell><cell></cell><cell></cell><cell cols="2">NTLB Caching</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Modeled page table architecture configurations.</figDesc><table><row><cell cols="2">Page Table Architecture Native Nested</cell><cell>Description</cell></row><row><cell>Radix</cell><cell>Nested Radix</cell><cell>Radix page tables with only 4KB pages</cell></row><row><cell>Radix THP</cell><cell>Nested Radix THP</cell><cell>Radix page tables with 4KB+huge pages</cell></row><row><cell>ECPTs</cell><cell>Nested ECPTs</cell><cell>Advanced ECPTs with only 4KB pages</cell></row><row><cell cols="2">ECPTs THP Nested ECPTs THP</cell><cell>Advanced ECPTs with 4KB + huge pages</cell></row><row><cell>-</cell><cell>Nested Hybrid</cell><cell>Hybrid Design with only 4KB pages</cell></row><row><cell>-</cell><cell cols="2">Nested Hybrid THP Hybrid Design with 4KB + huge pages</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Architectural parameters used in the evaluation.</figDesc><table><row><cell cols="2">Processor Parameters</cell></row><row><cell>Multicore chip</cell><cell>8 4-issue OoO cores, 128-entry ROB, 2GHz</cell></row><row><cell>L1 cache</cell><cell>32KB, 8-way, 2 cyc. round trip (RT), 64B line</cell></row><row><cell>L2 cache</cell><cell>512KB, 8-way, 16 cycles RT, 20 MSHRs</cell></row><row><cell>L3 cache</cell><cell>Slice: 2MB, 16-way, 56 cycles RT, 20 MSHRs</cell></row><row><cell cols="2">Per-Core MMU Parameters</cell></row><row><cell>L1 DTLB (4KB pages)</cell><cell>64 entries, 4-way, 2 cycles RT</cell></row><row><cell>L1 DTLB (2MB pages)</cell><cell>32 entries, 4-way, 2 cycles RT</cell></row><row><cell>L1 DTLB (1GB pages)</cell><cell>4 entries, 2 cycles RT</cell></row><row><cell>L2 DTLB (4KB pages)</cell><cell>1024 entries, 12-way, 12 cycles RT</cell></row><row><cell>L2 DTLB (2MB pages)</cell><cell>1024 entries, 12-way, 12 cycles RT</cell></row><row><cell>L2 DTLB (1GB pages)</cell><cell>16 entries, 4-way, 12 cycles RT</cell></row><row><cell cols="2">Radix Page Table Parameters</cell></row><row><cell>Nested TLB (NTLB)</cell><cell>24 entries, fully associative (FA), 4 cycles RT</cell></row><row><cell>Page Walk Cache (PWC)</cell><cell>3 levels, 32 entries/level, FA, 4 cycles RT</cell></row><row><cell>Nested PWC (NPWC)</cell><cell>5 levels, 16 entries/level, FA, 4 cycles RT</cell></row><row><cell cols="2">Elastic Cuckoo Page Table (ECPT) Parameters</cell></row><row><cell>Initial PTE gECPT/hECPT size</cell><cell>16384 entries × 3 ways</cell></row><row><cell cols="2">Initial PMD gECPT/hECPT size 16384 entries × 3 ways</cell></row><row><cell>Initial PUD gECPT/hECPT size</cell><cell>8192 entries × 3 ways</cell></row><row><cell>Initial PTE hCWT size</cell><cell>4096 entries × 2 ways</cell></row><row><cell>Initial PMD gCWT/hCWT size</cell><cell>4096 entries × 2 ways</cell></row><row><cell>Initial PUD gCWT/hCWT size</cell><cell>2048 entries × 2 ways</cell></row><row><cell>gCWC</cell><cell>16PMD + 2PUD entries, FA, 4 cycles RT</cell></row><row><cell>hCWC (in Step 1)</cell><cell>4PTE entries, FA, 4 cycles RT</cell></row><row><cell>hCWC (in Step 3)</cell><cell>16PTE + 4PMD + 2PUD, FA, 4 cycles RT</cell></row><row><cell>Shortcut Trans. Cache (STC)</cell><cell>10 entries, FA, 4 cycles RT</cell></row><row><cell>Hash functions: CRC</cell><cell>Latency: 2 cycles</cell></row><row><cell cols="2">Hybrid Design Parameters</cell></row><row><cell>Initial PTE hECPT size</cell><cell>16384 entries × 3 ways</cell></row><row><cell>Initial PMD hECPT size</cell><cell>16384 entries × 3 ways</cell></row><row><cell>Initial PUD hECPT size</cell><cell>8192 entries × 3 ways</cell></row><row><cell>Initial PTE hCWT size</cell><cell>4096 entries × 2 ways</cell></row><row><cell>Initial PMD hCWT size</cell><cell>4096 entries × 2 ways</cell></row><row><cell>Initial PUD hCWT size</cell><cell>2048 entries × 2 ways</cell></row><row><cell>hCWC</cell><cell>16PTE(Rows 1-3)+16PMD+2PUD, FA, 4 RT</cell></row><row><cell>Page Walk Cache (PWC)</cell><cell>16 entries, FA, 4 cycles RT</cell></row><row><cell>Nested TLB (NTLB)</cell><cell>24 entries, FA, 4 cycles RT</cell></row><row><cell cols="2">Main-Memory Parameters</cell></row><row><cell>Capacity; #Channels; #Banks</cell><cell>80GB; 4; 8</cell></row><row><cell>𝑡 𝑅𝑃 -𝑡 𝐶𝐴𝑆 -𝑡 𝑅𝐶𝐷 -𝑡 𝑅𝐴𝑆</cell><cell>11-11-11-28</cell></row><row><cell>Frequency; Data rate</cell><cell>1GHz; DDR</cell></row><row><cell cols="2">Host and VM Parameters</cell></row><row><cell>Host OS; Guest OS</cell><cell>Ubuntu Server 16.04; Ubuntu Cloud 16.04</cell></row><row><cell>Hypervisor</cell><cell>QEMU-KVM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Area and power of the hardware caches in the MMU. Page Table Allocation in 4KB Pages. The rest of the bar is the speedup of the Plain Nested ECPT design of Section 3.We focus first on the two nested configurations with only 4KB pages: Nested Radix (first bars) and Nested ECPTs (fifth bars). Nested ECPTs speeds-up the applications over Nested Radix by 1.04x-1.33x, with an average of 1.19x. The applications with the most speedup, like DC, MUMmer and SysBench, are typically those Speedup of the different architecture configurations over the Nested Radix configuration.</figDesc><table><row><cell>Configuration</cell><cell cols="3">Size (𝐵) Area (𝑚𝑚 2 ) Power (𝑚𝑊 )</cell></row><row><cell>Nested Radix</cell><cell>1680</cell><cell>0.01</cell><cell>2.9</cell></row><row><cell>Nested ECPTs</cell><cell>1488</cell><cell>0.03</cell><cell>5.2</cell></row><row><cell>Nested Hybrid</cell><cell>1408</cell><cell>0.02</cell><cell>2.8</cell></row><row><cell cols="4">the mean bars. Recall that the Nested ECPTs and Nested ECPTs</cell></row><row><cell cols="4">THP configurations are the Advanced design. To understand the</cell></row><row><cell cols="4">performance contributions of our new techniques of Section 4, their</cell></row><row><cell cols="4">bars are broken down into the effects of (i) STC, (ii) Step-1 PTE-</cell></row><row><cell cols="4">hCWT Caching, (iii) Step-3 PTE-hCWT Adaptive Caching, and</cell></row><row><cell>(iv)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Applications evaluated.</figDesc><table><row><cell></cell><cell cols="2">Application</cell><cell>Memory</cell></row><row><cell>Domain</cell><cell>Suite</cell><cell>Name</cell><cell>Footpr.</cell></row><row><cell></cell><cell></cell><cell>Betweenness Centrality (BC)</cell><cell>17.3 GB</cell></row><row><cell></cell><cell></cell><cell>Breadth-First Search (BFS)</cell><cell>9.3 GB</cell></row><row><cell></cell><cell></cell><cell>Connected Components (CC)</cell><cell>9.3 GB</cell></row><row><cell></cell><cell></cell><cell>Degree Centrality (DC)</cell><cell>9.3 GB</cell></row><row><cell cols="3">Graph analytics GraphBIG [64] Depth-First Search (DFS)</cell><cell>9.0 GB</cell></row><row><cell></cell><cell></cell><cell>PageRank (PR)</cell><cell>9.3 GB</cell></row><row><cell></cell><cell></cell><cell>Shortest Path (SSSP)</cell><cell>9.3 GB</cell></row><row><cell></cell><cell></cell><cell>Triangle Count (TC)</cell><cell>11.9 GB</cell></row><row><cell>HPC</cell><cell cols="2">Challenge [58] GUPS</cell><cell>64.0 GB</cell></row><row><cell>Bioinformatics</cell><cell>BioBench [5]</cell><cell>MUMmer</cell><cell>6.9 GB</cell></row><row><cell>Systems</cell><cell>SysBench [82]</cell><cell>SysBench</cell><cell>64.0 GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Histogram of the latency of the nested page walks in the MUMmer application.9.2 Impact of Adaptive PTE hCWT CachingRecall that, to support adaptive caching of PTE hCWT entries in Step 3 of Figure6, we measure the hit rates of both PMD hCWT and PTE hCWT entries in the hCWC. Figure12shows these values for our applications. The left chart shows that, in all the applications except GUPS and SysBench, PTE hCWT entries enjoy a very high hit rate in the hCWC. Therefore, the applications can benefit from enabling PTE hCWT caching. The right chart shows that, in GUPS and SysBench, PMD hCWT entries have a lower hit rate in the hCWC than in other applications. Based on this analysis, we define two thresholds (the dashed lines): if the hit rate of PTE hCWT entries is 0.5, we disable PTE hCWT caching; when PTE hCWT caching is disabled and the hit rate of PMD hCWT entries is above 0.85, we enable PTE hCWT caching.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Nested Radix THP</cell><cell></cell><cell cols="3">Nested ECPTs THP</cell><cell></cell><cell></cell></row><row><cell>0.04 0.06 0.08 0.10 0.12 Probability</cell><cell>Cache Hit</cell><cell>1st DRAM Access</cell><cell>2nd DRAM Access</cell><cell></cell><cell>3rd DRAM Access</cell><cell cols="3">4th DRAM Access</cell><cell></cell><cell cols="2">5th DRAM Access</cell><cell></cell><cell>6th DRAM Access</cell><cell>7th DRAM Access</cell></row><row><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell>90</cell><cell>170</cell><cell>250</cell><cell>330</cell><cell>410</cell><cell cols="2">490 MMU Cycles</cell><cell>570</cell><cell></cell><cell>650</cell><cell></cell><cell></cell><cell>730</cell><cell>810</cell><cell>890</cell><cell>970</cell></row><row><cell></cell><cell></cell><cell>Figure 11:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Nested Radix</cell><cell cols="3">Nested Radix THP</cell><cell>Nested ECPTs</cell><cell>Nested ECPTs THP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MMU Busy Cycles</cell><cell>0.4 0.6 0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0</cell><cell>BC</cell><cell>BFS</cell><cell>CC</cell><cell>DC</cell><cell>DFS</cell><cell>GUPS MUMmer</cell><cell>PR</cell><cell>SSSP SysBench</cell><cell>TC</cell><cell>GeoMean</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Figure 10: MMU busy cycles in nested configurations.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by NSF under grant CNS 1956007 and CNS 2107307. We thank Dan Tsafrir for his valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Comparison of Software and Hardware Techniques for x86 Virtualization</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Agesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XII)</title>
				<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XII)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Firecracker: Lightweight Virtualization for Serverless Applications</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Agache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Iordache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Piwonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana-Maria</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;20)</title>
				<meeting>the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting Hardwareassisted Page Walks for Virtualized Systems</title>
		<author>
			<persName><forename type="first">Jeongseob</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Seongwook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual International Symposium on Computer Architecture (ISCA&apos;12)</title>
				<meeting>the 39th Annual International Symposium on Computer Architecture (ISCA&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do-It-Yourself Virtual Memory Translation</title>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Mattan Erez, and Yoav Etsion</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BioBench: A Benchmark Suite of Bioinformatics Applications</title>
		<author>
			<persName><forename type="first">Kursad</forename><surname>Albayraktaroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau-Wen</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;05)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing and Exploiting Contiguity for Fast Memory Virtualization</title>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Alverti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Psomadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Computer Architecture (ISCA&apos;20)</title>
				<meeting>the 47th International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Konstantinos Nikas, Georgios Goumas, and Nectarios Koziris</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Web</forename><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName><surname>Services</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2" />
		<title level="m">Elastic Compute Cloud (EC2)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">AMD64 Virtualization Codenamed &quot;Pacifica&quot; Technology: Secure Virtual Machine Architecture Reference Manual</title>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<ptr target="http://developer.amd.com/wordpress/media/2012/10/NPT-WP-11-final-TM.pdf" />
		<title level="m">AMD-V 𝑇 𝑀 Nested Paging</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Samba: A Detailed Memory Management Unit (MMU) for the SST Simulation Framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Voskuilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hoekstra</surname></persName>
		</author>
		<idno>SAND2017-0002</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Sandia National Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software Prefetching and Caching for Translation Lookaside Buffers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kavita Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;94)</title>
				<meeting>the 1st USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;94)<address><addrLine>Monterey, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnav</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-06">2017. June 2017</date>
		</imprint>
	</monogr>
	<note>TACO)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Translation Caching: Skip, Don&apos;t Walk (the Page Table)</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 International Conference on Computer Architecture (ISCA&apos;10)</title>
				<meeting>the 2010 International Conference on Computer Architecture (ISCA&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SpecTLB: A Mechanism for Speculative Address Translation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual International Symposium on Computer Architecture (ISCA&apos;11)</title>
				<meeting>the 38th Annual International Symposium on Computer Architecture (ISCA&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Virtual Memory for Big Memory Servers</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)</title>
				<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accelerating Two-dimensional Page Walks for Virtualized Systems</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srilatha</forename><surname>Manne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)</title>
				<meeting>the 13th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XIII)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-reach Memory Management Unit Caches</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-46</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Translation-Triggered Prefetching</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</title>
				<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shared Last-level TLBs for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE 17th International Symposium on High Performance Computer Architecture (HPCA&apos;11)</title>
				<meeting>the 2011 IEEE 17th International Symposium on High Performance Computer Architecture (HPCA&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inter-Core Cooperative TLB Prefetchers for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)</title>
				<meeting>the 15th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Methodology for Performance Analysis of VMware vSphere under Tier-1 Applications</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Buell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Reza</forename><surname>Taheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VMWare Technical Journal</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simulation and Analysis Engine for Scale-Out Workloads</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Chachmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Richins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Christensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Supercomputing (ICS&apos;16)</title>
				<meeting>the 2016 International Conference on Supercomputing (ICS&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving Virtualization in the Presence of Software Managed Translation Lookaside Buffers</title>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubertus</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimi</forename><surname>Xenidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Computer Architecture (ISCA&apos;13)</title>
				<meeting>the 2013 International Conference on Computer Architecture (ISCA&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient Address Translation for Architectures with Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</title>
				<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">KVM/ARM: The Design and Implementation of the Linux ARM Hypervisor</title>
		<author>
			<persName><forename type="first">Christoffer</forename><surname>Dall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Nieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</title>
				<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing the Idle Task and Other MMU Tricks</title>
		<author>
			<persName><forename type="first">Cort</forename><surname>Dougan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mackerras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Yodaiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Symposium on Operating Systems Design and Implementation</title>
				<meeting>the Third Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Linux/ia64 Project: Kernel Design and Status Update</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Eranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mosberger</surname></persName>
		</author>
		<idno>HPL-2000-85</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Space Efficient Hash Tables with Worst Case Constant Access Time. Theory of</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Fotakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Spirakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="248" />
			<date type="published" when="2005-02">2005. Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-47</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Agile Paging: Exceeding the Best of Nested and Shadow Paging</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture (ISCA&apos;16)</title>
				<meeting>the 43rd International Symposium on Computer Architecture (ISCA&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large Pages May Be Harmful on NUMA Systems</title>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Decouchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Quéma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14)</title>
				<meeting>the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><surname>Google</surname></persName>
			<affiliation>
				<orgName type="collaboration">Cloud Compute Engine</orgName>
			</affiliation>
		</author>
		<ptr target="https://cloud.google.com/compute" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<ptr target="https://gvisor.dev/docs/" />
		<title level="m">Google. 2021. gVisor: Container Runtime Sandbox</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance Characteristics of Explicit Superpage Support</title>
		<author>
			<persName><forename type="first">Mel</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 International Conference on Computer Architecture (ISCA&apos;10)</title>
				<meeting>the 2010 International Conference on Computer Architecture (ISCA&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Itanium -A System Implementor&apos;s Tale</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Chubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mosberger-Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gernot</forename><surname>Heiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 USENIX Annual Technical Conference (USENIX ATC&apos;05)</title>
				<meeting>the 2005 USENIX Annual Technical Conference (USENIX ATC&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Proactively Breaking Large Pages to Improve Memory Overcommitment Performance in VMware ESXi</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongbeom</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Baskakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Virtual Execution Environments (VEE&apos;15)</title>
				<meeting>the 11th ACM International Conference on Virtual Execution Environments (VEE&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tailored Page Sizes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guvenilir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
				<meeting>the 2020 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework</title>
		<author>
			<persName><forename type="first">Nastaran</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Appavoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
				<meeting>the 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Devirtualizing Memory in Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Haria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Architectural Support for Translation Table Management in Large Address Space Machines</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International Symposium on Computer Architecture (ISCA&apos;93)</title>
				<meeting>the 20th Annual International Symposium on Computer Architecture (ISCA&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PowerPC Microprocessor Family: The Programming Environments Manual for 32 and 64-bit Microprocessors</title>
		<ptr target="https://wiki.alcf.anl.gov/images/f/fb/PowerPC_-_Assembly_-_IBM_Programming_Environment_2.3.pdf" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intel Virtualization Technology Specification for the IA-32 Intel Architecture</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/docs/processors/itanium/itanium-architecture-vol-1-2-3-4-reference-set-manual.html" />
		<title level="m">Itanium Architecture Software Developer&apos;s Manual</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/sites/default/files/managed/2b/80/5-level_paging_white_paper.pdf" />
		<title level="m">5-Level Paging and 5-Level EPT (White Paper</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/sunny_cove" />
		<title level="m">Sunny Cove Microarchitecture</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">and IA-32 Architectures Software Developer&apos;s Manual</title>
		<idno>Intel. 2019. 64</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html" />
		<title level="m">Intel® Optane™ Persistent Memory</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Basic Performance Measurements of the Intel Optane DC Persistent Memory Module</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirsaman</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">Joon</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramanya</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05714</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Look at Several Memory Management Units, TLB-refill Mechanisms, and Page Table Organizations</title>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</title>
				<meeting>the 8th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VIII)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">IBM POWER9 system software</title>
		<author>
			<persName><forename type="first">Joefon</forename><surname>Jann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mackerras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ludden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Ouren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Edelsohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018-06">2018. June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going the Distance for TLB Prefetching: An Application-driven Study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Symposium on Computer Architecture (ISCA&apos;02)</title>
				<meeting>the 29th International Symposium on Computer Architecture (ISCA&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Redundant Memory Mappings for Fast Access to Large Memories</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Ayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrián</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Ünsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)</title>
				<meeting>the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Energy-Efficient Address Translation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;16)</title>
				<meeting>2016 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Operating System Support for Virtual Machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">W</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 USENIX Annual Technical Conference (USENIX ATC&apos;03)</title>
				<meeting>the 2003 USENIX Annual Technical Conference (USENIX ATC&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<ptr target="https://git.kernel.org/pub/scm/virt/kvm/kvm.git/tree/arch/x86/mm/init_64.c#n224" />
		<title level="m">KVM. 2021. Page Table Allocation in KVM</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Coordinated and Efficient Huge Page Management with Ingens</title>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;16)</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<ptr target="https://github.com/torvalds/linux/blob/master/include/linux/pgtable.h" />
		<title level="m">Linux Kernel. 2021. Page Table Header File</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The HPC Challenge (HPCC) Benchmark Suite</title>
		<author>
			<persName><forename type="first">Piotr</forename><forename type="middle">R</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">F</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM/IEEE Conference on Supercomputing (SC&apos;06)</title>
				<meeting>the 2006 ACM/IEEE Conference on Supercomputing (SC&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simics: A Full System Simulation Platform</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Christensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Eskilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Hållberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Högberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moestedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengt</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">CSALT: Context Switch Aware Large TLB</title>
		<author>
			<persName><forename type="first">Yashwant</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 50th IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-50</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Prefetched Address Translation</title>
		<author>
			<persName><forename type="first">Artemiy</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CHiRP: Control-Flow History Reuse Prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 53rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">GraphBIG: Understanding Graph Computing in the Context of Industrial Solutions</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilie</forename><forename type="middle">G</forename><surname>Tanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis (SC&apos;15)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flemming</forename><surname>Friche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodler</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cuckoo Hashing. Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="144" />
			<date type="published" when="2004-05">2004. May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">HawkEye: Efficient Finegrained OS Support for Huge Pages</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Making Huge Pages Actually Useful</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perforated Page: Supporting Fragmented Memory Allocation for Large Pages</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyeong</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
				<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hybrid TLB Coalescing: Improving TLB Translation Coverage under Diverse Fragmented Memory Allocations</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taekyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungi</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Increasing TLB Reach by Exploiting Clustering in Page Translations</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA&apos;14)</title>
				<meeting>the 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Aamer Jaleel, and Abhishek Bhattacharjee</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanathan</forename><surname>Vaidyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 45th IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
	<note>CoLT: Coalesced Large-Reach TLBs</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Large Pages and Lightweight Memory Management in Virtualized Environments: Can You Have it Both Ways?</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ján</forename><surname>Veselŷ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scalable High Performance Main Memory System Using Phase-Change Memory Technology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><forename type="middle">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Rivers</surname></persName>
		</author>
		<idno>ISCA &apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture</title>
				<meeting>the 36th Annual International Symposium on Computer Architecture<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The Structural Simulation Toolkit</title>
		<author>
			<persName><forename type="first">F</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanine</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Scott</forename><surname>Cooper-Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Hemmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Kersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlow</forename><surname>Oldfield</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM/IEEE Conference on Supercomputing</title>
				<meeting>the 2006 ACM/IEEE Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DRAMSim2: A Cycle Accurate Memory System Simulator</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Cooper-Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB</title>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Recency-Based TLB Preloading</title>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Saulsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International Symposium on Computer Architecture (ISCA&apos;00)</title>
				<meeting>the 27th Annual International Symposium on Computer Architecture (ISCA&apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">BabelFish: Fusing Address Translations for Containers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Darbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gopireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
				<meeting>the 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The Architecture of Virtual Machines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nair</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2005.173</idno>
		<ptr target="https://doi.org/10.1109/MC.2005.173" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Synergistic TLBs for High Performance Address Translation in Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Shekhar</forename><surname>Srikantaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A modular, cross-platform and multi-threaded benchmark tool</title>
		<author>
			<persName><surname>Sysbench</surname></persName>
		</author>
		<ptr target="http://manpages.ubuntu.com/manpages/trusty/man1/sysbench.1.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Surpassing the TLB Performance of Superpages with Less Operating System Support</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI)</title>
				<meeting>the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The Linux Kernel Archives</title>
		<ptr target="https://www.kernel.org/doc/Documentation/vm/transhuge.txt" />
	</analytic>
	<monogr>
		<title level="m">Transparent Hugepage Support</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Memory Resource Management in VMware ESX Server</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 5th Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Characterizing and Modeling Non-Volatile Memory Systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO50266.2020.00049</idno>
		<ptr target="https://doi.org/10.1109/MICRO50266.2020.00049" />
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="496" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<idno>www.7-cpu.com. 2021</idno>
		<ptr target="https://www.7-cpu.com/cpu/Skylake.html" />
		<title level="m">Intel Skylake Timing</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Translation Ranger: Operating System Support for Contiguity-Aware TLBs</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture (ISCA&apos;19)</title>
				<meeting>the 46th International Symposium on Computer Architecture (ISCA&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Hash, Don&apos;t Cache (the Page Table)</title>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science (SIGMETRICS&apos;16)</title>
				<meeting>the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science (SIGMETRICS&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
