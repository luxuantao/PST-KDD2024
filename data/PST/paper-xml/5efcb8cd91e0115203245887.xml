<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Clustering with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-30">30 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Bryan Perozzi Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Clustering with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-30">30 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.16904v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. In this paper, we study unsupervised training of GNN pooling in terms of their clustering capabilities. We start by drawing a connection between graph clustering and graph pooling: intuitively, a good graph clustering is what one would expect from a GNN pooling layer. Counterintuitively, we show that this is not true for state-of-the-art pooling methods, such as MinCut pooling. To address these deficiencies, we introduce Deep Modularity Networks (DMON), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery of the challenging clustering structure of real-world graphs. In order to clarify the regimes where existing methods fail, we carefully design a set of experiments on synthetic data which show that DMON is able to jointly leverage the signal from the graph structure and node attributes. Similarly, on real-world data, we show that DMON produces high quality clusters which correlate strongly with ground truth labels, achieving state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years there has been a surge of research interest in developing varieties of Graph Neural Networks (GNNs) -specialized deep learning architectures for dealing with graph-structured data, such as social networks <ref type="bibr" target="#b46">[47]</ref>, recommender graphs <ref type="bibr" target="#b67">[68]</ref>, or molecular graphs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69]</ref>. GNNs leverage the structure of the data as computational graph, allowing the information to propagate across the edges of graphs <ref type="bibr" target="#b48">[49]</ref>. When many real-wold systems are represented as graphs, they exhibit locally inhomogeneous distributions of edges, forming clusters (also called communities or modules)groups of nodes with high in-group edge density, and relatively low out-group density. Clusters can correspond to interesting phenomena in the underlying graph, for example to education <ref type="bibr" target="#b56">[57]</ref> or employment <ref type="bibr" target="#b40">[41]</ref> in social graphs. GNNs have been shown to benefit from leveraging higher-order structural information that could arise from clusters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b30">31]</ref>, for example through pooling or trainable attention over edges <ref type="bibr" target="#b59">[60]</ref>.</p><p>Interestingly, most existing work on GNNs to leverage higher-order structure does not directly address node partitioning or the estimation of clusters within the computational graph. Furthermore, most works explore these mechanisms only within a semi-supervised or supervised framework, ignoring the fact that unsupervised graph clustering is often an extremely useful end-goal in itself -whether for data exploration <ref type="bibr" target="#b44">[45]</ref>, visualization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, genomic feature discovery <ref type="bibr" target="#b6">[7]</ref>, anomaly detection <ref type="bibr" target="#b43">[44]</ref>, or for many other use-cases discussed e.g. in <ref type="bibr" target="#b18">[19]</ref>. Additionally, many of the existing structure-aware methods have undesirable properties, such as relying on a multi-step optimization process which does not allow to optimize the objective via gradient descent end-to-end <ref type="bibr" target="#b45">[46]</ref>. Graclus <ref type="bibr" target="#b12">[13]</ref> DiffPool <ref type="bibr" target="#b68">[69]</ref> Top-k <ref type="bibr" target="#b19">[20]</ref> SAG <ref type="bibr" target="#b30">[31]</ref> MinCut <ref type="bibr" target="#b3">[4]</ref> DMON</p><p>In this work, we take an ab initio approach to the clustering problem in the GNN domain, bridging the gap between traditional graph clustering objectives and deep neural networks. We start by drawing a connection between graph pooling, which was typically studied in the literature as a regularizer for supervised GNN architectures, and fully unsupervised clustering. Specifically, we contribute:</p><p>• DMON, a method for unsupervised clustering in GNNs that allows optimization of cluster assignments in an end-to-end differentiable way.</p><p>• An empirical study of performance on synthetic graphs, illustrating the problems with existing work and how DMON allows for improved model performance in those regimes.</p><p>• Thorough experimental evaluation on real-world data, showing that many pooling methods poorly reflect hierarchical structures and are not able to make use of both graph structure and node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds upon a rich line of research on graph neural networks and graph pooling methods.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref> allow end-to-end differentable losses over data with arbitrary structure. They have been applied to an incredible range of applications, from social networks <ref type="bibr" target="#b46">[47]</ref>, to recommender systems <ref type="bibr" target="#b67">[68]</ref>, to computational chemistry <ref type="bibr" target="#b20">[21]</ref>. While GNNs are flexible enough to allow for unsupervised losses, most work follows the semi-supervised setting for node classification from <ref type="bibr" target="#b28">[29]</ref>. For a complete introductions to the vast topic we refer interested readers to detailed surveys <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Unsupervised training of GNNs is commonly done via maximizing mutual information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54]</ref> in a self-supervised fashion. Deep Graph Infomax (DGI) <ref type="bibr" target="#b60">[61]</ref> adapted the mutual informatonbased learning from Deep InfoMax <ref type="bibr" target="#b25">[26]</ref>, learning unsupervised representations for nodes in attributed graphs. InfoGraph <ref type="bibr" target="#b55">[56]</ref> extended the idea to learning representations of whole graphs instead of just nodes. A very similar approach was introduced independently in <ref type="bibr" target="#b26">[27]</ref> in the context of pre-training GNNs for producing representations of graphs, which was first tackled in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Graph pooling aims to tackle the hierarchical nature of graphs via iterative coarsening. Early architectures <ref type="bibr" target="#b12">[13]</ref> resorted to fixed axiomatic pooling, with no optimization of clustering while the network learns. DiffPool <ref type="bibr" target="#b68">[69]</ref> suggests to include a learnable pooling to GNN architecture. To help the convergence, DiffPool includes a link prediction loss to help encapsulate the clustering structure of graphs and an additional entropy loss to penalize soft assignments. Top-k <ref type="bibr" target="#b19">[20]</ref> and SAG pooling <ref type="bibr" target="#b30">[31]</ref> learn to sparsify the graph (select top-k edges for each node) with learned weights. MinCutPool <ref type="bibr" target="#b3">[4]</ref> pooling studies differentiable formulation of spectral clustering as a pooling strategy.</p><p>We summarize mainstream graph pooling methods in Table <ref type="table" target="#tab_0">1</ref> in terms of six desirable properties related to their clustering capabilities:</p><p>• Trainable. End-to-end training allows to capture both graph structure and node features.</p><p>• Unsupervised training is a desirable setting for clustering models. Works on supervised graph clustering <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b61">62]</ref> are outside of the scope of this work.</p><p>• Sparse. As graphs in the real-world vary in size and sparsity, methods can not be limited by a O(n 2 ) link prediction objectives, like DiffPool <ref type="bibr" target="#b68">[69]</ref>, or computing A 2 , like top-k pooling methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> .</p><p>• Node aggregation is crucial for our interpretation of graph pooling in terms of graph clustering. Both Top-k <ref type="bibr" target="#b19">[20]</ref> and SAG pooling <ref type="bibr" target="#b30">[31]</ref> only sparsify the graph and do not reduce the nodeset.</p><p>• Soft assignments allow for more flexibility reasoning about the interactions of clusters.</p><p>• Stable -the method should be stable in terms of the graph structure. DiffPool <ref type="bibr" target="#b68">[69]</ref> is not stable in terms of graph sparsity, while MinCutPool <ref type="bibr" target="#b3">[4]</ref> can not deal with uneven degree distribution.</p><p>Graph embeddings <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b58">59</ref>] can be thought of as (very restricted) unsupervised GNNs with an identity feature matrix, meaning each node learns its own positional representation <ref type="bibr" target="#b69">[70]</ref>. The learning process in graph embeddings is often done in a similar way to DGI though noise contrastive estimation <ref type="bibr" target="#b23">[24]</ref>. As far as we know, all pooling strategies for learning node embeddings without attributes have been axiomatic <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We introduce the necessary background for DMON, starting with the problem formulation, reviewing common graph clustering objectives and how they can be made differentiable efficiently.</p><formula xml:id="formula_0">Graph G = (V, E) is defined via a set of nodes V = (v 1 , . . . , v n ), |V | = n and edges E ⊆ V × V, |E| = m.</formula><p>We are interested in measuring the quality of graph partitioning function F : n → k that splits the set of nodes V into k partitions V i = {v j , F(v j ) = i}. Additionally, in contrast to standard graph clustering, we are also provided with node attributes X ∈ R n×s that provide additional information not reflected in the graph structure, but also correlated with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Clustering Quality Functions</head><p>Design of the objective function is crucial for the algorithm performance. We review two families of clustering quality functions amenable to spectral optimization, and review some of their shortcomings.</p><p>Cut-based metrics. In his seminal work <ref type="bibr" target="#b17">[18]</ref>, Fiedler suggested that the second (Fiedler) eigenvector of a graph Laplacian produces a graph cut minimal in terms of the weight of the edges. This plain notion of cut degenerates on real-world graphs, as it does not require partitions to be balanced in terms of size. It is possible to get normalized partitions with the use of ratio cut <ref type="bibr" target="#b62">[63]</ref>, which normalizes the cut by the product of the number of nodes in two partitions, or normalized cut <ref type="bibr" target="#b51">[52]</ref>, which uses total edge volume of the partition as normalization.</p><p>In real networks, however, there is evidence against existence of good cuts <ref type="bibr" target="#b31">[32]</ref> in ground-truth communities. This can be explained by the fact than a single node implicitly participates in many different clusters <ref type="bibr" target="#b16">[17]</ref>, e.g. a person in a social network is simultaneously connected with family and work friends, forcing the algorithm to merge these communities together.</p><p>Recently, MinCutPool <ref type="bibr" target="#b3">[4]</ref> adapted the notion of the normalized cut to use as a regularizer for pooling. While, theoretically, MinCutPool objective should be suitable for clustering nodes in graphs, we (i) show that it does not optimize its own objective function and (ii) provide evidence against this in the synthetic and real-world experiments.</p><p>Modularity <ref type="bibr" target="#b37">[38]</ref> approaches the same problem from a statistical perspective, incorporating a null model to quantify the significance of the clustering with respect to the random graph. In a fully random graph with given degrees, nodes u and v with degrees d u and d v are connected with probability dudv /2m. Modularity measures the divergence between the intra-cluster edges from the expected one:</p><formula xml:id="formula_1">Q = 1 2m ij A ij − d i d j 2m δ(c i , c j ),<label>(1)</label></formula><p>where δ(c i , c j ) = 1 if i and j are in the same cluster and 0 otherwise. Note Q ∈ ( −1 /2; 1] (it is 0 when there is no correlation of clusters with edge density), but it is not necessarily maximized at 1, and is only comparable across graphs with the same degree distribution. While problems with the modularity metric have been identified <ref type="bibr" target="#b21">[22]</ref>, it remains one of the most commonly-used and eminently useful graph clustering metrics in scientific literature <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spectral Modularity Maximization</head><p>Maximizing the modularity is proven to be NP-hard <ref type="bibr" target="#b4">[5]</ref>, however, a spectral relaxation of the problem can be solved efficiently <ref type="bibr" target="#b36">[37]</ref>. Let C ∈ 0, 1 n×k be the cluster assignment matrix and d be the degree vector. Then, with modularity matrix B defined as B = A − dd 2m , the modularity Q can be reformulated as:</p><formula xml:id="formula_2">Q = 1 2m Tr(C BC)<label>(2)</label></formula><p>Relaxing C ∈ R n×k , the optimal C maximizing Q is the top-k eigenvectors of the modularity matrix B. While B is dense, iterative eigenvalue solvers can take advantage of the fact that B is a sum of a sparse A and rank-one matrix − dd 2m , meaning that the matrix-vector product Bx can be computed efficiently as</p><formula xml:id="formula_3">Bx = Ax − d xd 2m</formula><p>and optimized efficiently with iterative methods such as power iteration or Lanczos algorithm.</p><p>One can then obtain clusters by means of spectral bisection <ref type="bibr" target="#b36">[37]</ref> with iterative refinement akin to Kernighan-Lin algorithm <ref type="bibr" target="#b27">[28]</ref>. However, these formulations operate entirely on the graph structure, and it is non-trivial to adapt them to work with attributed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Neural Networks</head><p>Graph Neural Networks are a flexible class of models that perform nonlinear feature aggregation with respect to graph structure. For the purposes of this work, we consider transductive GNNs that output a single embedding per node. Graph convolutional networks (GCNs) <ref type="bibr" target="#b28">[29]</ref> are simple yet effective <ref type="bibr" target="#b50">[51]</ref> message-passing networks that fit our criteria. Let X 0 ∈ R n×s be the initial node features and Ã = D − 1 2 AD − 1 2 be the normalized adjacency matrix, the output of t-th layer X t+1 is</p><formula xml:id="formula_4">X t+1 = SeLU( ÃX t W + XW skip )<label>(3)</label></formula><p>We make two changes to the classic GCN architecture: first, we remove the self-loop creation and instead use an W skip ∈ R s×s trainable skip connection, and, second, we replace ReLU nonlinearity with SeLU <ref type="bibr" target="#b29">[30]</ref> for better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we present DMON, our method for attributed graph clustering with graph neural networks. Inspired by the modularity function and its spectral optimization, we propose a fully differentiable unsupervised clustering objective which optimizes soft cluster assignments using a null model to control for inhomogeneities in the graph. We then discuss the challenge of regularizing cluster assignments, and present collapse regularization, a softer version of orthogonality loss, that against trivial solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DMON: Deep Modularity Networks</head><p>The challenge of clustering boils down to defining an optimization procedure over the cluster assignment matrix C. In DMON, we propose to obtain C via the output of a softmax function, which allows the (soft) cluster assignment to be differentiable. The input to the cluster assignment can be any differentiable message passing function, but here we specifically consider the case where a graph convolutional network is used to obtain soft clusters for each node as follows:</p><formula xml:id="formula_5">C = softmax(GCN( Ã, X)),<label>(4)</label></formula><p>where GCN is a (possibly) multi-layer convolutional network operating on an normalized adjacency matrix</p><formula xml:id="formula_6">Ã = D − 1 2 (A)D − 1 2 .</formula><p>We then propose to optimize this assignment with the following objective, which combines insights from spectral modularity maximization (2) with a regularization to ensure informative clusters:</p><formula xml:id="formula_7">L DMON = − 1 2m Tr(C BC) modularity + √ k n i C i F − 1 collapse regularization ,<label>(5)</label></formula><p>where • F is the Frobenius norm. We decompose the computation of Tr(C BC) as a sum of sparse matrix-matrix multiplication and rank-one degree normalization Tr(C AC − C d dC). This allows us to efficiently optimize DMON parameters in sparse regime. Without additional constraints on the assignment matrix C, spectral clustering for both min-cut and modularity objectives has spurious local minima: assigning all nodes to the same cluster produces a trivial locally optimal solution that traps gradient-based optimization methods. MinCut pooling <ref type="bibr" target="#b3">[4]</ref> addresses this problem by adapting spectral orthogonality constraint in the form of soft-orthogonality <ref type="bibr" target="#b1">[2]</ref> regularization C C − I F . We notice that this term is overly restrictive when combined with softmax class assignment -intuitively, when the value range of C is restricted to R ∩ [0, 1] the optimization of the soft-orthogonality regularizer dominates the loss.</p><p>We illustrate this problem in Figure <ref type="figure" target="#fig_0">1</ref>, which depicts the progress of optimization for both methods in terms of their main objective and regularizer term over the course of 200 epochs on Cora dataset. The soft orthogonality regularization term dominates the optimization for MinCutPool, such that the cut objective becomes worse than random over the course of training.</p><p>We fix this problem by proposing a relaxed notion of collapse regularization that prevents the trivial partition while not restricting the optimization of the main objective. The regularizer is a Frobenius norm of the (soft) cluster membership counts, normalized to range [0, 1]. It gets value of 0 when cluster sizes are perfectly balanced, and 1 in the case all clusters collapse to one. We also propose to stabilize the training by applying dropout <ref type="bibr" target="#b54">[55]</ref> to GNN representations before the softmax, preventing the gradient descent from getting stuck in the local optima of the highly non-convex objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We assess the clustering performance of DMON in terms of both graph clustering and label alignment. We release the implementation of DMON at this URL Baselines. As we study how to leverage the information from both the graph and attributes, we employ two baselines that employ either strictly graph or attribute information. We give a brief description of the baselines used below:</p><p>• k-means(features) is our baseline that only considers the feature data. We use the local Lloyd algorithm <ref type="bibr" target="#b33">[34]</ref> with the k-means++ seeding <ref type="bibr" target="#b0">[1]</ref> strategy.</p><p>• SBM <ref type="bibr" target="#b41">[42]</ref> is a baseline that only relies on the graph structure. We estimate a constrained stochastic block model with given number of k clusters, maximizing the modularity <ref type="bibr" target="#b37">[38]</ref> of the network.</p><p>• k-means(DGI) <ref type="bibr" target="#b60">[61]</ref> demonstrates the need of joint learning of clusters and representations. We learn unsupervised node representation with DGI and run k-means on the resulting representations.</p><p>• MinCutPool(graph, features) <ref type="bibr" target="#b3">[4]</ref> is a deep pooling method that we re-interpret as clustering.   Metrics. We measure both the graph-based metrics of clustering and label correlation to study clustering performance of attributed graphs both in terms of graph and attribute structure. For graph-level metrics, we report average cluster conductance (as per definition from <ref type="bibr" target="#b63">[64]</ref>) and graph modularity <ref type="bibr" target="#b37">[38]</ref>. For ground-truth label correlation analysis, we report normalized mutual information (NMI) between the cluster assignments and labels and pairwise F-1 score between all node pairs and their associated cluster pairs. Where possible, we normalize all metrics by multiplying them by 100 for ease of comparison.</p><formula xml:id="formula_8">C ↓ Q ↑ NMI↑ F1↑ C ↓ Q ↑ NMI↑ F1↑ C ↓ Q ↑ NMI↑ F1↑ C ↓ Q ↑ NMI↑ F1↑ k-m(</formula><p>Parameter settings. We run all experiments for 10 times and average results across runs. All models were implemented in Tensorflow 2 and trained on CPUs. We fix the architecture for all clustering networks to have one hidden layer with 512 neurons in it for real-world datasets and 64 for small synthetic graphs. We set the number of clusters to 16 for all datasets and methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simulation Experiments on Stochastic Block Model</head><p>To better explore the robustness of our approach to variance in the graph and node features, we propose a study on synthetic graphs using an attributed, degree-corrected stochastic block model (ADC-SBM). The SBM <ref type="bibr" target="#b52">[53]</ref> plants a partition of clusters ("blocks") in a graph, and generates edges via a distribution conditional on that partition. This model has been used extensively to benchmark graph clustering methods <ref type="bibr" target="#b18">[19]</ref>, and has recently been used for experiments on state-of-the-art GNNs <ref type="bibr" target="#b15">[16]</ref>.</p><p>In our version of the model, node features are also generated, using a multivariate mixture model, with the mixture memberships having some correspondence to the cluster memberships.  To generate an instance of the ADC-SBM, we fix a number of nodes n and a number of clusters k, and choose node cluster memberships uniformly-at-random. Define the matrix D k×k where D ij is the expected number of edges shared between nodes in clusters i and j. We determine D by fixing (1) the expected average degree of the nodes d ∈ {1, n}, and (2) the expected average sub-degree d out ≤ d of a node to any cluster other than its own. Note that the difference d in − d out , where d in := d − d out , controls the spectral detectability of the clusters <ref type="bibr" target="#b34">[35]</ref>. Finally, we generate a power-law n-vector θ, where θ i is proportional to i's expected degree. With the memberships and the parameters D and θ.</p><p>We use graph-tool <ref type="bibr" target="#b42">[43]</ref> generate the graphs.</p><p>To generate s-dimensional features, we first generate feature memberships from k f cluster labels. For graph clustering GNNs that operate both on edges and node features, it is important to examine performance on data where feature clusters diverge from or segment the graph clusters: thus potentially k f = k. We examine cases where feature memberships match, group, or nest the graph memberships, as illustrated in Figure <ref type="figure" target="#fig_1">3</ref>. With feature memberships in-hand, we generate k zero-mean feature cluster centers from a s-multivariate normal with covariance matrix σ 2 c • I s×s . Then, for feature cluster i ≤ k f , we generate its features from a s-multivariate normal with covariance matrix σ 2 • I s×s . Note that the ratio σ 2 c /σ 2 controls the expected value of the classical between/within-sum-of-squares of the clusters.</p><p>The above paragraphs describe a single generation of our synthetic benchmark model, the ADC-SBM. To explore robustness, we define a "default" ADC-SBM, and explore model parameters in a range around the defaults. We configure our default model as follows: we generate graphs with n = 1, 000 nodes grouped in k = 4 clusters, and s = 32-dimensional features grouped in k f = 4 matching feature clusters with σ = 1 intra-cluster center variance and σ c = 3 cluster center variance. We try to model real-world graphs' degree distribution with d = 20 average degree and d out = 2 average inter-cluster degree with power law parameters d min = 2, d max = 4, α = 2. In total, we consider 6 different scenarios, as described in Table <ref type="table" target="#tab_5">4</ref>. Figure <ref type="figure">5</ref> demonstrates overwhelming superiority of DMON in all considered scenarios. In scenario 1 we see that DMON can effectively leverage the feature signal to obtain outstanding clustering performance even when the graph structure is close to random, far beyond the spectral detectability threshold (pictured in gray). Scenario 2 demonstrates that even in the presence of a weak feature signal DMON outperforms stochastic SBM minimization, while MinCutPool needs two orders of magnitude stronger signal to attain the same performance. Scenario 3 demonstrates that DMON is not susceptible to nested feature clusters and can correctly recover underlying graph structure. Scenario 4 shows that DMON is more susceptible to grouped features, as it becomes hard to differentiate graph clusters with similar features, however, MinCutPool is struggling to pick up any signal in the data. Scenarios 5&amp;6 demonstrate that DMON is stable in terms of the degree distribution of the graph, while MinCutPool is not. We also notice that while the k-means(DGI) baseline offers some improvements over using features or the graph structure alone, it never surpasses the strongest signal provider in the graph, never being better than the best one between k-means(features) and SBM. Finally, we provide an illustration of the how MinCutPool collapsing into a single cluster while DMON succeeds in Figure <ref type="figure" target="#fig_2">4</ref>.  <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-world Data</head><p>We now move on to studies on real-world networks, featuring DMON and 4 baselines on 7 different datasets. DMON achieves better clustering performance than its neural counterparts on every single dataset and metric besides losing twice to DGI+k-means on Cora and Citeseer in terms of NMI.</p><p>Compared to SBM, a method that exclusively optimizes modularity, we are able to stay within 2 NMI percentage points, while simultaneously clustering the features. Surprisingly, on Citeseer and Pubmed we achieve better modularity than the method designed to optimize it -we attribute that to very high correlation between graph structure and the features. We also highlight that we beat MinCutPool in terms of conductance (average graph cut) on all datasets, even though it attempts to optimize for this metric.</p><p>On Amazon PC and Amazon Photo MinCutPool failed to converge, even with tuning the orthogonality regularization parameter. We attribute that to extremely uneven structure of these graphs, as popular products are co-purchased with a lot of other items, so the effects discussed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref> are prohibiting good cuts. This corresponds to high values of d ad d max in our synthetic scenarios 5 and 6.</p><p>Overall, DMON demonstrates excellent performance on both graph clustering and label correlation, successfully leveraging both graph and attribute information. Both synthetic and real-world experiments prove that DMON is vastly superior to its counterparts in attributed graph clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we study GNN pooling through the lens of attributed graph clustering. We introduce Deep Modularity Networks (DMON), an unsupervised objective and realize it with a GNN which can recover high quality clusters. We compare against challenging baselines that baselines that optimize structure (SBM), features (kmeans), or both (DGI+k-means), in addition to a recently proposed state-of-the-art pooling method (MinCutPool).</p><p>We explore the limits of GNN clustering methods in terms of both graph and feature signals using synthetic data, where we see that DMON better leverages structure and attributes than all existing methods. In extensive experiments on real datasets we show that the clusters found by DMON are more likely to correspond to ground truth labels, and have better properties as illustrated by clustering metrics (e.g. conductance or modularity). We hope that this work will further advancements in unsupervised learning for GNNs as well as attributed graph clustering, allowing further advances in graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We believe that better understanding of graph pooling through the lens of attributed graph clustering will allow for the design of more effective graph neural network architectures. Our reformulation encourages explicit grouping of nodes in graphs, which we envision will be useful in domains with naturally occurring community structure, e.g. social networks or information retrieval. Additionally, our M-GCN could serve as the "graph-context" head in semi-supervised architectures <ref type="bibr" target="#b66">[67]</ref>, allowing for stronger community-based constraints in those systems.</p><p>Our work also allows translation of open questions in network science about attributed graph clustering to the GNN domain, potentially enabling powerful new modelling solutions. Specifically, existing combinatorial <ref type="bibr" target="#b64">[65]</ref> and bayesian <ref type="bibr" target="#b38">[39]</ref> methods for attributed graph clustering use graph features to improve clustering accuracy. <ref type="bibr" target="#b38">[39]</ref> shows that using features allows their estimation-based approach to overcome the spectral "detectability" limit for graphs <ref type="bibr" target="#b34">[35]</ref>, a result that we replicate in this study. Using our model, more ideas from the network science community may be ported to GNN architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Optimization progress of Min-Cut and DMON on Cora dataset. MinCut optimizes the regularizer, while DMON minimizes its main objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of synthetic data. a): 4-cluster graph adjacency matrix. b): Covariance matrix of "matched" features: features that are clustered according to the graph clusters. c): Covariance matrix of "nested" features: features that are clustered by nesting of the graph clusters. d): Covariance matrix of "grouped" features: features that are clustered by grouping of the graph clusters.</figDesc><graphic url="image-1.png" coords="7,108.00,78.18,95.04,73.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) MinCutPool fails to recover ground-truth clusters while (b) DMON perfectly recovers ground-truth partition.</figDesc><graphic url="image-5.png" coords="7,385.20,523.22,56.43,56.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 2 2 6 Figure 5 :</head><label>5265</label><figDesc>Figure 5: Synthetic results on the ADC-SBM model with 6 different scenarios described in Table4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Related work in terms of six desirable clustering properties outlined in Section 2.</figDesc><table><row><cell>method</cell><cell>Trainable</cell><cell>Unsupervised</cell><cell>Sparse</cell><cell>Node pooling</cell><cell>Soft assignments</cell><cell>Stable</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on four datasets from<ref type="bibr" target="#b49">[50]</ref> in terms of graph conductance C, modularity Q, NMI with ground-truth labels, and pairwise F1 measure. We highlight best neural method performance.</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell>Pubmed</cell></row><row><cell></cell><cell cols="2">graph</cell><cell>labels</cell><cell cols="2">graph</cell><cell>labels</cell><cell>graph</cell><cell>labels</cell></row><row><cell>method</cell><cell>C ↓</cell><cell cols="2">Q ↑ NMI↑ F1↑</cell><cell>C ↓</cell><cell cols="2">Q ↑ NMI↑ F1↑</cell><cell>C ↓</cell><cell>Q ↑ NMI↑ F1↑</cell></row><row><cell cols="8">k-means(features) 61.7 19.8 18.5 27.0 60.5 30.3 24.5 29.2 55.8 33.4 19.4 24.4</cell></row><row><cell>SBM</cell><cell cols="7">15.4 77.3 36.2 30.2 14.2 78.1 15.3 19.1 39.0 53.5 16.4 16.7</cell></row><row><cell>k-means(DGI)</cell><cell cols="7">28.0 64.0 52.7 40.1 17.5 73.7 40.4 39.4 82.9</cell><cell>9.6</cell><cell>22.0 26.4</cell></row><row><cell>MinCut</cell><cell cols="7">23.3 70.3 35.8 25.0 14.1 78.9 25.9 20.1 29.6 63.1 25.4 15.8</cell></row><row><cell>DMON</cell><cell cols="3">12.2 76.5 48.8 48.8</cell><cell>5.1</cell><cell cols="3">79.3 33.7 43.2 17.7 65.4 29.8 33.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on four datasets from<ref type="bibr" target="#b50">[51]</ref> in terms of graph conductance C, modularity Q, NMI with ground-truth labels, and pairwise F1 measure. We highlight best neural method performance.</figDesc><table><row><cell cols="2">Amazon PC</cell><cell cols="2">Amazon Photo</cell><cell cols="2">Coauthor CS</cell><cell cols="2">Coauthor PHY</cell></row><row><cell>graph</cell><cell>labels</cell><cell>graph</cell><cell>labels</cell><cell>graph</cell><cell>labels</cell><cell>graph</cell><cell>labels</cell></row><row><cell>method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Synthetic ADC-SBM benchmark scenarios.</figDesc><table><row><cell cols="2">Scenario Parameter</cell><cell>Description</cell></row><row><cell>1</cell><cell>dout ∈ [2.0, 5.0]</cell><cell>Increase graph cluster mixing signal. Higher = weaker clusters.</cell></row></table><note>2 σc ∈ [10 −2 , 10 1 ] Increase feature cluster center variance. Higher = stronger clusters. 3 σc ∈ [10 −2 , 10 1 ] Increase feature cluster center variance, with nested feature clusters. 4 σc ∈ [10 −2 , 10 1 ] Increase feature cluster center variance, with grouped feature clusters. 5 d ∈ [2 2 , 2 7 ] Increase average degree. Higher = clearer graph signal. 6 dmax ∈ [2 2 , 2 10 ] Increase power law upper-bound. Higher = more extreme power law.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">github.com/google-research/google-research/tree/master/graph_embedding/dmon</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can we gain more from orthogonality regularizations in training deep cnns?</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4266" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mine: mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ulrik</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Delling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaertler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Görke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hoefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Nikoloski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Wagner</surname></persName>
		</author>
		<idno>arXiv preprint physics/0608255</idno>
		<title level="m">Maximizing modularity is hard</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>IEEE Signal Processing Magazine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting community structures in hi-c genomic data</title>
		<author>
			<persName><forename type="first">Irineo</forename><surname>Cabreros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristotelis</forename><surname>Tsirigos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Annual Conference on Information Science and Systems (CISS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="584" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Harp: Hierarchical representation learning for networks</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HARP: Hierarchical representation learning for networks</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Stéphan</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">De</forename><surname>Arazoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5693</idno>
		<title level="m">Hierarchical clustering for graph visualization</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometry-based edge clustering for graph visualization</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pak</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1277" to="1284" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Feng</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ego-splitting framework: From nonoverlapping to overlapping clusters</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Lattanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Paes Leme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algebraic connectivity of graphs</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czechoslovak mathematical journal</title>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Community in networks: A user guide</title>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Hric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph U-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance of modularity maximization in practical contexts</title>
		<author>
			<persName><forename type="first">Yves-Alexandre</forename><surname>Benjamin H Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>De Montjoye</surname></persName>
		</author>
		<author>
			<persName><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An efficient heuristic procedure for partitioning graphs. The Bell system technical journal</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical properties of community structure in large social and information networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
				<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">MILE: A multi-level framework for scalable graph embedding</title>
		<author>
			<persName><forename type="first">Jiongqian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Gurukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09612</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
				<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph spectra and the detectability of community structure in networks</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadakuditi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pre-training graph neural networks with kernels</title>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Dinh V Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Sperduti</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Modularity and community structure in networks</title>
		<author>
			<persName><surname>Mark Ej Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structure and inference in annotated networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Clauset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The virtual geographies of social networks: a comparative analysis of facebook, linkedin and asmallworld</title>
		<author>
			<persName><forename type="first">Zizi</forename><surname>Papacharissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New media &amp; society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="199" to="220" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient monte carlo and greedy heuristic for the inference of stochastic block models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName><surname>Peixoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The graph-tool python library</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName><surname>Peixoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06-01">2014. June 1, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable anomaly ranking of attributed neighborhoods</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
				<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discovering communities and anomalies in attributed graphs: Interactive visual exploration and summarization</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Focused clustering and outlier detection in large attributed graphs</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">Iglesias</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1346" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<title level="m">Collective classification in network data. AI magazine</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Estimation and prediction for stochastic blockmodels for graphs with latent block structure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Snijders</surname></persName>
		</author>
		<author>
			<persName><surname>Nowicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding the limitations of variational mutual information estimators</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Comparing community structure to characteristics in online collegiate social networks</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">D</forename><surname>Amanda L Traud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Kelsic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="526" to="543" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Verse: Versatile graph embeddings from similarity measures</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards efficient hierarchical designs by ratio cut partitioning</title>
		<author>
			<persName><forename type="first">Yen-Chuen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Kuan</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer-Aided Design</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Community detection in networks with node attributes</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 13th International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1151" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to cluster faces on an affinity graph</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
