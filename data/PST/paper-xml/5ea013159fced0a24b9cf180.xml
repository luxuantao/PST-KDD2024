<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AccelTCP: Accelerating Network Applications with Stateful TCP Offloading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Younggyoun</forename><surname>Moon</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">https</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seungeon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaist</forename><forename type="middle">;</forename><surname>Muhammad</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Asim</forename><surname>Jamshed</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Intel</forename><surname>Labs</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>KAIST</roleName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>February 25-27</addrLine>
									<postCode>2020 โข</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">SeungEon Lee KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muhammad</forename><surname>Asim Jamshed</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">https</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AccelTCP: Accelerating Network Applications with Stateful TCP Offloading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open access to the Proceedings of the 17th USENIX Symposium on Networked</head><p>Systems Design and Implementation (NSDI '20</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transmission Control Protocol (TCP) <ref type="bibr" target="#b22">[24]</ref> is undeniably the most popular protocol in modern data networking. It guarantees reliable data transfer between two endpoints without overwhelming either end-point nor the network itself. It has become ubiquitous as it simply requires running on the Internet Protocol (IP) <ref type="bibr" target="#b20">[23]</ref> that operates on almost every physical network.</p><p>Ensuring the desirable properties of TCP, however, often entails a severe performance penalty. This is especially pronounced with the recent trend that the gap between CPU capacity and network bandwidth widens. Two notable scenarios where modern TCP servers suffer from poor performance are handling short-lived connections and layer-7 (L7) proxying. Short-lived connections incur a serious overhead in processing small control packets while an L7 proxy requires large compute cycles and memory bandwidth for relaying packets between two connections. While recent kernel-bypass TCP stacks <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b59">61]</ref> have substantially improved the performance of short RPC transactions, they still need to track flow states whose computation cost is as large as 60% of the entire CPU cycles (Section ยง2). An alternative might be to adopt RDMA <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b41">43]</ref> or a custom RPC protocol <ref type="bibr" target="#b42">[44]</ref>, but the former requires an extra in-network support <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b68">70]</ref> while the latter is limited to closed environments. On the other hand, an application-level proxy like L7 load balancer (LB) may benefit from zero copying (e.g., via the splice() system call), but it must perform expensive DMA operations that would waste memory bandwidth.</p><p>The root cause of the problem is actually clear -the TCP stack must maintain mechanical protocol conformance regardless of what the application does. For instance, a key-value server has to synchronize the state at connection setup and closure even when it handles only two data packets for a query. An L7 LB must relay the content between two separate connections even if its core functionality is determining the back-end server.</p><p>AccelTCP addresses this problem by exploiting modern network interface cards (NICs) as a TCP protocol accelerator. It presents a dual-stack TCP design that splits the functionality between a host and a NIC stack. The host stack holds the main control of all TCP operations; it sends and receives data reliably from/to applications and performs control-plane operations such as congestion and flow control. In contrast to existing TCP stacks, however, it accelerates TCP processing by selectively offloading stateful operations to the NIC stack. Once offloaded, the NIC stack processes connection setup and teardown as well as connection splicing that relays packets of two connections entirely on NIC. The goal of AccelTCP is to extend the performance benefit of traditional NIC offload to short-lived connections and application-level proxying while being complementary to existing offloading schemes.</p><p>Our design brings two practical benefits. First, it significantly saves the compute cycles and memory bandwidth of the host stack as it simplifies the code path. Connection management on NIC simplifies the host stack as the host needs to keep only the established connections as well as it avoids frequent DMA operations for small control packets. Also, forwarding packets of spliced connections directly on NIC eliminates DMA operations and application-level processing. This allows the application to spend precious CPU cycles on its main functionality. Second, the host stack makes an offloading decision flexibly on a per-flow basis. When an L7 LB needs to check the content of a response of select flows, it opts them out of offloading while other flows still benefit from connection splicing on NIC. When the host stack detects overload of the NIC, it can opportunistically reduce the offloading rate and use the CPU instead.</p><p>However, performing stateful TCP operations on NIC is non-trivial due to following challenges. First, maintaining consistency of transmission control blocks (TCBs) across host and NIC stacks is challenging as any operation on one stack inherently deviates from the state of the other. To address the problem, AccelTCP always transfers the ownership of a TCB along with an offloaded task. This ensures that a single entity solely holds the ownership and updates its state at any given time. Second, stateful TCP operations increase the implementation complexity on NIC. AccelTCP manages the complexity in two respects. First, it exploits modern smart NICs equipped with tens of processing cores and a large memory, which allows flexible packet processing with C and/or P4 <ref type="bibr" target="#b31">[33]</ref>. Second, it limits the complexity by resorting to a stateless protocol or by cooperating with the host stack. As a result, the entire code for the NIC stack is only 1,501 lines of C code and 195 lines of P4 code, which is small enough to manage on NIC.</p><p>Our evaluation shows that AccelTCP brings an enormous performance gain. It outperforms mTCP <ref type="bibr" target="#b39">[41]</ref> by 2.2x to 3.8x while it enables non-persistent connections to perform comparably to persistent connections on IX <ref type="bibr" target="#b28">[30]</ref> or mTCP. AccelTCP's connection splicing offload achieves a full line rate of 80 Gbps for L7 proxying of 512-byte messages with only a single CPU core. In terms of real-world applications, AccelTCP improves the performance of Redis <ref type="bibr">[17]</ref> and HAProxy <ref type="bibr" target="#b4">[6]</ref> by a factor of 2.3x and 11.9x, respectively.</p><p>The contribution of our work is summarized as follows. <ref type="bibr" target="#b0">(1)</ref> We quantify and present the overhead of TCP protocol conformance in short-lived connections and L7 proxying. <ref type="bibr" target="#b1">(2)</ref> We present the design of AccelTCP, a dual-stack TCP processing system that offloads select features of stateful TCP operations to NIC. We explain the rationale for our target tasks of NIC offload, and present a number of techniques that reduce the implementation complexity on smart NIC. (3) We demonstrate a significant performance benefit of AccelTCP over existing kernel-bypass TCP stacks like mTCP and IX as well as the benefit to real-world applications like a key-value server and an L7 LB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>In this section, we briefly explain the need for an NICaccelerated TCP stack, and discuss our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TCP Overhead in Short Connections &amp; L7 Proxying</head><p>Short-lived TCP connections are prevalent in data centers <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b63">65]</ref> as well as in wide-area networks <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b64">66]</ref>. L7 proxying is also widely used in middlebox applications such as L7 LBs <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b34">36]</ref> and application-level gateways <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">19]</ref>. Unfortunately, application-level performance of these workloads is often suboptimal as the majority of CPU cycles are spent on TCP stack operations. To better understand the cost, we analyze the overhead of the TCP stack operations in these workloads.</p><p>To avoid the inefficiency of the kernel stack <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b58">60]</ref>, we use mTCP <ref type="bibr" target="#b39">[41]</ref>, a scalable user-level TCP stack on DPDK <ref type="bibr" target="#b8">[10]</ref>, as our baseline stack for evaluation. We use one machine for a server (or proxy) and four clients and four back-end servers, all equipped with a 40GbE NIC. The detailed experimental setup is in Section ยง6.</p><p>Small message transactions: To measure the overhead of a short-lived TCP connection, we compare the performance of non-persistent vs. persistent connections with a large number of concurrent RPC transactions. We spawn 16k connections where each transaction exchanges one small request and one small response (64B) between a client and a server. A non-persistent connection performs only a single transaction while a persistent connection repeats the transactions without a closure. To minimize the number of small packets, we patch mTCP to piggyback every ACK on the data packet.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows that persistent connections outperform non-persistent connections by 2.6x to 3.2x. The connection management overhead is roughly proportional to the number of extra packets that it handles; two packets per transaction with a persistent connection vs. six 1 packets for the same task with a non-persistent connection. Table <ref type="table" target="#tab_1">1</ref> shows the breakdown of the CPU cycles where almost 60% of them are attributed to connection setup and teardown. The overhead mainly comes from TCP protocol handling with connection table management, TCB construction and destruction, packet I/O, and L2/L3-level processing of control packets.</p><p>Our experiments may explain the strong preference to persistent connections in data centers. However, not all applications benefit from the persistency. When application data is inherently small or transferred sporadically <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b67">69]</ref>, it would result in a period of inactivity that taxes on server resources. Similarly, persistent connections are often deprecated in PHP applications to avoid the risk of resource misuse <ref type="bibr">[28]</ref>. In general, supporting persistent connections is cumbersome and error-prone because the application not only needs to keep track of connection states, but it also has to periodically check connection timeout and terminate idle connections. By eliminating the connection management cost with NIC offload, our work intends to free the developers from this burden to choose the best approach without performance concern.</p><p>Application-level proxying: An L7 proxy typically operates by (1) terminating a client connection (2) accepting a request from the client and determining the back-end server with it, and creating a server-side connection, and (3) relaying the content between the client and the back-end server. While the key functionality of an L7 proxy is to map a client-side connection to a back-end server, it consumes most of CPU cycles on relaying the packets between the two connections. Packet  relaying incurs a severe memory copying overhead as well as frequent context switchings between the TCP stack and the application. While zero-copying APIs like splice() can mitigate the overhead, DMA operations between the host memory and the NIC are unavoidable even with a kernel-bypass TCP stack.</p><p>Table <ref type="table">2</ref> shows the 1-core performance of a simple L7 proxy on mTCP with 16k persistent connections (8k connections for clients-to-proxy and proxy-to-backend servers, respectively). The proxy exchanges n-byte (n=64 or 1500) packets between two connections, and we measure the wire-level throughput at clients including control packets. We observe that TCP operations in the proxy significantly degrade the performance by 3.2x to 6.3x compared to simple packet forwarding with DPDK <ref type="bibr" target="#b8">[10]</ref>, despite using zero-copy splice(). Moreover, DMA operations further degrade the performance by 3.8x for small packets.</p><p>Summary: We confirm that connection management and packet relaying consume a large amount of CPU cycles, severely limiting the application-level performance. Offloading these operations to NIC promises a large potential for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NIC Offload of TCP Features</head><p>There have been a large number of works and debates on NIC offloading of TCP features <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b55">57]</ref>. While AccelTCP pursues the same benefit of saving CPU cycles 2 All 120 flow-processing cores in Agilio LX are enabled. and memory bandwidth, it targets a different class of applications neglected by existing schemes.</p><p>Partial TCP offload: Modern NICs typically support partial, fixed TCP function offloads such as TCP/IP checksum calculation, TCP segmentation offload (TSO), and large receive offload (LRO). These significantly save CPU cycles for processing large messages as they avoid scanning packet payload and reduce the number of interrupts to handle. TSO and LRO also improve the DMA throughput as they cut down the DMA setup cost required to deliver many small packets. However, their performance benefit is mostly limited to large data transfer as short-lived transactions deal with only a few of small packets.</p><p>Full Stack offload: TCP Offload Engine (TOE) takes a more ambitious approach that offloads entire TCP processing to NIC <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b65">67]</ref>. Similar to our work, TOE eliminates the CPU cycles and DMA overhead of connection management. It also avoids the DMA transfer of small ACK packets as it manages socket buffers on NIC. Unfortunately, full stack TOE is unpopular in practice as it requires invasive modification of the kernel stack and the compute resource on NIC is limited <ref type="bibr" target="#b10">[12]</ref>. Also, operational flexibility is constrained as it requires firmware update to fix bugs or to replace algorithms like congestion control or to add new TCP options. Microsoft's TCP Chimney <ref type="bibr" target="#b13">[15]</ref> deviates from the full stack TOE as the kernel stack controls all connections while it offloads only data transfer to the NIC. However, it suffers from similar limitations that arise as the NIC implements TCP data transfer (e.g., flow reassembly, congestion and flow control, buffer management). As a result, it is rarely enabled these days <ref type="bibr" target="#b26">[27]</ref>.</p><p>In comparison, existing schemes mainly focus on efficient large data transfer, but AccelTCP targets performance improvement with short-lived connections and L7 proxying. AccelTCP is complementary to existing partial TCP offloads as it still exploits them for large data transfer. Similar to TCP Chimney, AccelTCP's host stack assumes full control of the connections. However, the main offloading task is completely the opposite: Ac-celTCP offloads connection management while the host stack implements entire TCP data transfer. This design substantially reduces the complexity on NIC while it extends the benefit to an important class of modern applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Smart NIC for Stateful Offload</head><p>Smart NICs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b24">25]</ref> are gaining popularity as they support flexible packet processing at high speed with programming languages like C or P4 <ref type="bibr" target="#b31">[33]</ref>. Re- cent smart NICs are flexible enough to run Open vSwitch <ref type="bibr" target="#b60">[62]</ref>, Berkeley packet filter <ref type="bibr" target="#b47">[49]</ref>, or even keyvalue lookup <ref type="bibr" target="#b51">[53]</ref>, often achieving 2x to 3x performance improvement over CPU-based solutions <ref type="bibr" target="#b14">[16]</ref>. In this work, we use Netronome Agilio LX as a smart NIC platform to offload stateful TCP operations. As shown in Figure <ref type="figure">2</ref>, Agilio LX employs 120 flow processing cores (FPCs) running at 1.2GHz. 36 FPCs are dedicated to special operations (e.g., PCI or Interlaken) while remaining 84 FPCs can be used for arbitrary packet processing programmed in C and P4. One can implement the basic forwarding path with a match-action table in P4 and add custom actions that require a finegrained logic written in C. The platform also provides fast hashing, checksum calculation, and cryptographic operations implemented in hardware.</p><p>One drastic difference from general-purpose CPU is that FPCs have multiple layers of non-uniform memory access subsystem -registers and memory local to each FPC, shared memory for a cluster of FPCs called "island", or globally-accessible memory by all FPCs. Memory access latency ranges from 1 to 500 cycles depending on the location, where access to smaller memory tends to be faster than larger ones. We mainly use internal memory (IMEM, 8MB of SRAM) for flow metadata and external memory (EMEM, 8GB of DRAM) for packet contents. Depending on the flow metadata size, IMEM can support up to 128K to 256K concurrent flows. While EMEM would support more flows, it is 2.5x slower. Each FPC This hides memory access latency similarly to GPU.</p><p>Figure <ref type="figure">3</ref> shows the packet forwarding performance of Agilio LX as a function of cycles spent by custom C code, where L3 forwarding is implemented in P4. We see that it achieves the line rate (40 Gbps) for any packets larger than 128B. However, 64B packet forwarding throughput is only 42.9 Mpps (or 28.8 Gbps) even without any custom code. We suspect the bottleneck lies in scattering and gathering of packets across the FPCs. The performance starts to drop as the custom code spends more than 200 cycles, so minimizing cycle consumption on NIC is critical for high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AccelTCP Design Rationale</head><p>AccelTCP is a dual-stack TCP architecture that harnesses NIC hardware as a TCP protocol accelerator. So, the primary task in AccelTCP's design is to determine the target for offloading. In this regard, AccelTCP divides the TCP stack operations into two categories: central TCP operations that involve application data transfer and peripheral TCP operations required for protocol conformance or mechanical operations that can bypass the application logic. Central TCP operations refer to all aspects of application data transfer -reliable data transfer with handling ACKs, inferring loss and packet retransmission, tracking received data and performing flow reassembly, enforcing congestion/flow control, and detecting errors (e.g., abrupt connection closure by a peer). These are typically complex and subject to flexible policies, which demands variable amount of compute cycles. One can optimize them by exploiting flow-level parallelism <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b57">59]</ref> or by steering the tasks into fast and slow paths <ref type="bibr" target="#b46">[48]</ref> on kernel-bypass stacks. However, the inherent complexity makes it a poor fit for NIC offloading as evidenced by the full stack TOE approach.</p><p>Peripheral operations refer to the remaining tasks whose operation is logically independent from the application. These include traditional partial NIC offload tasks 3 , connection setup and teardown, and blind relaying of packets between two connections that requires no application-level intervention. Peripheral tasks are either stateless operations with a fixed processing cost or lightly stateful operations that synchronize the states for reliable data transfer. We mainly target these operations for offloading as they can be easily separated from the host side that runs applications.</p><p>Connection management offload: State synchronization at the boundary of a connection is a key requirement for TCP, but it is a pure overhead from the application's perspective. While NIC offload is logically desirable, conventional wisdom suggests otherwise due to complexity <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b46">48]</ref>. Our position is that one can tame the complexity on recent smart NICs. First, connection setup operations can be made stateless with SYN-cookies <ref type="bibr" target="#b17">[20]</ref>. Second, the common case of connection teardown is simple state transition, and modern smart NICs have enough resources to handle a few exceptions.</p><p>Connection splicing offload: Offloading connection splicing to NIC is conceptually complex as it requires state management of two separate connections on NIC. However, if the application does not modify the relayed content, as is often the case with L7 LBs, we can simulate a single logical connection with two physical connections. This allows the NIC to operate as a fast packet forwarder that simply translates the packet header. The compute cycles for this are fixed with a small per-splicing state.</p><p>To support the new offload tasks, we structure the dual-stack design with the following guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Full control by the host side:</head><p>The host side should enforce full control of offloading, and it should be able to operate standalone. This is because the host stack must handle corner cases that cannot benefit from offload. For example, a SYN packet without the timestamp option should be handled by the host stack as SYN-cookie-based connection setup would lose negotiated TCP options (Section ยง4). Also, the host stack could decide to temporarily disable connection offload when it detects the overload of the NIC.</p><p>2. Single ownership of a TCB: AccelTCP offloads stateful operations that require updating the TCB. However, maintaining shared TCBs consistently across two stacks is very challenging. For example, a send buffer may have unacknowledged data along with the last FIN packet. The host stack may decide to deliver all data packets for itself while it offloads the connection teardown to NIC simultaneously. Unfortunately, handling 3 Such as checksum calculation, TSO, and LRO.</p><p>ACKs and retransmission across two stacks require careful synchronization of the TCB. To avoid such a case, AccelTCP enforces an exclusive ownership of the TCB at any given time -either host or NIC stack holds the ownership but not both. In the above case, the host stack offloads the entire data to the NIC stack and forgets about the connection. The NIC stack handles remaining data transfer as well as connection teardown.</p><p>3. Minimal complexity on NIC: Smart NICs have limited compute resources, so it is important to minimize complex operations on NIC. A tricky case arises at connection teardown as the host stack can offload data transfer as well. In that case, the host stack limits the amount of data so that the NIC stack avoids congestion control and minimizes state tracking of data packets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AccelTCP NIC Dataplane</head><p>In this section, we present the design of AccelTCP NIC stack in detail. Its primary role is to execute three offload tasks requested by the host stack. Each offload task can be enabled independently and the host side can decide which flows to benefit from it. The overall operation of NIC offload is shown in Figure <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Connection Setup Offload</head><p>An AccelTCP server can offload the connection setup process completely to the NIC stack. For connection setup offload, the server installs the metadata such as local IP addresses and ports for listening on NIC, and the NIC stack handles all control packets in a three-way handshake. Then, only the established connections are delivered to the host stack.</p><p>AccelTCP leverages SYN cookies <ref type="bibr" target="#b17">[20]</ref> for stateless handshake on NIC. Stateless handshake enables a more efficient implementation as most smart NICs support fast one-way hashing functions in hardware <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b12">14]</ref>. When a SYN packet arrives, the NIC stack responds with an SYN-ACK packet whose initial sequence number (ISN) is chosen carefully. The ISN consists of 24 bits of a hash value produced with the input of the 4-tuple of a connection and a nonce, 3 bits of encoded maximum segment size (MSS), and time-dependent 5 bits to prevent replay attacks. When an ACK for the SYN-ACK packet arrives, the NIC stack verifies if the ACK number matches (ISN + 1). If it matches, the NIC stack passes the ACK packet up to the host stack with a special marking that indicates a new connection and the information on negotiated TCP options. To properly handle TCP options carried in the initial SYN, the NIC stack encodes all negotiated options in the TCP Timestamps option <ref type="bibr" target="#b19">[22]</ref> of the SYN-ACK packet <ref type="bibr" target="#b7">[9]</ref>. Then, the NIC stack can retrieve the information from the TSecr value echoed back with the ACK packet. In addition, we use extra one bit in the timestamp field to differentiate a SYN-ACK packet from other packets. This would allow the NIC stack to bypass ACK number verification for normal packets. The TCP Timestamps option is popular (e.g., enabled on 84% of hosts in a national-scale network <ref type="bibr" target="#b49">[51]</ref>), and enabled by default on most OSes, but in case a client does not support it, the NIC stack hands the setup process over to the host stack.</p><p>One case where SYN cookies are deprecated is when the server must send the data first after connection setup (e.g., SMTP server). In this case, the client could wait indefinitely if the client-sent ACK packet is lost as the SYN-ACK packet is never retransmitted. Such applications should disable connection setup offload and have the host stack handle connection setup instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Connection Teardown Offload</head><p>The application can ask for offloading connection teardown on a per-flow basis. If the host stack decides to offload connection teardown, it hands over the ownership of the TCB and remaining data in the send buffer to the NIC stack. Then, the host stack removes the flow entry from its connection table, and the NIC stack continues to handle the teardown.</p><p>Connection teardown offload is tricky as it must maintain per-flow states while it should ensure reliable delivery of the FIN packet with the offloaded data. To minimize the complexity, the host stack offloads connection teardown only when the following conditions are met. First, the amount of remaining data should be smaller than the send window size. This would avoid complex congestion control on NIC while it still benefits most short-lived connections. <ref type="foot" target="#foot_0">4</ref> Second, if the application wants to confirm data delivery at close(), the host stack should handle the connection teardown by itself. For example, an application may make close() to block until all data is delivered to the other side (e.g., SO_LINGER option). In that case, processing the teardown at the host stack is much simpler as it needs to report the result to the application. Fortunately, blocking close() is rare in busy TCP servers as it not only kills the performance, but a well-designed application-level protocol may avoid it. Third, the number of offloaded flows should not exceed a threshold, determined by available memory size on NIC. For each connection teardown, the host stack first checks the number of connection closures being handled by the NIC, and the host stack carries out the connection teardown if the number exceeds the threshold.   The NIC stack implements the teardown offload by extending the TSO mechanism. On receiving the offload request, it stores a 26-byte flow state 5 at the on-chip SRAM (e.g., 8MB of IMEM), segments the data into TCP packets, and send them out. Then, it stores the entire packets at the off-chip DRAM (e.g., 8GB of EMEM) for potential retransmission. This would allow tracking over 256k concurrent flows being closed on NIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Timeout management:</head><p>The teardown process requires timeout management for packet retransmission and for observing a timeout in the TIME_WAIT state. AccelTCP uses three duplicate ACKs and expiration of retransmission timeout (RTO) as a trigger for packet retransmission. For teardown offload, however, RTO is the main mechanism as the number of data packets is often too small for three duplicate ACKs. Also, any side that sends the FIN first would end up in the TIME_WAIT state for a timeout. A high-performance server typically avoids this state by having the clients initiate the connection closure, but sometimes it is inevitable. AccelTCP supports the TIME_WAIT state, but it shares the same mechanism as RTO management for the timer.</p><p>Unfortunately, an efficient RTO implementation on NIC is challenging. For multicore CPU systems, a list or a hash table implementation would work well as each CPU core handles only its own flows affinitized to it without a lock. However, smart NICs often do not guarantee 5 a 4-tuple of the connection, TCP state, expected sequence and ACK numbers, and current RTO. flow-core affinity, so a list-based implementation would incur huge lock contention with many processor cores.</p><p>We observe that RTO management is write-heavy as each offloaded flow (and each packet transmission) would register for a new RTO. Thus, we come up with a data structure called timer bitmap wheel, which allows concurrent updates with minimal lock contention. It consists of ๐ timer bitmaps where each bitmap is associated with a distinct timeout value. The time interval between two neighboring timer bitmaps is fixed (e.g., 100 us for Figure <ref type="figure" target="#fig_5">6</ref>). When one time interval elapses, all bitmaps rotate in the clockwise direction by one interval, like Figure <ref type="figure" target="#fig_5">6-(b</ref>). Bitmap rotation is efficiently implemented by updating a pointer to the RTO-expired bitmap every time interval. Each timer bitmap records all flows with the same RTO value, where the location of a bit represents a flow id (e.g., n-th bit in a bitmap refers to a flow id, n). When the RTO of a timer bitmap expires, all flows in the bitmap retransmit their unacknowledged packets. From the location of each bit that is set, one can derive the corresponding flow id and find the pointer to its flow state that holds all the metadata required for retransmission. Then, all bits in the bitmap are reset to zero and its RTO is reset to (N x (time interval)). RTO-expired flows register for a new RTO. When an ACK for the FIN of a flow arrives, the flow is removed from its RTO bitmap. One can implement an RTO larger than the maximum by keeping a counter in the flow state that decrements every expiration of the maximum RTO.</p><p>The timer bitmap wheel allows concurrent updates by multiple flows as long as their flow ids belong to different 32-bit words in the bitmap. Only the flows whose ids share the same 32-bit word contend for a lock for access. On the down side, it exhibits two overheads: memory space for bitmaps and bitmap scanning at RTO expiration. The memory consumption is not a big concern as it requires only 8KB for each bitmap for 64k concurrent flows being closed. We reduce the scanning overhead by having multiple cores scan a different bitmap region in parallel. reduce the scanning overhead, but we find that the cost for counter update is too expensive even with atomic increment/decrement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Connection Splicing Offload</head><p>Connection splicing offload on NIC allows zero-DMA data transfer. The key idea is to simulate a single connection by exploiting the NIC as a simple L4 switch that translates the packet header. An L7 proxy can ask for connection splicing on NIC if it no longer wants to relay packets of the two connections in the application layer. On a splicing offload request, the host stack hands over the states of two connections to NIC, and removes their TCBs from its connection table. The NIC stack takes over the ownership, and installs two L4 forwarding rules for relaying packets. The host stack keeps track of the number of spliced connections offloaded to NIC, and decides whether to offload more connections by considering the available memory on NIC. Figure <ref type="figure">7</ref> shows the packet translation process. It simply swaps the 4-tuples of two connections and translates the sequence/ACK numbers and TCP/IP checksums of a packet with pre-calculated offsets. While the Figure assumes that the proxy does not modify any content, but one can easily support such a case. For example, if a proxy modifies request or response headers before splicing, the host stack only needs to reflect the extra delta in sequence and ACK numbers into the pre-calculated offsets. One limitation in our current scheme is that the proxy may not read or modify the packets any more after splicing offload.</p><p>Efficient TCP/IP checksum update: Translating a packet header requires TCP/IP checksum update. However, recalculating the TCP checksum is expensive as it scans the entire packet payload. To avoid the overhead, AccelTCP adopts differential checksum update, which exploits the fact that the one's complement addition is both associative and distributive. Since only the 4-tuple of a connection and sequence and ACK numbers are updated, we only need to add the difference (or offset) of these values to the checksum. Figure <ref type="figure">8</ref> shows the algorithm. Upon splicing offload request, the NIC stack pre-calculates the offsets for IP and TCP checksums, respectively (Line 2-4). For each packet for translation, it adds the offsets to IP and TCP checksums, respectively (Line 7-8). One corner case arises if a sequence or an ACK number wraps around. In that case, we need to subtract 1 from the checksum to conform to 1's complement addition (Line 9-10).</p><p>Tracking teardown state: Since connection splicing operates by merging two connections into one, the NIC stack only needs to passively monitor connection teardown by the server and the client. When the spliced connection closes completely or if it is reset by any peer, the NIC stack removes the forwarding rule entries, and notifies the host stack of the closure. This allows reusing TCP ports or tracking connection statistics at the host.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AccelTCP Host Stack</head><p>The AccelTCP host stack is logically independent of the NIC stack. While our current implementation is based on mTCP <ref type="bibr" target="#b39">[41]</ref>, one can extend any TCP stack to harness our NIC offloading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Socket API Extension</head><p>AccelTCP allows easy porting of existing applications by reusing the epoll()-based POSIX-like socket API of mTCP. In addition, it extends the API to support flexible NIC offloading as shown in Figure <ref type="figure" target="#fig_6">9</ref>. First, AccelTCP adds extra socket options to mtcp_setsockopt() to enable connection setup and teardown offload to NIC. Note that the connection teardown offload request is advisory, so the host stack can decide not to offload the closure if the conditions are not met (Section ยง4.2). Second, Ac-celTCP adds mtcp_nsplice() to initiate splicing two connections on NIC. The host stack waits until all data  in the send buffer are acknowledged while buffering any incoming packets. Then, it installs forwarding rules onto NIC, sending the buffered packets after header translation. After calling this function, the socket descriptors should be treated as if they are closed in the application. Optionally, the application may specify a callback function to be notified when the spliced connections finish. Through the callback function, AccelTCP provides (i) remote addresses of the spliced connections, (ii) the number of bytes transferred after offloaded to NIC dataplane, and (iii) how the connections are terminated (e.g., normal teardown or reset by any peer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Host Stack Optimizations</head><p>We optimize the host networking stack to accelerate small message processing. While these optimizations are orthogonal to NIC offload, they bring a significant performance benefit to short-lived connections.</p><p>Lazy TCB creation: A full TCB of a connection ranges from 400 to 700 bytes even on recent implementations <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b39">41]</ref>. However, we find that many of the fields are unnecessary for short-lived connections whose message size is smaller than the initial window size. To avoid the overhead of a large TCB, AccelTCP creates the full TCB only when multiple transactions are observed. Instead, the host stack creates a small quasi-TCB (40 bytes) for a new connection. If the application closes the connection after a single write, the host stack offloads the teardown and destroys the quasi-TCB.</p><p>Opportunistic zero-copy: Recent high-performance TCP stacks <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b66">68]</ref> bypass the socket buffer to avoid extra memory copying. However, this often freezes the application-level buffer even after sending data, or overflows the host packet buffers if the application does not read the packets in a timely manner. AccelTCP addresses this problem by opportunistically performing a zero-copy I/O. When a stream of packets arrive in order, the application waiting for a read event will issue a read call. Then, the content of the packets is copied directly to the application buffer while any leftover is written to the receive socket buffer. When an application sends data on an empty socket buffer, the data is directly written to the host packet buffer for DMA'ing to NIC. Only when the host packet buffer is full, the data is written to the send socket buffer. Our scheme observes the semantics of standard socket operations, allowing easy porting of existing applications. Yet, this provides the benefit of zero-copying to most short-lived connections.</p><p>User-level threading: mTCP spawns two kernel-level threads: a TCP stack thread and an application thread on each CPU core. While this allows independent operations of the TCP thread (e.g., timer operations), it incurs a high context switching overhead. To address the problem, we modify mTCP to use cooperative userlevel threading <ref type="bibr" target="#b11">[13]</ref>. We find that this not only reduces the context switching overhead, but it also allows other optimizations like lazy TCB creation and opportunistic zero-copying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate AccelTCP by answering following questions. First, does stateful TCP offloading and host stack optimizations demonstrate a high performance in a variety of workloads? ( ยง6.1) Second, does it deliver the performance benefit to real-world applications? ( ยง6.2) Finally, is the extra cost of a smart NIC justifiable? ( ยง6.3)</p><p>Experiment setup: Our experimental setup consists of one server (or a proxy), four clients, and four backend servers. The server machine has an Intel Xeon Gold 6142 @ 2.6GHz with 128 GB of DRAM and a dual-port Netronome Agilio LX 40GbE NIC (NFP-6480 chipset). Each client has an Intel Xeon E5-2640 v3 @ 2.6GHz, and back-end servers have a mix of Xeon E5-2699 v4 @ 2.2GHz and Xeon E5-2683 v4 @ 2.1GHz. The client and backend server machines are configured with Intel XL710-QDA2 40GbE NICs. All the machines are connected to a Dell Z9100-ON switch, configured to run at 40 GbE speed. For TCP stacks, we compare AccelTCP against mTCP <ref type="bibr" target="#b39">[41]</ref> and IX <ref type="bibr" target="#b28">[30]</ref>. All TCP stacks employ DPDK <ref type="bibr" target="#b8">[10]</ref> for kernel-bypass packet I/O. Clients and back-end servers run mTCP patched to use cooperative user-level threading as AccelTCP. For IX experiments, we use two dual-port Intel X520-DA2 10GbE NICs, and enable all four ports bonded with a L3+L4 hash to balance the load as IX does not support 40GbE NICs. We verify that any single 10GbE port does not become the bottleneck based on port-level statistics at the switch.</p><p>Hyperthreading is disabled for mTCP and AccelTCP, and enabled for IX when it improves the performance. Our current prototype uses CRC32 to generate SYN cookies for connection setup. To prevent state explosion attacks, one needs to use a cryptographic hash function (such as MD5 or SHA2). Unfortunately, the API sup- port for hardware-assisted cryptographic operations in Agilio NICs is currently incomplete (for both C and P4 code), so we use CRC32 instead here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Microbenchmark</head><p>We evaluate AccelTCP's performance for handling shortlived TCP connections and L7 proxying, and compare against the performance of the state-of-the-art TCP stacks: mTCP <ref type="bibr" target="#b39">[41]</ref> and IX <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Short-lived Connection Performance</head><p>We evaluate the benefit of connection management offload by comparing the performance of TCP echo servers that perform 64B packet transactions with persistent vs. non-persistent connections. The TCP echo servers maintain 16k concurrent connections, and the performance results are averaged over one-minute period for five runs in each experiment. In the non-persistent case, a new connection is created immediately after every connection closure. AccelTCP offloads connection setup and offload to NIC while mTCP handles them using CPU.</p><p>For IX, we evaluate only the persistent connection case as IX experiences a crash when handling thousands of concurrent connections with normal teardown. Figure <ref type="figure" target="#fig_0">10</ref> compares the throughputs over varying numbers of CPU cores. AccelTCP achieves 2.2x to 3.8x better throughputs than non-persistent mTCP, comparable to those of persistent connections. Surprisingly, AccelTCP outperforms persistent connections by 13% to 54% for up to four CPU cores. This is because AccelTCP benefits from lazy TCB creation ( ยง5.2) while persistent connections suffer from a CPU bottleneck. However, its eight-core performance is 22% lower than that of persistent IX, implying a bottleneck on NIC. Overall, connection management offload brings a significant performance benefit, which enables short-lived connections to perform comparably to persistent connections. Table <ref type="table" target="#tab_5">3</ref> shows the breakdown of performance in terms of the contribution by each optimization. We find that connection setup and teardown offload improve the baseline performance by 2.3x while other host stack optimizations contribute by extra 1.5x.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> compares the goodputs over varying message sizes on a single CPU core. AccelTCP maintains the performance benefit over different message sizes with a speedup of 2.5x to 3.6x. The performance of messages larger than one MSS is limited at 20 Gbps, which seems impacted by our software TSO implementation on NIC. The current Agilio NIC SDK does not provide an API to exploit hardware TSO for programmable dataplane. We believe the single core performance would further improve with proper hardware support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Layer-7 Proxing Performance</head><p>We now evaluate connection splicing offload with a simple L7 LB called epproxy that inspects the initial request, determines a back-end server, and relays the content between the client and the server. We measure the wire-level, receive-side throughput (including control packets) on the client side over different message sizes. Clients spawn 8k concurrent connections with epproxy, and the proxy creates 8k connections with back- end servers. We confirm that both clients and back-end servers are not the bottleneck. We configure epproxy-mTCP to use eight cores while epproxy-AccelTCP uses only a single core as CPU is not the bottleneck. All connections are persistent, and we employ both ports of the Agilio LX NIC here. The NIC is connected to the host via 8 lanes of PCIe-v3 6 .</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows that AccelTCP-proxy outperforms epproxy-mTCP by 1.4x to 2.2x even if the latter employs 8x more CPU cores. We make two observations here. First, the performance of epproxy-AccelTCP reaches full 80 Gbps from 512-byte messages, which exceeds the PCIe throughput of the NIC. This is because epproxy-AccelTCP bypasses host-side DMA and fully utilizes the forwarding capacity of the NIC. Second, epproxy-AccelTCP achieves up to twice as large goodput as the epproxy-mTCP. For example, epproxy-AccelTCP actually performs 2.8x more transactions per second than epproxy-mTCP for 64B messages. This is because Ac-celTCP splices two connections into a single one while mTCP relays two connections. For each request from a client, epproxy-mTCP must send an ACK as well as a response packet from the back-end server. In contrast, epproxy-AccelTCP replays only the response packet with a piggybacked ACK from the back-end server.</p><p>We move on to see if epproxy-AccelTCP fares well on non-persistent connections. Figure <ref type="figure" target="#fig_0">13</ref> shows the performance over varying numbers of message transactions per connection. AccelTCP performs 1.8x better at a single transaction, and the performance gap widens as large as 2.4x at 128 transactions per connection. This confirms that proxying non-persistent connections also benefit from splicing offload of AccelTCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Application Benchmark</head><p>We investigate if AccelTCP delivers the performance benefit to real-world applications.</p><p>Key-value store (Redis): We evaluate the effectiveness of AccelTCP with Redis (v.4.0.8) [17], a popular 6 Theoretical maximum throughput is 63 Gbps according to <ref type="bibr" target="#b56">[58]</ref>.  in-memory key-value store. We use Redis on mTCP as a baseline server while we port it to use AccelTCP for comparison. We test with the USR workload from Facebook <ref type="bibr" target="#b27">[29]</ref>, which consists of 99.8% GET requests and 0.2% SET requests with short keys (&lt; 20B) and 2B values. For load generation, we use a Redis client similar to memtier_benchmark <ref type="bibr" target="#b15">[18]</ref> written in mTCP. We configure the Redis server and the clients to perform a single key-value transaction for each connection to show the behavior when short-lived connections are dominant. Table <ref type="table" target="#tab_7">4</ref> compares the throughputs. Redis-AccelTCP achieves 1.6x to 2.3x better performance than Redis-mTCP, and its performance scales well with the number of CPU cores. Figure <ref type="figure" target="#fig_8">14</ref> shows that mTCP consumes over a half of CPU cycles on TCP stack operations. In contrast, AccelTCP saves up to 75% of the CPU cycles for TCP processing. With AccelTCP, session initialization and destruction of Redis limits the performance. Our investigation reveals that the overhead mostly comes from dynamic memory (de)allocation (zmalloc() and zfree()) for per-connection metadata, which incurs a severe penalty for handling short-lived connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L7 LB (HAProxy):</head><p>We see if AccelTCP improves the performance of HAProxy (v.1.8.0) <ref type="bibr" target="#b4">[6]</ref>, a widely used HTTP-based L7 LB. We first port HAProxy to use mTCP and AccelTCP, respectively, and evaluate the throughput with the SpecWeb2009 <ref type="bibr" target="#b25">[26]</ref>-like workload. The workload consists of static files whose size ranges from 30 to 5,670 bytes with an average file size of 728 bytes. For a fair comparison, we disable any header rewriting in the both version after delivering the first HTTP request. We spawn 8k persistent connections, using simple epollbased clients and back-end servers running on mTCP. Table <ref type="table" target="#tab_8">5</ref> compares the throughputs with with 1 core and lower than that of HAProxy-mTCP. We observe that the performance benefit is much larger than in Section 6.1.2 because HAProxy has a higher overhead in applicationlevel request processing and packet relaying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cost-effectiveness Analysis</head><p>AccelTCP requires a smart NIC, which is about 3-4x more expensive than a normal NIC at the moment. For fairness, we try comparing the cost effectiveness by the performance-per-dollar metric. We draw hardware prices from Intel <ref type="bibr" target="#b9">[11]</ref> and Colfax <ref type="bibr" target="#b2">[4]</ref> pages (as of August 2019), and use the performance of 64B packet transactions on short-lived connections. Specifically, we compare the performance-per-dollar with a system that runs mTCP with a commodity NIC (Intel XL710-QDA2, $440) vs. another system that runs AccelTCP with a smart NIC (Agilio LX, $1,750). For CPU, we consider Xeon E5-2650v2 ($1,170) and Xeon Gold 6142 ($2,950). For simplicity, we only consider CPU and NIC as hardware cost. Table <ref type="table" target="#tab_9">6</ref> suggests that NIC offload with AccelTCP is 1.6x to 1.9x more cost-effective, and the gap would widen further if we add other fixed hardware costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Kernel-bypass TCP stacks: Modern kernel-bypass TCP stacks such as mTCP <ref type="bibr" target="#b39">[41]</ref>, IX <ref type="bibr" target="#b28">[30]</ref>, SandStorm <ref type="bibr" target="#b53">[55]</ref>, F-Stack <ref type="bibr" target="#b3">[5]</ref> deliver high-performance TCP processing of small message transactions. Most of them employ a fast user-level packet I/O <ref type="bibr" target="#b8">[10]</ref>, and exploit high parallelism on multicore systems by flow steering on NIC. More recently, systems like ZygOS <ref type="bibr" target="#b61">[63]</ref>, Shinjuku <ref type="bibr" target="#b40">[42]</ref>, and Shenango <ref type="bibr" target="#b57">[59]</ref> further improve kernel-bypass stack by reducing the tail latency, employing techniques like task stealing, centralized packet distribution, and dynamic core reallocation. We believe that these works are largely orthogonal but complementary to our work as AccelTCP would enhance these stacks by offloading connection management tasks to NIC.</p><p>NIC offload: Existing TCP offloads mostly focus on improving large message transfer either by offloading the whole TCP stack <ref type="bibr" target="#b48">[50]</ref> or by selectively offloading common send-receive operations <ref type="bibr" target="#b44">[46]</ref>. In contrast, our work focuses on connection management and proxying whose performance is often critical to modern network workloads, while we intentionally avoid the complexity of application data transfer offloading. UNO <ref type="bibr" target="#b50">[52]</ref> and Metron <ref type="bibr" target="#b43">[45]</ref> strive to achieve optimal network function (NF) performance with NIC offload based on runtime traffic statistics. We plan to explore dynamic offloading of a subset of networking stack features (or connections) in response to varying load in the future. To offload TCP connection management, any L2-L4 NFs that should run prior to TCP stack (e.g., firewalling or host networking) must be offloaded to NIC accordingly. Such NFs can be written in P4 <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b54">56]</ref> and easily integrated with AccelTCP by properly placing them at ingress/egress pipelines of the NIC dataplane.</p><p>L7 proxing and short RPCs: Our connection splicing is inspired by the packet tunneling mechanism of Yoda L7 LB <ref type="bibr" target="#b34">[36]</ref>. However, Yoda operates as a packet-level translator without a TCP stack, so it cannot modify any of relayed content. In contrast, an AccelTCP application can initiate the offload after any content modification. Also, AccelTCP packet translation runs on NIC hardware, promising better performance. Finally, we note that eRPC <ref type="bibr" target="#b42">[44]</ref> achieves 5.0 Mtps RPC performance (vs. 3.4 Mtps of AccelTCP) on a single core. However, ePRC is limited to data center environments while AccelTCP is compatible to TCP and accommodates any TCP clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we have presented AccelTCP that harnesses modern programmable NICs as a TCP protocol accelerator. Drawing the lessons from full stack TOE, Ac-celTCP's design focuses on minimizing the interaction with the host stack by offloading only select features of stateful TCP operations. AccelTCP manages the complexity on NIC by stateless handshake, single ownership of a TCB, and conditional teardown offload. In addition, it simplifies connection splicing by efficient packet header translation. We have also presented a number of optimizations that significantly improve the host stack.</p><p>We have demonstrated that AccelTCP brings a substantial performance boost to short-message transactions and L7 proxying. AccelTCP delivers a 2.3x speedup to Redis on a kernel-bypass stack while it improves the performance of HAProxy by a factor of 11.9. AccelTCP is available at https://github.com/acceltcp, and we hope our effort will renew the interest in selective NIC offload of stateful TCP operations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Small packet (64B) performance with non-persistent and persistent connections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Architecture of SoC-based NIC (Agilio LX)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Split of TCP functionality in AccelTCP can run up to 8 cooperative threads -access to slow memory by one thread would trigger a hardware-based context switch to another, which takes only 2 cycles. This hides memory access latency similarly to GPU.Figure3shows the packet forwarding performance of Agilio LX as a function of cycles spent by custom C code, where L3 forwarding is implemented in P4. We see that it achieves the line rate (40 Gbps) for any packets larger than 128B. However, 64B packet forwarding throughput is only 42.9 Mpps (or 28.8 Gbps) even without any custom code. We suspect the bottleneck lies in scattering and gathering of packets across the FPCs. The performance starts to drop as the custom code spends more than 200 cycles, so minimizing cycle consumption on NIC is critical for high performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: AccelTCP NIC offload (We show only active close() by server for (b), but it also supports passive close().)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Timer bitmap wheel for RTO management on NIC. T RTO represents the remaining time until retransmission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Socket API extension for AccelTCP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Performance of short-lived connections for varying message sizes on a single CPU core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: CPU breakdown of Redis on a single CPU core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CPU</figDesc><table><row><cell cols="3">usage breakdown of a user-level TCP echo server</cell></row><row><cell cols="2">(a single 64B packet exchange per connection)</cell><cell></cell></row><row><cell></cell><cell>64B</cell><cell>1500B</cell></row><row><cell>L7 proxy (mTCP)</cell><cell>2.1 Gbps</cell><cell>5.3 Gbps</cell></row><row><cell cols="2">L7 proxy with splice() (mTCP) 2.3 Gbps</cell><cell>6.3 Gbps</cell></row><row><cell>L3 forward at host (DPDK)</cell><cell>7.3 Gbps</cell><cell>39.8 Gbps</cell></row><row><cell>L3 forward at NIC 2</cell><cell cols="2">28.8 Gbps 40.0 Gbps</cell></row><row><cell cols="3">Table 2: L7 proxying and L3 forwarding performance on a</cell></row><row><cell>single CPU core</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Connection splicing on NIC dataplane. IP ๐ถ , IP ๐ , IP ๐ : IP addresses of client, proxy, and server, P ๐ถ , P ๐ : port numbers of client and server, P ๐๐ , P ๐๐ : port numbers of proxy for the client side and the server side, RBUF ๐ถ , RBUF ๐ : read buffers on each side, WBUF ๐ถ , WBUF ๐ : write buffers on each side.</figDesc><table><row><cell>IP C (P C ) Client</cell><cell>RBUF C ๏ WBUF C ๏</cell><cell>IP P (P pc ) IP P (P ps ) Proxy</cell><cell>๏ ๏ WBUF S</cell><cell>IP S (P S ) Server</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RBUF S</cell><cell></cell></row><row><cell cols="5">ฮ SEQ = (Last SEQ# written to WBUF S ) -(Last SEQ# read from RBUF C ) ฮ ACK = (Last SEQ# written to WBUF S ) -(Last SEQ# read from RBUF S )</cell></row><row><cell cols="2">IP C (P C ) ๏ IP P (P pc ) SEQ #: X ACK #: Y</cell><cell>OFFLOAD NIC</cell><cell cols="2">IP P (P ps ) ๏ IP S (P S ) SEQ #: X + ฮ SEQ ACK #: Y + ฮ ACK</cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Keeping a per-region counter might further USENIX Association 17th USENIX Symposium on Networked Systems Design and Implementation 83</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>On splicing offload for a flow from IP C (P C ) to IP S (P S ): CSO IP ๏ IP S + IP C CSO TCP ๏ CSO IP + P S + P ps -P C -P pc + ฮ SEQ + ฮ ACK Store CSO IP and CSO TCP For any next incoming packets from IP C (P C ) to IP S (P S ): Load CSO IP and CSO TCP CS IP ๏ CS IP + CSO IP CS TCP ๏ CS TCP + CSO TCP If (SEQ #) &gt; (-ฮ SEQ ), then CS TCP ๏ CS TCP -1 If (ACK #) &gt; (-ฮ ACK ), then CS TCP ๏ CS TCP -1 Figure 8: Differential checksum update. CSO: checksum offset, CS: checksum. Other notations are in Figure 7. Note that + and โ indicate 1's complement addition and subtraction.</figDesc><table><row><cell>1</cell></row><row><cell>2</cell></row><row><cell>3</cell></row><row><cell>4</cell></row><row><cell>5</cell></row><row><cell>6</cell></row><row><cell>7</cell></row><row><cell>8</cell></row><row><cell>9</cell></row><row><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Breakdown of contribution by each optimization on a single CPU core (64B packet transactions)</figDesc><table><row><cell></cell><cell cols="4">mTCP (non-persistent)</cell><cell></cell><cell cols="3">AccelTCP (non-persistent)</cell></row><row><cell></cell><cell cols="3">mTCP (persistent)</cell><cell></cell><cell></cell><cell cols="2">IX (persistent)</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>6 )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transactions/sec (x10</cell><cell>0 5 10 15</cell><cell cols="2">0.9 1.8 3.4 6.1</cell><cell></cell><cell>3.4 9.9</cell><cell></cell><cell></cell><cell>6.3 13.7</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of CPU cores</cell></row><row><cell cols="9">Figure 10: Throughputs of 64B packet transactions</cell></row><row><cell cols="2">Action</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mtps Speedup</cell></row><row><cell cols="5">Baseline (w/o NIC offload)</cell><cell></cell><cell></cell><cell>0.89</cell><cell>1.0x</cell></row><row><cell cols="6">+ Enable setup offload ( ยง4.1)</cell><cell></cell><cell>1.21</cell><cell>1.4x</cell></row><row><cell cols="6">+ Enable teardown offload ( ยง4.2)</cell><cell></cell><cell>2.06</cell><cell>2.3x</cell></row><row><cell cols="7">+ Enable opportunistic TCB creation &amp; opportunistic zero-copy ( ยง5.2)</cell><cell>3.42</cell><cell>3.8x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Redis performance for short-lived connections</figDesc><table><row><cell>1-core</cell><cell>8-core</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>L7 LB performance for SpecWeb2009-like workload</figDesc><table><row><cell></cell><cell>1-core</cell><cell>8-core</cell></row><row><cell>HAProxy-mTCP</cell><cell>4.3 Gbps</cell><cell>6.2 Gbps</cell></row><row><cell cols="3">HAProxy-AccelTCP 73.1 Gbps 73.1 Gbps</cell></row><row><cell></cell><cell cols="2">E5-2650v2 Gold 6142</cell></row><row><cell>mTCP (XL710-QDA2)</cell><cell>1.00</cell><cell>1.25</cell></row><row><cell>AccelTCP (Agilio LX)</cell><cell>1.93</cell><cell>1.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of normalized performance-per-dollar 8 cores. HAProxy-AccelTCP achieves 73.1 Gbps, a 11.9x better throughput than HAProxy-mTCP. The average response time of HAProxy-AccelTCP is 0.98 ms, 13.6x</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">RFC 6928<ref type="bibr" target="#b18">[21]</ref> suggests 10 MSS as the initial window size.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our shepherd Andrew Moore and anonymous reviewers of NSDI 2020 for their insightful comments and feedback on the paper. We also thank Ilwoo Park for applying user-level threading to the mTCP stack. This work was supported by Institute</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Connection setup: When an application listens on a socket whose setup offload option is enabled, the host stack sends a special control packet to NIC, carrying the listening address/port and TCP options that must be delivered to the remote host during connection setup (e.g., MSS, Window scale factor, Selective ACK, etc.). To notify a new connection, the NIC stack sets the Ethertype of the ACK packet to 0x090A, and delivers the negotiated options in the TCP Timestamps option. The host stack extracts only the TCP options, and ignores the NIC-generated timestamp value.</p><p>Connection teardown: For teardown offload, the host stack creates a TSO packet that holds all remaining data in the send buffer, and sets the EtherType to 0x090B. It also encodes other information such as MSS (2 bytes), current RTO (4 bytes), and current TCP state (2 bytes) in the special header area. The NIC stack notifies the host stack of the number of connections being closed on NIC by either sending a control packet or tagging at any packet delivered to host.</p><p>Connection splicing: For splicing offload, the host stack uses 0x090C as EtherType, and writes the sequence and ACK number offsets (4 bytes each), and a 4-tuple of a connection in the special header. When the splicing offload packet is passed to the NIC stack, a race condition may arise if some packets in the flows are passed up to the host stack at the same time. To ensure correct forwarding, the host stack keeps the connection entries until it is notified that the splicing rules are installed at NIC. For reporting a closure of spliced connections, NIC creates a special control packet holding the connection information and traffic statistics with the EtherType, 0x090D, and sends it up to the host stack. By monitoring those control packets, the host stack can keep track of the number of active spliced connections on NIC.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Agilioยฎ</surname></persName>
		</author>
		<author>
			<persName><surname>Smartnic</surname></persName>
		</author>
		<ptr target="https://www.netronome.com/m/documents/PB_Agilio_LX_2x40GbE.pdf" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Api</forename><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName><surname>Gateway</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/api-gateway/" />
		<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Colfax</forename><surname>Direct</surname></persName>
		</author>
		<ptr target="https://colfaxdirect.com.Ac-cessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High Performance Network Framework Based on DPDK</title>
		<author>
			<persName><forename type="first">F-Stack</forename></persName>
		</author>
		<ptr target="http://www.f-stack.org/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://www.haproxy.org/" />
		<title level="m">HAProxy: The Reliable, High Performance TCP/HTTP Load Balancer</title>
				<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="https://1.ieee802.org/dcb/802-1qau/" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">802</biblScope>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<ptr target="https://1.ieee802.org/dcb/802-1qbb/.Ac-cessed" />
	</analytic>
	<monogr>
		<title level="m">1Qbb -Priority-based Flow Control</title>
				<imprint>
			<biblScope unit="volume">802</biblScope>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Improving</forename><surname>Syncookies</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/277146/" />
		<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Dpdk</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="http://dpdk.org/" />
		<title level="m">Data Plane Development Kit</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://ark.intel.com" />
		<title level="m">Intel Product Specification</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tcp</forename><surname>Linux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Engines</forename><surname>Offload</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/148697/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://lthread.readthedocs.io" />
		<title level="m">Multicore / Multithread Coroutine Library</title>
				<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mellanox</forename><surname>Bluefieldโข</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smartnic</forename></persName>
		</author>
		<ptr target="http://www.mellanox.com/related-docs/prod_adapter_cards/PB_BlueField_Smart_NIC.pdf" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://download.microsoft.com/download/5/b/5/5b5bec17-ea71-4653-9539-204a672f11cf/scale.doc" />
		<title level="m">Microsoft Windows Scalable Networking Initiative</title>
				<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://www.netronome.com/m/redactor_files/WP_OVS_Benchmarking.pdf" />
		<title level="m">Open vSwitch Offload and Acceleration with Agilio SmartNICs</title>
				<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">RedisLabs/memtier_benchmark: NoSQL Redis and Memcache traffic generation and benchmarking tool</title>
		<ptr target="https://github.com/RedisLabs/memtier_benchmark" />
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://tools.ietf.org/html/rfc2663" />
		<title level="m">RFC 2663</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="https://tools.ietf.org/html/rfc4987" />
		<title level="m">RFC 4987</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://tools.ietf.org/html/rfc6928" />
		<title level="m">RFC 6928</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://tools.ietf.org/html/rfc7323" />
		<title level="m">RFC 7323</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://tools.ietf.org/html/" />
		<title level="m">RFC 791</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Accessed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://tools.ietf.org/html/" />
		<title level="m">RFC 793</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Accessed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="https://solarflare.com/wp-content/uploads/2018/11/SF-114649-CD-LATEST_Solarflare_AOE_SFA7942Q_Product_Brief.pdf" />
		<title level="m">Solarflare SFA7942Q with Stratix V A7 FPGA</title>
				<imprint>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<ptr target="https://www.spec.org/web2009/" />
		<title level="m">SpecWeb2009 Benchmark</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Why Are We Deprecating Network Performance Features (KB4014193</title>
		<ptr target="https://meta.wikimedia.org/wiki/Why_persistent_connections_are_bad#Why_persistent_connections_are_bad" />
	</analytic>
	<monogr>
		<title level="m">Why persistent connections are bad</title>
				<imprint>
			<date type="published" when="2019-08-27">2019-08-27</date>
			<biblScope unit="page" from="2019" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Workload Analysis of a Large-scale Key-value Store</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMET-RICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2012">2012</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The IX Operating System: Combining Low Latency, High Throughput, and Efficiency in a Protected Dataplane</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Primorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Network Traffic Characteristics of Data Centers in the Wild</title>
		<author>
			<persName><forename type="first">Theophilus</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Internet Measurement Conference (IMC &apos;10)</title>
				<meeting>the 2010 ACM Internet Measurement Conference (IMC &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Network Traffic Characteristics of Data Centers in the Wild</title>
		<author>
			<persName><forename type="first">Theophilus</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGCOMM conference on Internet measurement (IMC &apos;10)</title>
				<meeting>the 10th ACM SIGCOMM conference on Internet measurement (IMC &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">P4: Programming Protocol-Independent Packet Processors</title>
		<author>
			<persName><forename type="first">Pat</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Talayco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Full Hardware Based TCP/IP Traffic Offload Engine (TOE) Device and the Method Thereof</title>
		<author>
			<persName><forename type="first">Hsin-Chieh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Pang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuei-Yu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">416</biblScope>
			<date type="published" when="2010-01-12">January 12 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Server Network Scalability and TCP Offload</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Freimuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elbert</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Mraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><forename type="middle">M</forename><surname>Nahum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (ATC &apos;05)</title>
				<meeting>the 2015 USENIX Annual Technical Conference (ATC &apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yoda: A Highly Available Layer-7 Load Balancer</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Charlie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems (EuroSys &apos;16)</title>
				<meeting>the 11th European Conference on Computer Systems (EuroSys &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">RDMA over Commodity Ethernet at Scale</title>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitu</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Lipshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGCOMM Conference (SIGCOMM &apos;16)</title>
				<meeting>the 2016 ACM SIGCOMM Conference (SIGCOMM &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PacketShader: A GPU-Accelerated Software Router</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGCOMM Conference (SIGCOMM &apos;10)</title>
				<meeting>the 2010 ACM SIGCOMM Conference (SIGCOMM &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MegaPipe: A New Programming Interface for Scalable Network I/O</title>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;12)</title>
				<meeting>the USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hy-per4: Using P4 to Virtualize the Programmable Data Plane</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacobus</forename><surname>Van Der Merwe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International on Conference on emerging Networking EXperiments and Technologies (CoNEXT &apos;16)</title>
				<meeting>the 12th International on Conference on emerging Networking EXperiments and Technologies (CoNEXT &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems</title>
		<author>
			<persName><forename type="first">Eunyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinae</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Asim</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghwan</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;14)</title>
				<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shinjuku: Preemptive Scheduling for ๐second-scale Tail Latency</title>
		<author>
			<persName><forename type="first">Kostis</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">Tigar</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maziรจres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</title>
				<meeting>the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided RDMA Datagram RPCs</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;16)</title>
				<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Andersen. Datacenter RPCs can be General and Fast</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</title>
				<meeting>the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">NFV Service Chains at the True Speed of the Underlying Hardware</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Georgios P Katsikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Kostic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald Q Maguire</forename><surname>Steinert</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Metron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Symposium on Networked Systems Design and Implementation (NSDI &apos;18)</title>
				<meeting>the 15th Symposium on Networked Systems Design and Implementation (NSDI &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Efficient, Secure, and Flexible High Speed Packet Processing for Data Centers</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High performance packet processing with flexnic</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;16)</title>
				<meeting>the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">TAS: TCP Acceleration as an OS Service</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Stamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Systems (EuroSys &apos;19)</title>
				<meeting>the 14th European Conference on Computer Systems (EuroSys &apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kicinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolaas</forename><surname>Viljoen</surname></persName>
		</author>
		<title level="m">eBPF Hardware Offload to SmartNICs: cls bpf and XDP. Proceedings of Netdev 1.2, The Technical Conference on Linux Networking</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Connection Handoff Policies for TCP Offload Network Interfaces</title>
		<author>
			<persName><forename type="first">Hyong-Youb</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</title>
				<meeting>the 7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the state of ECN and TCP options on the Internet</title>
		<author>
			<persName><forename type="first">Mirja</forename><surname>Kรผhlewind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Neuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Trammell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Passive and Active Measurement Conference (PAM &apos;13)</title>
				<meeting>the 14th Passive and Active Measurement Conference (PAM &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">UNO: Uniflying Host and Smart NIC Offload for Flexible Packet Processing</title>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunseok</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarit</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><surname>Lakshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Symposium on Cloud Computing</title>
				<meeting>the 8th ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC</title>
		<author>
			<persName><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</title>
				<meeting>the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On Dominant Characteristics of Residential Broadband Internet Traffic</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vern</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Allman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM Internet Measurement Conference (IMC &apos;09)</title>
				<meeting>the 2009 ACM Internet Measurement Conference (IMC &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Network Stack Specialization for Performance</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Marinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">Nm</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="175" to="186" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switching ASICs</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongkeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGCOMM Conference (SIGCOMM &apos;17)</title>
				<meeting>the 2017 ACM SIGCOMM Conference (SIGCOMM &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">TCP Offload Is a Dumb Idea Whose Time Has Come</title>
		<author>
			<persName><surname>Jeffrey C Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Hot Topics in Operating Systems (Ho-tOS &apos;03)</title>
				<meeting>the 9th Workshop on Hot Topics in Operating Systems (Ho-tOS &apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding pcie performance for end host networking</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josรฉ</forename><surname>Fernando Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Lรณpez-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGCOMM Conference (SIGCOMM &apos;18)</title>
				<meeting>the 2018 ACM SIGCOMM Conference (SIGCOMM &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shenango: Achieving High CPU Efficiency for Latencysensitive Datacenter Workloads</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving Network Connection Locality on Multicore Systems</title>
		<author>
			<persName><forename type="first">Aleksey</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nickolai</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Systems (EuroSys &apos;12)</title>
				<meeting>the 7th European Conference on Computer Systems (EuroSys &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Arrakis: The Operating System Is the Control Plane</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">Rk</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Woos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The Design and Implementation of Open vSwitch</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Pettit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teemu</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarno</forename><surname>Rajahalme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pravin</forename><surname>Shelar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;15)</title>
				<meeting>the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ZygOS: Achieving Low Tail Latency for Microsecond-scale Networked Tasks</title>
		<author>
			<persName><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</title>
				<meeting>the 26th Symposium on Operating Systems Principles (SOSP &apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the Characteristics and Reasons of Long-lived Internet Flows</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM Internet Measurement Conference (IMC &apos;10)</title>
				<meeting>the 2010 ACM Internet Measurement Conference (IMC &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Inside the Social Network&apos;s (Datacenter) Network</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmeet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGCOMM Conference (SIGCOMM &apos;15)</title>
				<meeting>the 2015 ACM SIGCOMM Conference (SIGCOMM &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Comparison of Caching Strategies in Modern Cellular Backhaul Networks</title>
		<author>
			<persName><forename type="first">Shinae</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinjo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghwan</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Mobile Systems, Applications, and Services (MobiSys&apos;13)</title>
				<meeting>the 11th International Conference on Mobile Systems, Applications, and Services (MobiSys&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Design and Implementation of TCP/IP Offload Engine System over Gigabit Ethernet</title>
		<author>
			<persName><forename type="first">Zhong-Zhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han-Chiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Computer Communications and Networks</title>
				<meeting>the 15th International Conference on Computer Communications and Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">StackMap: Low-Latency Networking with the OS Stack and Dedicated NICs</title>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Yasukata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michio</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Santry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Eggert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC &apos;16)</title>
				<meeting>the 2016 USENIX Annual Technical Conference (ATC &apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Tuning the Aggressive TCP Behavior for Highly Concurrent HTTP Connections in Intra-datacenter</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianer</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geyong</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3808" to="3822" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Congestion Control for Large-Scale RDMA Deployments</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Lipshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehonatan</forename><surname>Liron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Haj Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
