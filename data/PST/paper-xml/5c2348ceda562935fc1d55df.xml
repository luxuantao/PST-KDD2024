<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cong</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab. of Machine Intelligence (MoE)</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">Junchi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cnjunchi.li.duke@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Lab. of Machine Intelligence (MoE)</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>tongzhang@tongzhang-ml.org</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C804E54A17D34C5B550D2F8EF8C4EBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new technique named Stochastic Path-Integrated Differential EstimatoR (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. We provide a few error-bound results on its convergence rates. Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of O min(n 1/2 -2 , -3 ) to find an -approximate first-order stationary point. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting. Our SPIDER technique can be further applied to find an ( , O( 0.5 ))-approximate second-order stationary point at a gradient computation cost of Õ min(n 1/2 -2 + -2.5 , -3 ) .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we study the optimization problem</p><formula xml:id="formula_0">minimize x∈R d f (x) ≡ E [F (x; ζ)]<label>(1.1)</label></formula><p>where the stochastic component F (x; ζ), indexed by some random vector ζ, is smooth and possibly non-convex. Non-convex optimization problem of form (1.1) contains many large-scale statistical learning tasks and is gaining tremendous popularity due to its favorable computational and statistical efficiency <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Typical examples of form (1.1) include principal component analysis, estimation of graphical models, as well as training deep neural networks <ref type="bibr" target="#b16">[17]</ref>. The expectation-minimization structure of stochastic optimization problem (1.1) allows us to perform iterative updates and minimize the objective using its stochastic gradient ∇F (x; ζ) as an estimator of its deterministic counterpart.</p><p>A special case of central interest is when the stochastic vector ζ is finitely sampled. In such finite-sum (or offline) case, we denote each component function as f i (x) and (1.1) can be restated as</p><formula xml:id="formula_1">minimize x∈R d f (x) = 1 n n i=1 f i (x) (1.2)</formula><p>where n is the number of individual functions. Another case is when n is reasonably large or even infinite, running across of the whole dataset is exhaustive or impossible. We refer it as the online (or streaming) case. For simplicity of notations we will study the optimization problem of form (1.2) in both finite-sum and online cases till the rest of this paper.</p><p>One important task for non-convex optimization is to search for, given the precision accuracy &gt; 0, an -approximate first-order stationary point x ∈ R d or ∇f (x) ≤ . In this paper, we aim to propose a new technique, called the Stochastic Path-Integrated Differential EstimatoR (SPIDER), which enables us to construct an estimator that tracks a deterministic quantity with significantly lower sampling costs. As the readers will see, the SPIDER technique further allows us to design an algorithm with a faster rate of convergence for non-convex problem <ref type="bibr">(1.2)</ref>, in which we utilize the idea of Normalized Gradient Descent (NGD) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>. NGD is a variant of Gradient Descent (GD) where the stepsize is picked to be inverse-proportional to the norm of the full gradient. Compared to GD, NGD exemplifies faster convergence, especially in the neighborhood of stationary points <ref type="bibr" target="#b24">[25]</ref>. However, NGD has been less popular due to its requirement of accessing the full gradient and its norm at each update. In this paper, we estimate and track the gradient and its norm via the SPIDER technique and then hybrid it with NGD. Measured by gradient cost which is the total number of computation of stochastic gradients, our proposed SPIDER-SFO algorithm achieves a faster rate of convergence in O(min(n 1/2 -2 , -3 )) which outperforms the previous best-known results in both finite-sum <ref type="bibr" target="#b2">[3]</ref>[32] and online cases <ref type="bibr" target="#b23">[24]</ref> by a factor of O(min(n 1/6 , -0.333 )).</p><p>For the task of finding stationary points for which we already achieved a faster convergence rate via our proposed SPIDER-SFO algorithm, a follow-up question to ask is: is our proposed SPIDER-SFO algorithm optimal for an appropriate class of smooth functions? In this paper, we provide an affirmative answer to this question in the finite-sum case. To be specific, inspired by a counterexample proposed by Carmon et al. <ref type="bibr" target="#b9">[10]</ref> we are able to prove that the gradient cost upper bound of SPIDER-SFO algorithm matches the algorithmic lower bound. To put it differently, the gradient cost of SPIDER-SFO cannot be further improved for finding stationary points for some particular non-convex functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>In the recent years, there has been a surge of literatures in machine learning community that analyze the convergence property of non-convex optimization algorithms. Limited by space and our knowledge, we have listed all literatures that we believe are mostly related to this work. We refer the readers to the monograph by Jain et al. <ref type="bibr" target="#b18">[19]</ref> and the references therein on recent general and model-specific convergence rate results on non-convex optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGD and Variance Reduction</head><p>For the general problem of finding approximate stationary points, under the smoothness condition of f (x), it is known that vanilla Gradient Descent (GD) and Stochastic Gradient Descent (SGD), which can be traced back to Cauchy <ref type="bibr" target="#b10">[11]</ref> and Robbins &amp; Monro <ref type="bibr" target="#b32">[33]</ref> and achieve an -approximate stationary point with a gradient cost of O(min(n -2 , -4 )) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Recently, the convergence rate of GD and SGD have been improved by the variance-reduction type of algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>. In special, the finite-sum Stochastic Variance-Reduced Gradient (SVRG) and online Stochastically Controlled Stochastic Gradient (SCSG), to the gradient cost of Õ(min(n 2/3 -2 , -3.333 )) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>First-order method for finding approximate second-order stationary points It has been shown that for machine learning methods such as deep learning, approximate stationary points that have at least one negative Hessian direction, including saddle points and local maximizers, are often not sufficient and need to be avoided or escaped from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, many literature study the problem of how to avoid or escape saddle points and achieve an ( , δ)-approximate second-order stationary point x at a polynomial gradient cost, i.e. an x ∈ R d such that ∇f (x) ≤ , λ min (∇ 2 f (x)) ≥ -δ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. Among them, the group of authors Ge et al. <ref type="bibr" target="#b14">[15]</ref>, Jin et al. <ref type="bibr" target="#b19">[20]</ref> proposed the noise-perturbed variants of Gradient Descent (PGD) and Stochastic Gradient Descent (SGD) that escape from all saddle points and achieve an -approximate secondorder stationary point in gradient cost of Õ(min(n -2 , poly(d) -<ref type="foot" target="#foot_1">4</ref> )) stochastic gradients. Levy <ref type="bibr" target="#b24">[25]</ref> proposed the noise-perturbed variant of NGD which yields faster evasion of saddle points than GD.</p><p>The breakthrough of gradient cost for finding second-order stationary points were achieved in 2016/2017, when the two recent lines of literatures, namely FastCubic <ref type="bibr" target="#b0">[1]</ref> and CDHS <ref type="bibr" target="#b7">[8]</ref> as well as their stochastic versions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>, achieve a gradient cost of Õ(min(n -1.5 +n<ref type="foot" target="#foot_0">3</ref>/4 -1.75 , -3.5 )) which serves as the best-known gradient cost for finding an ( , O( 0.5 ))-approximate second-order stationary point before the initial submission of this paper. 3 4 In particular, Agarwal et al. <ref type="bibr" target="#b0">[1]</ref>, Tripuraneni et al. <ref type="bibr" target="#b34">[35]</ref> converted the cubic regularization method for finding second-order stationary points <ref type="bibr" target="#b26">[27]</ref> to stochastic-gradient based and stochastic-Hessian-vector-product-based methods, and Allen-Zhu <ref type="bibr" target="#b1">[2]</ref>, Carmon et al. <ref type="bibr" target="#b7">[8]</ref> used a Negative-Curvature Search method to avoid saddle points. See also recent works by Reddi et al. <ref type="bibr" target="#b30">[31]</ref> for related saddle-point-escaping methods that achieve similar rates for finding an approximate second-order stationary point.</p><p>Other concurrent works As the current work is carried out in its final phase, the authors became aware that an idea of resemblance was earlier presented in an algorithm named the StochAstic Recursive grAdient algoritHm (SARAH) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Despite the fact that both our SPIDER-SFO and theirs adopt the recursive stochastic gradient update framework and our SPIDER-SFO can be viewed as a variant of SARAH with normalization, our work differ from their works in two aspects:</p><p>(i) Our analysis techniques are totally different from the version of SARAH proposed by Nguyen et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Their version can be seen as a variant of gradient descent, while ours hybrids the SPIDER technique with normalized gradient descent. Moreover, Nguyen et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> adopt a large stepsize setting (in fact their goal was to design a memory-saving variant of SAGA <ref type="bibr" target="#b12">[13]</ref>), while our SPIDER-SFO algorithm adopt a small stepsize that is proportional to . All these are essential elements of our superior achievements in convergence rates;</p><p>(ii) Our proposed SPIDER technique is a much more general variance-reduced estimation method for many quantities (not limited to gradients) and can be flexibly applied to numerous problems, e.g. stochastic zeroth-order method.</p><p>Soon after the initial submission to NIPS and arXiv release of this paper, we became aware that similar convergence rate results for stochastic first-order method were also achieved independently by the so-called SNVRG algorithm <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. SNVRG <ref type="bibr" target="#b39">[40]</ref> obtains a gradient complexity of Õ(min(n 1/2 -2 , -3 )) for finding an -approximate first-order stationary point and achieves a Õ( -3.5 ) gradient cost for finding an ( , O( 0.5 ))-approximate second-order stationary point <ref type="bibr" target="#b38">[39]</ref>. By exploiting the third-order smoothness, an SNVRG variant can also achieve an ( , O( 0.5 ))approximate second-order stationary point in Õ( -3 ) stochastic gradient costs <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Contributions</head><p>In this work, we propose the Stochastic Path-Integrated Differential Estimator (SPIDER) technique, which significantly avoids excessive access of stochastic oracles and reduces the time complexity. Such technique can be potential applied in many stochastic estimation problems.</p><p>(i) We propose the SPIDER-SFO algorithm (Algorithm 1) for finding approximate first-order stationary points for non-convex stochastic optimization problem (1.2), and prove the optimality of such rate in at least one case. Inspired by recent works Carmon et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, Johnson &amp; Zhang <ref type="bibr" target="#b21">[22]</ref> and independent of Zhou et al. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, this is the first time that the gradient cost of O(min(n 1/2 -2 , -3 )) in both upper and lower (finite-sum only) bound for finding first-order stationary points for problem (1.2) were obtained.</p><p>(ii) Following Allen-Zhu &amp; Li <ref type="bibr" target="#b3">[4]</ref>, Carmon et al. <ref type="bibr" target="#b7">[8]</ref>, Xu et al. <ref type="bibr" target="#b37">[38]</ref>, we propose SPIDER-SFO + algorithm for finding an approximate second-order stationary point for non-convex stochastic optimization problem. To best of our knowledge, this is also the first time that the gradient cost of Õ(min(n 1/2 -2 + -2.5 , -3 )) achieved with standard assumptions. We leave the details of SFO in the long version of our paper: https://arxiv.org/abs/1807.01695</p><p>(iii) We propose a new and simpler analysis framework for proving convergence to approximate stationary points. One can flexibly apply our proof techniques to analyze others algorithms, e.g. SGD, SVRG <ref type="bibr" target="#b21">[22]</ref>, and SAGA <ref type="bibr" target="#b12">[13]</ref>.</p><p>Notation. Throughout this paper, we treat the parameters L, ∆, σ, and ρ, to be specified later as global constants. Let • denote the Euclidean norm of a vector or spectral norm of a square matrix. Denote p n = O(q n ) for a sequence of vectors p n and positive scalars q n if there is a global constant C such that |p n | ≤ Cq n , and p n = Õ(q n ) such C hides a poly-logarithmic factor of the parameters. Denote 2 Stochastic Path-Integrated Differential Estimator: Core Idea</p><formula xml:id="formula_2">p n = Ω(q n ) if</formula><p>In this section, we present in detail the underlying idea of our Stochastic Path-Integrated Differential Estimator (SPIDER) technique behind the algorithm design. As the readers will see, such technique significantly avoids excessive access of stochastic oracle and reduces complexity, which is of independent interest and has potential applications in many stochastic estimation problems.</p><p>Let us consider an arbitrary deterministic vector quantity Q(x). Assume that we observe a sequence x0:K , and we want to dynamically track Q(x k ) for k = 0, 1, . . . , K. Assume further that we have an initial estimate Q(x 0 ) ≈ Q(x 0 ), and an unbiased estimate</p><formula xml:id="formula_3">ξ k (x 0:k ) of Q(x k ) -Q(x k-1 ) such that for each k = 1, . . . , K E [ξ k (x 0:k ) | x0:k ] = Q(x k ) -Q(x k-1 ).</formula><p>Then we can integrate (in the discrete sense) the stochastic differential estimate as</p><formula xml:id="formula_4">Q(x 0:K ) := Q(x 0 ) + K k=1 ξ k (x 0:k ).</formula><p>(2.1)</p><p>We call estimator Q(x 0:K ) the Stochastic Path-Integrated Differential EstimatoR, or SPIDER for brevity. We conclude the following proposition which bounds the error of our estimator Q(x 0:K ) -Q(x K ) , in terms of both expectation and high probability:</p><p>Proposition 1. The martingale variance bound has</p><formula xml:id="formula_5">E Q(x 0:K )-Q(x K ) 2 = E Q(x 0 )-Q(x 0 ) 2 + K k=1 E ξ k (x 0:k )-(Q(x k )-Q(x k-1 )) 2 . (2.2)</formula><p>Proposition 1 can be easily concluded using the property of square-integrable martingales. Now, let B map any x ∈ R d to a random estimate B i (x) such that, conditioning on the observed sequence x 0:k , we have for each k = 1, . . . , K,</p><formula xml:id="formula_6">E B i (x k ) -B i (x k-1 ) | x 0:k = V k -V k-1 .</formula><p>(2.3) At each step k let S * be a subset that samples S * elements in [n] with replacement, and let the stochastic estimator B S * = (1/S * ) i∈S * B i satisfy</p><formula xml:id="formula_7">E B i (x) -B i (y) 2 ≤ L 2 B x -y 2 ,<label>(2.4)</label></formula><p>and x kx k-1 ≤ 1 for all k = 1, . . . , K. Finally, we set our estimator V k of B(x k ) as</p><formula xml:id="formula_8">V k = B S * (x k ) -B S * (x k-1 ) + V k-1 .</formula><p>Applying Proposition 1 immediately concludes the following lemma, which gives an error bound of the estimator V k in terms of the second moment of V k -B(x k ) :</p><p>Lemma 1. We have under the condition (2.4) that for all k = 1, . . . , K,</p><formula xml:id="formula_9">E V k -B(x k ) 2 ≤ kL 2 B 2 1 S * + E V 0 -B(x 0 ) 2 . (2.5)</formula><p>It turns out that one can use SPIDER to track many quantities of interest, such as stochastic gradient, function values, zero-order estimate gradient, functionals of Hessian matrices, etc. Our proposed SPIDER-based algorithms in this paper take B i as the stochastic gradient ∇f i and the zeroth-order estimate gradient, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPIDER for Stochastic First-Order Method</head><p>In this section, we apply SPIDER to the Stochastic First-Order (SFO) method. We introduce the basic settings and assumptions in §3.1 and propose the main error-bound theorems for finding an -approximate first-order stationary point in §3.2. We conclude this section with the corresponding lower-bound result in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings and Assumptions</head><p>We first introduce the formal definition of an approximate first-order stationary point as follows.</p><p>Definition 1. We call x ∈ R d an -approximate first-order stationary point, or simply an FSP, if ∇f (x) ≤ .</p><p>(3.1)</p><p>For our purpose of analysis, we also pose the following assumption:</p><p>Assumption 1. We assume the following</p><formula xml:id="formula_10">(i) The ∆ := f (x 0 ) -f * &lt; ∞ where f * = inf x∈R d f (x) is the global infimum value of f (x);</formula><p>(ii) The component function f i (x) has an averaged L-Lipschitz gradient, i.e. for all x, y,</p><formula xml:id="formula_11">E ∇f i (x) -∇f i (y) 2 ≤ L 2 x -y 2 ;</formula><p>(iii) (For online case only) the stochastic gradient has a finite variance bounded by σ 2 &lt; ∞, i.e.</p><formula xml:id="formula_12">E ∇f i (x) -∇f (x) 2 ≤ σ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Upper Bound for Finding First-Order Stationary Points</head><p>Recall that NGD has iteration update rule</p><formula xml:id="formula_13">x k+1 = x k -η ∇f (x k ) ∇f (x k ) ,<label>(3.2)</label></formula><p>where η is a constant step size. The NGD update rule (3.2) ensures x k+1 -x k being constantly equal to the stepsize η, and might fastly escape from saddle points and converge to a second-order Algorithm 1 SPIDER-SFO: Input x 0 , q, S 1 , S 2 , n 0 , , and ˜ (For finding first-order stationary point)</p><formula xml:id="formula_14">1: for k = 0 to K do 2:</formula><p>if mod (k, q) = 0 then 3:</p><p>Draw S 1 samples (or compute the full gradient for the finite-sum case), let</p><formula xml:id="formula_15">v k = ∇f S1 (x k ) 4: else 5:</formula><p>Draw S 2 samples, and let</p><formula xml:id="formula_16">v k = ∇f S2 (x k ) -∇f S2 (x k-1 ) + v k-1 6:</formula><p>end if </p><formula xml:id="formula_17">x k+1 = x k -η • (v k / v k ) where η = Ln 0 12:</formula><p>end if</p><p>13: OPTION II for convergence rates in expectation 14: stationary point <ref type="bibr" target="#b24">[25]</ref>. We propose SPIDER-SFO in Algorithm 1, which resembles a stochastic variant of NGD with the SPIDER technique applied, so that one can maintain an estimate of ∇f (x k ) at a higher accuracy under limited gradient budgets.</p><formula xml:id="formula_18">x k+1 = x k -η k v k where η k = min Ln 0 v k ,<label>1</label></formula><p>To analyze the convergence rate of SPIDER-SFO, let us first consider the online case for Algorithm 1. We let the input parameters be</p><formula xml:id="formula_19">S 1 = 2σ 2 2 , S 2 = 2σ n 0 , η = Ln 0 , η k = min Ln 0 v k , 1 2Ln 0 , q = σn 0 ,<label>(3.3</label></formula><p>) where n 0 ∈ [1, 2σ/ ] is a free parameter to choose. <ref type="foot" target="#foot_2">5</ref> In this case, v k in Line 5 of Algorithm 1 is a SPIDER for ∇f (x k ). To see this, recall ∇f i (x k-1 ) is the stochastic gradient drawn at step k and</p><formula xml:id="formula_20">E ∇f i (x k ) -∇f i (x k-1 ) | x 0:k = ∇f (x k ) -∇f (x k-1 ).<label>(3.4)</label></formula><p>Plugging in V k = v k and B i = ∇f i in Lemma 1 of §2, we can use v k in Algorithm 1 as the SPIDER and conclude the following lemma that is pivotal to our analysis. Lemma 2. Set the parameters S 1 , S 2 , η, and q as in (3.3), and k 0 = k/q • q. Then under the Assumption 1, we have</p><formula xml:id="formula_21">E v k -∇f (x k ) 2 | x 0:k0 ≤ 2 .</formula><p>Here we compute the conditional expectation over the randomness of x (k0+1):k .</p><p>Lemma 2 shows that our SPIDER v k of ∇f (x) maintains an error of O( ). Using this lemma, we are ready to present the following results for Stochastic First-Order (SFO) method for finding first-order stationary points of (1.2). Theorem 1 (First-order stationary point, online setting, in expectation). Assume we are in the online case, let Assumption 1 holds, set the parameters S 1 , S 2 , η, and q as in (3.3), and set K = (4L∆n 0 ) -2 + 1. Then running Algorithm 1 with OPTION II for K iterations outputs a</p><p>x choose.</p><p>satisfying</p><formula xml:id="formula_22">E [ ∇f (x) ] ≤ 5 .<label>(3.5)</label></formula><p>The gradient cost is bounded by 24L∆σ • -3 + 2σ 2 -2 + 4σn -1 0 -1 for any choice of n 0 ∈ [1, 2σ/ ]. Treating ∆, L and σ as positive constants, the stochastic gradient complexity is O( -3 ).</p><p>The relatively reduced minibatch size serves as the key ingredient for the superior performance of SPIDER-SFO. For illustrations, let us compare the sampling efficiency among SGD, SCSG and SPIDER-SFO in their special cases. With some involved analysis of the algorithms above, we can conclude that to ensure per-iteration sufficient decrease of Ω( 2 /L), we have (i) for SGD the choice of mini-batch size is O σ 2 • -2 ;</p><p>(ii) for SCSG <ref type="bibr" target="#b23">[24]</ref> and Natasha2 <ref type="bibr" target="#b1">[2]</ref> the mini-batch size is O σ • -1.333 ;</p><p>(iii) for our SPIDER-SFO, only a reduced mini-batch size of O σ • -1 is needed.</p><p>Turning to the finite-sum case, analogous to the online case we let</p><formula xml:id="formula_23">S 2 = n 1/2 n 0 , η = Ln 0 , η k = min Ln 0 v k , 1 2Ln 0 , q = n 0 n 1/2 ,<label>(3.6)</label></formula><p>where</p><formula xml:id="formula_24">n 0 ∈ [1, n 1/2 ].</formula><p>In this case, one computes the full gradient v k = ∇f S1 (x k ) in Line 3 of Algorithm 1. We conclude our second upper-bound result: Theorem 2 (First-order stationary point, finite-sum setting, in expectation). Assume we are in the finite-sum case, let Assumption 1 holds, set the parameters S 2 , η k , and q as in (3.6), set K = (4L∆n 0 ) -2 + 1, and let S 1 = [n], i.e. we obtain the full gradient in Line 3. Then running Algorithm 1 with OPTION II for K iterations outputs a x satisfying</p><formula xml:id="formula_25">E ∇f (x) ≤ 5 .</formula><p>The gradient cost is bounded by n + 12(L∆) • n 1/2 -2 + 2n -1 0 n 1/2 for any choice of n 0 ∈ [1, n 1/2 ]. Treating ∆, L and σ as positive constants, the stochastic gradient complexity is O(n + n 1/2 -2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lower Bound for Finding First-Order Stationary Points</head><p>To conclude the optimality of our algorithm we need an algorithmic lower bound result <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>. Consider the finite-sum case and any random algorithm A that maps functions f : R d → R to a sequence of iterates in R d+1 , with</p><formula xml:id="formula_26">[x k ; i k ] = A k-1 ξ, ∇f i0 (x 0 ), ∇f i1 (x 1 ), . . . , ∇f i k-1 (x k-1 ) , k ≥ 1,<label>(3.7)</label></formula><p>where A k are measure mapping into R d+1 , i k is the individual function chosen by A at iteration k, and ξ is uniform random vector from [0, 1]. And [x 0 ; i 0 ] = A 0 (ξ), where A 0 is a measure mapping. The lower-bound result for solving (1.2) is stated as follows: Theorem 3 (Lower bound for SFO for the finite-sum setting). For any L &gt; 0, ∆ &gt; 0, and</p><formula xml:id="formula_27">2 ≤ n ≤ O ∆ 2 L 2 • -4 , for any algorithm A satisfying (3.7), there exists a dimension d = Õ ∆ 2 L 2 •n 2 -4</formula><p>, and a function f satisfies Assumption 1 in the finite-sum case, such that in order to find a point x for which ∇f (x) ≤ , A must cost at least Ω L∆ • n 1/2 -2 stochastic gradient accesses.</p><p>Note the condition n ≤ O( -4 ) in Theorem 3 ensures that our lower bound Ω(n 1/2 -2 ) = Ω(n + n 1/2 -2 ), and hence our upper bound in Theorem 1 matches the lower bound in Theorem 3 up to a constant factor of relevant parameters, and is hence near-optimal. Inspired by Carmon et al. <ref type="bibr" target="#b9">[10]</ref>, our proof of Theorem 3 utilizes a specific counterexample function that requires at least Ω(n 1/2 -2 ) stochastic gradient accesses. Note Carmon et al. <ref type="bibr" target="#b9">[10]</ref> analyzed such counterexample in the deterministic case n = 1 and we generalize such analysis to the finite-sum case n ≥ 1. Remark 1. Note by setting n = O( -4 ) the lower bound complexity in Theorem 3 can be as large as Ω( -4 ). We emphasize that this does not violate the O( -3 ) upper bound in the online case [Theorem 1], since the counterexample established in the lower bound depends not on the stochastic gradient variance σ 2 specified in Assumption 1(iii), but on the component number n. To obtain the lower bound result for the online case with the additional Assumption 1(iii), with more efforts one might be able to construct a second counterexample that requires Ω( -3 ) stochastic gradient accesses with the knowledge of σ instead of n. We leave this as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Further Extensions</head><p>Further extensions of our SPIDER technique can be successfully applied to reduce the complexity. Limited by space, we leave the details of the following important extensions in the long version of our paper at https://arxiv.org/abs/1807.01695 .</p><p>Upper Bound for Finding First-Order Stationary Points, in High-Probability Under more stringent assumptions on the moments of stochastic gradients, our Algorithm 1 with OPTION I achieves a gradient cost of Õ(min(n 1/2 -2 , -3 )) (note the additional polylogarithmic factor) with high probability. We detail the theorems and their proofs in the long version of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second-Order Stationary Point</head><p>To find a second-order stationary point with (3.1), we can fuse our SPIDER-SFO in Algorithm 1 (OPTION I taken) with a Negative-Curvature-Search (NC-Search) iteration. In the long version of our paper (and independent of <ref type="bibr" target="#b38">[39]</ref>), we proved rigorously that a gradient cost of Õ(min(n 1/2 -2 + -2.5 , -3 )) can be achieved under standard assumptions: Theorem 4 (Second-Order Stationary Point, Informal). There exists an algorithm such that under appropriate assumptions it takes to find a ( , √ ρ )-second-order stationary point, we have for the online case, when ≤ ρσ 2 the total number of stochastic gradient computations is Õ( -3 ); For the finite-sum case, when ≤ ρn, the total cost of gradient access is Õ(n -1.5 + n 1/2 -2 + -2.5 ).</p><p>Zeroth-Order Stationary Point After the NIPS submission of this work, we propose a second application of our SPIDER technique to the stochastic zeroth-order method for problem (1.2) and achieves individual function accesses of O(min(dn 1/2 -2 , d -3 )). To best of our knowledge, this is also the first time a complexity of individual function value accesses for non-convex problems has been improved to the aforementioned complexity using variance reduction techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Future Directions</head><p>We propose in this work the SPIDER method for non-convex optimization. Our SPIDER-type algorithms have update rules that are reasonably simple and achieve excellent convergence properties. However, there are still some important questions are left. For example, the lower bound results for finding a second-order stationary point are not complete. Specially, it is not yet clear if our Õ( -3 ) for the online case and Õ(n 1/2 -2 ) for the finite-sum case gradient cost upper bound for finding a second-order stationary point (when n ≥ Ω( -1 )) is optimal or the gradient cost can be further improved, assuming both Lipschitz gradient and Lipschitz Hessian.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2Ln 0 15 :</head><label>15</label><figDesc>end for 16: OPTION I: Return x K however, this line is not reached with high probability 17: OPTION II: Return x chosen uniformly at random from {x k } K-1 k=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>there is a global constant C such that |p n | ≥ Cq n . Let λ min (A) denote the least eigenvalue of a real symmetric matrix A. For fixed K ≥ k ≥ 0, let x k:K denote the sequence {x k , . . . , x K }. Let [n] = {1, . . . , n} and S denote the cardinality of a multi-set S ⊂ [n] of samples (a generic set that allows elements of multiple instances). For simplicity, we further denote the averaged sub-sampled stochastic estimator B S := (1/S) i∈S B i and averaged sub-sampled gradient ∇f S := (1/S) i∈S ∇f i . Other notations are explained at their first appearance.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Allen-Zhu<ref type="bibr" target="#b1">[2]</ref> also obtains a gradient cost of Õ( -3.25 ) to achieve a (modified and weakened) ( , O( 0.25 ))approximate second-order stationary point.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p><ref type="bibr" target="#b3">4</ref> Here and in many places afterwards, the gradient cost also includes the number of stochastic Hessian-vector product accesses, which has similar running time with computing per-access stochastic gradient.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>When n0 = 1, the mini-batch size is 2σ/ , which is the largest mini-batch size that Algorithm 1 allows to</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The authors would like to thank Jeffrey Z. HaoChen for his help on the numerical experiments, thank an anonymous reviewer to point out a mistake in the original proof of Theorem 1 and thank Zeyuan Allen-Zhu and Quanquan Gu for relevant discussions and pointing out references Zhou et al. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, also Jianqiao Wangni for pointing out references Nguyen et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, and Zebang Shen, Ruoyu Sun, Haishan Ye, Pan Zhou for very helpful discussions and comments. Zhouchen Lin is supported by National Basic Research Program of China (973 Program, grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (grant nos. 61625301 and 61731018), and Microsoft Research Asia.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was done while Cong Fang was a Research Intern with Tencent AI Lab. † Corresponding author. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding approximate local minima faster than gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 49th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natasha 2: Faster non-convex optimization than sgd</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variance reduction for faster non-convex optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="699" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neon2: Finding local minima via first-order oracles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex optimization: Algorithms and complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="231" to="357" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerated methods for non-convex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex Until Proven Guilty&quot;: Dimension-free acceleration of gradient descent on non-convex functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11606</idno>
		<title level="m">Lower bounds for finding stationary points i</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Méthode générale pour la résolution des systemes déquations simultanées</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cauchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus de l&apos;Academie des Science</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="536" to="538" />
			<date type="published" when="1847">1847</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Durrett</surname></persName>
		</author>
		<title level="m">Probability: Theory and Examples</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>th edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Escaping from saddle points -online stochastic gradient for tensor decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory</title>
		<meeting>The 28th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="797" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond convexity: Stochastic quasi-convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1594" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-convex optimization for machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="142" to="336" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to escape saddle points efficiently</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1724" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Accelerated gradient descent escapes saddle points faster than gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10456</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient descent only converges to minimizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 29th Conference on Learning Theory</title>
		<meeting>The 29th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1246" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-convex finite-sum optimization via scsg methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2345" to="2355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04831</idno>
		<title level="m">The power of normalization: Faster evasion of saddle points</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cubic regularization of newton method and its global performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="205" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SARAH: A novel method for machine learning problems using stochastic recursive gradient</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07261</idno>
		<title level="m">Stochastic recursive gradient algorithm for nonconvex optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Catalyst for gradientbased nonconvex optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Paquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A generic approach for escaping saddle points</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Perez-Cruz</surname></persName>
		</editor>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics<address><addrLine>Lanzarote, Canary Islands</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1233" to="1242" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<title level="m">A stochastic approximation method. The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="112" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic cubic regularization for fast nonconvex optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tripuraneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Regier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03594</idno>
		<title level="m">Lower bound for randomized first order convex optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tight complexity bounds for optimizing composite objectives</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01944</idno>
		<title level="m">First-order stochastic algorithms for escaping from saddle points in almost linear time</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08782</idno>
		<title level="m">Finding local minima via stochastic nested variance reduction</title>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07811</idno>
		<title level="m">Stochastic nested variance reduction for nonconvex optimization</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
