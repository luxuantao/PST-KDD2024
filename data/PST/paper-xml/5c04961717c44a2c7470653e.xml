<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised deep embedded clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yazhou</forename><surname>Ren</surname></persName>
							<email>yazhou.ren@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kangrong</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lili</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information Systems</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<addrLine>81 Victoria Street</addrLine>
									<postCode>188065</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">SMILE Lab</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="middle">Zidong</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Semi-supervised deep embedded clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C89E328BD81DC8F6A824DCB124C8632F</idno>
					<idno type="DOI">10.1016/j.neucom.2018.10.016</idno>
					<note type="submission">Received 2 July 2018 Revised 3 October 2018 Accepted 13 October 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semi-supervised learning Deep embedded clustering Pairwise constraints</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering is an important topic in machine learning and data mining. Recently, deep clustering, which learns feature representations for clustering tasks using deep neural networks, has attracted increasing attention for various clustering applications. Deep embedded clustering (DEC) is one of the state-of-theart deep clustering methods. However, DEC does not make use of prior knowledge to guide the learning process. In this paper, we propose a new scheme of semi-supervised deep embedded clustering (SDEC) to overcome this limitation. Concretely, SDEC learns feature representations that favor the clustering tasks and performs clustering assignments simultaneously. In contrast to DEC, SDEC incorporates pairwise constraints in the feature learning process such that data samples belonging to the same cluster are close to each other and data samples belonging to different clusters are far away from each other in the learned feature space. Extensive experiments on real benchmark data sets validate the effectiveness and robustness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is one of very extensively studied topics in artificial intelligence and enjoys a wide range of applications, ranging from document analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> , regional science <ref type="bibr" target="#b2">[3]</ref> , image retrieval <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> , annotation <ref type="bibr" target="#b6">[7]</ref> , segmentation <ref type="bibr" target="#b7">[8]</ref> , to network analysis <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> . In the past few decades, many clustering algorithms have been proposed, including k -means <ref type="bibr" target="#b11">[12]</ref> , hierarchical clustering <ref type="bibr" target="#b12">[13]</ref> , DBSCAN <ref type="bibr" target="#b13">[14]</ref> , Gaussian mixture model <ref type="bibr" target="#b14">[15]</ref> , non-negative matrix factorization based clustering methods <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> , mean shift clustering <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> , consensus clustering <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> , graph-based clustering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> , and so on. Despite being studied extensively, the performance of traditional clustering methods generally deteriorates with high dimensional data due to unreliable similarity metrics, a phenomenon known as the curse of dimensionality.</p><p>To mitigate the curse of dimensionality, a common way is to transform data from a high dimensional feature space to a lower one by applying dimension reduction techniques like principle component analysis (PCA) or feature selection methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> . Then, clustering is performed in the lower dimensional feature space. However, this scheme ignores the interconnection between features learning and clustering. To address this issue, the work <ref type="bibr" target="#b30">[31]</ref> proposes to perform clustering and feature learning simultaneously by integrating k -means and linear discriminant analysis (LDA) into a joint framework. Nevertheless, the representation ability of features learned by these shallow models is limited <ref type="bibr" target="#b31">[32]</ref> .</p><p>In recent years, deep neural networks (DNN) that own better representation ability have been broadly applied in many machine learning tasks <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> . Lately, some work has been done to successfully apply deep neural networks in clustering tasks <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref> . The resulting model is called deep clustering . Peng et al. <ref type="bibr" target="#b37">[38]</ref> and Tian et al. <ref type="bibr" target="#b38">[39]</ref> divide the deep clustering into two phases, i.e., feature transformation using DNN and clustering. In contrast, feature mapping and clustering are jointly learned in <ref type="bibr" target="#b39">[40]</ref> . Xie et al. <ref type="bibr" target="#b39">[40]</ref> propose deep embedded clustering (DEC) to learn a mapping from the high original feature space to a lower-dimensional one in which an effective objective is optimized. Yang et al. <ref type="bibr" target="#b40">[41]</ref> and Chang et al. <ref type="bibr" target="#b41">[42]</ref> make use of deep convolutional neural network (CNN) for image data clustering.</p><p>Traditional clustering methods refer to unsupervised settings. But, in many real machine learning and computer vision tasks, we know some prior knowledge such as pairwise constraints a-prior. There are typically two kinds of pairwise constraints: must-link constraints and cannot-link constraints. Must-link constraints specify that two instances are known to be in the same cluster in advance, while cannot-link constraints indicates the corresponding two instances belong to different clusters. Semi-supervised learning can use these constraints to improve the learning ability and has produced a huge impact over various machine learning applications <ref type="bibr" target="#b42">[43]</ref> . Lately, a number of semi-supervised clustering (SSC) methods which take advantage of pairwise constraints have been developed <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b47">[47]</ref> . Obviously, despite its success in clustering, DEC is not able to make use of such prior information to guide the clustering process and to further enhance the clustering performance.</p><p>To address this issue, we propose semi-supervised deep embedded clustering (SDEC) that incorporates semi-supervised information in DEC to further improve its effectiveness. By integrating pairwise constraints, SDEC considerably improves the quality of clustering results over DEC. Specifically, SDEC makes use of pairwise constraints in the feature learning process such that data samples from the same cluster are enforced to be close to each other and data samples from different clusters are enforced to be far away from each other in the learned feature space where the final cluster assignment is conducted.</p><p>The contributions of this paper are summarized below:</p><p>• We propose a new semi-supervised clustering scheme SDEC which simultaneously learns feature transformation and cluster assignment jointly by integrating with pairwise constraints.</p><p>A joint objective considering both the unlabeled data and prior information is developed. • By leveraging the prior knowledge of pairwise constraints, SDEC significantly improves the clustering performance of the stateof-the-art DEC. The proposed method is also robust to the choice of parameters. • SDEC can address the curse of dimensionality and is effective in clustering high-dimensional data. Experimental results on real image and document data sets demonstrate its effectiveness and robustness.</p><p>The rest of this paper is organized as follows: Section 2 reviews related work and Section 3 introduces the proposed method. Sections 4 and 5 present the experimental settings and the empirical results, respectively. Conclusion and future work are provided in Section 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep clustering</head><p>Deep clustering is a new category of clustering that has arisen in recent years. Inspired by the similarity between eigendecomposition in spectral methods and auto-encoder <ref type="bibr" target="#b48">[48]</ref> in learning lower-dimensional representation, Tian et al. <ref type="bibr" target="#b38">[39]</ref> were the first to introduce deep neural network in the field of clustering, which simply combines a nonlinear embedding of the original graph and k -means algorithm in the embedding space. Chen <ref type="bibr" target="#b49">[49]</ref> learns representation using a deep belief network and then runs nonparametric maximum margin clustering in the feature space. Shao et al. <ref type="bibr" target="#b50">[50]</ref> proposed a linear coder which can be stacked and layerwise trained to learn feature representation for graph clustering. Peng et al. <ref type="bibr" target="#b37">[38]</ref> incorporate structure prior in the representation learning via auto-encoder so that local and global subspace structure can be obtained. Then k -means is applied in the learned space to get the final clustering result. Law et al. <ref type="bibr" target="#b51">[51]</ref> proposed a deep supervised clustering metric learning method to learn data representation, given the ground-truth partition.</p><p>These algorithms mentioned above show commonplace in a two-stage procedure. They first learn representations in a low dimensional feature space, and then run clustering algorithm on the embedding space. In contract, Song et al. <ref type="bibr" target="#b52">[52]</ref> embedded an objective of clustering into the auto-encoder model such that the data representations in the embedding space are close to their corresponding cluster centers. Xie et al. <ref type="bibr" target="#b39">[40]</ref> proposed a framework called deep embedded clustering (DEC) which jointly learns feature representation and cluster assignments. Guo et al. <ref type="bibr" target="#b31">[32]</ref> improved the DEC framework and proposed improved deep embedded clustering (IDEC) with local structure preservation. Since DEC is unsupervised, it cannot use prior information to guide the learning process. In this paper, we develop a semisupervised version of DEC to alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-supervised clustering</head><p>Semi-supervised learning is a learning category falls between unsupervised learning and supervised learning, which utilizes both labeled and unlabeled data. There are generally three types of semi-supervised learning: semi-supervised classification <ref type="bibr" target="#b53">[53]</ref><ref type="bibr" target="#b54">[54]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref> , semi-supervised dimension reduction <ref type="bibr" target="#b57">[57]</ref><ref type="bibr" target="#b58">[58]</ref><ref type="bibr" target="#b59">[59]</ref> , and semi-supervised clustering. In semi-supervised clustering, pairwise must-link and cannot-link constraints are often used <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b61">61]</ref> . The constraints specify the relation of two data points in the data sets. <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref> are two variants of k -means that incorporate pairwise constraints. Basu and Mooney <ref type="bibr" target="#b44">[45]</ref> developed a hierarchical density based clustering algorithm under the semi-supervised setting. C-DBSCAN <ref type="bibr" target="#b64">[64]</ref> extends DBSCAN in semi-supervised scenario, handling the situation when the clusters are diffuse, partially overlapping, connected by bridges or having very different densities. Yu et al. utilized the semi-supervised information in ensemble clustering techniques to further improve their performance <ref type="bibr" target="#b65">[65]</ref><ref type="bibr" target="#b66">[66]</ref><ref type="bibr" target="#b67">[67]</ref><ref type="bibr" target="#b68">[68]</ref> .</p><p>Typical semi-supervised clustering methods work in the original feature space with worse representation ability. It is reasonable to do semi-supervised clustering with DNN to make SSC more powerful. Chen <ref type="bibr" target="#b69">[69]</ref> extended semi-supervised clustering to deep feature learning, which performs semi-supervised maximum margin clustering on the learned features of DNN and iteratively updates parameters according to most violate constraints, proving that semi-supervised information do improve the deep representation for clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-supervised deep embedded clustering</head><p>This section elucidates the proposed semi-supervised deep embedded clustering (SDEC) with pairwise constraints. Consider a data set X of n unlabeled samples</p><formula xml:id="formula_0">{ x i ∈ R d } n i =1</formula><p>where d is the dimension. The set of initial must-link constraints is denoted by ML = { (x i , x j ) : x i and x j belong to the same cluster } and the set of cannot-link constraints is CL = { (x i , x j ) : x i and x j belong to different clusters } , 1 ≤ i, j ≤ N . The number of cluster K is chosen according to prior knowledge, each cluster is represented by a center μ j , j = 1 , . . . , K. We seek to find a nonlinear transformation f θ : X → Z that maps the data from high-dimensional original space X to latent feature space Z . Here, θ represents the model parameters. The learned f θ is expected to favor the clustering task and semi-supervised information. Our final goal is to obtain an appropriate partition of data in feature space Z by utilizing the unlabeled data and the user-specified pairwise constraints.</p><p>In general, our proposed SDEC has two key steps, i.e., parameter initialization via stacked auto-encoder (SAE) <ref type="bibr" target="#b48">[48]</ref> and clustering with pairwise constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parameter initialization</head><p>We choose deep neural networks (DNN) to initialize the nonlinear transformation f θ due to its better representation ability. Concretely, we initialize the DNN structure with SAE, the same as what DEC <ref type="bibr" target="#b39">[40]</ref> does. Each layer of the network is a denoising auto-encoder <ref type="bibr" target="#b70">[70]</ref> trained to reconstruct the previous layer's output after random corruption. After training we concatenate all the encoder and decoder layers together to form a deep auto-encoder <ref type="bibr" target="#b48">[48]</ref> . Please refer to <ref type="bibr" target="#b39">[40]</ref> for more details.</p><p>Please cite this article as: Y. The encoder layers are exactly what we need as the initial mapping f θ between the original feature space and the latent learned space. The embedded data points { z i ∈ Z} n i =1 in the learned space Z are valid feature representations for the original input data samples. We then employ k -means clustering on them to obtain K initial centers { μ j } K j=1 in space Z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Clustering with pairwise constraints</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Minimization with KL divergence</head><p>DEC <ref type="bibr" target="#b39">[40]</ref> makes use of the student's t -distribution <ref type="bibr" target="#b71">[71]</ref> to measure the similarity between embedded point z i and center μ j as :</p><formula xml:id="formula_1">q i j = (1 + z i -μ j 2 ) -1 j (1 + z i -μ j 2 ) -1 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where</p><formula xml:id="formula_3">z i = f θ (x i ) ∈ Z corresponds to x i ∈ X after embedding, μ j</formula><p>is the center of the j th cluster in the embedded space, and • denotes L2-norm. q ij is considered as the probability of assigning data point i to cluster j , and q i = [ q i 1 , q i 2 , . . . , q iK ] T is considered as a soft assignment of data point i .</p><p>The DEC model iteratively refines the cluster assignments by learning from their high confidence assignments. In each step, DEC matches the soft assignment Q to an auxiliary target distribution P , which is computed as:</p><formula xml:id="formula_4">p i j = q 2 i j / f j j q 2 i j / f j , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where f j = i q i j . In DEC <ref type="bibr" target="#b39">[40]</ref> , KL divergence between the soft assignment Q and the target distribution</p><formula xml:id="formula_6">P ( KL (P Q ) = i j p i j log p i j q i j</formula><p>) is minimized to refine the nonlinear transformation f θ , i.e., the deep neural network structure initialized by the encoder layers of SAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Minimization with pairwise constraints</head><p>The main contribution of DEC is the use of KL divergence. It uses data points with high confidence as supervision and makes points in each cluster distribute more densely. However, DEC cannot make use of user-specified pairwise constraints to guide the clustering procedure. To address this, we consider adding pairwise constraints to the objective of DEC to lead the direction of clustering and embedding.</p><p>Firstly, we define a matrix to describe pairwise constraints ML and CL as 1 :</p><formula xml:id="formula_7">A = ⎡ ⎢ ⎢ ⎣ a 11 a 12 • • • a n 1 a 12 a 22 • • • a n 2 . . . . . . . . . . . . a 1 n a 2 n • • • a nn ⎤ ⎥ ⎥ ⎦ . (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>For must-link constraints, when x i and x k are assigned to the same cluster, a ik = 1 . If x i and x k satisfy cannot-link constraints, a ik = -1 . Other entities in this matrix are all zero. The pairwise constraints specify whether a pair of data examples belong to the same class (must-link constraints) or different classes (cannot-link constraints). We expect that points with the same label should be closer to each other, while points from different classes are far away from each other in latent feature space. To this end, we define the objective of SDEC as:</p><formula xml:id="formula_9">L = KL (P Q ) + λ 1 n n i =1 n k =1 a ik z i -z k 2 1</formula><p>The n × n matrix A is extremely sparse and is stored as a sparse matrix to save space in implementation.</p><formula xml:id="formula_10">= n i =1 K j=1 p i j log p i j q i j L u + λ 1 n n i =1 n k =1 a ik z i -z k 2 L s , (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>where n is the number of data points and λ is a trade-off parameter which is defined by the user. When λ = 0 , SDEC degenerates into DEC. Actually, minimizing Eq. ( <ref type="formula" target="#formula_10">4</ref>) can minimize the costs of violated constraints, thus being able to simultaneously learn feature representations and perform clustering assignments to favor the user-specified constraints. As Eq. ( <ref type="formula" target="#formula_10">4</ref>) shows, the overall loss function of the proposed SDEC can be divided into two parts, the unsupervised clustering loss L u and the semi-supervised constraint loss L s . L u is the KL divergence loss between the soft assignments q i and the auxiliary distribution p i . L u can learn the latent representations of original data that favor clustering tasks. The semi-supervised loss L s denotes the consistency between the learned representation { z i } n i =1 with the prior information A . Intuitively, if two points satisfy ( x i , x k ) ∈ ML , then a ik = 1 . To minimize Eq. ( <ref type="formula" target="#formula_10">4</ref>) , the distance between z i and z k will be small in the latent space Z . Similarly, if ( x i , x k ) ∈ CL , the distance z iz k 2 will be large in space Z . As a consequence, SDEC not only learns good representation for clustering, but also makes points from the same class more close and points from different classes separate from each other. In this way, those points in between-cluster areas can be pulled more correctly and the inappropriate cluster assignments can be somehow corrected with the use of prior information. The framework of SDEC is shown in Fig. <ref type="figure">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Optimization</head><p>We use the stochastic gradient descent (SGD) and backpropagation to optimize Eq. ( <ref type="formula" target="#formula_10">4</ref>) . It can be checked that the gradient of objective L w.r.t. feature-space embedding of each data point z i can be computed as:</p><formula xml:id="formula_12">∂L ∂z i = 2 K j=1 (1 + z i -μ j 2 ) -1 × (p i j -q i j )(z i -μ j ) + 2 λ n n k =1 a ik (z i -z k ) .</formula><p>(</p><formula xml:id="formula_13">)<label>5</label></formula><p>The gradient of L w.r.t. each cluster center μ j in space Z is calculated by:</p><formula xml:id="formula_14">∂L ∂μ j = -2 n i =1 (1 + z i -μ j 2 ) -1 × (p i j -q i j )(z i -μ j )<label>(6)</label></formula><p>The proofs of Eqs. ( <ref type="formula" target="#formula_13">5</ref>) and ( <ref type="formula" target="#formula_14">6</ref>) are given in Theorems 1 and 2 , respectively. During backpropagation, the gradients ∂ L / ∂ z i are passed down to update the DNN's parameter θ . The gradients ∂ L / ∂ μ i are used to update the clustering centers { μ j } K j=1 via SGD. We stop the algorithm if less than tol % of points change their cluster assignments between two consecutive updates or the maximal number of iterations is reached. Then, the final clustering result is obtained.</p><p>The detailed procedure is summarized in Algorithm 1 . As Algorithm 1 shows, SDEC updates the soft assignments every T iterations. As in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref> , T is considered as batch size and is always set to 256 throughout the experiments. When updating the clustering assignments (Lines 10 and 11 of Algorithm 1 ), the i th point is assigned to cluster j with the highest q ij value, i.e., y i ← arg max j q i j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Algorithm analysis</head><p>Theorem 1. The gradient of objective L w.r.t. z i is computed by Eq. ( <ref type="formula" target="#formula_13">5</ref>) .</p><p>Please cite this article as: Y. <ref type="bibr">Ren</ref>   Choose a batch of samples S ⊂ X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if iter% T == 0 then 7:</p><formula xml:id="formula_15">z i ← f θ (x i ) , ∀ x i ∈ X. 8:</formula><p>Compute all q i j values according to Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Compute all p i j values according to Eq. (2). Save old assignments: y old i ← y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Update label assignments: y i ← arg max j q i j . Update θ and { μ j } K j=1 via Eqs. ( <ref type="formula" target="#formula_13">5</ref>) and (6). 17: end for Proof. Objective L can be rewritten as:</p><formula xml:id="formula_16">L = n i =1 K j=1 p i j log p i j q i j + λ 1 n n i =1 n k =1 a ik z i -z k 2 , = n i =1 K j=1</formula><p>(p i j log p i j -p i j log q i j ) + λ</p><formula xml:id="formula_17">1 n n i =1 n k =1 a ik z i -z k 2 . (<label>7</label></formula><formula xml:id="formula_18">)</formula><p>Its gradient w.r.t. z i is:</p><formula xml:id="formula_19">∂L ∂z i = n i =1 K j=1</formula><p>(p i j log p i j -p i j log q i j ) + λ</p><formula xml:id="formula_20">1 n n i =1 n k =1 a ik z i -z k 2 , = - K j=1 ∂ (p i j log q i j ) ∂z i + 2 λ n n k =1 a ik (z i -z k ) , = - K j=1 p i j ∂ ( log q i j ) ∂z i + 2 λ n n k =1 a ik (z i -z k ) . (<label>8</label></formula><formula xml:id="formula_21">)</formula><p>When updating z i , p ij is already computed and is considered as a constant number. Thus, Eq. ( <ref type="formula" target="#formula_20">8</ref>) holds. We then compute:</p><formula xml:id="formula_22">∂ ( log q i j ) ∂z i = ∂ log (1+ z i -μ j 2 ) -1 j (1+ z i -μ j 2 ) -1 ∂z i , = ∂ ( log (1 + z i -μ j 2 ) -1 ) ∂z i - ∂ ( log j (1 + z i -μ j 2 ) -1 ) ∂z i , = - 2(z i -μ j ) 1 + z i -μ j 2 +2 j (z i -μ j )(1 + z i -μ j 2 ) -2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: NEUCOM [m5G; <ref type="bibr">October 26, 2018;</ref><ref type="bibr">13:39 ]</ref> × 1</p><formula xml:id="formula_23">j (1 + z i -μ j 2 ) -1 . (<label>9</label></formula><formula xml:id="formula_24">)</formula><p>Let S = j (1 + z i -μ j<ref type="foot" target="#foot_1">2</ref> ) -1 , then q i j S = (1 + z i -μ j 2 ) -1 . We can obtain:</p><formula xml:id="formula_25">∂ ( log q i j ) ∂z i = -2(z i -μ j )(1 + z i -μ j 2 ) -2 1 q i j S +2 j (z i -μ j )(1 + z i -μ j 2 ) -2 1 S . (<label>10</label></formula><formula xml:id="formula_26">)</formula><p>Substituting Eq. ( <ref type="formula" target="#formula_25">10</ref>) into Eq. ( <ref type="formula" target="#formula_20">8</ref>) , we have:</p><formula xml:id="formula_27">∂L ∂z i = 2 K j=1 p i j (z i -μ j )(1 + z i -μ j 2 ) -2 1 q i j S - j (z i -μ j )(1 + z i -μ j 2 ) -2 1 S + 2 λ n n k =1 a ik (z i -z k ) , = 2 K j=1 p i j (z i -μ j )(1 + z i -μ j 2 ) -2 1 q i j S -2 j (z i -μ j )(1 + z i -μ j 2 ) -2 1 S + 2 λ n n k =1 a ik (z i -z k ) , = 2 K j=1 p i j (z i -μ j )(1 + z i -μ j 2 ) -2 1 q i j S -2 j (z i -μ j )(1 + z i -μ j 2 ) -2 q i j q i j S + 2 λ n n k =1 a ik (z i -z k ) , = 2 K j=1 p i j (z i -μ j )(1 + z i -μ j 2 ) -2 1 q i j S -2 j (z i -μ j )(1 + z i -μ j 2 ) -2 q i j q i j S + 2 λ n n k =1 a ik (z i -z k ) , = 2 K j=1 p i j (z i -μ j )(1 + z i -μ j 2 ) -1 -2 K j=1 q i j (z i -μ j )(1 + z i -μ j 2 ) -1 + 2 λ n n k =1 a ik (z i -z k ) , = 2 K j=1 (p i j -q i j )(z i -μ j )(1 + z i -μ j 2 ) -1 + 2 λ n n k =1 a ik (z i -z k ) . (<label>11</label></formula><formula xml:id="formula_28">)</formula><p>The second ' = ' of Eq. ( <ref type="formula" target="#formula_27">11</ref>) holds is because that K j=1 p i j = 1 .</p><p>Theorem 2. The gradient of objective L w.r.t. μ j is computed by Eq. ( <ref type="formula" target="#formula_14">6</ref>) .</p><p>Proof. Similarly with Theorem 1 , it is not hard to prove that ∂L ∂μ j can be computed by Eq. ( <ref type="formula" target="#formula_14">6</ref>) . From another view, this theorem can be proven by the observation of <ref type="bibr" target="#b39">[40]</ref> . In <ref type="bibr" target="#b39">[40]</ref> , it is given that:</p><formula xml:id="formula_29">∂L u ∂μ j = -2 n i =1 (1 + z i -μ j 2 ) -1 × (p i j -q i j )(z i -μ j ) . (<label>12</label></formula><formula xml:id="formula_30">)</formula><p>Since L s is independent with μ j , ∂L </p><formula xml:id="formula_31">) = O (n c ) hold.</formula><p>Thus, the complexity of SDEC is O ( nD 2 ), which is linear to the data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>In this section, we first introduce the data sets used in our experiments. Then we describe the implementation in detail, including experiment setup of our algorithm, comparing methods and evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data sets</head><p>We evaluate the proposed method on several popular data sets:</p><p>• USPS: The USPS data set 2 contains 9298 grayscale images, obtained from the scanning of handwritten digits from envelopes by the U.S. postal service. • STL-10: The STL-10 data set<ref type="foot" target="#foot_2">3</ref> contains 13,0 0 0 color images with the size of 96-by-96. The images are categorized into 10 classes.</p><p>As in <ref type="bibr" target="#b39">[40]</ref> , we also use the concatenation of HOG feature and a 8-by-8 color map as input.</p><p>• CIFAR-10: The CIFAR-10 data set <ref type="foot" target="#foot_3">4</ref> is consisted of 60,0 0 0 images labeled as 10 classes. Each class contains 60 0 0 samples. We concatenate HOG feature and 8-by-8 color map to represent each picture, as same as STL-10. • MNIST: The MNIST data set<ref type="foot" target="#foot_4">5</ref> consists of 70,0 0 0 handwritten digits of 28 × 28 pixel size. We treat each gray image as a 784 dimensional vector. Each dimension is centered and normalized. • 20NG: 20NG is a subset of the 20 Newsgroups<ref type="foot" target="#foot_5">6</ref> , which is a popular data base for document analysis. 20NG contains 3 subcategories of 20-Newsgroup, i.e, comp.graphics, rec.autos, and sci.crypt.</p><p>We preprocess all the data sets in the same way as DEC <ref type="bibr" target="#b39">[40]</ref> and IDEC <ref type="bibr" target="#b31">[32]</ref> . Concretely, we normalize all data sets such that 1 d x i 2 2 ≈ 1 , for each example x i with the dimension d . The summary of the data sets and image samples are shown in Table <ref type="table" target="#tab_3">1</ref> and Fig. <ref type="figure" target="#fig_3">2</ref> , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment settings</head><p>As in DEC <ref type="bibr" target="#b39">[40]</ref> and IDEC <ref type="bibr" target="#b31">[32]</ref> , the structure of encoder layers of SAE is set to d -50 0-50 0-20 0 0-10 for all data sets, where d is  the dimension of input data. All layers are fully connected and all internal layers, except input, output and embedding layer, are activated by ReLU nonlinearity function. We pretrain and fine-tune the auto-encoder using the same parameter setting as DEC, to minimize the influence of parameter tuning.</p><p>For each data set, the pairwise constraint matrix A is generated randomly according to ground truth. We randomly select pairs of data points from the data sets: if two data points share the same label, we generate a must-link constraint. Otherwise, a cannot-link constraint is generated. The learning rate of SGD is 0.01. The convergence threshold tol % is set to 0.1%. For all algorithms, we set the number of clusters K to the number of ground truth categories. We independently run each algorithm 10 times and report the average results. t -test is used to assess the statistical significance of the results at 5% significance level.</p><p>To evaluate the effectiveness of our proposed algorithm (SDEC), we compare it with several benchmark algorithms. We first compare our algorithm with DEC <ref type="bibr" target="#b39">[40]</ref> and IDEC <ref type="bibr" target="#b31">[32]</ref> . The classic k -means algorithm <ref type="bibr" target="#b11">[12]</ref> is applied in both the original and embedding feature spaces. Consider that SDEC is a semi-supervised clustering algorithm, we also perform k -means with the supervision of pairwise constraints <ref type="bibr" target="#b62">[62]</ref> . The details of the comparing clustering methods are given in the following:</p><p>• k -means: Run k -means <ref type="bibr" target="#b11">[12]</ref> algorithm in the original feature space. • KM-cst (pairwise constrained k -means): k -means algorithm with pairwise constraints <ref type="bibr" target="#b62">[62]</ref> is applied in the original feature space. • AE+KM: Run k -means <ref type="bibr" target="#b11">[12]</ref> algorithm in the latent feature space Z obtained from SAE. The SAE is pretrained and fine-tuned following the same setting with our method. • AE+KM-cst: Apply pairwise constrained k -means <ref type="bibr" target="#b62">[62]</ref> in the latent space Z learned by SAE. • DEC: We use the authors' released code of DEC, with all the parameter settings the same as in <ref type="bibr" target="#b39">[40]</ref> . • IDEC: IDEC <ref type="bibr" target="#b31">[32]</ref>   the hyper-parameter is set to the same value that is reported in <ref type="bibr" target="#b31">[32]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation metric</head><p>To assess the performance of the comparing algorithms, we adopt clustering accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI) for evaluation. The values of NMI and ACC are both in [0,1], while the ARI values range in [ -1,1]. For the three metrics, the higher the values are, the better the clustering results are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on real data</head><p>When applying semi-supervised methods on each data set, the number of total pairwise constraints is set to n (the number of data points). The parameter λ of SDEC is set to 10 -5 . Tables <ref type="table" target="#tab_5">2</ref><ref type="table">3</ref><ref type="table" target="#tab_6">4</ref>show the clustering results measured by ACC, NMI, and ARI, respectively. In each row, the best and comparable results are high-   Several interesting observations can be obtained from these three tables: (1) As the tables show, the clustering performance of k -means (AE+KM) in the learned space is much better than k -means in the original data space, indicating that the great nonlinear representation power of deep neural network do favor the clustering tasks. (2) Three algorithms based on deep embedded clustering framework (i.e., DEC, IDEC, and SDEC), which jointly learns feature representation and cluster assignments, outperform AE+ k -means (AE+KM), which means iteratively updating the feature learning according to the clustering assignments learns better feature representations for clustering. (3) KM-cst generally performs better than k -means both in the original space and in the embedding space. This shows incorporating pairwise information do improve clustering performance. (4) The proposed SDEC achieves the best performance and outperforms unsupervised deep embedded clustering algorithm DEC and IDEC. Specifically, SDEC improves upon DEC by a large margin on 20NG with ACC and ARI. This shows the usage of little prior knowledge like pairwise constraints can significantly enhance the clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sensitivity analysis</head><p>In this section we test the sensitivity of SDEC w.r.t. the parameters λ and the number of pairwise constraints on the four image data sets. We first analyze the sensitivity of the parameter λ of SDEC with setting the number of pairwise constraints to n (the size of data sets). Since the difference between the two parts of SDEC's objective is huge (e.g., in an independent run of applying SDEC on MNIST, the value of KL divergence loss L u in Eq. ( <ref type="formula" target="#formula_10">4</ref>) is 1 . 3 × 10 -2 , while that of loss L s is -6 . 7 × 10 3 ), λ should be set to a quite small value. The test range of λ is [ 10 -8 , 10 -4 ] and Fig. <ref type="figure" target="#fig_4">3</ref> gives the results. As showed in Fig. <ref type="figure" target="#fig_4">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: NEUCOM [m5G; <ref type="bibr">October 26, 2018;</ref><ref type="bibr">13:39 ]</ref> when λ is relatively big (e.g., λ = 10 -4 ). The main reason is that the semi-supervised loss L s dominates in this case.</p><p>Then, we set λ = 10 -5 and test the sensitivity of the number of pairwise constraints n c , which ranges from 0 to 2 × n . Fig. <ref type="figure" target="#fig_5">4</ref> gives the results. It can be seen that with the increasing of n c , the performance of SDEC generally improves in the beginning. Then, SDEC performs stably in a wide range of n c . This shows that the initial introduction of pairwise constraints into deep embedded clustering will lead to a significant increase of performance, and then the performance becomes stable which means enough prior information has been captured. This observation is generally consistent with semi-supervised learning literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>In this paper, we propose a semi-supervised deep embedded clustering (SDEC) model. SDEC incorporates pairwise constraints to guide the process of feature learning, ensuring that must-link examples are close and cannot-link examples are distinct in the learned feature space. Both KL divergence loss and semisupervised loss are jointly optimized in the semi-supervised deep clustering framework to gain the deep representation for clustering. Extensive experiments on real image and document data sets demonstrate the effectiveness and robustness of SDEC. An interesting future direction is to exploit manifold constraints into deep embedded clustering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Algorithm 1 1 : 3 :</head><label>1113</label><figDesc>Fig. 1. The framework of SDEC. The upper part of the figure shows the parameter initialization of the algorithm. We use the encoder layers of a pretrained SAE to initialize the DNN structure. The box with solid line represents the learning process of SDEC. Pairwise constraints are added to the embedding layer Z to direct learning of feature representation. q denotes the soft assignment of each data point and is used to compute the KL divergence loss. SDEC takes advantage of both semi-supervised loss and KL divergence loss to update the parameters of DNN.</figDesc><graphic coords="4,135.03,57.00,316.32,310.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>Complexity analysis. The computational complexity of SDEC algorithm is O (nD 2 + nd e K + n c d e ) , where d e , n c , and D are the dimension of embedding space, the number of total pairwise constraints, and the maximum number of neurons in hidden layers of DNN, respectively. In general, K &lt; d e &lt; D and O (n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Image data sets. For each data set, each row represents one class and randomly shows ten image samples from this class.</figDesc><graphic coords="6,113.03,57.22,360.00,409.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sensitivity analysis of parameter λ of SDEC (ACC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sensitivity analysis of the number of pairwise constraints (ACC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig.3gives the results. As showed in Fig.3, SDEC performs stably in a wide range of λ. The performance of SDEC decreases sharply Please cite this article as: Y. Ren et al., Semi-supervised deep embedded clustering, Neurocomputing (2018), https://doi.org/10.1016/j.neucom.2018.10.016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Data sets used in the experiments.</figDesc><table><row><cell>Data set</cell><cell># Examples</cell><cell># Classes</cell><cell># Features</cell></row><row><cell>USPS</cell><cell>9298</cell><cell>10</cell><cell>256</cell></row><row><cell>STL-10</cell><cell>13,0 0 0</cell><cell>10</cell><cell>1428</cell></row><row><cell>CIFAR-10</cell><cell>60,0 0 0</cell><cell>10</cell><cell>180</cell></row><row><cell>MNIST</cell><cell>70,0 0 0</cell><cell>10</cell><cell>784</cell></row><row><cell>20NG</cell><cell>2965</cell><cell>3</cell><cell>7270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>is an improved version of DEC with local structure preservation. We set the parameters the same as DEC and Please cite this article as: Y. Ren et al., Semi-supervised deep embedded clustering, Neurocomputing (2018), https://doi.org/10.1016/j.neucom.2018.10.016</figDesc><table><row><cell>JID: NEUCOM</cell><cell>ARTICLE IN PRESS</cell><cell>[m5G; October 26, 2018;13:39 ]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>Clustering results measured by ACC (%).</figDesc><table><row><cell>Data</cell><cell cols="5">k -means KM-cst AE + KM AE + KM-cst DEC</cell><cell>IDEC</cell><cell>SDEC</cell></row><row><cell>USPS</cell><cell>65.67</cell><cell>68.18</cell><cell>70.28</cell><cell>71.87</cell><cell cols="3">75.81 75.86 76.39</cell></row><row><cell>STL-10</cell><cell>28.31</cell><cell>29.09</cell><cell>34.00</cell><cell>35.15</cell><cell>37.40</cell><cell cols="2">36.99 38.86</cell></row><row><cell cols="2">CIFAR-10 23.75</cell><cell>23.91</cell><cell>23.89</cell><cell>24.36</cell><cell cols="3">26.26 25.02 27.26</cell></row><row><cell>MNIST</cell><cell>52.98</cell><cell>54.27</cell><cell>74.09</cell><cell>75.98</cell><cell cols="3">84.94 83.85 86.11</cell></row><row><cell>20NG</cell><cell>33.77</cell><cell>33.89</cell><cell>40.81</cell><cell>47.71</cell><cell>50.11</cell><cell cols="2">53.63 78.12</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Clustering results measured by NMI (%).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell cols="5">k -means KM-cst AE + KM AE + KM-cst DEC</cell><cell>IDEC</cell><cell>SDEC</cell></row><row><cell>USPS</cell><cell>62.00</cell><cell>63.94</cell><cell>66.38</cell><cell>67.29</cell><cell cols="3">76.91 77.68 77.68</cell></row><row><cell>STL-10</cell><cell>24.40</cell><cell>24.79</cell><cell>29.37</cell><cell>29.75</cell><cell cols="3">32.43 32.53 32.84</cell></row><row><cell cols="2">CIFAR-10 14.67</cell><cell>14.21</cell><cell>15.80</cell><cell>16.03</cell><cell cols="2">16.99 17.27</cell><cell>17.20</cell></row><row><cell>MNIST</cell><cell>49.74</cell><cell>50.47</cell><cell>72.26</cell><cell>73.09</cell><cell cols="3">81.60 77.89 82.89</cell></row><row><cell>20NG</cell><cell>0.54</cell><cell>2.27</cell><cell>18.62</cell><cell>25.59</cell><cell cols="3">45.36 44.45 46.36</cell></row></table><note><p><p><p><p>lighted in boldface. To save space, the standard deviations (std) are not reported. In fact, the std values of SDEC are pretty small (i.e., SDEC obtains std values of 0.05%, 0.24%, 0.22%, 0.03%, and 0.03% on USPS, STL-10, CIFAR-10, MNIST, and 20NG, respectively).</p>Please cite this article as: Y.</p>Ren  </p>et al., Semi-supervised deep embedded clustering, Neurocomputing (2018), https://doi.org/10.1016/j.neucom.2018.10.016 ARTICLE IN PRESS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Clustering results measured by ARI (%).</figDesc><table><row><cell>Data</cell><cell cols="5">k -means KM-cst AE + KM AE + KM-cst DEC</cell><cell>IDEC</cell><cell>SDEC</cell></row><row><cell>USPS</cell><cell>53.26</cell><cell>56.03</cell><cell>57.51</cell><cell>58.90</cell><cell cols="3">68.79 69.48 69.86</cell></row><row><cell>STL-10</cell><cell>11.92</cell><cell>12.33</cell><cell>16.30</cell><cell>16.95</cell><cell cols="3">19.49 18.85 20.56</cell></row><row><cell cols="2">CIFAR-10 6.98</cell><cell>6.93</cell><cell>6.69</cell><cell>6.87</cell><cell>9.21</cell><cell>7.71</cell><cell>9.48</cell></row><row><cell>MNIST</cell><cell>37.12</cell><cell>38.17</cell><cell>64.73</cell><cell>66.52</cell><cell cols="3">77.30 73.44 79.20</cell></row><row><cell>20NG</cell><cell>0.01</cell><cell>0.01</cell><cell>1.10</cell><cell>9.24</cell><cell cols="3">26.77 26.31 44.78</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: Y. Ren et al., Semi-supervised deep embedded clustering, Neurocomputing (2018), https://doi.org/10.1016/j.neucom.2018.10.016</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.csie.ntu.edu.tw/ ∼ cjlin/libsvmtools/datasets/multiclass.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://cs.stanford.edu/ ∼ acoates/stl10/ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.cs.toronto.edu/ ∼ kriz/cifar.html .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://yann.lecun.com/exdb/mnist/ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://qwone.com/ ∼ jason/20Newsgroups/ . Please cite this article as: Y. Ren et al., Semi-supervised deep embedded clustering, Neurocomputing (2018), https://doi.org/10.1016/j.neucom.2018.10.016</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper was in part supported by Grants from the Natural Science Foundation of China (Nos. 61806043 , 61572111 , and 61832001 ), two Projects funded by China Postdoctoral Science Foundation (Nos. 2016M602674 and 2017M623007 ), a 985 Project of UESTC (No. A1098531023601041 ), three Fundamental Research Funds for the Central Universities of China (Nos. ZYGX2016J078, ZYGX2016Z003, and ZYGX2016J034). This research is supported by the National Research Foundation Singapore under its AI Singapore Programme (No. AISG-RP-2018-001).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bag-of-concepts: comprehending document representation through clustering words in distributed representation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="336" to="352" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive local structure learning for document co-clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big data clustering and its applications in regional science</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data for Regional Science</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Schintler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</editor>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CLUE: cluster-based retrieval of images by unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1187" to="1201" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised image-set clustering using an information theoretic framework</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Jacob Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="458" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating image clustering and codebook learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1903" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time computerized annotation of pictures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="985" to="1002" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric models for multiway data analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="475" to="487" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable nonparametric multiway data analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS<address><addrLine>San Diego, CA , USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09">2015. May 9-12</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational random function model for network modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2837667</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967">1967</date>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust multi-view data clustering with multi-view capped-norm k-means</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-weighted multi-view clustering with soft capped norm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust graph regularized nonnegative matrix factorization for clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="483" to="503" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A weighted adaptive mean shift clustering algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="794" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosted mean shift clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2014">2014b</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cluster ensembles -a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weighted-object ensemble clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Thirteenth International Conference on Data Mining</title>
		<meeting>the IEEE Thirteenth International Conference on Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weighted-object ensemble clustering: methods and analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="661" to="689" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distribution-based cluster structure selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3554" to="3567" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kernel-driven similarity learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Low-rank kernel learning for graph-based clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2018.09.009</idno>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local and global structure preserving based feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="147" to="157" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative cluster analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D L</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1573" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trendsë Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured inference for recurrent hidden semi-Markov model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. July 13-19</date>
			<biblScope unit="page" from="2447" to="2453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning compact recurrent neural networks with block-term tensor decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9378" to="9387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep subspace clustering with sparsity prior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1925" to="1931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning Literature Survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.cs.wisc.edu/∼jerryzhu/pub/ssl_survey.pdf" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schrödl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Semi-Supervised Clustering: Learning with Limited User Feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Integrating constraints and metric learning in semi-supervised clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised deep embedded clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.10.016</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2018.10.016" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Please cite this article as</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A probabilistic framework for semi-supervised clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep learning with nonparametric clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.03084</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Res. Repos</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep linear coding for fast graph clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3798" to="3804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep spectral clustering learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fourth International Conference on Machine Learning</title>
		<meeting>the Thirty-fourth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1985" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Auto-encoder based data clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</title>
		<meeting>the Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive regularization for transductive support vector machine</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semi-supervised learning from general unlabeled data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth IEEE International Conference on Data Mining</title>
		<meeting>the Eighth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient convex relaxation for transductive support vector machine</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative semi-supervised feature selection via manifold regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1033" to="1047" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-label linear discriminant analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing</title>
		<meeting>International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Random subspace based semi-supervised feature selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning and Cybernetics</title>
		<meeting>International Conference on Machine Learning and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">From Instance-level Constraints to Space-Level Constraints: Making the Most of Prior Knowledge in Data Clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://ilpubs.stanford.edu:8090/528/" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semi-supervised denpeak clustering with pairwise constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Pacific Rim International Conference on Artificial Intelligence</title>
		<meeting>the Fifteenth Pacific Rim International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="837" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Active semi-supervision for pairwise constrained clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Constrained K-Means Clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">20 0 0</date>
			<publisher>Microsoft Research</publisher>
			<pubPlace>Redmond</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">C-DBSCAN: density-based clustering with constraints</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Menasalvas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing</title>
		<meeting>the International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Double selection based semi-supervised clustering ensemble for tumor clustering from gene expression profiles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="727" to="740" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Incremental semi-supervised clustering ensemble for high dimensional data clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Adaptive ensembling of semi-supervised clustering solutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1577" to="1590" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semi-supervised ensemble clustering based on selected constraint projection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2018.2818729</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep transductive semi-supervised maximum margin clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.06237</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
