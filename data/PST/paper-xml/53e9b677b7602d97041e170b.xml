<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text Identification in Complex Background Using SVM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Datong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dalle Molle Institute for Perceptual Artificial Intelligence</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
							<email>bourlard@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Dalle Molle Institute for Perceptual Artificial Intelligence</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
							<email>jp.thiran@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Signal Processing Laboratory</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text Identification in Complex Background Using SVM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">326F8C998E2C83A698422351CB113C06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text identification</term>
					<term>support vector machine</term>
					<term>image and video OCR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a fast and robust algorithm to identify text in image or video frames with complex backgrounds and compression effects. The algorithm first extracts the candidate text line on the basis of edge analysis, baseline location and heuristic constraints. Support Vector Machine (SVM) is then used to identify text line from the candidates in edge-based distance map feature space. Experiments based on large amount of images and video frames from different sources showed the advantages of this algorithm compared to conventional methods in both identification quality and computation time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text embedded in image and video usually provides brief and important information about the content, such as name of a player or speaker, title, location and date of an event, category of the product etc. This kind of embedded text, referred to as closed caption, is a powerful knowledge source in building image and video indexing and retrieval system. The extraction of closed captions has therefore gained research importance recently.</p><p>Due to the huge amount of data carried by images and video, it is of very practical importance to detect and identify the text region as accurately as possible before performing any character recognition. However, text detection and identification is a difficult task because background, color, size of text strings may vary, even in a same image. Many papers <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b13">[14]</ref> show that available binarization methods, including global and adaptive thresholding (which has been well used in identifying characters printed on clean papers) do not work well for typical image and video frame. Furthermore, the image digitalization and compression also introduce noise that may blur the embedded text characters.</p><p>Previous work on text identification in image or video can be briefly classified into region-based, texturebased and edge-based methods.</p><p>Region-based methods detect characters as the monochrome regions satisfying certain heuristic constraints. The pixels of each character are assumed to have similar color and can be segmented from background by image segmentation <ref type="bibr" target="#b3">[4]</ref>[8] <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> or color clustering <ref type="bibr" target="#b16">[17]</ref> preprocess. The resulting monochrome regions are selected as characters under some simple heuristic constraints, such as the size, the height/width ratio of the region or baselines. Regionbased methods not only identify the embedded text regions but also segment characters from background. However, the monochrome constraint can not always be satisfied and, therefore, the methods are not robust to complex background and compressed video.</p><p>Texture-based methods make use of texture features to decide whether a pixel or block of pixels belongs to text or not. Wu et al. <ref type="bibr" target="#b4">[5]</ref>[6] <ref type="bibr" target="#b6">[7]</ref> proposed an algorithm based on K-means to identify text pixels on the basis of nine second order derivatives of Gaussians at three scales. Li et al. <ref type="bibr" target="#b14">[15]</ref> used a neural network to extract text blocks in Haar wavelet decomposition feature space. Zhong and Jain <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b15">[16]</ref> presented a text region identification approach to combine spatial variance (texture feature) and connected component (regions) analysis together. Texture-based method is able to detect text in complex background but is very time consuming <ref type="bibr" target="#b14">[15]</ref> and cannot always perform accurate localization <ref type="bibr" target="#b7">[8]</ref>.</p><p>Edge-based method detects the text by finding vertical edges. In <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b17">[18]</ref>, vertical edges are first detected and connected into text clusters by using a smoothing filter. As with region and texture-based methods, the text clusters are then selected by using heuristic constraints. Edge-based method performs fast text detection but also results in many false detections.</p><p>In the present paper, we introduce a fast and robust algorithm of text identification in image and video frame. As illustrated in Figure <ref type="figure">1</ref>, the algorithm consists of two steps: text line extraction and SVM-based text identification. In the first step, candidate text regions are quickly extracted by using edge analysis, and further segmented into text lines on the basis of baseline location and heuristic constraints at the aim of producing high text location rate and reasonable false alarms. In the ISBN 0-7695-1272-0/01 $10.00 (C) 2001 IEEE second step, the candidate text lines are identified by using a support vector machine (SVM) trained in a distance map feature space to lower the false alarm rate. The detail of the two steps are described in Section 2 and 3 respectively. Experiments and results are shown in Section 4. The discussion and conclusion are in the last Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Algorithm of text identification 2 Text Line Extraction</head><p>In this section, we present an algorithm to quickly extract text lines in images and key frames by exploiting two characteristics of closed captions. First, a visible character always forms some edges against its background. Second, a text string has a special kind of texture pattern, a rectangle shape and horizontal alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Candidate text region extraction</head><p>In our algorithm, the texture pattern of text string is simply regarded as a group of short vertical and horizontal edges mixed together. Although the compression process may blur parts of characters in image, the visible part of the text still has quite a different intensity compared to its neighbor's background. Varying-orientation edges can therefore always be detected in a text embedded region. These text edges are generally short and connected with each other in different orientations.</p><p>To detect this kind of short edge mixture pattern, we first detect the vertical and horizontal edges individually using "Canny" edge detector. Morphological dilation is then employed to connect edges into clusters. According to the type of edge (vertical or horizontal), different dilation operators are used so that the vertical edges are connected in horizontal direction while horizontal edges are connected in vertical direction. The dilation operators are designed to have rectangle shapes; in our case, 5×1 for the vertical operator and 3×6 for the horizontal operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2 Vertical and horizontal edge dilation operators</head><p>Since non-text areas usually formed by isolated or long vertical and horizontal edges do not occupy the same places in both vertical and horizontal dilated edge images at the same time, they can be removed by using an "AND" operation to the vertical and horizontal dilated edge images. Figure <ref type="figure" target="#fig_0">3</ref> illustrates the clusters resulting from this detection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original image Vertical edge dilation</head><p>Horizontal edge dilation Candidate text regions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text line location</head><p>The connected pixels in resulting clusters are grouped into candidate text regions. If a candidate region contains text lines, the top and bottom baselines of these text lines can be located.</p><p>Baseline location is based on the horizontal projection of the target region. Here, we simply regard a candidate text region as binary image and project it onto the Y-axis. Baselines can be located at the lowest values or the peak of the first order derivative of this Y-axis projection. In case that a candidate region may contains more than one text line, the candidate region is first segmented by the resulting baselines into several smaller regions. Baseline locating process is then performed iteratively on these new born regions.</p><p>Very small regions and non-baseline regions are removed in this process and the updated regions bounded with its baselines are illustrated in Figure <ref type="figure" target="#fig_1">4</ref> (left).</p><p>The typical heuristic character of a text string is then employed to select the text lines. In our experiments the text line should satisfy the following constraints: (1) the size of region is between 75 to 9000; (2) the horizontalvertical aspect ration is more than 1.2; (3) the height of the region is between 8 to 35. In general, the size of the text can vary greatly (more than 35 pixels high). Large characters can be detected by using the same algorithm on scaled image pyramid. Figure <ref type="figure" target="#fig_1">4</ref> (right) shows the extracted text lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline location</head><p>Extracted text lines SVM is a technique motivated by statistical learning theory and has been successful applied to numerous classification tasks. The key idea is to separate two classes with a decision surface that has maximum margin. The extensive discussion of SVM can be found in <ref type="bibr" target="#b20">[21]</ref> </p><formula xml:id="formula_0">( ) i b w x y i i ∀ ≥ + ⋅ 1 (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where w is normal to the hyperplane.</p><p>The margin of such a hyperplane:</p><formula xml:id="formula_2">0 = + ⋅ b x w</formula><p>is defined by the sum of the shortest distance from hyperplane to the closest positive example and the shortest distance from hyperplane to the closed negative example. Since this margin is simply</p><formula xml:id="formula_3">w / 2</formula><p>, where w is the Euclidean norm of w, the maximum margin can be given by minimizing 2 w subjecting to the constraints Eq. ( <ref type="formula" target="#formula_0">1</ref>).</p><p>For the nonlinear non-separable case, the learning task involves the minimization of , so called feature space, by choosing kernel ( ) ( ) ( )  </p><formula xml:id="formula_4">∑ = + m i i C w</formula><formula xml:id="formula_5">j i j i x x x x K φ φ ⋅ = ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Normalization and feature extraction</head><p>We first normalize the candidate text lines, which may have varying resolutions, to rectangles with 16 pixels in height by using bilinear interpolation (8 pixels between the baselines, 8 pixels for top and bottom boundary). Some examples are shown in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>The feature space we used has 256 dimensions corresponding to a 16×16 slide window in the normalized text region. Since the text may have varying gray-scales in images, the gray value is not a robust feature in this case. As explained below, we therefore use the distance map of each slide window as input feature of SVM.</p><p>The distance map <ref type="bibr" target="#b18">[19]</ref> DM(z) of window z is defined as the set of all the associated distance values ( )</p><formula xml:id="formula_6">y x v , in</formula><p>the window z with respect to a distance function dis according to</p><formula xml:id="formula_7">( ) ( ) ( ) ( )( ) [ ] i i B y x def y x y x dis y x v z y x i i , , , min , : , , ∈ = ∈ ∀ .</formula><p>Here, B is a set of strong edge points extracted in window z. The distance function used here is Euclidean. Figure <ref type="figure" target="#fig_5">6</ref> illustrates an example of distance map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SVM training and identification</head><p>The SVM was trained on a database consisting of 6000 samples labeled as text or non-text, using the software package developed at IDIAP and called SVMTorch <ref type="bibr" target="#b21">[22]</ref>. We used radial basis function kernel:</p><formula xml:id="formula_8">( )       = - - 2 2 2 exp , σ j X X j X X K</formula><p>where the kernel bandwidth σ was determined through cross-validation. The details of the requirement of the kernel and construction of the hyper-plane via a dual optimization process can be found in <ref type="bibr" target="#b20">[21]</ref>.</p><p>The output of the SVM ( )</p><formula xml:id="formula_9">z G</formula><p>estimates the confidence of the block of pixels in the 16×16 window z to be text. In the identification process, we slide the window every four pixels from left to right in each normalized text region and compute the confidence of each window. The confidence</p><formula xml:id="formula_10">( ) R Conf</formula><p>of a text region R was defined as:  Experiments were carried out on a database consisting of 18,000 video frames extracted from video of advertisements, sports, interviews, news, movies and 50 compressed images including the covers of journals, maps, and flyers. Each video frame or image has ISBN 0-7695-1272-0/01 $10.00 (C) 2001 IEEE 352x288 resolution in JPEG or MPEG format and was decompressed and converted into grayscale before applying text identification. Some video frames contain the same closed captions but with different backgrounds.</p><formula xml:id="formula_11">( ) ( ) ∑ ⊆         ⋅ = R z z d z G R Conf</formula><p>Performance of the text identification was measured in term of identification rate (IR) and false alarms. A text string is considered to be correctly identified if and only if the located baselines are in the valid ranges, which is labeled by human visual inspection as shown in Figure <ref type="figure" target="#fig_7">7</ref>. The false alarms are reported in terms of false region alarms and false pixel alarms. The false region alarm rate (FRR) is measured by the percentage of the number of false alarm regions in all the identified regions. The false pixel alarm rate (FPR) is defined as the area inside of false alarm region as a percentage of the whole area of the image.</p><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the performance of the proposed algorithm, where text line indicates the number of text strings containing at least two characters. We list both the performance for both text line extraction and identification. The fast text extraction algorithm correctly extracts about 99.3% text lines, but at the cost of high FRR and FPR. After applying the SVM identification, the FRR drops to 1.7% and FPR to 0.38%, while preserving a high IR as 98.7%.  <ref type="bibr" target="#b10">[11]</ref>, texture-based <ref type="bibr" target="#b4">[5]</ref> and edge-based <ref type="bibr" target="#b17">[18]</ref> methods, which is re-implemented by ourselves. It can be observed that the proposed algorithm results in a good tradeoff between high identification rate and low false alarm rates. Computation costs in Table <ref type="table" target="#tab_2">2</ref> are reported for Sun UltraSPARC-II with 333 MHz without counting the I/O consumption. CPU required by this algorithm is higher than region and edge based method but much lower than texture-based method. Figure <ref type="figure">8</ref> shows some text identification results, including correct identifications and false alarms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Text identification in image and video with complex backgrounds and compression effects is a difficult and challenging problem. In this paper, we have presented a fast text identification algorithm based on support vector machine. The algorithm first integrates the edge, and heuristic evidences to extract the candidate text lines and then identifies these candidates by using SVM.</p><p>The algorithm described in this paper does not use color, although many systems also make use of color information in detecting text in color images <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b16">[17]</ref>. The main reason is that the start point of our system, the edge evidence, is mostly coming from intensity in compressed image. Transforming the RGB color image to YUV color space and performing edge detection in U or V image can easily find out this fact. No temporal information is used in our algorithm. Since text may have different movements in video, text identification is usually performed before tracking the text among the video frames.</p><p>The algorithm presented here achieves high identification rate, as well as low false alarm rates. In fast text line extraction phrase, this algorithm is faster than (or equivalent to) other fast text identification methods, although the whole identification process is more CPU intensive than region-based and edge-based methods. The evaluation criterion of the identification result presented in this paper is on the basis of correct baseline localization. This criterion is stricter than complete cover criterion used in <ref type="bibr" target="#b5">[6]</ref>. With this criterion, we can measure the identification performance precisely without having to show the final character recognition result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Candidate text region extraction</figDesc><graphic coords="2,431.49,578.31,109.20,98.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Text line location</figDesc><graphic coords="3,186.93,417.27,111.12,99.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the penalty to errors and i ξ are positive slack variables that measure the amount of constraint violations. The training examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>using quadratic programming techniques. Once we have found the optimal j α , the classification of an unknown example z is based on the sign of function: be regarded as an alternative training technique for Radial Basis Function, Multi-Layer Perceptron and Polynomial classifiers. One of advantages of SVMs is that the learning task is insensitive to the relative numbers of training examples in positive and negative classes. For example, in our case, the candidate text lines usually involve 15.4% false alarms (in terms of regions). The number of positive examples thereby is roughly 6 times as the negative examples. Most learning algorithm based on Empirical Risk Minimization will tend to classify only the positive class correctly to minimize the error over data set. Since SVM aims at minimizing a bound on the generalization error of a model in high dimensional space, so called Structural Risk Minimization, rather than minimizing the error over data set, the training examples that are far behind the hyperplane will not change the support vectors. Therefore, SVM is used to identify text regions in the candidate text lines for achieving a lower false alarm rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Normalized text lines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Illustration of original image (left), strong edges in the original image (middle), and its distance map (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>is the distance between the center of window z and the center of the text region R, and 10 0 = σ . A candidate region R is identified as a text region if ( ) 0 ≥ R Conf .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc>Figure 7 Valid baseline ranges: the shading parts indicate the valid baseline range</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. In the present paper, we will only consider a binary classification task with m labeled training</figDesc><table><row><cell cols="8">examples:</cell><cell></cell><cell></cell><cell>( )( x y x , , 1 1</cell><cell>2</cell><cell>,</cell><cell>y</cell><cell>2</cell><cell>) ( x m y , ..., ,</cell><cell>m</cell><cell>)</cell><cell>,</cell><cell>where</cell></row><row><cell>y</cell><cell>i</cell><cell cols="2">=</cell><cell cols="2">±</cell><cell>1</cell><cell cols="6">indicating two different classes</cell><cell>i</cell><cell>=</cell><cell>, 1</cell><cell>,...., 2</cell><cell>m</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="10">For the linear separable case, we have a hyperplane</cell></row><row><cell cols="2">w</cell><cell>⋅</cell><cell cols="2">x</cell><cell cols="2">+</cell><cell>b</cell><cell>=</cell><cell>0</cell><cell cols="3">(decision surface) that separates all the</cell></row><row><cell cols="11">training examples:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Identification performance</head><label>1</label><figDesc></figDesc><table><row><cell>23037 text lines</cell><cell>IR</cell><cell>FRR</cell><cell>FPR</cell></row><row><cell>without SVM</cell><cell>99.3%</cell><cell>15.4%</cell><cell>2.13%</cell></row><row><cell>With SVM</cell><cell>98.7%</cell><cell>1.7%</cell><cell>0.38%</cell></row><row><cell cols="4">Table 2 compares the performance and running time</cell></row><row><cell cols="4">cost of the proposed algorithm with typical region-based</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 Performances and running costs</head><label>2</label><figDesc></figDesc><table><row><cell>X-based</cell><cell>IR</cell><cell>FRR</cell><cell>FPR</cell><cell>Sec. per</cell></row><row><cell>method</cell><cell></cell><cell></cell><cell></cell><cell>Image</cell></row><row><cell>Region</cell><cell>89.6</cell><cell>59.2%</cell><cell>5.8%</cell><cell>1.15</cell></row><row><cell>Texture</cell><cell>99.1%</cell><cell>11.5</cell><cell>3.3%</cell><cell>11.27</cell></row><row><cell>Edge</cell><cell>92.6%</cell><cell>24.1%</cell><cell>12.3%</cell><cell>0.52</cell></row><row><cell cols="2">Proposed 98.7%</cell><cell>1.7%</cell><cell>0.38%</cell><cell>2.76</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Dr. Samy Bengio and Dr. Jean-Marc Odobez for their comments on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Omnidocument technologies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bokser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1066" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nartker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OCR accuracy: UNLV&apos;s fifth annual test</title>
		<imprint>
			<date type="published" when="1996-09">September 1996</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Document Image Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>O'gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Los Alamitos</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition characters in scene images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksmatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="220" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding text in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Digital Libraries</title>
		<meeting>ACM Int. Conf. Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">(C) 2001 IEEE Figure 8 Identified text lines and false alarms in images or video frames</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1224" to="1229" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Textfinder: An automatic system to detect and recognize text in images</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document image clean-up and binarization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Symposium on Electronic Imaging</title>
		<meeting>SPIE Symposium on Electronic Imaging</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locating text in complex color images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1523" to="1536" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic text segmentation and text recognition for video indexing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
		<idno>TR-98-009</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Mannheim</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Mannheim</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic text recognition in digital videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Image and Video Processing IV</title>
		<meeting>SPIE, Image and Video essing IV</meeting>
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indexing and retrieval of digital video sequences based on automatic text recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM International Multimedia Conference</title>
		<meeting>4th ACM International Multimedia Conference<address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-11">November 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video ocr for digital news archives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Content Based Access of Image and Video Databases</title>
		<meeting><address><addrLine>Bombay</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video OCR: indexing digital news libraries by recognition of superimposed caption</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia System Special Issue on Video Libraries</title>
		<imprint>
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text enhancement in digital video using multiple frame integration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic text detection and tracking in digital video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Maryland Univ. LAMP Tech</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report 028</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic text localisation in images and video frames</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2055" to="2076" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identification of text on colored book and journal covers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sobottka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kronenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDAR</title>
		<imprint>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video skimming for quick browsing based on audio and image characterization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS- 95-186</idno>
		<imprint>
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distance transformations and skeletons of digitized pictures with applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Toriwaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yokoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in pattern recognition</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<meeting><address><addrLine>North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SVMTorch: Support Vector Machines for Large-Scale Regression Problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.idiap.ch/learning" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
