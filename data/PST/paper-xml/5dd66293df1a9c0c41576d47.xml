<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer vision algorithms and hardware implementations: A survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Feng</surname></persName>
							<email>xfeng@cqut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Chongqing University of Technology</orgName>
								<address>
									<postCode>400054</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Science Research Center</orgName>
								<orgName type="institution">Duke Kunshan University</orgName>
								<address>
									<postCode>215316</postCode>
									<settlement>Kunshan, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youni</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Chongqing University of Technology</orgName>
								<address>
									<postCode>400054</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuejiao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">Chongqing University of Technology</orgName>
								<address>
									<postCode>400054</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Du</surname></persName>
							<email>duming@dhu.edu.cnm.du</email>
							<affiliation key="aff2">
								<orgName type="institution">Donghua University</orgName>
								<address>
									<postCode>200051</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Research Center</orgName>
								<orgName type="institution">Duke Kunshan University</orgName>
								<address>
									<postCode>215316</postCode>
									<settlement>Kunshan, Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer vision algorithms and hardware implementations: A survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7963936D04A5EAB865830EF87C387C25</idno>
					<idno type="DOI">10.1016/j.vlsi.2019.07.005</idno>
					<note type="submission">Received 11 April 2019; Received in revised form 23 June 2019; Accepted 27 July 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Computer vision Hardware accelerator Deep convolutional neural network Artificial intelligence</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent progress of scientific technologies is producing a "Cambrian explosive" <ref type="bibr" target="#b0">[1]</ref> in developing new techniques that lead the world entering promptly into the new artificial intelligence (AI) era. Computer systems injected by the new AI techniques are intelligent to perceive and understand the visual world, and even smarter than humans in a number of specific tasks. The ability of being smart is primarily provided by a computer vision system, including both the algorithms and their hardware implementations, which gives us the ability to teach a computer to understand the physical world from vision.</p><p>Computer vision tasks seek to enable computer system automatically to see, identify and understand the visual world, simulating the same way that human vision does <ref type="bibr" target="#b1">[2]</ref>. Researchers in computer vision aspired to develop algorithms for such visual perception tasks including (i) object recognition in order to determine whether image data contains a specific object, (ii) object detection in order to localize instances of semantic objects of a given class, and (iii) scene understanding to parse an image into meaningful segments for analyzing. Given the broad range of mathematics being covered and the intrinsically difficult nature of recovering unknowns from insufficient information to fully specify the solution, the aforementioned tasks in the computer vision field are extremely challenging. Studying these problems is both theoretically and practically important.</p><p>Early efforts have made a great contribution to the philosophy of human vision and the basic computational theory of computer vision by exploiting well-designed features and feature descriptors combined with classical machine learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Although researchers have spent several decades to teach machines how to see, the most advanced machine at that time could only perceive common objects and struggled at recognizing numbers of natural objects with infinite shape variations similar to toddlers <ref type="bibr" target="#b4">[5]</ref>. Fortunately, researchers have believed that computer systems can go beyond regular object recognition and learn to reveal details and insights of the visual world by training them to see trillions of images and videos generated from Internet. To nourish the computer brain, the largest image classification dataset "ImageNet" <ref type="bibr" target="#b5">[6]</ref> that contains 15 million images across 22,000 classes of objects was created, upon which the well-known "deep learning" technology has demonstrated its overwhelming superiority over traditional computer vison algorithms that treat objects as a collection of shape and color features.</p><p>Deep learning is a particular class of machine learning algorithm, which typically simplifies the process of feature extraction and description through a multi-layer convolutional neural network (CNN). CNN aims to transform the high-dimension input image into low-dimension yet highly-abstracted semantic output. Powered by the massive data from ImageNet and the modern central processing units (CPUs) and graphics processing units (GPUs), methods based on deep neural network (DNN) achieve the state-of-the-art performance and bring an unprecedented development of computer vision in both algorithms and hardware implementations. In recent years, CNN has become the de-facto standard computation framework in computer vision. Numbers of deeper and more complicated networks are developed to make CNNs deliver nearhuman accuracy in many computer vision applications, such as classification, detection and segmentation. The high accuracy, however, comes at the price of large computational cost. As a result, dedicated hardware platforms, from the general-purpose GPUs to application-specific processors, are investigated to optimize for DNN-based workloads.</p><p>In this paper, we look into this rapid evolution of computer vision field by presenting a brief survey on the key algorithms that make computer systems perceivable and the underlying hardware platforms that make these algorithms applicable. In particular, we will discuss how the recent DNN algorithms accomplish the computer vision tasks (i.e. image classification, object detection and image segmentation) with high perception accuracy, and summarize the notable hardware units including GPUs, field-programmable gate arrays (FPGAs) and other advanced mobile hardware platforms that are adapted or designed to accelerate DNN-based computer vision algorithms. According to our knowledge, there are recent summaries in the literature that discuss the DNN-based algorithms for particular tasks, including image classification <ref type="bibr" target="#b6">[7]</ref>, object detection <ref type="bibr" target="#b7">[8]</ref>, image segmentation <ref type="bibr" target="#b8">[9]</ref>, and the corresponding hardware accelerators such as FPGAs <ref type="bibr" target="#b9">[10]</ref>. There is no comprehensive survey that covers both algorithm and hardware simultaneously. A thorough review of existing works from both topics is essential for researchers to understand the entire picture and motivate further progress in the computer vision field.</p><p>The reminder of this paper is organized as follows. In Section 2, we overview the computer vision algorithms for three visual perception tasks: image classification, object detection and image segmentation. Important hardware platforms including GPUs, FPGAs and other hardware accelerators for implementing the DNN-based algorithms are discussed in Section 3. Finally, we conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Computer vision algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image classification</head><p>Image classification is a kind of biologically primary ability of human visual perception system. It has been an active task and plays a crucial role in the field of computer vision, which aims to automatically classify images into pre-defined classes. For decades, researchers have laid path in developing advanced techniques to improve the classification accuracy. Traditionally, classification models can perform well only on small datasets such as CIFAR-10 <ref type="bibr" target="#b10">[11]</ref> and MNIST <ref type="bibr" target="#b11">[12]</ref>. The great-leap-forward development of image classification occurred when the large-scale image dataset "ImageNet" was created by Feifei Li in 2009 <ref type="bibr" target="#b5">[6]</ref>. It was almost the same time when the well-known deep learning technologies started to show great performance in classification and stepped onto the stage of computer vision.</p><p>Before the explosion of deep learning methods, research works put lots of efforts in designing scale-invariant features (e.g. SIFT <ref type="bibr" target="#b12">[13]</ref>, HOG <ref type="bibr" target="#b13">[14]</ref>, GIST <ref type="bibr" target="#b14">[15]</ref>), feature representations (e.g. Bag-of-Features <ref type="bibr" target="#b15">[16]</ref>, Fisher Kernel <ref type="bibr" target="#b16">[17]</ref>) and classifiers (e.g. SVM <ref type="bibr" target="#b17">[18]</ref>) for image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, these manually crafted features work against objects in natural images with complicated background, variant color, texture, illumination and ever-changing poses and view factors. At the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, AlexNet <ref type="bibr" target="#b20">[21]</ref> won the first prize by a significant margin over the second place that was based on SIFT and Fisher Vectors (FVs) <ref type="bibr" target="#b19">[20]</ref>. It demonstrates that the classification model based on deep CNN performs much more robustly than other conventional methods in the presence of large-scale variations. It also represents a remarkable milestone in the modern history of neural network after a long trough period.</p><p>A typical deep CNN model consists of several convolution layers followed by activation functions and pooling layers, and several fully connected layers before prediction. It comes into deep structure to facilitate filtering mechanisms by performing convolutions in multi-scale feature maps, leading to highly abstract and discriminative features.</p><p>AlexNet has 8 convolution layers, 3 pooling layers and 3 fully connected layers, with a total of 60 million parameters. It successfully uses ReLU as the activation function instead of sigmoid. Furthermore, data augmentation and dropout are widely used today as efficient learning strategies. AlexNet is hence known as the foundation work of modern deep CNN.</p><p>Inspired by AlexNet, VGGNet <ref type="bibr" target="#b21">[22]</ref> and GoogleNet <ref type="bibr" target="#b22">[23]</ref> focus on designing deeper networks to further improve accuracy. They were the runner-up and winner of ILSVRC in 2014 respectively. By repeatedly stacking 3 Â 3 convolutional kernels and 2 Â 2 maximum pooling layers, VGGNet successfully constructs a convolutional neural network of 16-19 layers. GoogleNet has 22 layers, but its floating-point operations and number of parameters are much less than those of AlexNet and VGGNet by removing the fully-connected layers and optimizing the operations of sparse matrices.</p><p>Although deeper networks offer better accuracy, simply increasing the number of layers cannot continuously improve accuracy because of vanishing/exploding gradient information during network training. ResNet <ref type="bibr" target="#b23">[24]</ref>, which makes another great progress of deep network structure, proposes to use a shortcut connection between residual blocks to make full use of information from previous layers and keep the gradients during backward propagation. By using this residual block, ResNet successfully trains very deep networks with up to 152 layers and was the winner of ILSVRC in 2015. Following the idea of ResNet, DenseNet <ref type="bibr" target="#b24">[25]</ref> establishes connections between all previous layers and the current layer. It concatenates and, therefore, reuses the features from all previous layers. DenseNet presents with great advantage in classification accuracy on ImageNet, as we can see in Table <ref type="table" target="#tab_0">1</ref>. Based on these works in the literature, connecting different network layers has shown promising improvement in learning representations of deeper networks.</p><p>By using ResNet or DenseNet as the major backbone structure, researchers focus on improving the functionality of neural network blocks. SENet <ref type="bibr" target="#b25">[26]</ref>, which was the winner of ILSVRC 2017, proposes a "squeeze-and-excitation" (SE) unit by taking channel relationship into account. It learns to recalibrate channel-wise feature maps by explicitly modeling the interdependencies among channels, which is consequently exploited to enhance informative channels and suppress other useless channels.</p><p>Despite the high classification performance of the aforementioned CNN models, appropriately designing the optimal network structure  <ref type="table" target="#tab_0">1</ref>.</p><p>The aforementioned deep networks facilitate classification to be more accurate. However, in many real-life classification applications, such as robotics, autonomous driving, smartphone, etc., the classification task is highly constrained by the computational resources that are available. The problem thus becomes to pursue the optimal accuracy subject to a limited computational budget (i.e. memory and/or MFLOPs). Therefore, a set of lightweight networks such as SqueezeNet <ref type="bibr" target="#b28">[29]</ref>, MobileNet <ref type="bibr" target="#b29">[30]</ref>, Shuf-fleNet <ref type="bibr" target="#b30">[31]</ref>, ShiftNet <ref type="bibr" target="#b31">[32]</ref> and FE-Net <ref type="bibr" target="#b32">[33]</ref> start a wave.</p><p>SqueezeNet substitutes most 3 Â 3 filters by 1 Â 1 filters and cuts down the numbers of input channels for 3 Â 3 filters to reduce the network complexity. To maximize the accuracy with a limited number of network parameters, it delays the down-sampling operation to avoid information loss in early layers. SqueezeNet is 50 Â smaller than AlexNet. If combined with deep compression <ref type="bibr" target="#b33">[34]</ref>, it can even be reduced to be 510 Â smaller than AlexNet. Depth-wise separable convolution is employed to decompose the standard convolution into depth-wise convolution and point-wise convolution in MobileNet <ref type="bibr" target="#b29">[30]</ref>. Depth-wise convolution performs convolution on each input channel with one filter, while point-wise convolution combines those separate channels by using 1 Â 1 convolution. This novel design of convolution reduces both the computational complexity and the number of parameters.</p><p>ShuffleNet <ref type="bibr" target="#b30">[31]</ref> uses point-wise group convolution that divides the input feature maps into groups and performs convolutions separately on each group to reduce computational cost. However, because the grouping operations limit the communication between different channels, Shuf-fleNet further shuffles the channels and feeds each group in the following layer with multiple channels from different groups in order to distribute information across channels.</p><p>In addition to the strategies adopted to reduce the computational cost of spatial convolution (e.g., depth-wise convolution), ShiftNet <ref type="bibr" target="#b31">[32]</ref> presents a parameter-free, FLOP-free shift operation to replace expensive spatial convolutions. The proposed shift operation is able to provide spatial information communication by shifting feature maps, making it possible to aggregate spatial information by the following point-wise convolutional layer. More recently, FE-Net <ref type="bibr" target="#b32">[33]</ref> further finds that only a few shift operations are sufficient to provide spatial information communication. A sparse shift layer (SSL) is proposed to perform shift operations on a small portion of feature maps only. With only 563 M FLOPs, FE-Net achieves the state-of-the-art performance among all major lightweight classification models on ImageNet, as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The aforementioned network models are briefly summarized in Table <ref type="table" target="#tab_0">1</ref>. In addition to the conventional image classification problem with thousands of classes and complex scenes, multi-label classification (e.g. face attributions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>) and fine-grained classification (e.g. Stanford Dogs classification <ref type="bibr" target="#b36">[37]</ref>) are also of great interest in the computer vision area.</p><p>Furthermore, the great success of deep learning in image domain has stimulated a variety of techniques to learn robust feature representations for video classification, where the semantic contents such as human actions <ref type="bibr" target="#b37">[38]</ref> or complex events <ref type="bibr" target="#b38">[39]</ref> are automatically categorized. Early works often treat a video clip as a collection of frames. Video classification is implemented by aggregating frame-level CNN features by averaging or encoding <ref type="bibr" target="#b39">[40]</ref>. Standard classifiers, such as SVM, are finally used for recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>In contrast to the frame-level classification methods, there are a number of other approaches applying end-to-end CNN models to learn the hidden spatio-temporal patterns in video. For example, the typical C3D features <ref type="bibr" target="#b42">[43]</ref> are derived from a deep 3-D convolutional network trained on the large-scale UCF101 dataset. Moreover, a two-stream approach <ref type="bibr" target="#b43">[44]</ref> is proposed to factorize the learning problem of video representation into spatial and temporal cues separately. Specifically, a spatial CNN is adopted to model the appearance information from RGB frames, while a temporal CNN is used to learn the motion information from the dense optical flow among adjacent frames.</p><p>Since the two-stream approach only depicts movements within a short time window and fails to consider the temporal order of different frames, several recurrent connection models for sequential data, including recurrent neural networks (RNNs) and long short-term memory (LSTM) models, are leveraged to model the temporal dynamics for videos. In Ref. <ref type="bibr" target="#b44">[45]</ref>, two two-layer LSTM networks are trained with features from the two-stream approach for action recognition. In Ref. <ref type="bibr" target="#b45">[46]</ref>, the LSTM model and CNN model are combined to jointly learn spatial-temporal cues for video classification. In Refs. <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, attention mechanism is introduced for convolutional LSTM models to discover relevant spatio-temporal volumes for video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object detection</head><p>Object detection, which is to determine and locate the object instances either from a large number of predefined categories in natural images or for a given particular object (e.g., Donald Trump's face, the distorted area in an image, etc.), is another important and challenging task in computer vision. Object detection and image classification share a similar technical challenge: both of them must handle a large number of highly variable objects. However, object detection is more difficult than image classification, as it must identify the accurate localization of the object of interest.</p><p>Historically, most research efforts have focused on detecting a single category of given objects such as pedestrian <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref> and face <ref type="bibr" target="#b49">[50]</ref> by designing a set of appropriate features (e.g. HOG <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>, Harr-like <ref type="bibr" target="#b49">[50]</ref>, LBP <ref type="bibr" target="#b50">[51]</ref>, etc.). In these works, objects are detected by using a set of predefined feature templates matching with each location in the image or feature pyramids. Standard classifiers such as SVM <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref> and Adaboost <ref type="bibr" target="#b49">[50]</ref> are often used for this purpose.</p><p>In order to build a general-purpose, robust object detection system, research community has started to develop large-scale, multi-class datasets in recent years. Pascal-VOC 2007 <ref type="bibr" target="#b51">[52]</ref> with 20 classes and MS-COCO <ref type="bibr" target="#b52">[53]</ref> with 80 object categories are two iconic object detection datasets. In these two datasets, detection results are evaluated by two possible metrics: (i) Average Precision (AP) by counting the correctly detected bounding boxes for which the overlap ratio exceeds 0.5, and (ii) mean Average Precision (mAP) by averaging the AP values associated with different thresholds of the overlap ratio.</p><p>Recently, deep learning has substantially advanced the object detection field. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, striking improvements in object detection accuracy have been demonstrated over both Pascal-VOC 2007 and MS-COCO by taking advantages of deep learning techniques. R-CNN <ref type="bibr" target="#b53">[54]</ref> was the first two-stage method among the earliest CNN-based generic object detection techniques. It adopts AlexNet to extract a fixed-length feature vector from each resized region proposal, which is the object candidate generated by selective search algorithm <ref type="bibr" target="#b54">[55]</ref>. Each region is then classified by a set of category-specific linear SVMs. The method shows significant improvement in mAP over the traditional state-of-the-art DPM detector <ref type="bibr" target="#b48">[49]</ref>. It is, however, not elegant and inefficient, due to its multistage complex pipeline and the redundant CNN feature extraction from numerous region proposals.</p><p>Inspired by the spatial pyramid pooling in SPPnet <ref type="bibr" target="#b55">[56]</ref> that leverages fixed-length feature outputs for arbitrary input image sizes, Fast R-CNN <ref type="bibr" target="#b56">[57]</ref> incorporates a ROI pooling layer before the fully-connected layer to obtain a fixed-length feature vector for each proposed region, so that only a single convolution operation is required for the input image. Fast R-CNN substantially improves the detection efficiency over R-CNN and SPPnet. However, it requires expensive computation for external region proposal, which now becomes the major bottleneck in overall efficiency.</p><p>To address the aforementioned challenge associated with computational complexity, Faster R-CNN <ref type="bibr" target="#b57">[58]</ref> further proposes a Region Proposal Network (RPN). It then integrates both RPN (for proposal generation) and Fast-RCNN (for region classification) into a unified, end-to-end network structure. RPN and Fast-RCNN share most of the convolution layers, and the features from the last shared layer are used for two separate tasks (i.e. proposal generation and region classification). With this highly efficient architecture, Faster R-CNN achieves 6 FPS inference speed on a GPU and the state-of-the-art detection accuracy on Pascal-VOC 2007.</p><p>As a follow-up work on Faster R-CNN, Mask R-CNN <ref type="bibr" target="#b58">[59]</ref> was later proposed to combine object detection and pixel-level instance segmentation based on Faster R-CNN. By using ResNet101-FPN as the backbone network, Mask R-CNN demonstrated the best detection accuracy on MS-COCO in 2017.</p><p>The two-stage R-CNN architectures are able to offer superior accuracy. However, real-time efficiency is required for object detection by many real-world applications. In this regard, the simple one-stage architectures are often preferred. YOLO <ref type="bibr" target="#b59">[60]</ref> is the first one-stage method that casts detection task as a regression problem. It divides an image into a number of S Â S grids and proposes B bounding boxes for each grid. Next, by using the CNN features of the input image globally, it directly predicts the coordinates, confidence scores and C-class probabilities for these bounding boxes. Without the proposal generation step, YOLO can achieve the real-time speed of 45 FPS. On the other hand, since YOLO investigates few prediction candidates, it is less accurate than the two-stage models, especially for small objects detection.</p><p>The Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b60">[61]</ref> follows a similar one-stage strategy. It outperforms YOLO in accuracy due to two major improvements. First, SSD extracts important features from multi-scale CNN feature maps. Second, it adopts a number of default bounding boxes by following the concept of anchor proposed by Faster R-CNN.</p><p>YOLOv2 absorbs the merit of SSD and Faster R-CNN by introducing the anchor mechanism <ref type="bibr" target="#b61">[62]</ref>. The new YOLOv2 model both improves detection accuracy and reduces inference time by a large margin over YOLO on Pascal-VOC 2007. However, its accuracy is still worse than the two-stage methods for generic detection tasks (e.g. MS-COCO).</p><p>In the latest implementation of YOLOv3 <ref type="bibr" target="#b62">[63]</ref>, several anchor boxes are assigned on three different scaled feature maps, thereby producing much more proposals than YOLO and YOLOv2. Small objects can thus be accurately detected from the anchors in low-level feature maps with small receptive fields. In addition, it uses a powerful backbone network darknet-53 with several sets of residual blocks. YOLOv3 achieves similar accuracy as Faster R-CNN, while maintaining real-time efficiency. It is the current state-of-the-art object detection framework for real-time applications.</p><p>We summarize the performance metrics of the aforementioned detection models in Table <ref type="table" target="#tab_1">2</ref>. In addition to the one-stage and two-stage architectures, there are several other spotlights for object detection. For example, the relationship of different objects is considered by designing an object relation module in Ref. <ref type="bibr" target="#b63">[64]</ref>. Generative Adversarial Network (GAN) is used to generate the super-resolution of small object patches <ref type="bibr" target="#b64">[65]</ref> or features <ref type="bibr" target="#b65">[66]</ref> to help with the detection of small objects.</p><p>In contrast to the significant progress in object detection focusing on still images, video object detection has received less attention. Generally, object detection for videos is realized by fusing the results of object detection on the current frame and object tracking from the previous frames. Deep SORT <ref type="bibr" target="#b66">[67]</ref> is a typical tracking-by-detection algorithm for multi-object tracking in videos. By integrating appearance information extracted from a CNN-based object detector with the original SORT <ref type="bibr" target="#b67">[68]</ref> algorithm, it is able to achieve real-time tracking. Recently, several approaches have been proposed for end-to-end video object detection. In Ref. <ref type="bibr" target="#b68">[69]</ref>, temporal feature aggregation is performed to improve feature quality and recognition accuracy. In Ref. <ref type="bibr" target="#b69">[70]</ref>, data redundancy between consecutive frames is exploited. These seminal works have been adopted by the winner of ImageNet Video Object Detection Challenge 2017 <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image segmentation</head><p>Image classification should recognize what objects are in the visual scene (as shown by the example in Fig. <ref type="figure" target="#fig_1">2(a)</ref>), while object detection reveals where the objects are (as shown by the example in Fig. <ref type="figure" target="#fig_1">2(b)</ref>). In this sub-section, we further focus on the problem of how the objects are exactly presented in the visual scene by using image segmentation.</p><p>Image segmentation is regarded as pixel-level classification, which aims at dividing an image into meaningful regions by classifying each pixel into a specific entity. In traditional image segmentation, the idea of unsupervised local region merging and splitting has been extensively explored based on clustering <ref type="bibr" target="#b71">[72]</ref>, optimizing global criteria <ref type="bibr" target="#b72">[73]</ref>, or user interaction <ref type="bibr" target="#b73">[74]</ref>. The blooming deep learning technologies have promoted large-scale supervised classification moving from image-level object classification to box-level object localization, and further to pixel-wise object segmentation. Therefore, today's image segmentation is object oriented and can be divided into two subtle branches: (i) semantic segmentation, which assigns each pixel in an image to a semantic object class, as shown in Fig. <ref type="figure" target="#fig_1">2 (c</ref>), and (ii) instance segmentation, which predicts different labels for different object instances as a further improvement to semantic segmentation, as shown in Fig. <ref type="figure" target="#fig_1">2 (d)</ref>.</p><p>In addition to classification and detection, the challenges of Pascal-VOC 2012 <ref type="bibr" target="#b75">[76]</ref> and MS-COCO provide segmentation competitions as  well. They are two important datasets for image segmentation on which most research works are evaluated. In addition to them, Cityscapes <ref type="bibr" target="#b76">[77]</ref> and MVD <ref type="bibr" target="#b77">[78]</ref> provide street scenes with a large number of traffic objects in each image. The popular metrics to evaluate pixel-level segmentation accuracy are pixel accuracy (PA, i.e., the proportion of pixels predicted correctly), mean pixel accuracy (mPA) of all classes, mean intersection over union (mIOU), and frequency weighted intersection over union (FWIOU). Among them, mIOU is often preferred for its simplicity and representativeness.</p><p>Early segmentation techniques based on deep learning usually apply CNN as the feature descriptor for each pixel that is described by its surrounding patch <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>. This CNN-based framework is problematic in efficiency and is not sufficiently accurate for redundant feature extraction. Fully convolutional network (FCN) <ref type="bibr" target="#b80">[81]</ref> is the forerunner that successfully implements pixel-wise dense predictions for semantic segmentation in an end-to-end CNN structure. It replaces the fully-connected layers of the well-known classification architectures (e.g.VGG <ref type="bibr" target="#b21">[22]</ref>, GoogleNet <ref type="bibr" target="#b22">[23]</ref>, etc.) with convolution layers to facilitate inputs of arbitrary sizes, and outputs a heatmap rather than a vector to indicate classification scores. Prediction loss is then measured by the pixel-wise loss between the upsampled heatmap using deconvolution and the labeled image of original size. FCN shows great improvement in pixel accuracy over traditional segmentation methods on Pascal-VOC 2012. However, the basic FCN structure fails to capture a large number of features and it does not consider spatial consistency between pixels, which hinders its application to certain problems and scenarios.</p><p>In any wise, the success of FCN architecture makes it popular and it has been actively followed by many subsequent segmentation works <ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref>. Generally, using classification models without fully-connected layers as the backbone network to produce low-resolution feature maps is referred to as the encoder, while the symmetric mapping from the low-resolution image to pixel-wise classification outcome is termed as the decoder. With the well-known backbone network as encoder, alternative CNN-based segmentation works are usually variant in decoder implementation. For example, the decoding stage of SegNet <ref type="bibr" target="#b81">[82]</ref> uses the max-pooling indices from corresponding feature maps in its encoder for upsampling. The resultant maps are then convolved with a set of filters to generate the restored feature maps for dense prediction. In another typical encoder-decoder framework U-Net <ref type="bibr" target="#b82">[83]</ref> that is designed for biomedical image segmentation, the upsampling layer directly concatenates with a set of cropped duplicates of corresponding feature maps in encoder to enhance resolution during the decoder process.</p><p>Image segmentation is a difficult problem that requires both good pixel-level accuracy, which relies on fine-grained local features, and classification accuracy, for which global context of the image is crucial to resolve local ambiguities. However, the pooling strategy in classic CNN architectures is a defect for losing the detailed information when multipooling steps are performed.</p><p>One possible and common solution to integrate context knowledge is to refine the output to have fine-grained details for accurate segmentation by Conditional Random Field (CRF) <ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref>. Alternatively, instead of using pooling strategy, the problem can be solved by expanding receptive fields in which each neuron is connected to a subset of neurons in the previous layer. Dilated convolution <ref type="bibr" target="#b87">[88]</ref>, which is a regular convolution with upsampled filters or dilated filters, is proposed to exponentially expand receptive fields without sacrificing resolution, as shown in Fig. <ref type="figure" target="#fig_4">3</ref>. The DeepLab <ref type="bibr" target="#b84">[85]</ref> model takes advantages of both dilated convolution and CRF refinement by post-processing to integrate context knowledge. As can be seen in Fig. <ref type="figure" target="#fig_2">4</ref>, it achieves much higher prediction accuracy than SegNet and FCN and is thus considered as a milestone work for semantic segmentation.</p><p>Several recently-proposed methods, such as RefineNet <ref type="bibr" target="#b83">[84]</ref> and PSPNet <ref type="bibr" target="#b88">[89]</ref>, try to avoid or restore the loss of down-sampling in encoder by fusing low-level and high-level features. RefineNet designs a decoder module by using both short-range and long-range residual connections to capture rich contextual information. It has achieved the state-of-the-art performance on 7 public datasets. In PSPNet, a pyramid pooling module is proposed to aggregate different region-based context information to exploit the capability of global context information.</p><p>Inspired by the spatial pyramid pooling, DeepLab V2 <ref type="bibr" target="#b86">[87]</ref> investigates an atrous spatial pyramid pooling (ASPP) module by incorporating dilated convolution with different sampling rates and spatial pyramid pooling to capture multi-level context information. In the latest DeepLab V3þ <ref type="bibr" target="#b89">[90]</ref>, an Xception module <ref type="bibr" target="#b90">[91]</ref> is introduced to the The aforementioned end-to-end architectures mainly focus on semantic segmentation. Comparatively, most works on instance segmentation follow the pipeline that segmentation precedes recognition. For example, DeepMask <ref type="bibr" target="#b91">[92]</ref> and the instance-sensitive fully convolutional network <ref type="bibr" target="#b92">[93]</ref> use Fast-RCNN to classify the learned segment proposals. The fully convolutional instance segmentation (FCIS) combines the segment proposal system <ref type="bibr" target="#b92">[93]</ref> and object detection in Ref. <ref type="bibr" target="#b93">[94]</ref> to predict object classes, boxes, and masks simultaneously. However, this method is not as accurate as Mask-RCNN <ref type="bibr" target="#b58">[59]</ref> on the MS-COCO instance segmentation challenge. Mask-RCNN extends Faster-RCNN by adding a mask prediction branch in addition to bounding box regression and class recognition. The very recent path aggregation network (PANet) <ref type="bibr" target="#b94">[95]</ref> enhances the feature hierarchy by a bottom-up path augmentation. With subtle computation overhead, it reaches the first place in the MS-COCO challenge of instance segmentation task and also represents the state-of-the-art on MVD and Cityscapes.</p><p>Although pixel-wise image segmentation is progressing rapidly with superior accuracy, it is still far from practical usage, such as video semantic segmentation, due to the high complexity of dense prediction. Therefore, developing highly efficient image segmentation framework is one of the grand challenges in the computer vision community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hardware implementation</head><p>The recent breakthroughs in developing computer vision algorithms are not only driven by deep learning technologies and large-scale datasets, but also relying on the major leaps of hardware acceleration that provides powerful parallel computing architectures to enable the efficient training and inference of large-scale, complex and multi-layered neural networks.</p><p>Hardware acceleration takes advantage of computer hardware to perform computing tasks with lower latency and higher throughput than the conventional software implementation running on general-purpose CPUs <ref type="bibr" target="#b97">[98]</ref>. Historically, the von-Neumann-style compute-centric architectures (e.g. CPUs) are primarily designed for effective serial computations with complex task scheduling. It suffers from high energy consumption and low memory bandwidth for data movement when evaluating the deep CNN networks that require parallel dense computation, high data reusability and large memory bandwidth <ref type="bibr" target="#b98">[99]</ref>.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref>(a) compares neural network with other approaches in terms of accuracy and scale (i.e. data/model size). The traditional machine learning methods, such as decision tree, SVM, etc., are referred as "Other Approaches" in Fig. <ref type="figure" target="#fig_3">5(a)</ref>. They are generally based on manually designed features. Due to the limited learning capability of these traditional methods, their accuracy cannot continuously increase with data/model scale. On the other hand, deep neural networks are highly scalable in their learning capability when deeper network structures and larger data sets are adopted.   Practically, an operation can be computed faster in the hardware platform that is application-specifically designed and/or programmed. Hardware acceleration thus steps forward for heavy customization in processing capability by allowing great parallelism, having specific datapaths for temporal variants, and reducing the overhead of instruction control <ref type="bibr" target="#b97">[98]</ref>. For decades, hardware customization in the form of GPUs, FPGAs, and application-specific integrated circuits (ASICs) offer a promising path in trading flexibility for computation efficiency, as seen in Fig. <ref type="figure" target="#fig_3">5(b)</ref>. In this section, we review a number of popular hardware implementations including GPUs, FPGAs and other application-specific accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graphics processing units (GPUs)</head><p>GPUs were initially developed to accelerate graphics processing. A GPU is particularly designed for integrated transform, lighting, triangle setup/clipping, and rendering <ref type="bibr" target="#b99">[100]</ref>. A modern GPU is not only a powerful graphics engine but also a highly parallelized computing processor featuring high throughput and high memory bandwidth for massive parallel algorithms, which is dubbed as GPU computing or general-purpose computing on GPU (GPGPU).</p><p>In contrast to multicore CPUs that are typically out-of-order, multiinstructional, running at high frequencies and using large-size caches to minimize the latency of a single-thread, GPGPUs consist of thousands of cores that are in-order, operating at lower frequencies and relying on smaller-size caches. To create high performance GPU-accelerated applications with parallel programming, a variety of development platforms such as compute unified device architecture (CUDA) <ref type="bibr" target="#b100">[101]</ref> and open computing language (OpenCL) <ref type="bibr" target="#b101">[102]</ref>, are studied and utilized for GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and high-performance computing (HPC) servers.</p><p>A number of hardware vendors have produced GPUs. Among them, Intel, Nvidia and AMD/ATI have been the market share leaders <ref type="bibr" target="#b99">[100]</ref>. As shown in Fig. <ref type="figure">6</ref>, the evolution of GPGPUs began in 2007, when Nvidia released its CUDA development environment. A great variety of GPUs have been designed for a specific usage, such as Nvidia GeForce GTX and AMD Radeon HD GPUs for powerful gaming, and Nvidia Quadro and Titan X series for professional workstation <ref type="bibr" target="#b99">[100]</ref>. More recently, the emergence of deep learning technology ushers in significant advances in GPU computing. Taking CNN as an example, it can take advantages of the nature of algorithmic parallelism in the following aspects <ref type="bibr" target="#b102">[103]</ref>: (i) the convolution operation of an n Â n matrix using a k Â k kernel can be in parallel;</p><p>(ii) the subsampling/pooling operation can be parallelized by executing different pooling operations separately; (iii) the activation of each neuron in a fully connected layer can be parallelized by creating a binary-tree multiplier. With great parallel processing structures and strong floating-point capabilities, GPGPUs have been recognized to be a good fit to accelerate deep learning. A number of GPU-based CNN libraries have been developed to facilitate highly optimized CNN implementation on GPUs, including cuDNN <ref type="bibr" target="#b103">[104]</ref>, Cuda-convnet <ref type="bibr" target="#b104">[105]</ref> and several other libraries built upon the popular deep learning frameworks, such as Caffe <ref type="bibr" target="#b105">[106]</ref>, Torch <ref type="bibr" target="#b106">[107]</ref>, Tensorflow <ref type="bibr" target="#b107">[108]</ref>, etc. Computational throughput, power consumption and memory efficiency are three important metrics when implementing deep learning on GPUs. Fig. <ref type="figure">6</ref> summarizes the peak performance of recent Nvidia GPUs for single-precision floating-point (FP32) arithmetic measured by GFLOPs and power consumption gauged by Thermal Design Power (TDP). The GeForce 10 series, based on the most powerful GPU architecture "Nvidia Pascal", is a set of consumer graphics cards released by Nvidia in 2016 <ref type="bibr" target="#b109">[110]</ref>. With an inexpensive GeForce GTX 1060, composed of 1280 CUDA cores delivering 3855 GFLOPs in computational throughput, one can get into deep learning with affordable cost.</p><p>For professional usage, Titan V and Tesla V100 are much more powerful and scalable than the GeForce 10 series based on the Pascal and a new Volta architecture, which integrates CUDA cores with the new Tensor core technology. Tensor cores are especially designed for deep learning, which offer an extremely wide memory bus. Compared to CUDA cores, they improve the peak performance by up to 12 Â for training and up to 6 Â for inference. In addition to their high throughput, Tensor cores allow efficient computation with 16-bit word-length, implying that the amount of transferred data can be doubled over 32bit arithmetic with the same memory bandwidth.</p><p>Nvidia Jetson is a leading low-power embedded platform that enables server-grade computing performance on edge devices. Jetson TX2 is based on the 16 nm NVIDIA Tegra "Parker" system-on-a-chip (SoC), which delivers 1 TFLOPs of throughput in a credit-card-sized module. A new series of RTX gaming cards (i.e., RTX 2070/2080/2080Ti) with Turing architectures were unveiled in August 2018. They have Tensor cores on board and support unrestricted 16-bit floating-point (FP16), 8bit integer (INT8) and 4-bit integer (INT4) arithmetic. Among them, RTX 2080Ti offers the promising performance with more than 100 TFLOPs in FP16. Fig. <ref type="figure">6</ref>. Computational throughput in terms of GFLOPS and power consumption in terms of TDP (watts) for single-precision floating-point arithmetic <ref type="bibr" target="#b108">[109]</ref>.</p><p>When evaluating memory efficiency for GPUs, memory size and memory bandwidth are two important metrics. Today, it is common to have 11-12 GB memory on start-of-the-art gaming cards. Many Tesla GPUs have 16 GB memory, and several Turing architecture Quadro models have 24 GB memory (e.g. RTX 6000) or 64 GB memory (e.g. RTX 8000). For a given deep learning task, the peak performance of GPU is often far from the actual performance. In most practical applications, the throughput of a GPU is about 15-20% of its peak performance <ref type="bibr" target="#b110">[111]</ref>. It, in turn, implies that evaluating DNNs is actually limited by memory bandwidth, instead of computing power. Adopting GPUs with high memory bandwidth is a valid strategy to speed up both training and inference. For example, GTX Titan XP (10,709 FP32 GFLOPS and 548 GB/s) can be faster than GTX 1080Ti (10,609 FP32 GFLOPS and 484 GB/s) by up to 13% on bandwidth-limited tasks. GTX Titan V (652.8 GB/s), Tesla V100 (900 GB/s) and P100 (720 GB/s) are even faster than GTX Titan XP <ref type="bibr" target="#b108">[109]</ref>. In addition to Nvidia, AMD has also released its high-end Vega GPUs with high memory bandwidth similar to Titan V <ref type="bibr" target="#b99">[100]</ref>.</p><p>In order to address the fundamental issue of limited computing throughput and memory bandwidth, multi-GPU systems allow single machine and multi-GPUs or even distributed multi-system, multi-GPU configurations. For a system with single machine and multi-GPUs working on separate tasks, one can directly access any available GPU without coding in CUDA. On the other hand, for multi-GPUs working on shared tasks such as training several models with different hyperparameters, distributed training is needed. Nvidia has a collective communications library (NCCL) <ref type="bibr" target="#b111">[112]</ref> that implements multi-GPU and multi-node collective communication primitives to make full use of all GPUs within and across multi-nodes with maximum bandwidth. Distributed training is now supported by many popular deep learning frameworks such as Tensorflow, Caffe, etc. These techniques reduce computational time linearly with the number of GPUs <ref type="bibr" target="#b111">[112]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Field-programmable gate arrays (FPGAs)</head><p>While GPUs have been demonstrated to offer extremely high throughput and are broadly used for hardware acceleration of DNNs, they are often not preferred for energy/power-constrained applications, such as IoT devices, due to their high power consumption. DNN acceleration thus moves towards an alternative solution based on energyefficient FPGAs. FPGA allows us to implement irregular parallelism, customized data type and application-specific hardware architecture, offering great flexibility to accommodate the recent DNN models that are featured with increased sparsity and compact network structures. Further, FPGA can be reprogrammed after manufacturing for desired functions and applications. Due to these attractive features, a large number of FPGA-based accelerators have been proposed for both HPC data centers <ref type="bibr" target="#b112">[113]</ref> and embedded applications <ref type="bibr" target="#b113">[114]</ref>.</p><p>FPGA is a component that contains an array of programmable logic blocks connected via a hierarchy of reconfigurable interconnects. Today's FPGAs usually contain (i) digital signal processing units (DSPs) for multiply-add-accumulation (MAC) operations, (ii) lookup tables (LUTs) for combinatorial logic operations, and (iii) block RAMs for on-chip data storage <ref type="bibr" target="#b9">[10]</ref>. Fig. <ref type="figure">7</ref> shows a typical FPGA architecture to implement DNNs <ref type="bibr" target="#b114">[115]</ref>. It consists of memory data management (MDM) unit, on-chip data management (ODM) unit, general matrix multiply (GEMM) unit that is implemented by a set of processing elements (PEs) to perform one or more MAC operations, and msic-layers (MLU) unit where ReLU pooling and batch normalization are computed.</p><p>The FPGA architecture in Fig. <ref type="figure">7</ref> executes a DNN model by following several major steps. It first fetches DNN weights and input feature maps into an on-chip buffer (i.e. ODM) from MDM. Next, the GEMM unit performs matrix operations and transfers the outcomes to MLU for ReLU/ batch normalization/pooling. The output of MLU goes to another ODM unit and will be accessed by the subsequent convolutional and/or fully connected layers. If the on-chip buffer does not have sufficiently large capacity, the intermediate results must be temporarily stored in on-chip or off-chip memory.</p><p>Historically, an FPGA system is often specified at register-transfer level (RTL) by using hardware description language (HDL) such as Verilog or VHDL. This low-level design methodology needs substantial efforts and hardware expertise to carefully describe the detailed hardware architecture, including the massive concurrency between different hardware modules. Recently, high-level synthesis (HLS) tools have been successfully developed to facilitate efficient FPGA design by using highlevel programming language such as C and Cþþ, and automatically compile high-level description to generate low-level specification (i.e. HDL) <ref type="bibr" target="#b115">[116]</ref>. With the aforementioned synthesis flow, the design cost of FPGA accelerators can be significantly reduced. However, there is an important tradeoff between RTL and HLS approaches in terms of design cost and system performance.</p><p>In practice, a DNN model is often trained or fine-tuned on a highperformance computing platform such as GPU, while FPGA acceleration is implemented for DNN inference to process given input data based on the pre-trained DNN model. As illustrated in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>, computer vision algorithms based on DNNs are often associated with high computational workload (i.e., a large number of FLOPs) and large memory storage (i.e., a large number of network parameters), while the memory bandwidth of FPGAs is often less than 10% of that of GPUs. Hence, the grant challenge is to find an efficient mapping from the pretrained complex DNN model to the limited hardware resources (i.e., high-density logic and memory blocks) offered by FPGAs. Such a technical challenge has been tackled via hardware-friendly algorithmic optimizations, including: (i) algorithmic operation, (ii) data-path optimization and (iii) model compression.</p><p>Algorithmic operation: Computational transforms, such as GEMM, fast Fourier transform (FFT) and Winograd transform, may be applied to feature maps and/or convolutional kernels to reduce the number of arithmetic operations during inference. GEMM is a popular way of processing DNNs in CPUs and GPUs, which vectorizes the computation of convolutional and fully connected layers <ref type="bibr" target="#b116">[117]</ref>. FFT casts 2D convolution to element-wise matrix multiplication, thereby reducing the arithmetic complexity. It is highly efficient for large kernel size (&gt;5) because of the large number of convolutional operations between the feature maps and kernels <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b117">118]</ref>. For small kernels where FFT is not preferred, Winograd transform <ref type="bibr" target="#b118">[119]</ref> provides an alternative way to reduce the number of multiplications by reusing the intermediate results. It can offer 7.28 Â runtime speed-up compared to GEMM when running VGGNet on a Titan X GPU <ref type="bibr" target="#b117">[118]</ref>, and deliver the throughput of 46 GOPs when running AlexNet on FGPA <ref type="bibr" target="#b119">[120]</ref>.</p><p>Data-path optimization: In order to fully exploit the parallelism, data path is optimized by unrolling the convolutional layers in CNNs and mapping them onto a limit number of PEs. In early FPGA Fig. <ref type="figure">7</ref>. A typical FGPA architecture to implement DNNs <ref type="bibr" target="#b114">[115]</ref>.</p><p>implementations, PEs are arranged in a 2D grid as a systolic array <ref type="bibr" target="#b120">[121]</ref><ref type="bibr" target="#b121">[122]</ref><ref type="bibr" target="#b122">[123]</ref>. Because such a simple architecture limits the CNN kernel size and does not offer data caching, it cannot achieve extremely high performance. Recently, loop optimization techniques, including loop reordering, unrolling, pipelining and tiling, have been proposed to address the aforementioned issue. Loop reordering tries to prevent redundant memory access between loops to increase cache usage efficiency <ref type="bibr" target="#b123">[124]</ref>. Loop unrolling and pipelining maximize the utilization of FPGA resources by exploring the parallelism of loop iterations <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b124">125,</ref><ref type="bibr" target="#b125">126]</ref>. Loop tiling deals with the issue posed by insufficient on-chip memory of FPGAs. It partitions the feature maps and weights of each layer fetched from memory into chunks, also referred to as tiles, to fit them into on-chip buffers <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b124">125,</ref><ref type="bibr" target="#b126">127]</ref>.</p><p>Model compression: DNNs often carry a significant number of redundant parameters and are mainly used for error-tolerant applications. Hence, a lot of efforts have been made to simplify DNN models and, consequently, reduce the complexity of their hardware implementation. These model compression methods can be broadly classified into three different categories: (i) pruning <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b127">[128]</ref><ref type="bibr" target="#b128">[129]</ref><ref type="bibr" target="#b129">[130]</ref><ref type="bibr" target="#b130">[131]</ref>, (ii) low-rank approximation <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b131">132]</ref>, and (iii) quantization <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b132">[133]</ref><ref type="bibr" target="#b133">[134]</ref><ref type="bibr" target="#b134">[135]</ref><ref type="bibr" target="#b135">[136]</ref><ref type="bibr" target="#b136">[137]</ref>.</p><p>First, pruning is usually the first step to reduce model redundancy by removing the least-important connections and/or parameters. Taking CNN as an example, we can remove its weights that are extremely small <ref type="bibr" target="#b33">[34]</ref> and/or cause high energy consumption <ref type="bibr" target="#b129">[130]</ref>. After pruning, the CNN model is highly sparse and can be efficiently implemented with FPGA by masking the zero weights for multiplications. Second, low-rank approximation decomposes the weight matrix of a convolutional or fully connected layer to a set of low-rank filters that can be evaluated with low computational cost <ref type="bibr" target="#b113">[114]</ref>. Finally, because fixed-point arithmetic requires less computational resources than floating-point arithmetic, feature maps, weight matrices and/or convolutional kernels can be quantized by using a fixed-point representation to further reduce the computational cost.</p><p>A straightforward approach is to encode each numerical value with the desired word-length according to its range <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b120">121]</ref>. Alternatively, a dynamic scheme may be adopted to assign different scaling factors to different numerical parameters within the same network <ref type="bibr" target="#b132">[133,</ref><ref type="bibr" target="#b133">134]</ref>. When the dynamic quantization method is applied to both the convolutional and fully-connected layers of AlexNet without fine-tuning, the classification accuracy is almost unchanged (&lt;1%) <ref type="bibr" target="#b134">[135]</ref>.</p><p>In the extreme case, a DNN may use binary weights and activations, resulting in an extremely compact representation that is referred to as binary neural network (BNN) <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b132">133,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b136">137]</ref>. A BNN can be evaluated with extremely low computational cost, as binary addition and multiplication can both be implemented with simple logic gates. Other than BNN, a ternary DNN <ref type="bibr" target="#b137">[138]</ref> sets its weights to þ1, 0 or À1, allowing each weight to be represented by 2 bits. On the other hand, the numerical operations in all neurons are implemented with floating-point arithmetic (FP32).</p><p>Table <ref type="table" target="#tab_2">3</ref> summarizes the performance of several typical CNN models deployed on FPGA using different model optimization methods. CNN models based on quantized arithmetic are highly efficient in terms of hardware utilization and power consumption; however, their accuracy is often compromised. On the other hand, CNN models based on low-rank approximation (e.g. SVD) and pruning carry a smaller number of weights, while simultaneously achieving high classification accuracy. The ternary ResNet <ref type="bibr" target="#b114">[115]</ref>, implemented with Intel StratixTM 10 FPGA <ref type="bibr" target="#b139">[140]</ref>, achieves a throughput of 12 TGOP/s, outperforming the throughput of Titan X Pascal GPU by 10%.</p><p>To make a comprehensive comparison of the state-of-the-art hardware accelerators, we present the key performance metrics of several mainstream accelerators (i.e. CPU, GPU and FPGA) with different network models (i.e. VGG and ResNet) in Table 4 <ref type="bibr" target="#b140">[141]</ref>. The FPGA cluster in Table <ref type="table" target="#tab_3">4</ref> is composed of 15 FPGA chips, as described in Ref. <ref type="bibr" target="#b141">[142]</ref>. Two important observations can be made from the data in Table <ref type="table" target="#tab_3">4</ref>. First, the throughput of FPGA is substantially higher than that of CPU, but it is often lower than the throughput of GPU. Second, among FPGA, CPU and GPU, FPGA offers the highest energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Application-specific hardware accelerators</head><p>A typical computer system is often heterogeneous, composed of a  heterogeneous suit of processors, such as CPUs added with other dissimilar processors, to meet the specific computing requirement. Processors that complement CPUs are known as application-specific coprocessors. In addition to the notable coprocessors such as GPUs and FPGAs, there are several specialized hardware units in the form of either stand-alone devices or coprocessors that are particularly developed for deep learning and/or other AI applications.</p><p>Tensor processing unit (TPU) <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b143">144]</ref>, a customized ASIC developed by Google, is a stand-alone device specifically designed for neural networks and tailored for the Google Tensorflow framework <ref type="bibr" target="#b107">[108]</ref>. TPU targets a high volume of low-precision (e.g., 8-bit) arithmetic. It has already powered many applications at Google, such as the search engine and AlphaGo <ref type="bibr" target="#b143">[144]</ref>. Intel Nervana neural network processor (NNP) <ref type="bibr" target="#b144">[145]</ref> is designed to provide the required flexibility of deep learning primitives while making its core hardware components as efficient as possible. Mobileye EyeQ <ref type="bibr" target="#b145">[146]</ref> is a family of SoC devices specialized for vision processing in autonomous driving. It shows the ability of handling complex and computationally intensive vision tasks, while maintaining low power consumption.</p><p>Various AI coprocessors for mobile platforms are developed recently, where an arms race seems around the corner. The mobile processor Qualcomm Snapdragon 845 contains a Hexagon 685 DSP core that supports sophisticated, on-device AI processing in camera, voice and gaming applications <ref type="bibr" target="#b146">[147]</ref>. Imagination Technologies <ref type="bibr" target="#b147">[148]</ref> develops a series of neural network accelerators (NNAs), such as PowerVR Series 2NX/3NX NNA. They are intellectual property (IP) cores that are designed to deliver high performance computation and low power consumption for embedded and mobile devices. The new "neural engine" by Apple <ref type="bibr" target="#b148">[149]</ref>, incorporating Apple A11/A12 Bionic SoC, is a pair of processing cores dedicated for specific machine learning algorithms, including Face ID, augmented reality, etc. HiSilicon Kirin 970 is the first mobile AI platform developed by HUAWEI <ref type="bibr" target="#b149">[150]</ref>. With a dedicated neural processing unit (NPU), its new heterogeneous computing architecture improves the throughput and energy efficiency by up to 25 Â and 50 Â respectively over a quad-core Cortex-A73 CPU cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>As a scientific discipline, computer vision has been a challenging research area and received significant attention. With the emergence of big data, advanced deep learning algorithms and powerful hardware accelerators, modern computer vision systems have dramatically evolved. In this paper, we conduct a comprehensive survey on computer vision techniques. Specially, we have highlighted the recent accomplishments in both the algorithms for a variety of computer vision tasks such as image classification, object detection and image segmentation, and the promising hardware platforms to implement DNNs efficiently for practical applications, such as GPUs, FPGAs and other new generation of hardware accelerators.</p><p>In the future, increasingly compact and efficient DNNs are needed for real-time and embedded applications. In addition, weakly supervised or unsupervised DNN schemes must be investigated to perceive all object categories in all open world scenes. Furthermore, highly energy-efficient hardware engines are required to extend the existing accelerators to a broad spectrum of challenging scenarios. To address the aforementioned grand challenges, massive innovations of computer vision systems, in terms of both algorithm developments and hardware designs, are expected over the next five or even ten years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Object detection accuracy on (a) Pascal-VOC 2007 and (b) MS-COCO for different object detection techniques based on deep learning.</figDesc><graphic coords="4,56.54,157.13,177.94,94.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of different visual perception problems [75]: (a) image classification, (b) object detection, (c) semantic segmentation, and (d) instance segmentation.</figDesc><graphic coords="5,127.22,182.75,136.62,114.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. mIOU for different semantic segmentation methods on Pascal-VOC 2012.</figDesc><graphic coords="6,56.54,593.21,177.94,92.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Comparison between neural networks and other approaches in terms of accuracy and scale (i.e. data/model size) [96], and (b) trade-off between flexibility and efficiency for different hardware implementations [97].</figDesc><graphic coords="6,467.06,546.35,71.01,152.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The receptive field of (a) 1-dilated convolution, (b) 2-dilated convolution, and (c) 4-dilated convolution [88].</figDesc><graphic coords="6,120.14,55.13,177.66,95.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of different CNN models on ImageNet classification task.</figDesc><table><row><cell>Model</cell><cell>Time</cell><cell>Accuracy</cell><cell>Num. of</cell><cell>Num. of</cell><cell>Num. of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Parameters</cell><cell>FLOPs</cell><cell>Layers</cell></row><row><cell>AlexNet [21]</cell><cell>2012</cell><cell>57.2%</cell><cell>60 M</cell><cell>720 M</cell><cell>8</cell></row><row><cell>VGGNet [22]</cell><cell>2014</cell><cell>71.5%</cell><cell>138 M</cell><cell>15,300 M</cell><cell>16</cell></row><row><cell>GoogleNet</cell><cell>2014</cell><cell>69.8%</cell><cell>6.8 M</cell><cell>1,500 M</cell><cell>22</cell></row><row><cell>[23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet [24]</cell><cell>2015</cell><cell>78.6%</cell><cell>55 M</cell><cell>2,300 M</cell><cell>152</cell></row><row><cell>DenseNet</cell><cell>2017</cell><cell>79.2%</cell><cell>25.6 M</cell><cell>1,150 M</cell><cell>190</cell></row><row><cell>[25]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SENet [26]</cell><cell>2017</cell><cell>82.7%</cell><cell>145.8 M</cell><cell>42,300 M</cell><cell>-</cell></row><row><cell>NASNet [27]</cell><cell>2018</cell><cell>82.7%</cell><cell>88.9 M</cell><cell>23,800 M</cell><cell>-</cell></row><row><cell>SqueezeNet</cell><cell>2016</cell><cell>57.5%</cell><cell>1.2 M</cell><cell>833 M</cell><cell>-</cell></row><row><cell>[29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MobileNet</cell><cell>2017</cell><cell>70.6%</cell><cell>4.2 M</cell><cell>569 M</cell><cell>28</cell></row><row><cell>[30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShuffleNet</cell><cell>2018</cell><cell>73.7%</cell><cell>4.7 M</cell><cell>524 M</cell><cell>-</cell></row><row><cell>[31]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShiftNet-A</cell><cell>2018</cell><cell>70.1%</cell><cell>4.1 M</cell><cell>1,400 M</cell><cell>-</cell></row><row><cell>[32]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FE-Net [33]</cell><cell>2019</cell><cell>75.0%</cell><cell>5.9 M</cell><cell>563 M</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Summary of different object detection architectures (FPS based on a Pascal Titan X GPU).</figDesc><table><row><cell>Architecture</cell><cell>mAP (Pascal-VOC</cell><cell>mAP (MS-</cell><cell>Num. of</cell><cell>FPS</cell></row><row><cell></cell><cell>2007)</cell><cell>COCO)</cell><cell>FLOPs</cell><cell></cell></row><row><cell>R-CNN [54]</cell><cell>66.0%</cell><cell>-</cell><cell>-</cell><cell>0.1</cell></row><row><cell>SPPnet [56]</cell><cell>63.1%</cell><cell>-</cell><cell>-</cell><cell>1</cell></row><row><cell>Fast R-CNN [57]</cell><cell>70.0%</cell><cell>35.9%</cell><cell>-</cell><cell>0.5</cell></row><row><cell>Faster R-CNN</cell><cell>73.2%</cell><cell>36.2%</cell><cell>-</cell><cell>6</cell></row><row><cell>[58]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [59]</cell><cell>-</cell><cell>39.8%</cell><cell>-</cell><cell>3.3</cell></row><row><cell>YOLO [60]</cell><cell>63.4%</cell><cell>-</cell><cell>-</cell><cell>45</cell></row><row><cell>SSD513 [61]</cell><cell>76.8%</cell><cell>31.2%</cell><cell>-</cell><cell>8</cell></row><row><cell>YOLOv2 (608)</cell><cell>78.6%</cell><cell>21.6%</cell><cell>62.94 G</cell><cell>67</cell></row><row><cell>[62]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLOv3 (416)</cell><cell>-</cell><cell>33.0%</cell><cell>65.86 G</cell><cell>35</cell></row><row><cell>[63]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Performance comparison of FPGA-based CNN accelerators.</figDesc><table><row><cell>CNN Model</cell><cell>FPGA Device</cell><cell>Optimization</cell><cell>Accuracy</cell><cell># of Param</cell><cell>Computation</cell><cell>Precision</cell><cell>Frequency</cell><cell>Throughput</cell><cell>Power</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell>(Top-5)</cell><cell>(M)</cell><cell>(GOP)</cell><cell></cell><cell>(MHz)</cell><cell>(GOP/s)</cell><cell>(W)</cell></row><row><cell>VGG-19 [139]</cell><cell>Arria10</cell><cell>-</cell><cell>90.1%</cell><cell>138</cell><cell>30.8</cell><cell>float32</cell><cell>370</cell><cell>866</cell><cell>41.7</cell></row><row><cell></cell><cell>GX1150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG-16 [114]</cell><cell>Zynq 7Z045</cell><cell>SVD</cell><cell>87.96%</cell><cell>50.2</cell><cell>30.5</cell><cell>fixed16</cell><cell>150</cell><cell>137</cell><cell>9.6</cell></row><row><cell>VGG-16 [134]</cell><cell>Arria10</cell><cell>Dynamic</cell><cell>88.1%</cell><cell>138</cell><cell>30.8</cell><cell>fixed8</cell><cell>150</cell><cell>645</cell><cell>-</cell></row><row><cell></cell><cell>GX1150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BNN: XNOR-Net</cell><cell>Stratix5</cell><cell>Binary</cell><cell>66.8%</cell><cell>87.1</cell><cell>2.3</cell><cell>fixed1</cell><cell>150</cell><cell>1,964</cell><cell>26.2</cell></row><row><cell>[137]</cell><cell>GSD8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ternary ResNet</cell><cell>Stratix10</cell><cell>Ternary, pruning</cell><cell>79.7%</cell><cell>61</cell><cell>1.4</cell><cell>float32</cell><cell>500</cell><cell>12,000</cell><cell>141.2</cell></row><row><cell>[115]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Performance comparison of neural networks on difference hardware platforms<ref type="bibr" target="#b140">[141]</ref>.</figDesc><table><row><cell>Model</cell><cell>Platform</cell><cell>Device</cell><cell>Precision</cell><cell>Frequency. (MHz)</cell><cell>Throughput (GOP/s)</cell><cell>Energy Efficiency (GOP/j)</cell><cell>Power (W)</cell></row><row><cell>VGG-19</cell><cell>CPU</cell><cell>Xeon E5-2650v2</cell><cell>float32</cell><cell>2,600</cell><cell>119</cell><cell>0.63</cell><cell>95</cell></row><row><cell></cell><cell>GPU</cell><cell>GTX TITAN X</cell><cell>float32</cell><cell>1,002</cell><cell>1,704</cell><cell>6.82</cell><cell>250</cell></row><row><cell></cell><cell>Cluster w/15 FPGAs</cell><cell>XC7 VX690T</cell><cell>fixed16</cell><cell>-</cell><cell>1,220 a</cell><cell>38.13</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>Cluster w/15 FPGAs</cell><cell>XC7 VX690T</cell><cell>fixed16</cell><cell>-</cell><cell>1,197 a</cell><cell>37.88</cell><cell>-</cell></row><row><cell></cell><cell>FPGA</cell><cell>Stratix-V GSD8</cell><cell>fixed8</cell><cell>120</cell><cell>117.8</cell><cell>19.1</cell><cell>-</cell></row><row><cell>ResNet-152</cell><cell>CPU</cell><cell>Xeon E5-2650v2</cell><cell>float32</cell><cell>2,600</cell><cell>119</cell><cell>0.63</cell><cell>95</cell></row><row><cell></cell><cell>GPU</cell><cell>GTX TITAN</cell><cell>float32</cell><cell>1,002</cell><cell>1,661</cell><cell>0.60</cell><cell>250</cell></row><row><cell></cell><cell>FPGA</cell><cell>Stratix 10 GX 2800</cell><cell>fixed8/16</cell><cell>300</cell><cell>789.44</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FPGA</cell><cell>Arria10 GX1150</cell><cell>fixed8/16</cell><cell>240</cell><cell>697.09</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FPGA</cell><cell>Arria10 GX1150</cell><cell>float16</cell><cell>150</cell><cell>315.5</cell><cell>-</cell><cell>-</cell></row><row><cell>ReNet-50</cell><cell>FPGA</cell><cell>Arria10 GX1150</cell><cell>float16</cell><cell>150</cell><cell>285.07</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FPGA</cell><cell>Arria10 GX1150</cell><cell>fixed8/16</cell><cell>240</cell><cell>599.61</cell><cell>-</cell><cell>-</cell></row></table><note><p>a Measured throughput value of a single FPGA.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Kobielus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Powering</surname></persName>
		</author>
		<ptr target="https://www.infoworld.com/article/3290104/powering-ai-the-explosion-of-new-ai-hardware-accelerators.html" />
		<title level="m">Explosion of New AI Hardware Accelerator, InfoWord</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/Computer_vision" />
		<title level="m">Computer vision, WikiPedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computer Vision: Algorithms and Applications</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</title>
		<author>
			<persName><forename type="first">Wilson</forename><surname>Geisler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Psyccritiques</publisher>
			<biblScope unit="page" from="581" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">Laura</forename><surname>Mcclure</surname></persName>
		</author>
		<ptr target="https://blog.ted.com/building-an-ai-with-the-intelligence-of-a-toddler-fei-fei-li" />
	</analytic>
	<monogr>
		<title level="m">Building an AI with the Intelligence of a Toddler: Fei-Fei Li at TED2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on image classification and activity recognition using deep convolutional neural network architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sornam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kavitha Muthusubash</surname></persName>
		</author>
		<author>
			<persName><surname>Vanitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02165</idno>
		<title level="m">Deep Learning for Generic Object Detection: a Survey</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
	</analytic>
	<monogr>
		<title level="j">A Review on Deep Learning Techniques Applied to Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of FPGA-based accelerators for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning Multiple Layers of Features from Tiny Images</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patros</forename><surname>Maragos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support-vector Networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Jorge S Anchez</surname></persName>
		</author>
		<author>
			<persName><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gang Sun, Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-Level Accuracy with 50x Fewer Parameters and &lt; 0.5 Mb Model Size</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ShuffleNet: an extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shift: a zero flop, zero parameter alternative to spatial convolutions</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">All you need is a few shifts: designing efficient convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: a deep learning approach</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">YFCC100M: the New Data in Multimedia Research</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: stanford dogs</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
	</analytic>
	<monogr>
		<title level="m">dataset of 101 human actions classes from videos in the wild</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="https://trecvid.nist.gov/" />
		<title level="m">TREC Video Retrieval Evaluation Multimedia event detection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Trailer generation via a point process-based visual attractiveness model</title>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiros</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<title level="m">Action Recognition Using Visual Attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video LSTM convolves, attends and flows for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">Viola</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName><forename type="first">Abdenour</forename><surname>Timo Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Microsoft coco: common objects in context, in: European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shao Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll Ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">You only look once: unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Redmon</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">Redmon</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhadi</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Redmon</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhadi</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: an Incremental Improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName><surname>Hu Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SOD-MTGAN: small object detection via multi-task generative adversarial network</title>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Finding tiny faces in the wild with generative adversarial network</title>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking with a deep association metric</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Flow-Guided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks_2017/Imagenet2017VID.pdf" />
		<title level="m">Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Picture segmentation using a recursive region splitting method</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Ohlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Raj</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Image Process</title>
		<imprint>
			<biblScope unit="page" from="313" to="333" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">User-friendly interactive image segmentation through unified combinatorial user inputs</title>
		<author>
			<persName><forename type="first">Wenxian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="2470" to="2479" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
	</analytic>
	<monogr>
		<title level="j">A Review on Deep Learning Techniques Applied to Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: a retrospective</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Method: automatic segmentation of mitochondria utilizing patch classification, contour pair classification, and automatically seeded level sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giuly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ellisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deeporgan: multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Segnet: a deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">RefineNet: multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><surname>Liang Chieh Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<biblScope unit="page" from="357" to="361" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks, in: International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale Context Aggregation by Dilated Convolutions</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Xception: deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll Ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="1990">2015. 1990-1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Keynote: Recent Advances in Artificial Intelligence via Machine Learning and the Implications for Computer System Design</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Hotchips</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Advanced computer architecture: digital signal processing (DSP): architecture &amp; processors</title>
		<author>
			<persName><surname>Shaaban</surname></persName>
		</author>
		<ptr target="http://meseec.ce.rit.edu/eecc722-fall2012/722-10-10-2012.pdf" />
	</analytic>
	<monogr>
		<title level="j">Lecture EECC</title>
		<imprint>
			<biblScope unit="volume">722</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Hardware acceleration, WikiPedia</title>
		<ptr target="https://en.wikipedia.org/wiki/Hardware_acceleration" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A survey of CPU-GPU heterogeneous computing techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Graphics processing unit, WikiPedia</title>
		<ptr target="https://en.wikipedia.org/wiki/Graphics_processing_unit" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Parallel computing experiences with CUDA</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="page" from="13" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">OpenCL: a parallel programming standard for heterogeneous computing systems</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">E</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Hardware Acceleration of Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Halvorsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Norwegian University of Science Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">MS thesis</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">CUDNN: Efficient Primitives for Deep Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Cudaconvet2</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://code.google.com/archive/p/cuda-convnet2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Caffe: convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Cl ement Farabet, Torch7: a matlab-like environment for machine learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing System Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Hardware for Deep Learning, Intento</title>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Sapunov</surname></persName>
		</author>
		<ptr target="https://blog.inten.to/hardware-for-deep-learning-current-state-and-trends-51" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Online. c 01ebbb6dc</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pascal Gpu Architecture</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Computation and Memory Bandwidth in Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<ptr target="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>A Medium Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<ptr target="https://developer.nvidia.com/nccl" />
		<title level="m">NVIDIA Collective Communications Library (NCCL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Accelerating Deep Convolutional Neural Networks Using Specialized Hardware</title>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Microsoft Research Whitepaper</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Going deeper with embedded fpga platform for convolutional neural network</title>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Can FPGAs beat GPUs in accelerating next-generation deep neural networks?</title>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Nurvitadhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Accelerating binarized convolutional neural networks with software-programmable FPGAs</title>
		<author>
			<persName><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">CLCAFFE: OpenCL accelerated CAFFE for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bottleson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Parallel and Distributed Processing Symposium Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Arithmetic Complexity of Computations</title>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathmatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dicecco</surname></persName>
		</author>
		<title level="m">Caffeinated FPGAs: FPGA Framework for Convolutional Neural Networks, Field-Programmable Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">A Massively Parallel Coprocessor for Convolutional Neural Networks, Application-specific Systems</title>
		<author>
			<persName><forename type="first">Murugan</forename><surname>Sankaradas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Architectures and Processors</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A dynamically configurable coprocessor for convolutional neural networks</title>
		<author>
			<persName><surname>Srimat Chakradhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Architect. News</title>
		<imprint>
			<biblScope unit="page" from="247" to="257" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Neuflow: a runtime reconfigurable dataflow processor for vision</title>
		<author>
			<persName><surname>Cl Ement Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">Atul</forename><surname>Rahman</surname></persName>
		</author>
		<title level="m">Design Space Exploration of FPGA Accelerators for Convolutional Neural Networks, Design, Automation &amp; Test in Europe</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A GPU-outperforming FPGA accelerator architecture for binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Emerg. Technol. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Loop tiling for reconfigurable accelerators</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Derrien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Sparse convolutional neural networks, in: Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">High-performance video content recognition with long-term recurrent convolutional network for FPGA</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Designing energy-efficient convolutional neural networks using energy-aware pruning</title>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">SPARCNet: a hardware accelerator for efficient deployment of sparse convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Emerg. Technol. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Learning separable filters</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Rigamonti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7024</idno>
		<title level="m">Training Deep Neural Networks with Low Precision Multiplications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained Toþ 1 Or-1</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FP-BNN: binarized neural network on FPGA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1072" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Fengfu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
	</analytic>
	<monogr>
		<title level="j">Ternary Weight Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Improving the performance of OpenCL-based FPGA accelerator for convolutional neural network</title>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Intel® Stratix® 10 Variable Precision DSP Blocks User Guide</title>
		<author>
			<persName><forename type="first">Fpga</forename><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Intel FPGA Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">Teng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04988</idno>
		<title level="m">A Survey of FPGA Based Deep Learning Accelerators: Challenges and Opportunities</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A framework for acceleration of CNN training on deeplypipelined FPGA clusters with work and weight load balancing</title>
		<author>
			<persName><surname>Tong Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Google&apos;s Tensor Processing Unit Explained: This Is what the Future Computing Looks like</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Osborne</surname></persName>
		</author>
		<ptr target="https://www.techradar.com/news/computing-components/processors/google-s-tensor-processing-unit-explained-this-is-what-the-future-of-computing-looks-like-1326915" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">TechRadar</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Google Supercharges Machine Learning Tasks with TPU Custom Chip</title>
		<author>
			<persName><forename type="first">Norm</forename><surname>Jouppi</surname></persName>
			<affiliation>
				<orgName type="collaboration">Google Could</orgName>
			</affiliation>
		</author>
		<ptr target="https://cloud.google.com/blog/products/gcp/google-supercharges-machine-learning-tasks-with-custom-chip" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Rao</surname></persName>
		</author>
		<ptr target="https://www.intel.ai/intel-nervana-neural-network-processors-nnp-redefine-ai-silicon/#gs.1s250i" />
		<title level="m">Intel Nervana Neural Network Processor (NNP) Redefine AI Silicon, Intel Website</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">The Evolution of EyeQ, Mobileye website</title>
		<author>
			<persName><surname>Mobileye</surname></persName>
		</author>
		<ptr target="https://www.mobileye.com/our-technology/evolution-eyeq-chip/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<author>
			<persName><surname>Qualcomm</surname></persName>
		</author>
		<ptr target="https://www.qualcomm.com/products/snapdragon-845-mobile-platform" />
		<title level="m">Snapdragon 845 Mobile Platform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Imagination Technology</title>
		<ptr target="https://www.imgtec.com/vision-ai/" />
	</analytic>
	<monogr>
		<title level="m">PowerVR vision &amp; AI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The iPhone X&apos;s New Neural Engine Exemplifies Apple&apos;s Approach to AI</title>
		<author>
			<persName><forename type="first">James</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="https://www.theverge.com/2017/9/13/16300464/apple-iphone-x-ai-neural-engine" />
	</analytic>
	<monogr>
		<title level="m">The Verge</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Huawei</surname></persName>
		</author>
		<ptr target="https://www.huawei.com/en/press-events/news/2017/9/mobile-ai-ifa-2017" />
		<title level="m">Reveals the Future of Mobile AI and IFA 2017, HUAWEI website</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
