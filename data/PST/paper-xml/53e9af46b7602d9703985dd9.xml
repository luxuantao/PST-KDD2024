<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparing Discriminating Transformations and SVM for Learning during Multimedia Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sean</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana</orgName>
								<address>
									<addrLine>Champaign 405 N Mathews Ave</addrLine>
									<postCode>61801 1-217-244-2960</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
							<email>xzhou2@ifp.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana</orgName>
								<address>
									<addrLine>Champaign 405 N Mathews Ave</addrLine>
									<postCode>61801 1-217-244-2960</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Huang</surname></persName>
							<email>huang@ifp.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana</orgName>
								<address>
									<addrLine>Champaign 405 N Mathews Ave</addrLine>
									<postCode>61801 1-217-244-2960</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparing Discriminating Transformations and SVM for Learning during Multimedia Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CAA17752C8C7CD1C6F42945A12412A21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval -Relevance feedback</term>
					<term>Query formulation</term>
					<term>Retrieval models</term>
					<term>Search Process Algorithms, Design, Experimentation, Human Factors, Theory Multimedia retrieval, relevance feedback, discriminating transform, support vector machine, kernel method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On-line learning or "relevance feedback" techniques for multimedia information retrieval have been explored from many different points of view: from early heuristic-based feature weighting schemes to recently proposed optimal learning algorithms, probabilistic/Bayesian learning algorithms, boosting techniques, discriminant-EM algorithm, support vector machine, and other kernel-based learning machines. Based on a careful examination of the problem and a detailed analysis of the existing solutions, we propose several discriminating transforms as the learning machine during the user interaction. We argue that relevance feedback problem is best represented as a biased classification problem, or a (1+x)-class classification problem. Biased Discriminant Transform (BDT) is shown to outperform all the others. A kernel form is proposed to capture non-linearity in the class distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The machine-aided retrieval of multimedia information-audio <ref type="bibr" target="#b31">[32]</ref>, image <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b25">[26]</ref>, or video <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b15">[16]</ref>, etc.-is achieved based on representations in the form of descriptors (or feature vectors), i.e., a set of real numbers. Two issues arise: one is the effectiveness of the representation, i.e., to what extent can the meaningful contents of the media be represented in these vectors? The other is the selection of similarity metric during the retrieval process. The latter is an important issue because the similarity metric dynamically depends upon the query class, which is unknown a priori, and can be user dependent and time varying, thus needs to be learned on-line through user interactions. In this paper, we focus our attention on the similarity metric issue, i.e., the on-line learning algorithms for content-based multimedia information retrieval.</p><p>The difference between content-based multimedia retrieval and traditional textual information retrieval lies in the fact that multimedia retrieval is conducted by the machine in a continuous feature space, while text or keyword-based retrieval is primarily performed in the discrete vector space of words; as a result, multimedia retrieval is inherently a nearest neighbor or a top-k ranking problem, while traditional keyword-based retrieval usually makes a binary "hit-or-miss" decision based on the occurrences of the keyword queries, although some rule-based ranking is possible.</p><p>For the purpose of quantitative analysis, we impose the assumption that the features selected in this paper possess adequate discriminating power to support the "ground-truth" classification in the user's mind. Note that in reality this is a strong assumption since it is very difficult to find a set of adequate features to represent high-level concepts and this is still an active research area. (Imagine the query for a music or video segment that "conveys excitement", or the query for a face picture that Figure <ref type="figure">1</ref> "A picture is worth a thousand words": different users at different times can be interested in either the "horse silhouette", the "sunset", or the overall artistic layoutâ€¦ ACM Multimedia 2001, Ottawa, Canada. Oral Presentation "looks stupid"-it is hardly imaginable that robust numerical descriptors even exist for such high-level and subtle concepts in the human minds.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Need for On-line Learning</head><p>Even if we assume that consensus interpretation of multimedia contents can be reached among all possible users at all times (-"universal classification assumption"), learning of similarity metrics is still desirable since different scenarios call for different similarity metrics-For example, "cars" are similar to each other more or less in terms of "shape", while "sunset" images are best discriminated from others by "color". Therefore a query for "cars" should be handled in a different way from that of the query for "sunsets", emphasizing different discriminating subspace of the original feature space. However, under this "universal classification assumption", which may hold for some applications such as medical image databases with specific, well-defined functionalities, off-line pre-clustering or learning may be feasible or even beneficial.</p><p>But in general, on-line learning with user in the loop is indispensable, because an inherent nature of multimedia information is its varying interpretations by different users at different times. In other words, the perceptual "similarity" depends upon the application, the user, and the context of usage. A piece of music can invoke different feelings in different people at different times; and "a picture is worth a thousand words" (Figure <ref type="figure">1</ref>). The time-varying interpretation or classification of multimedia information can only be dealt with using real-time learning algorithms.</p><p>Early CBIR systems invited the user into the loop by asking the user to provide a feature-weighting scheme for each retrieval task. This proved to be too technical and a formidable burden on the user's side. A more natural and friendlier way of getting user in the loop is to ask the user to give feedbacks regarding the relevance of the current outputs of the system. This is referred to as "relevance feedback" techniques ( <ref type="bibr" target="#b18">[19]</ref>[21] <ref type="bibr" target="#b11">[12]</ref>[25] <ref type="bibr" target="#b28">[29]</ref>[34], etc). Though this is an idea initiated in the text retrieval field <ref type="bibr" target="#b22">[23]</ref>, it seems to work even better in multimedia domain: it is easier to tell the relevance of an image or video segment than that of a text document-it takes time to read through a document while an image reveals its content instantly.</p><p>In CBIR systems on-line learning techniques or relevance feedback algorithms have been shown to provide dramatic performance boost <ref type="bibr" target="#b18">[19]</ref>[21] <ref type="bibr" target="#b11">[12]</ref>[29] <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Problem Statement</head><p>Since different types of multimedia information can be represented in the same form of feature vectors, media type becomes transparent to the machine. In this paper, we assume that each meaningful "unit" of information is represented by one feature vector. For images, the "unit" can be the whole image, image blocks, or segmented regions; and for videos, the "unit" can be shots, frames, or key frames, depending upon the application scenarios.</p><p>In the abstraction of the feature space, each "unit" of multimedia data becomes a point. Relevance feedback becomes a supervised classification problem, or an on-line learning problem in a batch mode, but with some unique characteristics. The uniqueness lies in at least three aspects: First, the machine needs to learn and respond in real time. In this paper, we target the multimedia information systems in which the similarity among the data points is dynamically determined by the current user for the current task. Therefore real-time response is critical.</p><p>Second, the number of training samples is very small relative to the dimension of the feature space, and to the requirements by popular learning machines such as support vector machines (SVM) <ref type="bibr" target="#b29">[30]</ref>. It should be underscored that the class densities, especially that of the negative examples, cannot be reliably modeled with such small sample size.</p><p>Third, the desired output is not necessarily a binary decision on each point, but rather a rank-ordered top-k returns. This is a less demanding task since the user actually does not care the rank or configuration of the negative points as long as they are far beyond the top-k returns. In fact algorithms targeting at binary classification is ill-fitted to this problem and performs poorly. This will be illustrated in details in subsequent sections.</p><p>In this paper, we use the phrase "relevance feedback" to denote the on-line learning process during multimedia information retrieval, based on the relevance judgments fed-back by the user. The procedure is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>Machine provides initial retrieval results, through query-bykeyword, or example, etc.; Then, iteratively:</p><p>â€¢ User provides judgment on the current results as to whether, and to what degree, they are relevant to her/his request;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>The machine learns and tries again.</p><p>In this paper, we designate the learning task as the learning of a discriminating subspace from the limited number of examples provided by the user in an interactive fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">STATE OF THE ART</head><p>Among different media types, on-line learning during image retrieval is the most active in recent years. We give a brief review of the state-of-the-art in relevance feedback techniques in the context of image retrieval. Again, many of these techniques are directly applicable for the retrieval of other media types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Variants</head><p>Before we get into the details of various techniques, the reader should note that under the same notion of "relevance feedback", different methods might have been developed under different assumptions or problem settings thus not comparable. The following lists some of the conceptual dimensions along which some methods greatly differ from others:</p><p>a. What is the user looking for? Some assumes the user is looking for "a particular target item" <ref type="bibr" target="#b5">[6]</ref>, while many others assume the user is looking for "similar" item to the query at hand <ref type="bibr">[11][12]</ref>[21] <ref type="bibr" target="#b21">[22]</ref>.</p><p>b. What to feedback? Some algorithm assumes the user will give a binary feedback for positive and negative examples <ref type="bibr" target="#b28">[29]</ref>; some only takes positive examples <ref type="bibr" target="#b11">[12]</ref>; some takes positive and negative examples with "degree of (ir)relevance" for each <ref type="bibr" target="#b20">[21]</ref>; some assumes the feedback is only a comparative judgment, i.e., the positive examples are not necessarily "relevant" to the target, but "more like the target than the negative ones" <ref type="bibr" target="#b5">[6]</ref>. The latter can be related to "query refinement" techniques in others <ref type="bibr" target="#b13">[14]</ref>.</p><p>c. Feature representation While most assume one feature vector per image/region, some extract features from image blocks <ref type="bibr" target="#b35">[36]</ref> and use mixture models as the representation <ref type="bibr" target="#b30">[31]</ref>. A Bayesian framework is then applicable for relevance feedback. Image local matching is possible given that meaningful local features can be differentiated in the mixture <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d. Class distribution</head><p>Another issue is what assumption to be imposed on the target class(es). Gaussian assumption is the most common and convenient one <ref type="bibr" target="#b11">[12]</ref>. However, recent kernel based algorithms can deal with non-linearity in an elegant way <ref type="bibr" target="#b3">[4]</ref>.</p><p>e. Data organization If a hierarchical tree structure is adopted in the database for more efficient access <ref type="bibr" target="#b0">[1]</ref>, the learning becomes more difficult since the tree-structure needs to be updated in real time. The trade-off offered by <ref type="bibr" target="#b0">[1]</ref> between the speed and accuracy in searching becomes crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>f. What to learn and how?</head><p>A majority of the work proposes to learn a new query and the relative importance of different features <ref type="bibr" target="#b17">[18]</ref>[21] <ref type="bibr" target="#b23">[24]</ref>, with some tries to learn a linear transformation in the feature space either with or without considering correlations among feature components <ref type="bibr" target="#b11">[12]</ref>[22] <ref type="bibr" target="#b20">[21]</ref>. While others treat it either as a learning <ref type="bibr" target="#b30">[31]</ref>[34], classification <ref type="bibr">[29][38]</ref>, or a density estimation <ref type="bibr" target="#b3">[4]</ref>[14] problem.</p><p>In the following section we discuss some of the major developments in relevance feedback techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Developments</head><p>In its short history, relevance feedback developed along the path from heuristic based techniques to optimal learning algorithms, with early work inspired by term-weighting and relevance feedback techniques in document retrieval <ref type="bibr" target="#b22">[23]</ref>. These methods proposed heuristic formulation with empirical parameter adjustment, mainly along the line of independent axis weighting in the feature space <ref type="bibr" target="#b18">[19]</ref>[21] <ref type="bibr" target="#b17">[18]</ref>[20] <ref type="bibr" target="#b23">[24]</ref>. The intuition is to emphasize more on the feature(s) that best clusters the positive examples and separates the positive and the negative.</p><p>Early works <ref type="bibr">[19][21]</ref> have clear birthmarks from document retrieval field. For example, In <ref type="bibr" target="#b20">[21]</ref>, learning based on "term frequency" and "inverse document frequency" in text domain is transformed into learning based on the ranks of the positive and negative images along each feature axis in the continuous feature space. <ref type="bibr" target="#b18">[19]</ref> quantizes the features and then groups the images or regions into hierarchical trees whose nodes are constructed through single-link clustering. Then weighting on groupings is based on "set operations".</p><p>Some use Kohonen's Learning Vector Quantization (LVQ) algorithm <ref type="bibr" target="#b32">[33]</ref> or Self-organizing Map (SOM) <ref type="bibr" target="#b12">[13]</ref> for dynamic data clustering during relevance feedback. Laaksonen et al. <ref type="bibr" target="#b12">[13]</ref> uses TS(Tree-Structured)-SOMs to index the images along different features. Positive and negative examples are mapped to positive and negative impulses on the maps and a low-pass operation on the maps is argued to implicitly reveal the relative importance of different features because a "good" map will keep positive examples cluster while negative examples scatter away. This is based on similar intuition as that of <ref type="bibr" target="#b17">[18]</ref>, where a probabilistic method is used to capture feature relevance.</p><p>Aside from their lack of optimality claim, the assumption of feature independence imposed in these methods is also artificial, unless independent components can be effectively extracted.</p><p>Later on researchers begin to look at this problem from a more systematic point of view by formulating it into an optimization, learning, classification, or density estimation problem. In <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b21">[22]</ref>, based on the minimization of total distances of positive examples from the new query, the optimal solutions turn out to be the weighted average as the new query and a whitening transform in the feature space (equivalent to principle component analysis (PCA) or the use of Mahalanobis distance). Additionally, Rui and Huang <ref type="bibr" target="#b21">[22]</ref> adopts a two-level weighting scheme to better cope with singularity issue due to the small number of training samples.</p><p>To take into account the negative examples, Schettini et al. <ref type="bibr" target="#b24">[25]</ref> updates the feature weights along each feature axis by comparing the variance of positive examples to the variance of the union of positive and negative examples.</p><p>Assuming that the user is searching for a particular target, and the feedback is in the form of "relative judgment", Cox et al. <ref type="bibr" target="#b5">[6]</ref> proposes the stochastic comparison search as its relevance feedback algorithm.</p><p>While most CBIR systems use well-established image features such as color histogram/moments, texture, shape, and structure features, there are alternatives. Tieu and Viola <ref type="bibr" target="#b28">[29]</ref> used more than 45,000 "highly selective features", and a boosting technique to learn a classification function in this feature space. The features were demonstrated to be sparse with high kurtosis, and were argued to be expressive for high-level semantic concepts. Weak 2class classifiers were formulated based on Gaussian assumption for both the positive and negative (randomly chosen) examples along each feature component, independently. The strong classifier is a weighted sum of the weak classifiers as in AdaBoost <ref type="bibr" target="#b8">[9]</ref>.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, Gaussian mixture model on DCT coefficients is used as image representation. Then Bayesian inference is applied for image regional matching and learning over time.</p><p>Recently there are also attempts to incorporate support vector machine (SVM) into relevance feedback process <ref type="bibr">[4][11]</ref>. However, SVM as a two-class classifier is not directly suitable for relevance feedback, because the training examples are far too few to be representatives of the true distributions <ref type="bibr" target="#b3">[4]</ref>. However a kernel based one-class SVM as density estimator for positive examples has been shown to outperform the whitening transform based linear method <ref type="bibr" target="#b3">[4]</ref>.</p><p>Formulated in the transductive learning framework, D-EM algorithm <ref type="bibr" target="#b34">[35]</ref> uses examples from the user feedback (labeled data) as well as other data points (unlabeled data). It performs discriminant analysis inside the EM iterations to select a subspace of features, such that the two-class assumption on the data distributions has better support. However, the computation induced by the D-EM iterations is expensive, which can make real-time implementation difficult based on the current hardware capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRADITIONAL DISCRIMINANT ANALYSIS AND TRANSFORMATIONS</head><p>To effectively compare the nature and merits of the algorithms presented in Section 2, it is desirable to analyze them from the feature space transformation point of view: indeed, the featureweighting scheme ( <ref type="bibr">[13][18]</ref>[25], etc.) is the simplified diagonal form of a linear transformation in the original feature space, assuming feature independence. While the Mahalanobis distance or the generalized Euclidean distance using the inverse of the covariance matrix of the positive examples <ref type="bibr" target="#b11">[12]</ref>[22] is a whitening transformation based on the configuration of the positive examples, assuming Gaussian distribution.</p><p>From pattern classification point of view, when only positive examples are to be considered and with Gaussian assumption, the whitening transformation is the optimal choice <ref type="bibr" target="#b6">[7]</ref>. When both positive and negative examples are to be considered, instead of the aforementioned various, seemingly plausible heuristics for feature-weighting <ref type="bibr" target="#b12">[13]</ref>[18] <ref type="bibr" target="#b24">[25]</ref>, two optimal linear transformations based on the traditional discriminant analysis are worth investigating. Of course "optimality" depends on the choice of the objective function; in this sense, it becomes a problem of formulating the best objective function:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-class Assumption</head><p>One is the two-class fisher discriminant analysis (FDA). The goal is to find a lower dimensional space in which the ratio of betweenclass scatter over within-class scatter is maximized.</p><formula xml:id="formula_0">W S W W S W W w T b T W max arg =<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">T y y T x x b m m m m m m m m S ) )( ( ) )( ( - - + - - =<label>(2)</label></formula><formula xml:id="formula_2">âˆ‘ âˆ‘ = = - - + - - = y x N i T y i y i N i T x i x i w m y m y m x m x S 1 1 ) )( ( ) )( (<label>(3)</label></formula><p>And, we use {x i , i = 1, â€¦, N x } to denote the positive examples, and {y i , i = 1, â€¦, N y } to denote the negative examples. m x , m y , and m are the mean vectors of the sets {x i }, {y i }, and {x i }âˆª{y i }, respectively. (See <ref type="bibr" target="#b6">[7]</ref> for details.)</p><p>For two-class discriminant analysis, it is part of the objective that negative examples shall cluster in the discriminating subspace. This is an unnecessary and potentially damaging requirement since the relatively small training sample cannot be representative for the overall population, especially for the negative examples. In fact, very likely the negative examples will belong to more than one class. Therefore the effort of rounding up all the negative examples can mislead the resulting discriminating subspace into the wrong direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-class Assumption</head><p>Another choice is the multiple discriminant analysis (MDA) <ref type="bibr" target="#b6">[7]</ref>,</p><p>where each negative example is treated as from a different class. It becomes a (N y + 1)-class discriminant analysis problem. The reason for the crude assumption on the number of negative classes is because the class labels within the negative examples are not available. One may suggest for the user to provide this information. However, from a user interface design point of view, it is reasonable for the user to click to indicate items as relevant versus non-relevant (say, "horses" and "non-horses"), but troublesome and unnatural for the user to further identify for the machine what the negative items really are ("these are tigers, those are zebras, and that is a table, â€¦").</p><p>For MDA the objective function has the same format as in Equation ( <ref type="formula" target="#formula_0">1</ref>). The difference is in the definitions of the scatter matrices:</p><formula xml:id="formula_3">âˆ‘ = - - + - - = y N i T i i T x x b m y m y m m m m S 1 ) )( ( ) )( (<label>(4)</label></formula><formula xml:id="formula_4">âˆ‘ = - - = x N i T x i x i w m x m x S 1 ) )( (<label>(5)</label></formula><p>In this setting, it is part of the objective that all negative examples shall be apart from one another in the discriminating subspace. This is again an unnecessary and potentially damaging requirement since several negative examples can come from the same class. The effort of splitting them up can mislead the resulting discriminating subspace into the wrong direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Clustering</head><p>Without more detailed labels on the negative examples except for the label of "negative", these two are the only sensible solutions available from the tradition discriminant analysis framework. One may argue that unsupervised clustering techniques (EM, or mean shift <ref type="bibr" target="#b4">[5]</ref>) can be applied to find out the number of clusters automatically. However, a meaningful clustering of a set of points actually depends on the subspace selection-an image of a "red table" is not necessarily closer to a "white table" than a "red horse" unless a proper discriminating subspace can be specified in the first place-which is exactly what the system is trying to learn. Iin addition, these iterative algorithms are usually too timeconsuming to achieve real time responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discriminating Transformation</head><p>For both FDA and MDA, the columns of the optimal W are the generalized eigenvector(s) V associated with the largest eigenvalue(s) Î›, i.e.,</p><formula xml:id="formula_5">S b V = Î›S w V<label>(6)</label></formula><p>A discriminating transformation matrix is defined as</p><formula xml:id="formula_6">A = VÎ› 1/2 (7)</formula><p>In the new space x new = A T x old , the following "actions" are employed to ensure the optimal ratio in Equation (1)-for FDA: the positive centroid is "pushed" apart from the negative centroid, while examples of the same label are "pulled" closer to one another; for MDA: the positive centroid and every negative examples are "pushed" apart from one another, while positive examples are "pulled" closer to one another.</p><p>It is important to point out that the effective dimension of the new space is independent of the original dimensionality. For FDA, since the rank of S b is only one, the discriminating subspace has dimension one, i.e., the transformation is always a projection onto a line. This will severely limit its ability in informative modeling, even in the kernel form. For MDA, there can be multiple non-zero eigenvalues, and the number of effective subspace dimensions is at most min{N x , N y }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BIASED DISCRIMINANT ANALYSIS</head><p>Instead of confining ourselves to the traditional settings of the discriminant analysis, we propose a new form of discriminant analysis, namely, biased discriminant analysis (BDA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">(1+x)-class Assumption</head><p>We first define the (1+x)-class classification problem or biased classification problem as the learning problem in which there are an unknown number of classes but the user is only interested in one class, i.e., the user is biased toward one class. And the training samples are labeled by the user as only "positive" or "negative" as to whether they belong to the target class or not. Thus the negative examples can come from an uncertain number of classes.</p><p>Much research has addressed this problem simply as a two-class classification problem with symmetric treatment on positive and negative examples, such as FDA. However the intuition is like "all happy families are alike, each unhappy family is unhappy in its own fashion"(Leo Tolstoy's Anna Karenina); or we say, "all positive examples are alike in a way, each negative example is negative in its own way". Therefore it is necessary to distinguish a real two-class problem from a (1+x)-class problem. When the negative examples are far from representative for their true distributions-which is certainly true in our case-this distinction becomes critical. (Tieu and Viola <ref type="bibr" target="#b28">[29]</ref> used a random sampling strategy to increase the number of negative examples thus their representative power. This is somewhat dangerous since unlabeled positive examples can be included in these "negative" samples.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Biased Discriminant Analysis (BDA)</head><p>For a biased classification problem, we ask the following question instead: what is the optimal discriminating subspace in which the positive examples are "pulled" closer to one another while the negative examples are "pushed" away from the positive ones?</p><p>Or mathematically, what is the optimal transformation such that the ratio of "the negative scatter with respect to positive centroid" over "the positive with-in class scatter" is maximized? We call this biased discriminant analysis (BDA) due to the biased treatment toward the positive examples. We define the biased criterion function</p><formula xml:id="formula_7">W S W W S W W x T y T W max arg =<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">âˆ‘ = - - = y N i T x i x i y m y m y S 1 ) )( (<label>(9)</label></formula><formula xml:id="formula_9">âˆ‘ = - - = x N i T x i x i x m x m x S 1 ) )( (<label>(10)</label></formula><p>The optimal solution and transformations are of the same formats as those of FDA or MDA, subject to the differences defined by Equation ( <ref type="formula" target="#formula_8">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>.</p><p>Note that the discriminating subspace of BDA, obtained through a transformation of the form similar to the one in Equation ( <ref type="formula">7</ref>), has effective dimension of min{N x , N y }, the same as MDA and higher than that of FDA. Even though the weighting of the eigenvectors by the square roots of their corresponding eigenvalues does not affect the value of the objective function in Equation ( <ref type="formula" target="#formula_7">8</ref>), the resulting biased discriminating transform (BDT) matrix has the form of a "generalized whitening transform", or a "discriminative whitening transform". BDT can be regarded as the informative modeling of positive examples incorporating discriminative information from negative examples. Whitening transform is the special case when only positive examples are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Regularization and Discounting Factors</head><p>It is well known that the sample-based plug-in estimates of the scatter matrices based on Equations (2)(3)(4)(5)(9)(10) will be severely biased for small number of training examples, i.e., the largest eigenvalue becomes larger, while the small ones smaller.</p><p>A compensation or regularization can be done by adding small quantities to the diagonal of the scatter matrices <ref type="bibr" target="#b9">[10]</ref>. The regularized version of S x , with n being the dimension of the original space and I being the identity matrix, is:</p><formula xml:id="formula_10">I S tr n S S x x r x ] [ ) 1 ( Âµ Âµ + - =<label>(11)</label></formula><p>The parameter Âµ control shrinkage toward a multiple of the identity matrix. And tr[] denotes the trace operation for a matrix.</p><p>The influence of the negative examples can be tuned down by a discounting factor Î³, and the discounted version of S y is: BDA captures the essential nature of the problem with minimal assumption. In fact, even the Gaussian assumption on the positive examples can be further relaxed by incorporating kernels.</p><formula xml:id="formula_11">I S tr n S S y y d y ] [ ) 1 ( Î³ Î³ + - =<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">KERNEL-BASED BIASED DISCRIMINANT ANALYSIS (KBDA)</head><p>To take into account non-linearity in the data, we propose a kernel-based approach.</p><p>The original BDA algorithm is applied in a "feature space" 1 , which is related to the original space by a non-linear mapping Ï† :</p><p>x â†’ Ï† (x). Since in general the number of components in Ï†(x) can be very large or even infinite, this mapping is too expensive and will not be carried out explicitly, but through the evaluation of a kernel K, with elements k ij = Ï† T (x i ) Ï† (x j ). This is the same idea adopted by the support vector machine <ref type="bibr" target="#b29">[30]</ref>, kernel PCA, and kernel discriminant analysis <ref type="bibr">[1][15]</ref>. The trick is to rewrite the BDA formulae using only dot-products of the form Ï† i T Ï† j , so that the reproducing kernel matrix can be substituted into the formulation and the solution, eliminate the need for direct nonlinear transformations.</p><p>Using superscript Ï† to denote quantities in the new space, we have the objective function in the following form: 1 A term used in kernel machine literatures to denote the new space after the nonlinear transform-this is not to be confused with the feature space concept previously used to denote the space for features/descriptors extracted from the media data.</p><p>where</p><formula xml:id="formula_12">âˆ‘ = - - = y x x N i T i i y m y m y S 1 ) ) ( )( ) ( ( Ï† Ï† Ï† Ï† Ï† (14) And âˆ‘ = - - = x x x x N i T i i m x m x S 1 ) ) ( )( ) ( ( Ï† Ï† Ï† Ï† Ï† (15)</formula><p>Since w* is the eigenvector(s), it can be expressed as a weighted sum of input vectors:</p><formula xml:id="formula_13">Î± Ï† Î± Ï† Î± Î¦ = + = âˆ‘ âˆ‘ = + = y x x N j j N j N i i i y x w 1 1 ) ( ) (<label>(16)</label></formula><p>It can be shown that the numerator of ( <ref type="formula">13</ref>) can be rewritten as:</p><formula xml:id="formula_14">Î± Î± Î± Î± Î± Ï† Ï† Î± Ï† Ï† Ï† T y x y y x y T N j T mx y mx y T N j T x j x j T T y T x N x N y j j y I K K I K K K K K K m y m y w S w ) )( ( ) )( ( ) ) ( )( ) ( ( 1 1 - - = - - = Î¦ - - Î¦ = âˆ‘ âˆ‘ = = (<label>17</label></formula><formula xml:id="formula_15">)</formula><p>Where Î¦ is defined in ( <ref type="formula" target="#formula_13">16</ref>), and</p><formula xml:id="formula_16">j j y j y x T mx j T y K K m K y K = Î¦ = Î¦ = :, ) ( , ), ( Ï† Ï† and y N x</formula><p>I is an N x by N y matrix of all elements being 1 / N x .</p><p>Similarly, rewrite the denominator of ( <ref type="formula">13</ref>),</p><formula xml:id="formula_17">Î± Î± Î± Î± Î± Î± Ï† T x x x T T T x x x T T x x x x x x T x T K I I K K I I I I K I K K I K K w S w x N x x N x N x N x N ) ( ) )( ( ) )( ( - = - - = - - =<label>(18)</label></formula><p>and x N x I is an N x by N x matrix of all elements being 1 / N x . Now we can solve for Î±, which is the eigenvector(s) associated with the largest eigenvalue(s) for the generalized eigenanalysis problem defined by Equation ( <ref type="formula" target="#formula_7">8</ref>), <ref type="bibr" target="#b16">(17)</ref>, and <ref type="bibr" target="#b17">(18)</ref>.</p><p>With optimal Î±'s, the projection of a new pattern z onto w is given by:</p><formula xml:id="formula_18">âˆ‘ âˆ‘ = + = + = y x x N j N j N i i i T z y k z x k z w 1 1 ) , ( ) , ( ) ( Î± Î± Ï†<label>(19)</label></formula><p>In this nonlinearly transformed new space with w's (weighted by the square-rooted eigenvalues) as the axes, the nearest neighbors of the positive centroid are returned as the outputs of the learning process. If not satisfied, the user can give further judgments on these new outputs to enter another round of relevance feedback interaction. The machine can combine the new feedbacks with all the previous feedbacks together in the next round of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">COMPARISONS AND ANALYSIS</head><p>Using image retrieval as the application, we compare the three proposed discriminating transforms to the optimal two level whitening transforms <ref type="bibr" target="#b21">[22]</ref>, and compare the kernel versions with SVM, on both synthetic data and real world image databases. The scenario is "query by example" followed by several rounds of relevance feedback by the user. For each round, the machine learns an optimal transform, linear or non-linear, from the training examples fed-back by the user; then all training and testing points are transformed into the new space, where the new query is the mean of the transformed positive examples, and its Euclidean nearest neighbors are returned for further feedbacks from the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Linear/Quadratic Case</head><p>For the non-kernel versions of FDA, MDA, and BDA, all the transform matrices are linear, and the decision boundaries are either linear or quadratic. To illustrate the advantages of BDA over FDA or MDA, we use some toy problems as depicted in Figure <ref type="figure" target="#fig_2">2</ref>. Original data are in 2-D feature space, and positive examples are "o"s and negetive examples are "x"s in the figure. FDA, MDA, and BDA are applied to find the best projection direction by their own criterion functions for each case, and the resulting (generalized) eigenvector corresponding to the maximum eigenvalue is drawn in solid green, dash-dotted red, and dashed blue straight lines, respectively. The would-be projections of the data points onto these eigenvectors are also draw as bell-shaped curves to the side of the corresponding eigenvectors, assuming Gaussian distribution for each mode. The thicker curves represent the projections of the positive modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Toy Problems</head><p>Here, FDA treats positive and negative examples equally, i.e., it tries to decrease the scatter among negative examples as part of the effort. This makes it a bad choice in cases (b) or (d). Without any prior knowledge about the number of classes to which the negative examples belong, MDA can only treat each example as a separate class/mode. Since MDA has in its criterion function the tendency of increasing the scatter among all classes/modes, which includes the scatter among negative examples, this makes it a bad choice for cases (a) and (c).</p><p>In all cases, BDA yields good separation of negative examples from positive ones, as well as clustering of positive examples (it finds a balance between these two goals, which are embedded in the criterion function). Note from (c) to (d), the two negative modes move apart from each other and toward the positive ones. FDA and MDA cannot adapt to the changing configurations and will fail for one of the two cases: for (c) MDA fails and for (d) FDA fails. Whereas BDA is able to adapt to the change and gives better separation in both cases.</p><p>FDA and MDA are inadequate for biased classification or biased dimensionality reduction problems because of their forceful assumption on the number of modes. BDA avoids making this assumption by directly modeling the problem into the objective function hence gives better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Image Database Testing</head><p>In this experiments, a COREL image set of 17695 images are tested. A feature space of 37 dimensions is used with 9 color moments, 10 wavelet moments <ref type="bibr" target="#b26">[27]</ref>, and 18 edge-based structure features <ref type="bibr" target="#b36">[37]</ref>. Without prior knowledge on feature-class correlations, all feature components are normalized to normal distributions.</p><p>For the first round with only one positive example-the query image-the system uses Euclidean distance metric to retrieve the 20 nearest neighbors. Subsequently, a subject selects the training examples on the fly. Up to 20 rounds of feedback (or until convergence) are performed for every query under each of the four relevance feedback schemes: two-level optimal whitening transform (WT) <ref type="bibr" target="#b21">[22]</ref>, FDA, MDA, and BDA. Altogether over 1000 rounds of subject guided retrieval/relevance feedback are performed over 20 classes of images (See Figure <ref type="figure" target="#fig_3">3</ref> for some examples. It should be noted that some of the semantic classes in the Corel set are too difficult for content-based retrieval using the currently available low-level features. The ones shown here are the relatively "good examples" that can yield reasonable initial results for further user interactions). The numbers of hits in top 20 are recorded for different schemes. And their means and variances are compared in Table <ref type="table" target="#tab_0">1</ref>.</p><p>It is apparent that all the three proposed transforms outperform the WT scheme based solely on positive examples, especially the MDA and BDA-based transforms. BDA not only yields the highest average score, but also has the minimum variation, which indicates the most robust performance. FDA and MDA have larger performance variation because they are affected by the clustering patterns in negative examples, which are generally  unstable. MDA in this case is close to BDA in performance because the subject for this test tends to give small number (average around 3) of negative examples that are usually not from the same class, i.e., if two "tigers" appear when searching for "horses", the subject only mark one of them as negative to see whether the other one can be "pushed" out in the next roundwith negative examples all coming from different classes, the problem associated with MDA can not be fully observed (See Section 3.2 for analysis.) WT has low average score and large performance variation mainly because it is prone to be trapped at local minimum, which is frequently observed in our experiments. Figure <ref type="figure">4</ref> illustrate this point with a hypothetic feature space configuration, as well as a real image retrieval example. It shows that using BDT the system can climb out of local minimum with the "push" from negative examples.</p><p>All the four algorithms run in real time on a Pentium III PC, with a maximum latency of less than 2 seconds during relevance feedback on the COREL set of 17695 images with a 37 dimensional feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Non-linear Case</head><p>For the non-linear case, we compare the kernel BDA with BDA, and SVM, over the same RBF kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Does Kernel Help?</head><p>To test the ability of the KBDA in dealing with non-linearly distributed positive examples, six sets of synthetic data in twodimensional space are used (see Figure <ref type="figure">5</ref>).</p><p>A significant boost in averaged hit rates is observed when using KBDA.</p><p>Next we try KBDA on a real image database to see whether it helps to introduce kernel for nonlinearity in the real world applications.</p><p>A fully labeled set of 500 images from COREL is used for automated testing in the next experiment. It contains five classes, each with 100 images. Each round 10 positive and 10 negative images are randomly drawn as training samples. For each round the error rate in the top 100 returns is recorded as the performance measures. 500 rounds of testing are performed on the 5 classes and the averaged hit rates are shown in Figure <ref type="figure">6</ref> where four schemes are compared: WT, FDA, BDA, and KBDA. One can see that KBDA outperforms BDA on average by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">KBDA vs. SVM</head><p>It is also desirable to see how the proposed kernel method compares with support vector machines (SVM).</p><p>SVM assumes that negative examples are representative of the true distribution, which is far from the reality for the relevance feedback scenario. In the information retrieval application, given the usually large number of classes the unlabeled areas in the feature space are very likely to be negative. When SVM is directly implemented as a two-class learning machine during information retrieval, the result is that after the user's feedback, the machine returns a totally different set of points, with most of them likely to be negative. Of course incremental training iterations <ref type="bibr" target="#b1">[2]</ref> can be applied to eventually arrive at the correct boundary, but this may require a significant number of further iterations and more training examples, from an extremely patient user.</p><p>Here  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we briefly reviewed existing relevance feedback techniques. Emphasize was put on the analysis of the unique characteristics of multimedia information retrieval problems and the corresponding on-line learning algorithms. A novel scheme was proposed with experimental results supporting its superior performance than existing schemes. The proposed KBDA algorithm can be applied not only in multimedia information retrieval problems, but also for other classification problems whenever the number of negative training samples is too small to be representative for the true distribution. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>12 )</head><label>12</label><figDesc>With different combinations of the (Âµ, Î³) values, the regularized and/or discounted BDA provides a rich set of alternatives: (Âµ = 0, Î³ = 1) gives a subspace that is mainly defined by minimizing the scatters among the positive examples, resembling the effect of a whitening transform; (Âµ = 1, Î³ = 0) gives a subspace that mainly separates the negative from the positive centroid, with minimal effort on clustering the positive examples; (Âµ = 0, Î³ = 0) is the full BDA and (Âµ = 1, Î³ = 1) represents the extreme of discounting all configurations of the training examples and keep the original feature space unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparing FDA, MDA, and BDA for dimensionality reduction from 2-D to 1-D. (a) FDA and BDA yield projection with nice class separation, MDA failed; (b) MDA and BDA yield projection with nice class separation, FDA failed; From (c) to (d), Notice the two modes of the negative examples moved apart from each other and toward the positive examples, and BDA is able to adapt to the change and gives better class separation in both cases. MDA fails in (c), and FDA fails in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Some example images from the Corel set used in the experiments.</figDesc><graphic coords="7,333.78,72.00,215.88,174.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5</head><label>45</label><figDesc>Figure 4. The open circles represent positive examples and the crosses negative. (a) the system uses Euclidean distance for one query; (b) the system uses the subspace spanned by the positive examples. It can stagnate at a local minimum; (c) adding negative examples the system finds a better transformation; (d) Top 20 returns with only positive feedback. The system stagnates at this point, repeating the same response; (e) Adding negative feedback and using BDA can pull the system out of stagnation and arrive at a much better solution.</figDesc><graphic coords="8,177.12,175.50,116.94,82.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>we compare KBDA and SVM in the context of face and non-face classification under small number of training samples, this example shall reveal more clearly the nature of the two algorithms. Among the 1000 faces and 1000 non-face images, some examples are shown in Figure7. All the images are 16-by-16 in size and the original pixel values are used as the features, resulting in a 256-dimensional space. We use different numbers of positive and negative examples to train a KBDA and a SVM learner. For the SVM, the distance to the hyperplane is used to rank order all the points and the percentage of face images in the top 1000 returns is used to compare with the hit rate of KBDA in its top 1000 returns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8</head><label>8</label><figDesc>Figure 8 illustrates the experimental results with a fixed number of positive examples-100, and a varying number of negative examples-from 1, 2, â€¦, to 500. The vertical axis denotes the hit rate in top 1000 returns. Each point on the two curves represents the averaged rate of 100 random trials. This figure clearly shows that when the number of negative examples is small (&lt; 200), KBDA outperforms SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 Figure 8</head><label>78</label><figDesc>Figure 7 Some examples of the face and non-face images</figDesc><graphic coords="9,333.90,72.00,100.68,100.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Comparing relevance feedback results: the first row is the averaged number of hits in top 20, and the second row shows their variances.</head><label>1</label><figDesc></figDesc><table><row><cell>No feedback</cell><cell>WT</cell><cell>FDA</cell><cell>MDA</cell><cell>BDA</cell></row><row><cell>8.2</cell><cell>13.0</cell><cell>13.9</cell><cell>16.2</cell><cell>17.0</cell></row><row><cell>8.43</cell><cell>17.43</cell><cell>16.50</cell><cell>10.26</cell><cell>8.86</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>This work was supported in part by National Science Foundation under the grants CDA 96-24396 and EIA 99-75019. The authors would like to thank the anonymous reviews for their comments and suggestions!</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized discriminant analysis using a kernel approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Baudat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Anouar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2385" to="2404" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental and decremental support vector machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical Browsing and Search of Large Image Databases</title>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-class SVM for Learning in Image Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Image Proc. (ICIP&apos;2001)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">October 7-10, 2001</date>
		</imprint>
	</monogr>
	<note>submitted to IEEE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distribution free decomposition of multivariate data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Optimized Interaction Strategy for Bayesian Relevance Feedback</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Query by image and video content: The qbic system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>IEEE Computers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Japanese Society for Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="771" to="780" />
			<date type="published" when="1999-09">September, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularized Discriminant Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">405</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporate Support Vector Machines to Content-Based Image Retrieval with Relevance Feedback</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Image Proc. (ICIP&apos;2000)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">Sep 10-13, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MindReader: Query databases through multiple examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Of the 24th VLDB Conf</title>
		<meeting>Of the 24th VLDB Conf<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PicSOM: Self-Organizing Maps for Content-Based Image Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koskela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNN&apos;99</title>
		<meeting>of IJCNN&apos;99<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relevance feedback and category search in image databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meilhac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nastar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia Computing and Systems</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A mathematical programming approach to the Kernel Fisher algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multimodal pattern matching for audio/visual query and retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-01">Jan 2001</date>
			<pubPlace>San Jose, CA</pubPlace>
		</imprint>
	</monogr>
	<note>SPIE Photonics West, Storage and Retrieval for Media Databases</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Query Refinement for Image Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nastar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meilhac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition CVPR&apos;98</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic feature relevance learning for content-based image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="150" to="164" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling User Subjectivity in Image Libraries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Processing (ICIP&apos;96)</title>
		<meeting><address><addrLine>Lausanne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Query Reformulation for Content Based Multimedia Retrieval in MARS</title>
		<author>
			<persName><forename type="first">K</forename><surname>Porkaew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Multimedia Computing and Systems (ICMCS&apos;99)</title>
		<imprint>
			<date type="published" when="1999-06">June, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relevance Feedback: A Power Tool in Interactive Content-Based Image Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1998">Sept., 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing learning in image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, South Carolina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic text processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrated Browsing and Querying for Image Database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="26" to="39" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Content-based Color Image Retrieval with Relevance Feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gagliardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Image Processing (ICIP&apos;99)</title>
		<meeting><address><addrLine>Kobe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VisualSEEk: a Fully Automated Content-Based Image Query System</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Intern. Conf. Multimedia</title>
		<meeting>ACM Intern. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transform features for texture classification and discrimination in large image databases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Image Processing</title>
		<meeting>IEEE Int&apos;l Conf. Image essing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Content-based video indexing and retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="75" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting Image Retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian relevance feedback for content-based image retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.IEEE Workshop on Content-based Access of Image and Video Libraries, CVPR&apos;00</title>
		<meeting>.IEEE Workshop on Content-based Access of Image and Video Libraries, CVPR&apos;00<address><addrLine>Hilton Head Island, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contentbased Classification Search and Retrieval of Audio</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keislar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wheaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia Magazine</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iterative Refinement by Relevance Feedback in Content-Based Digital Image Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia 98</title>
		<meeting><address><addrLine>Bristol, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-09">September 1998</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interaction in Content-based Image Retrieval: a state-of-the-art review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on Visual Information Systems, Visual 2000</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminant EM Algorithm with Application to Image Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition<address><addrLine>Hilton Head Island, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image retrieval: Feature primitives, feature representation, and relevance feedback</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Access of Image and Video Libraries, CVPR-2000</title>
		<meeting><address><addrLine>Hilton Head, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edge-based structural feature for content-based image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters, Special issue on Image and Video Indexing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A generalized relevance feedback scheme for image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hunag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of SPIE</title>
		<imprint>
			<biblScope unit="volume">4210</biblScope>
			<date>November 6-7, 200</date>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Internet Multimedia Management Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
