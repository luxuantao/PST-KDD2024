<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9B521386028B04F3B9FCD15DBAEB57B</idno>
					<idno type="DOI">10.1109/TIP.2005.854478</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-With the fast development of visual noise-shaping related applications (visual compression, error resilience, watermarking, encryption, and display), there is an increasingly significant demand on incorporating perceptual characteristics into these applications for improved performance. In this paper, a very important mechanism of the human brain, visual attention, is introduced for visual sensitivity and visual quality evaluation. Based upon the analysis, a new numerical measure for visual attention's modulatory aftereffects, perceptual quality significance map (PQSM), is proposed. To a certain extent, the PQSM reflects the processing ability of the human brain on local visual contents statistically. The PQSM is generated with the integration of local perceptual stimuli from color contrast, texture contrast, motion, as well as cognitive features (skin color and face in this study). Experimental results with subjective viewing demonstrate the performance improvement on two PQSM-modulated visual sensitivity models and two PQSM-based visual quality metrics.</p><p>Index Terms-Just-noticeable difference (JND), noise shaping, perceptual quality significance map (PQSM), visual attention, visual quality evaluation, visual sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH THE fast development of visual noise-shaping related applications (e.g., visual compression, error resilience, watermarking, encryption, and display), research on human visual sensitivity analysis and quality evaluation has drawn a lot of attention from scientists and researchers <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Visual sensitivity refers to the ability of human observers to detect noise or distortion in the view field. Numerically, visual sensitivity can be regarded as the inverse of the just-noticeable difference (JND), which determines the visibility thresholds in pixels <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> or subbands <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Visual quality metrics (VQMs) are designed to predict perceived image and video quality by measuring the detectability <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref> or annoyance of noise/distortion <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> introduced via visual processing.</p><p>Visual noise shaping is to allocate the inevitable noise or distortion into some subbands or spatial areas so that the resultant visual variation is the least noticeable or annoying to the human Manuscript received <ref type="bibr">March 31, 2004</ref>; revised <ref type="bibr">October 5, 2004</ref>. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Zhigang (Zeke) Fan.</p><p>Z. Lu, W. Lin, E. Ong, and S. Yao are with the Institute for Infocomm Research, Agency for Science, Technology, and Research, Singapore 119613 (e-mail: zklu@i2r.a-star.edu.sg; wslin@i2r.a-star.edu.sg; epong@i2r.a-star.edu.sg; ssyao@i2r.a-star.edu.sg).</p><p>X. Yang is with the Department of Electronic Engineering, Institute of Image Communication and Information Processing, Shanghai Jiaotong University, Shanghai 200030, China (e-mail: xkyang@ieee.org).</p><p>Digital Object Identifier 10.1109/TIP.2005.854478</p><p>visual system (HVS). With regard to the human visual perception, the noise or distortion introduced into image/video can be classified into three categories: 1) imperceivable noise; 2) nearthreshold noise; and 3) suprathreshold distortion. Imperceivable noise is below JND and, therefore, hard to be perceived by the HVS. Near-threshold noise is just above the thresholds while suprathreshold distortion is much stronger than JND. Generally, suprathreshold distortion appears in the form of structural patterns, such as blockiness, ringing, blurring and jerkiness in decoded visual signal <ref type="bibr" target="#b17">[18]</ref>.</p><p>Visual attention is the result of several millions of years of evolution <ref type="bibr" target="#b18">[19]</ref>, and the research on visual attention began more than 100 years ago <ref type="bibr" target="#b19">[20]</ref>. It can be defined as a set of strategies to reduce the computational cost of the search processes inherent in visual perception <ref type="bibr" target="#b20">[21]</ref>. It has top-down (or knowledge/task-driven) and bottom-up (or stimulus-driven) mechanisms <ref type="bibr" target="#b21">[22]</ref>. In the former mechanism, attention is under the overt control of the subject and related to cognition processing in the human brain <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>; it is voluntary, effortful, and has a slow (sustained) time course <ref type="bibr" target="#b24">[25]</ref>. In the latter mechanism, attention is driven by external stimuli and some fast perception processing of the human brain draws attention to a particular location; it is automatic and has a transient time course. Generally, the stimuli involved in bottom-up control include luminance, color, orientation and motion contrast, while the features involved in top-down control are pattern, shape, and other cognitive processing related features. Moreover, audition, touching, and other sensories also affect visual attention <ref type="bibr" target="#b25">[26]</ref>. Because the top-down control has a much longer time course, it may play a more important role on the shift and distribution of visual attention. Visual attention modulates all levels of visual perception <ref type="bibr" target="#b26">[27]</ref>, including visual sensitivity <ref type="bibr" target="#b27">[28]</ref> and, therefore, visual quality evaluation.</p><p>The simplified concept of visual attention has been adapted for video quality evaluation <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b28">[29]</ref>, under a simple assumption that the HVS' focus position is on the center of the image, a weighted SNR metric is proposed according to the eccentricity and contrast sensitivity function (CSF), based on a fixed gradient model of visual attention. In <ref type="bibr" target="#b29">[30]</ref>, a bottom-up visual attention model is proposed to weight the visual quality metric <ref type="bibr" target="#b10">[11]</ref> for accuracy improvement, mainly based on location, contrast, color, luminance, and motion, without consideration of a gradient model. There is a need for automatic estimation to include both bottom-up stimuli and top-down features for visual sensitivity and quality evaluation. Moreover, the motion suppression effect <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> has to be considered since motion affects visual sensitivity and quality assessment significantly in video; integration of multiple stimuli and features would allow the application to a wider scope of visual signal; and a flexible gradient model is more realistic in the human perception <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>.</p><p>In this paper, perceptual quality significance map (PQSM) is proposed to reflect the modulatory aftereffects of visual attention, on visual sensitivity and quality evaluation. The PQSM is automatically estimated with both bottom-up stimuli and topdown features. A general formulation is proposed for multiple stimuli/feature integration to capture the basic ideas of visual attention for practical applications. Color contrast, texture contrast, motion, skin color, and face features are extracted and integrated in the current implementation, under a flexible gradient model. The PQSM for an image or video can be incorporated in JND estimators and VQMs enhanced performance.</p><p>The rest of this paper is organized as follows. Section II reviews the related work on biological and psychological mechanisms of visual sensitivity and visual attention, as well as on existing models for JND estimation, VQMs and visual attention. The proposed computational model of PQSM generation, two PQSM-modulated JND models, and two PQSM-based VQMs are presented in Sections III-V, respectively. The experimental results are demonstrated in Section VI, in comparison with the associated subjective test data. The conclusion and future work are given in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH WORK</head><p>This section reviews the previous work related to the proposed PQSM models. In Section II-A, biological and psychological evidence is presented for visual sensitivity, followed by a review of the existing JND estimators. Section II-B introduces the current VQMs. In Section II-C, characteristics of and relevant research on visual attention are discussed; the section also provides the ground of some strategies for the proposed PQSM estimation algorithm in Section III. In Section II-D, current techniques combining visual attention with various visual processing tasks are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Sensitivity Analysis 1) Biological and Psychological Mechanisms Behind Visual</head><p>Sensitivity: In general, visual sensitivity includes amplitude sensitivity, motion sensitivity <ref type="bibr" target="#b36">[37]</ref>, and flicker sensitivity <ref type="bibr" target="#b37">[38]</ref>. Among them, amplitude sensitivity is the basic. The latter two are closely linked, and much more complex than amplitude sensitivity. Amplitude sensitivity has been intensively explored, and is typically modeled as the output of the biological and psychological mechanisms of the HVS <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>Visual sensitivity results from the anatomy, the limitations and imperfections of the human eye, such as the optical properties of the HVS, the photo-electric transmission curve of photoreceptor, the distribution of photoreceptor on retina, the response of bipolar cells and ganglion cells in the second layer of retina, and the noise introduced in signal via vision path. After entering the human brain, visual signal is mainly processed in cortex area V1. Electro-physiological experiments have shown that the response of neurons in V1 exhibits a band limited properties <ref type="bibr" target="#b40">[41]</ref>. The HVS decomposes visual data into perceptual channels with spatial/temporal frequencies, orientations, and colors <ref type="bibr" target="#b41">[42]</ref>.</p><p>Masking between two or among more visual channels usually results in an increase of the visibility thresholds <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The masking in zero frequency channel is called luminance adaptation, the masking in nonzero frequency channel is called contrast masking, and the masking within a channel or with other channel(s) is called intrachannel masking and interchannel masking. The common factors considered for visual visibility thresholds are: 1) spatial and temporal frequencies, 2) luminance, and 3) contrast orientations.</p><p>The visual sensitivity is enhanced on precued spatial locations <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>, due to the aftereffects of visual attention. The visibility thresholds in the precued areas are lower than the other nonattentional areas, with the recorded differences varying from 0 to 9.4 dB because of different experimental designations. Itti et al. reported that visual attention can elevate the sensitivity with spatial and temporal frequencies by 30%, the sensitivity with orientations by 40%, and the sensitivity peak altitude by 5.2 dB <ref type="bibr" target="#b27">[28]</ref>.</p><p>Another global factor affecting visual sensitivity is motion suppression <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> caused by the motion of object projection on retina. It shows the suppression can reach about 0.6 log units, or 12 dB in maximum. It is believed that motion on retina image increases the processing cost of visual perception and, therefore, suppresses the visual sensitivity. Motion suppression happens in the low attentional areas when the motion is different to that in high-attentional areas. Since the eye movement follows the shift of visual attention, motion suppression can be regarded as another aftereffect of visual attention. It is worth noting that inaccurate pursuit of eye movement also brings about motion on retina in saccadic condition, which causes motion suppression.</p><p>2) Computational Models for Visual Sensitivity: The existing computational visual sensitivity or JND models can be classified into two categories: pixel based <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and transform based <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> (extensively investigated especially in DCT domain). A subband domain JND estimator can be more precise, because it can really take into account the different interaction between signals or components in the masking effects. However, the pixel domain estimation is computationally simpler, and has its advantages in some applications, e.g., motion estimation (before subband coefficients are available), perceptual evaluation for already-decoded images/video.</p><p>In Chou and Li's model <ref type="bibr" target="#b7">[8]</ref>, the JND of a pixel is obtained as <ref type="bibr" target="#b0">(1)</ref> where is the average background luminance, is the contrast value, which is the maximum output of high-pass filtering at four directions, and</p><p>) and ) represent contrast masking and luminance adaptation, respectively (for 8-bit image presentation)</p><formula xml:id="formula_0">(2) if if (3)</formula><p>where and . All the parameters were empirically determined by fitting the model with subjective test results <ref type="bibr" target="#b7">[8]</ref> under certain viewing conditions (i.e., a monitor with its associated gamma function, viewing distance, ambiance illumination). The conversion between grey levels and display luminance has been factored in the valuation of the parameters in the above equations. The model is only for luminance components.</p><p>Yang et al. <ref type="bibr" target="#b8">[9]</ref> extended Chou and Li's model to account for multiple channels and the combined effect of contrast masking and luminance adaptation, and, therefore, the spatial JND threshold for a pixel can be expressed as <ref type="bibr" target="#b3">(4)</ref> where represents the luminance adaptation in each zero frequency channel, represents the intrachannel contrast masking, reflects the interchannel masking between zero frequency channel and contrast channel, and denotes , and in space. Combined with temporal masking, the final JND is obtained as <ref type="bibr" target="#b4">(5)</ref> where is the recorrection function for temporal masking <ref type="bibr" target="#b49">[50]</ref>.</p><p>Watson et al. proposed an analytic formula for JND thresholds on each DCT frequency component <ref type="bibr" target="#b4">[5]</ref> as the product of a luminance adaptation , a temporal contrast masking function , a spatial contrast masking function , and an orientation suppression function <ref type="bibr" target="#b5">(6)</ref> where , , and represent spatial horizontal frequency, spatial vertical frequency, and temporal frequency, respectively. is the inverse of the magnitude response of a first-order discrete IIR low-pass filter with a sample rate of and a time constant of <ref type="bibr" target="#b6">(7)</ref> is the inverse of a Gaussian function <ref type="bibr" target="#b7">(8)</ref> where is the display resolution in pixels/degree, and the factor of converts from DCT frequencies to cycles/degree; corresponds to the radial frequency at which the threshold is elevated by a factor of .</p><p>is expressed as <ref type="bibr" target="#b8">(9)</ref> where and are parameters. All parameters were decided by fitting with the subjective test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Quality Metrics (VQMs)</head><p>Visual sensitivity plays an important role in visual quality gauging. Imperceivable noise has no or relatively insignificant contribution on the change of visual quality. Under near-threshold conditions, visual quality is evaluated by measuring the detectability of distortion; under suprathreshold conditions, visual quality is evaluated by measuring the annoyance of distortion. In the visual communication applications, imperceivable noise usually results from perceptually lossless compression; near-threshold noise usually results from lossy compression at medium or high bit rates; and suprathreshold distortions exist in low or very low bit-rate compression. Measuring the annoyance of suprathreshold distortions is very complex, since it is a highly subjective process that involves with the high level activities of human brains, such as pattern matching, object recognition and scene perception. It depends on both the distortion characteristics and the original visual contents, and artifacts differing qualitatively in their appearance may produce different levels of annoyance even though they have the same sensitivity threshold and the same error energy <ref type="bibr" target="#b50">[51]</ref>.</p><p>Some VQMs measure the detectability of distortion <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Among these VQMs, Teo et al. <ref type="bibr" target="#b10">[11]</ref> and Winkler <ref type="bibr" target="#b3">[4]</ref> used steerable pyramid transform to separate input video into several channels, and the contrast gain control has been realized by an excitatory nonlinearity function. JNDs can be directly used to facilitate the distortion evaluation, like in Watson's metric <ref type="bibr" target="#b4">[5]</ref> in DCT domain and Lubin's metric <ref type="bibr" target="#b11">[12]</ref> in spatial domain. In <ref type="bibr" target="#b13">[14]</ref>, perceptual errors are evaluated in flat, edge and texture regions, respectively, before integration by Fuzzy integral. Le Callet et al. <ref type="bibr" target="#b12">[13]</ref> proposed another color image quality metric; based on their psychophysical experimental results, the visual representations of errors distributed over color, spatial, and frequency dimensions between two images are computed and then evaluated via error pooling.</p><p>Some other VQMs measure the strength of structural distortions. In <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, prior knowledge of coding (structural) artifacts specific for decoded images/video is used in VQMs. Blockiness, ringing, and blurring are oft-occurring coding artifacts <ref type="bibr" target="#b17">[18]</ref>. Karunasekera's model <ref type="bibr" target="#b1">[2]</ref> estimates blockiness artifacts via horizontal and vertical high-pass filters and masked edge errors via a nonlinear transform. A no-reference blocking artifact measurement algorithm proposed by Wu <ref type="bibr" target="#b14">[15]</ref> uses a weighted mean square difference along block boundaries as the blockiness measure. In the structural similarity index (SSIM) proposed by Wang et al. <ref type="bibr" target="#b15">[16]</ref>, visual distortion is evaluated with luminance, contrast, and structural changes, as well as motion in video. Ong et al. <ref type="bibr" target="#b16">[17]</ref> detected and combined three major disturbing artifacts (namely, damaged edge, blockiness, and ringing) to give final quality scores for low bit-rate visual communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visual Attention Models</head><p>Biological research has proved that allocating attention to a spatial location in the visual field is associated with an increase in cortical response at that location <ref type="bibr" target="#b51">[52]</ref>, and this means more HVS computational resource is allocated to high attentional areas than low attentional areas. The formation of visual attention is a very complex process concerning all aspects of visual processing in the human brain, such as processing of visual contents <ref type="bibr" target="#b18">[19]</ref>, stimuli and feature integration <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, feature binding <ref type="bibr" target="#b54">[55]</ref>, and object perception.</p><p>Moreover, visual attention has capacity limit <ref type="bibr" target="#b55">[56]</ref>, in spite of some arguments in this topic <ref type="bibr" target="#b56">[57]</ref>. Perceptual processing of multiple items or large attentional areas are not independent and the visual sensitivity enhancement can be reduced in such cases, due to the limited computational resource of the human brain.</p><p>The shape of high attentional area can be arbitrary <ref type="bibr" target="#b57">[58]</ref>, and the gradient of visual attention is flexible <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Two special cases are the attentional gradient model <ref type="bibr" target="#b58">[59]</ref> and the zoom-lens model <ref type="bibr" target="#b59">[60]</ref>. In the vicinity of a highly attentional object, such as a human head, the gradient of attention is quite steep <ref type="bibr" target="#b60">[61]</ref>, and sometimes the distribution can be discrete-as in the zoom lens model.</p><p>Other properties of visual attention are listed as follows.</p><p>• The HVS is not blind out of attention, although some research shows that some big and flashy changes could be ignored in peripheral vision <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref> (due to the fact that the limited capacity of the human brain, especially short-term visual memory <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, may be so engaged with the current visual objects that no resource can be reallocated to process new changes). • Visual attention enhances not only the visual sensitivity, but also observers' performance in a wide variety of other visual tasks, e.g., the integration of multiple stimuli <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. • Visual attention can be either object based or area based <ref type="bibr" target="#b65">[66]</ref>. • The integration of multiple stimuli on visual attention is nonlinear additivity <ref type="bibr" target="#b66">[67]</ref>. • Most importantly, it is stimulus contrast rather than absolute stimulus strength that guides the bottom-up attention <ref type="bibr" target="#b66">[67]</ref>- <ref type="bibr" target="#b68">[69]</ref>. Since Broadbent <ref type="bibr" target="#b70">[71]</ref> first proposed the filter theory of selective attention, a number of computational models on visual attention have been developed <ref type="bibr" target="#b71">[72]</ref>- <ref type="bibr" target="#b74">[75]</ref> based on Treisman's stimulus integration theory <ref type="bibr" target="#b75">[76]</ref>. All these models adapt a twostage framework: The first stage preattentively processes all incoming visual information equally and in a parallel fashion; the second stage filters and combines the extracted information to form a salience map. The final attentional position is selected by a embedded decision module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Combining Visual Attention With Visual Processing</head><p>The concept of visual attention has been adopted in visual processing in somewhat simplified forms <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b76">[77]</ref>- <ref type="bibr" target="#b82">[83]</ref>. In <ref type="bibr" target="#b76">[77]</ref> and <ref type="bibr" target="#b77">[78]</ref>, region of interest (ROI) can be regarded as a discrete visual attentional map. The maximum shift <ref type="bibr" target="#b78">[79]</ref> method and the generalized bitplane-by-bitplane shift method <ref type="bibr" target="#b83">[84]</ref> were proposed to enhance the ROI coding quality. In <ref type="bibr" target="#b79">[80]</ref>, Reddy proposed a level of details control algorithm in virtual environment to remove extraneous details which the user cannot perceive and, thus, to optimize the computational resource assignment on rendering and display with little or no perceptual artifacts. Wang et al. <ref type="bibr" target="#b80">[81]</ref> gave a foveation scalable video coding algorithm for image compression with preselected fixation points. A similar technique based on a fixed gradient model of visual attention was used by Lee et al. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> to reduce spatial resolution of image nonuniformly, and the resolution reduced image can be then used for optimal rate control in video compression <ref type="bibr" target="#b81">[82]</ref>, image quality assessment <ref type="bibr" target="#b28">[29]</ref>, and video communication <ref type="bibr" target="#b82">[83]</ref>; the eye fixation positions are either predefined or found by an eye tracker.</p><p>Some other techniques with embedded visual attention estimation were proposed in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b84">[85]</ref>- <ref type="bibr" target="#b86">[87]</ref>. As mentioned in the Introduction, Osberger et al. <ref type="bibr" target="#b29">[30]</ref> combined his bottom-up visual attention model with Teo's VQM <ref type="bibr" target="#b10">[11]</ref> for accuracy improvement by adding weights to attentional areas. In the multiple resolution rendering technique proposed by Cater et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PERCEPTUAL QUALITY SIGNIFICANCE MAP (PQSM) ESTIMATION</head><p>As we have already known, more computational resource of the human brain is allocated to high attentional areas than low attentional areas, and this is the reason of visual attention's modulation on visual sensitivity in different areas. We propose a new numerical expression, PQSM, to represent the combined effect of the modulation of visual attention and the extra computational cost imposed by motion suppression, on visual sensitivity and visual perception. The PQSM is designed to reflect the statistical allocation of the human brain's processing resource on local visual contents. We do not attempt to implement a full visual attention model but rather aim at a practical solution inspired by the relevant physiological and psychological evidence. The concept was first proposed in <ref type="bibr" target="#b87">[88]</ref>, and improved in <ref type="bibr" target="#b88">[89]</ref>. As pointed out in Section II-A1, conceptually, the influence of motion suppression can be regarded as an aftereffect of visual attention. However, for the convenience in computation, the PQSM for an image can be determined as the product of the influence of motion suppression and the visual attention measure derived from the other stimuli/features, since motion suppression is a global factor (Section III-C). The hierarchical PQSM can be extended to the temporal axis for video, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The proposed three modules to generate a PQSM are illustrated in the left-hand side portion of Fig. <ref type="figure" target="#fig_2">2:</ref> 1) the visual  attention module to extract and integrate multiple stimuli/features to form a visual attentional map; 2) the post-processing module to smooth and implement a flexible gradient model for the modified visual attentional map; and 3) the motion suppression module to include motion influence for the final PQSM. Fig. <ref type="figure" target="#fig_3">3</ref> shows the next level of details for the PQSM generation. Only those well-established findings on visual attention presented in Section II will be modeled in the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Attention Module</head><p>The proposed visual attention model is driven by both bottom-up, space-based stimuli (color, texture, and motion) and top-down, object-based features (skin color and face). Selection of the stimuli/features is based on a balance of computational efficiency and output accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Derivation of Stimuli and Features:</head><p>• Color contrast: Color contrast is one of the basic stimuli that draws attention, and a bigger color difference from the background color usually attracts higher attention. A three-step approach has been developed for deriving and scaling a color contrast stimulus in an RGB color image. 1) The dynamic k-means algorithm clusters the 8 8 image regions with a predefined variation. If the size of the biggest cluster is more than a threshold (e.g., a half of the image), the background color is calculated as the mean in the cluster; otherwise, color contrast is not a stimulus in the image, because only pixels with color sufficiently distinguished from the background attract attention (if no obvious reference cluster exists, no pixel would stand out due to its color). 2) The distance between a color value and the background color is calculated by <ref type="bibr" target="#b9">(10)</ref> The distance histogram is constructed:</p><p>, and is the number of pixels in the image with . 3) A scale is calculated by <ref type="bibr" target="#b10">(11)</ref> The color contrast stimulus is scaled as</p><formula xml:id="formula_1">(12)</formula><p>Finally, is truncated so that , where is used in this paper. Obviously, the bigger color variation from the dominant color a pixel is, the bigger contribution it has toward the PQSM. If more pixels in an image have bigger color variation, the impact of a certain color variation will not be as significant as the situation otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Texture contrast: The average gradient is obtained with horizontal and vertical Sobel operators in a region (of size 3 3 in this paper). The texture stimulus is derived and scaled with a similar three-step method as in the formation of color contrast stimulus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Motion: Motion is one of the major stimuli on visual attention <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>. Motion detected from video can be divided into relative motion and absolute motion.</p><p>The former is the object motion against the background or other objects in the scene, while the latter is the motion against the frame of viewing (the combination of camera motion and object motion in the real-word coordinates <ref type="foot" target="#foot_0">1</ref> ). Relative motion is a stimulus on visual attention. Black's multiple layer dense flow estimation algorithm <ref type="bibr" target="#b91">[92]</ref> is adopted to estimate the absolute motion , and the relative motion is estimated via Zhang's estimation algorithm <ref type="bibr" target="#b92">[93]</ref>. The scaled relative motion is obtained with the similar scaling procedures as Steps 1)-3) in the estimation of color contrast stimulus; Step 1) for clustering is not needed because the relative motion of background is always zero.</p><p>It is not easy to evaluate the effect of and toward visual significance in general. Based upon the observation on most digital images in practical use, here we give a simple heuristic set of rules. 1) Usually, the attentional level of an object is low when relative motion is low (as shown in the upper two rows of Table <ref type="table" target="#tab_1">I</ref>).</p><p>1) The attentional level of an object is relatively significant when relative motion is high. However, since the camera's motion often indicates the most important object/region in the visual field, the attentional contribution of a motion stimulus for an object with both high absolute motion and high relative motion is merely moderate (because the object is usually not of the primary interest); the highest attentional contribution occurs with low absolute motion and high relative motion. These two circumstances are represented as the two lower rows in Table <ref type="table" target="#tab_1">I</ref>.</p><p>The motion stimulus generated by relative motion should be adjusted by absolute motion (i.e., motion mapping) <ref type="bibr" target="#b12">(13)</ref> where is an adjusting function defined by Table <ref type="table" target="#tab_2">II</ref>, based on the concept of Table <ref type="table" target="#tab_1">I</ref> and the experimental results with the standard video sequences commonly in use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Skin color: We include some useful cognitive features to achieve a more effective extraction for practical usage. Human body and warm color <ref type="bibr" target="#b73">[74]</ref> are always cognitive-related features on visual attention. A statistical model is adopted to detect regions with skin color on domain. The skin color stimulus is denoted as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Face: As another cognitive feature, face is a more significant stimulus on visual attention. Rowley's face detection algorithm <ref type="bibr" target="#b93">[94]</ref> is used to locate faces in image.</p><p>The result of skin color detection is used to recorrect the face detection result because Rowley's algorithm is merely based on gray-scale image. The face stimulus is denoted as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Integration:</head><p>Although it has been proven that bottom-up controlled attention and top-down controlled attention are processed in different areas in the brain <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b94">[95]</ref>, their integration is mostly processed in the prefrontal cortex <ref type="bibr" target="#b94">[95]</ref>. Nothdurft's nonlinear additivity model <ref type="bibr" target="#b66">[67]</ref> is adapted to integrate the stimuli and features together. In his model, the combined saliency effect of stimuli and can be expressed as <ref type="bibr" target="#b13">(14)</ref> where <ref type="bibr" target="#b14">(15)</ref> where represents the adjustment for the combined effect, and represent the cross-dimensional coupling factors between and , and and fall in the range of [0, 1]. In this paper, ( <ref type="formula">14</ref>) is extended to stimuli/features <ref type="bibr" target="#b15">(16)</ref> where , ; is an appropriate nonlinear function, and, in this paper, is chosen; only the coupling with the main stimulus/feature is considered. A bigger value for ( in the current implementation) is adopted because of the high correlation between the face and skin color. On the other hand, it is believed that color and luminance contrasts attract independent attention <ref type="bibr" target="#b69">[70]</ref>, so we have . We have , and the other coupling factors are set to 0.25. Obviously, more research is needed in determining the optimum parameters.</p><p>Equation ( <ref type="formula">16</ref>) satisfies <ref type="bibr" target="#b16">(17)</ref> where represents the combined effect of visual stimuli. We can see that ( <ref type="formula">14</ref>) is equivalent to <ref type="bibr" target="#b15">(16)</ref> when .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Post-Processing Module</head><p>For efficiency, the post-processing module is performed on block representation <ref type="bibr" target="#b17">(18)</ref> where is index of block representation, is a collection of pixels in block , is the size of , and are visual attention values in block . In this paper, the block size is set to 8 8 and . A flexible kernel is used to mimic the flexible gradients with visual attention <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> and is defined as</p><formula xml:id="formula_2">if if (<label>19</label></formula><formula xml:id="formula_3">)</formula><p>where and . The normalized kernel is obtained by <ref type="bibr" target="#b19">(20)</ref> Obviously, the proposed kernel fits the flexible gradients of visual attention: When the value of is high, the result of the kernel is steep, and vice verse.</p><p>The enhanced visual attentional map is derived as ( <ref type="formula">21</ref>)   To address the visual attention's capability limit issue mentioned in Section II-C, the visual attentional map is further modified as if if <ref type="bibr" target="#b21">(22)</ref> where is a predefined constant to reflect the visual attentional capacity of the human brain. are the horizontal and vertical block size of video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motion Suppression Module</head><p>Motion suppression is caused by object motion on the retina, which can be measured by absolute motion if the eye's movement on the visual field is smooth and slow. The relationship between visual attention and motion suppression is outlined in Table <ref type="table" target="#tab_3">III</ref>, so the motion suppression effect can be determined as the product of and the influence of motion suppression <ref type="bibr" target="#b22">(23)</ref> where is a function defined in Table <ref type="table" target="#tab_4">IV</ref>, which is an implementation of Table <ref type="table" target="#tab_3">III</ref>. Table IV is set for pixels, in line with the response speed of actual video acquisition devices (if pixels, the image is prone to blurring error). Tables II and III are defined for video in PAL (720 576, 50 Hz) format. As for NSTC (720 486, 60 Hz) video, is adjusted by multiplying by a factor of 6/5. In general, the motion measures in ( <ref type="formula">23</ref>) should be degrees per second. With fixed viewing distance and display framerate, pixels per frame is equivalent to degrees per second. where , , , and are the corresponding modulation functions, as exemplified in Fig. <ref type="figure" target="#fig_4">4</ref>. In general, with a higher PQSM measure, , , and take lower values, and takes a higher value. For PQSM-modulated JND Model 2, ( <ref type="formula">6</ref>) can be rewritten with the th DCT block of th channel as <ref type="bibr" target="#b29">(30)</ref> where the modulatory aftereffect on contrast orientation is ignored, and</p><p>and can be yielded by substituting</p><p>for and in ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>). , , and are the corresponding modulation functions, as exemplified in Fig. <ref type="figure" target="#fig_4">4</ref>. For , , and , the modulation functions also can be expressed as <ref type="bibr" target="#b33">(34)</ref> and for , , , and , they can be expressed as <ref type="bibr" target="#b34">(35)</ref> where is a factor to control the slope for each modulation function.</p><p>With higher PQSM measure, the turning point of temporal and spatial contrast masking curves ( and ) are pushed to higher frequencies, and the luminance adaptation tolerances are reduced. Itti's experimental results mentioned in Section II-A.1 have been used in determining the actual maximum and minimum values of each modulation function. The value of each modulation function are obtained by tuning the combined effect to fit Itti's experimental results, as shown in Table <ref type="table" target="#tab_5">V</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PQSM-BASED VIDEO QUALITY METRICS (VQMS)</head><p>It is expected that the PQSMs modulation on visual sensitivity models enhances the performance of different VQMs. To demonstrate this, two VQMs are used in this paper: VQM is a PQSM modulated MSE metric, and VQM is the PQSM modulated Wang's SSIM <ref type="bibr" target="#b15">[16]</ref>. Only luminance component has been used for the sake of efficiency, since luminance plays a much more important role in human visual perception than chrominance components.</p><p>Let , , and denote the original video sequence, the degraded video sequence, and the JND profile estimated by PQSM-modulated JND Model 1, respectively. The perceptual distortion of VQM can be expressed as <ref type="bibr" target="#b35">(36)</ref> where if if <ref type="bibr" target="#b36">(37)</ref> In <ref type="bibr" target="#b35">(36)</ref>, any distortion below the detectability threshold is excluded from accumulation for the visual distortion score. Equations ( <ref type="formula">36</ref>) and ( <ref type="formula">37</ref>) are enhanced formulation to Chou and Li's peak signal-to-perceptual noise ratio (PSPNR) <ref type="bibr" target="#b7">[8]</ref>, since an above-threshold error is scaled by the threshold.</p><p>Because SSIM values are block-based <ref type="bibr" target="#b15">[16]</ref>, the perceptual distortion of VQM can be expressed for th block as <ref type="bibr" target="#b37">(38)</ref> where</p><formula xml:id="formula_6">if if (<label>39</label></formula><formula xml:id="formula_7">)</formula><p>where represents the th block, and denote the block-based and . and are the constants to map PQSM values into an appropriate range nonlinearly. With the choice of , VQM accounts for the effect of both PQSM and : If the maximum block-based error is below , it is not accumulated for ; otherwise, the PQSM values are nonlinearly and monotonously scaled as the weighting for visual annoyance measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PQSM Estimation</head><p>The intermediate and final PQSM estimation results on the 30th frame of Suzie, Autumn leaves and Foreman video test sequences are shown in Figs. <ref type="figure" target="#fig_5">5</ref><ref type="figure" target="#fig_6">6</ref><ref type="figure" target="#fig_7">7</ref>, respectively. In Fig. <ref type="figure" target="#fig_5">5</ref>, all detected stimuli and features have contributed in forming the final PQSM indicating the attentional levels in the image, largely in line with the human perception. The shapes of the face and the eyes are somewhat arbitrarily set, since Rowley's algorithm <ref type="bibr" target="#b93">[94]</ref> only indicates the upper right and bottom left corners of a face, and the centers of eyes. This simple representation of face and eyes is not accurate, but sufficient for the application. Fig. <ref type="figure" target="#fig_6">6</ref> shows the results for a natural scene, where the detected stimuli and features except for human faces contribute to the final result.   It is worth noting that some false skin color has been detected but does not bring about significant impact toward the final PQSM. In Fig. <ref type="figure" target="#fig_7">7</ref>, face detection is a failure (face detection is currently still a challenging task because of the variability in scale, location, orientation, and pose <ref type="bibr" target="#b96">[97]</ref>), but the skin color and relative motion contrast still lead to reasonable alignment with the HVS observation. Inclusion of mutlistimuli/features helps to increase the application scopes and algorithm robustness. In general, relative motion, face and skin color are the more important factors to the resultant PQSM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PQSM-Modulated JND Models</head><p>To evaluate the performance of proposed PQSM-modulated JND Models 1 and 2 against their original models, noise is injected into video according to the JND models. For a pixel-based JND model (i.e., the original Yang's model <ref type="bibr" target="#b8">[9]</ref> or PQSM-modulated JND Model 1), a noise-injected image frame can obtained as <ref type="bibr" target="#b39">(40)</ref> where is for Yang's model, or for PQSM-modulated JND Model 1;</p><p>for perceptually lossless noise (if the visibility threshold is correctly determined), and for perceptually lossy noise;</p><p>gives a random number, and is used here just to control the sign of the associated term in <ref type="bibr" target="#b39">(40)</ref> so that no artificial pattern is added in the spatial space and along the temporal axis.</p><p>Such a noise injection scheme can be used to examine the performance of against . A more accurate JND model should derive a noise injected image (or video) with better visual quality under the same level of noise (controlled by ), because it is capable of shaping more noise onto the less perceptually significant regions in the image. PSNR is used here just to denote the injected noise level under different test conditions. With the same PSNR, the JND model relating to a better subjective visual quality score is a better model. Alternatively, with the same perceptual visual quality score, the JND model relating to a lower PSNR is the better model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For a DCT-based JND profile</head><p>, the injected noise is obtained as <ref type="bibr" target="#b40">(41)</ref> where is for the original Watson's model <ref type="bibr" target="#b4">[5]</ref>, or for PQSM-modulated JND Model 2; and have similar meanings as in <ref type="bibr" target="#b39">(40)</ref>. Therefore, the degraded video sequence is <ref type="bibr" target="#b41">(42)</ref> where DCT and DCT denote DCT transform and inverse DCT transform, respectively.</p><p>To compare Yang's JND model and the proposed PQSMmodulated JND Model 1, the noise injected images are used for four test sequences: Suzie, Autumn leaves, Foreman, and Harp. An example of the generated JND profiles and noise injected images are shown in Fig. <ref type="figure" target="#fig_8">8</ref>. Fig. <ref type="figure" target="#fig_8">8(a)</ref> shows the generated JND profile by PQSM-modulated JND model 1 while Fig. <ref type="figure" target="#fig_8">8(b)</ref> shows the generated JND profile by Yang's JND model, with the upper, lower left, and lower right parts being the , , and components, respectively; Fig. <ref type="figure" target="#fig_8">8(c</ref>) and (e) are the noise-injected images by PQSM-modulated JND Model 1 with different PSNR levels; Fig. <ref type="figure" target="#fig_8">8(d</ref>) and (f) are the noise-injected images by Yang's Model, with similar or slightly higher PSNRs in comparison with Fig. <ref type="figure" target="#fig_8">8(c</ref>) and (e). Fig. <ref type="figure" target="#fig_9">9</ref> gives a close-up view in the most sensitive region (i.e., with highest PQSM values) of Fig. <ref type="figure" target="#fig_8">8</ref> for better visualization, and, as can be seen, PQSM-modulated JND Model 1 yields better visual quality in the noise-injected images.</p><p>PQSM-modulated JND Model 2 is compared with Watson's JND model in the similar manner. Fig. <ref type="figure" target="#fig_10">10</ref> shows a close-up view in the most sensitive region of the 30th frame of Suzie sequence under two PSNR conditions. As expected, PQSM-modulated JND Model 2 yields better visual quality in the noise-injected images.</p><p>To confirm the above-mentioned visual quality observation, formal subjective viewing tests have been conducted for the noise-injected sequences based on the two pairs of JND models (namely, Yang's JND model and PQSM-modulated JND Model 1, Watson's JND model and PQSM-modulated JND Model 2), with the noise conditions listed in the upper portions of Tables VI and VII. Each display and scoring session for a pair of noise-injected sequences generated by a pair of JND models is organized as: Video Sequence I, two seconds of grey screen, Video Sequence II, two seconds of grey screen, Video Sequence I, two seconds of grey screen, Video Sequence II, two seconds of grey screen, scoring. Both the display order of the sequences in a session, the order of noise-injection conditions, and the order of th four test sequences were randomized for subjects. The preference opinion scores (POSs) are given by subjects to indicate the quality comparison: 1)</p><p>, the visual quality of Sequence is better than Sequence ; 2) , viewer can not decide which sequence has better quality; and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>, the visual quality of Sequence is better than Sequence .</p><p>Eight subjects (four of them are with average image processing knowledge and the rest are naive) were involved in the experiments. Their eyesight is either normal or has been corrected to be normal with spectacles. The subjective visual quality assessment was performed in a typical laboratory environment with normal fluorescent ceiling light, using a EIZO T965 professional color monitor with resolution of 1600 1200, screen refresh rate at 85 Hz. The luminance, Gamma curve and Saturation setup are using the "Movie" display mode. The viewing distance is approximately four times of the image height.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PQSM-Based VQMs</head><p>In Section V, VQM and VQM have been formulated after the consideration of the modulatory aftereffects of visual attention and motion suppression. In this section, their performance is compared against their original forms before such modulatory aftereffects are included, that is, the PSNR (equivalent to MSE) measure and the SSIM measure <ref type="bibr" target="#b15">[16]</ref>. The performance has been evaluated, using the most extensive publicly-accessible database for visual quality assessment, VQEG Phase-I test set <ref type="bibr" target="#b97">[98]</ref>, which includes 20 SDTV test sequences, their 320 decoded sequences and the associated subjective rating results [difference mean opinion scores (DMOSs)].</p><p>In <ref type="bibr" target="#b97">[98]</ref>, a three-parameter logistic function is used to estimate the predicted DMOS from VQR (video quality rating) and the output of a VQM <ref type="bibr" target="#b42">(43)</ref> where , , and are parameters derived from fitting VQR to DMOS. Three methods are adopted in VQEG <ref type="bibr" target="#b97">[98]</ref> to evaluate the prediction accuracy of VQMs.</p><p>• Method 1 : Pearson linear correlation coefficient between DMOS and DMOS.</p><p>• Method 2 : Spearman rank order correlation coefficient between DMOS and DMOS.</p><p>• Method 3 : Outlier ratio of outlier sequences to the total number of testing sequences. The higher and are and the lower is, the better match with DMOS a VQM achieves. In the ideal match, and have the value of 1 and is 0. The experimental results are listed in Table <ref type="table" target="#tab_9">VIII</ref>, comparing VQM with the PSNR measure and VQM with the SSIM measure. The three major test groups of the VQEG Phase-I data have been used: 1) the 50-Hz set with 180 decoded sequences in PAL format; 2) the 60-Hz set with 180 decoded sequences in NSTC format; and 3) all 320 decoded sequences. The PSNR results are obtained from <ref type="bibr" target="#b97">[98]</ref>, while the SSIM results are obtained from <ref type="bibr" target="#b15">[16]</ref>. As can be seen, VQM outperforms the PSNR in all three test groups and all three evaluation methods ( to ). For the comparison between VQM and the SSIM, although the SSIM results of the 50-and 60-Hz data sets are unavailable, we can still see that the accuracy of VQM is improved over the SSIM assessment for all 320 decoded sequences. As for the comparison between VQM and VQM , the performance of VQM is better than that of VQM , because VQM accounts for structural errors and visual annoyance in decoded images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>This paper first gives a comprehensive review on the existing research effort related to the studies of visual attention, computational models of visual sensitivity, and perceptual quality/distortion metrics. We then demonstrate that accounting for visual attention's modulatory aftereffects improves the visual sensitivity and the quality/distortion evaluation. In this paper, we build the model according to the basic ideas of visual attention that are just sufficient to the problems being tackled and also include important cognitive features for more effective applications in practice.</p><p>A numerical measure of visual attention's modulatory aftereffects in an image or video, PQSM, is proposed. The devised PQSM estimation algorithm detects motion, color contrast and texture contrast as bottom-up stimuli, and also skin color and face top-down features, in this paper. Two PQSM-modulated visual sensitivity (JND) models and two PQSM-modulated VQMs are also presented for evaluating the proposed PQSM. Extensive experimental results with subjective viewing confirm the improvement on both JND determination and visual quality gauge.</p><p>The proposed PQSM basically provides local perceptual cues of significance toward visual quality. How picture quality is gauged shapes the design, implementation and optimization of most visual processing tasks. Therefore, apart from the demonstrated applications in JND determination and visual quality gauge, the proposed PQSM can facilitate perceptually-optimized image/video compression, watermarking, error resilience and many other processes.</p><p>As for the possible further work, more features/stimuli may be included, especially semantic features and auditory stimuli (auditory stimuli play an important role on spatial attention selection <ref type="bibr" target="#b98">[99]</ref>). Temporal effect of visual attention is another direction for improvement <ref type="bibr" target="#b95">[96]</ref>. It is believed that bottom-up stimuli are dominative on visual attention during a short period (100 800 ms) when scene appears/changes, and afterwards, top-down features will possibly dominate. On the other hand, after a period (several seconds to a few minutes, depending on the type of stimulus), a stimulus' attentional level decreases with the time of its appearance in the visual field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General hierarchical PQSM [h: hierarchical maps for an image (from the full-resolution map to the roughest map)].</figDesc><graphic coords="4,354.72,66.06,143.00,90.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc><ref type="bibr" target="#b84">[85]</ref>, a task map built by a bottom-up visual attentional model modulates the spatiotemporal CSF to guide a progressive animation system taking full advantage of image-based rendering. In Dhavale et al.'s paper<ref type="bibr" target="#b85">[86]</ref>, Itti's attention model<ref type="bibr" target="#b73">[74]</ref> is combined with a foveation filter to keep more details in video at predicted eye fixation positions. Yang et al.<ref type="bibr" target="#b86">[87]</ref> used a visual sensitivity map modulated by skin color based attention for rate control in videophone compression applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of generating PQSM modulated visual sensitivity map.</figDesc><graphic coords="5,93.12,66.06,144.00,177.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flowchart of PQSM generation.</figDesc><graphic coords="5,51.12,273.42,228.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Modulation functions for PQSM-modulated JND Model 1 and 2.</figDesc><graphic coords="7,314.10,349.44,228.00,87.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example 1 on PQSM estimation: (a) 30th frame of video sequence "Suzie"; (b) absolute motion map; (c) relative motion contrast stimulus; (d) face-eye stimulus; (e) skin color stimulus; (f) color contrast stimulus; (g) texture stimulus; (h) integrated visual attentional map; (i) block-based attentional map after post-processing; and (j) the final PQSM with motion suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example 2 on PQSM estimation: (a) 30th frame of video sequence "Autumn leaves"; (b) absolute motion map; (c) relative motion contrast stimulus; (d) face-eye stimulus; (e) skin color stimulus; (f) color contrast stimulus; (g) texture stimulus; (h) integrated visual attentional map; (i) block-based attentional map after post-processing; and (j) the final PQSM with motion suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Example 3 on PQSM estimation: (a) 30th frame of video sequence "Foreman"; (b) absolute motion map; (c) relative motion contrast stimulus; (d) face-eye stimulus; (e) skin color stimulus; (f) color contrast stimulus; (g) texture stimulus; (h) integrated visual attentional map; (i) block-based attentional map after post-processing; and (j) the final PQSM with motion suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Embedding noise into the 30th frame of video "Foreman": (a) JND map generated by the proposed PQSM-modulated JND Model 1; (b) JND map generated by Yang's model; (c) noise-injected image based on PQSM-modulated JND Model 1 with PSNR = 32:27 dB; (d) noise-injected image based on Yang's JND model with PSNR = 32:84 dB; (e) noise-injected image based on PQSM-modulated JND Model 1 with PSNR = 24:61 dB; and (f) noise-injected image based on Yang's JND model with PSNR = 25:18 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Details of the most sensitive (with the highest PQSM values) region of Fig. 7: (a) original image; (b) noise-injected image based on PQSM-modulated JND Model 1 with PSNR = 32:27 dB; (c) noise-injected image based on Yang's JND model with PSNR = 32:84 dB; (d) noise-injected image based on PQSM-modulated JND Model 1 with PSNR = 24:61 dB; and (e) noise-injected image based on Yang's JND model with PSNR = 25:18 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Details of most sensitive (with the highest PQSM values) region of sequence "Suzie": (a) original image; (b) noise-injected image based on PQSM-modulated JND Model 2 with PSNR = 31:26 dB; (c) noise-injected image based on Watson's JND model with PSNR = 31:37 dB; (d) noise-injected image based on PQSM-modulated JND Model 2 with PSNR = 27:53 dB; and (e) noise-injected image based on Watson's JND model with PSNR = 27:59 dB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Modeling Visual Attention's Modulatory Aftereffects on Visual Sensitivity and Quality Evaluation Zhongkang Lu, Senior Member, IEEE, Weisi Lin, Senior Member, IEEE, Xiaokang Yang, Senior Member, IEEE, EePing Ong, and Susu Yao</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I MOTION</head><label>I</label><figDesc>ATTENTIONAL LEVEL BY RELATIVE MOTION AND ABSOLUTE MOTION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ADJUSTING</head><label>II</label><figDesc>FUNCTION g (v ; v ) (v IS THE MAXIMUM VALUE OF SCALED RELATIVE MOTION)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SUPPRESSION</head><label>III</label><figDesc>ON VISUAL ATTENTION BY ABSOLUTE MOTION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV ABSOLUTE</head><label>IV</label><figDesc>MOTION SUPPRESSION FUNCTION f (vam ; v )</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V VALUES</head><label>V</label><figDesc>OF PARAMETER IN MODULATION FUNCTIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VI is the viewing results between Yang's JND model and PQSM-modulated JND Model 1, and Table VII is the results between Watson's JND model and PQSM-modulated JND Model 2. In Table VI, indicates that the quality of the sequence with Yang's JND Model is better, indicates that two sequences has the same quality, and indicates that the quality of the sequence with PQSM-modulated JND Model 1 is better.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI POSs</head><label>VI</label><figDesc>OF THE NOISE-INJECTED IMAGES GENERATED BY YANG'S JND MODEL AND PQSM-MODULATED JND MODEL 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII POSs</head><label>VII</label><figDesc>OF THE NOISE-INJECTED IMAGES GENERATED BY WATSON'S JND MODEL AND PQSM-MODULATED JND MODEL 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII PERFORMANCE</head><label>VIII</label><figDesc>COMPARISON FOR VQM A AND VQM B WITH VQEG PHASE-I DATA SET Similar notations are used in TableVII. Subjective viewing results confirm that accounting for the modulatory aftereffects of visual attention and motion suppression enhances the performance of JND models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If the camera follows exactly the object, the absolute motion is zero.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. T. Kanade, Dr. T. Moriyama, and Dr. H. A. Rowley of Carnegie Mellon University for providing their face detection codes, and Assoc. Prof. M. J. Black of Brown University for providing source code of his optical flow algorithm.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zhongkang Lu (S'97-M'99-SM'04) received the B.Eng. degree in biomedical engineering from Southeast University, Nanjing, China, in 1993, and the M.Eng. and Ph.D. degrees in electrical engineering from Shanghai Jiaotong University, Shanghai, China, in 1996 and 1999, respectively.</p><p>Between 1996 and 1998, he was an exchange student with the Department of Electronic and Information Engineering, the Hong Kong Polytechnic University. From 1999 to 2001, he was a Research Fellow with School of Electrical and Electronic Engineering, Nanyang Technological University. Thereafter, he joined the Institute for Infocomm Research, Singapore, as a Research Scientist. His research interests include computational aspects of human vision, perceptual visual signal processing, pattern recognition, and computer vision.</p><p>Weisi Lin (M'92-SM <ref type="bibr">'98)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Visible Differences Predictor: An Algorithm for The Assessment of Image Fidelity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="179" to="206" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A distortion measure for blocking artifacts in image based on human visual sensitivity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="1995-06">Jun. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perceptual feedback in multigrid motion estimation using an improved Dct quantization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Epifanio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Artigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1411" to="1427" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision Models and Quality metrics for image processing applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process. Lab., Ecole Polytecnique Federale De Lausanne (EPFL), Swiss Fed. Inst. Technol., Lausanne</title>
		<imprint>
			<date type="published" when="2000-12">Dec. 2000</date>
			<pubPlace>Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DVQ: A digital video quality metric based on human vision</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mcgowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Elect. Imag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal sensitivity and visual attention for efficient rendering of dynamic environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="page" from="39" to="65" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial noise shaping based on human visual sensitivity and its application to image coding</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A perceptually tuned subband image coder based on the measure of just-noticeable-distortion profile</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Just-noticeable-distortion profile with nonlinear additivity model for perceptual masking in color images</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2003-04">Apr. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="609" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Luminance-model-based DCT quantization for color image compression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision, Visual Processing, and Digital Display III</title>
		<meeting>SPIE Human Vision, Visual essing, and Digital Display III</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1666</biblScope>
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual image distortion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1994-11">Nov. 1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="982" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Method and apparatus for estimation digital video quality without using a reference video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual color image quality metric using adequate error pooling for coding scheme evaluation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision and Electronic Imaging VII</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<meeting>SPIE Human Vision and Electronic Imaging VII<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">Jun. 2002</date>
			<biblScope unit="volume">4662</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image coding quality assessment using fuzzy integrals with a three-component image model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new distortion measure for video coding blocking artifacts</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCT</title>
		<meeting>ICCT</meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="658" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error measurement to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low bit rate video quality assessment based on perceptual characteristics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moschetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Processing</title>
		<meeting>Int. Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of Hbrid MC/DPCM/DCT video coding distortions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process</title>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="247" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visual attention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<editor>B. Goldstein</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Blackwell</publisher>
			<biblScope unit="page" from="272" to="310" />
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Principles of Psychology</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1890">1890</date>
			<publisher>Harvard Univ. Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion understanding: task-directed attention and representations that like perception with action</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="280" />
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Models of bottom-up and top-down visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Computat. Neur. Syst., California Inst. Technol</title>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<pubPlace>Pasadena</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The neural mechanisms of top-down attentional control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hopfinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Buonocore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Mangun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="284" to="291" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mechanisms of visual attention in the human cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="341" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sensory mediation of stimulus-driven attentional capture in multiple-cue displays</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychophys</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crossmodal links between audition and touch in covert endogenous spatial attention</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcglone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychophys</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="901" to="924" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the role of selective attention in visual perception</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="825" to="830" />
			<date type="published" when="1998-02">Feb. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the modulatory effect of attention on human spatial vision</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Foveated video quality assessment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual vision models for picture quality assessment and compression applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Osberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space Centre for Satellite Navigation</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From low level perception to high level perception, a coherent approach for visual attention modeling</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thoreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Francois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision and Electronic Imaging IX</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<meeting>SPIE Human Vision and Electronic Imaging IX<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-01">Jan. 2004</date>
			<biblScope unit="volume">5292</biblScope>
			<biblScope unit="page" from="284" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pattern thresholds for moving and stationary gratings during smooth eye movement</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrast dependency of saccadic compression and suppression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Michels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lappe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2327" to="2336" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Engineering observations from spatiovelocity and spatiotemporal visual models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Daly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Models and Applications to Image and Video Processing</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="179" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An ERP study of sustained spatial attention to stimulus eccentricity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psych</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamics of the spatial scale of visual attention revealed by brain event-related potentials</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parasuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Brain Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentional modulation of threshold sensitivity to first-order motion and second-order motion patterns</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ledgeway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="2927" to="2936" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Research into the dynamic nature of the human fovea cortex system with intermittent and modulated light. I. Attenuation characteristics with white and colored light</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="777" to="784" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G J</forename><surname>Barten</surname></persName>
		</author>
		<title level="m">Contrast Sensitivity of the Human Eye and Its Effects on Image Quality</title>
		<meeting><address><addrLine>Bellingham, WA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wavelet-based color image compression: exploiting the contrast sensitivity function</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Nadenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="70" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L D</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K D</forename><surname>Valois</surname></persName>
		</author>
		<title level="m">Spatial Vision</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vision channels, contrast sesitivity, and functional vision</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><forename type="middle">P</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision and Electronic Imaging IX</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<meeting>SPIE Human Vision and Electronic Imaging IX<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-01">Jan. 2004</date>
			<biblScope unit="volume">5292</biblScope>
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactions of chromatic components on the perceptual quantization of the achromatic component</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision and Electronic Imaging</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<meeting>SPIE Human Vision and Electronic Imaging<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="volume">3644</biblScope>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust approach for color image quality assessment</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Visual Communications and Image Processing</title>
		<meeting>Int. Conf. Visual Communications and Image essing</meeting>
		<imprint>
			<date type="published" when="2003-07">Jul. 2003</date>
			<biblScope unit="volume">5150</biblScope>
			<biblScope unit="page" from="1573" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Enhancement of perceptual sensitivity as the result of selectively attending to spatial locations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Bashinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Bacharach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychopys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="280" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sensitivity and criterion effects in the spatial cueing of visual attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychopys</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="383" to="399" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stimulus probability directs spatial attention: an enhancement of sensitivity in humans and monkeys</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Ciaramitaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Glimcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A locally adaptive perceptual masking threshold model for image coding</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Safranek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1883" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A perceptual model for JPEG applications based on block classification, texture masking, and luminance masking</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="428" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A perceptually optimized 3-D subband image codec for video communication over wireless channels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="1996-02">Feb. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detectability and annoyance value of MPEG-2 artifacts inserted in uncompressed video sequences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Human Vision and Electronic Imaging V</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Rogowitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Pappas</surname></persName>
		</editor>
		<meeting>SPIE Human Vision and Electronic Imaging V<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="volume">3959</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Feature-based attention influences motion processing gain in macaque visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C M</forename><surname>Trujillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="page" from="575" to="579" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The role of attention in temporal integration</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A W</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="145" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention enhances feature integration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Schyns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1793" to="1798" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatiotemporal characteristics of dynamic feature binding in visual working memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2107" to="2123" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The information capacity of visual attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Pelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="983" to="995" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The effect of spatial cues on visual sensitivity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attention and scintillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vanrullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="2191" to="2196" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The spatial structure of visual attention</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<editor>Attention and Performance XI, M. I. Posner and O. S. Marin</editor>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Lawrence Erlbaum</publisher>
			<biblScope unit="page" from="171" to="187" />
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual attention within and around the field of focal attention: A zoom lens model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D S</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception Psychophys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Numerical calculation of electrostatic field surrounding a human head in visual display environments</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Michelsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Elect</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Changeblindness as a result of &apos;mudsplashes</title>
		<author>
			<persName><forename type="first">V</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kearsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amemiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cookson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attention induced motion blindness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahraie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niedeggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1613" to="1617" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The capacity of visual working memory for features and conjunctions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="279" to="281" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Large capacity storage of integrated objects before change blindness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Spekreijsea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A F</forename><surname>Lamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="164" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spatial attention and object-based attention: A comparison within a single task</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Salience from feature contrast: additivity across dimensions</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Nothdurft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1183" to="1201" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Salience from feature contrast: temporal properties of saliency mechanisms</title>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2421" to="2435" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Salience from feature contrast: variations with texture density</title>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3181" to="3200" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Color and luminance contrasts attract independent attention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Denti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1134" to="1137" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Broadbent</surname></persName>
		</author>
		<title level="m">Perception and Communication</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Pergamon</publisher>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: toward the underlying neural circuitry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhanea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K W</forename><surname>Winkya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuzhonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Artif. Intell.</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Brain Theory and Neural Networks</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arbib</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1196" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A computational model of attentive visual system induced by cortical neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koshizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Akatsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsujino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="881" to="887" />
			<date type="published" when="2002-06">Jun. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A feature integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psych</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">JPEG 2000 Part I Final Committee Draft Version 1.0, ISO/IEC JTC 1/SC 29/WG 1 (ITU-T SG8) Std</title>
		<imprint>
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Part II Final Committee Draft Version 1.0, ISO/IEC JTC 1/SC 29/WG 1 (ITU-T SG8) Std</title>
		<author>
			<persName><surname>Jpeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-12">2000. Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">JPEG2000 still image coding system: An overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Skodras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consum. Electron</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1103" to="1127" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Perceptually modulated level of detail for virtual environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Dept. Comp. Sci., Univ. Edinburgh</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Edinburgh, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Embedded foveation image coding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1397" to="1410" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Foveated video compression with optimal rate control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="977" to="992" />
			<date type="published" when="2001-07">Jul. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fast algorithms for foveated video processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Generalized bitplane-by-bitplane shift method for JPEG2000 ROI coding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2002-09">Sep. 2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">detail to attention: exploiting visual tasks for selective rendering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Eurographics Workshop on Rendering</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Christensen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</editor>
		<meeting>13th Eurographics Workshop on Rendering</meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Saliency-based multi-foveated MPEG compression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 7th Int. Symp. Signal Processing and Its Applications</title>
		<imprint>
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Local visual perceptual clues and its use in videophone rate control</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>presented at the ISCAS</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Perceptualquality significance map (PQSM) and its application on video quality distortion metrics</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>Hong Kong</publisher>
			<date type="published" when="2003-04">Apr. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="617" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">PQSM based RF and NR video quality metrics</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE VCIP</title>
		<meeting>SPIE VCIP</meeting>
		<imprint>
			<date type="published" when="2003-07">Jul. 2003</date>
			<biblScope unit="volume">5150</biblScope>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Attention and apparent motion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="219" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Neural strength of visual attention gauged by motion adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1015" to="1018" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: parametric and piecewise-smooth flow fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Global motion estimation and robust regression for video coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>presented at the ICASSP</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Distributed neural activity during object, spatial and integrated processing in humans</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Simon-Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brodsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Brain Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The time course of selective visual attention: Theory and experiments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pollatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="2925" to="2945" />
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Detecting faces in images: A survey</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Final report from the video quality expert group on the validation of objective models of video quality assessment</title>
		<author>
			<persName><surname>Vqeg</surname></persName>
			<affiliation>
				<orgName type="collaboration">Video Quality Expert Group</orgName>
			</affiliation>
		</author>
		<ptr target="http://www.vqeg.org" />
		<imprint>
			<date type="published" when="2000-03">2000. Mar</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Identification of visual stimuli is improved by accompanying auditory stimuli: the role of eye movements and sound location</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Snowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="795" to="810" />
			<date type="published" when="2001-07">Jul. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
