<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SocioPhone: Everyday Face-to-Face Interaction Monitoring Platform Using Multi-Phone Sensor Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Youngki</forename><surname>Lee</surname></persName>
							<email>youngkilee@smu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Systems</orgName>
								<address>
									<country>Singapore Management University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chulhong</forename><surname>Min</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chanyou</forename><surname>Hwang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaeung</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Web Science and Technology Division</orgName>
								<orgName type="institution" key="instit2">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Inseok</forename><surname>Hwang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Mobile Software Platform</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Younghyun</forename><surname>Ju</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chungkuk</forename><surname>Yoo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miri</forename><surname>Moon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Web Science and Technology Division</orgName>
								<orgName type="institution" key="instit2">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uichin</forename><surname>Lee</surname></persName>
							<email>uclee@kaist.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Knowledge Service Engineering Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junehwa</forename><surname>Song</surname></persName>
							<email>junesong@nclab.kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SocioPhone: Everyday Face-to-Face Interaction Monitoring Platform Using Multi-Phone Sensor Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFF3D20C9AF8C24555299AC30AD083B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>K.8 [Personal Computing]: General; C.3 [Special-Purpose and Application-based Systems]: Real-time and embedded systems Interaction</term>
					<term>Conversation</term>
					<term>Social</term>
					<term>Platform</term>
					<term>Volume Topography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose SocioPhone, a novel initiative to build a mobile platform for face-to-face interaction monitoring. Face-toface interaction, especially conversation, is a fundamental part of everyday life. Interaction-aware applications aimed at facilitating group conversations have been proposed, but have not proliferated yet. Useful contexts to capture and support face-to-face interactions need to be explored more deeply. More important, recognizing delicate conversational contexts with commodity mobile devices requires solving a number of technical challenges. As a first step to address such challenges, we identify useful meta-linguistic contexts of conversation, such as turn-takings, prosodic features, a dominant participant, and pace. These serve as cornerstones for building a variety of interaction-aware applications. SocioPhone abstracts such useful meta-linguistic contexts as a set of intuitive APIs. Its runtime efficiently monitors registered contexts during in-progress conversations and notifies applications on-the-fly. Importantly, we have noticed that online turn monitoring is the basic building block for extracting diverse meta-linguistic contexts, and have devised a novel volume-topography-based method. We show the usefulness of SocioPhone with several interesting applications: SocioTherapist, SocioDigest, and Tug-of-War. Also, we show that our turnmonitoring technique is highly accurate and energy-efficient under diverse real-life situations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Face-to-face social interaction is an integral part of human life; everyday, people dine with family, have meetings with colleagues, and spend time with friends. A promising new direction for mobile sensing lies in capturing and utilizing sophisticated social contexts during daily face-to-face interactions. Early interaction-aware applications have been emerging and show its potential usefulness <ref type="bibr" target="#b22">[22]</ref> <ref type="bibr" target="#b27">[27]</ref>. For example, MeetingMediator <ref type="bibr" target="#b22">[22]</ref> displays the skew of individuals' verbal participations to promote group brainstorming. Another application helps a user remember the name of the person he is talking with, to help avoid the awkward experience of forgetting a name <ref type="bibr" target="#b27">[27]</ref>. However, building interaction-aware applications involves severe challenges without system-level support. First of all, such applications are still in an early stage and most developers do not know which contexts to leverage during daily conversations. Furthermore, monitoring conversations requires implementing complicated inference logics, repetitive learning and testing to improve recognition accuracy, and significant optimization of battery use.</p><p>In this paper, we propose SocioPhone, a mobile platform for faceto-face interaction monitoring. Ideally, a full-fledged interaction monitoring platform would capture a variety of communicative cues expressed during face-to-face interaction such as verbal cues (spoken words and sentences), aural cues (tones, pitch), and visual cues (gesture, eye contact). As a first step, SocioPhone focuses on monitoring meta-linguistic contexts that provide useful information about conversations without requiring computation-intensive semantic inference on conversation contents. SocioPhone provides applications with a set of intuitive APIs to monitor rich metalinguistic contexts on the fly (See Section 2); applications can submit simple monitoring requests to obtain contexts of interests. The SocioPhone runtime monitors registered contexts in a highlyefficient and precise manner, based on our new volume-topographybased turn monitoring technique (See Section 4).</p><p>In its core, SocioPhone monitors conversational turns, the basic unit of conversation; a turn is a continuous speech segment where a person starts and ends her speech <ref type="bibr" target="#b3">[3]</ref> <ref type="bibr" target="#b10">[10]</ref>. We have noticed that monitoring turns is a first crucial step to deriving many interesting aspects of a conversion, e.g., how long and often one talks, how quickly she responds, who talks more or less, and how fast a conversation progresses. More interestingly, turn analysis enables high-level social inference, such as one's role in a conversation and problematic situations <ref type="bibr" target="#b9">[9]</ref>[16] <ref type="bibr" target="#b35">[35]</ref>. Future mobile applications will be tightly interwoven with sophisticated interactions, e.g., dynamic conversational flows and relational behaviors, in-situ; this will enrich and broaden the set of potential applications, from interaction facilitations to collaborative decision making, and even to psychological care. In a broader view, monitoring turns can also serve as the prerequisite for speaker-specific vocal inference and content analysis in real-time, such as assessing a speaker's emotional state and performing deep semantic analysis.</p><p>Online turn monitoring is a primitive building block, but it is challenging to implement it on everyday personal mobile devices. Existing voice recognition techniques such as speaker recognition <ref type="bibr" target="#b7">[7]</ref> and speaker diarization <ref type="bibr" target="#b2">[2]</ref> <ref type="bibr" target="#b4">[4]</ref> rarely consider the challenges of mobile environments, e.g., unconstrained acoustic situations, realtime monitoring, and battery limitations. A potential approach to turn monitoring would be to continuously execute crafted speaker recognition logic, as in SpeakerSense <ref type="bibr" target="#b27">[27]</ref> (See Section 3.1). However, this has a number of shortcomings. First, short-lasting turns (1-2 seconds) are common in casual conversations <ref type="bibr" target="#b2">[2]</ref>, but cannot be detected reliably. Existing techniques mostly require long speech segments (e.g., 3-8 seconds.) for reliable recognition to ensure statistical confidence of the windowed voice samples with respect to the speaker-specific pre-constructed spectral model <ref type="bibr" target="#b27">[27]</ref> <ref type="bibr" target="#b40">[40]</ref>. More challenging, daily conversations do not occur in an ideal setting; dynamic ambient noises inevitably distort one's vocal signatures, leading to poor recognition accuracy. Furthermore, running speaker recognition on smartphones consumes significant power, (&gt; 400 mW) for high-rate sound sensing and heavy computation <ref type="bibr" target="#b27">[27]</ref> <ref type="bibr" target="#b31">[31]</ref>.</p><p>To address the challenges for online turn monitoring, we propose an on-the-spot multi-phone sensor fusion approach; multiple smartphones work together to detect turn changes and associated speakers, along with a short in-situ training. Naturally placed phones belonging to conversation group members simultaneously sense a speaker's voice signals, but capture the signals with different strengths depending on their positions. Such relative sensory readings can be fused in realtime to form a volume topography, i.e., a signature vector of volume values sensed over different phones.</p><p>Our key observation is that such a topography is unique to each speaker, showing enough discrimination power to identify turns and associated speakers. With a short training period, e.g., <ref type="bibr" target="#b30">30</ref>-60 seconds at the beginning of a conversation, frequent turn-taking of speakers can be very quickly and precisely traced through simple vector matching.</p><p>Our volume-topography-based technique has important advantages for online turn monitoring. First, volume parameters can be instantly and reliably estimated, even with a very short sensing window, e.g., 0.3 seconds; this allows us to monitor dynamic turn-taking behavior in a highly agile way. Second, volume-topography is less susceptible to diverse environmental noises as it is built in-situ to reflect the current noise characteristics. Third, our approach is computationally much lighter than existing techniques <ref type="bibr" target="#b7">[7]</ref>[27]; it does not require complex signal processing such as MFCC extraction and GMM matching. Finally, we note that the method works well even at very low sampling rates (as low as 500 Hz), which has the potential to reduce users' privacy concerns.</p><p>SocioPhone shows the potential to transform a personal mobile device into a social device that is aware of fine-grained face-to-face interaction contexts. So far, a number of mobile sensing systems have been proposed; yet, most of them focus on sensing personal status <ref type="bibr" target="#b25">[25]</ref>[28] <ref type="bibr" target="#b29">[29]</ref>. A few systems aim at capturing social contexts to facilitate interaction, but they provide only coarse-grained contexts such as encounters or presence of conversation <ref type="bibr" target="#b11">[11]</ref> <ref type="bibr" target="#b27">[27]</ref>.</p><p>We now summarize the contribution of this paper. First, we propose SocioPhone, a novel mobile interaction monitoring platform; it provides useful APIs to monitor 'turn' and turn-derived metalinguistic contexts. Second, as a key building block, we propose a new online turn-monitoring technique based on the volume topography constructed on the spot by collaborative sound sensing. In addition, we adopt and craft other supporting components to build SocioPhone as a working platform. Third, we prototype three promising applications, SocioTherapist, SocioDigest, and Tug-of-War on SocioPhone, and show their potential use. Finally, through extensive experiments, we show that our technique outperforms the state-of-the-art techniques in terms of accuracy, noise-resiliency, and resource usage.</p><p>The rest of the paper is organized as follows. Section 2 motivates face-to-face interaction monitoring and introduces the SocioPhone API and our applications. Section 3 presents the technical challenges of daily conversation and online turn monitoring. Section 4 describes the volume-topography-based technique in detail, and Section 5 presents the platform implementation. In Section 6, we show the effectiveness of our technique, and discuss potential issues in Section 7. We present related work in Section 8, and conclude the paper in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SOCIOPHONE API And APPLICATIONS 2.1 Meta-Linguistic Interaction Monitoring</head><p>Developing a mobile platform to monitor everyday face-to-face interaction opens a broad spectrum of design considerations. First of all, it is important to identify core system requirements for interaction monitoring and abstract them as common interfaces. In addition, we need to devise techniques to support diverse real-life interaction situations that are often disorderly, noisy, and dynamic. Unconstrained mobile environments make it difficult to simply adopt existing technologies that were mostly developed for rather orderly and lab-like environments. Finally, the issues of computation-and energy-efficiency are further intensified in mobile environments.</p><p>In this paper, we take a first step toward an online conversation monitoring platform; it supports diverse applications with metalinguistic conversational contexts in unconstrained mobile environments. While there has been much work on conversation analysis from various angles <ref type="bibr" target="#b3">[3]</ref>[10] <ref type="bibr" target="#b17">[17]</ref> <ref type="bibr" target="#b35">[35]</ref>, it is important to note that they focus on offline analysis of collected records. The challenges of online monitoring have not been thoroughly explored yet. Figure <ref type="figure" target="#fig_0">1</ref> shows the high-level process of meta-linguistic conversation monitoring composed of two layers: online turn segmentation and meta-linguistic context inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online turn segmentation:</head><p>Online turn segmentation forms a common basis for any conversation-monitoring system. As the core technical effort, we focus on executing online turn segmentation using smartphones. As a conversation progresses, it identifies turns continually; each turn is annotated with a triple, (speaking person, start time, end time). To be more specific, the monitor extracts a number of useful turn features from identified turns and complementarily prosodic features from sound samples to infer high-level interaction contexts. First, turn features are largely classified as those describing individual participants (e.g., speaking length, number of turns, duration statistics), relations among participants (e.g., turn taking orders, pair-wise turn-taking frequencies), and the whole interaction session (e.g., duration of speaking and non-speaking turns). Prosodic features are also useful indicators of social behavior <ref type="bibr" target="#b38">[38]</ref> and complement the turn features. Example features are pitch, energy, loudness, rhythm, as well as spectral features like formants, bandwidths, spectrum intensity.</p><p>When these simple features are combined, high-level interaction contexts can be further inferred, which are essential for delivering rich interaction-aware applications. For example, a fast-paced conversation can be identified from turn durations. Also, the sparseness of a conversation could be measured from the length and the distribution of the non-speaking turns, which an application may correlate with the progress or troublesome status of the on-going interaction. More complicated inference can be performed using the features. For example, one can understand the most (or the least) dominant person, the roles of participants, their role-playing patterns, and emergent leaders (See Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SocioPhone API</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the key SocioPhone APIs to facilitate monitoring rich meta-linguistic information in daily face-to-face interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monitoring sessions and turns:</head><p>The two key primitives are registerSessionStartListener() and registerTurnChangeListener(), with which applications can trace conversational sessions and turns on-the-fly. Once the former is registered, SocioPhone notifies applications of the Session structure upon the start/end of a conversation and join/leave of a participant. See Table <ref type="table" target="#tab_2">2</ref> for the Session structure. Applications may designate people or places of interest with the "CONDITION" clause. Upon notification of a session start, applications can further request turn monitoring with registerTurnChangeListener(). Then, SocioPhone provides the Turn information (Table <ref type="table" target="#tab_3">3</ref>) continuously upon each turn-taking event, i.e., alternation of a speaker or occurrence of pause.</p><p>Monitoring meta-linguistic interactions: Applications also can retrieve rich prosodic features associated with each turn using enableProsodicFeatures(); such features are provided only with explicit requests to save resources. The API currently provides volume, energy, and pitch features with their means and variances. SocioPhone also provides a set of convenient APIs for informative turn features and their patterns. For example, getSparsity() returns how far the speaking turns are separated by non-speaking turns. registerDominanceListener() encapsulates complex social inference to find someone with dominance over the conversation. Note that applications can replace the built-in inference engine with custom implementations.</p><p>Querying interaction history: In addition to real-time monitoring, SocioPhone supports querying the interaction history of a user. Applications can use getOnGoingSessionHistory() to query the ongoing session, and getPastInteractionHistory() to query completed sessions. Example queries are "How many turns has John taken within last 10 minutes" and "Which three friends has John spoken to the most this week?" SocioPhone provides a conventional SQL interface to support flexible and easy querying of stored Session and Turn information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Example Applications on SocioPhone</head><p>To demonstrate the usefulness of SocioPhone and its APIs, we designed and prototyped three interaction-aware applications.</p><p>SocioTherapist: Nonverbal social interaction and turn-taking deficits are a specific characteristic of young autistic children <ref type="bibr" target="#b32">[32]</ref>.</p><p>In speech therapy sessions for autistic children, the therapist often employs a stimulus, e.g., a toy, to evoke verbal turn-takings from a child. Upon a successful response, the child is reinforced with small rewards such as verbal encouragement or a snack <ref type="bibr" target="#b24">[24]</ref>.</p><p>SocioTherapist is a smartphone application for children with a mild degree of autism, and is designed to mimic stimuli and reinforcements in-situ during daily social interactions. The motivation and design have been largely advised by a local kindergarten in collaboration with us <ref type="bibr" target="#b18">[18]</ref>. The symptoms of those mildly autistic children are not so severe to require full-time treatment in a special education facility. Instead, they attend regular kindergartens as well as periodic dedicated sessions with a speech therapist. However, in daily interactions out of the clinic without the therapist's guidance, they often experience difficulties with turntaking when chatting or playing with other non-autistic children. Delayed or failed turn-taking may discontinue their interactions, or even result in eventual social isolation.  We prototyped SocioTherapist on top of SocioPhone APIs; a callback is triggered for every turn-taking event, i.e., when the speaker has been switched. Through the Turn instance, SocioTherapist easily obtains the properties for the newly started turn, e.g., its speaker, the timestamp it started, etc.</p><p>To implement a few basic criteria for desirable turn-taking behaviors, we consulted a speech therapist for autistic children. Accordingly, our initial prototype of SocioTherapist looks for initiations, long-lasting turns, and rapid responses. An initiation is a newly begun turn breaking a long silence. A long-lasting turn indicates a completed turn which lasted for a sufficient duration of time, ruling out short utterances like "Wow!" or "I got it." A rapid response is a newly begun turn immediately after another person's turn. When such turn-takings occur, SocioTherapist displays small rewards on the phones, i.e., well-known robotic characters for children gradually upgraded upon desirable turn-takings.</p><p>Our pilot deployment was encouraging. A group of three children played together for 15 minutes with SocioTherapist, including one with a mild degree of autism. The deployment was entirely supervised by a child education professional, who acknowledged clearly noticeable increases of utterances from the autistic child in both frequency and duration of turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SocioDigest:</head><p>The ubiquity of mobile sensing allows us to digitally capture and archive what we see and what we do everyday. This is also known as life-logging <ref type="bibr" target="#b36">[36]</ref>. As highly social beings, we believe that it is a natural expansion of life-logging to archive our finegrained interactions around our daily social circles.</p><p>In this light, we have been developing SocioDigest, an application providing daily report on a user's 24/7 face-to-face conversations. Figure <ref type="figure">2</ref>(a) shows a daily report for a PhD student, illustrating relative times he talked to his colleagues and family. SocioDigest further reports the detailed anatomy of each conversation session. Figure <ref type="figure">2</ref>(b) shows the relative total time durations for which each participating person talked in a conversation session. Based on the report, SocioDigest gives the user a small suggestion as well.</p><p>SocioDigest is implemented with SocioPhone APIs and easily retrieves the turn-wise durations from the timestamp attributes of the Turn instances. Figure <ref type="figure">2</ref>(c) reports even more details, the turntaking graph. Each vertex denotes a participant of the conversation, and the edge thickness denotes the numbers of turns exchanged between the pair. A thick edge implies that this person would be the most respondent to me, or I was to him/her as well.</p><p>We conducted a mini-deployment study of a preliminary version of SocioDigest; Section 6.4 discusses the settings and lessons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tug-of-War:</head><p>In group meetings or brainstorming, the level of participation of each individual may vary greatly; there might be someone who mostly remains silent, whereas a few might talk excessively, unwantedly giving others few chances to talk. It was reported that encouraging balanced participations from all individuals yields better outcomes in brainstorming <ref type="bibr" target="#b22">[22]</ref>.</p><p>Tug-of-War is a smartphone application that monitors turn-takings of participants and provides in-situ graphical feedback of how long each has talked so far. It is inspired by SensibleOrb <ref type="bibr" target="#b33">[33]</ref>, which employed dedicated wearable sensors called Sociometric Badges to monitor individuals' utterances. While we do not claim that its design is novel, the objective is to provide the key features of SensibleOrb on commodity mobile devices in everyday groupmeeting setting. Using SocioPhone APIs and the participants' own smartphones enables convenient, rapid, and low-cost development of the monitoring functionalities of SensibleOrb. The lines of code of our prototype is only 75 (without counting those for GUI), demonstrating the effectiveness of SocioPhone to facilitate the development of interaction-aware applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CHALLENGES IN DAILY CONVERSATION MONITORING</head><p>We studied characteristics of daily conversations in real-life settings to understand the key requirements for our platform. To this end, we collected real-life conversation data using a custom smartphone logger that continuously recorded sound and performed off-line extraction of conversation through a voice-activity detection tool <ref type="bibr" target="#b37">[37]</ref>. We deployed the software to five university students and collected data for ten days (total 753 user-hours). Although our dataset is limited in size and population, analyzing such real-life data gives us valuable insights into the challenges of dailyinteraction monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction patterns:</head><p>The following observations strongly influenced the design of SocioPhone. First, we found that participants spend 4.5 hours a day, on average, in face-to-face conversations. This shows that new mobile applications to support our daily interactions have the potential to appeal to many developers and users. Also, we can see that conversation monitoring should be performed in an energy-efficient way to support such long interaction times. Second, conversations consist of many short speaking turns. Figure <ref type="figure">3</ref> illustrates a turn-taking history of speakers in a sample conversation that we collected using throat microphones (see Section 6.1). In the figure, we find that short, spontaneous turns dominate the conversation. Thus, daily-conversation monitoring must capture such short turn-takings. Third, Figure <ref type="figure">4</ref> shows that more than 50% of conversations last more than 5 minutes. Moreover, conversations lasting longer than 5 minutes account for 83% of the total conversation time, and conversations longer than 10 minutes do for 70% of the total time. Separating the short, active learning phase and the long, energy-efficient monitoring phase is a key aspect of our design. We will describe this in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environmental characteristics:</head><p>Real-life acoustic environments are largely different from ideal lab environments, especially in terms of noise, making everyday conversation monitoring challenging. To understand these noise characteristics, we initially analyzed Signalto-Noise Ratios (SNRs) during conversations; we measure the SNR values by applying the WADA-SNR library <ref type="bibr" target="#b21">[21]</ref> to the conversation periods. Figure <ref type="figure">5</ref> shows the broad range of SNR values in real-life situations, mostly from -5 dB to 45 dB. The quality of recorded sound could vary greatly according to place (e.g., a silent meeting room, a noisy coffee shop), phone positions (e.g., on a table, in a pocket), and performance of microphones. This implies that conversation monitoring should be robust enough to handle noisy real-life environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limitations of Existing Techniques</head><p>As a baseline approach, we can consider a representative speakerrecognition method <ref type="bibr" target="#b7">[7]</ref>[27] that has been well-established over several decades. Figure <ref type="figure">6</ref> shows its processing pipeline. It first splits continuous sound data into fixed frames, extracts cepstral features (MFCC) from each frame, and matches them with pre-built Gaussian mixture models (GMM) of MFCCs, containing unique vocal features of speakers. We now summarize key limitations of this approach for daily conversation monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slow, inaccurate speaking turn detection:</head><p>The baseline speakerrecognition pipeline hardly detects the highly-interactive turntakings of daily conversations. This is because it generally requires 3-8-sec windows for reliable recognition, while turn-taking events often occur within smaller windows; note that a study reports two seconds of average turn length <ref type="bibr" target="#b2">[2]</ref>. Figure <ref type="figure">7</ref> shows that as the window size increases, a window is more likely to contain multiple people's speech, degrading the accuracy of speaker recognition. (see Section 6 for the definition of accuracy) One may consider reducing the window size, but the accuracy drops significantly when a window size is too short. As the pipeline relies on the spectral signature of a person's speech, it must listen long enough to obtain statistically representative spectrum from the speaker and thereby identify who he is reliably. Instantaneous spectrum largely varies even for a single speaker, depending on his intonation and which consonants he pronounces <ref type="bibr" target="#b40">[40]</ref>. With a short window, such socalled "atypical" sounds easily dominate the overall spectrum, making model matching difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vulnerability to real-life acoustic environments:</head><p>The accuracy of a speaker-recognition pipeline can be easily compromised by background noises and phone positions in real-life situations. Figure <ref type="figure">8</ref> shows the effect of noise in different places, i.e., a quiet classroom, a noisy café, and a living room with TV sound, as well as different phone positions, i.e., on the desk, in the pocket, and in the bag (see Figure <ref type="figure">5</ref> for SNR of each place); we used a 4-second window, which provides the highest overall accuracy. The results are mainly attributed to several factors, namely poor SNR, noise-vulnerability of MFCC <ref type="bibr" target="#b5">[5]</ref>, and GMM-mismatch in real, distorted data. While there are solutions to handle these problems such as noise cancellation, in-situ model building, and collaborative sensing <ref type="bibr" target="#b4">[4]</ref>[31], their improvements are known to be limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High energy consumption:</head><p>The baseline pipeline consumes a significant power. As shown in Table <ref type="table" target="#tab_5">4</ref>, the overall recognition process consumes 437 mW on a Galaxy Nexus phone; its 1750 mAh battery will drain in about 14 hours to only perform the recognition. In particular, MFCC extraction and GMM matching require 54 mW and 204 mW, respectively. A system could filter out non-voice parts to avoid frequent execution of resource-demanding recognition logic <ref type="bibr">[27][29]</ref>. However, the logic still needs to examine entire conversations, which are long enough (e.g., 4.5 hours a day) to significantly impact the battery life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitation of existing collaborative sensing approaches:</head><p>Recent work exploits collaboration opportunities with nearby phones for effective context monitoring <ref type="bibr">[26][31]</ref>. These approaches can be applied for conversation monitoring. One may execute the recognition pipeline on co-located phones and aggregate their inference results for better accuracy <ref type="bibr" target="#b31">[31]</ref>. Alternatively, for resource saving, only one phone may run the pipeline and share the results with the others <ref type="bibr" target="#b26">[26]</ref>. However, since these systems still use a conventional approach to speaker recognition, they suffer from low accuracy when detecting frequent, short turn-takings.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IN-SITU TURN MONITORING</head><p>To address aforementioned challenges, we devised a novel turn monitoring technique. In this section, we present the details of our turn-detection algorithm and practical implementation issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Consider a group conversation scenario with three people as in Figure <ref type="figure">9</ref>. When a person speaks, multiple phones acting as wireless receivers can capture the sound signals that the person (or transmitter) generates. Each phone measures a speaker's voice signal strength (or volume in µPa). When a speaker's phone is placed right next to the speaker (mostly true in practice), this phone is likely to measure the strongest signal strength among all the neighboring phones. A simple approach to speaker recognition is then to select a phone (and its owner) that has the strongest signal strength; this naïve method is called a Volume-peak-based algorithm.</p><p>In real-life situations, however, this approach has the following limitations: (1) location and placement of phones are not controllable (e.g., a phone may be placed in a pocket), (2) some of the phones may not be available (e.g., due to limited resources or poor recording quality), and (3) peak detection is susceptible to background noise.</p><p>To handle such limitations, we devise a Volume topography-based method that leverages the relative difference of recorded signal strengths over multiple phones. As in Figure <ref type="figure">9</ref>, speaker A's voice has been recorded over three phones with different volumes (represented as a volume vector). Due to relative position differences, each speaker will have a unique volume signature (or topography) over three phones. These phones can collaboratively build a topography database a priori (say during a learning phase), and we can identify the speaker by matching a newly measured volume vector with the topography database.</p><p>Our method is advantageous in several ways. First, it is much lighter than existing speaker recognition systems like <ref type="bibr" target="#b27">[27]</ref>, since we limit complex signal processing only in the learning phase. Second, volume vectors can be reliably obtained even with a very short sensing window, e.g., 300 ms, and thus enable turn-taking monitoring in a highly agile way; a turn is simply extracted by aggregating consecutive results. Third, the volume topography is less vulnerable in noisy acoustic environments; the background noises easily distort the users' voice spectra, but the topography itself is mostly consistent as long as the spatial placements of the phones and the speakers are consistent. In addition, the volume topography can be quickly re-trained in-situ to update phone positions and noise characteristics. Such in-situ topography also enables our method to work even when some phones may not be available (i.e., number of monitoring phones &lt; number of users).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Volume Topography-based Algorithm</head><p>Training data collection: During the learning phase, each phone samples the incoming sound at the rate of 8 kHz. The sampled audio stream is segmented into 300 ms-frames (i.e., 2,400 samples). For a given time t, each phone i calculates p(t,i), the power of the frame from phone i at time t, i.e., the average of the square of the audio signals. Thus, we have a feature vector, P(t) = (p(t,1), p(t,2), …, p(t,np)), where np is the number of monitoring phones; note that np may not be equal to the group size. For adequate learning, phones collect the feature vectors for L seconds, where L is a system parameter for a learning period. We use L=60 seconds in three-user experiments, obtaining 200 vectors in total.</p><p>Feature vector transformation: One of the key challenges is to define the feature vector so that it has discrimination power. Our initial approach was to simply use P(t) itself. Figure <ref type="figure" target="#fig_0">10</ref>(a) plots P(t) for a three-user group as in Figure <ref type="figure" target="#fig_6">16</ref>(a). In this case, we were able to differentiate three users, but we found that this approach performs poorly in discriminating non-speech turns (or silent turns). Our alternative was to normalize the vector as P(t) = P(t) / E(t), where E(t) is an average of a vector P(t). Figure <ref type="figure" target="#fig_0">10</ref>(b) plots P(t) for the same situation. This approach distinguishes human speech from non-speech well. However, we find that discrimination is weak when the number of phones is less than the group size due to loss of degrees of freedom (i.e., the sum of P(t) is always 1). Figure <ref type="figure" target="#fig_0">10(c)</ref> shows P(t) with one fewer phone.</p><p>To overcome this, we define the feature vector as the product of P(t) and the decibel measured on phone i, i.e., P(t) = {D(t,1) × p(t,1) / E(t), …, D(t,np) × p(t,np) / E(t)}, where D(t,i) is defined as follows , where pref is the standard reference sound pressure level, i.e., 20 μPa. In addition to the second approach, it discriminates better even with fewer phones. Figure <ref type="figure" target="#fig_0">10(d)</ref> and (e) show P(t) using three and two phones, respectively.</p><p>Topography generation: From the training dataset, we build a set of audio-signal signatures, i.e., volume topographies, for each group member plus the non-speech case (the moment when no member speaks). For an n-member group, we use k-means clustering where k is set to n+1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier training and classification:</head><p>The input dataset collected during the learning phase is used, namely feature vectors labeled with a cluster-ID. We select a multi-class SVM classifier, known as Turn recognition: A turn is detected if two consecutive frames belong to different clusters. We do not consider non-speech turns in a user's speech of less than 300ms; they are regarded as small pauses and often ignored <ref type="bibr" target="#b2">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping audio signatures (cluster-IDs) to group members (member-IDs):</head><p>In the learning phase, we also build a mapping table that converts cluster-IDs to member-IDs. We use a conventional speaker recognition technique <ref type="bibr" target="#b27">[27]</ref>. Each phone trains the recognition algorithm for its owner a priori by building a reference speech model. At the end of learning phase, each phone uses all original frames that belong to each audio signature to generate MFCC and compute GMM likelihood. The cluster head collects the GMM likelihoods from its members and determines the mappings of cluster-IDs onto member-IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Practical Issues</head><p>Energy-efficient conversation detection: The first step of turn monitoring is to detect whether a group conversation has started. Given that a conversation starts when people talk with one another, SocioPhone periodically monitors ambient sound to detect voice activity (e.g., analyzing about 2-sec-long audio signals in every 30 seconds). To be precise, the incoming sound wave is sampled at the rate of 8 kHz and the samples are segmented into frames. The duration of a frame is 2,048 ms (i.e., 16,384 samples per frame). For a given frame, we calculate two metrics, root mean square (RMS) and zero-crossing rate (ZCR). Then, we decide whether the sound is human speech using an offline-trained decision tree, which is commonly used in human-speech detection <ref type="bibr" target="#b27">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group formation and head selection:</head><p>Upon the detection of voice activity, SocioPhone discovers nearby friends by performing Bluetooth scanning. It retains MAC addresses of a user's friends; the list can be collected using conventional peer introduction mechanisms (only once per friend) <ref type="bibr" target="#b13">[13]</ref>. If it finds any registered friends, a group network is formed. In a group, one phone is selected as a head and coordinates the collaborative turn detection; it collects volume features from other phones, matches them to the topography, and shares the results. The head is randomly selected; the difference of resource consumption between a head and a member is marginal (See Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Duration of a learning phase:</head><p>Each user should speak at least once in a learning phase. To determine the proper duration, Figure <ref type="figure" target="#fig_0">11</ref> shows the accuracy while varying group sizes and learning durations. For the group sizes of three or four, accuracy tapers off around the 60 seconds, whereas it tapers off around the 90 seconds for group sizes of five. Reasonably assuming that the learning duration is proportional to the group size, we set the duration to the group size n × 20 seconds; our experiment shows 95% of speaking turns are shorter than 20 seconds (see the details in Section 6.2). During the learning phase, the topography-based turn monitoring will not be available on-the-fly. SocioPhone can apply the volumepeak-based algorithm in parallel, which does not require any training a priori. Also, it is worth noting that many daily conversations last quite long as discussed in Section 3.</p><p>Sampling rate selection: Figure <ref type="figure" target="#fig_0">12</ref> shows that SocioPhone achieves highly stable accuracy at sampling rates as low as 500 Hz (see Section 6 for configurations). Using low sampling rates has two major benefits: energy saving by reduced computation and privacy preserving even if the sampled speech is temporally stored. We elaborate the latter one. By the Nyquist sampling theorem, with the speech signal sampled at 500 Hz, we can reconstruct only the signals whose frequencies are no higher than 250 Hz, i.e., half the sampling frequency. Then, we refer to the articulation index (AI), a value quantifying the intelligibility of a given speech signal <ref type="bibr" target="#b14">[14]</ref>, where AI = 1 for most intelligible, zero for completely unintelligible. For example, AI is 0.9 for a low-passed speech signal cut off at 5000 Hz, 0.1 at 500 Hz and zero at 250 Hz or lower. Therefore, the speech that SocioPhone samples is largely unintelligible, which potentially preserves users' privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time synchronization:</head><p>To align feature vectors at the same time, we synchronize the phone clocks by well-known means (e.g., GPS or NTP). Note that phones may be out of sync by 1-2 seconds in WCDMA network. To investigate the required level, we performed an experiment with a 3-user group by deliberately making synchronization errors in one of the phones. Figure <ref type="figure" target="#fig_0">13</ref> shows that our algorithm tolerates about 100ms of errors. SocioPhone periodically checks the availability of GPS (once a day when a user is outdoors) and fixes the time from the GPS receiver whose time is accurate to 200 nanoseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion on Potential Improvements</head><p>Detection of a conversation group: In our current design, SocioPhone simply identifies conversation group members by an initial Bluetooth scan. We assume a single-group interaction among collocated friends. This assumption holds in many daily life situations, but sometimes groups may be partitioned into subgroups. We admit that further study is required to enable robust and practical detection of conversation groups, especially to deal with such multiple sub-group situations. One possible way would be to dynamically divide the sub-groups by analyzing overlapping speech patterns <ref type="bibr" target="#b6">[6]</ref>; note that overlapping speech is limited within a single conversation group as people often speak once at a time whereas </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of monitoring phones:</head><p>In real situations, participation from all available phones does not always guarantee the best accuracy; excluding a phone may achieve higher accuracy if that phone shows poor recording quality. However, it is challenging to estimate the expected accuracy in advance. One possible approach is to check if the signal-to-noise ratio (SNR) is above a certain threshold, but reliable SNR calculation is difficult and also consumes much power. Another alternative approach is to leverage phone placements. For example, from our empirical studies, we observe that turns are monitored more accurately by excluding phones in bags. To apply this method, we need to incorporate phone-placement detection, such as <ref type="bibr" target="#b30">[30]</ref>. Besides accuracy, we can further consider the available power of phones to select the monitoring phones. For example, SocioPhone can exclude phones with little battery remaining, e.g., &lt; 10%, if the number of phones is greater than three and most are qualified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise reduction:</head><p>Our technique is resilient to some forms of noise such as ambient noise that may persist or fluctuate but uniformly applies to all participating phones. An example is the background human utterances in a restaurant where people at the surrounding tables are chatting in similar tones. However, volume-topography may be vulnerable to nearby point-sourced noise; for example, an announcement from a nearby loud speaker or a cup rattling next to a specific phone. Given such a point-sourced noise, the large variance of phone-to-source proximity significantly distorts the volume topography. To improve robustness against such point-sourced noises, pre-filtering of non-human vocal spectrum at the recording stage would narrow down the vulnerable bandwidth. Techniques like spectral subtraction and Wiener filtering <ref type="bibr" target="#b8">[8]</ref> could be leveraged for this purpose.</p><p>Handling dynamic situations: Our technique properly operates when the relative positions of users and their phones are mostly fixed. However, diverse events may dynamically occur during a conversation, e.g., join and leave of a new member, moving phones, turning on a TV, which potentially compromises the monitoring accuracy. First, if the topography is successfully built in the learning phase but such dynamic events appear during the monitoring phase, we believe that the probability estimates of the SVM classifier can be used to handle the events. Figure <ref type="figure" target="#fig_0">14</ref> shows empirical behaviors of the probability estimates in the case of a group conversation with three users. The value mostly remains above 0.8 without any dynamic event (Figure <ref type="figure" target="#fig_0">14(a)</ref>). When a fourth person joined the group and started speaking (at the 60 second mark), the values dropped to around 0.6 for about 10-20 seconds, as shown in Figure <ref type="figure" target="#fig_0">14(b</ref>). For such a sudden drop within a predefined duration, the topography can be retrained in the background (during which the old ones are still used). Second, the topography training and associated classification can be spoiled when dynamic events occur during the learning phase. In such a case, the clues suggesting a retraining may be found from multiple sources, e.g., erratic turntaking patterns which are unlikely in normal conversations, considerably low probability estimates, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection of overlapping speeches: As our technique classifies</head><p>each speech frame into a single speaker, it fails to detect overlapping turns in which the multiple speakers talk at the same time. However, the portion of such overlapping speeches is not significant in our daily conversation. From our experiment with three people in a café (Figure <ref type="figure" target="#fig_0">17</ref> (c)), the total time of overlapping speech is under 10% of the total conversation time; a study also reports the overlapping ratio from 6% to 14% <ref type="bibr" target="#b39">[39]</ref>. Also, most overlapping speeches are short, less than 2 seconds. Accordingly, meta-linguistic features can be extracted properly regardless of the overlap. For some applications, however, overlap detection can be useful; for example, the successful interruptions are considered to infer the leadership in the group discussion <ref type="bibr" target="#b35">[35]</ref>. Note that even in field of speech diarization, identifying overlapping speech and the associated speakers remains an on-going challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PLATFORM IMPLEMENTATION</head><p>We have implemented a SocioPhone prototype in Java using Android SDK 4.0. It runs as a middleware and fully supports the SocioPhone APIs. Figure <ref type="figure" target="#fig_5">15</ref> shows the system architecture of the prototype. We implemented Turn Detector and Conversation Group Detector using the techniques introduced in Section 4. Here, we briefly explain the role of other system components.</p><p>Monitoring Planner decides how to perform turn monitoring. Its key role is to determine the feasibility of collaborative turn detection. Source Selector first figures out how many phones participate; it checks if the phone has sufficient battery power and if its sound signals are clear enough for discriminative volume topography. If there are sufficiently many sources available, Execution Planner performs turn monitoring with the volume-topography-based method. Otherwise, it performs conventional speaker recognition. Note that SocioPhone may ask users to place their phones in a better position when the collaborative method is not possible. Additionally, Feature Extractor processes prosodic features such as volume, pitch, and their variation over segmented sound signals.</p><p>Pattern Analyzer infers a number of meaningful social contexts by combining turn information and prosodic features. In the current prototype, it supports the following contexts: dominance and leadership in a conversation group, conversation asymmetry, interactivity, and sparseness. To infer the dominance and leadership, Pattern Analyzer applies a supervised SVM over the turn and prosodic features <ref type="bibr">[3][17]</ref>. It also identifies interactivity, sparseness, and skewness, applying heuristic metrics as follows:  Level of interactivity: # of speaking turns per minute  Level of sparseness: # of non-speaking turns over three seconds per minute  Level of skewness: standard deviation of # of speaking turns for all participants</p><p>Besides the above examples, Pattern Analyzer can flexibly incorporate other algorithms to infer diverse contexts. For example, emergent leaders in a conversation group can be further inferred using the method in <ref type="bibr" target="#b35">[35]</ref>. Another method can infer expressiveness from volume and pitch <ref type="bibr" target="#b38">[38]</ref>. We leave detailed evaluation of these derived contexts as a future work.</p><p>Interaction History Manager supports SQL queries from applications. To support the queries, it stores 'conversation session' and 'turn' information in an internal database. For efficiency, SocioPhone holds the turn information for the on-going session in the memory, while flushing it to persistent storage when the conversation completes. Internally, it is implemented using SQLite, a light-weight database in Android.</p><p>Network Interface: SocioPhone uses Bluetooth for peer discovery and communication. We considered using Wi-Fi Direct since it provides adequate features such as ad-hoc peer discovery and message broadcasting. However, it consumes too much power to use in everyday monitoring, as it is designed for short-term highbandwidth communication. According to our measurements, exchanging messages every second through Wi-Fi Direct requires about 413 mW of power. For the same, Bluetooth communication requires only 138 mW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>Our goal is to fully evaluate SocioPhone's performance under a range of real-life situations. However, since real-life sound sensing is affected by a number of factors simultaneously, a direct, fully unorganized deployment would make isolating the root causes of performance changes extremely challenging. As an initial step, we carefully select representative real-life scenarios, and we identify independent parameters that may largely affect SocioPhone's performance, as shown in Table <ref type="table" target="#tab_6">5</ref>. Then, we rehearse diverse variants of the scenarios by applying different combinations of the parameters to understand the causality of performance inclines or declines. Based on such understanding, we also describe our experiences and lessons learned from subsequent unorganized realworld deployments of SocioPhone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Scenarios and parameters. For performance evaluation of SocioPhone, we consider three conversation situations in different places, i.e., seminar room, home, and café (See Figure <ref type="figure" target="#fig_6">16</ref>(a)-(c)).</p><p>We vary the following parameters to reflect diverse real situations: the group size, the number of available phones, the phone positions, and the direction of microphones. By default, we assume a casual conversation with three participants. Each participant's phone is placed on a table and the microphones are directed to their owners. Table <ref type="table" target="#tab_6">5</ref> lists the default values and variations. Each conversation is 15 minutes of unscripted, free talking. For all experiments, we use Galaxy Nexus phones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative techniques we developed for comparisons:</head><p>SinglePipe is a conventional speaker recognition system, as shown in Figure <ref type="figure">6</ref>. Each phone runs its own recognition pipeline and uses the results separately. The performance is reported as the average value measured over all phones.</p><p>CombinePipe is developed based on DarwinPhones <ref type="bibr" target="#b31">[31]</ref>. It runs SinglePipe on every phone and makes the final inference by combining GMM likelihoods to improve accuracy.</p><p>SharePipe applies the idea from CoMon <ref type="bibr" target="#b26">[26]</ref>. Among multiple phones, only a single phone runs SinglePipe and shares the results with other phones to save energy. We omit the inference accuracy of SharePipe, since it is expected to be the same as SinglePipe.</p><p>All the alternatives are built on conventional speaker recognition methods. We attempt to carefully select their parameters to show their best performance. First, from the previous lessons <ref type="bibr" target="#b31">[31]</ref>, we apply well-crafted speaker models for each situation and use only the models of the interactants participating in the conversation session. Also, as these methods generate results over a fixed-size sensing window, we apply a four-second window by default, which provides the best accuracy in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics:</head><p>We adopt two key evaluation metrics: accuracy and resource efficiency in terms of energy and CPU. We measure energy consumption using a Monsoon PowerMeter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Turn-monitoring accuracy:</head><p>We adopt the duration-weighted accuracy used for speaker diarization <ref type="bibr" target="#b2">[2]</ref>. It is the ratio of correctly  Ground-truth annotation: Correct ground truth is a precondition for the integrity of monitoring accuracy. For accurate and fine-grained annotation, we use throat microphones (See Figure <ref type="figure" target="#fig_6">16(d)</ref>). The throat microphone records only its wearer's voice while suppressing external sound; it directly senses throat vibrations instead of vibrating air molecules. We also videotaped all conversations for post-hoc analysis. Note that manual tagging did not work properly due to the highly interactive nature of real-life conversations and difficulty of accurately tagging the start and end of a turn. Also, manual tagging was inconsistent across persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Turn-Monitoring Accuracy</head><p>We investigate turn-monitoring accuracy at the default setting. Table <ref type="table" target="#tab_7">6</ref> summarizes the results. SocioPhone shows the highest accuracy, 92.9%; it accurately and quickly detects turn-takings by inspecting volume vectors every 0.3 seconds. The others show around 80%. They hardly segment the turns precisely due to their larger 4-second window for reliable inference. Note that CombinePipe slightly outperforms SinglePipe, since phones are in a similar situation and the combined inference benefit is marginal.</p><p>To see the detailed differences, we plot partial results from SocioPhone and CombinePipe over the ground truth as in Figure <ref type="figure" target="#fig_0">17</ref>. SocioPhone captures the overall turn-taking pattern well. CombinePipe also recognizes speakers well in long-speaking turns, but often misses short, interactive turns. Figure <ref type="figure" target="#fig_0">18</ref> plots the CDF of speaking-turn durations. 45% of turns are less than four seconds, which is the window size of SinglePipe-based pipelines. More than 80% of speeches are less than 10 seconds, implying the importance of fine-grained turn segmentation for casual conversation. We find similar patterns in the 4-or 5-interactant conversations. Note that the topic or type of conversation could change the distribution, but the general trend would remain stable.</p><p>We observe that the interactants' speaking turns are sometimes overlapped. In our experiments, overlapped speech accounts for 1%-10% of the whole session time; the average duration of overlapped turns is 0.8-1 second. Interestingly, all the techniques mostly choose one speaker among the actually speaking speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Effect of Number of Phones</head><p>We investigate the effect of the group size and the number of available phones on the turn-monitoring accuracy. In addition to the default setting, we consider two more situations with four and five interactants. Figure <ref type="figure" target="#fig_0">19</ref> shows the results with different group sizes. SocioPhone outperforms the others regardless of the group size by 12-19%. Even with 5 interactants, it shows the accuracy of 83%, while the accuracies of other techniques are below 70%.</p><p>We further examine the accuracy while varying the number of phones actually monitoring. We report the average accuracy over all possible combinations; e.g., in the case of three phones for five interactants, we calculate the average accuracy for all 10 combinations. We exclude SinglePipe since it runs on one phone. As shown in Figure <ref type="figure">20</ref>, SocioPhone outperforms CombinePipe except when only two phones are available. This shows that the volume topography-based method works well even if a small portion of phones is unavailable, e.g., 78% accuracy with only three phones and five interactants. When the number of available phones is much smaller than the group size, our method performs worse than CombinePipe, e.g., two available phones for a 5-interactant conversation. In these cases, SocioPhone had better use the conventional speaker recognition method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Effect of Phone Placement and Direction</head><p>We then investigate the effect of phone placement and direction. To equalize external variables such as noises and conversation patterns, we simultaneously deploy multiple phones on each interactant (Figure <ref type="figure" target="#fig_6">16(d)</ref>). In this section, we omit the results with 3 and 4 interactants since their results are similar to a 5-interactant.</p><p>Effect of phone placement: Figure <ref type="figure" target="#fig_0">21</ref>(a) depicts the accuracy in 5interactanat conversation by increasing the number of phones in pockets. Except the case of (5,0) and (0,5), we report the average accuracy over all possible combinations. SocioPhone shows around 75% of accuracy even with three phones placed in a pocket and two phones on a table. This is similar to the accuracy of CombinePipe with all five phones on the table. CombinePipe also outperforms SinglePipe since a few phones with better sound quality disproportionately contribute to the results.  We find an interesting result in the 3-interactant conversation (see Figure <ref type="figure">23</ref> for the setting). Figure <ref type="figure">22</ref> shows that the F1 score of each interactants depends on which phone is placed in a pocket. An F1 score is the harmonic mean of precision and recall. When a user B in Figure <ref type="figure">23</ref> puts his phone in his pocket, the accuracy is much higher and F1-score of interactants are more balanced, compared to other cases. From the video review, we find that the relative distances between interactants are different. The distance between A and C is much shorter than B. Interestingly, it is unexpected that uniformly distributed phones on the table will be more helpful. We speculate that an imbalance of recoding volume makes inference more difficult for users with relatively close positions such as A and C when C's phone is in C's pocket. This implies that the relative position of interactants as well as the phones will be a key to estimating the expected accuracy.</p><p>Figure <ref type="bibr">21(b)</ref> shows the accuracy putting some phones in a bag. In some cases, CombinePipe outperforms SocioPhone, but the overall trend is similar to the previous experiment. The accuracy of all pipelines is much lower than the previous experiment due to lower quality audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of phone direction:</head><p>We next measure the accuracy by varying phones' direction. The direction hardly affects accuracy. This is because the length of the smartphone is much shorter than the relative distance among smartphones, and thus the volume level or frequency-domain features are well maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Effect of Places</head><p>We next examine the effect of background noise on turn-monitoring in different places: seminar room, home and café. In a café, background music is played and other guests are chattering. For home, we experimented in a living room with a TV turned on. To quantify the sensed audio quality, we use the SNR, with the method presented in Section 3.</p><p>Figure <ref type="figure">24</ref> depicts the average SNR in the three places. As expected, the SNR in the classroom, 40.7, is much higher than those in the home and café, 28.4, and 11.8, respectively. The SNR in home is also different from that in café. It might be due to the directivity of the microphone as well as the ambience of the noise. Café noise is spread out, whereas TV sound at home has more directivity.</p><p>Due to the degradation in audio quality, the monitoring accuracy at home or in the café drops compared to the seminar room, for all techniques. However, even in the café, which is an uncontrolled, noisy situation, we could observe that SocioPhone performs effectively at about 80% accuracy, whereas SinglePipe and CombinePipe show accuracy under 60%. The reason why SocioPhone is more robust against background noise may result from the different characteristic of MFCC features and the volume topography. Due to the logarithms, MFCC is easily influenced by low energies from noise. However, even in a noisy situation, people tend to speak louder than the background noise. Thus, a phone can still record an interactant's voice louder and thus, the volume topography will be maintained more stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Resource Usage for Turn Monitoring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Cost of Turn Monitoring</head><p>We evaluate the system cost for turn monitoring in terms of power consumption and CPU utilization. Table <ref type="table" target="#tab_8">7</ref> shows the results of SocioPhone and the other techniques with the default setting in Table <ref type="table" target="#tab_6">5</ref>. All techniques but SinglePipe operates in two modes, head and member. A head takes charge of the coordination and final inference. A member transmits the required information to the head. Overall, SocioPhone consumes much less power at higher accuracy in turn monitoring; it consumes about 280 mW, whereas others range from 436 to 512 mW; a 1750 mAh battery would last about 23 hours with SocioPhone and 12-14 hours with others. The difference of power consumption between a head and a member on SocioPhone is marginal because of the light-weight matching. For SharePipe, a member consumes much less power, i.e., 89.3 mW since it performs no recognition-related processing. Interestingly, CombinePipe's member consumes 35.3 mW more than the head, since Bluetooth consumes more power for transmission than for reception. SocioPhone also uses much fewer CPU cycles by avoiding complex speaker recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Cost Breakdown of Turn Monitoring</head><p>For turn monitoring, SocioPhone continuously performs three common operations: (1) sound recording, (2) feature computation, and (3) feature and result transmission. A head performs extra operations for learning and matching.</p><p>Table <ref type="table" target="#tab_9">8</ref> shows the power breakdown of SocioPhone. The top power consumer is sound recording, i.e., 57% of the entire power. The sound recording includes acquiring the wake lock in Android, about 30 mW. Bluetooth communication consumes about 98.5 mW due to frequent messaging every 0.3 seconds. However, logging applications such as SocioDigest may not need instantaneous turn results, and messages can be buffered and bulk-transmitted. We measured that Bluetooth consumes only 58.7 mW when messaging every 10 seconds. We omit the CPU breakdown since the total CPU usage is less than 2%.</p><p>We measure the training time of a head of SocioPhone. In the learning phase, a head performs the following three operations: (1) K-means clustering, (2) SVM model generation, and (3) speaker labeling using MFCC and GMM. Table <ref type="table">9</ref> shows the CPU time for different group sizes. Since speaker labeling is executed on every phone for the same workload, we report the average value. Interestingly, (1) and ( <ref type="formula">2</ref>) take only 160 ms even with 5-interactant data for 60 seconds. The bottleneck is (3), mapping the clusters onto speakers, which takes four seconds. Offloading such complex processing into the server might be useful to further optimize SocioPhone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Deployment Experience</head><p>We conduct additional experiments to observe SocioPhone's performance under more natural interaction situations. Here, we do not attempt to show general performance characteristics but present notable lessons on the performance and user experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Deployment in Natural Situations</head><p>For experiments, we recruited four frequently-interacting graduate students (P1, P2, P3, and P4) who did not know about SocioPhone in advance, and installed SocioPhone on their own smartphones; all the subjects were males in their twenties. For a natural setting, we let them freely have conversation sessions at school for a weekday. For ground truth, we asked them to wear throat microphones upon the start of a conversation and also to video-record the conversation sessions by themselves using a tripod; each participant was compensated with KRW 50000 (about USD 45) to participate in the study.</p><p>We first look at a case in which they go to a seminar room for brainstorming. We could see that P1 and P2 put their phones on the desk, P3 put his in his pant pocket, and P4 put his in his backpack; in the case of P2's phone, the microphone was not facing towards him.</p><p>In this natural brainstorming, the overall accuracy of SocioPhone is about 75%. This is 13% lower than the case of four phones on the desk as in Section 6.2.1 (88%). Table <ref type="table" target="#tab_10">10</ref> shows the results per participant including non-speaking turns. Interestingly, while the precision for speaking turns is very high overall (&gt; 96%), the recall is not as good, especially for P3 and P4 whose phones are not in open-air positions; SocioPhone often misses their speaking turns. In the case of P2, the recall is also quite low, 60%, even though his phone is on the desk; this is different than our previous observations, indicating that the direction of the microphone hardly affects the accuracy. From our video review, we strongly suspect that this is because P2 speaks in a calm tone so that his turn is often identified as non-speaking turns; interestingly, others asked him several times to speak again.</p><p>We investigate another case of three of the participants going to a café to have a casual conversation. Unlike when brainstorming, all participants comfortably put their phones on the coffee table.</p><p>Unfortunately, in this setting, the ground truth collected by throat microphones was not accurate as the participants did not wear them tight enough. Instead of investigating accuracy, from the video recordings and SocioPhone logs, we found several scenes that cause notable performance problems. In brief, very short, instantaneous noises often led to misclassification. First, after taking a sip of coffee, putting the cup on the table makes a loud noise, especially to the nearby phone. Second, P3 sometimes shakes his leg and his leg touches the table. In addition, we notice that when P2 uses his phone to check his Facebook (we asked him afterwards), his screen taps also create loud noises to the very phone, causing instant misclassification. We expect that the noise reduction techniques discussed in Section 4.4 can be further incorporated to filter out such instantaneous noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Experiences with SocioDigest</head><p>We conducted a mini-deployment of SocioPhone for three consecutive days, to encompass a broader set of our daily interactions and find lessons regarding further in-the-wild issues and user experiences. We recruited 15 users for SocioDigest introduced in Section 2.3, many of whom are within the same social circle, i.e., lab members.</p><p>This mini-deployment enlightened us about future considerations for SocioPhone to work fully robustly in-the-wild. For example, SocioPhone did not perform well under some conditions, such as when everyone keeps their phones in their pants pockets. This often occurred when participants make unplanned small talk while standing. To circumvent it, we distributed wind vests to the participants and recommended that they put their phones in its chest pocket instead of the pants one. Besides, SocioPhone needs to be further improved to isolate the true conversation groups in some situations such as multiple independent conversation groups talking at the same time very closely.</p><p>On reviewing his daily conversation report, a user who is 31-yearold Ph.D. student, was surprised that most of his daily talks are concentrated on a few people. An interesting observation is that, on the first day SocioDigest reported that his wife is not highly ranked in terms of conversation length. On the next day, the total length did not change much but his wife and he exchanged quite more turns; he said it was his small effort to make better use of his limited amount of time at home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>Privacy: Privacy is a primary concern when audio recording is involved. Our SocioPhone design addresses privacy issues as much as possible. As SocioPhone works on mobile devices only, it does not provide any raw audio recording or rich features like MFCC to the third-party servers from which original speech can be inferred. More importantly, SocioPhone limits its sample frequency to 500 Hz and shares only simple volume features from which it is almost impossible to recover the original linguistic contents. Within a conversation group, partial exposure concerns still remain, such as the size of a conversation group or emotional tone of speech. We expect sharing of this kind of information would be reasonably acceptable among people in the same group.</p><p>In addition, malicious applications running on SocioPhone have the potential to secretly report any private meta-linguistic contexts. SocioPhone provides users with an access control interface by which users can easily ensure that only trusted applications access SocioPhone. We also expect future mobile OSs would incorporate real-time information tracing facilities to monitor unexpected usage of private data, as proposed in TaintDroid <ref type="bibr" target="#b12">[12]</ref>. Finally, regardless of its privacy-preserving techniques, we admit the inherent limitations that nearby people might perceive intrusive from being audio-recorded itself <ref type="bibr" target="#b23">[23]</ref>.</p><p>Beyond meta-linguistic contexts: SocioPhone can incorporate existing speech recognition techniques to additionally provide semantic information like topics. For example, an application can recommend YouTube videos based on what a group has talked about so far. Supporting these advanced functions requires a better understanding of the resource requirements involved. The basic interaction awareness provided by SocioPhone can be an initial clue to determine when to selectively conduct heavy speech recognition given a device's limited resources.</p><p>We can further extend SocioPhone to capture visual cues such as gesture or eye contact. It is possible to adopt a gesture monitoring system like E-Gesture featuring energy-efficiency and resilience to activity-generated noises <ref type="bibr" target="#b34">[34]</ref>. An interesting direction would be capturing eye contacts with new hardware like Google Glass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>Conversation analysis: Everyday social interaction has been a long-studied area in sociology. They studied formal models and methods to understand everyday interactions, such as video-taping a conversation and structuring a schematic by turns and their orders <ref type="bibr" target="#b15">[15]</ref>. Our platform can provide a way to bring these research efforts and findings onto real-life services, enabling a variety of useful interaction-aware applications.</p><p>Interaction-aware applications: Initial applications are emerging to leverage social contexts during face-to-face interactions. For example, Pentland et al. infers meaningful social relationships by analyzing large volumes of daily social interaction data collected by mobile devices (e.g., Bluetooth scanning). Also, they propose several applications such as Sensible Orb <ref type="bibr" target="#b33">[33]</ref> and Meeting Mediator <ref type="bibr" target="#b22">[22]</ref> for workplace meeting situations. As a mobile platform, SocioPhone facilitates such applications in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker recognition and diarization:</head><p>In the fields of artificial intelligence, there have been significant efforts to infer diverse information from sound signals, including speaker, words, and emotions <ref type="bibr" target="#b2">[2]</ref>[4] <ref type="bibr" target="#b7">[7]</ref>[8] <ref type="bibr" target="#b17">[17]</ref>[39] <ref type="bibr" target="#b40">[40]</ref>. However, daily conversation monitoring on mobile devices imposes new requirements such as highly-interactive turns, dynamic acoustic situations, real-time processing, and the resource limitations of the mobile devices.</p><p>We may also consider using speaker diarization techniques to extract fine-granule "who spoke when" information <ref type="bibr">[2][4]</ref>; example applications include conversation-structure analysis for meeting records or automatic index building on media contents. However, it is difficult to directly apply these techniques in our environments. First, they are designed for post-conversation analysis; they hardly support real-time monitoring, which is the key to enable timely interaction-aware services as in Tug-of-War and SocioTherapist. For applications based on offline profiling such as SocioDigest, diarization techniques might be useful but requires careful consideration. It requires huge power and storage (2.5 GB per day at 16-bit 16kHz PCM) to capture and store raw sound data. Also, the error rates of such techniques needs to be further investigated in daily-interaction situations, as most of the previous studies are based on highly quality-controlled sounds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSION</head><p>In this paper, we propose the design and implementation of SocioPhone, a mobile interaction-monitoring platform. It provides a set of APIs to monitor turn and turn-derived meta-linguistic contexts during conversations in progress. In its core, it incorporates highlyefficient online turn-monitoring techniques based on the volume topography collaboratively created by conversation participants' phones. We built several interesting applications on top of SocioPhone: SocioTherapist, SocioDigest, and Tug-of-War. Moreover, we showed that our turn monitoring technique offers significant advantages over comparative techniques in terms of both accuracy and battery usage. We believe SocioPhone is a first crucial step to build a full-fledged mobile platform for daily face-to-face interaction monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">ACKNOWLEDGEMENT</head><p>We thank our shepherd, Prof. Landon Cox, for his valuable comments to improve the quality of this paper. We also thank Taiwoo Park, Haechan Lee, Yuhwan Kim, and Changhoon Lee, for their great help on building and demonstrating interesting applications, and Dr. Hyojung Shin for his valuable inputs for the project. This work was supported by the NRF of Korea grant</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Online turn segmentation and meta-linguistic conversation monitoring</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 Figure 4 .Figure 5 .</head><label>2345</label><figDesc>Figure 3. Turn-taking patterns in a sample conversation case</figDesc><graphic coords="4,216.30,61.20,80.46,142.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 Figure 8 .</head><label>678</label><figDesc>Figure 6. Typical speaker recognition pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure 9. Illustration of online turn monitoring</figDesc><graphic coords="6,74.88,128.16,76.56,65.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 .Figure 12 .Figure 13 .</head><label>111213</label><figDesc>Figure 11. Effect of learning phase duration (left) Figure 12. Effect of sampling rate (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. SocioPhone system architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Experimental setup</figDesc><graphic coords="9,322.26,61.20,230.88,191.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 18 .Figure 19 .</head><label>1819</label><figDesc>Figure 17. Turns over time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 21 .Figure 22 .Figure 24 .</head><label>212224</label><figDesc>Figure 20. Effect of number of available phones</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>context monitoring systems: Some previous works propose mobile platforms to facilitate monitoring of user contexts on-the-fly [19][20][25][28][29]. Most work focuses on efficiently monitoring personal contexts such as location, activity, and emotion. SocioPhone expands the scope of context-awareness toward daily face-to-face interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Key APIs of SocioPhone Monitoring conversation sessions and turns</head><label>1</label><figDesc></figDesc><table><row><cell>registerSessionStartListener (callback(Session), conditions)</cell></row><row><cell>registerTurnChangeListener (callback(Turn))</cell></row><row><cell>* conditions = TARGET_PERSON | TARGET_PLACE</cell></row><row><cell>class Session{ /* seeTable 2 */}; class Turn{ /* see Table 3 */};</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Monitoring prosodic features &amp; interaction characteristics</head><label></label><figDesc></figDesc><table><row><cell>enableProsodicFeature (session_id, /* features to enable */)</cell></row><row><cell>* Feature ={energy_avg, energy_var, pitch_avg, energy_var, …}</cell></row><row><cell>getSparsity (window_time | window_turns)</cell></row><row><cell>getInteractivity (window_time | window_turns)</cell></row><row><cell>getAsymmetry (window_time | window_turns)</cell></row><row><cell>registerDominanceListener (callback(Interactant), Inferrer)</cell></row><row><cell>registerLeadershipListener (callback(Interactant), Inferrer)</cell></row><row><cell>Querying interaction history</cell></row><row><cell>getOnGoingSessionHistory("SQL_Query_Statement");</cell></row><row><cell>getPastInteractionHistory("SQL_Query_Statement");</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Session table</head><label>2</label><figDesc></figDesc><table><row><cell>sID</cell><cell>Interactants</cell><cell>start_time</cell><cell>end_time</cell><cell cols="2">place …</cell></row><row><cell cols="6">1 Sheldon, Leonard Nov-6 19:20 Nov-6 21:05 Office …</cell></row><row><cell>2</cell><cell>Wife</cell><cell cols="4">Nov-6 22:50 Nov-6 23:08 Home …</cell></row><row><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 . Turn table</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">sID tID</cell><cell>speaker</cell><cell cols="4">start_time end_time prosodic_ptr …</cell></row><row><cell>1</cell><cell>1</cell><cell>Sheldon</cell><cell>19:20:35</cell><cell>19:20:39</cell><cell>pointers to</cell><cell>…</cell></row><row><cell>1</cell><cell>2</cell><cell>Myself</cell><cell>19:20:39</cell><cell>19:21:04</cell><cell>Prosodic</cell><cell>…</cell></row><row><cell>1</cell><cell>3</cell><cell cols="2">NOBODY 19:21:04</cell><cell>19:21:11</cell><cell>table entries</cell><cell>…</cell></row><row><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell><cell>…</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . Power cons. of speaker recognition on Galaxy Nexus</head><label>4</label><figDesc></figDesc><table><row><cell>Component</cell><cell cols="3">Idle Sensing Preprocessing MFCC GMM Total</cell></row><row><cell cols="2">Avg. power (mW) 13.5 160.9</cell><cell>4.0</cell><cell>54.2 204.2 437</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 . SocioPhone evaluation parameters Parameters Default values Other values used</head><label>5</label><figDesc></figDesc><table><row><cell># of interactants</cell><cell>3</cell><cell>4, 5</cell></row><row><cell>Place</cell><cell>Seminar room</cell><cell>Home, café</cell></row><row><cell># of avail. phones</cell><cell>3</cell><cell>2, 4, 5</cell></row><row><cell>Phone Position</cell><cell>On a table</cell><cell>In a pocket / a bag</cell></row><row><cell>Mic. Direction</cell><cell>To the owner</cell><cell>Reversed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 . Monitoring result in default situation</head><label>6</label><figDesc></figDesc><table><row><cell>System</cell><cell cols="3">Monitoring accuracy (%) Accuracy Precision Recall</cell></row><row><cell>SinglePipe</cell><cell>76.3</cell><cell>76.3</cell><cell>85.9</cell></row><row><cell>CombinePipe</cell><cell>80.4</cell><cell>80.4</cell><cell>90.6</cell></row><row><cell>SocioPhone</cell><cell>92.9</cell><cell>97.1</cell><cell>94.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 . Energy consumption and CPU utilization</head><label>7</label><figDesc></figDesc><table><row><cell>Pipeline</cell><cell>Head</cell><cell></cell><cell cols="2">Member</cell></row><row><cell></cell><cell cols="4">Power(mW) CPU(%) Power CPU</cell></row><row><cell>SinglePipe</cell><cell>436.8</cell><cell>15</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SharePipe</cell><cell>481.9</cell><cell>17.3</cell><cell>89.3</cell><cell>&lt; 2</cell></row><row><cell>CombinePipe</cell><cell>476.8</cell><cell>19.2</cell><cell>512.1</cell><cell>18.3</cell></row><row><cell>SocioPhone</cell><cell>282.1</cell><cell>&lt; 2</cell><cell>278.6</cell><cell>&lt; 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">. Power breakdown</cell><cell></cell></row><row><cell>Operation</cell><cell cols="2">Power(mW)</cell><cell>Ratio</cell></row><row><cell>Idle</cell><cell>13.5</cell><cell></cell><cell>0.05</cell></row><row><cell>Recording</cell><cell>160.9</cell><cell></cell><cell>0.57</cell></row><row><cell>Feature computation</cell><cell>5.7</cell><cell></cell><cell>0.02</cell></row><row><cell>Classification</cell><cell>3.5</cell><cell></cell><cell>0.01</cell></row><row><cell>Communication</cell><cell>98.6</cell><cell></cell><cell>0.35</cell></row><row><cell cols="4">Table 9. CPU time in the learning phase</cell></row><row><cell># of interactats</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>(1) Clustering</cell><cell>3.4 ms</cell><cell>13.6 ms</cell><cell>15.2 ms</cell></row><row><cell>(2) SVM training</cell><cell>89.0 ms</cell><cell cols="2">125.8 ms 147.2 ms</cell></row><row><cell>(3) Speaker labeling</cell><cell></cell><cell>4624.8 ms</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 . Precision and recall in a natural situation</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Precision(%) Recall(%)</cell><cell>Time(sec.)</cell></row><row><cell>P1(on the desk)</cell><cell>96.4</cell><cell>89.0</cell><cell>244</cell></row><row><cell>P2(on the desk')</cell><cell>97.4</cell><cell>63.1</cell><cell>190</cell></row><row><cell>P3(in the pocket)</cell><cell>98.0</cell><cell>54.0</cell><cell>98</cell></row><row><cell>P4(in the bag)</cell><cell>98.5</cell><cell>76.4</cell><cell>199</cell></row><row><cell>Non-speaking</cell><cell>19.2</cell><cell>88.8</cell><cell>48</cell></row><row><cell>Total</cell><cell>97.4</cell><cell>74.1</cell><cell>779</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This work was done while this author was at KAIST.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UX-oriented Mobile SW Platform) funded by the Ministry of Knowledge Economy of Korea</title>
		<author>
			<persName><surname>Mest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012-10041313</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>2012-0005733) and the SW Computing R&amp;D Program of KEIT</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to Machine Learning, 1 st edition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speaker Diarization: A Review of Recent Research</title>
		<author>
			<persName><forename type="first">X</forename><surname>Anguera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bozonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fredouille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Analysis of Group Conversations: Modeling Social Verticality. Computer Analysis of Human Behavior</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="293" to="322" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multistage Speaker Diarization of Broadcast News</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meihner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic Detection of Interaction Groups</title>
		<author>
			<persName><forename type="first">O</forename><surname>Brdiczka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maisonnasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reignier</surname></persName>
		</author>
		<editor>ICMI</editor>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker recognition: a tutorial</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1437" to="1462" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New insights into the noise reduction Wiener filter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sensing and Modeling Human Networks using the Sociometer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Of timing, Turn-Taking and Conversations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistic Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="1998-05">Nov. 5. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sense and Sensibility in a Pervasive World</title>
		<author>
			<persName><forename type="first">C</forename><surname>Efstratious</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Rachuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TaintDroid: An Information-Flow Tracking System for Realtime Privacy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Enck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Persistent Personal Names for Globally Connected Mobile Devices</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lesniewski-Lass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rhea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factors Governing the Intellligibility of Speech Sounds</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="119" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Interaction Order</title>
		<author>
			<persName><forename type="first">E</forename><surname>Goffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Some Consequences of Deep Interruption in Taskoriented Communication</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating Dominance in Multi-Party Meetings Using Speaker Diarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring Interchild Behavioral Relativity in a Shared Social Setting: a Field Study in a Kindergarten</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Coordinated Sensing Flow Execution Engine for Concurrent Mobile Sensing Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Symphoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orchestrator: An Active Resource Orchestration Framework for Mobile Context Monitoring in Sensor-rich Mobile Environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PerCom</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust Signal-to-Noise Ratio Estimation Based on Waveform Amplitude Distribution Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>InterSpeech</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meeting Mediator: Enhancing Group Collaboration using Sociometric Feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring Privacy Concerns about Personal Sensing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Consolvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hightower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Natural Language Teaching Paradigm for Nonverbal Autistic Children</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Koegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>O'dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Koegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Autism and Developmental Disorders</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="200" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobile Context Monitoring Platform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Mobicon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of ACM (CACM)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoMon: Cooperative Ambience Monitoring Platform with continuity and benefit awareness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobisys</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J B</forename><surname>Brush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Karson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Speakersense</surname></persName>
		</author>
		<title level="m">Energy Efficient Unobtrusive Speaker Identification on Mobile Phones. In Pervasive</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SoundSense: Scalable Sound Sensing for People-Centric Application on Mobile Phones</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choundhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Jigsaw continuous sensing engine for mobile phone applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SenSys</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatically Detecting Phone Context through Discovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Pocket</surname></persName>
		</author>
		<author>
			<persName><surname>Bag</surname></persName>
		</author>
		<author>
			<persName><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Darwin Phones: the Evolution of Sensing and Inference on Mobile Phones</title>
		<author>
			<persName><forename type="middle">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiSys</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Defining the Social Deficits of Autism: The Contribution of Non-verbal Communication Measures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ungerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Psychology and Psychiatry</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sensible Organizations: Technology and Methodology for Automatically Measuring Organizational Behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Olguin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Waber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>E-Gesture</surname></persName>
		</author>
		<title level="m">A Collaborative Architecture for Energy-efficient Gesture Recognition with Hand-worn Sensor and Mobile Devices</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>SenSys</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Identifying emergent leadership in small groups using nonverbal communicative cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez-Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Aran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Mast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<idno>ICMI 2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond Total Capture: A Constructive Critique of Lifelogging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="77" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Statistical model-based voice activity detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust Speech Rate Estimation for Spontaneous Speech</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2190" to="2201" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speech and Crosstalk Detection in Multichannel Audio</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wrigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="91" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inferring Colocation and Conversation Networks from Privacy-sensitive Audio with Implications for</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Social Science. ACM Trans. Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
