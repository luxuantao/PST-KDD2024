<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localised Adaptive Spatial-Temporal Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-12">12 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenying</forename><surname>Duan</surname></persName>
							<email>wenyingduan@ncu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoxi</forename><surname>He</surname></persName>
							<email>hexiaoxi@um.edu.mo</email>
						</author>
						<author>
							<persName><forename type="first">Zimu</forename><surname>Zhou</surname></persName>
							<email>zimuzhou@cityu.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Lothar</forename><surname>Thiele</surname></persName>
							<email>thiele@ethz.ch</email>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Rao</surname></persName>
							<email>raohong@ncu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nanchang University Nanchang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science and Technology University of Macau Macau</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">D-ITET ETH Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Nanchang University Nanchang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localised Adaptive Spatial-Temporal Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-12">12 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2306.06930v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph sparsification</term>
					<term>spatial-temporal graph neural network</term>
					<term>spatialtemporal data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-ofthe-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graphless and purely temporal, we record no drop in accuracy for the majority of tested datasets, with only minor accuracy deterioration observed in the remaining datasets. However, when the partially or fully localised ASTGNNs are reinitialised and retrained on the same data, there is a considerable and consistent drop in accuracy. Based on these observations, we reckon that (i) in the tested data, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies and, thus, can be essentially ignored for inference; and (ii) although the spatial dependencies provide redundant information, it is vital for the effective training of ASTGNNs and thus cannot be ignored during training. Furthermore, the localisation of ASTGNNs holds the potential to reduce the heavy computation overhead required on large-scale spatial-temporal data and further enable the distributed deployment of ASTGNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>An increasing number of modern intelligent applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref> rely on spatial-temporal data, i.e., data collected across both space and time. Spatial-temporal data often contain spatial and temporal dependencies, i.e., the current measurement at a particular location has causal dependencies on the historical status at the same and other locations. Learning these spatial and temporal dependencies is usually the essence of spatial-temporal data mining and plays a vital role in spatial-temporal inference <ref type="bibr" target="#b0">[1]</ref>.</p><p>Spatial-temporal data can be effectively represented by spatialtemporal graph models, which can depict complex relationships and interdependencies between objects in non-Euclidean domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. Of our particular interest are adaptive spatial-temporal graph neural networks (ASTGNNs), a popular class of spatial-temporal graph models which have demonstrated outstanding performance in applications involving spatial-temporal data such as traffic forecasting, blockchain price prediction, and biosurveillance forecasting <ref type="bibr">[3, 6-8, 24, 25]</ref>.</p><p>In this work, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? We limit our investigations to ASTGNNs, as they represent the stateof-the-art spatial-temporal graph model architecture. ASTGNNs usually model spatial dependencies with adaptive graph convolutional layers. The spatial dependencies are captured by learning the graph adjacency matrices. Therefore, the localisation of an AST-GNN is achieved via sparsifying the adjacency matrices, which can also be understood as pruning edges in the spatial graph. Note that the localisation of an ASTGNN refers to the sparsification of only the adjacency matrices capturing the spatial dependencies. It is crucial not to be confused with sparsifying other weight matrices, such as those used in the temporal modules, often seen in GNN sparsification with pre-defined graph architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>. An initial clue which indicates that the localisation of ASTGNNs could be feasible, probably even to an extreme extent, is shown in Figure . 1). Here we show the distribution of the elements in the spatial graph's adjacency matrix from an ASTGNN trained on the PeMSD8 dataset <ref type="bibr" target="#b3">[4]</ref>. It is obvious that most edges in the spatial graph have weights close to zero.</p><p>We are interested in the localisation of spatial-temporal graph models for the following reasons:</p><p>? A deeper understanding of the spatial and temporal dependencies in the data. Although it is commonly accepted that both spatial and temporal dependencies are vital for inference, it is unclear whether and to what extent the information provided by these dependencies overlaps. If the localisation induces a marginal accuracy drop, then the information provided by the spatial dependencies is already largely included in the information contained in the temporal dependencies and, thus, unnecessary for inference. ? Resoruce-efficient ASTGNN designs. ASTGNNs are notoriously computation heavy as the size of the spatial graph grows quadratically with the number of vertices, thus limiting their usage on large-scale data and applications. The localisation of ASTGNNs may significantly reduce the resource requirement of these spatial-temporal graph models and enable new spatial-temporal applications. ? Distributed deployment of spatial-temporal graph models. In many cases, the data to construct the spatial-temporal graph models are collected via distributed sensing systems e.g., sensor networks. However, making predictions of each vertex using these models requires the history of other vertices, thereby involving data exchange between sensor nodes. Localisation of spatial-temporal graph models may enable individual sensor nodes to make predictions autonomously without communicating with each other, which saves bandwidth and protects privacy in a distributed system.</p><p>We explore the localisation of ASTGNNs via Adaptive Graph Sparsification (AGS), a novel algorithm dedicated to the sparsification of adjacency matrices in ASTGNNs. The core of AGS is a differentiable approximation of the ? 0 -regularization of a mask matrix, which allows the back-propagation to go past the regularizer and thus enables a progressive sparsification while training. We apply AGS to two representative ASTGNN architectures and nine different spatial-temporal datasets. The experiment results are surprising. (i) The spatial adjacency matrices can be sparsified to over 99.5% without deterioration in test accuracy on all datasets. (ii) Even fully localised ASTGNNs, which effectively degenerate to purely temporal models, can still provide decent test accuracy (no deterioration on most tested datasets while only minor accuracy drops on the rest). (iii) When we reinitialise the weights of the localised ASTGNNs and retrain them on the spatial-temporal datasets, we cannot reinstate the same inference accuracy. Figure . 2 summarises our experiments and observations.</p><p>Our empirical study implies two hypotheses. (i) In the tested spatial-temporal datasets, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies. Therefore, the spatial dependencies can be safely ignored for inference without a noteworthy loss of accuracy. (ii) Although the information contained in the spatial and temporal dependencies overlaps, such overlapping provides the vital redundancy necessary for properly training a spatial-temporal graph model. Thus, the spatial dependencies cannot be ignored during training.</p><p>Our main contributions are summarised as follows:</p><p>? To the best of our knowledge, this is the first study on the localisation of spatial-temporal graph models. We surprisingly observed that spatial dependencies could be largely ignored during inference without losing accuracy. Extensive experiments on common spatial-temporal datasets and representative ASTGNN architectures demonstrated that only a few edges (less than 0.5% on all tested datasets) are required to maintain the inference accuracy. More surprisingly, when the spatial dependencies are completely ignored, i.e., the ASTGNNs are fully localised, they can still maintain a decent inference accuracy (no deterioration on most tested datasets, minor drops on the rest). ? With further investigations, we suggest the hypothesis that, although spatial dependencies can be primarily ignored during inference, they can drastically improve training effectiveness. This is supported by the observation that, if we reinitialise all parameters in the sparsified ASTGNNs and retrain them with the same data, the retrained networks yield considerably and consistently worse accuracy. ? To enable the localisation of ASTGNNs, we propose Adaptive Graph Sparsification (AGS), a novel graph sparsification algorithm dedicated to ASTGNNs. The core of AGS is a differentiable approximation of the ? 0 -regularization of a mask matrix, which allows the back-propagation to go past the regularizer and thus enables a progressive sparsification while training.</p><p>Figure <ref type="figure">2</ref>: An overview of our experiments and observations. We train ASTGNNs on spatial-temporal datasets, achieving baseline accuracies. Then we localise the ASTGNNs with the proposed algorithm AGS, achieving accuracies comparable to the dense graph baselines. Finally, we reinitialise the localised ASTGNNs and retrain them on the same datasets, resulting in considerably and consistently deteriorated accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is relevant to the following threads of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatial-Temporal Graph Neural Networks</head><p>Spatial-temporal graph neural networks (STGNNs) play an essential role in spatial-temporal data analysis for their ability to learn hidden patterns of spatial irregular signals varying across time <ref type="bibr" target="#b23">[24]</ref>. These models often combine graph convolutional networks and recurrent neural networks. For example, Graph Convolutional Recurrent Network (GCRN) <ref type="bibr" target="#b20">[21]</ref> combines an LSTM with ChebNet. Diffusion Convolutional Recurrent Neural Network <ref type="bibr" target="#b18">[19]</ref> incorporates a proposed diffusion graph convolutional layer into GRU in an encoder-decoder manner to make multi-step predictions. Alternatively, CNN-based models can represent the temporal relations in spatial-temporal data in a non-recursive manner. For instance, CGCN <ref type="bibr" target="#b27">[28]</ref> combines 1D convolutional layers with GCN layers. ST-GCN <ref type="bibr" target="#b25">[26]</ref> composes a spatial-temporal model for skeleton-based action recognition using a 1D convolutional layer and a Partition Graph Convolution (PGC) layer. More recent proposals such as ASTGCN <ref type="bibr" target="#b10">[11]</ref>, STG2Seq <ref type="bibr" target="#b1">[2]</ref>, and LSGCN <ref type="bibr" target="#b11">[12]</ref> further employ attention mechanisms to model dynamic spatial dependencies and temporal dependencies. In addition, some researchers consider the out-of-distribution generalisation of STGNN, and propose a domain generalisation framework based on hypernetworks to solve this problem <ref type="bibr" target="#b9">[10]</ref>. However, these models adopt a predefined graph structure, which may not reflect the complete spatial dependency.</p><p>To capture the dynamics in graph structures of spatial-temporal data, an emerging trend is to utilize adaptive spatial-temporal graph neural networks (ASTGNNs). Graph WaveNet <ref type="bibr" target="#b24">[25]</ref> proposes an AGCN layer to learn a normalized adaptive adjacency matrix without a pre-defined graph. ASTGAT introduces a network generator model that generates an adaptive discrete graph with the Gumbel-Softmax technique <ref type="bibr" target="#b14">[15]</ref>. The network generator can adaptively infer the hidden correlations from data. AGCRN <ref type="bibr" target="#b2">[3]</ref> designs a Node Adaptive Parameter Learning enhanced AGCN (NAPL-AGCN) to learn node-specific patterns. Due to its start-of-the-art performance, NAPL-AGCN has been integrated into various recent models such as Z-GCNETs <ref type="bibr" target="#b6">[7]</ref>, STG-NCDE <ref type="bibr" target="#b7">[8]</ref>, and TAMP-S2GCNets <ref type="bibr" target="#b5">[6]</ref>.</p><p>Despite the superior performance of ASTGNNs, they incur tremendous computation overhead, mainly because (i) learning an adaptive adjacency matrix involves calculating the edge weight between each pair of nodes, and (ii) the aggregation phase is computationally intensive. We aim at efficient ASTGNN inference, particularly for large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Sparsification for GNNs</head><p>With graphs rapidly growing, the training and inference cost of GNNs has become increasingly expensive. The prohibitive cost has motivated growing interest in graph sparsification. The purpose of graph sparsification is to extract a small sub-graph from the original large one. SGCN <ref type="bibr" target="#b15">[16]</ref> is the first to investigate graph sparsification for GNNs, i.e., pruning input graph edges, and learned an extra DNN surrogate. NeuralSparse <ref type="bibr" target="#b28">[29]</ref> prunes task-irrelevant edges from downstream supervision signals to learn robust graph representation. More recent works such as UGS <ref type="bibr" target="#b4">[5]</ref> and GBET <ref type="bibr" target="#b26">[27]</ref> explore graph sparsification from the perspective of the winning tickets hypothesis.</p><p>The aforementioned works only explore graph sparsification for vanilla GNNs and non-temporal data with pre-defined graphs. Our work differs by focusing on spatial-temporal GNNs with adaptive graph architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>This section provides a quick review of the representative architectures of ASTGNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial-Temporal Data as Graph Structure</head><p>Following the conventions in spatial-temporal graph neural network research <ref type="bibr">[2, 3, 6-8, 11, 12, 19, 21, 24-26, 28]</ref>, we represent the spatial-temporal data as a sequence of discrete frames X with G = {V, E}, where X = {? 1 , ? 2 , . . . , ? ? }. The graph G is also known as the spatial network, which consists of a set of nodes V and a set of edges E. Let |V | = ? . Then the edges are presented with an adjacency matrix ? ? ? R ? ?? , where ? ? ? R ? ?? is the node feature matrix with dimension ? at timestep ?, for ? = 1, . . . ,? .</p><p>Given graphs G and T historical observations X T ={? ? -T , . . ., ? ? -1 } ? R T ?? ?? , we aim to learn a function F which maps the historical observations into the future observations in the next H timesteps:</p><formula xml:id="formula_0">? ? , . . . , ? ? +H = F (? ? -T , . . . , ? ? -1 ; ?, G)<label>(1)</label></formula><p>where ? denotes all the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling the Spatial Network G</head><p>Since we focus on the role of spatial dependencies, we explain the state-of-the-art modelling of the spatial network in spatial-temporal graph neural networks (STGNNs).</p><p>The basic method to model the spatial network G at timestep ? with its node feature matrix ? ? in representative STGNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref> is Graph Convolution Networks (GCNs). A one-layer GCN can be defined as:</p><formula xml:id="formula_1">? ? = ? D -1 2 ?? D -1 2 ? ? ?<label>(2)</label></formula><p>where ? = ? + ? ? is the adjacency matrix of the graph with added self-connections. ? ? is the identity matrix. D is the degree matrix. ? ? R ? ?? is a trainable parameter matrix. ? (?) is the activation function. ? ? ? R ? ?? is the output. All information regarding the input ? ? at timestep ? is aggregated in ? ? .</p><p>A key improvement to model the spatial network is to adopt Adaptive Graph Convolution Networks (AGCNs) to capture the dynamics in the graph G, which leads to the adaptive spatial-temporal graph neural networks (ASTGNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b24">25]</ref>. In the following, we briefly explain Adaptive Graph Convolutional Recurrent Network (AGCRN) <ref type="bibr" target="#b2">[3]</ref> and the extension with transformers, denoted as AGFormer, two representative ASTGNN models.</p><p>? AGCRN. It enhances the GCN layer by combining the normalized self-adaptive adjacency matrix with a Node Adaptive Parameter Learning (NAPL), which is known as NAPL-AGCN.</p><formula xml:id="formula_2">A ??? = ?? ? ???? ???? EE ? ? ? = ? A ??? ? ? E? G<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">? G ? R ? ?? ?? and E? G ? R ? ?? ?? . A ??? ? R ? ??</formula><p>is the normalized self-adaptive adjacency matrix <ref type="bibr" target="#b24">[25]</ref>. Modelling the spatial network with NAPL-AGCN achieves stateof-the-art performances on multiple benchmarks, and it has been widely adopted in diverse ASTGNN variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. However, NAPL-AGCN is much more inefficient than GCNs, since A ??? is a matrix without zeros, while the adjacency matrix of a pre-defined graph in GCNs is far more sparse than A ??? . This motivates us to explore the localisation of spatial-temporal graph models taking ASTGNNs as examples.</p><formula xml:id="formula_4">?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ADAPTIVE GRAPH SPARSIFICATION</head><p>This section presents Adaptive Graph Sparsification (AGS), a new algorithm dedicated to the sparsification of adjacency matrics in ASTGNNs. Formulation. The NAPL-AGCN-based ASTGNNs with normalized self-adaptive adjacency matrix A ??? can be trained using the following objective:</p><formula xml:id="formula_5">L (?, A ??? ) = ? ?T ? ? V ? (?,?) -? (?,?) 1 |V | ? |T| (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where T is a training set, ? is a train sample, and ? (?,?) is the ground-truth of node ? in ?.</p><p>Given a pre-trained model F (?; ?, A ??? ), we introduce a mask M A to prune the adjacency matrix A ??? . The shape of M A is identical to A ??? . Specifically, given F (?; ?, A ??? ), we obtain M A by optimizing the following objective:</p><formula xml:id="formula_7">L ??? = L (?, A ??? ? M A ) + ? ?M A ? 0 , ?M A ? 0 = ? ?? ?=1 ? ?? ?=1 ? (?,? ) , ? (?,? ) ? {0, 1}<label>(5)</label></formula><p>where ? is the element-wise product, ? (?,? ) corresponds to binary "gate" that indicates whether an edge is pruned, and ? is a weighting factor for ? 0 -regularization of M A . Sparsification Algorithm. An intuitive way to get M A is to initialize a trainable weight matrix U ? R ? ?? and map the entry ? (?,? ) ? U into binary "gates" using a Bernoulli distribution:</p><formula xml:id="formula_8">? (?,? ) = B ? (?,? )</formula><p>, where B is a Bernoulli distribution. However, directly introducing U to model F (?; ?, A ??? ) has two problems.</p><p>? It may be unscalable for large-scale graphs.</p><p>? The ? 0 sparsity penalty is non-differentiable.</p><p>For the scalability issue, we adopt a node adaptive weight learning technique to reduce the computation cost by simply generating U with the node embedding E: Forward to compute the loss in Eq.( <ref type="formula" target="#formula_5">4</ref>). Forward to compute the loss in Eq.( <ref type="formula" target="#formula_7">5</ref>).</p><formula xml:id="formula_9">U = E? E<label>(6</label></formula><p>10:</p><p>Back-propagate to update ? and A ??? ? M A . 11: end while where ? E ? R ? ?? is a trainable matrix.</p><p>For the non-differentiable issue, we introduce the hard concrete distribution instead of the Bernoulli distribution <ref type="bibr" target="#b8">[9]</ref>, which is a contiguous relaxation of a discrete distribution and can approximate binary values.</p><p>Accordingly, the computation of binary "gates" ? ( ?, ?) can be formulated as: ? ? U (0, 1), ? (?,? ) = ??? log ?log(1 -?) + log ? (?,? ) /? s(?,? ) = ? (?,? ) (? -?) + ?, ? (?,? ) = min 1, max 0, s(?,? ) <ref type="bibr" target="#b6">(7)</ref> where ? is a uniform distribution, ??? is a sigmoid function, ? is a temperature value and (? -?) is the interval with ? &lt; 0 and ? &gt; 1. We set ? = -0.1 and ? = 1.1 in practice. Then, M A is applied to prune the lowest-magnitude entries in A ??? , w.r.t. pre-defined ratios ? ? .</p><p>Alg. 1 outlines the procedure of AGS. The pruning begins after the network is pre-trained (Line 5). First, we sort edges in the adjacency matrix by magnitude in ascending order (Line 6). Then we perform pruning iteratively (Line 7). The top ? ? of the edges are removed, and the rest are retained. We identify the remaining edge by setting its corresponding entry in M A to 1 in each iteration (Line 8). The edges to be pruned are removed using Eq.(5) (Line 9). Discussions. We make two notes on our AGS algorithm.</p><p>? AGS differs from prior magnitude-based pruning methods designed for graphs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> in that (i) it sparsifies adaptive graphs in spatial-temporal GNNs rather than vanilla GNNs and non-temporal data with pre-defined graphs; and (ii) it does not require iterative retraining on the pruned graphs to restore model accuracy. ? Pruning the adjacency matrix with AGS notably reduces the complexity of ASTGNNs for inference. The inference time </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To answer the question of whether and to what extent we can localise a spatial-temporal graph model, we conducted extensive experiments explained in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Neural Network Architecture</head><p>We evaluate the performance of AGS over two representative NAPL-AGCN-based ASTGNN architectures: AGCRN <ref type="bibr" target="#b2">[3]</ref> and its extension AGFormer. AGCRN is a state-of-the-art ASTGNN architecture combining AGCN and RNN layers. AGCN layers are used to capture the spatial dependencies, while RNN layers are there to model the temporal dependencies. AGFormer, on the other hand, can be regarded as an alternative version of the AGCRN, in which the RNN layers are substituted by Transformer layers. We intentionally chose these two ASTGNN architectures sharing the same spatial module but using different temporal modules, to show that both the effectiveness of AGS and our observations on the learned spatial dependencies are orthogonal to the temporal modules involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets and Configurations</head><p>The localisation of ASTGNNs is evaluated on nine real-world spatialtemporal datasets from three application domains: transportation, blockchain and biosurveillance. Table <ref type="table" target="#tab_3">1</ref> summarizes the specifications of the datasets used in our experiments. The detailed datasets and configurations are provided in Appendix A.1. The details on tuning hyperparameters are provided in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Experimental Results</head><p>Our major experiments are illustrated in Figure . 2. We first train AGCRNs and AGFormers on the nine spatial-temporal datasets, Error bars in these figures show the standard deviation of five runs. We made the following observations from these results:</p><p>? The localisation of AGCRNs and AGFormers is possible.</p><p>Applying AGS on AGCRNs and AGFormers and localising them to a localisation degree of 99% incurs no performance degradation across all datasets. On the contrary, in many experiments, the test accuracy keeps improving until 99%localisation. Further localisation of AGCRNs up to 99.5% still induces no accuracy drop against the non-localised baselines. ? Full localisation of AGCRNs is still practical. Even when we fully localise the AGCRNs, which in effect turn them into independent RNNs ignoring all spatial dependencies, they can still provide decent test accuracies. As shown in Figure . 6, on transportation datasets (PeMSD3, PeMSD4, PeMSD7 and PeMSD8), only minor drops are observed. On blockchain datasets (Bytom, Decentraland and Golem) and biosurveillance datasets (CA&amp;TX), we can observe that the test accuracy is no worse at 100% sparsity compared with the nonlocalised baselines. ? Localised AGCRNs cannot be relearned without the dense spatial graphs. As shown in Figure . 6, when we reinitialise the partially or fully localised AGCRNs and then retrain them on the nine datasets, we can observe a consistent and considerable drop in inference accuracy. Upon these observations, we suggest the following hypotheses:</p><p>? In many spatial-temporal datasets, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies. Therefore, the spatial dependencies can be safely ignored for inference without a noteworthy loss of accuracy. ? Although the information contained in the spatial and temporal dependencies overlaps, such overlapping provides the vital redundancy necessary for properly training a spatialtemporal graph model. Thus, the spatial dependencies cannot be ignored during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION STUDIES 6.1 Impact on Resource Efficiency</head><p>As mentioned in Sec. 1, one of the reasons that we are particularly interested in the localisation of ASTGNNs is its resource efficiency. Non-localised ASTGNNs usually learn spatial graphs that are complete. Therefore, the number of edges and consequently the computation overhead grows quadratically with the number of vertices. Localisation of ASTGNNs is equivalent to pruning edges in the learned spatial graph, which could dramatically reduce the computation overhead associated with the spatial dependencies and thus improve resource efficiency. To this end, we calculated the amount of computation during inference of 99%-localised AGCRNs and AGFormers, measured in FLOPs, and summarised the results in Table <ref type="table" target="#tab_5">2</ref>. We can see that the localisation of AGCRNs and AG-Formers effectively reduces the amount of computation required for inference. The acceleration is more prominent on AGCRNs against AGFormers because a larger portion of the total computation required by AGFormers is used on their temporal module (transformer layers), whereas AGCRNs use much lighter temporal modules (RNN layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Localised AGCRNs vs. Other Non-Localised ASTGNNs</head><p>In Figure. and AGFormers and all tested datasets. We reckon that this improvement is caused by the regularisation effect of sparsifying the spatial graph, which may suggest that the non-localised AGCRNs and AGFormers all suffer from overfitting to a certain degree.</p><p>Recent works on ASTGNNs proposed improved architectures of AGCRN, including Z-GCNETs <ref type="bibr" target="#b6">[7]</ref>, STG-NCDE <ref type="bibr" target="#b7">[8]</ref>, and TAMP-S2GCNets <ref type="bibr" target="#b5">[6]</ref> for different applications. We are therefore curious about how our localised AGCRNs compare to these variations. Hence we compare the test accuracy of 99%-localised AGCRNs with these architectures. Results are shown in Table <ref type="table" target="#tab_2">3</ref>, Table <ref type="table" target="#tab_7">4</ref> and Table 5. We can see that our localised AGCRNs can generally provide competitive inference performance even against those delivered by state-of-the-art architectures. This observation also agrees with our first hypothesis mentioned in Sec. 5.3: in many spatial-temporal datasets, the information provided by the spatial dependencies are primarily included in the information provided by the temporal dependencies. Therefore, different spatial modules, given that the temporal modules are properly trained, may not make a significant difference in inference performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Localisation of Non-temporal Graphs</head><p>To further investigate spatial dependencies and indirectly test our hypotheses, we conduct additional experiments and extend AGS to non-temporal graphs. We attempt to sparsify the spatial graph pre-given by non-temporal datasets, including Cora, CiteSeer, and Pubmed <ref type="bibr" target="#b19">[20]</ref>. On these datasets, we train two non-temporal graph neural network architectures, GCN <ref type="bibr" target="#b13">[14]</ref> and GAT <ref type="bibr" target="#b22">[23]</ref>. On GCN and GAT, as they don't own node embeddings E, we can use the representation H learned by pre-trained GCN and GAT to replace E in <ref type="bibr" target="#b5">(6)</ref>, then prune the edges as in Alg. 1, where the weighting factor ? takes control of graph sparsity.</p><p>Table <ref type="table" target="#tab_9">6</ref> shows the accuracy of localised GCN and GAT nontemporal datasets. The pre-defined graphs of Cora, Citeseer, and PubMed are sparsified to 30%, 50%, 80% and 100%, respectively. We can observe a significant drop in the test accuracy among all  localised non-temporal graph models. This indicates that, in the absence of temporal dependencies, the information provided by spatial dependencies plays a major role during inference and thus can not be ignored via localisation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? To facilitate our investigation, we propose AGS, a novel algorithm dedicated to the sparsification of adjacency matrices in ASTGNNs. We use AGS to localise two ASTGNN architectures: AGCRN and AGFormer, and conduct extensive experiments on nine different spatial-temporal datasets. Primary experiment results showed that The spatial adjacency matrices could be sparsified to over 99.5% without deterioration in test accuracy on all datasets. Furthermore, when the ASTGNNs are fully localised, we still observe no accuracy drop on the majority of the tested datasets, while only minor accuracy deterioration happened on the rest datasets. Based on these observations, we suggest two hypotheses regarding spatial and temporal dependencies: (i) in the tested data, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies and, thus, can be essentially ignored for inference; and (ii) although the spatial dependencies provide redundant information, it is vital for effective training of ASTGNNs and thus cannot be ignored during training. Last but not least, we conduct additional ablation studies to show the practical impact of ASTGNN's localisation on resource efficiency and to verify our hypotheses from different angles further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram of edge weights of the spatial graph from an ASTGNN trained on the PeMSD8 dataset.</figDesc><graphic url="image-1.png" coords="2,53.80,83.69,240.22,133.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test accuracies of original and localised (up to 99%) AGCRNs and AGFormers, tested on transportation datasets (PeMSD3, PeMSD4, PeMSD7 and PeMSD8). Horizontal dash lines represent the baselines of non-localised AGCRNs and AGFormers.</figDesc><graphic url="image-3.png" coords="6,53.80,83.69,504.40,352.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test accuracies of original and localised (up to 99%) AGCRNs and AGFormers, tested on biosurveillance datasets (CA and TX). Horizontal dash lines represent the baselines of non-localised AGCRNs and AGFormers.</figDesc><graphic url="image-4.png" coords="7,53.80,83.69,240.25,308.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test accuracies of original and localised (up to 99%) AGCRNs and AGFormers, tested on blockchain datasets (Bytom, Decentraland and Golem). Horizontal dash lines represent the baselines of non-localised AGCRNs and AGFormers.</figDesc><graphic url="image-5.png" coords="8,53.80,83.69,504.37,140.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Test accuracies of localised (99.1% to 100%) AGCRNs and their reinitialised&amp;retrained counterparts, tested on all datasets. Horizontal dash lines represent the baselines of non-localised AGCRNs.</figDesc><graphic url="image-6.png" coords="8,53.80,269.69,504.40,352.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="3,53.80,83.69,504.41,184.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is the embedding dimension for ? ? ? . Each row of E presents the embedding of the node. During training, E is updated to learn the spatial dependencies among all nodes. Instead of directly learning ? in (2) shared by all nodes, NAPL-AGCN uses E? ? . Finally, to capture both spatial and temporal dependencies, AGCRN integrates NAPL-AGCN and Gated Recurrent Units (GRU) by replacing the MLP layers in GRU with NAPL-AGCN.? AGFormer. It extends AGCRN by modelling the temporal dependencies with transformers<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. A Transformer is a stack of transformer blocks. One block contains a multihead self-attention mechanism and a fully connected feedforward network. We replace the MLP layers in the multihead self-attention mechanism with NAPL-AGCN to construct a transformer-based ASTGNN model, which we call AGFormer.</figDesc><table /><note><p>G to learn node-specific parameters. From the view of one node (e.g., node ?), E ? ? G are the corresponding node-specific parameters according to its node embedding E</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Algorithm 1: Adaptive Graph Sparsification (AGS) Input: X: input data, F ?; ?, A ??? : Spatial-temporal GNN with initialization self-adaptive adjacency matrix A ??? , N 1 : number of pre-training iterations, N 2 : number of sparsification iterations, ? ? : pre-defined sparsity level for graph. Output: F ?; ?, A ??? ? M A 1: while iteration ? &lt; N 1 do</figDesc><table><row><cell>2:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>3 :</head><label>3</label><figDesc>Back-propagate to update ? and A ??? . 4: end while 5: Obtain pre-trained F ?; ?, A ??? . 6: Sort entries in A ??? by magnitude in an ascending order then obtain list ? = {? ? } ? ?? ?=1 . 7: while iteration ? &lt; N 2 and 1 -</figDesc><table><row><cell>8:</cell><cell>set M A (?,? )</cell><cell>= 1 if A</cell><cell>(?,? )</cell><cell>?M A ? 0 ?A??? ? 0</cell><cell>&lt; ? ? do</cell></row></table><note><p>??? ? ? :? 2 ? ? . 9:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets used in experiments. AGCN layers is O (?2 ? + ?? T ? 2 + ?T A ??? 0 ? + ??? ). After sparsification, the inference time complexity of NAPL-AGCN layers is O (? 2 ? + ?? T ? 2 + ?T A ??? ? M A 0 ? + ??? ),where ? 2 ? is the time complexity of computing adaptive adjacency matrix, ? is the embedding dimension,? is the number of nodes, ? ??? ? ? ? 0 is the number of remaining edges, ? is the representation dimension of node features, T is the length of input, ? is the number of layers.</figDesc><table><row><cell>Datasets</cell><cell>#Nodes</cell><cell>Range</cell></row><row><cell>PeMSD3</cell><cell>358</cell><cell>09/01/2018 -30/11/2018</cell></row><row><cell>PeMSD4</cell><cell>307</cell><cell>01/01/2018 -28/02/2018</cell></row><row><cell>PeMSD7</cell><cell>883</cell><cell>01/07/2017 -31/08/2017</cell></row><row><cell>PeMSD8</cell><cell>170</cell><cell>01/07/2016 -31/08/2016</cell></row><row><cell>Bytom</cell><cell>100</cell><cell>27/07/2017 -07/05/2018</cell></row><row><cell>Decentral</cell><cell>100</cell><cell>14/10/2017 -07/05/2018</cell></row><row><cell>Golem</cell><cell>100</cell><cell>18/02/2017 -07/05/2018</cell></row><row><cell>CA</cell><cell>55</cell><cell>01/02/2020 -31/12/2020</cell></row><row><cell>TX</cell><cell>251</cell><cell>01/02/2020 -31/12/2020</cell></row><row><cell cols="3">complexity of unpruned NAPL-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>3, Figure. 4 and Figure. 5, we can clearly see that the localisation up to 99% is able to improve the test accuracy slightly. For example, 99%-localised AGCRNs outperform non-localised AGCRNs by decreasing the RMSE/MAE/MAPE by 3.6%/3.7%/2.0% on PeMSD3. Such improvement is consistently observed across both AGCRNs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Computation cost during inference on original and 99%-localised AGCRNs and AGformsers. The amount of computation is measured in MFLOPs, and acceleration factors are calculated in the round brackets.</figDesc><table><row><cell>Methods</cell><cell>PeMSD3</cell><cell>PeMSD4</cell><cell>PeMSD7</cell><cell cols="3">Computation Cost for Inference (MFLOPs) PeMSD8 Decentraland Bytom</cell><cell>Golem</cell><cell>CA</cell><cell>TX</cell></row><row><cell>Original AGCRN</cell><cell>400.26</cell><cell>188.59</cell><cell>1131.41</cell><cell>153.06</cell><cell>8.58</cell><cell>8.58</cell><cell>8.58</cell><cell>161.42</cell><cell>850.29</cell></row><row><cell>Original AGFormer</cell><cell>122.01</cell><cell>99.45</cell><cell>453.89</cell><cell>47.39</cell><cell>15.02</cell><cell>15.02</cell><cell>15.02</cell><cell>21.50</cell><cell>266.97</cell></row><row><cell>Localised AGCRN</cell><cell cols="9">253.33(?1.6?) 80.55(?2.3?) 237.56(?4.8?) 119.93(?1.3?) 2.82(?3.0?) 2.82(?3.0?) 2.82(?3.0?) 145.50(?1.1?) 706.21(?1.2?)</cell></row><row><cell cols="10">Localised AGFormer 80.13(?1.5?) 68.64(?1.4?) 199.17(?2.3?) 37.95(?1.3?) 11.75(?1.3?) 1.75(?1.3?) 11.75(?1.3?) 19.56(?1.1?) 226.43(?1.1?)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance of 99%-localised AGCRNs compared with other non-localised ASTGNN architectures on transportation datasets.</figDesc><table><row><cell>Methods</cell><cell>Datasets Metrics</cell><cell>MAE</cell><cell cols="2">PeMSD3 RMSE MAPE</cell><cell>MAE</cell><cell cols="2">PeMSD4 RMSE MAPE</cell><cell>MAE</cell><cell cols="3">PeMSD7 RMSE MAPE MAE</cell><cell cols="2">PeMSD8 RMSE MAPE</cell><cell>MAE</cell><cell>Average RMSE MAPE</cell></row><row><cell>AGCRN</cell><cell></cell><cell>15.98</cell><cell>28.25</cell><cell>15.23%</cell><cell>19.83</cell><cell>32.30</cell><cell>12.97%</cell><cell>22.37</cell><cell>36.55</cell><cell>9.12%</cell><cell>15.95</cell><cell>25.22</cell><cell cols="2">10.09% 18.53</cell><cell>30.58</cell><cell>11.85%</cell></row><row><cell>Z-GCNETs</cell><cell></cell><cell>16.64</cell><cell>28.15</cell><cell>16.39%</cell><cell>19.50</cell><cell>31.61</cell><cell>12.78%</cell><cell>21.77</cell><cell>35.17</cell><cell>9.25%</cell><cell>15.76</cell><cell>25.11</cell><cell cols="2">10.01% 18.42</cell><cell>30.01</cell><cell>12.11%</cell></row><row><cell>STG-NCDE</cell><cell></cell><cell>15.57</cell><cell>27.09</cell><cell>15.06%</cell><cell cols="2">19.21 31.09</cell><cell>12.76%</cell><cell cols="2">20.53 33.84</cell><cell>8.80%</cell><cell cols="2">15.45 24.81</cell><cell>9.92%</cell><cell cols="2">17.69 29.21</cell><cell>11.64%</cell></row><row><cell cols="2">TAMP-S2GCNets</cell><cell>16.03</cell><cell>28.28</cell><cell>15.37%</cell><cell>19.58</cell><cell>31.64</cell><cell>13.22%</cell><cell>22.16</cell><cell>36.24</cell><cell>9.20%</cell><cell>16.17</cell><cell>25.75</cell><cell cols="2">10.18% 18.49</cell><cell>30.48</cell><cell>11.99%</cell></row><row><cell cols="2">Localised AGCRN</cell><cell cols="2">15.41 27.21</cell><cell cols="2">14.93% 19.55</cell><cell>31.88</cell><cell cols="2">12.70% 21.03</cell><cell>34.56</cell><cell>8.53%</cell><cell>15.63</cell><cell>24.78</cell><cell>9.78%</cell><cell>17.91</cell><cell>29.61</cell><cell>11.49%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of 99%-localised AGCRNs compared with other non-localised ASTGNN architectures on biosurveillance datasets. 67?0.42 13.36?0.58 52.96?3.92 47.89% ?2.22 TAMP-S2GCNets 76.53?0.87 371.60?2.68 92.90?1.57 11.29?1.05 48.21?3.17 52.34% ?3.87 Localised AGCRN 86.22?1.43 423.91?3.01 44.12?0.35 12.31?0.59 52.88 ?3.73 42.47%?1.41</figDesc><table><row><cell>Methods</cell><cell>Datasets Metrics</cell><cell>MAE</cell><cell>CA RMSE</cell><cell>MAPE</cell><cell>MAE</cell><cell>TX RMSE</cell><cell>MAPE</cell></row><row><cell>AGCRN</cell><cell></cell><cell cols="2">91.23?1.69 448.27?2.78</cell><cell>53.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance of 99%-localised AGCRNs compared with other non-localised ASTGNN architectures on blockchain datasets.</figDesc><table><row><cell>Model</cell><cell>Bytom</cell><cell cols="2">MAPE in % Decentraland Golem</cell></row><row><cell>AGCRN</cell><cell>34.46?1.37</cell><cell>26.75?1.51</cell><cell>22.83?1.91</cell></row><row><cell>Z-GCNETs</cell><cell>31.04?0.78</cell><cell>23.81?2.43</cell><cell>22.32?1.42</cell></row><row><cell>STG-NCDE</cell><cell>29.65?0.63</cell><cell>24.13?1.07</cell><cell>22.24?1.53</cell></row><row><cell>TAMP-S2GCNets</cell><cell>29.26?1.06</cell><cell>19.89?1.49</cell><cell>20.10?2.30</cell></row><row><cell cols="3">Localised AGCRN 28.17?0.93 13.33?0.31</cell><cell>21.71?0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Classification accuracy (%) of localised GCN and GAT on citation graph datasets.</figDesc><table><row><cell>Sparsity(%)</cell><cell>Cora GCN GAT GCN GAT GCN GAT Citeseer PubMed</cell></row><row><cell>0%</cell><cell>80.20 82.10 69.40 72.52 78.90 79.00</cell></row><row><cell>30%</cell><cell>80.35 83.17 69.23 72.31 79.14 79.23</cell></row><row><cell>50%</cell><cell>72.73 75.40 69.37 72.70 78.82 79.31</cell></row><row><cell>80%</cell><cell>65.19 70.81 58.47 63.18 68.37 77.03</cell></row><row><cell>100%</cell><cell>56.22 63.29 53.13 57.50 61.02 64.25</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The authors are grateful to the KDD anonymous reviewers for many insightful suggestions and engaging discussion which improved the quality of the manuscript.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Additional Dataset Details</head><p>The detailed datasets and configurations are as follows:</p><p>? Transporation: We use four widely-studied traffic forecasting datasets from Caltrans Performance Measure System (PeMS): PeMSD3, PeMSD4, PeMSD7 and PeMSD8 <ref type="bibr" target="#b3">[4]</ref>. Following <ref type="bibr" target="#b2">[3]</ref>, PeMSD3, PeMSD4, PeMSD7, and PeMSD8 are split with a ratio of 6:2:2 for training, validation and testing. The traffic flows are aggregated into 5-minute intervals. We conduct 12-sequence-to-12-sequence forecasting, the standard setting in this domain. The accuracy is measured in Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). ? Blockchain: We use three Ethereum price prediction datasets: Bytom, Decentral and Golem <ref type="bibr" target="#b17">[18]</ref>. These data are represented in graphs, with nodes and edges being the addresses of users and digital transactions, respectively. The  number of sparsifcation iterations interval between two consecutive time points is one day. Following <ref type="bibr" target="#b5">[6]</ref>, Bytom, Decentraland, and Golem token networks are split with a ratio of 8:2 for training and testing. We use 7 days of historical data to predict a future of 7 days. MAE, RMSE, and MAPE are also used as accuracy metrics. ? Biosurveillance: We use California (CA) and Texas (TX), COVID-19 biosurveillance datasets <ref type="bibr" target="#b5">[6]</ref> used to forecast the number of hospitalized patients. The time interval of these datasets is one day. Following <ref type="bibr" target="#b5">[6]</ref>, CA and TX are split with a ratio of 8:2 for training and testing. Three days of historical data are used to predict a future of fifteen days. With these two datasets, MAE and RMSE are not used for measuring the inference accuracy, as their values are extremely small and, thus, difficult to reflect the performances realistically. Following <ref type="bibr" target="#b5">[6]</ref>, we only use MAPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experiment Details</head><p>The dataset-specific hyperparameters chosen for AGCRNs and AG-Formers are summarised in Table <ref type="table">7</ref>. All experiments are implemented in Python with Pytorch 1.8.2 and executed on a server with one NVIDIA RTX3090 GPU. We optimize all the models using the Adam optimizer <ref type="bibr" target="#b12">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Notation</head><p>Frequently used notations are summarized in Table <ref type="table">8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatio-temporal data mining: A survey of problems and methods</title>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Atluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">STG2Seq: Spatial-Temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salil</forename><forename type="middle">S</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Sarit</forename><surname>Kraus</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10">2019. 2019. August 10-16, 2019</date>
			<biblScope unit="page" from="1981" to="1987" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional recurrent network for traffic forecasting</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17804" to="17815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freeway Performance Measurement System: Mining Loop Detector Data</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Skabardonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pravin</forename><surname>Varaiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanfeng</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.3141/1748-12</idno>
		<ptr target="https://doi.org/10.3141/1748-12" />
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">1748</biblScope>
			<biblScope unit="page" from="96" to="102" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified Lottery Ticket Hypothesis for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1695" to="1706" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting</title>
		<author>
			<persName><forename type="first">Yuzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Segovia-Dominguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Coskunuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><forename type="middle">R</forename><surname>Gel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series Forecasting</title>
		<author>
			<persName><forename type="first">Yuzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Segovia-Dominguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><forename type="middle">R</forename><surname>Gel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1684" to="1694" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph Neural Controlled Differential Equations for Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Jeongwhan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwangyong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeehyun</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noseong</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="6367" to="6374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Sparse Neural Networks through L 0 Regularization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>ICLR 2018</idno>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combating Distribution Shift for Accurate Time Series Forecasting via Hypernetworks</title>
		<author>
			<persName><forename type="first">Wenying</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lothar</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IEEE International Conference on Parallel and Distributed Systems, ICPADS 2022</title>
		<meeting><address><addrLine>Nanjing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-01-10">2022. January 10-12, 2023</date>
			<biblScope unit="page" from="900" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting</title>
		<author>
			<persName><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-third AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LSGCN: Long Short-Term Traffic Prediction with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Rongzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuyin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, Christian Bessiere</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2355" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A Method for Stochastic Optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive spatial-temporal graph attention networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">Xiangyuan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SGCN: A Graph Sparsifier Based on Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengmin</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><surname>Zafarani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-47426-3_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-47426-3_22" />
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining -24th Pacific-Asia Conference, PAKDD 2020</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Hady</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raymond</forename><surname>Lauw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chi-Wing</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexandros</forename><surname>Ntoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-11">2020. May 11-14, 2020</date>
			<biblScope unit="volume">12084</biblScope>
			<biblScope unit="page" from="275" to="287" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5243" to="5253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dissecting Ethereum Blockchain Analytics: What We Learn from Topology and Geometry of the Ethereum Graph?</title>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umar</forename><surname>Islambekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuneyt</forename><surname>Gurcan Akcora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><forename type="middle">R</forename><surname>Gel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9781611976236.59</idno>
		<ptr target="https://doi.org/10.1137/1.9781611976236.59" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 SIAM International Conference on Data Mining, SDM 2020</title>
		<editor>
			<persName><forename type="first">Carlotta</forename><surname>Demeniconi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</editor>
		<meeting>the 2020 SIAM International Conference on Data Mining, SDM 2020<address><addrLine>Cincinnati, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05-07">2020. May 7-9, 2020</date>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured Sequence Modeling with Graph Convolutional Recurrent Networks</title>
		<author>
			<persName><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-04167-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-04167-0_33" />
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -25th International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Long</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chi-Sing</forename><surname>Leung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Seiichi</forename><surname>Ozawa</surname></persName>
		</editor>
		<meeting><address><addrLine>Siem Reap, Cambodia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-12-13">2018. 2018. December 13-16, 2018</date>
			<biblScope unit="volume">11301</biblScope>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph Wavenet for Deep Spatial-Temporal Graph Modeling</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019. 19</date>
			<biblScope unit="page" from="1907" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Early-Bird GCNs: Graph-Network Co-optimization towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/20873" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="8910" to="8918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J?r?me</forename><surname>Lang</surname></persName>
		</editor>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. July 13-19, 2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust Graph Representation Learning via Neural Sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
