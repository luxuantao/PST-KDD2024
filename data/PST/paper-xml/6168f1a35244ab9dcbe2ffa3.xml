<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH CONDENSATION FOR GRAPH NEURAL NET-WORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-14">14 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<email>jinwei2@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
							<email>lingxiao@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
							<email>shichang@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH CONDENSATION FOR GRAPH NEURAL NET-WORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-14">14 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.07580v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>153,932 training nodes 154 training nodes</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many real-world data can be naturally represented as graphs such as social networks, chemical molecules, transportation networks, and recommender systems <ref type="bibr" target="#b0">(Battaglia et al., 2018;</ref><ref type="bibr" target="#b40">Wu et al., 2019b;</ref><ref type="bibr" target="#b47">Zhou et al., 2018)</ref>. As a generalization of deep neural networks for graph-structured data, graph neural networks (GNNs) have achieved great success in capturing the abundant information residing in graphs and tackle various graph-related applications <ref type="bibr" target="#b40">(Wu et al., 2019b;</ref><ref type="bibr" target="#b47">Zhou et al., 2018)</ref>.</p><p>However, the prevalence of large-scale graphs in real-world scenarios, often on the scale of millions of nodes and edges, poses significant computational challenges for training GNNs. More dramatically, the computational cost continues to increase when we need to retrain the models multiple times, e.g., under incremental learning settings, hyperparameter and neural architecture search. To address this challenge, a natural idea is to properly simplify, or reduce the graph so that we can not only speed up graph algorithms (including GNNs) but also facilitate storage, visualization and retrieval for associated graph data analysis tasks.</p><p>There are two main strategies to simplify graphs: graph sparsification <ref type="bibr" target="#b29">(Peleg &amp; Schäffer, 1989;</ref><ref type="bibr" target="#b34">Spielman &amp; Teng, 2011)</ref> and graph coarsening <ref type="bibr" target="#b25">(Loukas &amp; Vandergheynst, 2018;</ref><ref type="bibr" target="#b24">Loukas, 2019)</ref> . Graph sparsification approximates a graph with a sparse graph by reducing the number of edges, while graph coarsening directly reduces the number of nodes by replacing the original node set with its subset. However, these methods have some shortcomings: (1) sparsification becomes much less promising in simplifying graphs when nodes are also associated with attributes as sparsification does not reduce the node attributes; (2) the goal of sparsification and coarsening is to preserve some graph properties such as principle eigenvalues <ref type="bibr" target="#b25">(Loukas &amp; Vandergheynst, 2018)</ref> that could be not optimal Figure <ref type="figure" target="#fig_3">1</ref>: We study the graph condensation problem, which seeks to learn a small, synthetic graph, features and labels {A , X , Y } from a large, original dataset {A, X, Y}, which can be used to train GNN models that generalize comparably to the original. Shown: An illustration of our proposed GCOND graph condensation approach's empirical performance, which exhibits 95.3% of original graph test performance with 99.9% data reduction.</p><p>for the downstream performance of GNNs. In this work, we ask if it is possible to significantly reduce the graph size while providing sufficient information to well train GNN models.</p><p>Motivated by dataset distillation <ref type="bibr" target="#b37">(Wang et al., 2018)</ref> and dataset condensation <ref type="bibr" target="#b46">(Zhao et al., 2020)</ref> which generate a small set of images to train deep neural networks on the downstream task, we aim to condense a given graph through learning a synthetic graph structure and node attributes. Correspondingly, we propose the task of graph condensation<ref type="foot" target="#foot_0">1</ref> . It aims to minimize the performance gap between GNN models trained on a synthetic, simplified graph and the original training graph. In this work, we focus on attributed graphs and the node classification task. We show that we are able to reduce the number of graph nodes to as low as 0.1% while training various GNN architectures to reach surprisingly good performance on the synthetic graph. For example, in Figure <ref type="figure" target="#fig_3">1</ref>, we condense the graph of the Reddit dataset with 153,932 training nodes into only 154 synthetic nodes together with their connections. In essence, we face two challenges for graph condensation: (1) how to formulate the objective for graph condensation tractable for learning; and (2) how to parameterize the to-be-learned node features and graph structure. To address the above challenges, we adapt the gradient matching scheme in <ref type="bibr" target="#b46">(Zhao et al., 2020)</ref> and match the gradients of GNN parameters w.r.t. the condensed graph and original graph. In this way, the GNN trained on condensed graph can mimic the training trajectory of that on real data. Further, we carefully design the strategy for parametrizations for the condensed graph. In particular, we introduce the strategy of parameterizing the condensed features as free parameters and model the synthetic graph structure as a function of features, which takes advantage of the implicit relationship between structure and node features, consumes less number of parameters and offers better performance.</p><p>Our contributions can be summarized as follows:</p><p>1. We make the first attempt to condense a large-real graph into a small-synthetic graph, such that the GNN models trained on the large graph and small graph have comparable performance. We introduce a proposed framework for graph condensation (GCOND) which parameterizes the condensed graph structure as a function of condensed node features, and leverages a gradient matching loss as the condensation objective. 2. Through extensive experimentation, we show that GCOND is able to condense different graph datasets and achieve comparable performance to their larger counterparts. For instance, GCOND approximates the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%. Our approach consistently outperforms coarsening, coreset and dataset condensation baselines. 3. We make several important findings towards the condensation problem: (a) condensed graphs can generalize well to different GNN test models; (b) condensing node features and structural information simultaneously can benefit the performance; and (c) the condensed graphs are able to extract meaningful patterns hidden in the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Dataset Distillation &amp; Condensation. Dataset distillation (DD) <ref type="bibr" target="#b37">(Wang et al., 2018;</ref><ref type="bibr" target="#b1">Bohdal et al., 2020;</ref><ref type="bibr" target="#b28">Nguyen et al., 2021)</ref> aims to distill knowledge of a large training dataset into a small synthetic dataset, such that a model trained on the synthetic set is able to obtain the comparable performance to that of a model trained on the original dataset. To improve the efficiency of DD, dataset condensation (DC) <ref type="bibr" target="#b46">(Zhao et al., 2020;</ref><ref type="bibr" target="#b45">Zhao &amp; Bilen, 2021)</ref> is proposed to learn the small synthetic dataset by matching the gradients of the network parameters w.r.t. large-real and small-synthetic training data. However, these methods are designed exclusively for image data and are not applicable to non-Euclidean graph-structured data where samples (nodes) are interdependent. In this work, we generalize the problem of dataset condensation to graph domain and we seek to jointly learn the synthetic node features as well as graph structure. Additionally, our work relates to coreset methods <ref type="bibr" target="#b38">(Welling, 2009;</ref><ref type="bibr" target="#b32">Sener &amp; Savarese, 2017;</ref><ref type="bibr" target="#b31">Rebuffi et al., 2017)</ref>, which seek to find informative samples from the original datasets. However, they rely on the presence of representative samples, and tend to give suboptimal performance.</p><p>Graph Sparsification &amp; Coarsening. Graph sparsification and coarsening are two means of reducing the size of a graph. Sparsification reduces the number of edges while approximating pairwise distances <ref type="bibr" target="#b29">(Peleg &amp; Schäffer, 1989</ref><ref type="bibr">), cuts (Karger, 1999)</ref> or eigenvalues <ref type="bibr" target="#b34">(Spielman &amp; Teng, 2011)</ref> while coarsening reduces the number of nodes with similar constraints <ref type="bibr" target="#b25">(Loukas &amp; Vandergheynst, 2018;</ref><ref type="bibr" target="#b24">Loukas, 2019;</ref><ref type="bibr" target="#b6">Deng et al., 2020)</ref>, typically by grouping original nodes into super-nodes, and defining their connections. <ref type="bibr" target="#b2">Cai et al. (2021)</ref> proposes a GNN-based framework to learn these connections to improve coarsening quality. <ref type="bibr">Huang et al. (2021)</ref> adopts coarsening as a preprocessing method to help scale up GNNs. Graph condensation also aims to reduce the number of nodes, but aims to learn synthetic nodes and connections in a supervised way, rather than unsupervised grouping as in these prior works. Graph pooling is also related to our work, but it targets at improving graph-level representation learning (see Appendix D).</p><p>Graph Neural Networks. Graph neural networks (GNNs) are a modern way to capture the intuition that inferences for individual samples (nodes) can be enhanced by utilizing graph-based information from neighboring nodes <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017;</ref><ref type="bibr" target="#b22">Klicpera et al., 2018;</ref><ref type="bibr" target="#b36">Veličković et al., 2018;</ref><ref type="bibr" target="#b40">Wu et al., 2019b;</ref><ref type="bibr">a)</ref>. Due to their prevalence, various real-world applications have been tremendously facilitated including recommender systems <ref type="bibr" target="#b41">(Ying et al., 2018a;</ref><ref type="bibr" target="#b10">Fan et al., 2019)</ref>, computer vision <ref type="bibr" target="#b23">(Li et al., 2019)</ref> and drug discovery <ref type="bibr" target="#b8">(Duvenaud et al., 2015)</ref>.</p><p>Graph Structure Learning. Our work is also related to graph structure learning, which explores methods to learn graphs from data. One line of work <ref type="bibr" target="#b7">(Dong et al., 2016;</ref><ref type="bibr" target="#b9">Egilmez et al., 2017)</ref> learns graphs under certain structural constraints (e.g. sparsity) based on graph signal processing. Recent efforts aim to learn graphs by leveraging GNNs <ref type="bibr" target="#b13">(Franceschi et al., 2019;</ref><ref type="bibr" target="#b19">Jin et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. However, these methods are incapable of learning graphs with smaller size, and are thus not applicable for graph condensation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we present our proposed graph condensation framework, GCOND. Consider that we have a graph dataset T = {A, X, Y}, where A ∈ R N ×N is the adjacency matrix, N is the number of nodes, X ∈ R N ×d is the d-dimensional node feature matrix and Y ∈ {0, . . . , C − 1} N denotes the node labels over C classes. Graph condensation aims to learn a small, synthetic graph dataset</p><formula xml:id="formula_0">S = {A , X , Y } with A ∈ R N ×N , X ∈ R N ×D , Y ∈ {0, .</formula><p>. . , C − 1} N and N N , such that a GNN trained on S can achieve comparable performance to one trained on the much larger T . Thus, the objective can be formulated as the following bi-level problem, min</p><formula xml:id="formula_1">S L (GNN θ S (A, X), Y) s.t θ S = arg min θ L(GNN θ (A , X ), Y ),<label>(1)</label></formula><p>where GNN θ denotes the GNN model parameterized with θ, θ S denotes the parameters of the model trained on S, and L denotes the loss function used to measure the difference between model predictions and ground truth, i.e. cross-entropy loss. However, optimizing the above objective can lead to overfitting on a specific model initialization. To generate condensed data that generalizes to a distribution of random initializations P θ0 , we rewrite the objective as follows:</p><formula xml:id="formula_2">min S E θ0∼P θ 0 [L (GNN θ S (A, X), Y)] s.t. θ S = arg min θ L(GNN θ(θ0) (A , X ), Y ). (2)</formula><p>where θ(θ 0 ) indicates that θ is a function acting on θ 0 . Note that the setting discussed above is for inductive learning where all the nodes are labeled and test nodes are unseen during training. We can easily generalize graph condensation to transductive setting by assuming Y is partially labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GRAPH CONDENSATION VIA GRADIENT MATCHING</head><p>To tackle the optimization problem in Eq. ( <ref type="formula">2</ref>), one strategy is to compute the gradient of L w.r.t S and optimize S via gradient descent, as in dataset distillation <ref type="bibr" target="#b37">(Wang et al., 2018)</ref>. However, this requires solving a nested loop optimization and unrolling the whole training trajectory of the inner problem, which can be prohibitively expensive. To bypass the bi-level optimization, we follow the gradient matching method proposed in <ref type="bibr" target="#b46">(Zhao et al., 2020)</ref> which aims to match the network parameters w.r.t. large-real and small-synthetic training data by matching their gradients at each training step. In this way, the training trajectory on small-synthetic data S can mimic that on the large-real data T , i.e., the models trained on these two datasets converge to similar solutions (parameters). Concretely, the parameter matching process for GNNs can be modeled as follows:</p><formula xml:id="formula_3">min S E θ0∼P θ 0 T −1 t=0 D θ S t , θ T t with θ S t+1 = opt θ L GNN θ S t (A , X ), Y and θ T t+1 = opt θ L GNN θ T t (A, X), Y<label>(3)</label></formula><p>where D(•, •) is a distance function, T is the number of steps of the whole training trajectory, opt θ is the update rule for model parameters, and θ S t , θ T t denote the model parameters trained on S and T at time step t, respectively. Since our goal is to match the parameters step by step, we then consider one-step gradient descent for the update rule opt θ :</p><formula xml:id="formula_4">θ S t+1 ← θ S t − η∇ θ L GNN θ S t (A , X ), Y and θ T t+1 ← θ T t − η∇ θ L GNN θ T t (A, X), Y<label>(4)</label></formula><p>where η is the learning rate for the gradient descent. Based on the observation made in <ref type="bibr" target="#b46">Zhao et al. (2020)</ref> that the distance between θ S t and θ T t is typically small, we can simplify the objective as a gradient matching process as follows,</p><formula xml:id="formula_5">min S E θ0∼P θ 0 T −1 t=0 D (∇ θ L (GNN θt (A , X ), Y ) , ∇ θ L (GNN θt (A, X), Y))<label>(5)</label></formula><p>where θ S t and θ S t are replaced by θ t . The distance D is further defined as the sum of the distance dis at each layer. Given two gradients G S ∈ R d1×d2 and G T ∈ R d1×d2 at a specific layer, the distance dis(•, •) used for condensation is defined as follows,</p><formula xml:id="formula_6">dis(G S , G T ) = d2 i=1 1 − G S i • G T i G S i G T i (6)</formula><p>where G S i , G T i are the i-th column vectors of the gradient matrices. With the above formulations, we are able to achieve parameter matching through an efficient strategy of gradient matching.</p><p>We note that jointly learning the three variables A , X and Y is highly challenging, as they are interdependent. Hence, to simplify the problem, we fix the node labels Y while keeping the class distribution the same as the original labels Y.</p><p>Graph Sampling. GNNs are often trained in a full-batch manner <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b40">Wu et al., 2019b)</ref>. However, as suggested by previous works that reconstruct data from gradients <ref type="bibr" target="#b49">(Zhu et al., 2019)</ref>, large batch size tends to make reconstruction more difficult because more variables are involved during optimization. To make things worse, the computation cost of GNNs gets expensive on large graphs as the forward pass of GNNs involves the aggregation of enormous neighboring nodes. To address the above issues, we sample a fixed-size set of neighbors on the original graph in each aggregation layer of GNNs and adopt a mini-batch training strategy. To further reduce memory usage and ease optimization, we calculate the gradient matching loss for nodes from different classes separately, as matching the gradients w.r.t. the data from a single class is easier than that from all classes. Specifically, for a given class c, we sample a batch of nodes of class c together with a portion of their neighbors from large-real data T . We denote the process as (A c , X c , Y c ) ∼ T . For the condensed graph A , we sample a batch of synthetic nodes of class c but do not sample Preprint their neighbors as we need to learn the connections with other nodes. We denote the process as</p><formula xml:id="formula_7">(A c , X c , Y c ) ∼ S.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODELING CONDENSED GRAPH DATA</head><p>One essential challenge in the graph condensation problem is how to model the condensed graph data and resolve dependency among nodes. The most straightforward way is to treat both A and X as free parameters. However, the number of parameters in A grows quadratically as N increases. The increased model complexity can pose challenges in optimizing the framework and increase the risk of overfitting. Therefore, it is desired to parametrize the condensed adjacency matrix in a way where the number of parameters does not grow too fast. On the other hand, treating A and X as independent parameters overlooks the implicit correlations between graph structure and features, which have been widely acknowledged in the literature <ref type="bibr" target="#b30">(Pfeiffer III et al., 2014;</ref><ref type="bibr" target="#b33">Shalizi &amp; Thomas, 2011</ref>); e.g., in social networks, users interact with others based on their interests, while in e-commerce, users purchase products due to certain product attributes. Hence, we propose to model the condensed graph structure as a function of the condensed node features:</p><formula xml:id="formula_8">A = g Φ (X ), with A ij = Sigmoid MLP Φ ([x i ; x j ]) + MLP Φ ([x j ; x i ]) 2 (7)</formula><p>where MLP Φ is a multi-layer neural network parameterized with Φ and [•; •] denotes concatenation. In Eq. ( <ref type="formula">7</ref>), we intentionally control A ij = A ji to make the condensed graph structure symmetric since we are mostly dealing with symmetric graphs. It can also adjust to asymmetric graphs by setting</p><formula xml:id="formula_9">A ij = Sigmoid(MLP Φ ([x i ; x j ]).</formula><p>Then we rewrite our objective as</p><formula xml:id="formula_10">min X ,Φ E θ0∼P θ 0 T −1 t=0 D (∇ θ L (GNN θt (g Φ (X ), X ), Y ) , ∇ θ L (GNN θt (A, X), Y))<label>(8)</label></formula><p>Note that there are two clear benefits of the above formulation over the naïve one (free parameters).</p><p>Firstly, the number of parameters for modeling graph structure no longer depends on the number of nodes, hence avoiding jointly learning O(N 2 ) parameters; as a result, when N gets larger, GCOND suffers less risk of overfitting. Secondly, if we want to grow the synthetic graph by adding more synthetic nodes condensed from real graph, the trained MLP Φ can be employed to infer the connections of new synthetic nodes, and hence we only need to learn their features.</p><p>Alternating Optimization Schema. Jointly optimizing X and Φ is often challenging as they are directly affecting each other. Instead, we propose to alternatively optimize X and Φ: we update Φ for the first τ 1 epochs and then update X for τ 2 epochs; the process is repeated until the stopping condition is met -we find empirically that this does better as shown in Appendix B.</p><p>Sparsification. In the learned condensed adjacency matrix A , there can exist some small values which have little effect on the aggregation process in GNNs but still take up a certain amount of storage (e.g. 4 bytes per float). Thus, we remove the entries whose values are smaller than a given threshold δ to promote sparsity of the learned A . We further justify that suitable choices of δ for sparsification do not degrade performance a lot in Appendix B.</p><p>The detailed algorithm can be found in Algorithm 1 in Appendix C. In detail, we first set the condensed label set Y to fixed values and initialize X as node features randomly selected from each class. In each outer loop, we sample a GNN model initialization θ from a distribution P θ . Then, for each class we sample the corresponding node batches from T and S, and calculate the gradient matching loss within each class. The sum of losses from different classes are used to update X or Φ. After that we update the GNN parameters for τ θ epochs. When finishing the updating of condensed graph parameters, we use A = ReLU(g Φ (X ) − δ) to obtain the final sparsified graph structure.</p><p>A "Graphless" Model Variant. We now explore another parameterization for the condensed graph data. We provide a model variant named GCOND-X that only learns the condensed node features X without learning the condensed structure A . In other words, we use a fixed identity matrix I as the condensed graph structure. Specifically, this model variant aims to match the gradients of GNN parameters on the large-real data (A, X) and small-synthetic data (I, X ). Although GCOND-X is unable to learn the condensed graph structure which can be highly useful for downstream data analysis, it still shows competitive performance in Table <ref type="table" target="#tab_1">2</ref> in the experiments because the features are learned to incorporate relevant information from the graph via the matching loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we design experiments to validate the effectiveness of the proposed framework GCOND. We first introduce experimental settings, then compare GCOND against representative baselines with discussions and finally show some advantages of GCOND. Our code will be released at https://github.com/ChandlerBang/GCond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP</head><p>Datasets. We evaluate the condensation performance of the proposed framework on three transductive datasets, i.e., Cora, Citeseer <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016)</ref> and Ogbn-arxiv <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, and two inductive datasets, i.e., Flickr <ref type="bibr" target="#b43">(Zeng et al., 2020)</ref> and Reddit <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>. We use the public splits for all the datasets. For the inductive setting, we follow the setup in <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref> where the test graph is not available during training. Dataset statistics are shown in Appendix A.</p><p>Baselines. We compare our proposed methods to five baselines: (i) one graph coarsening method <ref type="bibr" target="#b24">(Loukas, 2019;</ref><ref type="bibr">Huang et al., 2021)</ref>, (ii-iv) three coreset methods (Random, Herding <ref type="bibr" target="#b38">(Welling, 2009)</ref> and K-Center <ref type="bibr" target="#b11">(Farahani &amp; Hekmatfar, 2009;</ref><ref type="bibr" target="#b32">Sener &amp; Savarese, 2017</ref>)), and (v) dataset condensation (DC). For the graph coarsening method, we adopt the variation neighborhoods method implemented by <ref type="bibr">Huang et al. (2021)</ref>. For coreset methods, we first use them to select nodes from the original dataset and induce a subgraph from the selected nodes to serve as the reduced graph. In Random, the nodes are randomly selected. The Herding method, which is often used in continual learning <ref type="bibr" target="#b31">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b3">Castro et al., 2018)</ref>, picks samples that are closest to the cluster center. K-Center selects the center samples to minimize the largest distance between a sample and its nearest center. We use the implementations provided by <ref type="bibr" target="#b46">Zhao et al. (2020)</ref> for Herding, K-Center and DC. As vanilla DC cannot leverage any structure information, we develop a variant named DC-Graph, which additionally leverages graph structure during test stage, to replace DC for the following experiments. A comparison between DC, DC-Graph, GCOND and GCOND-X is shown in Table <ref type="table" target="#tab_0">1</ref> and their training details can be found in Appendix A.3.</p><p>Evaluation. We first use the aforementioned baselines to obtain condensed graphs and then evaluate them on GNNs for both transductive and inductive node classification tasks. For transductive datasets, we condense the full graph with N nodes into a synthetic graph with r% of N nodes, where r is the ratio of synthetic nodes to original nodes. For inductive datasets, we only condense the training graph since the rest of the full graph is not available during training. The choices of r<ref type="foot" target="#foot_1">2</ref> are listed in Table <ref type="table" target="#tab_1">2</ref>. For each r, we generate 5 condensed graphs with different seeds. To evaluate the effectiveness of condensed graphs, we have two stages: (1) a training stage, where we train a GNN model on the condensed graph, and (2) a test stage, where the trained GNN uses the test graph (or full graph in transductive setting) to infer the labels for test nodes. The resulting test performance is compared with that obtained when training on original datasets. All experiments are repeated 10 times, and we report average performance and variance.</p><p>Hyperparameter settings. As our goal is to generate highly informative synthetic graphs which can benefit GNNs, we choose one representative model, <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>, for performance evaluation. For the GNN used in condensation, i.e., the GNN θ (•) in Eq. ( <ref type="formula" target="#formula_10">8</ref>), we adopt SGC <ref type="bibr" target="#b39">(Wu et al., 2019a)</ref> which removes the ReLU activation from GCN. In our gradient matching problem, SGC enjoys the advantage of being twice-differentiable while sharing similar graph filtering behavior as GCN. Unless otherwise stated, we use 2-layer models with 256 hidden units. The weight decay and dropout for the models are set to 0 in condensation process. More details for hyper-parameter tuning can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON WITH BASELINES</head><p>In this subsection, we test the performance of a 2-layer GCN on the condensed graphs, and compare the proposed GCOND and GCOND-X with baselines. Notably, all methods produce both structure and node features, i.e. A and X , except DC-Graph and GCOND-X. Since DC-Graph and GCOND-X do not produce any structure, we simply use an identity matrix as the adjacency matrix when training GNNs solely on condensed features. However, during inference, we use the full graph   (transductive setting) or test graph (inductive setting) to propagate information based on the trained GNNs. This training paradigm is similar to the C&amp;S model <ref type="bibr" target="#b17">(Huang et al., 2020)</ref> which trains an MLP without the graph information and performs label propagation based on MLP predictions. Table <ref type="table" target="#tab_1">2</ref> reports node classification performance; we make the following observations:</p><formula xml:id="formula_11">Herding (A , X ) K-Center (A , X ) Coarsening (A , X ) DC-Graph (X ) GCOND-X (X ) GCOND (A ,</formula><p>Obs 1. Condensation methods achieve promising performance even with extremely large reduction rates. Condensation methods, i.e., GCOND, GCOND-X and DC-Graph, outperform coreset methods and graph coarsening significantly at the lowest ratio r for each dataset. This shows the importance of learning synthetic data using the guidance from downstream tasks. Notably, GCOND achieves 79.8%, 80.1% and 79.3% at 1.3%, 2.6% and 5.2% condensation ratios at Cora, while the whole dataset performance is 81.2%. The GCOND variants also show promising performance on Cora, Flickr and Reddit at all coarsening ratios. Although the gap between whole-dataset Ogbn-arxiv and our methods is larger, they still outperform baselines by a large margin.</p><p>Obs 2. Learning X instead of (A , X ) as the condensed graph can also lead to good results. GCOND-X achieves close performance to GCOND on 11 of 15 cases. Since our objective in graph condensation is to achieve parameter matching through gradient matching, training a GNN on the learned features X with identity adjacency matrix is also able to mimic the training trajectory of GNN parameters. One reason could be that X has already encoded node features and structural information of the original graph during the condensation process. However, there are many scenarios where the graph structure is essential such as the generalization to other GNN architectures (e.g., GAT) and visualizing the patterns in the data. We will provide more details in the following subsections.</p><p>Obs 3. Condensing node features and structural information simultaneously can lead to better performance. In most cases, GCOND and GCOND-X obtain much better performance than DC-Graph. One key reason is that GCOND and GCOND-X can take advantage of both node features and structural information in the condensation process. We notice that DC-Graph achieves a highly Preprint comparable result (90.5%) on Reddit at 0.2% condensation ratio to the whole dataset performance (93.9%). This may indicate that the original training graph structure might not be useful. To verify this assumption, we train a GCN on the original Reddit dataset without using graph structure (i.e., setting A train = I), but allow using the test graph structure for inference using the trained model. The obtained performance is 92.5%, which is very close to the original performance 93.9%, indicating that training without graph structure can still achieve comparable performance. We also note that learning X , A simultaneously creates opportunities to absorb information from graph structure directly into learned features, lessening reliance on distilling graph properties reliably while still achieving good generalization performance from features.</p><p>Obs 4. Larger condensed graph size does not strictly indicate better performance. Although larger condensed graph sizes allow for more parameters which can potentially encapsulate more information from original graph, it simultaneously becomes harder to optimize due to the increased model complexity. We observe that once the condensation ratio reaches a certain threshold, the performance becomes stable. However, the performance of coreset methods and graph coarsening is much more sensitive to the reduction ratio. Coreset methods only select existing samples while graph coarsening groups existing nodes into super nodes. When the reduction ratio is too low, it becomes extremely difficult to select informative nodes or form representative super nodes by grouping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GENERALIZABILITY OF CONDENSED GRAPHS</head><p>Next, we illustrate the generalizability of condensed graphs from the following two perspectives: (1) the graphs that are condensed based on one specific GNN can be used to train other GNNs; and (2) graphs can be condensed using different GNNs and they all show reasonable transfer performance.</p><p>Different Architectures. Next, we show the generalizability of the graph condensation procedure. Specifically, we show test performance when using a graph condensed by one GNN model to train different GNN architectures. Specifically, we choose APPNP <ref type="bibr" target="#b22">(Klicpera et al., 2018)</ref>, GCN, SGC <ref type="bibr" target="#b39">(Wu et al., 2019a)</ref>, GraphSAGE <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>, Cheby <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref> and GAT <ref type="bibr" target="#b36">(Veličković et al., 2018)</ref>. We also include MLP and report the results in Table <ref type="table" target="#tab_3">3</ref>. From the table, we find that the condensed graphs generated by GCOND show good generalization on different architectures. We may attribute such transferability across different architectures to similar filtering behaviors of those GNN models, which have been studied in <ref type="bibr" target="#b27">Ma et al. (2020)</ref>; <ref type="bibr" target="#b50">Zhu et al. (2021)</ref>.</p><p>Versatility of GCOND. The proposed GCOND is highly composable in that we can adopt various GNNs inside the condensation network. We investigate the performances of various GNNs when using different GNN models in the condensation process, i.e., GNN θ (•) in Eq. ( <ref type="formula" target="#formula_10">8</ref>). We choose APPNP, Cheby, GCN, GraphSAGE and SGC to serve as the models used in condensation and evaluation. Note that we omit GAT due to its deterioration under large neighborhood sizes <ref type="bibr" target="#b26">(Ma et al., 2021)</ref>.</p><p>We choose Cora and Ogbn-arxiv to report the performance in Table <ref type="table" target="#tab_4">4</ref> where C and T denote con-  densation and test models, respectively. The graphs condensed by different GNNs all show strong transfer performance on other architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS ON CONDENSED DATA</head><p>Statistics of Condensed Graphs. In Table <ref type="table" target="#tab_5">5</ref>, we compare several properties between condensed graphs and original graphs. Note that a widely used homophily measure is defined in <ref type="bibr" target="#b48">(Zhu et al., 2020)</ref> but it does not apply to weighted graphs. Hence, when computing homophily, we binarize the graphs by removing edges whose weights are smaller than 0.5. We make the following observations. First, while achieving similar performance for downstream tasks, the condensed graphs contain fewer nodes and take much less storage. Second, the condensed graphs are less sparse than their larger counterparts. Since the condensed graph is on extremely small scale, there would be almost no connections between nodes if the condensed graph maintains the original sparsity. Third, for Citeseer, Cora and Flickr, the homophily information are well preserved in the condensed graphs.</p><p>Visualization. We present the visualization results for all datasets in Figure <ref type="figure">4</ref>, where nodes with the same color are from the same class. Notably, as the learned condensed graphs are weighted graphs, we use black lines to denote the edges with weights larger than 0.5 and gray lines to denote the edges with weights smaller than 0.5. From Figure <ref type="figure">4</ref>, we can observe some patterns in the condensed graphs, e.g., the homophily patterns on Cora and Citeseer are well preserved. Interestingly, the learned graph for Reddit is very close to a star graph where almost all the nodes only have connections with very few center nodes. Such a structure can be meaningless for GNNs because almost all the nodes receive the information from their neighbors. In this case, the learned features X play a major role in training GNN parameters, indicating that the original training graph of Reddit is not very informative, aligning with our observations in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The prevalence of large-scale graphs poses great challenges in training graph neural networks. Thus, we study a novel problem of graph condensation which targets at condensing a large-real graph into a small-synthetic one while maintaining the performances of GNNs. Through our proposed framework, we are able to significantly reduce the graph size while approximating the original performance. The condensed graphs take much less space of storage and can be used to efficiently train various GNN architectures. In the future, we plan to investigate interpretable graph condensation and graph condensation for graph-level applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASETS AND HYPER-PARAMETERS</head><p>A.1 DATASETS</p><p>We evaluate the proposed framework on three transductive datasets, i.e., Cora, Citeseer <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2016)</ref> and Ogbn-arxiv <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, and two inductive datasets, i.e., Flickr <ref type="bibr" target="#b43">(Zeng et al., 2020)</ref> and Reddit <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>. Since all the datasets have public splits, we download them from PyTorch Geometric <ref type="bibr" target="#b12">(Fey &amp; Lenssen, 2019)</ref> and use those splits throughout the experiments. Dataset statistics are shown in Table <ref type="table" target="#tab_6">6</ref>. A.2 HYPER-PARAMETER SETTING Condensation Process. For DC, we tune the number of hidden layers in a range of {1, 2, 3} and fix the number of hidden units to 256. We further tune the number of epochs for training DC in a range of {100, 200, 400}. For GCOND, without specific mention, we adopt a 2-layer SGC <ref type="bibr" target="#b39">(Wu et al., 2019a)</ref> with 256 hidden units as the GNN used for gradient matching. The function g Φ that models the relationship between A and X is implemented as a multi-layer perceptron (MLP). Specifically, we adopt a 3-layer MLP with 128 hidden units for small graphs (Cora and Citeseer) and 256 hidden units for large graphs (Flickr, Reddit and Ogbn-arxiv). We tune the training epoch for GCOND in a range of {400, 600, 1000}. For GCOND-X, we tune the number of hidden layers in a range of {1, 2, 3} and fix the number of hidden units to 256. We further tune the number of epochs for training GCOND-X in a range of {100, 200, 400}. We tune the learning rate for all the methods in a range of {0.1, 0.01, 0.001, 0.0001}. Furthermore, we set δ to be 0.05, 0.05, 0.01, 0.01, 0.01 for Citeseer, Cora, Ogbn-arxiv, Flickr and Reddit, respectively.</p><p>For the choices of condensation ratio r, we divide the discussion into two parts. The first part is about transductive datasets. For Cora and Citeseer, since their labeling rates are very small (5.2% and 3.6%, respectively), we choose r to be {25%, 50%, 100%} of the labeling rate. Thus, we finally choose {1.3%, 2.6%, 5.2%} for Cora and {0.9%, 1.8%, 3.6%} for Citeseer. For Ogbn-arxiv, we choose r to be {0.1%, 0.5%, 1%} of its labeling rate (53%), thus being {0.05%, 0.25%, 0.5%}. The second part is about inductive datasets. As the nodes in the training graphs are all labeled in inductive datasets, we simply choose {0.1%, 0.5%, 0.1%} for Flickr and 0.05%, 0.1%, 0.2% for Reddit.</p><p>Evaluation Process. During the evaluation process, we set dropout rate to be 0 and weight decay to be 0.0005 when training various GNNs. The number of epochs is set to 3000 for GAT while it is set to 600 for other models. The initial learning rate is set to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 TRAINING DETAILS OF DC-GRAPH, GCOND-X AND GCOND</head><p>DC-Graph: During the condensation stage, DC-Graph only leverages the node features to produce condensed node features X . During the training stage of evaluation, DC-Graph takes the condensed features X together with an identity matrix as the graph structure to train a GNN. In the later test stage of evaluation, the GNN takes both test node features and test graph structure as input to make predictions for test nodes.</p><p>GCOND-X: During the condensation stage, GCOND-X leverages both the graph structure and node features to produce condensed node features X . During the training stage of evaluation, GCOND-X takes the condensed features X together with an identity matrix as the graph structure to train a Depth Versus Hidden Units. We vary the number of model layers (GCN) in a range of {1, 2, 3, 4} and the number of hidden units in a range of {16, 32, 64, 128, 256}, and test them on the condensed graphs of Cora and Citeseer. The results are reported in Table <ref type="table">9</ref>. From the table, we can observe that changing the number of layers impacts the model performance a lot while changing the number of units does not.</p><p>Propagation Versus Transformation. We further study the effect of propagation and transformation on the condensed graph. We use Cora as an example and use SGC as the test model due to its decoupled architecture. Specifically, we vary both the propagation layers and transformation layers of SGC in the range of {1, 2, 3, 4, 5}, and report the performance in Table <ref type="table" target="#tab_7">10</ref>. As can be seen, the condensed graph still achieves good performance with 3 and 4 layers of propagation. Although the condensed graph is generated under 2-layer SGC, it is able to generalize to 3-layer and 4-layer SGC. When increasing the propagation to 5, the performance degrades a lot which could be the cause of the oversmoothing issue. On the other hand, stacking more transformation layers can first help boost the performance but then hurt. Given the small scale of the graph, SGC suffers the overfitting issue in this case. In addition, we provide the t-SNE (Van der Maaten &amp; Hinton, 2008) plots of condensed node features to further understand the condensed graphs. In Cora and Citeseer, the condensed node features Preprint are well clustered. For Ogbn-arxiv and Reddit, we also observe some clustered pattern for the nodes within the same class. In contrast, the condensed features are less discriminative in Flickr, which indicates that the condensed structure information can be essential in training GNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ALGORITHM</head><p>We show the detailed algorithm of GCOND in Algorithm 1. In detail, we first set the condensed label set Y to fixed values and initialize X as node features randomly selected from each class. In each outer loop, we sample a GNN model initialization θ from a distribution P θ . Then, for each class we sample the corresponding node batches from T and S, and calculate the gradient matching loss within each class. The sum of losses from different classes are used to update X or Φ. After that we update the GNN parameters for τ θ epochs. When finishing the updating of condensed graph parameters, we use A = ReLU(g Φ (X ) − δ) to obtain the final sparsified graph structure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE RELATED WORK</head><p>Graph pooling. Graph pooling <ref type="bibr" target="#b44">(Zhang et al., 2018;</ref><ref type="bibr" target="#b42">Ying et al., 2018b;</ref><ref type="bibr" target="#b14">Gao &amp; Ji, 2019</ref>) also generates a coarsened graph with smaller size. <ref type="bibr" target="#b44">Zhang et al. (2018)</ref> is one the first to propose an end-to-end architecture for graph classification by incorporating graph pooling. Later, DiffPool <ref type="bibr" target="#b42">(Ying et al., 2018b)</ref> proposes to use GNNs to parameterize the process of node grouping. However, those methods are majorly tailored for the graph classification task and the coarsened graphs are a byproduct graph during the representation learning process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Condensed graphs sometimes exhibit structure mimicking the original (a, b, d). Other times (c, e), learned features absorb graph properties and create less explicit graph reliance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test accuracy and sparsity under different values of δ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )Figure 4 :</head><label>a4</label><figDesc>Figure 4: The t-SNE plots of node features in condensed graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>GCOND for Graph CondensationInput: Training data T = (A, X, Y), pre-defined condensed labels Y Initialize X as node features randomly selected from each class for k = 0, . . . , K − 1 do Initialize θ0 ∼ P θ 0 for t = 0, . . . , T − 1 doD = 0 for c = 0, . . . , C − 1 do Compute A = gΦ(X ); then S = {A , X , Y } Sample (Ac, Xc, Yc) ∼ T and (A c , X c , Y c ) ∼ S detailed in Section 3.1 Compute L T = L (GNN θ t (Ac, Xc), Yc) and L S = L (GNN θ t (A c , X c ), Y c ) D ← D + D(∇ θ t L T , ∇ θ t L S ) if t%(τ1 + τ2) &lt; τ1 then Update X ← X − η1∇ X D else Update Φ ← Φ − η2∇ΦD Update θt+1 ← opt θ (θt, S, τ θ )τ θ is the number of steps for updating θ A = ReLU(gΦ(X ) − δ) Return: (A , X , Y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Information comparison used during condensation, training and test for reduction methods; all baselines mimc the GCOND setting. A , X and A, X are condensed (original) graph and features, respectively.</figDesc><table><row><cell></cell><cell>DC</cell><cell cols="2">DC-Graph GCOND-X</cell><cell>GCOND</cell></row><row><cell cols="3">Condensation Xtrain Xtrain</cell><cell cols="2">Atrain, Xtrain Atrain, Xtrain</cell></row><row><cell>Training</cell><cell>X</cell><cell>X</cell><cell>X</cell><cell>A , X</cell></row><row><cell>Test</cell><cell>Xtest</cell><cell cols="2">Atest, Xtest Atest, Xtest</cell><cell>Atest, Xtest</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>GCOND and GCOND-X achieves promising performance in comparison to baselines even with extremely large reduction rates. We report transductive performance on Citeseer, Cora, Ogbnarxiv; inductive performance on Flickr, Reddit. Performance is reported as test accuracy (%).</figDesc><table><row><cell></cell><cell></cell><cell>Baselines</cell><cell>Proposed</cell></row><row><cell>Dataset</cell><cell>Ratio (r)</cell><cell>Random (A , X )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>X )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Whole</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell></row><row><cell></cell><cell>0.9%</cell><cell>54.4±4.4 57.1±1.5 52.4±2.8 52.2±0.4 66.8±1.5 71.4±0.8 70.5±1.2</cell><cell></cell></row><row><cell>Citeseer</cell><cell>1.8%</cell><cell>64.2±1.7 66.7±1.0 64.3±1.0 59.0±0.5 66.9±0.9 69.8±1.1 70.6±0.9</cell><cell>71.7±0.1</cell></row><row><cell></cell><cell>3.6%</cell><cell>69.1±0.1 69.0±0.1 69.1±0.1 65.3±0.5 66.3±1.5 69.4±1.4 69.8±1.4</cell><cell></cell></row><row><cell></cell><cell>1.3%</cell><cell>63.6±3.7 67.0±1.3 64.0±2.3 31.2±0.2 67.3±1.9 75.9±1.2 79.8±1.3</cell><cell></cell></row><row><cell>Cora</cell><cell>2.6%</cell><cell>72.8±1.1 73.4±1.0 73.2±1.2 65.2±0.6 67.6±3.5 75.7±0.9 80.1±0.6</cell><cell>81.2±0.2</cell></row><row><cell></cell><cell>5.2%</cell><cell>76.8±0.1 76.8±0.1 76.7±0.1 70.6±0.1 67.7±2.2 76.0±0.9 79.3±0.3</cell><cell></cell></row><row><cell></cell><cell>0.05%</cell><cell>47.1±3.9 52.4±1.8 47.2±3.0 35.4±0.3 58.6±0.4 61.3±0.5 59.2±1.1</cell><cell></cell></row><row><cell>Ogbn-arxiv</cell><cell>0.25%</cell><cell>57.3±1.1 58.6±1.2 56.8±0.8 43.5±0.2 59.9±0.3 64.2±0.4 63.2±0.3</cell><cell>71.4±0.1</cell></row><row><cell></cell><cell>0.5%</cell><cell>60.0±0.9 60.4±0.8 60.3±0.4 50.4±0.1 59.5±0.3 63.1±0.5 64.0±0.4</cell><cell></cell></row><row><cell></cell><cell>0.1%</cell><cell>41.8±2.0 42.5±1.8 42.0±0.7 41.9±0.2 46.3±0.2 45.9±0.1 46.5±0.4</cell><cell></cell></row><row><cell>Flickr</cell><cell>0.5%</cell><cell>44.0±0.4 43.9±0.9 43.2±0.1 44.5±0.1 45.9±0.1 45.0±0.2 47.1±0.1</cell><cell>47.2±0.1</cell></row><row><cell></cell><cell>1%</cell><cell>44.6±0.2 44.4±0.6 44.1±0.4 44.6±0.1 45.8±0.1 45.0±0.1 47.1±0.1</cell><cell></cell></row><row><cell></cell><cell>0.05%</cell><cell>46.1±4.4 53.1±2.5 46.6±2.3 40.9±0.5 88.2±0.2 88.4±0.4 88.0±1.8</cell><cell></cell></row><row><cell>Reddit</cell><cell>0.1%</cell><cell>58.0±2.2 62.7±1.0 53.0±3.3 42.8±0.8 89.5±0.1 89.3±0.1 89.6±0.7</cell><cell>93.9±0.0</cell></row><row><cell></cell><cell>0.2%</cell><cell>66.3±1.9 71.0±1.6 58.5±2.1 47.4±0.9 90.5±1.2 88.8±0.4 90.1±0.5</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Graph condensation can work well with different architectures. Avg. stands for the average test accuracy of APPNP, Cheby, GCN, GraphSAGE and SGC. SAGE stands for GraphSAGE.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Data</cell><cell>MLP</cell><cell>GAT</cell><cell>APPNP</cell><cell>Cheby</cell><cell>GCN</cell><cell>SAGE</cell><cell>SGC</cell><cell>Avg.</cell></row><row><cell>Citeseer r = 1.8%</cell><cell>DC-Graph GCOND-X GCOND</cell><cell>X X A , X</cell><cell>66.2 69.6 63.9</cell><cell>--55.4</cell><cell>66.4 69.7 69.6</cell><cell>64.9 70.6 68.3</cell><cell>66.2 69.7 70.5</cell><cell>65.9 69.2 66.2</cell><cell>69.6 71.6 70.3</cell><cell>66.6 70.2 69.0</cell></row><row><cell>Cora r = 2.6%</cell><cell>DC-Graph GCOND-X GCOND</cell><cell>X X A , X</cell><cell>67.2 76.0 73.1</cell><cell>--66.2</cell><cell>67.1 77.0 78.5</cell><cell>67.7 74.1 76.0</cell><cell>67.9 75.3 80.1</cell><cell>66.2 76.0 78.2</cell><cell>72.8 76.1 79.3</cell><cell>68.3 75.7 78.4</cell></row><row><cell>Ogbn-arxiv r = 0.25%</cell><cell>DC-Graph GCOND-X GCOND</cell><cell>X X A , X</cell><cell>59.9 64.1 62.2</cell><cell>--60.0</cell><cell>60.0 61.5 63.4</cell><cell>55.7 59.5 54.9</cell><cell>59.8 64.2 63.2</cell><cell>60.0 64.4 62.6</cell><cell>60.4 64.7 63.7</cell><cell>59.2 62.9 61.6</cell></row><row><cell>Flickr r = 0.5%</cell><cell>DC-Graph GCOND-X GCOND</cell><cell>X X A , X</cell><cell>43.1 42.1 44.8</cell><cell>--40.1</cell><cell>45.7 44.6 45.9</cell><cell>43.8 42.3 42.8</cell><cell>45.9 45.0 47.1</cell><cell>45.8 44.7 46.2</cell><cell>45.6 44.4 46.1</cell><cell>44.2 45.6</cell></row><row><cell>Reddit r = 0.1%</cell><cell>DC-Graph GCOND-X GCOND</cell><cell>X X A , X</cell><cell>50.3 40.1 42.5</cell><cell>--60.2</cell><cell>81.2 78.7 87.8</cell><cell>77.5 74.0 75.5</cell><cell>89.5 89.3 89.4</cell><cell>89.7 89.3 89.1</cell><cell>90.5 91.0 89.6</cell><cell>85.7 84.5 86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Cross-architecture performance is shown in test accuracy (%). SAGE: GraphSAGE. Graphs condensed by different GNNs all show strong transfer performance on other architectures.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) Cora, r=2.6%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Ogbn-arxiv, r=0.05%</cell><cell></cell></row><row><cell>C\T</cell><cell>APPNP</cell><cell>Cheby</cell><cell>GCN</cell><cell>SAGE</cell><cell>SGC</cell><cell>C\T</cell><cell>APPNP</cell><cell>Cheby</cell><cell>GCN</cell><cell>SAGE</cell><cell>SGC</cell></row><row><cell cols="6">APPNP 72.1±2.6 60.8±6.4 73.5±2.4 72.3±3.5 73.1±3.1</cell><cell cols="6">APPNP 60.3±0.2 51.8±0.5 59.9±0.4 59.0±1.1 61.2±0.4</cell></row><row><cell cols="6">Cheby 75.3±2.9 71.8±1.1 76.8±2.1 76.4±2.0 75.5±3.5</cell><cell cols="6">Cheby 57.4±0.4 53.5±0.5 57.4±0.8 57.1±0.8 58.2±0.6</cell></row><row><cell>GCN</cell><cell cols="5">69.8±4.0 53.2±3.4 70.6±3.7 60.2±1.9 68.7±5.4</cell><cell>GCN</cell><cell cols="5">59.3±0.4 51.8±0.7 60.3±0.3 60.2±0.4 59.2±0.7</cell></row><row><cell cols="6">SAGE 77.1±1.1 69.3±1.7 77.0±0.7 76.1±0.7 77.7±1.8</cell><cell cols="6">SAGE 57.6±0.8 53.9±0.6 58.1±0.6 57.8±0.7 59.0±1.1</cell></row><row><cell>SGC</cell><cell cols="5">78.5±1.0 73.5±1.1 80.1±0.6 78.2±0.9 79.3±0.7</cell><cell>SGC</cell><cell cols="5">59.7±0.5 49.5±0.8 59.2±1.1 58.9±1.6 60.5±0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison between condensed graphs and original graphs. The condensed graphs have fewer nodes and are more dense.Citeseer, r=0.9% Cora, r=1.3% Ogbn-arxiv, r=0.25% Flickr, r=0.5% Reddit, r=0.1%</figDesc><table><row><cell></cell><cell cols="5">Whole GCOND Whole GCOND Whole</cell><cell>GCOND</cell><cell cols="4">Whole GCOND Whole GCOND</cell></row><row><cell>Accuracy</cell><cell>70.7</cell><cell>70.5</cell><cell>81.5</cell><cell>79.8</cell><cell>71.4</cell><cell>63.2</cell><cell>47.1</cell><cell>47.1</cell><cell>94.1</cell><cell>89.4</cell></row><row><cell>#Nodes</cell><cell>3,327</cell><cell>60</cell><cell>2,708</cell><cell>70</cell><cell>169,343</cell><cell>454</cell><cell>44,625</cell><cell>223</cell><cell>153,932</cell><cell>153</cell></row><row><cell>#Edges</cell><cell>4,732</cell><cell>1,454</cell><cell>5,429</cell><cell cols="2">2,128 1,166,243</cell><cell>3,354</cell><cell cols="4">218,140 3,788 10,753,238 301</cell></row><row><cell>Sparsity</cell><cell>0.09%</cell><cell>80.78%</cell><cell cols="3">0.15% 86.86% 0.01%</cell><cell>3.25%</cell><cell cols="2">0.02% 15.23%</cell><cell>0.09%</cell><cell>2.57%</cell></row><row><cell cols="2">Homophily 0.74</cell><cell>0.65</cell><cell>0.81</cell><cell>0.79</cell><cell>0.65</cell><cell>0.07</cell><cell>0.33</cell><cell>0.28</cell><cell>0.78</cell><cell>0.04</cell></row><row><cell>Storage</cell><cell cols="5">47.1 MB 0.9 MB 14.9 MB 0.4 MB 100.4 MB</cell><cell>0.3 MB</cell><cell cols="4">86.8 MB 0.5 MB 435.5 MB 0.4 MB</cell></row></table><note>(a) Cora, r=2.5% (b) Citeseer, r=1.8% (c) Arxiv, r=0.05% (d) Flickr, r=0.1% (e) Reddit, r=0.1%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics. The first three are transductive datasets and the last two are inductive datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="3">#Classes #Features Training/Validation/Test</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>140/500/1000</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>120/500/1000</cell></row><row><cell cols="2">Ogbn-arxiv 169,343</cell><cell>1,166,243</cell><cell>40</cell><cell>128</cell><cell>90,941/29,799/48,603</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>7</cell><cell>500</cell><cell>44,625/22312/22313</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 57,307,946</cell><cell>210</cell><cell>602</cell><cell>15,3932/23,699/55,334</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Test accuracy of SGC on different transformations and propagations for Cora, r=2.6%</figDesc><table><row><cell>Trans\Prop</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>1</cell><cell cols="5">77.09±0.43 79.02±1.17 78.12±2.13 74.04±3.60 61.19±7.73</cell></row><row><cell>2</cell><cell cols="5">76.94±0.50 79.01±0.57 79.11±1.15 77.57±1.03 72.37±4.25</cell></row><row><cell>3</cell><cell cols="5">75.28±0.58 77.95±0.67 74.16±1.50 70.58±3.71 58.28±8.90</cell></row><row><cell>4</cell><cell cols="5">66.87±0.73 66.54±0.82 59.24±1.60 43.94±6.33 30.45±9.67</cell></row><row><cell>5</cell><cell cols="5">46.44±0.91 37.29±3.23 16.05±2.74 15.33±2.79 15.33±2.79</cell></row></table><note>B.4 VISUALIZATION OF NODE FEATURES.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We aim to condense both graph structure and node attributes. A formal definition is given in Section 3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We determine r based on original graph size and labeling rate -see Appendix A for details.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MORE EXPERIMENTS B.1 ABLATION STUDY</head><p>Different Parameterization. We study the effect of different parameterizations for modeling A and compare GCOND with modeling A as free parameters in Table <ref type="table">7</ref>. From the table, we observe a significant improvement by taking into account the relationship between A and X . This suggests that directly modeling the structure as a function of features can ease the optimization and lead to better condensed graph data.</p><p>Joint optimization versus alternate optimization. We perform the ablation study on joint optimization and alternate optimization when updating Φ and X . The results are shown in Table <ref type="table">8</ref>.</p><p>From the table, we can observe that joint optimization always gives worse performance and the standard deviation is much higher than alternate optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SPARSIFICATION</head><p>In this subsection, we investigate the effect of threshold δ on the test accuracy and sparsity. In detail, we vary the values of the threshold δ used for truncating adjacency matrix in a range of {0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8}, and report the corresponding test accuracy and sparsity in Figure <ref type="figure">3</ref>. From the figure, we can see that increasing δ can effectively increase the sparsity of the obtained adjacency matrix without affecting the performance too much.</p><p>Table <ref type="table">9</ref>: Test accuracy on different numbers of hidden units (H) and layers (L). When L=1, there is no hidden layer so the number of hidden units is meaningless.</p><p>(a) Cora, r=2.6% </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Flexible dataset distillation: Learn labels instead of images</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bohdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08572</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph coarsening with neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uxpzitPEooJ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Francisco M Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolás</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphzoom: A multilevel spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lGO0EKDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning laplacian matrix in smooth graph signal representations</title>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="6160" to="6173" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph learning from data under laplacian and structural constraints</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Hilmi E Egilmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Pavez</surname></persName>
		</author>
		<author>
			<persName><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="825" to="841" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zanjirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farahani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Hekmatfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up graph neural networks via graph coarsening</title>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;21)</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random sampling in cut, flow, and network design problems</title>
		<author>
			<persName><surname>David R Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="383" to="413" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph reduction with spectral and cut guarantees</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">116</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectrally approximating large graphs with smaller graphs</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3237" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning discrete adaptive receptive fields for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=pHkBwAaZ3UK" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01777</idno>
		<title level="m">A unified view on graph neural networks as graph signal denoising</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dataset meta-learning from kernel ridgeregression</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=l-PrrQrK0QR" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph spanners</title>
		<author>
			<persName><forename type="first">David</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><forename type="middle">A</forename><surname>Schäffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of graph theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="116" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attributed graph models: Modeling network structure with correlated attributes</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">La</forename><surname>Fond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
				<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="831" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001-2010, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Homophily and contagion are generically confounded in observational social network studies</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohilla</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological methods &amp; research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="239" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral sparsification of graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="981" to="1025" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
				<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Dataset condensation with differentiable siamese augmentation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08259</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Dataset condensation with gradient matching</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep leakage from gradients</title>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
