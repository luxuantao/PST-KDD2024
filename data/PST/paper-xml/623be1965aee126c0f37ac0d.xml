<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Input-specific Attention Subnetworks for Adversarial Detection</title>
				<funder ref="#_ews7jea">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-23">23 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emil</forename><surname>Biju</surname></persName>
							<email>emilbiju@alumni</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anirudh</forename><surname>Sriram</surname></persName>
							<email>anirudhs@smail.iitm.ac</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
							<email>pratyushkpanda@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<email>miteshk@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Input-specific Attention Subnetworks for Adversarial Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-23">23 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12298v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-attention heads are characteristic of Transformer models. Individual attention heads are interpretable in different ways. One, for a token in an input sentence, we can visualize the attention paid by a head to all other tokens. Such attention patterns are attractive linguistically and have come to define roles for attention heads <ref type="bibr" target="#b19">(Pande et al., 2021)</ref>. Two, the output of attention heads from various layers can be probed for their ability to encode information related to the "NLP pipeline" <ref type="bibr" target="#b8">(Jawahar et al., 2019;</ref><ref type="bibr" target="#b25">Tenney et al., 2019;</ref><ref type="bibr" target="#b26">van Aken et al., 2019)</ref>. Three, attention patterns of heads can represent knowledge learnt by a teacher model when distilling to a smaller student model <ref type="bibr" target="#b10">(Jiao et al., 2020)</ref>. While individual attention heads are interpretable in the above ways, it is found that attention heads in models such as BERT are over-provisioned and can be pruned. For instance, <ref type="bibr" target="#b14">Michel et al. (2019)</ref> showed that a model with 16 attention heads per layer can be pruned to just one. <ref type="bibr" target="#b28">Voita et al. (2019)</ref> and <ref type="bibr" target="#b2">Budhraja et al. (2020)</ref> have shown similar results with different pruning techniques across tasks.</p><p>In the above methods, while interpretation of attention heads is input-specific, pruning of heads is input-agnostic. Can these two be combined, i.e., can we prune attention heads in an input-specific manner creating opportunities for interpretation? We explore this idea to identify an altogether different utility of attention heads -namely adversarial detection which is the task of differentiating between authentic and adversarial inputs. Specifically, we propose a method to obtain an input-specific attention subnetwork (IAS), which is a subnetwork where a subset of attention heads is masked without affecting the output of the model for that input. Such subnetworks could vary across inputs representing how the model works for each input. This is particularly important for adversarial detection, as adversarial inputs do not reveal themselves in what the model outputs but may leave tell-tale signs in how the model computes this output.</p><p>In this work, we present a technique to efficiently compute IAS and demonstrate its utility in adversarial detection with significantly improved accuracy over all current methods. To this end, we propose three sets of features from IAS. The first feature, F mask , is simply the attention mask that identifies if an attention head is retained or pruned in IAS. The second feature, F flip , characterizes the output of a "mutated" IAS obtained by toggling the mask used for attention heads in the middle layers of IAS. The third feature, F lw , characterizes the outputs of IAS as obtained layer-wise with a separately trained classification head for each layer. We train a classifier, called AdvNet, with these features as inputs to predict if an input is adversarial.</p><p>We report results on 10 NLU tasks from the GLUE benchmark (SST2, MRPC, RTE, SNLI, MNLI, QQP, QNLI) and elsewhere (Yelp, AG News, IMDb). For each of these tasks, we first create a benchmark of adversarial examples com-bining 11 attack methodologies like Word order swap <ref type="bibr" target="#b20">(Pruthi et al., 2019)</ref>, embedding swap <ref type="bibr" target="#b17">(Mrk?i? et al., 2016)</ref>, word deletion <ref type="bibr" target="#b4">(Feng et al., 2018)</ref>, etc. In total, the benchmark contains 5,686 adversarial examples across tasks and attack types. To the best of our knowledge, this dataset is the most extensive benchmark available on the considered tasks. Across all these tasks and attack types, we compare our adversarial detection technique against state-ofthe-art methods such as DISP <ref type="bibr" target="#b37">(Zhou et al., 2019)</ref>, NWS <ref type="bibr" target="#b16">(Mozes et al., 2021)</ref>, and FGWS <ref type="bibr" target="#b16">(Mozes et al., 2021)</ref>. Our method establishes the best results in all tasks and attack types, with an average improvement of 7.45% over the best method for each task. Our detector achieves an accuracy of 80-90% across tasks suggesting effective defense against adversarial attacks.</p><p>Having established the utility of attention heads for adversarial detection, we perform several ablation studies. First, we compare different combinations of the features demonstrating that they are mutually informative and thus combining them all works best. Second, we show that CutMix data augmentation <ref type="bibr" target="#b34">(Yun et al., 2019)</ref> improves accuracy, demonstrating the first use of this method in adversarial detection in NLP tasks. Third, we show that the detector is more accurate as the size of the language model scales. This is encouraging because larger language models are expected to have increased spurious correlations and thus are more vulnerable to adversarial attacks. Fourth, we show that the detector performs well even for modest training sizes of adversarial examples, suggesting effective generalization. In summary, we propose a novel relation between attention heads and adversarial detection. The effectiveness of the resultant detector establishes that the mask of attention heads captures critical information about how a Transformer model works for a given input.</p><p>The rest of the paper is organized as follows. We detail our core method of computing IAS in the next section. In Section 3 we discuss the features from IAS for adversarial detection. We detail the experimental setup along with the dataset creation process in Section 4. We present our results in Section 5 and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Input-Specific Attention Subnetworks</head><p>In this section, we describe Input-specific Attention Subnetworks (IAS) and the computational approach to identify IAS for a given input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>We consider a BERT-style encoder model where each layer consists of multi-headed self-attention and position-wise FFN. Let an input x consist of T tokens each represented by d v -dimensional vectors. Let X j ? R T ?dv be the representation at the input of the j th layer. Let W Q ji , W K ji , W V ji be the projection matrices of the i th self-attention head in the j th layer. We define</p><formula xml:id="formula_0">Q ji = X j W Q ji , K ji = X j W K ji , V ji = X j W V</formula><p>ji as the query, key, and value corresponding to the head respectively. Each selfattention head performs a scaled dot-product attention on the query, key, and value to generate the head's output. The output of all the heads in a layer are concatenated and passed through the FFN.</p><formula xml:id="formula_1">Head ji (X j ) = softmax Q ji K T ji ? d k V ji (1) Layer j (X j ) = concat i [Head ji (X j )]W O j (2)</formula><p>where d k is the dimensionality of each key vector and W O j is a learnable parameter. A pre-trained model is fine-tuned on a specific task, such as sentiment classification. Let ? be the set of trainable network parameters which are optimized to minimize a task-specific training loss for each input x:</p><formula xml:id="formula_2">L ? (x) = L CE (f (x, ?), y),<label>(3)</label></formula><p>where f (?) is the function computed by the model with parameters ? for input x, L CE is the standard cross-entropy loss function and y is the expected model output for input x. The overall training loss is averaged across all |x| inputs, i.e., L ? = 1 |x| x L ? (x). Let f (?) represent the output class generated from f (?) and ? * be the set of optimal network parameters obtained after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representing IAS</head><p>In an IAS, a subset of attention heads are pruned. We represent a continuous relaxation of pruning by modifying Eqn. 1 to weigh the output of each head by a scalar gating value g ji ? [0, 1]. The j th layer of the modified network is given by Layer</p><formula xml:id="formula_3">m j (X j ) = concat i [g ji ?Head ji (X j )]W O j (4)</formula><p>During inference, we constrain the gating values to be binary to characterize either exclusion or inclusion of a head: g ji is replaced by g b ji ? {0, 1} which defines the attention mask for the input x: of layers and m is the number of heads per layer. We represent the output class predicted by the IAS for an input x by f g (x, ? * , g b ). We call the subset of attention heads that are assigned a gating value of 1 as active heads and note that the active heads jointly define a subnetwork, called IAS. We illustrate IAS with an example. Figure <ref type="figure">1</ref> shows the BERT-Base model with 12 layers and 12 heads per layer. For two specific inputs, the corresponding attention masks are shown with their active heads in green. Thus, IAS is input-specific and characterizes how the model processes the input in a relatively low-dimensional space of [0, 1] 144 .</p><formula xml:id="formula_4">g b (x) = {g b ji } ? {0, 1}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Computing IAS</head><p>We compute IAS by treating the gating values as free variables to optimize the task-specific loss (Eqn. 3) for a given input x. In this optimization, the network parameters ? are frozen. Each gating value, g ji is defined as g ji = f HC (p ji ), where p ji is the free variable that is optimized and f HC is a version of the hard concrete distribution <ref type="bibr" target="#b12">(Louizos et al., 2017)</ref> given as</p><formula xml:id="formula_5">1 1+e ??(log(1-p ji )-log(p ji ))</formula><p>, where, ?=6 gave the best results for our work. Let g be the gating vector as optimized by minimizing the loss for a specific input. We need to enforce that g is binary. Unlike approaches by <ref type="bibr" target="#b28">Voita et al. (2019)</ref> and <ref type="bibr" target="#b31">Wang et al. (2020)</ref>, we do not include a regularization term in the training objective. Instead, we retain only those heads for which the gating values ascend the fastest towards 1, as measured after a certain ? number of epochs. Specifically, each binary value g b ji is derived from g ji after ? epochs as:</p><formula xml:id="formula_6">g b ji (x) = 1, if g ji (x) ? ? ? max(g(x)) 0, otherwise<label>(5)</label></formula><p>where, ?(&lt; 1) is a thresholding parameter and max(g(x)) is the largest among nm gating values.</p><p>For our work, we set ? = 10 and ? = 0.8. Two exceptional cases may arise. First, if the binary gating values of all heads in a layer are thresholded to 0, then the largest gating value in that layer is forced to 1 to ensure information flows through the network. Second, if the IAS predicts the wrong class for that input, then ? is reduced successively in steps of 0.2 until the output of the IAS is correct. For 98% of the inputs, the subnetwork predicted the target class within ? = 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model for Adversarial Detection</head><p>In this section, we explain how we extract features from the IAS and the design of the classifier for adversarial detection. We use the term target class to refer to the class predicted by the complete finetuned network for an input. For authentic inputs, this translates to the true class while for adversarial inputs, this refers to the adversarial class that the model is fooled into predicting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention mask F mask</head><p>The IAS identifies a subnetwork through which important information flows for a particular input. We hypothesize that this flow could be different for authentic and adversarial inputs. Thus, the first feature we extract, F mask , is just the pre-activation value p for the gating values of each head in the IAS. Thus, for a BERT-base model with 12 layers and 12 heads per layer, F mask is a 144 dimensional vector. We also define F bmask which uses the binary gated values g b instead of the real-values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features from flipping heads in IAS F flip</head><p>Adversarial inputs rely heavily on the network architecture and specific parameter combinations to fool the model <ref type="bibr" target="#b30">(Wang et al., 2019)</ref>. Hence, slight changes to network parameters can render an adversarial perturbation non-adversarial. We thus hypothesize (and later illustrate in Section 5.2) that if we flip some of the heads in the IAS, it could significantly change the output for adversarial inputs but not by as much for authentic inputs. Which heads should we flip? We take motivation from studies that show that middle layers of BERT capture syntactic relations <ref type="bibr" target="#b7">(Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b6">Goldberg, 2019)</ref> and are multi-skilled <ref type="bibr" target="#b19">(Pande et al., 2021)</ref>, making them crucial for prediction. In contrast, the initial layers are responsible for phraselevel understanding while the last few layers are highly task-specific <ref type="bibr" target="#b8">(Jawahar et al., 2019)</ref>. Hence, we choose to flip the gating values g b of heads in the middle layers of IAS, specifically, the middle n 3 layers, i.e., we drop heads that were earlier active and include earlier inactive heads. We denote the modified gating vector after flipping as g f .</p><formula xml:id="formula_7">g f ji = g b ji , if j ? n 3 or j ? 2 n 3 1 -g b ji , if n 3 ? j &lt; 2 n 3 (6)</formula><p>We run each input x through this mutated subnetwork and obtain a 4-dimensional feature vector, F flip consisting of the predicted class given by f g (x, ? * , g f ), the target class y, the confidence of prediction, and a flag asserting equality between predicted and target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Layer-wise auxiliary features F lw</head><p>Studies <ref type="bibr" target="#b31">(Wang et al., 2020;</ref><ref type="bibr" target="#b33">Xie et al., 2019)</ref> have shown that intermediate representations of adversarial inputs diverge from those of authentic inputs as we progress into deeper layers. This indicates that layer-wise information may be discriminative of adversarial inputs. Hence, instead of having a single classifier head processing the output of the final layer, we propose to train a classifier head at the output of each layer and use the classes predicted by them as features in adversarial detection. Specifically, on the fine-tuned complete model, we freeze the standard model parameters to ? * and train n -1 classifiers separately with a classifier head attached to each of the first n -1 layers to predict the target class. Following the convention in Eqn. 3, the training loss for the l th classifier head with parameters ? l on input x is given by:</p><formula xml:id="formula_8">L ? l (x) = L CE (f l g (x, ? * ? ? l , {1} nm ), y),<label>(7)</label></formula><p>where f l g (?) gives the output class computed by the l th classification head of a network with gating vector g. The overall training loss is given by</p><formula xml:id="formula_9">L ? = 1 (n-1)|x| x l L ? l (x).</formula><p>Let ? * be the set of optimal parameters obtained after training.</p><p>Then for a given input, we construct the IAS after flipping heads as given by the gating vector g f and compute the outputs of the n -1 layer-wise classifiers, i.e., the output of the l th classifier head is given by f l g (x, ? * ? ? * l , g f ). We then create an n + 1 dimensional feature, F lw , which consists of the n -1 output labels with two other scalars: (a) the number of these outputs that match the target class, and (b) the number of times these outputs change when traversed in the order of layers.</p><p>In summary, we compute the features as follows. First, the model is fine-tuned on the task. Then, layer-wise classification heads are trained while keeping the model parameters frozen. Thus, given an input, we first optimize and compute IAS from which we extract F mask . Then, the gating values of the middle layers are flipped and we extract F flip . Finally, on the IAS with flipped heads, layer-wise classifier outputs are used to extract F lw .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classifier for adversarial detection</head><p>We refer to our classifier as AdvNet, which takes as input, an (nm + n + 5)-dimensional vector F (x) which is the concatenation of F mask , F flip , F lw and generates a binary output classifying if a given input is authentic or adversarial. AdvNet consists of two 1-D convolutional layers with ReLU activation, two fully connected layers with sigmoid activation, and a final classification layer with softmax activation. Since adversarial inputs are slow and computationally expensive to generate, we employ the CutMix algorithm <ref type="bibr" target="#b34">(Yun et al., 2019)</ref> for data augmentation. In CutMix, we slice out patches from feature vectors of multiple inputs in the training set, each of which could be authentic or adversarial, and combine them to generate new feature vectors. Their respective ground truth labels are mixed in proportion to the length contributed by each patch (see Figure <ref type="figure" target="#fig_0">2</ref>). Formally, if {x i } R i=1 is a random subset of training set samples, an augmented feature vector from CutMix is defined by</p><formula xml:id="formula_10">F ( x) = concat i [F (x i )[p i : p i+1 ]],</formula><p>where 0 = p 1 &lt; p 2 &lt; ... &lt; p R+1 = nm + n + 5 and the mixed ground truth label is given by y = i y i (p i+1 -p i ). Using soft labels by mixing ground truth labels also offers better generalization and learning speed <ref type="bibr" target="#b18">(M?ller et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NLU tasks for evaluation</head><p>We choose the following 10 standard NLU tasks for performing our experimental studies: SST-2 <ref type="bibr" target="#b24">(Socher et al., 2013)</ref>, Yelp polarity <ref type="bibr">(Zhang et al., 2015a)</ref>, IMDb <ref type="bibr" target="#b13">(Maas et al., 2011)</ref>, AG News <ref type="bibr">(Zhang et al., 2015b)</ref>, MRPC <ref type="bibr" target="#b3">(Dolan and Brockett, 2005)</ref>, RTE <ref type="bibr" target="#b29">(Wang et al., 2018)</ref>, MNLI <ref type="bibr" target="#b32">(Williams et al., 2018)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref>, QQP 1 and QNLI <ref type="bibr" target="#b29">(Wang et al., 2018;</ref><ref type="bibr" target="#b21">Rajpurkar et al., 2016)</ref>. We refer the reader to Appendix A for further details on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset creation</head><p>To perform adversarial detection, we require a combined set of authentic and adversarial samples for each task. First, we fine-tune a BERT-based model for each task using its publicly available training set. Then, samples from its test set for which the fine-tuned model makes correct predictions constitute the set of authentic samples for that task. Second, we generate adversarial samples by attacking the fine-tuned model using a broad set of 11 hard attack types to comprehensively test AdvNet's performance and its generalizability to diverse perturbations. The attacks include word-level attacks: deletion <ref type="bibr" target="#b4">(Feng et al., 2018)</ref>, antonyms, synonyms, embeddings <ref type="bibr" target="#b17">(Mrk?i? et al., 2016)</ref>, order swap <ref type="bibr" target="#b20">(Pruthi et al., 2019)</ref>, PWWS <ref type="bibr" target="#b22">(Ren et al., 2019)</ref>, TextFooler <ref type="bibr" target="#b11">(Jin et al., 2020</ref>) and character-level attacks: substitution, deletion, insertion, order swap <ref type="bibr" target="#b5">(Gao et al., 2018)</ref>. We use the popular TextAttack framework <ref type="bibr" target="#b15">(Morris et al., 2020)</ref> for implementations of these attacks. Resulting perturbed samples that successfully fool our complete fine-tuned model constitute the set of adversarial samples for that task. On the combined authentic and adversarial set, we make a 70-10-20 split for creating training, validation and test sets for adversarial detection using AdvNet. Our dataset contains a total of 5,686 adversarial inputs across tasks and attack types and is publicly available at https: //github.com/emilbiju/Bert-Paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>Our adversarial detection model, AdvNet, contains two 1D convolutional layers followed by two fully connected layers. The two convolutional layers 1 quoradata.quora.com/First-Quora-Data set-Release-Question-Pairs have a kernel size of 3 and generate 32 and 16 output feature maps. The two fully connected layers have output dimensions of 32 and 16 with dropout rates of 0.1. We use the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001. We train the model for 100 epochs with early stopping on an NVIDIA K80 GPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Discussion</head><p>In this section, we first analyse the IAS (Section 5.1) and the constituent features of AdvNet (Section 5.2). We then perform a comparative study with state-of-the-art adversarial detection methods (Section 5.3). Lastly, we perform ablation studies to understand the effect of task, model size, feature combinations and training set attacks on the performance of AdvNet (Section 5.4). Unless otherwise stated, the plots pertain to experiments on the SST-2 dataset with the BERT-Base model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Active heads in IAS</head><p>We first check the number of active heads in IAS for a given input. To do so, we plot the progression of gating values with epochs when optimizing them for a given input (see Figure <ref type="figure" target="#fig_1">3</ref>). We observe that only a small fraction of heads (shown in green) are active at the end of the optimization process, thus resulting in a sparse vector. The green curves that are below the blue (threshold) line correspond to the two exceptional cases discussed at the end of Section 2.3. While the above plot was for a single randomly selected input, in Figure <ref type="figure" target="#fig_2">4</ref> we show the fraction of inputs with a given number of active heads for all the datasets used in this work. The relatively small modes and the right skew distributions imply that the extracted IAS are often sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature-specific analysis</head><p>We now analyze the individual effectiveness of the three features proposed in Section 3. Attention mask (F mask ). We first show that the attention mask is strongly correlated with the input's target class. To do so, we project the binary vector g(x) for each authentic input x onto a 2Dplane using the t-SNE method (van der Maaten and Hinton, 2008) as shown in Figure <ref type="figure" target="#fig_4">5</ref>(a), (b). We observe that inputs from different classes separate into distinctly separate clusters. Thus, the attention mask is discriminative of an input's target class as the choice of active heads depends on it. Interestingly, even if the attention computed for the same word location in two distinct inputs are the same, the heads attending to each word and responsible for generating different output classes are different.</p><p>We present a similar plot with both authentic and adversarial inputs in Figure <ref type="figure" target="#fig_4">5(c</ref>). We note that adversarial inputs group together with the authentic inputs whose true class is the same as their adversarial/target class. Within clusters of the same target class, there is a only a moderate distinction between adversarial and authentic inputs. But we show in further experiments that a better separation is possible when the complete nm-dimensional vector is used as opposed to a 2D projection.</p><p>Features from flipping heads in IAS (F flip ). For each of the datasets, we compute the percentage of authentic and adversarial inputs which generated non-target class predictions. We find that the mutated IAS after flipping heads in the middle layers is more likely to predict the correct target class output for an authentic input than an adversarial one.</p><p>We also study the confidence of the mutated IAS in making these predictions using a CDF plot (Figure <ref type="figure" target="#fig_5">6</ref>) over the output logit corresponding to the target class.</p><p>We observe that F flip predicts the target class with higher confidence in case of authentic inputs than adversarial ones. Specifically, only 9% of authentic inputs had prediction confidence lower than 0.85 as compared to 20% of adversarial inputs. Further, it predicts a non-target class with high confidence for some adversarial inputs. For example, 30% of adversarial inputs with prediction confidence higher than 0.85 gave the wrong prediction. In contrast, flipping the initial/final layers of the IAS instead of the middle layers did not significantly change the model prediction for either authentic or adversarial samples, making it difficult it to distinguish them. Layer-wise auxiliary features (F lw ). In Figure <ref type="figure" target="#fig_6">7</ref>, we plot the distribution of auxiliary output mismatches (non-target class predictions) across network layers. We observe that for most layers, the fraction of authentic inputs having target class predictions is higher than adversarial inputs. The differences are particularly large for the last few layers. On average across datasets, we observed that 52.5% of adversarial inputs generate more than 2 auxiliary output predictions that do not match the target class while only 23.1% of authentic inputs do the same. Additionally, when traversing the layerwise outputs in order, we observed that the output predictions of adversarial inputs switch among possible classes more often than for authentic inputs (see Appendix D). These observations justify the features that we include in F lw . Based on the above analyses, we have demonstrated that all 3 features of IAS are informative for adversarial detection. Our results in the next section corroborate these findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance on Adversarial Detection</head><p>Following the observations in the previous section, we use AdvNet with the identified features for adversarial detection. We compare the performance of AdvNet with the current state-of-theart approaches for detecting adversarial inputs for BERT-based models, viz., FGWS <ref type="bibr" target="#b16">(Mozes et al., 2021)</ref>, NWS <ref type="bibr" target="#b16">(Mozes et al., 2021)</ref>, DISP <ref type="bibr" target="#b37">(Zhou et al., 2019)</ref> and FreeLB <ref type="bibr" target="#b38">(Zhu et al., 2019)</ref>. We briefly describe these methods in Appendix C.</p><p>As seen in Table <ref type="table" target="#tab_1">1</ref>, AdvNet significantly outperforms existing approaches across all 10 datasets with an average improvement of 7.45%. We report an improvement of 6.53% for the 3 sentiment analysis datasets (SST-2, Yelp, IMDb), 8.05% for the 4 NLI datasets (RTE, SNLI, MNLI, QNLI) and 6.98% for the 2 paraphrase detection datasets (MRPC, QQP) over the respective best methods.</p><p>Another baseline that we compare with is Certified Robustness Training <ref type="bibr" target="#b9">(Jia et al., 2019)</ref>. While this work is not aimed at adversarial detection, it provides bounds on model robustness for word substitution perturbations. For making a comparison with our work, we note that the fraction of adversarial samples that are correctly detected as adversarial translates to robustness for binary classification tasks. We report robustness of 87% for word substitution-based attacks and 81% across all 11 attacks for IMDb, while the best upper bound obtained through certified robustness training is 75%.</p><p>When comparing across datasets, we observe that AdvNet performs better on simpler sentence labelling datasets like SST-2 and AG News when compared to more complex tasks like RTE and MRPC which require comparison between sentences. Existing work <ref type="bibr" target="#b19">(Pande et al., 2021)</ref> shows that for simpler tasks, the BERT heads perform discrete non-overlapping roles, while for complex tasks, there is greater overlap in head roles and a few heads perform more than one role. We hypothesize that this nature implies that the attention masks for different inputs even belonging to the same type (authentic or adversarial) can vary widely. This reduces the consistency of features across input types making the detection harder. Nevertheless, AdvNet establishes state-of-the-art results across datasets. A detailed analysis of the performance of AdvNet across tasks and attack types is provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation studies</head><p>We now evaluate how variations in model size, training set size, and the choice of feature combinations effect performance of AdvNet. Effect of model size. IAS can be computed for Transformer networks of any size. We compare BERT-Small and BERT-Base models in terms of  performance of AdvNet as shown in Table <ref type="table" target="#tab_1">1</ref>. We observe that, across datasets, AdvNet performs better in detecting adversarial inputs fed to the larger BERT-Base model (108M parameters) as opposed to the smaller BERT-Small model (25M parameters). The increase in accuracy averaged across tasks is a significant 10.76%. We hypothesize that this is because models with more layers encode more information and allow for a better build-up of semantic information which means that individual heads play more discrete roles. This better performance for the larger model is encouraging as the more accurate and larger language models are expected to be more vulnerable to adversarial attacks. Effect of training set size. In Figure <ref type="figure" target="#fig_7">8</ref> Using different feature combinations. We had shown that each of the three features are informative in Section 5.2. In Table <ref type="table" target="#tab_2">2</ref>, we report the performance of AdvNet by ablating various model components. The first 3 columns report accuracies when only one of the three features is passed at a time to the model. We observe that F mask performs better than F flip and F lw . This suggests that the attention mask is the most important feature input to the model. We analyze the roles of individual gating values using GradCAM (see Appendix F). Next, we test the performance when the boolean attention mask F bmask is used instead of the realvalued vector F mask along with F flip and F lw . The lower accuracy indicates that the real values are more informative. Finally, we test the model performance when CutMix is not used and conclude that augmenting the training set using CutMix provides higher accuracy as seen in the last row of Table <ref type="table" target="#tab_1">1</ref> which uses all 3 features along with CutMix. Defense Transferability Analysis. Next, we perform a study to understand how well the model can perform on unseen attack types. For this purpose, we train AdvNet with samples from only x% of the 11 attack types and report results both on test samples from the remaining attack types and the complete test set for x ? {25, 50, 75} in Table <ref type="table" target="#tab_4">3</ref>. We observe that even when AdvNet is trained with only 75% of the attack types, the test results on new attacks outperform existing approaches for most datasets, thus showing that our model can generalize to unseen attack methods. Besides, at all three values of x, the results on the complete test set closely agree with the results on the new attack types. This indicates that the reduction in accuracy at lower x values can largely be attributed to a smaller training set than to a lack of defense transferability.  In summary, our results show that (a) the 3 IAS features are individually informative, (b) AdvNet significantly improves on baseline methods across datasets, (c) AdvNet performance improves with model size and does not drop much on reducing training sets, (d) AdvNet achieves the best performance when all 3 features are used along with CutMix augmentation, and (e) AdvNet generalizes well to new attack types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>In this work, we present an altogether new utility of attention heads in Transformer networks -to detect adversarial attacks. We defined input-specific attention subnetworks (IAS) and proposed a method to compute them efficiently. We extracted 3 features from IAS and showed their utility in distinguishing adversarial samples from authentic ones. We demonstrated that our approach significantly improves the state-of-the-art accuracy across datasets and attack types. Our work suggests that input-specific model perturbations provide strong signals to interpret Transformer-based models such as large language models. Further, the sparse nature of the identified IAS indicate opportunities for input-specific model optimization. In future work, we would like to extend this study to tasks beyond NLU, including vision and speech-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Ethics and broader impact</head><p>One of the main challenges with deep neural models is their lack of explainability. These models typically have inherent biases resulting from the training data, parameter combinations and other factors that lead to unexpected responses to certain inputs. This is further complicated when adversarial agents target to manipulate the output of deep neural models. We see our work on creating and using attention subnetworks for adversarial detection as a part of the broader effort towards Responsible AI. Such a solution is particularly important in situations where deep neural models make decisions that affect physical safety, digital security and equal opportunity. However, we acknowledge that this additional visibility into the model comes at an added cost -inference under uncertainty of adversarial detection is more expensive. We encourage system designers to trade-off computational and runtime considerations for security when deploying such solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets used for authentic examples</head><p>The 10 datasets used in this work were listed in Section 4.1. Here, we provide additional details about these datasets. SST-2 <ref type="bibr" target="#b24">(Socher et al., 2013)</ref>, Yelp polarity <ref type="bibr">(Zhang et al., 2015a)</ref> and IMDb <ref type="bibr" target="#b13">(Maas et al., 2011)</ref> are binary sentiment classification datasets. AG News <ref type="bibr">(Zhang et al., 2015b)</ref> consists of news headlines classified into one of 4 categories (world, sports, business, sci/tech) and MRPC <ref type="bibr" target="#b3">(Dolan and Brockett, 2005)</ref> is a paraphrase dataset which contains sentence pairs with binary labels indicating whether they are semantically equivalent or not. RTE <ref type="bibr" target="#b29">(Wang et al., 2018)</ref>, MNLI <ref type="bibr" target="#b32">(Williams et al., 2018)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref> contain sentence pairs with labels indicating whether one sentences entails, contradicts or is neutral with respect to the other sentence. QQP is again a paraphrase dataset but unlike MRPC which contains sentences, it contains question pairs taken from Quora with binary labels indicating whether they are semantically equivalent or not. QNLI contains questioncontext pairs with a binary label indicating whether the context sentence contains the answer to the question or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Examples of adversarial attacks</head><p>In Table <ref type="table" target="#tab_6">4</ref>, we provide examples for each of the 11 attack types that we use to generate adversarial inputs for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Other methods for Adversarial Detection</head><p>We briefly describe the four methods that we compare with in Table <ref type="table" target="#tab_1">1</ref>.</p><p>? FGWS <ref type="bibr" target="#b16">(Mozes et al., 2021)</ref>: Here, a word frequency-guided approach is used to identify infrequent words in an input sentence and replace them with more frequent, semantically similar words. Then, the difference in prediction confidence of the Transformer-based model between the original and substituted sentences is considered. If this value is above a threshold, the sentence is predicted to be adversarial. ? NWS: This is the naive word substitution baseline used in <ref type="bibr" target="#b16">Mozes et al. (2021)</ref>. Here, each out-of-vocabulary word in an input sentence is replaced with a random word from a set of semantically related words, following which the same process as above is used to predict input authenticity.</p><p>? DISP <ref type="bibr" target="#b37">(Zhou et al., 2019)</ref>: In this approach, a BERT-based perturbation discriminator predicts whether each token in the input sentence is authentic or perturbed. If none of the tokens are predicted to be perturbed, the input sentence is considered authentic. ? FreeLB <ref type="bibr" target="#b38">(Zhu et al., 2019)</ref>: This is an adversarial training approach where adversarial perturbations are added to word embeddings and the resulting adversarial loss is minimized to promote higher invariance in the embedding space. ? Certified Robustness Training <ref type="bibr" target="#b9">(Jia et al., 2019)</ref>: This approach uses Interval Bound Propagation (IBP) to obtain an upper bound on the worst-case loss resulting from any word substitution-based perturbation. This has been applied to CNN and LSTM-based language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysing F flip and F lw</head><p>In the second column of Table <ref type="table" target="#tab_7">5</ref>, for each of the datasets, we show the percentage of authentic and adversarial inputs which generated non-target class predictions. Further, in the third column of Table <ref type="table" target="#tab_7">5</ref> we show the percentage of (authentic, adversarial) inputs whose layer-wise outputs showed more than one switch. These results show that the F flip and F lw are individually informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Adversarial detection accuracy for different attack types</head><p>In Table <ref type="table" target="#tab_8">6</ref>, we present the breakup of model accuracy across individual attack types. We observe that for text classification tasks like SST-2, Yelp and AG News the accuracy for Embedding and Synonym swap attack types are much higher compared to other datasets. We also note that in case of both word and character-level attacks, Deletion and Substitution operations are the ones with least detection accuracy across almost all datasets. Finally, we observe that the performance for detecting adversarial inputs generated by PWWS and TextFooler attacks remain fairly consistent across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Refereeing heads in adversarial detection</head><p>In this section, we explore the influence of each gating value in generating the prediction for our adversarial detection model. We make use of the Grad-CAM <ref type="bibr" target="#b23">(Selvaraju et al., 2017)</ref>    have large gradients from the target class (authentic or adversarial) flowing through them. Among these, we consider neurons that correspond to the gating values, i.e, F mask and call the heads corresponding to them as refereeing heads. From Figure <ref type="figure" target="#fig_8">9</ref>, we observe that word swap attacks like antonyms, synonyms, and embeddings require a greater number of refereeing heads, while character-level attacks need fewer. This is because character-level changes make the token invalid, i.e, the model treats it as a unknown token absent in the vocabulary. Since this changes the input embedding sequence more dramatically <ref type="bibr" target="#b0">(Biju et al., 2020)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Demonstration of CutMix used to mix patches from two input feature vectors of length L each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The trajectory of gating values of individual heads during the optimization to compute IAS. Only a few heads (in green) reach the threshold and remain active in IAS.</figDesc><graphic url="image-1.png" coords="5,327.53,401.38,175.50,127.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fraction of inputs with a given number of active heads from BERT-Base. Notice that in most cases, only 20-40 heads out of 144 remain active.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Projections with t-SNE on the attention mask for (a) SST-2, (b) AG News, and (c) authentic and adversarial inputs. Projection of attention masks are strongly discriminative of class and weakly of adversarial inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CDF over the target class output logit of the mutated IAS. The large area below the green curve with logit value&lt;0.5 corresponds to a large number of adversarial inputs whose mutated IAS predict a non-target class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Fractions of authentic and adversarial inputs that generate a non-target class prediction at each layerwise classification head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of training set size on accuracy of adversarial detection with AdvNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Fraction of refereeing heads used by the adversarial detection model across various adversarial attack types. The split of these across 4 layer subsets is also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the adversarial detection accuracy of AdvNet using features extracted from fine-tuned BERT-Small and BERT-Base models with other state-of-the-art approaches for adversarial detection.</figDesc><table><row><cell>Model</cell><cell cols="3">SST-2 Yelp AG News MRPC IMDb SNLI RTE MNLI QQP QNLI</cell></row><row><cell>FGWS</cell><cell>71.93 78.36</cell><cell>70.41</cell><cell>69.85 75.98 75.41 71.23 60.23 73.52 78.14</cell></row><row><cell>NWS</cell><cell>70.31 74.72</cell><cell>65.62</cell><cell>68.02 65.72 71.82 64.27 56.94 70.20 74.58</cell></row><row><cell>DISP</cell><cell>68.73 70.15</cell><cell>66.38</cell><cell>62.22 75.23 72.92 66.40 59.34 69.86 76.92</cell></row><row><cell>FreeLB</cell><cell>77.60 82.54</cell><cell>75.55</cell><cell>72.41 79.85 79.80 64.29 58.10 65.69 76.40</cell></row><row><cell>AdvNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/ BERT-Small 78.57 76.72</cell><cell>78.63</cell><cell>75.05 74.09 72.07 73.64 64.26 68.71 74.47</cell></row><row><cell cols="2">w/ BERT-Base 90.74 87.68</cell><cell>91.78</cell><cell>84.61 81.18 82.50 80.43 72.61 75.27 86.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on feature combinations.</figDesc><table><row><cell>, we show</cell></row><row><cell>how the performance of AdvNet changes as the</cell></row><row><cell>amount of training data changes. We observe that</cell></row><row><cell>AdvNet performs well even when it uses only a</cell></row><row><cell>fraction of the training set. Specifically, even at</cell></row><row><cell>40% of the training examples used, AdvNet out-</cell></row><row><cell>performs the results obtained with existing state-of-</cell></row><row><cell>the-art models on most tasks. This suggests that the</cell></row><row><cell>CutMix data augmentation is effective and the Ad-</cell></row><row><cell>vNet model is sample-efficient. This is particularly</cell></row><row><cell>important because designing adversarial examples</cell></row><row><cell>for each dataset remains a challenging task.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Defense transferability study of AdvNet with varying percentages of attack types included in the train set. Each tuple contains the test accuracy on new attack types and on all attack types respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>approach to identify critical neurons in the input layer of AdvNet that</figDesc><table><row><cell>Attack Type</cell><cell>Perturbed Text</cell></row><row><cell>Original Text</cell><cell>it 's a charming and often affecting journey.</cell></row><row><cell>Word-level attacks</cell><cell></cell></row><row><cell>Deletion</cell><cell>it's a _ and often affecting journey.</cell></row><row><cell>Antonyms</cell><cell>it's a repulsive and often affecting journey.</cell></row><row><cell>Synonyms</cell><cell>it's a charming and often affecting passage.</cell></row><row><cell>Embeddings</cell><cell>it's a charming and quite affecting journey.</cell></row><row><cell>Order Swap</cell><cell>it's charming and affecting a often journey.</cell></row><row><cell>PWWS</cell><cell>it's a entrance and often strike journey.</cell></row><row><cell>TextFooler</cell><cell>it's a charming and _ affecting journey.</cell></row><row><cell>Original Text</cell><cell>a sometimes tedious film.</cell></row><row><cell>Character-level attacks</cell><cell></cell></row><row><cell>Substitution</cell><cell>a sometimes tidious fylm.</cell></row><row><cell>Deletion</cell><cell>a som_times tedio_s film.</cell></row><row><cell>Insertion</cell><cell>a sometimeDs tvedious film.</cell></row><row><cell>Order Swap</cell><cell>a smoetimes tedoius film.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Examples of 11 attack types used for adversarial data creation. '_' represents a deleted character and there is no character present at that position in the adversarial sample.</figDesc><table><row><cell></cell><cell>Non-target o/p</cell><cell>Switches&gt;1</cell></row><row><cell>Dataset</cell><cell>(Mutated)</cell><cell>(Layer-wise)</cell></row><row><cell>SST-2</cell><cell>(12.3, 34.2)</cell><cell>(37.9, 54.8)</cell></row><row><cell>IMDb</cell><cell>(0.33, 2.18)</cell><cell>(0.16, 1.45)</cell></row><row><cell>Yelp</cell><cell>(3.8, 5.3)</cell><cell>(0.83, 1.08)</cell></row><row><cell>AG News</cell><cell>(6.6, 22.8)</cell><cell>(3.2, 17.0)</cell></row><row><cell>MRPC</cell><cell>(21.3, 24.3)</cell><cell>(10.3, 8.77)</cell></row><row><cell>RTE</cell><cell>(24.5, 22.2)</cell><cell>(44.2, 50.9)</cell></row><row><cell>SNLI</cell><cell>(2.83, 96.0)</cell><cell>(11.6, 41.0)</cell></row><row><cell>MNLI</cell><cell>(11.0, 24.8)</cell><cell>(24.3, 42.5)</cell></row><row><cell>QQP</cell><cell>(3.2, 1.3)</cell><cell>(6.2, 6.8)</cell></row><row><cell>QNLI</cell><cell>(5.7, 1.0)</cell><cell>(13.8, 11.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Percentages of (authentic, adversarial) inputs whose (a) mutated subnetworks generated non-target class predictions; (b) layer-wise outputs showed more than one switch.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>, small deviations from standard gating patterns are sufficient to mislead the model leading to fewer refereeing heads. Since introducing synonym and embedding based perturbations change the embeddings input to the model by a smaller extent, larger deviations from the gating pattern are required to block or pass selective chunks of information to mislead the model. Accuracies across datasets for each attack type. Legend: SUB-substitution, DEL-deletion, SYN-synonym, EMBED-embedding, INS-insertion, SWAP-order swap, TF-TextFooler. Refer Section 4.1 for descriptions of attack types. The second column provides the number of adversarial samples generated by us for each task across all 11 attack types.</figDesc><table><row><cell cols="2">Dataset #Adv</cell><cell cols="3">Word-level attacks</cell><cell>Character-level attacks</cell></row><row><cell></cell><cell cols="5">samples DEL ANT SYN EMBED SWAP PWWS TF SUB DEL INS SWAP</cell></row><row><cell>SST-2</cell><cell>739</cell><cell>0.84 0.96 0.95</cell><cell>0.96</cell><cell>0.75</cell><cell>0.81 0.76 0.92 0.80 0.87 0.89</cell></row><row><cell>Yelp</cell><cell>589</cell><cell>0.75 0.92 0.92</cell><cell>0.96</cell><cell>0.88</cell><cell>0.80 0.95 0.93 0.77 0.88 0.88</cell></row><row><cell>AG News</cell><cell>829</cell><cell>0.88 0.96 0.92</cell><cell>0.96</cell><cell>0.82</cell><cell>0.83 0.84 0.89 0.84 0.85 0.88</cell></row><row><cell>MRPC</cell><cell>712</cell><cell>0.75 0.75 0.9</cell><cell>0.72</cell><cell>0.94</cell><cell>0.84 0.82 0.86 0.79 0.76 0.92</cell></row><row><cell>IMDb</cell><cell>321</cell><cell>0.80 0.76 0.85</cell><cell>0.89</cell><cell>0.80</cell><cell>0.82 0.81 0.94 0.75 0.96 0.79</cell></row><row><cell>SNLI</cell><cell cols="2">1262 0.61 0.80 0.78</cell><cell>0.88</cell><cell>0.78</cell><cell>0.76 0.79 0.85 0.88 0.65 0.83</cell></row><row><cell>RTE</cell><cell>541</cell><cell>0.75 0.84 0.86</cell><cell>0.87</cell><cell>0.79</cell><cell>0.77 0.73 0.82 0.76 0.82 0.82</cell></row><row><cell>MNLI</cell><cell>548</cell><cell>0.67 0.80 0.72</cell><cell>0.85</cell><cell>0.78</cell><cell>0.80 0.76 0.78 0.80 0.86 0.76</cell></row><row><cell>QQP</cell><cell>307</cell><cell>0.70 0.82 0.74</cell><cell>0.80</cell><cell>0.75</cell><cell>0.76 0.74 0.78 0.81 0.86 0.77</cell></row><row><cell>QNLI</cell><cell>395</cell><cell>0.80 0.90 0.92</cell><cell>0.92</cell><cell>0.90</cell><cell>0.82 0.86 0.82 0.86 0.82 0.82</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="institution">Samsung</rs> and <rs type="institution">IITM Pravartak</rs> for supporting our work through their <rs type="programName">joint fellowship program</rs>. We also wish to thank the anonymous reviewers for their efforts in evaluating our work and providing us with constructive feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ews7jea">
					<orgName type="program" subtype="full">joint fellowship program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint transformer/RNN architecture for gesture typing in indic languages</title>
		<author>
			<persName><forename type="first">Emil</forename><surname>Biju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.87</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="999" to="1010" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the weak link between importance and prunability of attention heads</title>
		<author>
			<persName><forename type="first">Aakriti</forename><surname>Budhraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhura</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3230" to="3235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pathologies of neural models make interpretations difficult</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3719" to="3728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Black-box generation of adversarial text sequences to evade deep learning classifiers</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Lou</forename><surname>Soffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Security and Privacy Workshops (SPW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="50" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05287</idno>
		<title level="m">Assessing bert&apos;s syntactic abilities</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019-57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Certified robustness to adversarial word substitutions</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerem</forename><surname>G?ksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is BERT really robust? A strong baseline for natural language attack on text classification and entailment</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8018" to="8025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l_0 regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01312</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">X</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Yong</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Grigsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frequency-guided word substitutions for detecting textual adversarial examples</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mozes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bennett</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00892</idno>
		<title level="m">Counter-fitting word vectors to linguistic constraints</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in bert</title>
		<author>
			<persName><forename type="first">Madhura</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakriti</forename><surname>Budhraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preksha</forename><surname>Nema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09115</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11268</idno>
		<title level="m">Combating adversarial misspellings with robust word recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples through probability weighted word saliency</title>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Shuhuai Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1085" to="1097" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.74</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How does bert answer questions? a layer-wise analysis of transformer representations. CIKM &apos;19, page 1823-1832</title>
		<author>
			<persName><forename type="first">Betty</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>L?ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358028</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09418</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial sample detection for deep neural network through model mutation testing</title>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1245" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interpret neural networks by extracting critical subnetworks</title>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6707" to="6720" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01626</idno>
		<title level="m">Character-level Convolutional Networks for Text Classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2015b. Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to discriminate perturbations for blocking adversarial attacks in text classification</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1496</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4904" to="4913" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Freelb: Enhanced adversarial training for language understanding</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.11764</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
