<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look-Ahead Based Fuzzy Decision Tree Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ming</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and Computer Science</orgName>
								<orgName type="laboratory">Artificial Neural Systems Laboratory</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
								<address>
									<postCode>45221-0030</postCode>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ravi</forename><surname>Kothari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and Computer Science</orgName>
								<orgName type="laboratory">Artificial Neural Systems Laboratory</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
								<address>
									<postCode>45221-0030</postCode>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Look-Ahead Based Fuzzy Decision Tree Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2844E308402357A75A15DADF15ACC0AB</idno>
					<note type="submission">received November 17, 2000; revised February 26, 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Decision tree</term>
					<term>classification</term>
					<term>fuzzy ID3</term>
					<term>fuzzy systems</term>
					<term>gain ratio</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Decision tree induction is typically based on a top-down greedy algorithm that makes locally optimal decisions at each node. Due to the greedy and local nature of the decisions made at each node, there is considerable possibility of instances at the node being split along branches such that instances along some or all of the branches require a large number of additional nodes for classification. In this paper, we present a computationally efficient way of incorporating look-ahead into fuzzy decision tree induction. Our algorithm is based on establishing the decision at each internal node by jointly optimizing the node splitting criterion (information gain or gain ratio) and the classifiability of instances along each branch of the node. Simulations results confirm that the use of the proposed look-ahead method leads to smaller decision trees and as a consequence better test performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D ECISION trees represent a simple and powerful method of induction from labeled instances <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. At a given node of the tree, a stopping criterion (based on the fraction of instances correctly classified or the cardinality of the compatible instances or some other similar measure) is used to determine if additional child nodes are necessary. If additional child nodes are required, they are added based on a node splitting criterion and the process is repeated for each of the new internal nodes until a completely discriminating tree is obtained. This standard approach to decision tree construction thus, corresponds to a top-down greedy algorithm that makes locally optimal decisions at each node.</p><p>One of the strengths of decision trees compared to other methods of induction is the ease with which they can be extended to nonnumeric domains. This allows decision trees to be used in situations where considerable cognitive uncertainty is present and the representation of the instances is in terms of symbolic or fuzzy attributes <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The uncertainty may be present in obtaining numeric values of the attributes or in obtaining the exact class to which a specific instance belongs. Fuzzy decision trees have also been used either in conjunction with or as precursors to other empirical model construction paradigms <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Of course, there exists close parallels between the development in the areas of crisp and fuzzy decision tree induction.</p><p>One of the challenges in decision tree induction is to develop algorithms that produce decision trees of small size and depth. In part, smaller decision trees lead to lesser computational expense in determining the class of a test instance. More signifi-cantly, however, larger decision trees lead to poorer generalization (test) performance <ref type="bibr" target="#b11">[12]</ref>. Motivated by these considerations, a large number of algorithms have been proposed toward producing smaller decision trees. Broadly these may be classified into three categories.</p><p>1)</p><p>The first category includes those efforts that are based on different criteria to split the instances at each node. Some examples of the different node splitting criteria include entropy or its variants <ref type="bibr" target="#b0">[1]</ref>, the chi-square statistic <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, the G statistic <ref type="bibr" target="#b13">[14]</ref>, and the GINI index of diversity <ref type="bibr" target="#b11">[12]</ref>. Despite these efforts, there appears to be no single node splitting that performs the best in all cases <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>; nonetheless there is little doubt that random splitting performs the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>The second category is based on pruning a decision tree either during the construction of the tree or after the tree has been constructed. In either case, the idea is to remove branches will little statistical validity (see for example, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>The third category of efforts toward producing smaller decision trees is motivated by the fact that a locally optimum decision at a node may give rise to the possibility of instances at the node being split along branches, such that instances along some or all of the branches require a large number of additional nodes for classification. The so called look-ahead methods attempt to establish a decision at a node by analyzing the classifiability of instances along each of the branches of a split <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Surprisingly, mixed results (ranging from look-ahead makes no difference to look-ahead produces larger trees <ref type="bibr" target="#b20">[21]</ref>) are reported in the literature when look-ahead is used. Of these approaches, look-ahead has been the least frequently studied. In addition, the fact that it produces mixed results is counter intuitive. In this paper, we propose a novel method of evaluating the classifiability of instances along the branches of a node split <ref type="bibr" target="#b21">[22]</ref>. Based on this method, we propose a fuzzy decision tree induction algorithm that utilizes look-ahead to produce smaller decision trees. We have organized the rest of the paper as follows. In Section II, we briefly review Fuzzy ID3 (F-ID3) <ref type="bibr" target="#b4">[5]</ref> as well as the gain ratio (GR) method <ref type="bibr" target="#b1">[2]</ref> and in that context, establish the need for examining the structure of the instances in each branch of a node. In Section III, we propose a new mechanism for look-ahead and propose a look-ahead based fuzzy decision tree induction algorithm. Experimental results and presented in Section IV and our conclusions appear in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FUZZY DECISION TREE INDUCTION</head><p>In this section, we introduce the notation used in the rest of the paper and briefly examine the popular F-ID3 algorithm <ref type="bibr" target="#b4">[5]</ref>   as well as the GR <ref type="bibr" target="#b1">[2]</ref> method of inducing decision trees. Though we integrate the proposed look-ahead method with F-ID3 and GR, it is in fact easy to integrate it with other node splitting criterion, for example, the one based on minimum classification ambiguity <ref type="bibr" target="#b3">[4]</ref>.</p><p>Let the universe of objects be described by attributes . An attribute takes values of fuzzy subsets . There are a total of training instances. Based on the attributes, an object is classified into fuzzy subsets . F-ID3 is based on an adaptation of the classical definition of entropy. This adaptation, or fuzzy entropy as it is also called, for a subset can be defined as <ref type="bibr" target="#b0">(1)</ref> where, is the relative frequency of the th subset of attribute with respect to and defined as <ref type="bibr" target="#b1">(2)</ref> and is the cardinality of a fuzzy set <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The attribute chosen to split the instances at a given node is based o, <ref type="bibr" target="#b2">(3)</ref> where, returns that value of the index for which is the smallest and (4)</p><p>It is easy to see that choosing an attribute to split the instances based on the above equations results in the maximum decrease in fuzzy entropy, i.e., one that maximizes the cardinality of compatible instances.</p><p>F-ID3, however, has an obvious drawback just like ID3 algorithm. It favors attributes with a large number of possible attribute values. To discourage a large number of partitions, a factor based on the entropy of the size of the splits was proposed giving rise to the GR measure <ref type="bibr" target="#b1">[2]</ref>. More specifically <ref type="bibr" target="#b4">(5)</ref> where is the information gain defined as with being the fuzzy entropy at a node, is as given by ( <ref type="formula">4</ref>) and is the information value of attribute defined as <ref type="bibr" target="#b5">(6)</ref> One problem is that GR might choose attributes with very low IV scores, rather than those with high information gain. To avoid this, we can simply calculate the average fuzzy entropy value for all the attributes at a node then select only from among those with below average fuzzy entropy values. Irrespective of whether information gain or the gain ratio is used, the construction of a decision tree represents a greedy search strategy that implements a locally optimum decision at each node. The difficulty is that a combination of locally optimal decisions can not guarantee the global optimum tree i.e., a tree with smallest size. Indeed it is an NP-hard problem to find the smallest tree or one with the least number of leaves <ref type="bibr" target="#b17">[18]</ref>.</p><p>To illustrate this further and to motivate the consideration of look-ahead in the construction of a decision tree we present an illustrative example. Consider the data in Table <ref type="table" target="#tab_0">I</ref>, which is a modification of the data used in <ref type="bibr" target="#b3">[4]</ref>. There are four attributes. decision tree that is possible. In these trees, a node is considered terminal if the cardinality M( <ref type="formula">1</ref>) is less than one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>These attributes and the values they can assume are defined below</head><p>There are three classes with class labels Plan A, Plan B, and Plan C. The numbers in Table <ref type="table" target="#tab_0">I</ref> are the membership values. Membership values are not probabilities, thus, it is not necessary for membership values of all linguistic terms of an attribute to add to one.</p><p>The construction of the tree, based on F-ID3 proceeds as follows. At the root node, the fuzzy entropy for all the attributes based on (4) are Thus, using (3), we choose the attribute Outlook, which has the lowest fuzzy entropy at the root node. Repeating this further for each of the child nodes, we arrive at the decision tree shown in the top panel of Fig. <ref type="figure" target="#fig_0">1</ref>. On the contrary, if attribute Wind (which does not have the lowest fuzzy entropy but leads to a simpler distribution of patterns at the child nodes) is chosen, a smaller sized tree is obtained as shown in the bottom panel of Fig. <ref type="figure" target="#fig_0">1</ref>. In these trees, a node is considered terminal if the cardinality of incorrectly classified instances is less than one. It is because of this that one finds all child nodes with the same class label. This implies that at the parent node was greater than 1; however at each the child nodes is less than 1. This example, though simple, illustrates that choosing an attribute based on jointly considering the node splitting criterion (information gain here) and classifiability of the instances along all the child branches can lead to smaller decision trees <ref type="foot" target="#foot_0">1</ref> .</p><p>A similar situation arises when GR is considered as shown below As before we would still choose Outlook which has the highest GR at the root node. In this case, the overall tree has 19 nodes (not shown), which is still larger than that shown in the bottom panel of Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LOOKAHEAD BASED FUZZY DECISION TREE INDUCTION</head><p>The goal of look-ahead is to evaluate the classifiability of instances that are split along branches of a give node. The question is how to evaluate the classifiability? Prior efforts (within the context of crisp decision trees) used a classifier parameterized identically to the one producing the split to evaluate the classifiability of the instances along each branch of the split. For example, when a linear discriminant is used to split the instances, then a linear discriminant is used to evaluate the instances in each of the two subsets of instances resulting from the split. This corresponds to a single step look-ahead. The difficulty however, is that to obtain a good estimate of the classifiability one would typically need a multilevel look-ahead. Even so, it is possible that in a situation where three-step look-ahead is used, a very different outcome would result if a four-step look-ahead is used. The argument can be generalized and it becomes clear that if a classifier is actually used to evaluate the classifiability then an exhaustive look-ahead is necessary. Clearly, this is not feasible in all but the most trivial of cases. As earlier efforts used a few (one or two mostly) step look-ahead, they did not consistently obtain better results <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Indeed, in some cases they obtained poorer results with look-ahead than without.</p><p>We propose to characterize the classifiability of instances that are split along branches of a give node using a nonparametric method. The motivation for our approach can be more easily explained within the context of a crisp dataset. Consider a twoclass situation with attributes. In dimensions ( variables and one for the class label), one may thus, visualize a surface. When the instances of a class are interlaced with another class, this surface is rough. However, when there are compact and disjoint class regions, this surface is considerably smoother. This is akin to visualizing the class label as defining the surface in dimensions. Fig. <ref type="figure" target="#fig_1">2</ref> shows the distribution of some instances in two-dimensions (2-D). The third dimension is the class label (zero for one class and one for the other). It is clear that the distribution in the bottom panel is more amenable to subsequent classification. It may also be observed that the surface formed by the class label is considerably smoother in the bottom panel. Thus, we propose to evaluate the classifiability of instances that are split along branches of a give node in terms of the smoothness of the class label surface of instances assigned to that branch. A widely used way of characterizing the smoothness (or roughness) of a surface in image processing is through the use of a cooccurrence matrix <ref type="bibr" target="#b22">[23]</ref> and we adopt it here with some modifications to characterize the texture of the class-label surface 2 .</p><p>In the following, we formally define the characterization of the class-label surface.</p><p>Definition 1: Let denote the membership value of instance for the th value of th attribute. The distance between instance and is defined by <ref type="bibr" target="#b6">(7)</ref> For any instance in the data set, we can find those instances that are within a circular neighborhood of radius , of instance 2 It is conceivable that there are other methods of evaluating the classifiability.</p><p>For example, one can use the purity of the k-nearest neighbors of an instance. It is not immediately clear if any advantage is obtained by such a method over the one proposed here. Our choice of a texture based method however does conform to the intuitive notion of class label surface roughness as used here. , based on the distance defined above. We can then define the local cooccurrence matrix for instance as follows.</p><p>Definition 2: Let denote the membership value of instance for class and let . The local cooccurrence matrix of instance is defined by <ref type="bibr" target="#b7">(8)</ref> The size of the local cooccurrence matrix is (recall that is the number of classes). It captures the distribution of instances around a specific instance. In other words, the element of matrix shows the number of class instances that are within the neighborhood of instance when instance belongs to class with membership . Note, child nodes are created when an attribute is used to split the instances at a given node. We can get the cooccurrence matrix for each branch (after the th attribute is selected) by simply adding the cooccurrence matrix of all the instances along that branch and the overall cooccurrence matrix of the instances by summing the cooccurrence matrix along each branch.</p><p>Definition 3: Local coccurrence matrix after attribute is selected <ref type="bibr" target="#b8">(9)</ref> where denotes any instance in the th child node of current node.</p><p>Note that in the ideal case (perfect classifiability) matrix becomes a diagonal matrix. In general, the off-diagonal terms correspond to instances of different classes that occur with a neighborhood of radius . We first normalize such that the sum of the elements of is one. Thus, we can define the classifiability, as</p><p>where is an element in row and column of the matrix . The look-ahead term allows us to formulate an objective function that allows a decision to be made at a node so as to jointly optimize the node splitting criterion (information gain, GR, or any other chosen criterion) as well as classifiability of instances along all branches of the node. More specifically, we define the following objective function <ref type="bibr" target="#b10">(11)</ref> where, can be equal to as defined in ( <ref type="formula">4</ref>) or be equal to as defined in <ref type="bibr" target="#b4">(5)</ref>, and represents the look-ahead term discussed above. An attribute selected to split the instances at a given node is then based on <ref type="bibr" target="#b11">(12)</ref> Equations ( <ref type="formula">9</ref>)-( <ref type="formula">12</ref>) can then be used to choose an attribute to split the instances at a particular node. The resulting split in this case corresponds to one that maximizes the number of correct classifications at the node as well as facilitates the classification of the instances in each branch of the node. In the next section, we present some experimental results obtained using the proposed algorithm. Of course, when in ( <ref type="formula">11</ref>) is 0, the proposed algorithm becomes identical to the F-ID3 algorithm or the GR algorithm (depending on what is defined as).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>We present results with seven data sets to illustrate that the proposed look-ahead based fuzzy decision tree induction algorithm can produce smaller trees. Except for the first simulation, all data sets can be obtained from the University of California, Irvine, Machine Learning Repository <ref type="bibr" target="#b23">[24]</ref> or from the restricted area of the UCI Machine Learning Repository <ref type="bibr" target="#b24">[25]</ref>.</p><p>For each of the data sets we obtain the performance of Fuzzy ID3 based on information gain by itself (IG), Fuzzy ID3 combined with the proposed method of look-ahead (IG LA), GR by itself and GR combined with the proposed method of lookahead (GR LA). In general, it is possible to incorporate the proposed method of look-ahead with other node splitting criterion (for example, with the minimum classification ambiguity <ref type="bibr" target="#b3">[4]</ref>).</p><p>We followed a consistent method of fuzzifying the data for all cases. When an attribute is categorical, the fuzzification is quite straightforward. We just treat each of the possible values of the attribute as a fuzzy subset. In this case, the membership value in a fuzzy subset is either zero or one. For numerical attributes, we use the -means clustering algorithm to cluster the attribute values in to three clusters representing three linguistic terms . The choice of the number of clusters is arbitrary though we were guided by the notion that a value can typically be thought of as being low, average, or high. We generate the memberships based on a triangular membership function and the three cluster centers ( with ) obtained through -means clustering. Specifically (13) ( <ref type="formula">14</ref>) <ref type="bibr" target="#b14">(15)</ref> In all our simulations, the class label is not fuzzified, i.e., each instance belongs to a single class.</p><p>When separate training and testing data sets are present, results are reported based on construction of the decision tree with the training data and subsequent labeling of the instances in the testing data. When no separate training and testing data sets are present, we ran the algorithm three times. For each of the runs, we randomly divided the available data into two partitions-a training partition and a test partition. In this case, numbers reported are based on averaging over the three runs. The inference rule for the testing data is quite simple. Assume at some node, we choose attribute to split the data. When a test instance comes in, it goes to th child node if it has the highest membership value in th possible value of attribute . This means that the fuzzy inference is actually the same as the nonfuzzy inference with the nonfuzzy partition for continuous features using three intervals:</p><p>. These intervals correspond to the three membership functions defined in ( <ref type="formula">13</ref>)- <ref type="bibr" target="#b14">(15)</ref>. We repeat this procedure until we arrive at a leaf node where we classify the instance according to the class label at that leaf node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set I: The Illustrative Example of Section II</head><p>The first simulation is based on the example data set of Section II. To decide on the attribute to select based on the proposed method, we first obtain the local cooccurrence matrix and classifiability of the instances along the branches of the root node using ( <ref type="formula">9</ref>) and <ref type="bibr" target="#b9">(10)</ref> for each of the attributes as and and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and and</head><p>From the above, it is clear that the use of the attribute Wind results in much greater classifiability. Using <ref type="bibr" target="#b10">(11)</ref> and ( <ref type="formula">12</ref>) we also obtain the attribute Wind as the attribute of choice for the root node. Proceeding in this way, we obtain the decision tree shown in bottom panel of Fig. <ref type="figure" target="#fig_0">1</ref>. Comparing it to the original decision tree, it is clear that the one obtained using the proposed algorithm is significantly smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MONK's Problem</head><p>For the next three simulations, we consider the well known MONK's data sets available at the UCI Machine Learning Repository <ref type="bibr" target="#b23">[24]</ref>. The MONK's data sets are actually three subproblems. The domains for all MONK's problems are the same. There are 432 instances that belong to two classes and each instance is described by seven attributes . Among the seven attributes, there is one ID attribute (a unique symbol for each instance), which is not related to classification and is ignored in our simulations.</p><p>Data Set II: MONK-1 Problem: The target concept associated with the MONK-1 problem is or . Table <ref type="table" target="#tab_2">II</ref> summarizes the results obtained. It is clear that the use of look-ahead results in better performance. F-ID3 has 85 nodes while F-ID3 LA has only 41 nodes. The results with GR and GR LA are similar. The testing accuracy is also better. Data Set III: MONK-2 Problem: The target concept associated with the MONK-2 problem is: exactly two of . Table <ref type="table" target="#tab_2">II</ref> shows the results obtained. As in the previous case, the use of the proposed look-ahead method results in a considerable improvement. Data Set IV: MONK-3 Problem: The target concept associated with the MONK-3 problem is ( and ) or ( and ). 5% noise is added to the training set. Results obtained are shown in Table <ref type="table" target="#tab_2">II</ref>.</p><p>In this case, there is only a slight improvement. This is because of the agreement of the first term and the second  term of <ref type="bibr" target="#b10">(11)</ref>. In other words, an attribute that has highest information gain, happens to have the highest classifiability. This, however, is usually not the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Set V: Breast Cancer Data</head><p>For the next simulation we consider a real world dataset, i.e., the breast cancer data available in the restricted area of the UCI Machine Learning Repository <ref type="bibr" target="#b24">[25]</ref>. The dataset has 286 instances that are described by nine attributes and belong to two classes: no-recurrence-events and recurrence-events. There are a total of nine instances that have missing attribute values. We removed those nine instances and used the remaining 277 instances. As there is no separate testing data in this case, we used 70% of the data for training and 30% of the data for testing. The algorithm was run thrice, each time with a randomly sampled training data set (70% of the total data) and a testing data set (30% of the total data). Average of the results obtained are shown in Table <ref type="table" target="#tab_2">II</ref> and reflect both a smaller decision tree obtained as well as a slight improvement in test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Set VI: Wisconsin Breast Cancer Data</head><p>For the following simulation, we took the Wisconsin Breast Cancer data. Each pattern in the data set has nine inputs and an associated class label (benign or malignant). The two classes are known to be linearly inseparable. The total number of instances are 699 (458 benign, and 241 malignant), of which 16 instances have a single missing attribute. We removed those 16 instances and used the remaining 683 instances. As there is no separate testing data in this case, we used 50% of the data for training and 50% of the data for testing. In part, the reason for the equal split in this case as opposed to the 70%-30% split of the previous simulation is that there are a larger number of instances and using 50% of the data for training resulted in a sufficiently large training data set. As before, the algorithm was run thrice, each time with a randomly sampled training data set (50% of the total data) and a testing data set (50% of the total data). Average of the results obtained are shown in Table <ref type="table" target="#tab_2">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Data Set VII: Glass Identification Data</head><p>For the final simulation, we consider the glass identification data set. There are totally 214 instances and each instance is described by ten attributes (including an Id attribute). All attributes are continuous valued. There are two classes: window glass and nonwindow glass. As there is no separate testing data in this case, we used 70% of the data for training and 30% of the data for testing. As before, the algorithm was run thrice, each time with a randomly sampled training data set (70% of the total data) and a testing data set (30% of the total data). The results obtained are shown in Table <ref type="table" target="#tab_2">II</ref>.</p><p>A final comment about the simulations. In general, there is no definite way of knowing an appropriate value to use for [see <ref type="bibr" target="#b7">(8)</ref>] and [see <ref type="bibr" target="#b10">(11)</ref>]. Increasing the value of results in a larger emphasis being placed on the classifiability term and a smaller emphasis being placed on the information gain (or GR) term. In general, we found to consistently provide good results. In all our simulations was fixed at one. in general should increase linearly with , the number of attributes. Indeed, should be large enough such that at least a few instances are present within that neighborhood of each instance. should also be small enough such that classifiability is evaluated locally. Table <ref type="table" target="#tab_3">III</ref> shows the effect of varying for the MONK-1 data set when IG LA is used and when GR LA is used and Table <ref type="table" target="#tab_4">IV</ref> shows the effect of varying for the same data set. As these tables show there exist a considerable range of values for and for which the same results are obtained. However, as the first row of Table <ref type="table" target="#tab_4">IV</ref> shows that when is small then it is possible that there are not enough instances in the neighborhood of an instance making it difficult to evaluate classifiability. In that case, the algorithm degrades to one without look-ahead, i.e., plain IG or the GR algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we presented a novel approach for evaluating the classifiability of instances based on evaluating the texture of the class label surface. Based on that, we also proposed an algorithm for fuzzy decision tree induction using look-ahead. The proposed method of look-ahead when combined with either information gain or with gain ratio outperforms standalone information gain and gain ratio respectively. More specifically, it results in smaller decision trees and as a consequence better generalization (test) performance.</p><p>The proposed algorithm does require additional computation when compared to F-ID3 or GR. In particular, the look-ahead term requires finding instances that are within a distance from a given instance. The inter-instance distance, however, can be computed once, stored, and does not need to be computed at each node. This additional time may well be justified in most situations in light of the considerably improved performance that results from the decision trees constructed using the proposed algorithm.</p><p>The method of look-ahead proposed in this paper can also be used in crisp decision tree induction. Within the context of crisp decision tree induction, it is common to use a linear discriminant at each node. One step, or a few-step look-ahead, when used, is also based on a linear discriminant. Since exhaustive look-ahead is not feasible, it is conceivable that entirely different results are obtained with a -step look-ahead when compared to those obtained using a -step look-ahead. In addition, the use of a linear discriminant in doing the look-ahead implicitly assumes a model that is satisfied by the data distribution. The proposed method of look-ahead is nonparametric and therefore does not suffer from the bias of an assumed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The decision trees generated by choosing the attribute with the lowest fuzzy entropy at each node (top). The bottom panel shows an alternative and smaller</figDesc><graphic coords="3,160.74,218.52,268.80,153.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three-dimensional plot to show the smoothness of the surface caused by different distribution of instances. The distribution in the top panel is harder to classify (rough surface); the one on the bottom is easier to classify (smoother surface).</figDesc><graphic coords="4,321.60,222.84,213.12,161.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,116.94,89.22,359.52,213.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,113.04,117.24,367.20,332.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SYNTHETIC</head><label>I</label><figDesc>DATA TO MOTIVATE THE NEED FOR LOOK-AHEAD. NUMBERS IN THE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TABLE ARE THE MEMBERSHIP VALUES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II EXPERIMENTAL</head><label>II</label><figDesc>RESULTS. TRAINING AND TESTING ACCUARCY IS EXPRESSED IN PERCENTAGES. NODES IN THE DECISION TREE WERE ADDED UNTIL FEWER THANERRORTOLERANCE INSTANCES ARE IN ERROR. WHEN THE CLASS LABLE IS CRISP, THE SUM OF THE INCONSISTANT CARDINALITIES EQUALS THE NUMBER OF INSTANCES MISCLASSIFIED. PERFORMANCE IS REPORTED FOR IG, IG+LA (INFORMATION GAIN WITH THE PROPOSED MEHTOD OF LOOK-AHEAD), GR, AND GR+LA (GAIN RATION WITH THE PROPOSED METHOD OF LOOK-AHEAD) BASED ALGORITHMS. .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EFFECT</head><label>III</label><figDesc>OF VARYING ON THE IG+LA AND GR+LA ALGORITHMS FOR THE MONK-1 DATA SET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EFFECT</head><label>IV</label><figDesc>OF VARYING r ON THE IG+LA AND GR+LA ALGORITHMS FOR THE MONK-1 DATA SET.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This smaller tree was actually obtained using the algorithm we propose in this paper. Details of this algorithm appear in Section III.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">5: Programs for Machine Learning</title>
		<author>
			<persName><surname>C4</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fuzzy decision tree algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Induction of fuzzy decision trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous ID3 algorithm with fuzzy entropy measures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sztandera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Fuzzy Syst</title>
		<meeting>IEEE Int. Conf. Fuzzy Syst</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exemplar learning in fuzzy decision trees</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Janikow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FUZZ-IEEE</title>
		<meeting>FUZZ-IEEE</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1500" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fuzzy decision trees: Issues and methods</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating fuzzy rules by learning from examples</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1414" to="1427" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fuzzy neural trees</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sztandera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia Comput. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="87" to="104" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ontogenic neuro-fuzzy algorithm: F-CID3</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Cios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sztandera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="383" to="402" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fuzzy neural trees</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Sztandera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1/4</biblScope>
			<biblScope unit="page" from="155" to="177" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Research and Developments in Expert Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hart</surname></persName>
		</author>
		<editor>M. Bramer</editor>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Ed, Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expert systems-Experiments with rule induction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mingers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical comparison of selection measures for decision-tree induction</title>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="319" to="342" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A further comparison of splitting rules for decision-tree induction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simplifying decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the optimization of fuzzy decision trees</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oversearching and layered search in empirical learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cameron-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Artificial Intelligence</title>
		<meeting>14th Int. Conf. Artificial Intelligence<address><addrLine>San Mateo, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1019" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Heuristic search for model structure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Elder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from Data: Artificial Intelligence and Statistics V, Lecture Notes in Statistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fischer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-J</forename><surname>Lenz</surname></persName>
		</editor>
		<meeting><address><addrLine>Vienna, Austria, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="131" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lookahead and pathology in decision tree induction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Artificial Intell</title>
		<meeting>14th Int. Conf. Artificial Intell<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1025" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<title level="m">Lecture Notes in Pattern Recognition</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<title level="m">Computer Robot Vision</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Irvine, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Databases. Department of</title>
		<ptr target="http://www.ics.uci.edu/~mlearn/ML-Repository.html" />
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Irvine, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information and Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
